@&#MAIN-TITLE@&#
Prioritising data items for business analytics: Framework and application to human resources

@&#HIGHLIGHTS@&#
Identifies the selection of data items for business analytics as research question.Reviews business analytic literature on relevance of research question.Addresses research question with multi-criteria decision analysis framework.Applies framework to average HR department to provide list of standard data items.

@&#KEYPHRASES@&#
Business analytics,Business intelligence,Data requirements,Human resources,Multi-criteria decision analysis,

@&#ABSTRACT@&#
The popularity of business intelligence (BI) systems to support business analytics has tremendously increased in the last decade. The determination of data items that should be stored in the BI system is vital to ensure the success of an organisation's business analytic strategy. Expanding conventional BI systems often leads to high costs of internally generating, cleansing and maintaining new data items whilst the additional data storage costs are in many cases of minor concern – what is a conceptual difference to big data systems. Thus, potential additional insights resulting from a new data item in the BI system need to be balanced with the often high costs of data creation. While the literature acknowledges this decision problem, no model-based approach to inform this decision has hitherto been proposed. The present research describes a prescriptive framework to prioritise data items for business analytics and applies it to human resources. To achieve this goal, the proposed framework captures core business activities in a comprehensive process map and assesses their relative importance and possible data support with multi-criteria decision analysis.

@&#INTRODUCTION@&#
Business functions such as customer service, human resources (HR), IT, legal support, marketing, procurement, production, project delivery, R&D and sales can draw on widely used business intelligence (BI) system solutions. BI systems can be seen as the technological foundation to conduct business analytics (Chiang, Goes, & Stohr, 2012; Lim, Chen, & Chen, 2012). Liberatore and Luo (2010) define business analytics as a set of methods that transform raw data into action by generating insights for organisational decision making. For instance, knowing that younger women are more likely to buy a pricier version of a certain product helps marketers target advertisements appropriately. A widely endorsed representation of business analytics observes organisations mature from using descriptive quantitative data analyses to explain what is happening now through predictive analyses to estimate what will happen in the future to prescriptive analyses using management science tools to help decide what to do next (Delen & Demirkan, 2013; Liberatore & Luo, 2011; Lustig, Dietrich, Johnson, & Dziekan, 2010). All modern BI systems provide efficient data storage, allow data exploration with descriptive statistics using aggregation and grouping commands and present the results on attractive management dashboards. In addition, more advanced BI systems possess ready-to-use predictive data mining tools. Prescriptive analytics often require highly specialised add-ons for a BI system (e.g., pricing and inventory management tools).Operational research techniques have supported the development of BI systems in multiple ways (Mortenson, Doherty, & Robinson, 2015). Tracking and visualising key performance indicators (KPIs) is a traditional function of BI systems, and operational research has been employed to develop and structure such KPIs (Fortuin, 1988; Smith & Goddard, 2002). Optimisation methods play an important role in data mining, which in turn is commonly used in BI systems (Corne, Dhaenens, & Jourdan, 2012; Olafsson, Li, & Wu, 2008). ‘Soft’ OR has been applied to the design of new BI systems (e.g., Martínez, Lista, & Florez, 2013; Sørensen et al., 2010), and decision analysis has informed the evaluation of BI systems (e.g., Lin, Tsai, Shiang, Kuo, & Tsai, 2009; Rouhani, Ghazanfari, & Jafari, 2012). Finally, operational research methods such as forecasting, scenario analysis and simulation have been implemented on top of BI systems to enhance organisations’ business analytic capabilities (e.g., Negash & Gray, 2008; Telhada, Dias, Sampaio, Pereira, & Carvalho, 2013).This study addresses the decision problem of data items that a business function should store in its BI system to conduct business analytics. As most BI data is internally created, expanding a BI database leads to high administrative costs for generating, cleansing and maintaining new data items whereas data storage costs are typically of much lower magnitude – a conceptual difference to big data systems. The high costs of data creation require business function to be selective. We propose a prescriptive framework based on multi-criteria decision analysis to help business functions make this selection decision, i.e. to help business functions decide ‘what to do next’ in their analytic strategy to use the language of the aforementioned classification.To demonstrate the usefulness of the proposed framework, we apply it to HR. HR analytics are still in their infancy with a heavy focus on descriptive analytics (e.g., Infohrm, 2010; Smith, 2013), some applications of predictive analytics (e.g., Cascio & Boudreau, 2011; Fitz-enz, 2010) and little research in prescriptive analytics (e.g., Bordoloi & Matsuo, 2001; Canós & Liern, 2008). However, the interest in business analytics has grown tremendously in many HR departments within the last few years (Aral, Brynjolfsson, & Wu, 2012; Levenson, 2011). The launches of several HR BI systems such as Fusion, OrgVue, SuccessFactor and WorkDay reflect this development.This paper is organised as follows: Section 2 highlights the practical relevance of a framework to prioritise data items based on a literature review and our experience. Section 3 describes the framework in detail. Section 4 is dedicated to the application of the framework to HR. Section 5 concludes the study.The selection11This section is informed by a systematic literature search conducted in August 2015 using the Google Scholar functions. The 86 information system journals listed by the Association for Information Systems (2012) were chosen as a source. This list also includes several management and management science journals. The full-text search term (‘business analytics’ OR ‘business intelligence’ OR ‘data warehouse’) AND (‘data requirements’ OR ‘information requirements’ OR ‘data selection’ OR ‘data prioritisation’ OR ‘data prioritization’)) was used. All publications that discuss theoretical and practical aspects of the data selection problem for BI systems or address this problem in a case study are considered (inclusion criteria). The search identified 261 publications, of which 189 were excluded based on title and abstract and 54 after a full-text review. The remaining 18 publications that met the inclusion criteria are marked by asterisks in the reference list and cited in Section 2 of this paper. Further relevant publications have been identified through an unsystematic literature search using Google Scholar and through reviewing the bibliographies of selected papers.of data items that should be acquired and managed in a BI system to conduct business analytics is a significant challenge (e.g., Ballou & Tayi, 1999; Kirsch & Haney, 2006; Lawyer & Chowdhury, 2004; Loeb, Rai, Ramaprasad, & Sharma, 1998; March & Hevner, 2007; Mishra, Padhy, & Panigrahi, 2013; Sahay & Ranjan, 2008; Sen & Sen, 2005; Watson, Fuller, & Ariyachandra, 2004). Ultimately, the success of any BI system relies on available data (Ramakrishnan, Jones, & Sidorova, 2012). Although countless analyses (or metrics) for business data circulate in professional and academic literature, the problem of raw data that business functions should routinely collect in BI systems is surprisingly underexplored. However, this problem is of high practical relevance for four reasons. First, not all data items are equally useful. Business functions can usually draw on some multi-purpose data items that support many analyses, whereas other data items are only required for one particular analysis. Not surprisingly, multi-purpose data items tend to be judged as more relevant by practitioners in our experience and are therefore more frequently collected (see Section 4.3). Second, the extract-transform-load (ETL) process of cleansing data items from legacy systems and external sources to transfer them to a new BI system typically accounts for more than 50 per cent of the time and costs of a BI project (Davenport, Harris, de Long, & Jacobson, 2001; Shen, Liu, & Chen, 2012; Wang, Hu & Zhou, 2012). Third, in a study of small and medium enterprises (SMEs), which have previously setup or are in the process of setting up a BI system, 13 of 20 SMEs mentioned a lack of existing data items for business analytics as a major barrier for succeeding in their analytic strategy (Olszak & Ziemba, 2012; see also Marx, Mayer, & Winter, 2011). This observation is in line with our experience when talking to or working with practitioners. However, the creation of these missing data items is even more time consuming and expensive than cleansing and transferring them from an existing database. Fourth, the continuous update of a BI system with recent data can lead to high reoccurring costs (Choudhury & Sampler, 1997; Eppler & Helfert, 2004; Even, Shankaranarayanan, & Berger, 2007; Gilad & Gilad, 1985; Morrison, 2015). Examples of such costs include administrator time for monitoring that data entries are up to date, gathering customer opinions, license fees for proprietary market data, license and training costs for software packages for the central collection of data items and staff time for typing details about project milestones. Thus, potentially high costs associated with loading a BI system with data make it imperative that business functions are judicious about data items that they incorporate.Drucker (1995) and March and Hevner (2007) noted that managers should be able to generate both internal and external information from BI system's data. They considered internal information to include accounting and financial measures, productivity measures, measures relating to organisation's core competences and competitive advantages and measures relating to the allocation of organisation's scarce resources. On the other hand, they considered external information to include analyses of markets, customers, technologies, worldwide finance data and changes in world economy.Precise data items required to generate this internal and external information are typically derived from simple surveys, unstructured interviews and qualitative requirement models. Beynon-Davies (2004), Gilad and Gilad (1985), Kim and Gilbertson (2007), Loeb et al. (1998), Loshin (2012) and Shanks and Darke (1999) used surveys and manager interviews in their case studies to learn about users’ analytic needs. Qualitative data-requirement models aim at either integrating a selection of existing data sources into the BI system (supply-driven approach), systematically exploring users’ data needs (demand-driven approach) or combining both approaches in a meaningful manner. Dori, Feldman, and Sturm (2008), Romero and Abelló (2010b) and Takecian (2013) developed supply-driven approaches that derive the initial selection of data items from the conceptual model of operational databases. Prakash and Gosain (2008) constructed a demand-driven data-requirement model by breaking down organisation's goals into concrete decisions that are informed by simplified SQL queries (see also Romero & Abelló, 2010a). The demand-driven approach from Paim and Castro (2003) employed the Use Case system modelling language to explore how end users are likely to interact with the BI system, which leads to a list of required data items. Giorgini, Rizzi, and Garzetti (2008), Maté, Trujillo, and Franch (2014) and Mazón and Trujillo (2009) applied the widely used i* system modelling language to structure the actions of organisation's stakeholders (as data suppliers) and the goal-oriented decision making of managers (as users in demand of data), which allows gaining an overview of potentially useful data items for the BI system. Maté and Trujillo (2012) and Mazón, Trujillo, and Lechtenbörger (2007) also aimed at linking the data supply from existing databases with data requirements expressed by end users by employing advanced modelling languages. All mentioned models are concerned with (i) exploring data items that are potentially useful for the BI system and (ii) the structuring of the data items from a BI system architecture viewpoint. These models do not provide guidance on how the relevance of data items should be systematically assessed.According to Wetherbe (1991) and in line with the decision analysis and ‘soft’ OR literature (e.g., Phillips & Phillips, 1993; Rosenhead & Mingers, 2001), the discussion of data requirements in workshops tends to lead to better outcomes because of a wider knowledge base, a broader perspective and particularly the opportunity for mutual learning about the decision problem. The works of Ormerod (1995, 1996, 1998, 2005) on mixing ‘soft’ OR techniques to develop information system strategies and of Checkland (1988), Checkland and Holwell (1997) and Winter, Brown, and Checkland (1995) on the link between soft system methodology and information systems provide good examples of engaging workshop designs; however, these researchers do not explicitly address the problem of selecting data items.In addition, even workshops as a stand-alone approach, either by allowing end users to create their wish list or by giving them a prepared list of choices, often do not lead to completely satisfying outcomes. When allowing end users to create their wish list of data items, many BI systems initially fail to satisfy managers’ expectations because the managers usually do not know in advance what data and analyses they need (Jenkins, Naumann, & Wetherbe, 1984; Lederer & Prasad, 1993; Romero & Abelló, 2010b). The addition of the cleansed and tested missing datasets later to a previously launched BI system often leads to massively higher costs (Wetherbe, 1991) and typically takes three months each (Eckerson, 2005). Alternatively, when providing managers with a list of choices for potential analyses and required data items, they typically want all of them (Eckerson, 2010; Judd, Paddock, & Wetherbe, 1981).It becomes clear that engaging managers in a discussion about data items that they need in their BI system is required but not sufficient. A decision model which prioritises data items for business analytics would greatly inform such discussions. Our systematic search of the literature indicates that a formal model to prioritise data items for business analytics seems hitherto not to have been published. The present study addresses this gap in the literature.Finally, it should be noted that the problem described in this study primarily applies to conventional BI systems and not to big data systems.22The distinction between conventional BI systems and big data systems here is analogous to the distinction between BI&A 1.0 and BI&A 2.0/3.0 by Chen et al. (2012). According to a large survey by Kart et al. (2013), only 8% of the organisations have big data solutions in place in some of their business functions.Conventional BI systems are based on the paradigm that the user knows what questions (or hypotheses) they wish to answer and with what kind of analyses. Such BI systems allow the user to conduct their analyses on a well-structured relational database with managed data items using their background knowledge of the business. Thus, it is the user who decides on the relevance of particular data items to answer a particular question—a decision is made before accessing the data. Creating new data is often connected with some sorts of costs. In conventional BI systems, the organisation is typically the producer of new data (Chen, Chiang, & Storey, 2012) and therefore must bear the data creation costs. Whilst the creation of some data items is essential for the organisation's operations (e.g., finance accounts, client address and delivery status), many other data items exist whose creation is optional. Examples include rating of quality of delivered goods (procurement), reasons for investigating expense claims (finance), number and length of client visits (sales), types of problem solutions provided to individual customer calls (customer service) and case mix managed by in-house lawyers (legal support). The primary constraint for amassing large amounts of data in conventional BI systems is the high cost of internal data creation (including ETL).In contrast to BI systems, big data systems as, for instance, defined by Anderson (2008),Mayer-Schönberger and Cukier (2013) and Katal Wazid, & Goudar (2013) employ machine learning on large, diverse and unstructured datasets to make inferences about the probabilities of certain events or associations. In big data systems, it is the machine that decides how relevant particular data items are based on statistical analyses—a decision is made after accessing the data. To be effective, big data systems require vast streams of very cheaply generated data items. Sources for this cheap data may include data already generated by the organisation's enterprise resource system, customer interactions with web 1.0 systems (e.g., search terms and clickstreams), social media and RFID chips. The primary constraint for amassing large amounts of data in big data systems is IT costs for storage, computing power and human capital.We decompose business analytics into four hierarchical layers: process map, decisions, analyses and data items. The resulting top-down linking model connects data items to processes (see Fig. 1).The structure of the linking model is rooted in three common concepts in the enterprise information system literature. First, models assigning data items to processes can be found in many variants in the literature (e.g., Olson, 2003; Scheer, Abolhassan, Jost, & Kirchmer, 2002) as most data items are primarily used in particular business processes rather than being relevant to all of them. Second, managers usually do not really know what data items they need, as explained in Section 2. It is therefore recommended that managers not be asked what data or analyses they want but rather what decisions they have to make (Eckerson, 2010; March & Hevner, 2007; Wetherbe, 1991). Following this advice, important management decisions serve as a bridge between analyses and processes in the top-down linking model. Third, the process/decision-analyses-data hierarchy closely resembles the knowledge-information-data paradigm in enterprise information systems (Tuomi, 2000).BI systems are typically designed to be process oriented to make sure they support all core activates of a business function to a satisfying degree (Golfarelli, Rizzi, & Cella, 2004). Borrowed from supply chain management, the process map divides a business function into value streams, which in turn are further split into single processes. The decision layer captures the core questions to which the business function must be prepared to provide answers. Assigning each decision to a process ensures that the list of decisions considers all relevant business activities. The analysis layer comprises analytical models and metrics, which are of relevance for a particular business function. Each standard decision is informed by a set of analyses. The data-item layer contains the raw numerical and qualitative data required for conducting the analyses. Value-stream-to-process, process-to-decision and decision-to-analysis links are conceptualised as one-to-many relationships, but many-to-many relationships are allowed for analysis-to-data-item links, meaning that an analysis can require multiple data items, and a data item can feed into multiple analyses.In most cases, it is not reasonable to assume that organisations pay equal attention to each of the different decisions in the linking model. As an example, consider the following two marketing decisions: ‘How should we price our products in a particular market?’ and ‘How can the effectiveness of our social media copywriting be improved?’ Data that feeds the first decision are usually deemed much more valuable than the data required for the second one. This observation is incorporated into the framework by assigning different relative importance weights to decisions. The weighting approach used is explained in detail in two subsections: ‘hierarchical weighting’ and ‘neutral-good swing weighting’.Comparing the importance of decisions from very different business processes is cognitively demanding and often leads to somewhat arbitrary judgements (Saaty, 1990) The finance decisions ‘How much cash flow do we need?’ versus ‘Do we need to intervene on our expense claim policy?’ may serve as an example. Therefore, hierarchical weighting is used to break down the challenge so that only decisions from the same process are weighted against each other:•On the value stream level, the weights wh∈ [0, 1] for value streams h are rated against each other such that∑hwh=1.On the process level, the weights whi∈ [0, 1] for each process i within value stream h are rated against each other. The process weights whimust total 1 for each value stream, i.e.∑iwhi=1∀h.On the decision level, the weights whij∈ [0, 1] for each decision j within process i of value stream h are rated against each other. Again, the decision weights whijmust total 1 for each process, i.e.∑jwhij=1∀(h,i).This structure follows the well-established value-tree approach in multi-criteria decision analysis (Keeney & Raiffa, 1976).Simple direct-importance judgements (e.g., asking participants to rate the priority of each value stream on a scale between 0 [irrelevant] and 10 [absolutely crucial]) regularly lead to distorted results, for two reasons. First, participants might interpret an importance scale quite differently (e.g., what 5 actually means on a 0–10 scale) if not provided with an unambiguous description of the scale's meaning (Belton & Stewart, 2002). Second, the value of enhanced decision making in a business activity depends on how important the resulting performance difference is for the organisation rather than its absolute value (Cascio & Boudreau, 2011; Keeney, 2002). For instance, the HR process ‘Administer Payroll’ is highly relevant, but the value of improving this HR process by better decision making is very limited for most Western organisations as it is usually already done well enough.To avoid these two traps, we use swing weighting (Edwards & Barron, 1994; von Winterfeldt & Edwards, 1986) with neutral-good ranges (Bana e Costa, 1996; Bana e Costa, De Corte, & Vansnick, 2002). Assume an ‘average organisation’ of larger size whose capability for making decisions in the business function of interest is only average (median), meaning that 50 per cent of the other organisations are better in making each of the standard decisions and 50 per cent are worse. A decision swing (h, i, j) for decision j in process i of value stream h is defined as the improvement of the average organisation only in its capability of making decision j from ‘average’ to belonging to the ‘top 10 per cent’, ceteris paribus.33Instead of comparing the value of improving the decision-making capability for certain decisions, processes and value streams, some participants found it helpful to imagine how many things can go realistically wrong in an average organisation.By using swings, all participants base their importance assessments on a sufficiently shared perception of how far an average organisation's decision-making capabilities can be realistically enhanced in each area.Using classic swing weighting, the assessor p is first asked to name their most important decision swing within process i of value stream h, say(h,i,j^), (Bottomley & Doyle, 2001). This swing receives a provisional weightwhij^p*=100per cent. Doing so establishes a reference scale. For all other decisions j in this process, the assessor is asked to estimate the value of swing (h, i, j) relative to(h,i,j^). For instance,whij1p*=33per cent andwhij2p*=67per cent would mean that improving j1 and j2 have, respectively, one-third and two-thirds of the value of improving decisionj^. But improving decision j1 and j2 simultaneously should be judged as valuable as improving decisionj^by an assessor p who provides consistent importance judgements (consistency check). The provisional weightswhijp*of each participant are later normalised to whijpsuch that∑jwhijp=1∀(h,i). Afterwards, the geometric mean of the normalised weights whijpfrom all participants is assigned towhij*. The geometric mean is often used as simple consensus estimator by decision analysts in the absence of a real group discussion (e.g., Saaty, 1990; Vaidya & Kumar, 2006). Finally, all decision weightswhij*are again normalised such that∑jwhij=1∀(h,i).The process weights whiand value-stream weights whare also elicited by letting participants compare the average-to-top-performer swings in successful decision making for processes i and value streams h, respectively. The overall weight wjof each single decision is the product whwhiwhijwith h and i referring, respectively, to the value stream and process to which decision j belongs. Note that∑jwj=1.To keep participants motivated to think deeply about the relative values of swings, they should be engaged in conversations about the reasons behind their judgements (Keeney, 1992). Weighting consistency checks (French, Maule, & Papamichail, 2009), as in the example given in the last-but-not-least paragraph, should also be carried out. To make the preference elicitation more interactive, participants can be asked to do the swing weighting with cards on A3 sheets with the instructions for the weighting method (see Fig. 2).Instead of swing weighting, other methods such as AHP (Saaty, 1990), MACEBTH (Bana e Costa & Vansnick, 1994), SMART (Edwards, 1977; Edwards & Barron, 1994) and trade-off weighting (Keeney & Raiffa, 1976) would certainly be equally appropriate from a technical perspective (Pöyhönen & Hämäläinen, 2001). We chose swing weighting mainly because of its limited need for pairwise comparisons and its high technical transparency for non-experts. In line with research from Mau-Crimmins, De Steiguer, and Dennis (2005) and Schoemaker and Waid (1982), our hope is that the simplicity of this method will foster its acceptance and uptake by practitioners (see also Belton & Stewart, 2002; Edwards, von Winterfeldt, & Moody, 1988; Keeney & von Winterfeldt, 2007).Not all decisions can be informed equally well by business analytics. Long-range strategic decisions (e.g., ‘How many different suppliers should we have?’) are often harder to support with analytical reasoning than repetitive operational decisions (e.g., ‘Do we need to intervene on our suppliers’ delivery performance?’). Our proposed framework therefore incorporates the parameter data support dj, which indicates to what extent decisions should realistically be driven by data. For each decision j, professionals with a strong analytical background are asked to provide estimates for djon a 0–1 scale. For instance,dj=0.4means that the decision should be based on 40 per cent quantitative thinking and 60 per cent qualitative thinking (e.g., experience, common sense, logic or observations). Analogous to the weights and again following the ‘wisdom-of-the-crowd’ thinking (Ariely et al., 2000; Surowiecki, 2005), the geometric mean of the participants’ data support estimates is later assigned to the decision.In trials of the framework, participants judged it as difficult and tedious to assign precise numbers to dj. To make obtaining parameters a more pleasant task, a symmetric Likert-type scale of measurement (Carifio & Rocco, 2007; Likert, 1932) with five response options is used instead: almost none (dj=10percent%), low (dj=30percent%), medium (dj=50percent%), high (dj=70percent%) and almost sufficient (dj=90percent%). In addition, the response option ‘no data support possible’ (dj=0percent%) is included for decisions that cannot be supported meaningfully by analytical reasoning. To make the survey a more pleasant activity, the participating professionals can be asked to put coloured circular stickers on each decision to indicate into which of the six available data-support classes they see it belonging. Fig. 3shows an example of this assessment procedure.By using analyses as an intermediate layer, the framework links data items to decisions. As an approximation, we gauge all data items as equally useful for informing the decisions to which they are linked. This approximation is necessary to avoid eliciting a weight parameter for each single analysis and each single data item; a drawback is that it makes the value of a data item within a decision dependent on the number of other data items linked to this decision. This simplification may lead to an over- or underestimation of the contribution of a data item to inform a particular decision. We also assume that even if the decision maker does not possess all data items required for a certain analysis, the responsible person may nevertheless be able to draw some conclusions from it. For example, educated guesses, industry surveys, simulations or break-even analyses might be used for the missing data items (e.g., Cascio & Boudreau, 2011; Jackson, 2007).φjl=1if data item l is linked (via at least one analysis) to decision j; otherwise 0. Also, let njbe the number of data items l feeding into decision j. Furthermore, recall that the product whwhiwhijis the overall weight of decision j and djadjusts this weight by how useful data support is for making this decision. The priority index for data item l is defined as additive functionI(l)=∑hWh∑iWhi∑jWhijdjφjlnj.I(l) can be interpreted as an estimate of the overall relative importance of data item l for improving an averagely run business function.The development of the framework presented in this study was initiated and funded by a medium-sized consultancy specialising in business analytics. The consultancy wanted to provide clients of its HR software package with advice about what data they should store about their employees and HR activities. The project was conducted in the embedded research mode (Marshall et al., 2014; Wong, 2009), an emerging trend in applied research where organisations employ and host academics to conduct rigorous in-house research that responds to the needs of the hosting organisation and the academic community.The process map captures the standard activities of an HR department as described in numerous handbooks (e.g., Armstrong & Taylor, 2014; Bratton & Gold, 2012; Mitchell & Gamlem, 2012). While the grouping in chapters (‘value streams’) and subsections (‘processes’) varies between different books, a general consensus exists as to the main activities in HR. The process map depicted in Fig. 4 reflects the opinions of several HR consultants and consultancy clients involved in the embedded research project on how to group these standard activities in a meaningful way.Typical HR decisions include ‘What actions should be taken on individual absence cases?’ and ‘How much spare capacity do we need for business continuity?’ Through consulting HR handbooks and interviewing HR professionals, a total of 55 standard HR decisions were identified (see Appendix A). The list consists of operational decisions with a repetitive character and strategic one-off decisions.Typical HR analyses include the correlation between performance score and compensation, prediction of number of employees expected to leave the company, time to fill vacancies, employee count and estimated monetary value of performance difference in role. We gathered 298 descriptive and predictive HR analyses and assigned them to the appropriate decisions. A total of 202 are selected from the academic and professional literature (e.g., Bassi, Carpenter, & McMurrer, 2010; Becker, Ulrich, Huselid, & Huselid, 2001; Boudreau, 2010; Cascio & Boudreau, 2011; Davenport, Harris, & Shapiro, 2010; Fitz-enz, 2010; Fitz-enz & Davison, 2001; Infohrm, 2010; Kavanagh & Thite, 2009; SilkRoad, 2012; Smith, 2013) and the others from past consulting projects and suggestions provided by the HR analysts interviewed for this research. Fig. 5counts the number of analyses belonging to each HR process. One may infer from this balloon chart that some HR processes (e.g., Recruitment and Workforce planning) have received much attention by academics and HR analytics professionals in the last decades whereas others (e.g., Flexible working and Organisational design) have been less extensively addressed.Examples of HR data items are competence assessment, date of birth, disciplinary date, email traffic between employees, having received particular HR communication message, job board usage data, participation in work-life balance programme, salary benchmarks, strategic importance of role, time to full productivity and user feedback on online application process. The set of HR analyses used in this case study requires 126 data items grouped into 16 categories (see Table 1).To elicit the parameters required by the decision-analysis component of the framework, 24 HR professionals from 15 organisations with a diverse mix of size, ownership and industry were interviewed (see Table 2). On average, the volunteers needed 10 minutes to weight the value streams, 25 minutes to weight the processes, 30 minutes to weight the decisions and 40 minutes to assess the data support for decisions. Most participants mentioned after the interview that they found physically 'playing’ with the cards and circular stickers (as opposed to populating tables with numbers) fun and that they were not bored during the preference-elicitation session.The overall HR process weightswi=whwhiare depicted in Fig. 8. The decision weights and data support judgements are provided in Appendix A. Summing up the overall weighted data support (i.e.,∑hwh∑iwhi∑jwhijdj), one may conclude that enhanced data collection and data use can contribute 36 per cent of the effort required for an average HR department to become top performing in all its activities—a number judged as reasonable by the research participants.The resulting top 30 HR data items according to the suggested framework are presented in Fig. 6. Since many HR analyses can be dimensionalised by location, role, function/department and manager ID, these four items inevitably received the highest scoring. The less common data items of time to full productivity and time spent on onboarding/induction are present in particular because they influence the organisation's turnover costs, which in turn is important for computing the return on investment for many HR activities.To compare the framework's suggestion on most-valued HR data items with current practices, 20 HR departments from 16 different countries were asked to share the data items they collected about their employees. The survey results are shown in Fig. 7.44Note that the purely administrative data items like employee ID, tax number or email address are excluded from the list because they are usually not relevant for business analytics.A total of 23 items can be found in both top 30 lists, although in a quite different order. However, it also becomes clear that many of the surveyed companies collect hardly any HR data for business analytics beyond what is already available in their enterprise resource-planning software, despite the fact that 18 out of the surveyed 20 HR departments stated they already performed some form of HR analytics.Reflecting on Fig. 6, the top 30 HR data items do not pose any difficulties to the technical capabilities of a conventional BI system. More general, slow-moving, linked ‘people data’ is at the heart of HR analytics (Slinger & Morrision, 2014), i.e. employees being linked directly or indirectly to things such as roles, managers, competence requirements and recruitment costs. Therefore, HR datasets are in most cases not particularly demanding in terms of volume, velocity and variety to recall the original definition of big data (De Mauro, Greco, & Grimaldi, 2015). On the other hand, many HR departments will nevertheless find collecting those data items challenging for three reasons. First, to ensure that data items are measured in a valid, consistent and legal way, it may be necessary to invest resources in the development of new methods and policies for the collection of some data items (e.g., a validated staff survey and a toolkit to estimate the time to full productivity). Second, gathering some of the data items for all employees/roles in an organisation for the first time can be very time consuming (e.g., competence assessment and successor information), and the administrative time requirements for keeping these HR data items up to date are also not always negligible (Morrison, 2015). Third, the central collection of some data items may require the purchase of and training sessions for new software packages/extensions to administer the data (e.g., training and absence management), which can be a substantial investment requiring a convincing business case.Given that no external data sources (e.g., graduate statistics, job board data, reward benchmark data, and social media presence) made it into the top 30 mirrors the current state of HR analytics: still inward-looking and just beginning to explore the opportunities of analytics (Deloitte, 2014; Green, 2014; Jacobs, 2014). In fact, only very few of the 298 HR metrics which were gathered from the literature and HR professionals required the creation or acquisition of data items not typed in manually by staff. Applying the framework again in a few years’ time may lead to a different top 30 list. When looking into the analytics literature of other business functions such as finance (e.g., Bragg, 2014; Lee, Lee, & Lee, 2009), marketing (e.g., Grigsby, 2015; Venkatesan, Farris, & Wilcox, 2014) and procurement (e.g., Feigin, 2011; Pandit & Marmanis, 2008), one would expect slightly more complex data items in terms of volume and velocity and to some extent also more variety in their top 30 lists.It should be mentioned that constructing the linking model is not just about prioritising an already-existing list of potential data items. The linking model also offers a systematic way to gain a better awareness of the kind of HR analyses that are possible in different HR activity areas and of the kind of HR data items an organisation could collect.The 30 data items in Fig. 6 now form the standard data items together with some administrative data items like tax ID of a commercial HR software package. Using the new framework, the constancy where the present research was carried out has also begun to provide clients with more customised support on what HR data items they should gather about their employees. Clients prioritise HR data by swing weighting the value of improvement from ‘where the business function is now’ to ‘where the business function wants to be’ for each value stream, process and decision.

@&#CONCLUSIONS@&#
