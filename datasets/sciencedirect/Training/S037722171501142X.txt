@&#MAIN-TITLE@&#
Global optimization advances in Mixed-Integer Nonlinear Programming, MINLP, and Constrained Derivative-Free Optimization, CDFO

@&#HIGHLIGHTS@&#
We review the recent advances in global optimization for Mixed Integer Nonlinear Programming, MINLP.We review the recent advances in global optimization for Constrained Derivative-Free optimization, CDFO.We present theoretical contributions, software implementations and applications for both MINLP and CDFO.We discuss possible interactions between the two areas of MINLP and CDFO.We present a complete test suite for MINLP and CDFO algorithms.

@&#KEYPHRASES@&#
MINLP,Deterministic global optimization,Derivative-free,Grey-/Black-box,Constraints,

@&#ABSTRACT@&#
This manuscript reviews recent advances in deterministic global optimization for Mixed-Integer Nonlinear Programming (MINLP), as well as Constrained Derivative-Free Optimization (CDFO). This work provides a comprehensive and detailed literature review in terms of significant theoretical contributions, algorithmic developments, software implementations and applications for both MINLP and CDFO. Both research areas have experienced rapid growth, with a common aim to solve a wide range of real-world problems. We show their individual prerequisites, formulations and applicability, but also point out possible points of interaction in problems which contain hybrid characteristics. Finally, an inclusive and complete test suite is provided for both MINLP and CDFO algorithms, which is useful for future benchmarking.

@&#INTRODUCTION@&#
This review introduces recent advances in the global optimization literature in terms of theoretical advances, applications, algorithms, software and test problems. The two categories discussed, Mixed-Integer Nonlinear Programming (MINLP) and Constrained Derivative-Free Optimization (CDFO) or Constrained Grey/Black-Box Problems, encompass a large portion of existing optimization problems and applications.Using deterministic global optimization of nonconvex MINLP to solve industrially-relevant problems is not new; Floudas and Aggarwal (1990), Visweswaran and Floudas (1990), Floudas and Visweswaran (1990) and Floudas and Visweswaran (1993) introduce the first, practically-applicable deterministic global optimization algorithms to solve process networks to global optimality ≈25 years before this article was written. Recent increased activity in: developing algorithms; constructing computational frameworks using these algorithms; designing mathematical models suitable for MINLP has developed deterministic global optimization of MINLP to a point where it is becoming promising for real-world use. In practice, global optimization of MINLP is tractable via many heterogeneous algorithms pieced together into a framework; this review highlights complementary algorithmic components.CDFO refers to problems for which derivatives of the objective function and/or constraints of the original model are not directly used for obtaining the global optimum. In the absence of mathematical structure and derivative information, it becomes impossible to conquer the capabilities of the MINLP literature in terms of problem sizes and theoretical guarantees. In typical CDFO applications, derivative information is either: (1) available but deceptive; (2) prohibitively expensive; or (3) completely unavailable. Historically, the first significant contributions proposing direct-search and pattern search concepts relying simply on function evaluations were published in the 1960s (Hooke & Jeeves, 1961; Nelder & Mead, 1965; Spendley, Hext, & Himsworth, 1962). At that time, the computational and theoretical developments for numerical finite-differentiating and global optimization were inefficient, while direct-search methods were simple in implementation and had the capability of converging to improved solutions with a competitive computational cost. For a thorough introduction to the direct-search history and theoretical developments, please refer to an earlier review of Kolda et al. (Kolda, Lewis, & Torczon, 2003). More recently, significant advances in mathematical analysis, computer power, automatic differentiation and global optimization theory and algorithms have enabled the optimization of complex and large nonlinear problems with theoretical guarantee. However, there is still high interest in CDFO methods because they are suitable for problems which deterministic global optimization methods are unable to handle due to lack of information, noise, non-smoothness and discontinuities (Conn, Scheinberg, & Vicente, 2009b; Kolda et al., 2003; Martelli & Amaldi, 2014; Rios & Sahinidis, 2013). In the first textbook on Derivative-Free Optimization, Conn et al. (2009b) recognize that optimization without derivatives is one of the very challenging open problems in science and engineering, which has a vast number of potential practical applications.Although the two literatures apparently reference significantly different applications, we aim to show that MINLP and CDFO methods can benefit significantly from each other, or can be synergistically used to solve hybrid problems containing mixed MINLP and CDFO characteristics. For example, MINLP and CDFO problems have been combined in decomposition algorithms where the lower level problem is formulated as a mixed-integer linear or nonlinear problem while the upper level problem is optimized using a derivative-free approach (Martelli & Amaldi, 2014). Moreover, several theoretical advances from the deterministic constrained optimization literature, such as penalty, barrier functions and filters have been used in recent CDFO algorithms (Agarwal & Biegler, 2013; Audet & Dennis, 2006, 2009). Finally, CDFO methods often rely on surrogate approximation models which are fitted based on input-output data, or merit functions for selecting new samples. Consequently, CDFO methods require the solution of parameter estimation problems and global optimization problems, which can be solved globally using deterministic MINLP concepts such as branching and construction of underestimators. In this review we distinguish the theory, capabilities and applications of each category separately in order to span the large set of applications which existing methods can solve. However, we also discuss points of interaction of the two, since we believe this will be a major future direction in the global optimization literature. Tables in this manuscript convey the domains associated with each contribution; the purpose is building a quick-access guide of relevant contributions and their integration into advancing global optimization for both MINLP and CDFO. This manuscript also concentrates heavily on applications; this focus reflects our view that examining both MINLP and CDFOthrough the lens of applications is especially valuable.This paper is structured as follows. The problem definitions of MINLP and CDFO and their subclasses are introduced in Section 2. Section 3 considers application domains for both categories, while Section 4 presents prior reviews, textbooks, and edited books. Next, the theoretical advances for global optimization for both MINLP and CDFO are described in Sections 5 and 6, respectively. Section 7 discusses existing and future potential interactions between MINLP and CDFO. The algorithmic advances and software for MINLP and CDFO problems are discussed in Sections 8 and 9, while Section 10 presents a test suite of optimization problems.This section defines MINLP and several important sub-classes. Section 2.1.1 defines the most general class of MINLP; Section 2.1.2 defines quadratically-constrained, boolean quadratic and quadratic assignment problems and Section 2.1.3 refers to polynomial and signomial mixed-integer optimization problems.MINLP is defined as:(MINLP)minxbiLO≤f0(x)s.t.biLO≤fi(x)≤biUP∀i∈M:={1,…,M}xjLO≤fi(xj≤xjUP∀j∈N:={1,…,N}xj∈Z∀j∈I⊆NwhereM,N,andIrepresent sets of constraints, variables, and discrete variables, respectively. The objective and constraints are functionsfi:RN↦R∀i∈{0,…,M}. ParametersbiLO∈R∪{−∞}andbiUP∈R∪{+∞}bound the set of constraintsM; parametersxjLO∈R∪{−∞}andxjUP∈R∪{+∞}bound the set of variablesN. We assume that it is possible to infer finite bounds on the variablesxand that the image of fiis finite onx. Typical expressions for f0(x) and fi(x) are:(1)fi(x)=ci+aiTx+xTQix+∑s∈Scs,i·∏j∈Nxjps,i,j+∑j∈Nce,i,jexj+∑j∈Ncℓ,i,jlogxjwhere the powers ps, i, jand coefficients ci, ai, Qi, cs, i, ce, i, j, cℓ, i, jare constant reals;s∈Sindexes the signomial terms. More examples of nonlinear terms found in MINLP solver software are listed in Table 1.Because composite functions may yield combinatorial possibilities beyond the ones listed in Table 1, handling MINLP requires methodologies that effectively address generic, multivariate terms (Floudas, 2000). Complementary techniques for addressing generic functional forms include (1) the αBB methodology that generates convexifying quadratic or exponential relaxations on expression aggregates via an interval Hessian matrix (Adjiman, Androulakis, & Floudas, 1998a; Adjiman, Dallwig, Floudas, & Neumaier, 1998b; Liu & Floudas, 1993; Maranas & Floudas, 1995) and (2) factorable programming trees that break expressions into their component parts through directed acyclic graph representations (Belotti, Lee, Liberti, Margot, & Wächter, 2009; McCormick, 1976; Mitsos, Chachuat, & Barton, 2009; Smith & Pantelides, 1999; Tawarmalani & Sahinidis, 2005; Vigerske, 2012). Because the methodological tradeoffs between αBB and factorable programming trees produce complementary convergence behavior (Bompadre & Mitsos, 2011), the most generic global optimization tools are hybrid algorithms that opportunistically exploit the tightest relaxation at each node of a global optimization search tree (Gatzke, Tolsma, & Barton, 2002; Misener & Floudas, 2014a).A mixed-integer quadratically-constrained quadratic program (MIQCQP) is defined as a MINLP where all of the nonlinearities are quadratic; symbols have the same meaning as in Section 2.1.1 and equivalent bounded-ness requirements hold:(MIQCQP)minxbiLO≤c0+a0Tx+xTQ0xs.t.biLO≤ci+aiTx+xTQix≤biUP∀i∈M:={1,…,M}xjLO≤xj≤xjUP∀j∈N:={1,…,N}xj∈Z∀j∈I⊆NRecall that MIQCQP is mathematically equivalent to a mixed-integer quadratically-constrained program (MIQCP) with the introduction of a variablexN+1as the objective function and the addition of a constraintxN+1≥c0+a0Tx+xTQ0x.WhenQi=0∀i∈M,MIQCQP is referred to as a mixed-integer quadratic program (MIQP). WhenM=⌀,MIQCQP is a box-constrained MIQP. Mixed-integer is dropped from all previous definitions wheneverI=⌀. A boolean quadratic program (BQP) hasM=⌀,I=N,andxjUP=1∀j∈N.Many practically-relevant models have some sort of interesting special structure with respect to the matrices Qi. For example, the quadratic assignment problem (QAP) is a BQP with problem structure based on multiplying flow and distance matrices (Anstreicher, 2003; Loiola, de Abreu, Boaventura-Netto, Hahn, & Querido, 2007). The max-cut problem, which maximizes the weights on the edges in an undirected graph (Rendl, Rinaldi, & Wiegele, 2010), and the maximum clique problem (Bomze, Budinich, Pardalos, & Pelillo, 1999) are other classic problems that may be formulated as MIQP with special mathematical structure enabling solution of large-scale instances. One common way of representing MIQCQP is via an undirected graph representation; see in Fig. 1some of the special structure patterns formed by MIQCQP including process networks, computational geometry, and MIQP (Misener, Smadbeck, & Floudas, 2014b).Another important, restricted subclass of MINLP is mixed-integer signomial optimization (MISO); symbols have the same meaning as in Section 2.1.1 and equivalent bounded-ness requirements hold:(MISO)minxbiLO≤∑s=1S0cs,0·∏j∈Nxjps,0,js.t.biLO≤∑s∈Scs,i·∏j∈Nxjps,i,j≤biUP∀i∈M:={1,…,M}xjLO≤xj≤xjUP∀j∈N:={1,…,N}xj∈Z∀j∈I⊆NWhenps,i,j∈Z∀s∈S;i∈M;j∈N,the MISO is a mixed-integer polynomial optimization problem.The majority of the existing CDFO methods have been developed for box constrained problems, where a set of input variables with known lower and upper bounds affect the objective through a black/grey-box model (BCDFO).BCDFO is defined as:(BCDFO)minxbjLO≤f0(x)s.t.xjLO≤xj≤xjUP∀j∈N:={1,…,N}xj∈Z∀j∈I⊆NwhereNandIrepresent sets of variables and discrete variables, respectively, and parametersxjLO∈R∪{−∞}andxjUP∈R∪{+∞}bound the set of variablesN.In BCDFO problems there is only one unknown function (objective f0(x)) which must be either approximated or sampled in order to identify the global optimum within the region bounded byxjLOandxjUP. We prefer to omit the term Mixed Integer from the formulations of derivative-free problems since the majority of established methods refers to problems for whichI=⌀. However, noteworthy recent developments incorporating discrete variables will be described in the following sections.The general case of CDFO is:(CDFO)minxbiLO≤f0(x)s.t.bkLO≤fk(x)≤bkUP∀k∈K:={1,…,K}biLO≤fi(x)≤biUP∀i∈M:={K+1,…,M+K}xjLO≤xj≤xjUP∀j∈N:={1,…,N}xj∈Z∀j∈I⊆NwhereK,M,NandIrepresent sets of known constraints, unknown constraints, variables, and discrete variables, respectively. The known constraints are functionsfk:RN↦R∀i∈{1,…,K}. However, no assumptions can be made about the form of the objective f0 and unknown constraints fi. The total number of constraints isK+M,while a general CDFO problem can contain only known constraints,M=⌀,or only unknown constraints,K=⌀. ParametersbkLO∈R∪{−∞},bkUP∈R∪{+∞},biLO∈R∪{−∞}andbiUP∈R∪{+∞}bound the set of known and unknown constraintsKandM; parametersxjLO∈R∪{−∞}andxjUP∈R∪{+∞}bound the set of variablesN. There have been several extensions of algorithmic developments for incorporation of general constraints, and fewer developments which deal with discrete variables in general CDFO problems. These will be described in detail in the following sections.By definition, CDFO literature deals with unknown forms, thus the distinction between different types of formulations is not possible. Theoretical developments in the CDFO field have to– directly or indirectly– rely on function evaluations. A direct approach is purely driven by samples which guide the progress of optimization towards descent directions, while an indirect approach uses samples to parametrize postulated functions which are then optimized to guide the search. For the direct approach, mathematical analysis has enabled to establish theoretical developments which guarantee convergence to local first-order or second-order stationary points using samples (zeroth order information) (Conn et al., 2009b; Kolda et al., 2003). For the indirect case, any theoretical development from the MINLP literature is applicable, since the postulated functions (also known as surrogate functions) should be optimized. These typically contain constant, linear, bilinear, quadratic, exponential, polynomial and even composite terms (Table 1). The dependence on sampling and the complete or partial lack of theoretical closed-form equations makes convergence to global optimality, in the sense which is described in the MINLP literature, extremely challenging and in most cases not possible. Thus, it should be stressed that when a problem can be sufficiently described by a well-posed, explicit MINLP formulation, then CDFO methods are not recommended. Despite the above, the applicability of CDFO methods is still significant and we aim to prove this through the next sections of this paper.Deterministic global optimization for MINLP is highly relevant to application domains where there is high reward for fractional improvements and sufficient time to explore the search space. Table 2lists some of these domains and gives example problems. Belotti et al. (2013b) also provide list of MINLP applications. The shorter list of Belotti et al. (2013b) incorporates: MIP formulations, local techniques, and convex nonlinearities whereas each of the entries in Table 2 correspond to NLP and MINLP with nonconvex nonlinearities that have been solved using deterministic global optimization. We include a range of references for each application to demonstrate the depth of study in some application areas.The most established application domain is the area of process systems engineering; here the traditional applications are in areas such as refinery operations, water networks, and process synthesis via superstructure optimization. Other domains have emerged in areas such as: biological and biomedical engineering; computational chemistry; computational geometry; finance. A major challenge of globally optimizing MINLP models in both biological engineering and finance is the size of the optimization problems. In our view, it is absolutely critical to examine deterministic global optimization of MINLPthrough the lens of applications, since deterministic global optimization is important for particular problem domains. Therefore, we are particularly interested in highlighting MINLP approaches applied to practically-relevant problems.A primary driving force for the increasing need for CDFO is the development of high-fidelity simulations to represent industrial processes with a high level of detail, coupling multiple physical, chemical, and mechanical phenomena (Forrester, Sobester, & Keane, 2008). For example, Computational Fluid Dynamic (CFD) models have been used to accurately represent industrial processes, such as gasifiers, turbine combustors, stirred tank reactors and many more. Each simulation of a CFD model for a specific set of input conditions may take several minutes or more, based on the level of detail in the model equations, the geometry complexity and discretization scheme. Computationally expensive models cannot be optimized using deterministic global optimization algorithms which would require multiple function calls to approximate derivative information. Another main advantage of CDFO methods is their independence from derivatives which are highly affected by numerical noise embedded in the original model. Finite-differentiating techniques are known to fail in the presence of noise, while CDFO methods have been proven to be significantly more consistent in the presence of noise (Conn et al., 2009b; Kolda et al., 2003). An example of this problem arises in cases of expensive large-scale flowsheet simulations with recycle loops, where numerical integration issues lead to unreliable derivatives. In addition, potential applications of CDFO methods are not limited to expensive or noisy computer models, since they can be used to optimize non-smooth problems, with multiple local optima and discontinuities, which are qualities that may cause certain derivative-based methods to fail. Finally, CDFO methods enable the optimization of grey/black-box models which for various reasons such as proprietary restrictions or legacy related issues are available only in the form of input-output data. We define as a black-box system, any simulation for which the user has the ability to change the input variables and obtain the set of output variable values without knowing the underlying model equations which lead to the result. The term Grey-box is used for similar problems, with the difference that we do have access to a subset of the model equations.Typical applications of problems with the above characteristics come from the mechanical, aerospace, civil, chemical and biomedical engineering fields as well as geosciences, where large systems of partial differential equations coupled with nonlinear correlations are formed to represent systems including design aspects, kinetics, fluid dynamics, mass and heat transfer and many more. CDFO methods have a wide range of applications that include aircraft routing, design of hybrid fuel cell vehicles, optimization of cardiovascular geometries, structure design of nano materials, circuit design, flowsheet optimization, process synthesis of biodiesel production and supply chain management. A list of the noteworthy and diverse applications, along classifications related to the type of CDFO problem solved in each work, are reported in Table 3.A number of major reviews, textbooks, and edited books have been published in recent years; these are organized in Table 4with brief descriptions. Many of these works address: (1) application domains for MINLP; (2) algorithms for solving MINLP; and (3) software for MINLP.Due to the very recent advances in derivative-free optimization theory and the maturity of the literature, there is sparsity of major review papers and textbooks. The first textbooks were introduced in 2008-2009, and several noteworthy review papers have been written in the recent years (Table 5). In Conn et al. (2009b), the authors provide a very comprehensive description of trust-region frameworks, local-search and pattern search, positive spanning sets, sampling, linear and non-linear local models, interpolation, regression, well-poisedness, simplicial search and more. The focus of this book is unconstrained or box-constrained derivative-free optimization using local-search approaches. The book of Forrester et al. (2008) focuses on model-based search, which is a different class of CDFO. The authors provide theory and algorithms for surrogate model fitting and search criteria for global derivative-free search, from a more application-driven perspective.A crucial component for an effective branch-and-cut framework is algorithm heterogeneity; we need to combine the constitutive elements of: preprocessing, convex underestimators, cutting planes, branching, bounding, and heuristics into an integrated approach. Here we advocate an all-of-the-above approach; an ideal implementation incorporates many possible effective strategies and then selects appropriate algorithms for any given problem.To organize the individual contributions, Sections 5.2–5.6 describe algorithmic contributions and Tables 6–8present the contributions by area. Fig. 2illustrates the domain of the contributions in the context of a global optimization branch-and-cut framework.Automatic reformulations, while common in MIP, are relatively rare in MINLP (Bussieck & Vigerske, 2010). Recommendations made to modelers (i.e., non-automatic reformulations) often include: using linear formulations if possible (Harjunkoski, Westerlund, Isaksson, & Skrifvars, 1996); using convex functional forms if possible (Harjunkoski, Westerlund, Pörn, & Skrifvars, 1998); dissaggregating terms (Tawarmalani, Ahmed, & Sahinidis, 2002; Tawarmalani & Sahinidis, 2002b); adding redundant constraints into the model formulation (Ahmetović & Grossmann, 2011; Karuppiah & Grossmann, 2006; Ruiz & Grossmann, 2011b). Any MINLP solution method introducing auxiliary variables is doing automatic reformulations, but it is currently unclear (for the general case) what are the best automatic reformulations.Some semi-automatic and fully-automatic reformulations have been developed for MIQCQP; these often take the form of reducing the number of nonconvex bilinear terms. Ben-Tal, Eiger, and Gershovitz (1994) showed that reformulating MIQCQP may produce a smaller problem, Audet, Brimberg, Hansen, Le Digabel, and Mladenovic (2004) eliminated bilinear terms in the pooling problem through mass balances at the intermediate nodes, and Liberti and Pantelides (2006) generalized the contribution of Audet et al. (2004) to automatically eliminate unnecessary bilinear terms in MIQCQP.A stand-alone reformulation engine with no associated solver is ROSE, the reformulation optimization software engine (Liberti, Cafieri, & Savourey, 2010; Liberti, Cafieri, & Tarissan, 2009a). The MIQCQP solver GloMIQO (Misener & Floudas, 2013) represents a major effort to elucidate special structure via reformulations and integrates reformulation techniques that can be implemented generically and applied universally. The GloMIQO reformulation uses the observation that disaggregating bilinear terms tightens the relaxation of MIQCQP and actively takes advantage of any redundant linear constraints added to the model; GloMIQO may add bilinear terms to the model formulation to create tight reformulation-linearization technique relaxations.Other preprocessing algorithms include: heuristics for developing a good upper bound (see Section 5.6); detecting symmetry (Liberti, 2012); building a directed acyclic graph representation of the MINLP (Belotti et al., 2009; Vigerske, 2012).Nonconvex terms in MINLP are typically replaced with rigorous convex underestimators for solving the lower-bounding problem of the MINLP; solving the relaxed, convex minimization problem gives deterministic lower bounds on the global optimum (Floudas, 2000). In the context of a branch-and-bound deterministic global optimization algorithm, sufficient convergence conditions are (Horst & Tuy, 1996): (1) any unfathomed node can be further refined and (2) the original MINLP and underestimating problem converge as the refinements approach infinity.Table 6 records a variety of rigorous convex underestimators designed for nonconvex MINLP. Observe that this part of the MINLP framework has been richly studied; deriving tight convex underestimators for MINLP is an especially important component of solving MINLP. There is a trend towards developing good underestimators for very specific terms, for example: Meyer and Floudas (2003); (2004) introduce the explicit facets of the convex and concave envelope of trilinear terms; Liberti and Pantelides (2003) find the convex hull for odd degree monomials; Gounaris and Floudas (2008a) give results on specific products of univariate functions. The solver ANTIGONE (Misener & Floudas, 2014a) implements these heterogeneous choices using a polymorphic programming strategy; specialized terms inherit from more general base classes of terms.With respect to implementations, a dominant trend is in hybrid approaches that implement multiple possible relaxations and have them compete in real-time while solving an MINLP (Bompadre & Mitsos, 2011; Gatzke et al., 2002); variants of this heterogenous strategy are implemented in ANTIGONE (Misener & Floudas, 2014a, 2014b) and BARON (Zorn & Sahinidis, 2014a). A trend complementing the current dominance of hybrid relaxations is the area of reducing relaxations to their necessary set. As the possible number of relaxations grow, several authors suggest eliminating weaker cuts based on logical inferences or choosing appropriate relaxations based on extensive computational experience: Lundell, Westerlund, and Westerlund (2009); Sherali, Dalkiran, and Liberti (2012); Cafieri, Hansen, Létocart, Liberti, and Messine (2012); Ballerstein and Michaels (2014); Dalkiran and Sherali (2013); Skjäl and Westerlund (2014); Guzman, Hasan, and Floudas (2014).Convex underestimators may be generated statically at each node (i.e., in branch-and-bound) or at each iterative refinement level. In a branch-and-cut framework, tight convex underestimators may expedite deterministic global optimization by increasing the objective lower bound with respect to divide-and-conquer algorithms based on interval analysis (Adjiman et al., 1998a; Adjiman et al., 1998b). Ideally, convex relaxations both tightly approximate the original function and are computationally inexpensive to generate, but these design goals are often at odds with one another. The convex hull, the smallest convex set containing the original nonconvex term or function, is the tightest possible convex underestimator.Tight convex underestimators are also valuable in adaptive refinement approaches that continue refining the root node rather than spatially branching on the search space. The solvers αR and αSGO solve a sequence of convex MINLP with progressively more integer break points (Lundell, Skjäl, & Westerlund, 2013). Iterative refinement of tight underestimators has also been applied to examples including gas networks (Domschke et al., 2011; Geissler, Morsi, & Schewe, 2013).In addition to statically-generated underestimators, convex underestimators may be added dynamically as cutting planes where, after a node is optimized, separating hyperplanes are added to the underestimating optimization problem and the node is resolved (usually via a warm start). By only adding constraints which are violated in the solution of the underestimated problem, optimization algorithms can avoid introducing more constraints than necessary; this is especially relevant in polyhedral relaxations of many-dimensional expressions where the number of candidate facets may grow combinatorially (Misener et al., 2014b).Cutting planes developed for MINLP include those based on: pseudo-convex MINLP problems (Westerlund, Skrifvars, Harjunkoski, & Pörn, 1998), outer approximation of convex terms and linearization of other convex underestimators (Misener & Floudas, 2014b; Tawarmalani & Sahinidis, 2005), multi-term quadratic expressions (Bao, Sahinidis, & Tawarmalani, 2009; Luedtke, Namazifar, & Linderoth, 2012; Misener et al., 2014b), multilinear functions (Belotti, Miller, & Namazifar, 2010b; Qualizza, Belotti, & Margot, 2012; Rikun, 1997), optimizing convex quadratic functions over nonconvex sets (Bienstock & Michalka, 2014), and other cutting plane classes based on nonlinear functional forms (D’Ambrosio, Frangioni, Liberti, & Lodi, 2010; Richard & Tawarmalani, 2010). A review on cutting plane methods for MINLP can be found in Nowak (2005); multivariable and multiterm relaxations are typically favoured for MINLP because the tightest convex relaxation of each individual is not typically equivalent to the tightest possible relaxation of the entire MINLP (Westerlund, Lundell, & Westerlund, 2011).Dividing the search space via domain branching approaches manageable problem sizes via recursion. Most implementations divide one node into two by branching on a single variable (Adjiman et al., 1998a; Adjiman et al., 1998b; Belotti et al., 2013b); the commonly-used approach is reliability branching, a technique that integrates strong branching with a pseudocost heuristic to predict the best branching variable (Achterberg, Koch, & Martin, 2005; Belotti et al., 2009). Applying reliability branching to MINLP was introduced by Belotti et al. (2009) for the solver software implementation Couenne; it is additionally implemented in ANTIGONE/GloMIQO (Misener & Floudas, 2012b, 2013, 2014a) and SCIP (Berthold, Gleixner, Heinz, & Vigerske, 2012a; Berthold, Heinz, & Vigerske, 2012b). Table 7 records an interesting array of branching methods designed for MINLP.Implementations for solving MINLP models often rely heavily on bounds tightening to reduce the feasible space. Feasibility-based bounds tightening (FBBT), which uses interval arithmetic to place bounds on expressions by recursively overestimating each of the participating functions and operators, is the simplest and most computationally inexpensive technique (Adjiman et al., 1998a; Adjiman et al., 1998b; Androulakis, Maranas, & Floudas, 1995; Audet, Hansen, Jaumard, & Savard, 2000; Belotti et al., 2009; Neumaier, 1990; Sherali & Tuncbilek, 1995, 1997; Wolfe, 2000). An alternative, optimization-based bounds tightening (OBBT) methodology for determining these interval estimates is to minimize and maximize the expression under the bound constraints and possibly additional linear and convex constraints from the problem. These subproblems provide tighter bounds than interval arithmetic (Maranas & Floudas, 1995; Tawarmalani & Sahinidis, 2002b). A decreased number of nodes in the branch-and-bound tree may justify the increased computational effort needed to find better estimates.FBBT and OBBT represent extremes. FBBT is cheap but not especially effective; OBBT is computationally demanding but may significantly reduce the bounds. Table 8 organizes contributions to domain reduction algorithms.The final piece of any successful deterministic global optimization framework is good heuristic algorithms. While local MINLP methods are possibly applicable to deterministic global optimization, it is important to understand that effective heuristics for a deterministic MINLP framework are not necessarily the same as effective stand-alone heuristics. So far there are relatively few works that consider heuristics in the broader context of deterministic global optimization; exceptions include the protein structure prediction algorithm of Klepeis, Pieja, and Floudas (2003a, 2003b), Undercover (Berthold & Gleixner, 2013a), and the computational studies of Berthold (2014).There are several heuristic algorithms recently developed for finding a first feasible solution of MINLP; a few of the promising feasibility pumps include those of: Bonami et al. (2008); Liberti, Mladenović, and Nannicini (2011); Boland, Eberhard, Engineer, and Tsoukalas (2012); D’Ambrosio, Frangioni, Liberti, and Lodi (2012); Nannicini and Belotti (2012). Fast-fail algorithms have been identified as the most effective for integration into deterministic global optimization of MINLP (Berthold, 2014). There are many more heuristic MINLP algorithms (Belotti et al., 2013b), but their applicability with respect to deterministic global optimization has not been established (Berthold, 2014).Numerical issues represent an interesting trade-off with respect to correctness versus solving speed. While some work has been done in rigorous global optimization that formally verifies nonlinear functions including semialgebraic and transcendental functions (Domes, 2009; Domes & Neumaier, 2014), the most commonly-used solver software generates relaxations and cutting planes via floating point arithmetic and then uses floating point-based LP and NLP solvers for finding underestimators and heuristic solutions. Numerical instability can be at least partially mitigated using validated interval arithmetic (Brönnimann, Melquiond, & Pion, 2003, 2006; de Moura & Passmore, 2013) for FBBT but, especially for badly-scaled optimization problems, combining diverent solving strategies may induce numerical trouble because of the variance in tolerances between solvers (including clash between different sub-solvers of a single meta-solver).Another interesting set of numerical issues related to global optimization comes from a solver trying to meet user-set tolerances; the cluster problem refers to relaxations slowly converging to a global solution by generating many boxes around a single minimizer (Du & Kearfott, 1994; Wechsung, Schaber, & Barton, 2014). Mitigating the cluster problem typically involves minding relaxation convergence rate (Bompadre & Mitsos, 2011).Undoubtedly, lack of derivative-information is a very critical loss for efficient and fast optimization. In the area of Constrained Derivative-Free Optimization, CDFO, recent developments either rely on input-output data (direct-search), or employ an intermediate step of developing cheaper and smooth approximate models using the input-output data (model-based). Additionally, CDFO methods can be divided into two categories based on their search strategy: (1) local-search methods which require a single initial point and an initial local trust region size, and proceed by making small movements within a local subspace, and (2) global search methods which explore the entire bounded region during each iteration. The third main categorization of CDFO methods is related to the nature of the search strategy, which can be deterministic or stochastic. Finally, there have been recent developments of hybrid approaches which combine characteristics from different algorithmic developments.In this category, the theoretical developments focus on sampling using positive spanning sets and mechanisms for assurance of descent. Convergence to first-order and second-order optimal solutions is guaranteed under certain smoothness and differentiability assumptions of the objective function and the constraints within the local region (Conn, Scheinberg, & Vicente, 2009a; Kolda et al., 2003). Sampling-based local CDFO optimization includes direct-search, pattern-search and simplex based methods. Convergence has been studied extensively in the literature, where a set of updating rules of the trust region guarantee convergence to local optima when the size of the trust-region is sufficiently small. All local direct-search CDFO methods incorporate a set of rules to ensure descent away from stationarity, which in concept is identical to the mechanism followed by derivative-based algorithms. Direct-search methods require that the function is sampled at locations defined by positive spanning sets in order to move towards the direction of the best function value. Moreover, the mechanism of local CDFO methods must ensure that strict criteria of the geometry of the sampling design are satisfied, in order to theoretically guarantee true stationarity. Measures of geometry such as the cosine measure of positive spanning sets, the normalized simplex volume and the poisedness constant are described in detail in (Conn et al., 2009b). Finally, local CDFO algorithms converge when the step size, mesh size, simplex diameter or line-search parameter is sufficiently small, which has been theoretically linked with convergence to stationary points. A schematic of local-search type steps is shown in Fig. 3, where initially a random initial point is selected, followed by a set of expansions and contractions until the step size is driven to zero. It should be noted that purely direct-search methods do not exactly follow the schematic representation shown in Fig. 3, since in the direct-search case the new centers must lie on a predefined lattice. Fig. 3 is developed as a general representation of both direct-search and trust-region search which will be described next. The disadvantages of local direct-search CDFO are the high dependence on the initial point, entrapment within closest local optimum, and large number of function calls to guarantee convergence (Audet, Béchard, & Le Digabel, 2008b; Conn et al., 2009b). In order to increase the probability for convergence to the global optimum, multi-start approaches can be used. However, these are not efficient in cases where the model of interest is computationally expensive. When a good starting point is available or when sampling does not require significant computational cost, CDFO methods in this category guarantee convergence to stationarity with reliability.Algorithmic developments and convergence have been studied for box-constrained problems (BCDFO) (Audet & Dennis, 2002; Garcia-Palomares, Garcia-Urrea, & Rodriguez-Hernandez, 2013; Lewis & Torczon, 1999; Lucidi & Sciandrone, 2002); for known linear constrained problems (Audet & Dennis, 2002; Kolda, Lewis, & Torczon, 2006; Lewis, Shepherd, & Torczon, 2007; Lewis & Torczon, 2000); for general constrained problems (Abramson, 2002; Abramson & Audet, 2006; Abramson, Audet, Dennis, & Le Digabel, 2009c; Audet et al., 2008b; Audet & Dennis, 2004, 2006, 2009; Audet, Dennis, & Le Digabel, 2010; Liuzzi, Lucidi, & Sciandrone, 2010). Constraints are handled by penalty methods such as extreme barrier (Abramson & Audet, 2006; Abramson et al., 2009c; Audet et al., 2008b; Audet & Dennis, 2006), progressive barrier (Audet & Dennis, 2009), filters (Audet & Dennis, 2004), sequential penalty merit functions (Liuzzi et al., 2010); smoothed exactl−∞penalty function (Liuzzi & Lucidi, 2009) and exact penalty merit functions (Fasano, Liuzzi, Lucidi, & Rinaldi, 2014; Gratton & Vicente, 2014), or restoration steps which are independent from the objective function (Arouxét, Echebest, & Pilotta, 2015; Martinez & Sobral, 2013). Allowing pattern-search methods to handle a dense set of polling directions instead of a finite set of polling directions was a significant development for studying the convergence of generally constrained derivative-free problems which are Lipschitz even near a limit point (Audet et al., 2008b; Audet & Dennis, 2006). A recent methodology proposed in (Vicente & Custódio, 2012) studies convergence of local-search adaptive search in the case of discontinuous objective functions and general constraints, which is useful for many real applications. Methods which are based on Augmented Lagrangian concepts for treatment of constraints have the disadvantage of a dependence on the selected parameters (or weights) of the penalty terms. However, such methods provide better theoretical guarantees to locally optimal solutions, when compared to filter based methods.Local-search CDFO contributions have been extended for handling mixed-integer variable problems in the work of (1) Audet and Dennis (2001) for box-constrained problems; (2) Abramson, Audet, and Dennis (2001) for general constrained CDFO problems through a filter approach; (3) Lucidi, Piccialli, and Sciandrone (2005) and Abramson, Audet, Chrissis, and Walston (2009a) for constrained problems with known linear constraints; (4) Liuzzi, Lucidi, and Rinaldi (2012) for box-constrained problems based on the earlier method desribed in (Lucidi & Sciandrone, 2002); and (5) Liuzzi, Lucidi, and Rinaldi (2015b) for general constrained mixed-integer problems extending an earlier sequential penalty approach described in (Liuzzi et al., 2010). A contribution which diverges from the above, but belongs in the local-search direct-search category is developed for purely discrete problems, for which an implicit and dense closed set is available (Vicente, 2009). The main challenge for extending the capabilities of local-search derivative-free methods to handle discrete variables is the need to assume a user-defined criterion for local optimality with respect to the discrete variables. User-defined discrete local optimality requires setting the set of neighbors amongst which the local optimizer is identified. Convergence to stationary points is guaranteed using similar concepts to the continuous case, however, the local mesh of points is an augmented set containing positive spanning directions defined by combinations of discrete and continuous variables within the local trust region; or else a union of a finite number of lattices in the continuous space with the integer space (CDFO).Local search model-based methods are also described as trust-region methods. Instead of relying simply on function evaluations, a trust-region subproblem is first defined by fully linear/quadratic models, typically built through interpolation or regression, or other interpolating models such as kriging and radial basis functions. Similar to direct-search CDFO, descent is guaranteed away from stationarity by combining contraction and expansion mechanisms with the final objective of the reduction of the trust-region radius. As opposed to direct-search, the next iterate center point does not need to lie on the pattern, mesh or simplex edges (Fig. 3). Algorithmic comparisons have shown that by using a local model which approximates the true function with acceptable accuracy within the trust region, convergence is expedited (Conn et al., 2009b).Several noteworthy box-constrained trust-region CDFO methods have been developed such as the Surrogate Management Framework of Booker et al. (1999) and the extension of mesh adaptive search in (Abramson, Asaki, Jr., Jr., & Sottile, 2012), which both use kriging models; Oeuvray (2005); Oeuvray and Bierlaire (2007) which use radial-basis function models (BOOSTERS algorithm);the Implicit-filtering algorithm which uses fully quadratic approximations of the gradient in order to move towards descent directions along negative simplex gradients (Gilmore & Kelley, 1995); the BOBYQA algorithm in (Powell, 2009) which uses local quadratic models; and finally a recent approach which uses interpolating polynomials (Gratton, Toint, & Tröltzsch, 2011). The LINCOA algorithm (Powell, 2013b) uses quadratic approximations for the unknown objective function and performs optimization in the presence of known linear constraints. Conejo, Karas, Pedroso, Ribeiro, and Sachine (2013) develop a method which uses local quadratic models of the objective function for problems with a known convex feasible region. General constrained CDFO problems have been studied by Caballero and Grossmann (2008) and Sankaran, Audet, and Marsden (2010) using local kriging models for the objective and constraints; Powell (1994), (2013a) developed the COBYLA algorithm which uses local linear approximations for the unknown objective and the unknown constraints which is an approach revisited recently by March and Willcox (2012); Conn and Le Digabel (2013) employ quadratic models for the objective and constraints; Müller, Shoemaker, and Piche (2013) use radial-basis function models; and finally Müller and Shoemaker (2014) use an ensemble of various surrogate-models to best fit the objective and constraints of the problem. A recent extension of the BOBYQA algorithm for mixed variable programming has been published which uses quadratic approximations and local integer search, with guaranteed identification of locally optimal points (Newby & Ali, 2014). Quadratic models are also used to represent inequality constraints in conjunction with mesh-adaptive direct-search (Audet, Ianni, Le Dibagel, & Tribes, 2014), in order to expedite local convergence with a fewer number of samples. A representative collection of interpolating and non-interpolating surrogate models used for all model-based methods are reported in Table 9.Global-search CDFO refers to methods which do not require a starting point, while they consider the region defined byxjLOandxjUPas the search space of the algorithm. The DIRECT algorithm (Jones, Perttunen, & Stuckman, 1993) divides the box constrained space into rectangles and samples each center point. Progression of the algorithm occurs by dividing rectangles with lowest values, until typically a maximum number of function calls is reached. Huyer and Neumaier (1999) developed a global optimization methodology for box constrained problems called Multilevel Coordinate Search (MCS), which allows an irregular partitioning procedure, while new points do not need to be center points of the rectangles. Global-search direct-search methods have been developed for box-constrained problems and have been tested on low-dimensional problems. A recent extension of the DIRECT algorithm was proposed in Di Pillo, Lucidi, and Rinaldi (2013), where the authors employ an exact penalty function and an optional local search extension in order to locate improved solutions to general constrained problems. This method is tested on a large set of problems with up to 10 dimensions and constraints.Theoretical developments for the methodologies which belong to the global-search model-based group are related to surrogate-model fitting, parameter estimation, sampling design and criteria for selecting the next sampling point. This category of CDFO is particularly suitable for computationally expensive applications. Theoretical convergence can be sacrificed, if good optimal solutions are located with a minimum number of function calls. In the global search CDFO literature, theoretical developments lie mostly in the combinations of surrogate-models used and the criterion for the selection of the next promising sample until convergence. An overview of the steps for global model-based CDFO is shown in Fig. 4, where an initial set of samples is collected within the entire search space; models are fitted for the objective, constraints, or the lumped penalized objective using parameter estimation; the formulated approximation model is optimized using derivative-based optimization in order to find promising solutions which are added to the sampling set. This procedure is repeated until the convergence criteria of the algorithm (i.e. no improvement in consecutive iterations, maximum number of function calls) are met. As pointed out in Fig. 4, there are two stages during which CDFO can really benefit from theoretical advances of MINLP, namely parameter estimation and optimization of the fitted model. This is an aspect which has not been discussed in the literature significantly, since typically local solvers are used to optimize the formulated surrogate model which may in turn be non-convex and exhibit multiplicity of local optima. The majority of the developed algorithms guarantee asymptotic convergence in the limit of infinite samples (Forrester & Keane, 2009; Forrester et al., 2008; Jakobsson, Patriksson, Rudholm, & Wojciechowski, 2010; Jones, 2001; Jones, Schonlau, & Welch, 1998; Müller et al., 2013).The key theoretical difference of all global-search CDFO methods lies in the surrogate method used to approximate the unknown models, as well as the criterion or strategy used to locate the next sampling point. The different types of interpolating and non-interpolating surrogate models used in the literature are summarized in Table 9. The second key theoretical component is the strategy for selecting the next sampling location, since the fitted model accuracy during the initial iterations is not reliable in regions which have not been sampled sufficiently. As a result, if the algorithm proceeds by blindly trusting the model predictions, there is high risk that the method will converge prematurely to a local optimum, without exploring promising regions of the search space. Jones et al. (1998) provided a formal mathematical criterion (Expected Improvement, EI) which ensured that both the model prediction as well as the uncertainty of the model are incorporated within one equation during each iteration, in order to provide the next sampling point. Using theoretical developments from the MINLP literature, such as convex underestimators and bounds tightening, Jones et al. (1998) optimized the EI Criterion, ensuring that any new sample will have at least some distance from existing samples. The EI criterion based algorithm uses interpolating kriging surrogate models and was developed for box constrained problems. Other global-search surrogate-based methods which employ kriging approximations within box constrained regions have been developed by Forrester and Jones (2008) and Quan, Yin, Ng, and Lee (2013). Radial-basis functions have also been used in global-search surrogate-based CDFO methods for box-constrained problems (Björkman & Holmström, 2000; Holmström, Quttineh, & Edvall, 2008b; Jakobsson et al., 2010; Regis & Shoemaker, 2007a, 2007b, 2007c, 2013b). Yao, Chen, Huang, and van Tooren (2014) use a hybrid Neural-Network Radial-basis function model within a global-search framework which forces gradient estimations in local regions to agree with actual gradient information. Jones (2001) compares the performance of kriging-based surrogate models to quadratic non-interpolating models for global-search optimization, while Viana, Haftka, and Watson (2013) develop approaches using multiple surrogate predictions to locate promising new sampling points within a box-constrained region.Several contributions have extended the capabilities of the above methods to handle constraints. Le Thi, Vaz, and Vicente (2012) employ radial basis functions to approximate the black-box objective function and introduce a difference of convex functions (D.C.) programming approach for optimizing the surrogate formulation with box and known linear constraints. Sasena, Papalambros, and Goovaerts (2002) use probability of constraint satisfaction, extreme barrier methods and treatment of each constraint using individual surrogate functions to handle general non-linear constraints within the EI framework, while Parr, Keane, Forrester, and Holden (2012) use multi-objective optimization. Villemonteix, Vazquez, Sidorkiewicz, and Walter (2009a); Villemonteix, Vazquez, and Walter (2009b) introduce a new minimizer entropy kriging-based criterion for locating new samples, which must be evaluated using conditional simulations in order to incorporate constraint satisfaction, while Kleijnen, van Beers, and van Nieuwenhuyse (2010) develop a method for solving constrained Integer Nonlinear Problems using kriging approximations. General constraints have been approximated by radial basis functions in the works of Rashid, Ambani, and Cetinkaya (2013); Regis (2011, 2014), Regis and Shoemaker (2005), while Holmström et al. (2008b) incorporate constraints in an aggregated penalty term which is added to the objective function, transforming the problem to box-constrained. Boukouvala and Ierapetritou (2013), (2014) employ kriging approximations for the objective function and the lumped feasibility-based criterion which aims to represent the feasible space of the problem through a single function. Recent work of Müller et al. (2013) extends the capabilities of the radial-basis function based black-box method for mixed-integer problems, under the assumption of continuity of the objective and constraints surfaces with respect to the discrete variables.Constrained stochastic CDFO s come from the evolutionary literature and rely on random sampling strategies, or strategies based on probabilistic criteria. Random search initiated as a concept in the 1950’s, proving asymptotic convergence results and the literature of Stochastic CDFO methods has been covered in the book of Zabinsky (2003) and the reviews of Das and Suganthan (2011) and Martelli and Amaldi (2014). The first algorithm to use random samples from a centroid template was the COMPLEX method (Box, 1965), which proceeded by replacing the worst identified feasible points. Controlled Random Search algorithms such as Simulated Annealing and Hide and Seek were later on developed, based on which new solutions are generated based on a sequence of probability distributions. Other developments which belong in this category are Genetic Algorithms, Particle Swarm Optimization, Memetic Algorithms and Tabu-search (Das & Suganthan, 2011; Ong, Nair, & Keane, 2003; Pal, Csendes, Markot, & Neumaier, 2012; Sun, Garibaldi, Krasnogor, & Zhang, 2013). There exist several developments which extend the capabilities of stochastic methods to mixed-integer optimization, such as the work of Laguna, Gortźar, Gallego, Duarte, and Martí (2014) which is developed to solve purely discrete variable problems with general constraints using scatter search. Overall, it is believed that these methods require a significant number of evaluations and should be used in cases where the space is necessarily large, complex, or poorly understood and more sophisticated mathematical analysis is not applicable (Conn et al., 2009b). This review does not contain a comprehensive list of stochastic optimization literature, since this is has been performed earlier.Several algorithmic developments combine two or more techniques in order to exploit the advantages of different methodologies in terms of convergence, sampling requirements and efficiency. Pattern search methods have been combined with genetic algorithms, and the resulting hybrid methodology has significantly better performance when compared to pure evolutionary algorithms (Egea, Martí, & Banga, 2010; Egea, Rodrigues-Fernandez, Banga, & Marti, 2007a; Egea, Vries, Alonso, & Banga, 2007b). Griffin and Kolda (2010) integrate the DIRECT algorithm with a stochastic generating set search approach to solve box-constrained problems, while Hemker and Werner (2011) combine DIRECT with surrogate-based local search for general constrained CDFO problems. More recently, Liuzzi, Lucidi, and Piccialli (2015a) propose modifications of the DIRECT algorithm in order to expedite convergence using local searches and transformations of the feasible domain. Vaz and Vicente (2007), (2009) integrate Particle Swarm Optimization (PSO) with pattern search components for solving box constrained and linearly constrained CDFO problems. Martelli and Amaldi (2014) combine three different techniques, constrained Particle Swarm, pattern search and the COMPLEX method to solve non-smooth problems with multiple nonlinear constraints.Moreover, decomposition methods have been developed to solve complex problems such as optimization of steam networks (Colmenares & Seider, 1989), where steam pressures and temperatures are optimized in the lower level using deterministic global optimization, while water and mass flow is optimized in the upper level using the COMPLEX stochastic method. Similarly, Gassner and Maréchal (2009) develop a decomposition algorithm for total-site optimization integrated with heat exchangers and utility systems. Following their approach, the upper level is a black-box non-smooth problem, while the lower level problem is formulated as a MINLP. Martelli, Amaldi, and Consonni (2011) follow a similar approach to optimize a heat recovery steam cycle application; decomposing the problem into a lower level Linear Program and a Constrained Particle Swarm upper level program. Even though the aforementioned decomposition approaches constitute an interesting approach for combining CDFO with MINLP, the stochasticity of the CDFO methods used for obtaining bounds on the upper level problems cannot guarantee global convergence or uniqueness of results.In addition, there exist several methodologies using surrogate models to approximate a part of an original MINLP model which is treated as a black-box, and develop an iterative approach collecting additional samples in order to improve convergence towards a global solution (Davis & Ierapetritou, 2007, 2008, 2009; Henao & Maravelias, 2011). Finally, Garcia-Palomares, Gonzalez-Castano, and Burguillo-Rial (2006) propose combining global search with a final local search; a recent development combines global search and local direct-search optimization in an box-constrained DFO algorithm (GLODS) which aims to identify multiple local solutions without using random multi-start methods (Custódio & Madeira, 2015).Undoubtedly, in the CDFO literature each development may have a significance since there are many characteristics which will dictate its relative performance in a specific application (i.e., model computational cost, smoothness, non-linearity, number of constraints, number of dimensions, sparsity, complex and reduced feasible regions and more). Moreover, despite the fact that convergence guarantees are important from a theoretical perspective, the number of function calls is a limiting factor towards attainment of even local optimality in many practical applications. Hence, in this case it is more important to attain better solutions with less number of samples- at the cost of theoretical convergence. Evidently, advances in CDFO have focused in the area of local optimization, while the literature is still far from guaranteeing convergence to global solutions due to the lack of mathematical structure, derivative-information and valid lower and upper bounds. Extending the capabilities of local-search CDFO methods to global methods have been proposed using multistart approaches (Audet et al., 2010), which are suitable only in the case of fast black-box simulations for which multiple function calls would be possible. Designing computationally efficient algorithms with parallelization capabilities is required in this case, (Audet, Dennis, & Le Digabel, 2008c), however as the simulation time of the optimization model increases sampling issues will still limit the applicability of local-search methods.On the other hand, there has been increasing interest in global-search model-based methods which typically employ surrogate approximations to represent unknown or black/grey-box components of a system, while they do not require an initial point and do not restrict the search to a local region. This is an interesting approach which can be seamlessly integrated within existing MINLP software described in Section 5, combining any known information with unknown while taking advantage of all of the existing capabilities of MINLP theory. Using this paradigm, hybrid MINLP/CDFO global optimization solvers can be developed to solve problems for which a large amount of the mathematical structure is known, while a fraction of it is provided as an input-output black-box system. For example, if a large-scale problem can be decomposed into two separable parts as follows: (1) a deterministically known MINLP formulation and (2) a black-box or unknown set of equations, then efficient decomposition strategies have the potential of being developed. In such an example, one can iterate between fixing variables which are involved in the black-box part and obtaining upper and lower bounds on the problem by using a global MINLP solver. Such an approach would then limit the search bounds for the employed CDFO algorithm. However, for such an approach to work efficiently there is more work that needs to be done towards obtaining bounds for the solutions of the CDFO problems within larger and more complex search regions.Additionally, there still exist challenges in the area of MINLP optimization (e.g. trigonometric functions) for which advances in CDFO such as the development of surrogate approximations with error bounding properties can be developed in order to develop better estimators of the non-convex functions.Another important future direction of CDFO methods is the incorporation techniques to detect the structure of the problem and the model sparsity in order to reduce the complexity of the problem as well as to reduce the search-space. However, since the analytical forms are not available explicitly, machine learning techniques, sensitivity analysis and statistical methods will play an important role towards this direction. For example, variable ranking methods have been incorporated in recent works (Adjengue, Audet, & Yahia, 2013; Regis & Shoemaker, 2013a) with promising results for problems with increased dimensionality. Finally, it is certain that CDFO literature has still yet to take full advantage of the theoretical developments of MINLP theory. Existing literature has focused on the use of surrogate model functions which lead to complex and non-convex representations, which in turn are not globally optimized using existing MINLP software. This trend creates several questions:1.Can the efficiency of an algorithm be improved when the complex surrogate functions used to represent black-box models are globally optimized?How useful and reliable is a fitted surrogate model in representing an unknown function, when the parameters of the model are not globally optimized? What effect does the uncertainty in the model parameters have in the performance of the method?Is it possible to guarantee global convergence by developing rigorous bounds on the error estimates of fitted models?Based on all of the above, it is expected that there are still many exciting contributions that will be made in the area of global CDFO, which will be highly intertwined with the MINLP literature.A summarized list of the currently available global optimization solvers for MINLP and their original sources is included in Table 10. Global optimization solver software for MINLP has been covered most completely by Bussieck and Vigerske (2010); the book of Belotti et al. (2013b) and the review of D’Ambrosio and Lodi (2013) also discuss several software implementations for MINLP. We augment their work with Table 11, which documents algorithmic advances for each of several deterministic global optimization codes for MINLP. The emphasis here is on the way each code base also represents an algorithmic framework with many constituent components.The newest global optimization code bases for MINLP are: αR / αSGO (Lundell et al., 2013; Lundell et al., 2009; Lundell & Westerlund, 2009, 2012), under active development at Abo Akademi; ANTIGONE (Misener & Floudas, 2014a; 2014b), released in 2013; GloMIQO (Misener & Floudas, 2012b, 2013), released in 2012; SCIP (Vigerske, 2012), MINLP component released in 2012.The newest code bases successfully implement some of the most recent algorithms: αSGO uses exponential and power-law transformations for signomials (Lundell & Westerlund, 2009; Maranas & Floudas, 1997) and the spline αBB underestimators of Meyer and Floudas (2005b) for general terms; ANTIGONE couples a hybrid relaxation strategy with polymorphic programming techniques to admit many possible relaxations into the lower bounding problem (Misener & Floudas, 2014a); GloMIQO focuses on finding and exploiting special mathematical structure in MIQCQP (Misener & Floudas, 2014b); SCIP integrates several new relaxations and bounding techniques into the software design (Vigerske, 2012). Several of the older code bases are also actively maintained with new algorithmic advances: αBB has been extended to incorporate bilevel optimization (Kleniati & Adjiman, 2014a, 2014b); BARON has been extended to consider intermediate polynomial substructures (Zorn & Sahinidis, 2014b); LINDO has been extended with a range of functional forms (Lin & Schrage, 2009).In the literature, there exist few methodologies which are designed to handle constraints efficiently, some of which have also lead to commercially available software. Box constrained grey/black-box optimization software has been described in the recent review of Rios and Sahinidis (2013). Table 12presents the most recent available software which can solve box-constrained CDFO and general constrained CDFO problems, with or without discrete variables. Methods which can handle general non-linear constraints and/or discrete variables are highlighted in the table. In Table 12, we have restricted the references to software that is easily available to the academic community, in most cases at no cost for the user.Table 13presents an MINLP test suite. This test set ranges in application from computational geometry problems to process networks problems; the case studies have 1 – 17, 136 nonlinear terms. Misener and Floudas (2014a) show complete results across this entire test set and demonstrate the capacity of each available solver with respect to several classes of criteria.The test suite of problems suitable for CDFO problems has not been consistent across the literature due to the wide range of applications and the different nature of the methodologies which are typically developed for the purpose of a specific case study. For example, several algorithms have been developed for computationally expensive applications, for which convergence to stationarity is not of great importance due to strict upper limits in function calls. Such algorithms typically employ global model-based concepts and are tested on low-dimensional problems. On the other hand, local pattern search methods are typically developed for faster grey/black-box models, which may be noisy or non-smooth. Such methods typically require multiple function calls to guarantee convergence to a local minimum and are tested on problems with higher dimensions. Consequently, it is difficult to create a pool of benchmark problems for CDFO problems due to their limitations in terms of number of input variables and number of constraints. Table 14summarizes a comprehensive set of box-constrained and general constrained global optimization set of problems which may potentially be used for evaluation of CDFO developments to test the performance, sampling requirements and dimensionality limits of a new algorithmic development. The majority of the sets of problems are taken from collections in books (Floudas et al., 1999; Schittkowski, 1987) and are often modified in order to fit the testing criteria of the algorithmic development. Specifically the number of test problems that we report in the table from online test libraries corresponds to problems which do not exceed a total of 300 in variables (continuous or discrete) or constraints, which is a reasonable bound for the current capabilities of state-of-the art methods in the CDFO literature.

@&#CONCLUSIONS@&#
This review article presents and organizes contributions for MINLP and CDFO problems into their contribution domain. In addition, we show the interconnections between the contributions and their individual places within an algorithmic whole. It is clear that the theoretical advances in MINLP are significantly greater, due to the maturity of the literature, as well as the availability of explicit functions and derivatives of the mathematical models. On the other hand, global optimality for CDFO cannot be mathematically guaranteed within a finite number of steps due to the lack of analytical forms, derivatives and valid lower/upper bounds. However, the goal of CDFO algorithms is to locate improved local solutions without having exact knowledge of the form, convexity or smoothness of the underlying model. As a conclusion, if a problem can be entirely mathematically formulated in any of the forms which are provided in the MINLP definitions, the use of an appropriate MINLP solver is suggested. However, if any part of the model is unreliable, unknown or computationally expensive, then CDFO methods can be extremely effective. The significance of the areas that we address in this manuscript becomes evident through the large and diverse set of applications of both MINLP and CDFO found in the literature. This clearly demonstrates the importance of recent developments and their major impact on improving many real-world problems coming from industry and science.