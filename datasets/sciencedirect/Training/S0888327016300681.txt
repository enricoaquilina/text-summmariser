@&#MAIN-TITLE@&#
A machine learning approach to nonlinear modal analysis

@&#HIGHLIGHTS@&#
A new approach to nonlinear modal analysis is proposed based on the property of statistical independence.The approach is a nonlinear generalisation of the Principal Orthogonal Decomposition.

@&#KEYPHRASES@&#
Nonlinear modal analysis,Machine learning,Data-based analysis,Self-Adaptive Differential Evolution,

@&#ABSTRACT@&#
Although linear modal analysis has proved itself to be the method of choice for the analysis of linear dynamic structures, its extension to nonlinear structures has proved to be a problem. A number of competing viewpoints on nonlinear modal analysis have emerged, each of which preserves a subset of the properties of the original linear theory. From the geometrical point of view, one can argue that the invariant manifold approach of Shaw and Pierre is the most natural generalisation. However, the Shaw–Pierre approach is rather demanding technically, depending as it does on the analytical construction of a mapping between spaces, which maps physical coordinates into invariant manifolds spanned by independent subsets of variables. The objective of the current paper is to demonstrate a data-based approach motivated by Shaw–Pierre method which exploits the idea of statistical independence to optimise a parametric form of the mapping. The approach can also be regarded as a generalisation of the Principal Orthogonal Decomposition (POD). A machine learning approach to inversion of the modal transformation is presented, based on the use of Gaussian processes, and this is equivalent to a nonlinear form of modal superposition. However, it is shown that issues can arise if the forward transformation is a polynomial and can thus have a multi-valued inverse. The overall approach is demonstrated using a number of case studies based on both simulated and experimental data.

@&#INTRODUCTION@&#
Modal Analysis is arguably the framework for structural dynamic testing of linear structures. While theoretical precursors existed before, the field really flourished in the 1970s with the advent of laboratory-based digital FFT analysers. The philosophy and main theoretical basis of the framework was encapsulated in the classic book [1] quite early and essentially stands to this day. The overall idea is to characterise structural dynamic systems in terms of a number of structural invariants: natural frequencies, dampings, mode shapes, and FRFs which can be computed from measured excitation and response data from the structure of interest.While linear modal analysis has arguably reached its final form – although meaningful work remains to done in terms of parameter estimation etc. – no comprehensive nonlinear theory has yet emerged. Various forms of nonlinear modal analysis have been proposed over the years, but each can only preserve a subset of the desirable properties of the linear. Good surveys of the state of the art at recent waypoints can be found in references [2,3].Most of the theories of nonlinear modal analysis proposed so far depend on demanding algebra or detailed and intensive numerical computation based on equations of motion. The current paper proposes an approach based only on measured data from the system of interest and adopts a viewpoint, inspired by machine learning, of learning a transformation into ‘normal modes’ from the data.The layout of the paper is as follows. Section 2 covers the main features of linear modal analysis, while Section 3 discusses how the approach breaks down for nonlinear systems. Section 4 gives a very condensed survey of some of the main approaches to nonlinear modal analysis taken in the past and takes some of the ideas presented as motivation for a new approach based on measured data. Examples of the new approach are presented in Section 5 via a number of case studies and the paper concludes with some overall discussion.One begins here with the standard equation of motion for a Multi-Degree-of-Freedom (MDOF) linear system,(1)[m]{y¨}+[c]{ẏ}+[k]{y}={x(t)}where{x(t)}is an excitation force,{y}is the corresponding displacement response of the system and[m],[c]and[k]are, respectively, the mass, damping and stiffness matrices of the system. (Throughout this paper, square brackets will denote matrices, curly brackets will denote vectors and overdots will indicated differentiation with respect to time.)Linear modal analysis is predicated on the existence of a linear transformation of the responses,(2)[Ψ]{u}={y}such that, the equations of motion in the transformed coordinates,(3)[M]{u¨}+[C]{u̇}+[K]{u}=[Ψ]T{x(t)}={p}have diagonal mass, damping and stiffness matrices:[M],[C]and[K]. The matrix[Ψ]is referred to as the modal matrix. The modal matrix encodes patterns of movement or coherent11Modes are commonly referred to as periodic motions of the structure; however, this term seems not to capture the spatial synchronisation aspect of the mode shape. For this reason, the term coherent motion is preferred here.motions of the structure. (It is well-known that matters are actually a little more complicated than this, e.g. complete diagonalisation of the matrices is only obtained if the damping matrix is proportional or of Rayleigh type, but the reader can consult basic texts for the details [1,4].) It immediately follows from Eq. (3) that all the degrees of freedom in the transformed coordinate system are uncoupled and each response ui(t) satisfies a Single-Degree-of-Freedom (SDOF) equation of motion,(4)miu¨i+ciu̇i+kiui=piwhere the miare referred to as modal masses. Corresponding to each of the new degrees of freedom are the standard natural frequencies and damping ratios,(5)ωni=kimi,ζi=ci2mikiIt is reasonable to refer to quantities like these as modal invariants because they are independent of the level of excitation. It is well-known that there is a dual frequency-domain representation of Eq. (4),(6)Ui(ω)=Gi(ω)Pi(ω)whereUi(ω)(resp.Pi(ω)) is the Fourier transform of ui(t) (resp. pi(t)) andGi(ω)is a modal Frequency Response Function (FRF), defined by,(7)Gi(ω)=1−miω2+iciω+kiThe overall structural FRF matrix[H]defined by,(8){Y(ω)}=[H(ω)]{X(ω)}can be recovered as a linear sum of the modal FRFs, i.e.(9)Hij(ω)=∑k=1Nψikψjk−mkω2+ickω+kkwhere the Hijare also invariant under changes of excitation level. Eq. (9) is a manifestation of the Principle of Superposition for linear systems [4]. From this theory, one can see that modal analysis allows the decomposition of an N-DOF system into N independent SDOF systems; the dynamics of the individual SDOF systems then allow the reconstruction of the general dynamics via superposition. If only a single mode, or ui, is excited, all the physical degrees of freedom yiwill be in linear proportion to that mode and the structure will thus execute a coherent motion; this fact can serve as an alternative definition of a mode.Unfortunately, most real structures have nonlinear dynamics e.g. nonlinear equations of motion [4]. The effect of structural nonlinearity on modal analysis is very destructive. Generally, all of the quantities previously described as modal invariants become dependent on the amplitude of excitation (energy). Furthermore, decoupling of the system into SDOF systems (certainly by linear transformation) is lost. One generally loses superposition.However, the engineer still needs to characterise the structure and faces essentially three alternatives [5]:1.Retain the philosophy and basic theory of modal analysis but learn how to characterise nonlinear systems in terms of the particular ways in which amplitude invariance is lost. This alternative encompasses linearisation.Retain the philosophy of modal analysis but extend the theory to encompass objects which are amplitude invariants of nonlinear systems.Discard the philosophy and seek theories that address the nonlinearity directly.Although practicality will often drive one to accept option (1), this cannot be regarded as a permanent solution to the issues presented by nonlinearity. It is the belief of the current authors that one needs to proceed by developing a mixture of options (2) and (3). Nonlinear modal analysis – the subject of the current paper – falls within the ideas of alternatives (2) and (3).Some would argue that nonlinear modal analysis does not even make sense as a term; Murdock argues [6]:“The phrase ‘mode interactions’ is an oxymoron. The original context for the concept of modes was a diagonalizable linear system with pure imaginary eigenvalues. In diagonalized form, such a system reduces to a collection of linear oscillators. Therefore, in the original coordinates the solutions are linear combinations of independent oscillations, or modes, each with its own frequency. The very notion of mode is dependent on the two facts that the system is linear and that the modes do not interact.”However, this is arguably somewhat pessimistic and based on a limiting semantics. In general, one might argue that engineers are not necessarily thinking of the mathematical basis when they refer to a mode; one might further argue that engineers are generally working with two main ideas regarding what a ‘mode’ is:(a)A coherent motion of the structure (it may be global or local).A decomposition into lower-dimensional dynamical systems – the motions of which correspond to ‘modes’. Superposition may or may not be possible.In general, it is not possible to keep all the properties of a linear normal mode (LNM) when passing to a nonlinear theory. Based on the two ideas above of what a mode should be, the origins of nonlinear modal analysis are centred around two main concepts for a nonlinear normal mode (NNM). Adopting definition (a) led to the idea of a Rosenberg normal mode[7]; definition (b) led to the idea of a Shaw/Pierre normal mode[8].Based on idea (a) – the coherent motion concept – Rosenberg observed [7]:•For linear systems, normal solutions are periodic with all coordinate motions sharing the same period.The ratios of displacements of given masses are constant for all time i.e.ui=ciu1, where the ciare constants.Based on a number of assumptions: symmetric systems, conservative systems, no internal resonance, Rosenberg essentially defined a mode as a periodic motion of the system where the second property above was generalised toui=fi(u1). All masses still move with the same period and all pass through equilibrium at the same time. There is no general principle for reconstruction from these modes although Rosenberg did give special cases where it was possible. He also required that the definition reduced to the correct one for linear systems and provided a simple generalisation. Although the Rosenberg idea has generated a great deal of progress [9–11], the remainder of this paper will concentrate on the Shaw–Pierre concept.The Shaw–Pierre concept of a nonlinear normal mode is based on idea (b) – decomposition in terms of lower-dimensional dynamical systems [8]. They observed that linear modal analysis can be reformulated in terms of invariant subspaces: if motion is initiated with a subset of LNMs present (even one), only these modes persist. For nonlinear systems, the NNMs are defined in terms of invariant manifolds; if motion is initiated on such a manifold it stays there for all time.There are a number of advantages to the Shaw–Pierre formulation over that of Rosenberg; perhaps foremost is that non-conservative systems are naturally accommodated (although recent work has also attempted to address this issue in the Rosenberg framework [12,13]). In the Shaw–Pierre approach, the equations of motion are cast in first-order form so that displacements and velocities are on equal footing. Suppose{y}=(y1,…,yn)Tis the vector of displacements and{z}=(z1,…,zN)Tis the vector of velocities, then the equations of motion are expressed as,(10){ẏ}={z}(11){ż}={f({y},{z})}Then the NNM definition generalises the Rosenberg ansatz to say that all coordinates in an NNM are functions of a single displacement/velocity pair (u,v),(12)(y1z1y2z2⋮yNzN)=(uvY2(u,v)Z2(u,v)⋮YN(u,v)ZN(u,v))Substituting the anzatz into the equations of motion generates a system of partial differential equations,(13)∂Yi(u,v)∂uv+∂Zi(u,v)∂vuf1(u,v,Y2(u,v),Z2(u,v),…,YN(u,v),ZN(u,v))=Zi(u,v)(14)∂Zi(u,v)∂uv+∂Yi(u,v)∂vuf1(u,v,Y2(u,v),Z2(u,v),…,YN(u,v),ZN(u,v))=fi(u,v,Y2(u,v),Z2(u,v),…,YN(u,v),ZN(u,v))Unfortunately, these equations are at least as difficult to solve as the original equations of motion; however, one can adopt a power series solution,(15)yk=Yk(u,v)=∑i∑jakijuivj(16)zk=Zk(u,v)=∑i∑jbkijuivjand can then solve a system of algebraic equations for the coefficients akijand bkij. (Clearly, more sophisticated methods of solving the PDEs are also applicable, e.g. reference [14] presents a Galerkin-based procedure.) Shaw and Pierre also showed that the modes can be approximately recombined to the physical motions i.e. approximate superposition. A polynomial expansion of the form above will be one of the main ingredients in the new NNM procedure proposed in this paper; the other major ingredient will come from consideration of the Principal Orthogonal Decomposition.All the methods presented so far are based on the equations of motion of the system of interest. However, there also exists a decomposition-motivated approach based on modes adapted to sampled time data – the Principal Orthogonal Decomposition (POD) [15]. The method is essentially Principal Component Analysis (PCA) from the discipline of multivariate statistics [16]. Just as in linear modal analysis, one adopts a linear transformation,(17){y}=[ϕ]{u}The difference between linear modal analysis and the POD is in the transformation matrix;[ϕ]is constructed as the matrix that diagonalises the covariance matrix[Σu]of the transformed variable{u}, i.e.(18)[Σu]=E[({u}−{u¯})({u}−{u¯})T]where E denotes the expectation operator and overbars denote mean values. The first mode is then a linear combination of the yiwith maximal power; the second mode is the orthogonal combination with next greatest power etc. The combinations are termed the Principal Orthogonal Modes (POMs). (The usual means of constructing the transformation matrix is via singular value decomposition; in that case, the singular values reflect the power associated with each POM.) The important point for the current paper is that the POMs are statistically independent, they do not influence each other in any way; this idea will be adopted in the current paper as a definition of orthogonality or normality for NNMs in general. The reason for statistical independence is that the uiin the transformed basis are uncorrelated because of the diagonalisation of[Σu]i.e.(19)E[(ui−u¯i)(uj−u¯j)]=0i≠jThe relevant background is now in place to allow the proposal of a new approach to NNMs. Like the POD, it is a data-based approach; however, unlike the POD it can be applied to nonlinear systems. The idea is to take a transformation like the Shaw–Pierre transformation – a truncated multinomial – but in this case,(20)uk=ϕk−1(y1,z1,…,yN,zN)=∑iakiyi+∑i∑jakijyiyj+∑i∑jbkijyizj+∑i∑jckijzizj+…so that the obtained ukare statistically independent i.e. are unpredictable from each other. Adopting a machine learning approach, one can learn the undetermined coefficients from measured data, freeing the decomposition from complicated analysis. Furthermore, one does not need equations of motion. Motivated by the POD example, one can see that the problem can be framed as an optimisation problem. In the case of the POD, the solution can be obtained by varying the transformation coefficients in order to minimise the off-diagonal elements of the covariance matrix[Σu]or the second-order correlations as given in Eq. (19). Unfortunately, zeroing the second-order correlations is not sufficient to give independence for nonlinear systems; one would need to remove all higher-order correlations between different uialso. Thankfully, this is not an unsurmountable problem in principle, one simply adds the correlations up to a given order in the objective function to be minimised.As one might infer from the quote from Murdock earlier, the question of terminology concerning NNMs can sometimes evoke spirited discussion. The authors take the viewpoint that words can mean what engineers intend them to mean and are happy to define the objects constructed here as nonlinear normal modes. The justification for the term nonlinear is obvious, the term mode is used in the sense of a decomposition into lower-dimensional systems. Finally, the term normal is applied as the independence assumption adopted here can be regarded as an orthogonality condition.The observant reader may wish to question the novelty of this approach, and they might be correct to do so; some justification is required here. What is proposed here may appear to be no more than a rather brute force approach to independent component analysis (ICA) [17]; the novelty here rests in the assertion that statistical independence is sufficient to motivate a meaningful definition of an NNM, and in the proposed optimisation framework. In fact, the analysis here represents an aspect of a more ambitious programme of work based on the concept of optimisation-based simplifying transformations22For want of a better name.for nonlinear systems. The idea will be to cast various methods of nonlinear dynamic analysis into a unified machine learning framework based on optimisation; preliminary work showing that normal form analysis is accommodated by the proposed framework has been presented in [18,19]. In fact, it has already been identified that linear modal analysis is, in a sense, equivalent to ICA; reference [20] identified a one-to-one relationship between vibration modes and ICA modes. The correspondence between modal analysis and ICA allowed the development of an output-only modal analysis method based on blind source separation [21].The optimisation-based process can now be illustrated through a number of examples, both computational and experimental.The method proposed here will be illustrated on a series of systems of growing complexity; the first two are based on synthetic simulated data and the last is experimental.Throughout this section, it will prove useful to use the term ‘mode’ in a slightly different sense than in the earlier discussion. In the linear theory, as discussed in Section 2, each modal DOF contributes a single resonance peak to the system FRF estimated from the physical responses. This gives the engineer or physicist another working definition of ‘mode’ i.e. the motions associated with a resonance. In the results of this section, one of the criteria for assessing the effectiveness of the transformations will be visual i.e. the extent to which the transformed spectra appear to represent a single resonance peak, so the word ‘mode’ will be used to denote a single resonance peak, which in turn will represent the belief that the transformed variable does indeed represent an SDOF system. Of course, one must take care here. If an SDOF nonlinear system is excited with high enough forcing, the FRF will also show peaks associated with harmonics of the fundamental resonance; the FRF of an MDOF system can show peaks at combination frequencies associated with the linear natural frequencies. All of this means that the association ‘peak=mode’ is an oversimplification; however, this association will prove useful in the discussion which follows.The system of interest will be a nonlinear two-DOF lumped parameter system as illustrated in Fig. 1.The equations of motion are,(21)[m00m]{y¨1y¨2}+[2c−c−c2c]{ẏ1ẏ2}+[2k−k−k2k]{y1y2}+{k3y130}={x10}The model parameters adopted here were: m=1.0, c=0.1, k=10,k3=1500. (Note that this means the damping is proportional, so the underlying linear system truly uncouples under the standard linear modal transformation.) With the parameter values stated, the natural frequencies of the underlying linear system are 0.5Hz and 0.87Hz. Data were simulated with a sampling frequency of 100Hz using a fixed-step 4th-order Runge–Kutta algorithm and the excitation x1 was chosen to be a Gaussian white noise sequence with zero mean and standard deviation 5.0, low-pass filtered onto the interval[0,50]Hz. In total, 50000 points were simulated; these were mainly used in order to estimate the spectral densities shown later (by the Welch method), only 5000 points were used for the optimisation process.For the first illustration, the uiwere computed using the standard modal transformation of the underlying linear system. Fig. 2shows the results. Both modes are present in the power spectral densities (PSDs) for the transformed coordinates; the system is clearly not uncoupled by standard linear modal analysis, as one would expect. The level of distortion of the second ‘mode’ is very clear; the parameters of the system and the level of forcing have been chosen in order to give highly nonlinear response. Further evidence for the large effect of nonlinearity is given by the large shifts in resonance frequencies from the linear natural frequencies.For the second illustration, the optimisation approach proposed in the last section was adopted. Because the optimisation problem is a continuous one, the Self-Adaptive Differential Evolution (SADE) algorithm was used here, as it has proved powerful for other structural dynamic problems in the recent past [22]. The object of the exercise was to learn the parameters aijof the linear transformation,(22){u1u2}=[a11a12a21a22]{y1y2}(writing the transformation in a matrix form turns out to be advantageous in terms of the Matlab [23] computation.)In order to reproduce the results of the (linear) POD, the objective function was chosen as the sum of second-order correlations between the uj. Because the POD is constrained to generate an orthogonal transformation matrix; a penalty term was added to the objective function to enforce the constraint. To improve the conditioning of the problem, the response data were standardised before optimisation. The objective function was thus,(23)J=|{A1}·{A2}|+|Cor(u1,u2)|where{Ai}is the ith column of the transformation matrix[A], and Cor denotes the linear coefficient of correlation between the two transformed variables.Of course, the POD transformation could be estimated using the standard eigenvalue approach; the objective here was to benchmark the SADE approach on a case where the correct transformation could be verified independently. In all the applications of SADE in this paper, 10 runs were made with the coefficients initialised randomly between −10 and 10. This is usual as SADE is a stochastic algorithm; one makes several runs with different initialisations in order to reduce the possibility of hitting local minima. As SADE is an evolutionary algorithm, the probability of hitting a local minimum is already reduced. As a further simplification, the expansion only used the displacement variables yi; this would be sufficient for the POD of a linear system.The results of the POD computation are shown in Fig. 3; the system is not uncoupled. Although the first ‘mode’ is separated out more effectively than when the modal matrix is used, the second ‘mode’ is actually worse. The[A]matrix estimated corresponded very closely to the linear algebraic estimate via PCA.The final illustration is based on a SADE optimisation using a cubic transformation,(24){u1u2}=[a11a12a21a22]{y1y2}+[b11b12b13b14b21b22b23b24]{y13y12y2y1y22y23}and an objective function which minimised all correlations up to third order. Again, to simplify the computation, the terms with velocities were disregarded. As in the last example, a penalty function was used to impose orthogonality on the{Ai}coefficient vectors obtained; however, it is not immediately clear what constraints should be used for the nonlinear problem as the transformation is nonlinear. Orthogonality has the advantage that it reduces to the correct constraint in the linear case. (It is clearly necessary that any credible definition of an NNM should reduce to the definition of an LMN in the limit that a system behaves linearly.) To help the search, the linear transformation coefficients were initialised in a narrow interval around the linear PCA values; the b coefficients were initialised in the interval[−100,100]. The results of the computation are shown in Fig. 4.The results show a major improvement on the linear results: modal and POD. There seems to be excellent uncoupling into what appear to be individual ‘modes’. This observation is of course based upon the preconception, discussed earlier, that a ‘mode’ would reveal itself as an isolated peak in the spectrum, and is an artefact of linear thinking. However, there is an issue with the current results as follows. Across the 10 SADE runs performed, the transformations varied in their ability to uncouple the DOFs when the results were judged ‘by eye’, based on visual inspection and armed with linear preconceptions. A neighbouring solution with higher cost function actually appeared to achieve better separation ‘by eye’ and the results from this transformation are shown in Fig. 5.This result was not unexpected; the reason lies with the definition of the objective/cost function. The problem is that the function used in the optimisation does not guarantee statistical independence, it is only designed to remove all correlations between variables up to third order. This means that correlations above third order are not controlled. Suppose that the transformation with lowest cost leaves substantial correlations at fourth order; it is plausible that a less successful transformation on the lower-order correlations may accidentally have better behaviour on the higher-order ones and thus represent an improvement in terms of true statistical independence. There are other potential avenues for improvement on the cost function. Firstly, it is not immediately clear what constraints should be imposed on the coefficient vectors for the transformations; even if orthogonality is one necessary constraint, there may be others. Secondly, the constraint should be imposed using a Lagrange multiplier in the objective function given in Eq. (23); the value of the multiplier was set arbitrarily to unity; in the usual practice of machine learning the value of the weighting should be set in a principled manner like cross-validation on an independent data set. Finally, the transformation here was restricted to use displacement variables only even though the system simulated had damping, so velocities should perhaps have been included. Presumably, velocities would also be needed if the (linear) damping was non-proportional. These matters certainly require further investigation.The second simulated system is simply a three-DOF analogue of the first, a chain system with equations of motion,(25)[m000m000m]{y¨1y¨2y¨3}+[2c−c0−c2c−c0−c2c]{ẏ1ẏ2ẏ3}+[2k−k0−k2k−k0−k2k]{y1y2y3}+{k3(y1)300}={x1(t)00}with the same mass, damping and stiffness parameters as before. The excitation force and sampling parameters were also the same. The full nonlinear transformation adopted for SADE is given by,(26){u1u2u3}=[a11a12a13a21a22a23a31a32a33]{y1y2y3}+[b11…b19b21…b29b31…b39]{y13y12y2y12y3y23y22y1y22y3y33y32y1y32y2}with only the aijparameters used for the PCA/POD example. Once again, the notation[A]=[{A1}{A2}{A3}]is adopted. Third-order correlations were included in the objective function as before, thus it was given by,(27)J=|{A1}·{A2}|+|{A1}·{A3}|+|{A2}·{A3}|+|Cor(u1,u2)|+|Cor(u1,u3)|+|Cor(u2,u3)|+|Cor(u13,u2)|+|Cor(u13u3)|+|Cor(u23,u1)|+|Cor(u23,u3)|+|Cor(u33,u1)|+|Cor(u33,u2)|As a basis for comparison, the results of attempting to decouple the system by using the modal matrix of the underlying nonlinear system are shown in Fig. 6. The results are very poor in terms of decoupling, the second and third modes are dominated by the first mode in all the transformed degrees of freedom.When linear SADE (corresponding to PCA/POD) is applied, the results are somewhat better (Fig. 7); the first mode still dominates; however, the second and third modes are better represented in u2 and u3.Finally, the nonlinear (cubic) SADE transformation was applied; the results are shown in Fig. 8. The nonlinear transformation is far more effective in separating the modes; the first mode only substantially appears in u1, where it is almost perfectly isolated. Furthermore, much clearer emphasis on the second and third modes is obtained in u2 and u3. In the lower amplitude u2 and u3, one can see that the nonlinear transformation has induced, or rather amplified, some components at combination frequencies e.g. the second harmonic of the third mode is visible in u3. Although the results are excellent, there is some degradation compared with the 2DOF results; this might be expected on the basis that many more coefficients are included in the optimisation problem. As in the 2DOF case, the SADE population produced a transformation with higher cost which appeared to decouple better by eye, and this is shown in Fig. 9.The final case study is just intended as a demonstration on experimental data, rather than on a substantially more complex system. The data were taken from the bookshelf structure developed at Los Alamos National Laboratories within the Engineering Institute [24]. The structure is a three-storey base-excited model of a shear building. Although the overall structure is linear, it is possible to engage a bumper mechanism between floors which introduces a nonlinear contact mechanism. The data studied here correspond to a situation where the bumper is engaged and the dynamics are therefore nonlinear. Fig. 10shows response spectra (corresponding to a sampling frequency of 320Hz) for the four floors (including the base) under a broadband base excitation when the bumper was engaged between the top two floors. The spectra only appear to show evidence of three ‘modes’ as a result of the base excitation, so the SADE algorithm was applied here as if the system had 3DOF and the responses of the floors above ‘ground’ were taken as the DOFs.When SADE was applied with a linear transformation (corresponding to the POD/PCA), the results were as shown in Fig. 11; although the second ‘mode’ is separated out cleanly in u2, the other transformed DOFs are clearly not pure. The situation is much improved when a cubic transformation is used with SADE, as illustrated in Fig. 12; each of the transformed DOFs is clearly dominated by one of the modes with only small artefacts showing evidence of remaining interaction. In the case of the experimental data set, the transformation with the lowest cost produced a much less clear extraction of single peaks, so it is the best SADE solution selected ‘by eye’ which has been presented here.The different case studies here have all shown that the proposed decomposition shows promise and does support interpretation as a ‘modal’ decomposition. Before discussing the strengths and weaknesses revealed by this study, it is necessary to consider the other important desirable ingredient of a modal decomposition; i.e. superposition.As discussed in Section 2, linear modal analysis has arguably two key ingredients; the first is a (linear) transformation into independent ‘modal’ coordinates, the second is an inverse (linear) transformation from the modal coordinates back to physical coordinates. So far in this paper, only the first transformation has been addressed. The forward transformation was found via an optimisation process that did not explicitly specify the targets of the transformation, but rather, demanded a certain condition that those targets (the transformed variables) should satisfy (i.e. statistical independence). This mode of machine learning, without explicit targets is usually referred to as unsupervised learning[25]. It has long been recognised that PCA or the POD can be cast as an unsupervised learning problem; in fact, it was shown in the beautiful paper by Roweis and Gharamini [26] that the POD belongs to a class of linear problems (both static and dynamic) that can be solved within a unifying Expectation Maximisation (EM) framework.The interesting thing about the inverse transformation required here, is that the targets are now specified i.e. the vector{y}mapping to the modal{u}is known in each case; this means that learning the inverse transformation is a standard (nonlinear) regression problem addressable by supervised learning[25]. There are any number of learning algorithms which could be employed in the current context; the one chosen here is the Gaussian Process (GP) algorithm [25,27]. The GP has been chosen because, apart from providing a principled Bayesian approach to regression, it very naturally provides confidence intervals for its predictions; this property will turn out to be rather interesting. Although GPs are being used more often in a structural dynamic context, they are still not commonplace, so some background theory will be provided here in order to make this paper a little more self-contained.The basic premise of the Gaussian process (GP) method is to perform inference over functions directly, as opposed to inference over parameters of functions.For simplicity, the discussion here will assume that the system of interest has a single output variable. Adapting the notation of [27], let[X]=[{x}1,{x}2…{x}N]Tdenote a matrix of multivariate training inputs, and{y}denote the corresponding vector of training outputs. The input vector for a testing point will be denoted by the column vector{x⁎}and the corresponding (unknown) output byy⁎.A Gaussian process prior is formed by assuming a (Gaussian) distribution over functions,(28)f({x})∼GP(m({x}),k({x},{x}))wherem({x})is the mean function andk({x},{x′})is a positive-definite covariance function.One of the defining properties of the GP is that the density of a finite number of outputs from the process is multivariate normal. Together with the known marginalisation properties of the Gaussian density, it is therefore possible to consider the value of this function only at the points of interest: training points and predictions. Allowing{f}to denote the function values at the training points[X], andf⁎to denote the predicted function value at a new point{x⁎}, one has,(29)({f}f⁎)∼N({0},[K([X],[X])K([X],{x⁎})K({x⁎},[X])K({x⁎},{x⁎})])where a zero-mean prior has been used for simplicity (see [27] for a discussion), andK([X],[X])is a matrix whosei,jthelement is equal tok({x}i,{x}j). Similarly,K([X],{x⁎})is a column vector whose ith element is equal tok({x}i,{x⁎}), andK({x⁎},[X])is the transpose of the same.In order to relate the observed target data{y}to the function values{f}, a simple Gaussian noise model can be assumed,(30){y}∼N({f},σn2[I])where[I]is the identity matrix andσn2constitutes a hyperparameter which can easily be identified by optimisation. Since one is not interested in the variable{f}, it can be marginalised (integrated out) from Eq. (29)[27], as the relevant integral,(31)p({y})=∫p({y}|{f})p({f})d{f}is over a multivariate Gaussian and is therefore analytically tractable. The result is the joint distribution for the training and testing target values,(32)({y}y⁎)∼N(0̲,[K([X],[X])+σn2[I]K([X],{x⁎})K({x⁎},[X])K({x⁎},{x⁎})+σn2])In order to make use of the above, it is necessary to re-arrange the joint distributionp({y},y⁎)into a conditional distributionp(y⁎|{y}). Using standard results for the conditional properties of a Gaussian reveals [27],(33)y⁎∼N(m⁎({x⁎}),k⁎({x⁎},{x⁎}))where(34)m⁎({x}⁎)=k({x}⁎,[X])[K([X],[X])+σn2[I]]−1{y}is the posterior mean of the GP and,(35)k⁎({x}⁎,{x′})=k({x}⁎,{x′})−K({x}⁎,[X])[K([X],[X])+σn2[I]]−1K([X],{x′})is the posterior variance.Thus the GP model provides a posterior distribution for the unknown quantityy⁎. The mean from Eq. (33) can then be used as a ‘best estimate’ for a regression problem, and the variance can also be used to define confidence intervals.There does remain the question of the choice of covariance functionk({x},{x′}). In practice, it is often useful to take a squared-exponential function of the form:(36)k({x},{x}′)=σf2exp(−12l2∥{x}−{x}′∥2)although various other forms are possible (see [27]). Eq. (36) is the form adopted here. The covariance function involves the specification of two hyperparametersσf2and l. The hyperparameters can be optimised using an evidence framework, along with the noise parameterσn2[27]. Denoting the complete set of these parameters as{t}, they can be found by maximising a function,(37)f({t})=−12{y}T[K([X],[X])+σn2[I]]{y}−12log|K([X],[X])+σn2[I]|which is equal to the log of the evidence, up to some constant. Since the number of hyperparameters in this case is small, the optimisation can be carried out simply by gradient descent.The first illustration of inversion/superposition here will concern the 2DOF case study exhibited earlier. As with all machine learning algorithms, a principled application requires the definition of a training set (on which to learn the mapping) and a testing set (on which to validate the mapping). In order to give the simplest learning problem and to ensure that the GP is trained to interpolate rather than to extrapolate, the training set was taken as the odd-numbered samples from 2000 points of{{u},{y}}data, and the testing set comprised the even-numbered samples.33It is important to be clear on what is meant by the word interpolation here. The GP does not interpolate in a temporal sense i.e. the value at a testing point is not obtained by taking temporal neighbours and applying, say, a Lagrangian formula. The GP is a static map and is insensitive to the temporal nature of the training data; in fact, the training data could be randomly re-ordered and the interpolant, which is over the input-space, would not change. In the particular case here, where a squared-exponential function has been chosen for the covariance kernel, the GP is in fact equivalent to a radial-basis function interpolant [28] or RBF neural network [29].The results from the trained GP when re-presented with the training data are shown in Fig. 13. The GP results on the training data are nearly perfect; the figure does not show the confidence bounds on the predictions as they would be indistinguishable from the prediction line.When the GP predictions are computed on the training and testing sets, one sees something very interesting. The predictions are just as accurate as they were on the training data, except for some isolated points which show a distinct prediction error (Fig. 14). The explanation of the isolated errors requires a little careful thought. Fig. 15shows a close-up of one of the regions with prediction errors; two important observations can be made. The first observation is that the prediction errors are still (of course) very small on the alternate points that were present in the training data. This shows clearly that the GP errors are not the result of extrapolation as the points with marked prediction errors are directly between training points. Another reason why one might suspect something subtle is that the set of erroneous predictions at alternate points is introducing structure into the predictions with a much shorter correlation length than elsewhere in the (largely very smooth) responses. Clearly the GP has learned a correlation length scale appropriate to the majority of the data and this raises a question as to why short-scale errors should arise. The authors believe that the explanation for the errors lies in the nature of the forward transformation. The forward transformation adopted here was a cubic multinomial in the physical responses; this means that the inverse transformation is necessarily multi-valued. The errors in the GP predictions are due to the fact that the GP is a smooth single-valued function and will therefore produce erroneous results if the physical data move (smoothly) between sheets of the true inverse function.The second interesting observation on the GP inverse transformation is that the 99.7%(3σ)confidence intervals expand at the erroneous predictions and bound them; this is precisely what is required of confidence intervals. This behaviour is not completely intuitive and requires explanation; however, an analytical explanation does not appear to be trivial. Work is in progress on this matter based on the fact that the factorK({x}⁎,[X])[K([X],[X])+σn2[I]]−1in the GP predictive mean is also present in the predictive variance, and it thus seems plausible that the factor driving the predictions away from their true values might also help to inflate the confidence bounds at those points. A ‘hand waving’ argument regarding the behaviour of a GP attempting to predict a multi-valued function runs as follows. If a number of training sample pairs have the same{x}input, but distinct outputs y, the GP can only assume that the y values are sampled from a single Gaussian; even if the training data are from a noise-free function, the only way the GP can generate different outputs is to inflate the output variance at the point{x}.The second illustration of learning the inverse is for the 3DOF simulated system in the case studies. In this case, the inversion appears to be much more demanding as a result of the less successful forward transformation. Only the results for the response y1 will be presented. As before, the GP was trained on alternate (odd-numbered) points from 2000 and tested on all 2000. The GP predictions on the testing set with 99.7% confidence intervals are shown in Fig. 16. Although further study is required, one supposes that the lower accuracy is a result of the fact that truly independent uicoordinates were not obtained in the forward transformation and that some information was lost. As always, it is encouraging to see that the confidence intervals do bound the true values.There is clear degradation in passing from the inversion results for the 2DOF system to the those from the 3DOF system; this will be discussed in the following section. Similar degradation is visible in the results for the bookshelf data; this was not a surprise as it gave similar results on the forward transformation to those from the 3DOF simulated system.

@&#CONCLUSIONS@&#
The current paper proposes a new approach to nonlinear modal analysis based on a generalisation of the Principal Orthogonal Decomposition (POD) and also motivated by the Shaw-Pierre approach to nonlinear modal analysis. The approach is based on optimising a nonlinear transformation from the physical coordinates to a frame in which the coordinate variables are statistically independent. Independence is used here as the measure of ‘normality’ or orthogonality of the derived ‘modes’. Because independence is central to the approach taken here, it currently only makes sense for random data; however, that issue bears further investigation. An advantage of the approach taken here is that complicated algebraic analysis is not needed. In fact, even the details of the equations of motion are not needed; this makes the method particularly suited to experimental investigation of nonlinear systems; in principle, only measured responses are needed. Of course, the dependence on data can bring a disadvantage, which is that the transformation learned may only be applicable for the level of excitation experienced by the structure when training data were collected. This is an issue with any machine learning method – the problem of generalisation. The solution (in the context of machine learning) is to measure training data which truly capture the operational range of the structure of interest; failing this, one should regard the models obtained as only effective models, to be used in similar circumstances to those experienced in training. Of course, generalisation can be an issue with analytical methods also; if an approximate form for the transformation is computed, like the polynomial form used in the original Shaw–Pierre paper on NNMs, then the mapping may also be input-dependent. The issue of generalisation is also going to affect the computation of the inverse modal transformation in the data-based approach proposed. The results presented here are quite preliminary, they arguably raise as many issues as they settle and much remains to be done before a robust and principled methodology can emerge.The foremost issue to be dealt with concerns the objective function used for optimisation. The main idea of the approach here is to secure statistical independence of the transformed variables or modes and this cannot be accomplished by considering a finite number of correlation statistics. However, other statistics which do measure total independence are available. The most direct assertion of independence for a set of variables uiis that their joint density functionp(u1,…,un)factors into a product of the marginal densities,(38)p(u1,…,un)=p(u1)…p(un)where,(39)p(u1)=∫−∞∞…∫−∞∞p(u1,…,un)du2…dunetc.Setting aside the difficulty of estimating high-dimensional densities [30,31], one needs to convert the condition (38) into an objective function. This can be accomplished by defining a metric on the space of densities, one of the most well-known quantities of this nature is the Kullback–Leibler (KL) divergence44There is an abuse of terminology here; the KL divergence is not truly a metric as it is not symmetric i.e.DKL[p(x)∥q(x)]≠DKL[q(x)∥p(x)]and does not satisfy the triangle inequality; it is in fact a premetric.[32,33],(40)DKL[p(x)∥q(x)]=∫−∞∞p(x)logp(x)q(x)dx(with the obvious generalisation to multivariate densities). The KL divergence is positive semi-definite, and zero only whenp(x)=q(x). Now, Total correlation can be defined as the Kullback–Leibler distance from the joint pdf of a set of variables to the product of the marginal pdfs,(41)C(U1,…,Un)=DKL[p(u1,…,un)∥p(u1)…p(un)]This function is only zero if all the uiare mutually independent and can therefore serve as an objective function for measuring independence. The quantity can also be understood in terms of entropy [32,33],(42)C(U1,…,Un)=[∑i=1nH(Ui)]−H(U1,…,Un)where H is the usual (Shannon) entropy,(43)H(x)=−∫−∞∞p(x)logp(x)dx(with the obvious generalisation to multivariate densities). The interpretation of Eq. (42)is straightforward; entropy represents expected information content, the equation says that there is no more information in the joint density than in the sum over the marginal densities – another independence criterion. Unfortunately, although the total correlation is an excellent candidate for the objective function required here, it can be very time-consuming to compute.Other candidates for the objective function exist; one possibility was motivated by the fact that general correlation measures can be overly sensitive to linear relationships between variables e.g. the Pearson Coefficient. Distance Correlation was introduced by Gabor Szekely in 2005 [34] as a more refined tool for measuring nonlinear correlations. The definition is lengthy and the reader is referred to the original reference, but like total correlation, distance correlation is only zero for independent variates. Unfortunately, the issue for optimisation is that, like total correlation, distance correlation is much more expensive to compute than the covariance matrix. Fast computation of total independence measures is one of the more important areas of further work concerning the current paper.These matters concerning the objective function are important because of the issue raised concerning the case studies. The issue is that those transformations which visibly conform best to our preconceptions of a modal decomposition (clear, isolated, single resonance peaks) are not always those with the lowest objective function values. The current hypothesis of the authors is that this is simply because the objective function used here does not encode true statistical independence.Another important issue raised in the paper is the nature of the ‘modal’ transformation; here a cubic multinomial has been adopted. This will almost certainly represent a truncation of the ‘true’ transformation; however, increasing the polynomial order of the transformation causes the number of parameters for optimisation to grow very fast, quickly making the problem intractable from a computational viewpoint. A polynomial transformation was also adopted here, because it arguably allows a ‘physical’ interpretation of the coefficients e.g. they could, in principle, be compared with those from the analytical Shaw-Pierre approach. However, there may be advantages in adopting a nonparametric ‘black-box’ form for the transformations. In fact, as observed above, the content of the approach presented here represents a variant of independent component analysis (ICA). Powerful nonparametric approaches to ICA – like kernel-ICA [35] – have been developed recently, and their application to nonlinear modal analysis will be presented in a sister paper to the current one, a (very) preliminary study can be found in [36]. One of the possibilities of adopting the nonparametric approach is that a transformation with a single-valued inverse could be learned, avoiding the issue which arose in this analysis. It is important to mention here that an attempt at nonlinear modal analysis using machine learning has already been presented in [37]. That paper was motivated by the fact that the linear POD is entirely equivalent to PCA, so that a nonlinear ‘modal’ analysis could be implemented by using a nonlinear variant of PCA. Nonlinear PCA was implemented in [37] by using an auto-associative neural network (AANN); this gives a form of unsupervised learning which was identified as a form of PCA in [38]. A five-layer AANN was used in which the central ‘bottleneck’ layer gave a reduced-dimension representation of the measured (physical) variables. The authors of [37] distinguished between ‘modal’ and ‘non-modal’ transformations depending on a single or multiple AANNs respectively. What distinguishes the approach in [37] from the one proposed here is that the variables from the bottleneck layer of an AANN are not proven to be statistically independent (analysis of AANNS is somewhat intractable) and thus are not necessarily ‘modes’ in the sense proposed here. That said, the method proposed in [37] is a perfectly acceptable approach to dimension reduction for nonlinear systems and has been successfully applied in the context of model reduction [39]. Furthermore, the AANN approach in [37] learns the forward and inverse transformations simultaneously in a nonparametric manner and thus neatly avoids any issues associated with multi-valued functions.It is important to consider other basic restrictions built into the transformations represented here. The first is that the transformations are functions of displacement variables only. This assumption is made here on the basis that the case studies presented were given proportional damping so that a real linear transformation on the displacements was known to suffice in the linear case. Proportional damping was thus assumed in order to try and control the complexity of the optimisation problem; although velocities do not appear to have been needed here, they will almost certainly need to be considered for a general nonlinear non-proportionally damped system. Finally, the examples presented here have been chosen to exclude the possibility of internal resonances; if such resonances are present, the transformations would have to allow the possibility of mapping into higher dimensional manifolds representing more than one ‘modal’ variable at a time. As in the case of non-proportional damping, internal resonances do not represent an obstruction to the validity of the method proposed here, they will simply increase the dimension of the optimisation problem.Setting aside the issue of whether the approach presented here can be accepted as true nonlinear modal analysis, it is almost certainly a useful tool for model reduction and system identification. The transformed variables uipresented in the case studies here clearly resemble SDOF systems in terms of their spectral response. One would therefore hope that identification of the processes{x}⟶uiwould be easier than direct identification of the process{x}⟶{y}; the former identification would then allow the latter via superposition. The approach presented here allows for direct identification of the generative processes{x}⟶uivia standard methods of nonlinear system identification or via machine learning. Given the likely complexity of the equations of motion for the processes{x}⟶uia nonparametric approach like Gaussian process NARX models may be indicated [40].As a final observation, the paper [26] presented a unified Bayesian framework for linear systems which included the POD as a special case. Work is currently underway to see if the framework can be extended to nonlinear systems and thus provide an alternative approach to nonlinear modal analysis.