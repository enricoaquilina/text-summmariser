@&#MAIN-TITLE@&#
An iterative longest matching segment approach to speech enhancement with additive noise and channel distortion

@&#HIGHLIGHTS@&#
Speech enhancement with both additive noise and channel distortion.Use of corpus data to reduce uncertainty of speech to be estimated.Use of corpus data to reduce training and testing data mismatch for speech recognition.An iterative algorithm to improve speech estimate.Improved results on Aurora 4 for speech recognition and speech enhancement.

@&#KEYPHRASES@&#
Corpus-based speech modeling,Longest matching segment,Noisy speech,Channel distortion,Speech enhancement,Speech recognition,

@&#ABSTRACT@&#
This paper presents a new approach to speech enhancement from single-channel measurements involving both noise and channel distortion (i.e., convolutional noise), and demonstrates its applications for robust speech recognition and for improving noisy speech quality. The approach is based on finding longest matching segments (LMS) from a corpus of clean, wideband speech. The approach adds three novel developments to our previous LMS research. First, we address the problem of channel distortion as well as additive noise. Second, we present an improved method for modeling noise for speech estimation. Third, we present an iterative algorithm which updates the noise and channel estimates of the corpus data model. In experiments using speech recognition as a test with the Aurora 4 database, the use of our enhancement approach as a preprocessor for feature extraction significantly improved the performance of a baseline recognition system. In another comparison against conventional enhancement algorithms, both the PESQ and the segmental SNR ratings of the LMS algorithm were superior to the other methods for noisy speech enhancement.

@&#INTRODUCTION@&#
This paper presents a new approach to speech enhancement from single-channel measurements involving noise, channel distortion (i.e., convolutional noise) and their combination, and demonstrates its application to improving speech recognition and to improving noisy speech quality. Modeling combined noise and channel distortion has been a major challenge in robust speech recognition. Research has been conducted in two main directions. The first is robust features; examples include speech enhancement (Couvreur and van Hamme, 2000; Deng et al., 2004; Logan and Robinson, 1997; Stouten et al., 2004), RASTA filtering (Hermansky and Morgan, 1994), feature normalization (De la Torre et al., 2005; Furui, 1981; Viikki and Laurila, 1998), SPLICE (Deng et al., 2000), and feature space adaptation (Li et al., 2002; Saon et al., 2001). The second research direction is robust acoustic models; examples include adaptive model training such as MAP (Gauvain and Lee, 1994), MLLR and CMLLR (Gales, 1998; Kim and Gales, 2011), predictive noise compensation such as parallel model combination (Gales and Young, 1995) and vector Taylor series compensation (Acero et al., 2000), joint uncertainty decoding (Liao and Gales, 2007) which combines feature transformation and model compensation, and missing-feature theory (Raj and Stern, 2005). Recent new developments include discriminative models (Ragni and Gales, 2011) and deep neural network based techniques (Seltzer et al., 2013). The work described in this paper is a complement to the robust feature approaches. We present a new approach to extracting clean speech features from single-channel measurements with both background noise and channel distortion. In extracting the features we focus on the reduction of the training and testing data mismatch which is critical to the success of speech recognition.In speech enhancement, most current approaches impose few or very loose constraints on the underlying speech to be estimated. As a result, they require specific knowledge about the noise for noise removal and hence speech recovery. The typical constraint or prior for the underlying speech is the probability distribution of the speech short-time discrete Fourier transform (DFT) coefficients or spectral amplitudes (e.g., Cohen, 2005; Ephraim and Malah, 1984; Lotter and Vary, 2005; Martin, 2002; Martin and Breithaupt, 2003). The common methods for noise estimation include prediction by using neighboring measurements without significant speech content based on voice activity detection, minimum statistics, time-recursive averaging, MMSE-based high-resolution noise DFT estimation and their combination (e.g., Cohen, 2003; Hendriks et al., 2010; Lin et al., 2003; Martin, 2001; Rangachari and Loizou, 2006; Sohn and Kim, 1999). Some recent studies (e.g., Chinaev et al., 2012) have considered noise estimation based on some initial estimate of the speech power. Data-driven speech models, built on the training data of real speech, represent a different way of imposing prior or constraint on the speech to be estimated. Common speech models include vector-quantization (VQ) codebooks (e.g., Naidu and Srinivasan, 2012; Srinivasan et al., 2006), Gaussian mixture models (GMM) (e.g., Kundu et al., 2008), hidden Markov models (HMM) (e.g., Ephraim et al., 1989; Sameti and Deng, 2002; Zhao and Kleijn, 2007), and inventory-based models which use prerecorded phonetic-class speech segments to restrict the enhanced signals (e.g., Nickel et al., 2012; Raj et al., 2011; Xiao et al., 2009). Some of the modeling techniques used in robust speech recognition have also found use in data-driven models for speech enhancement (e.g., Roux and Hershey, 2012; Seltzer et al., 2005). In this research, we further tighten the constraint for the speech to be estimated. We use a corpus consisting of complete speech utterances with little manipulation to provide examples of both short-time spectral shapes and up to sentence-long spectral variation for the speech to be extracted from noise and channel distortion. We show that the tightened constraint with long speech segments for the underlying speech could help to reduce the requirement for specific knowledge about the noise and channel, and could help to obtain an improved speech estimate in terms of improved speech recognition and speech enhancement performance.This work is an extension of our previous work described in Ming et al. (2011, 2013). In Ming et al. (2011), we described a corpus-based approach for speech enhancement from additive noise. In Ming et al. (2013), we extended this approach to addressing the problem of separating two simultaneous speakers (i.e., speech separation). In this paper, we further extend this approach in three aspects. First, we extend the approach to single-channel speech enhancement with both additive noise and channel distortion (i.e., convolutional noise). Second, in Ming et al. (2011) we modeled unknown noise using a combination of multicondition model training and missing-data decoding; in this extended research we present an improved method to model noise for speech estimation, which shares some characteristics with the speech separation method described in Ming et al. (2013). Finally, we further extend the single-pass estimation algorithm to an iterative estimation algorithm; the new algorithm uses the previous corpus-based noise and channel estimates to update the corpus speech model for improved speech estimates. We demonstrate the improved performance for the new approach through experiments for speech recognition and speech enhancement.The remainder of this paper is organized as follows. Section 2 outlines the assumptions made in this research for modeling noisy speech, and the key idea of the proposed approach for speech estimation. Section 3 introduces the first part of the proposed algorithm, including corpus-based modeling of speech with noise and channel distortion, and the longest matching segment algorithm for speech estimation. Section 4 describes a further development of the algorithm, including the refinement of the initial estimates and an iterative estimation algorithm for new, improved speech estimates. Experimental studies of the new approach for speech enhancement and as a preprocessor for feature extraction for speech recognition are described in Section 5. Finally, conclusions are presented in Section 6.Let X1:T={xt:t=1, 2, …, T} represent a wideband, clean speech signal, expressed as the time series of the signal's logarithmic short-time power spectra (STPS) xt, where t is the discrete frame time. Consider a single-channel measurement of X1:Tin an adverse condition, with both background noise and channel distortion. Let Y1:T={yt:t=1, 2, …, T} be the measured signal. In this study, we assume no specific knowledge about the noise and channel. We only assume that the noise statistics and channel frequency characteristic change slower than the speech. This slowly varying noise and communication channel assumption forms the basis of most current methods for speech enhancement and speech recognition (for example, spectral subtraction, RASTA filtering, minimum-statistics based noise predication, cepstral feature normalization, model/feature space adaptation and prediction, etc.). In this paper, we describe a novel way of applying this assumption to speech estimation from noise and channel distortion. Specifically, we assume that real-world, slowly varying noises can be approximated by piecewise stationary random processes. Assuming independence between the speech and noise, the noisy speech signal can be expressed as(1)yt=ln(ext+h+ent)where we use h to represent the log channel characteristic assuming it is fixed during the utterance, and ntto represent the log STPS of the noise assuming it is piecewise stationary. Assume that ntis subject to a Gaussian distribution. By piecewise stationarity we mean(2)nϵ∼N(μnt,Σnt)forϵ∈[t,τ]That is, from t the noise statistics (mean vector and covariance matrix)λnt=(μnt,Σnt)will remain invariant for a segment of consecutive frames from time t to τ, as a function of t, while the speech statistics may change on a frame-by-frame basis. Butλntcan change across the segments to model nonstationary noise. Except for this local stationarity, we do not assume specific knowledge about the noise, i.e., the value ofλntand the length of the measurement segment τ on which the noise can be assumed stationary. Nor do we assume specific knowledge about the channel characteristic h.We propose a new approach for speech estimation based on the time-variation differences between the speech, noise and channel, as assumed above. In our approach, we assume that we have a clean, wideband speech corpus to provide temporal-spectral examples of the speech to be extracted. We use a simplified example to illustrate our idea. Consider the power spectral density (PSD) as the statistics of a signal in the linear-spectral domain. Suppose Fig. 1shows, on the top, the noisy signal PSD yk,tfor a specific frequency bin k sampled at consecutive discrete frame times t, consisting of the clean signal PSD xk,tand some unknown noise PSD nk,t. Below the noisy signal, Fig. 1 shows, on the left, a corpus of pre-recorded sample PSD sk,tof the clean signal xk,t, and on the right, examples of stationary noise PSD of variable noise levels used to model the piecewise stationary measurement noise assuming the noise model (2). As mentioned previously, the noise PSD can change from one level to another, on a segment-by-segment basis, to model globally nonstationary noise. For yk,tat each t, we aim to find a corpus sample and a noise candidate which, when added, match the given yk,t. Knowing the make-up of this matched combination we can obtain an estimate of the clean signal using the matched corpus sample. However, this will not easily work if we focus on matching short measurements. As illustrated in the upper part of Fig. 1, given a single short-time noisy PSD measurement, there can be many different matched combinations between the corpus sample and the noise candidate. This explains why speech enhancement based on short measurements (e.g., single frames) requires specific knowledge about the noise for resolving the uncertainty. But, if we focus on matching longer measurements, e.g., segments of consecutive frames, and assume stationary noise (and hence a constant noise PSD) in the segment, then the number of possible matched combinations reduces (see the lower part of Fig. 1), subject to the nonnegative, constant noise PSD constraint. The longer the stationary noise segment and the matched combination found, the more specific the matched corpus sample segment and hence the signal estimate. This example can be extended to include a channel change in the measurement, which, in our assumption, only introduces a time-invariant gain change in each frequency bin in the corpus samples to form the match. Therefore, we propose the longest matching segment (LMS) approach: at each time t, we find the longest noisy segment from t that can assume stationary noise and has an accordingly matched corpus speech segment, subject to a constant channel factor. As illustrated in the above example, if the noise and the channel change slower than the speech, and can be approximated with piecewise stationarity or invariance, this approach may lead to the estimates of the matched corpus speech segments with the least uncertainty. Since it is difficult to obtain accurate PSD estimates for nonstationary speech and noise, we implement the LMS approach for the log STPS features using the above assumed statistics for the noise, and using the corpus-based statistics, described below, for the speech. The following section details the modeling of the speech log STPS features and the basic LMS algorithm for speech estimation.Assume that we have a clean, wideband speech corpus. We build the speech enhancement system by first normalizing all the corpus speech utterances to a common gain. Let Ω={S1:Γ} represent the corpus, consisting of gain-normalized sample speech utterances S1:Γ={st:t=1, 2, …, Γ}, where strepresents the log STPS of the speech frame at time t. As in Ming et al. (2011, 2013), we model the whole corpus Ω by using a GMM, and model each sample utterance S1:Γ by using a corresponding Gaussian sequenceλS1:Γ={λst:t=1,2,…,Γ}, whereλst=(μst,Σst)is a Gaussian taken from the corpus GMM that produces maximum likelihood for the frame st. The corpus utterance modelλS1:Γcan be viewed as a template-based statistical model for speech; it captures all spectral temporal variations in S1:Γ, and yet it models each frame with a smoothed Gaussian distribution. We use such models of corpus speech utterances to provide temporal-spectral examples for the speech to be extracted.Given a noisy speech signal, we normalize its gain to the gain of the corpus speech data. This gain normalization may be performed in two steps: (a) normalize the average gain of the noisy signal to that of the corpus data, and (b) detect the frequency bands in the normalized noisy signal with a higher average gain than that of the corresponding frequency bands of the corpus data, if found further adjust the gain of the normalized noisy signal so that these frequency bands will have the same average gain as that of the corresponding corpus data. Therefore, in such a gain-normalized noisy signal, the underlying speech signal's gain may be smaller than the matched corpus speech signal's gain due to the existence of noise, and due to channel distortion which can cause a loss of speech energy at certain frequency bands. To model the corpus utterance S1:Γ with a gain change, which is common to all the frequency bands, and a channel change, which can be different for different frequency bands, we use the modelλS1:Γ,g+h={λst,g+h:t=1,2,…,Γ}, whereλst,g+h=(μst+g1+h,Σst)is the Gaussian for the corpus frame stwith a gain change g and a channel change h, where 1 denotes a unit vector.We model the unknown, piecewise stationary measurement noise by first generating a stationary zero-mean white noise with the same gain as the corpus speech data. We obtain a model for this noise by estimating a Gaussian density (i.e., (2)) with a diagonal-covariance matrix for the log power spectrum of the simulated noise data. From this gain-normalized noise model, noise models at other gain levels can be obtained conveniently by adding for each level a corresponding gain change to the mean vector of the gain-normalized noise model. We use λn=(μn, Σn) to represent the statistics of this gain-normalized stationary white noise, and λn,q=(μn+q, Σn) to represent the noise with a gain change vector q. By allowing different gain levels for different frequency bands in q, the noise model λn,qis capable of simulating stationary colored noise based on the stationary white noise model. Additionally, as in most systems, in our experiments for each given noisy utterance, we collect some measurements from the beginning and end of the signal, which we assume does not contain speech, to obtain a Gaussian density estimate for the noise. This alternative noise model was used along with the white noise model as the noise candidates.Given a gain-normalized noisy utterance Y1:T, we use Yt:τ={yϵ:ϵ=t, t+1, …, τ} to represent a segment from time t consisting of consecutive frames from t to τ. In a similar way, we useλSζ:η={λsϵ:ϵ=ζ,ζ+1,…,η}to model a corpus speech segment Sζ:ηtaken from a corpus speech utterance S1:Γ and consisting of the consecutive frames from time ζ to η, and useλSζ:η,g+h={λsϵ,g+h:ϵ=ζ,ζ+1,…,η}to model the same corpus speech segment with a gain change g and a channel change h. Assume that each log STPS vector consists of K frequency-band components. Let yk,t, hkand sk,trepresent the k’th frequency-band component of the noisy measurement yt, channel characteristic h and corpus speech frame st, respectively, and letλsk,t=(μsk,t,Σsk,t)represent the corresponding Gaussian statistics for sk,t,λsk,t,g+hk=(μsk,t+g+hk,Σsk,t)represent the corresponding Gaussian statistics modeling sk,twith a gain change g and a channel change hk, andλnk,qk=(μnk+qk,Σnk)represent the k’th component of the statistics λn,qof the simulated noise, modeling the noise at the k’th frequency-band with a gain change qk. In these expressions, we assume diagonal covariance matrices for both the speech and noise log STPS vectors.Consider a statistical approach to compare the noisy segment Yt:τand a corpus segment Sζ:η. Assume stationary noise in Yt:τand assume that, compared to the corresponding corpus speech segment, the speech segment in Yt:τis subject to a fixed gain change and a fixed channel change, which we do not assume specific knowledge of. We write the likelihood function of Yt:τassociated with Sζ:ηas(3)p(Yt:τ|λSζ:η)≃maxg,h,qp(Yt:τ|λSζ:η,g+h,λn,q)=maxg≤0∏k=1Kmaxhk≤0maxqk≤ln(1−exp(g))∏ϵ=tτp(yk,ϵ|λsk,w(ϵ),g+hk,λnk,qk)In (3), we assume conditional independence between the frames and the frequency-band components in Yt:τ(conditioned on the segment Sζ:η), andp(yk,ϵ|λsk,w(ϵ),g+hk,λnk,qk)is the likelihood of the noisy measurement yk,ϵgiven the measurement model (1) and the statistics of the speech (represented by the corpus model), gain, channel and noise. In our experiments presented in this paper, we use a linear time-warping functionw(ϵ)=ζ+ϵ−tto compare the two segments and compare only equal-length segments (in our previous experiments with the LMS method for speech segment match, we have often found that using dynamic time warping (DTW) to calculate the match likelihood is insignificant in improving the match accuracy). For the noisy utterance with the gain normalized to the corpus data, as described above, the inside speech gain g≤0 due to the existence of noise; g=0 means there is no noise in Yt:τ. Given a speech gain g, the maximum allowable noise gain can be approximately written as ln(1−exp(g)) for the noise model λnalso with the gain normalized to the corpus data, so that the noise power plus the speech power do not exceed the noisy utterance power11For illustration, supposePy2,Ps2, andPn2represent the gain-normalized average power of the noisy measurement, corpus speech and noise, respectively. The gain normalization leads toPy2=Ps2=Pn2. Therefore for additive noise we may assume thatPy2≃GPs2+(1−G)Pn2=exp(g)Ps2+(1−exp(g))Pn2, where g=lnG is the logarithmic speech gain. Hence the corresponding logarithmic noise gain is approximately limited by ln(1−exp(g)).; colored noises are accounted for with the white noise model by selecting different noise gain levels qkin different frequency bands to match the given measurement. The negative channel characteristic hkin each frequency band represents the distortion of the wideband speech signal in that band caused by the channel effect; hk=0 means there is no channel distortion in the frequency band. The likelihood of the match between the two segments Yt:τand Sζ:ηis decided through optimizing the parameters g, qkand hkon the segment level assuming stationary noise and constant channel characteristic in the segment. In other words, given a noisy segment Yt:τ,p(Yt:τ|λSζ:η)indicates the likelihood of the noisy segment with stationary noise and with an accordingly matched corpus segment Sζ:η, subject to a time-invariant channel factor. In our experiments, the maximization in (3) is performed by selecting the parameters g and hkfrom a set of predefined statistics modeling a range of possible signal-to-noise ratios (SNRs) and channel distortion, with details given later. Given the speech and noise statistics, we use the log-normal approximation (Gales and Young, 1993) to calculate the likelihoodp(yk,ϵ|λsk,w(ϵ),g+hk,λnk,qk)which takes the form of a Gaussian function (see Eqs. (11)–(16) in Section 4.1 for details of how to calculate this likelihood with the appropriate model parameters). At each time t, we aim to find the longest noisy segment Yt:τ(by extending τ) that can assume stationary noise and has an accordingly matched corpus segment (Fig. 1), subject to a time-invariant channel factor. We achieve this through maximizing the likelihood (3) among all the other likelihoods. This can be formulated as a maximum a posteriori (MAP) problem (i.e., Eq. (8)). The following presents the details.Assume an equal prior probability P for all possible speech segments. We define the posterior probability of the match of a corpus speech segment Sζ:ηgiven the noisy segment Yt:τ, assuming stationary noise and a fixed channel change in Yt:τ, as(4)P(λSζ:η|Yt:τ)=p(Yt:τ|λSζ:η)Pp(Yt:τ)≃p(Yt:τ|λSζ:η)∑Sθ:ϑ†∈Ωp(Yt:τ|λSθ:ϑ†)+p(Yt:τ|ϕ)wherep(Yt:τ|λSζ:η)is the likelihood function defined in (3). The denominator, the average likelihood of the noisy segment p(Yt:τ), is expressed as a sum of two terms. The first term is the average likelihood of the given noisy segment Yt:τassuming that it contains stationary noise and has an accordingly matched corpus speech segment; the second term, p(Yt:τ|ϕ), represents the average likelihood of Yt:τwhen the previous assumption does not hold. The conditions that violate the assumption may include: Yt:τis too long to find a matched speech segment in the corpus, or Yt:τis too long to be modeled by stationary noise, or both. We use the following expression to model the likelihood of Yt:τassociated with unseen speech segments and/or nonstationary noise(5)p(Yt:τ|ϕ)=maxg≤0∏k=1Kmaxhk≤0∏ϵ=tτ∑sk,w(ϵ)∈Ω∑qk≤ln(1−exp(g))p(yk,ϵ|λsk,w(ϵ),g+hk,λnk,qk)P(sk,w(ϵ))P(qk)In (5), for each frame yϵin Yt:τ, an average likelihood is calculated over all corpus speech frames and all different noise statistics, to account for the unseen speech segment and/or nonstationary noise in Yt:τ, where the different noise statistics are simulated by the white noise statistics with variable gain levels in each frequency band, andP(sk,w(ϵ))and P(qk) represent the prior probabilities of the individual speech frames and noise statistics, respectively. We can see the resemblance of (5) with a GMM used to model text-independent speech (the term “text” corresponds to the segmental dynamics of the speech and noise in discussion). In our experiments, we use uniform priorsP(sk,w(ϵ))and P(qk).Noisy segments with mismatched corpus speech segments and/or with nonstationary noise are likely to result in low likelihoods of match defined by (3) but not necessarily low likelihoods of mismatch defined by (5), and hence are likely to result in low posterior probabilities of match based on (4). For the noisy segment Yt:τwhich contains stationary noise and has an accordingly matched corpus speech segmentSˆζ:η, we can assume that the corresponding likelihood of match based on (3) is greater than the corresponding likelihood of mismatch based on (5) (explained below) and hence, we will have a large posterior probability based on (4). Thus, the posterior probability (4) can be used to identify the matched stationary noise and corpus speech segment combination. Given a noisy segment, a large posterior probability will be obtained for a corpus speech segment if the noisy segment contains stationary noise and is matched accordingly by the corpus speech segment; a small posterior probability may indicate a mismatched corpus speech segment and/or nonstationary noise in the given noisy segment. The reason that we can assumep(Yt:τ|ϕ)≤p(Yt:τ|λSˆζ:η), wherep(Yt:τ|λSˆζ:η)represents the likelihood of match associated with the matched corpus segmentSˆζ:ηwith a correspondingly matched stationary noise segment represented by the gain vectorqˆ, is that, based on (5), we can write p(Yt:τ|ϕ) approximately as(6)p(Yt:τ|ϕ)≃maxg≤0∏k=1Kmaxhk≤0∏ϵ=tτp(yk,ϵ|λsˆk,w(ϵ),g+hk,λnk,qˆk)P(sˆk,w(ϵ))P(qˆk)≤maxg≤0∏k=1Kmaxhk≤0∏ϵ=tτp(yk,ϵ|λsˆk,w(ϵ),g+hk,λnk,qˆk)=p(Yt:τ|λSˆζ:η)The first approximation is based on the assumption that the matched and hence highly likely stationary noise and corpus speech segment combination dominates the mixture-based likelihood.To locate the longest noisy segment Yt:τwith stationary noise and with a matched corpus speech segment, hence to remove as much of the uncertainty of the estimate of the matched corpus speech segment as possible as illustrated in Fig. 1, we point out an important property of the posterior probability (4): its value increases when a longer noisy segment, with stationary noise, is matched. Assume that the noisy segment Yt:τwith stationary noise is matched by the corpus speech segmentSˆζ:η, in the sense that the likelihood of matchp(Yt:τ|λSˆζ:η)≥p(Yt:τ|λSθ:ϑ†)for anySθ:ϑ†≠Sˆζ:η, andp(Yt:τ|λSˆζ:η)≥p(Yt:τ|ϕ). Then we can have the following inequality concerning the posterior probabilities of the match of variable-length corpus segments and noisy segments with stationary noise(7)P(λSˆζ:w(ϵ)|Yt:ϵ)≤P(λSˆζ:η|Yt:τ)where Yt:ϵwith ϵ≤τ is a noisy segment starting at the same time as Yt:τbut not lasting as long, andSˆζ:w(ϵ)is the corresponding corpus subsegment matching the shorter noisy segment Yt:ϵ. This inequality can be proved conveniently (see Ming et al., 2011, 2013). Based on (7), therefore, we can obtain an estimate of the longest noisy segment Yt:τfrom t with stationary noise and with a matched corpus speech segment, through maximizing the posterior probabilityP(λSζ:η|Yt:τ)with respect to τ and the corpus segment candidate Sζ:η. We express the estimates as(8)Sˆζ(t):η(τmax),hˆt,qtˆ,gˆt=argmaxτmaxSζ:η∈ΩP(λSζ:η|Yt:τ)where τmax denotes the maximum τ found, andYt:τmaxcorresponds to the longest noisy segment found from t which can assume stationary noise and has an accordingly matched corpus segmentSˆζ(t):η(τmax), in terms of the maximum posterior probability. As indicated, this longest match is found by first finding for each fixed-length noisy segment Yt:τthe most-likely match, and then finding the Yt:τwith maximum length τmax that results in the maximum posterior probability. Along with the estimate of the matched corpus speech segment, we can also obtain the estimates of the corresponding channel characteristichˆt, stationary noise statisticsλn,qˆtand gain of the matched corpus speech segmentgˆtfrom (3) which form the longest segment match. The following outlines the algorithm for solving the estimation problem (8):For each test segment Yt:τfrom tFor each segment length τCalculate p(Yt:τ|ϕ) using (5)For each corpus segment Sζ:ηCalculatep(Yt:τ|λSζ:η)using (3) and record the optimal parametershˆt,qˆtandgˆtFor each corpus segment Sζ:ηCalculate posteriorP(λSζ:η|Yt:τ)using (4)Obtain the matched corpus segment and parameters with maxP(λSζ:η|Yt:τ)at given τObtain the longest matched corpus segment and parameters with maxP(λSζ:η|Yt:τ)over τSince we assume the channel frequency characteristic remains invariant during an utterance, we can obtain a smoothed channel estimate by averaging the individual segment-based estimateshˆtover the whole utterance; on average, each segment-based estimate is weighted by the corresponding segment posterior probability. Leth˜represent the smoothed channel estimate; we use the expression(9)h˜=1P¯∑t=1ThˆtP(λSˆζ(t):η(τmax)|Yt:τmax)where the posterior probability obtained in (8) is used as a confidence score for each segment-based channel estimate, andP¯is a normalization factor equalling the sum of the posterior probabilities across the utterance. In our experiments, we have found that the above smoothing operation is useful to correct the channel estimation biases, which may arise from those matched segments which have little speech content, or are short and hence have small posterior probabilities.A similar operation can be applied to the segment-based stationary noise statistics estimatesλn,qˆt. This may lead to more accurate noise estimates, especially for the noise with nonstationary characteristics. While we assume locally stationary noise in each matched segment, we assume that the noise statistics can change across the segments to model nonstationary noise. The noise estimates for the same noisy frame from different longest matched segments may each contain some information about the on-going nonstationary noise and can be averaged to obtain a smoothed noise estimate. Fig. 2presents an example, showing the temporal power variation of a segment of restaurant noise (taken from the Aurora 4 database) during a nonstationary event, measured at the output of a mel-frequency filter centered at 1622Hz. In Fig. 2, each horizontal straight-line segment corresponds to the mean estimate of the log power of the noise in an appropriate longest matched noisy segment based on (8); the dotted curve corresponds to a smoothed mean estimate obtained by averaging at each frame time the multiple noise mean estimates for the time from the different longest matched segments. In general, denoting byλ˜nϵ=(μ˜nϵ,Σ˜nϵ)the smoothed noise statistics estimate at time ϵ, we use the expression(10)λ˜nϵ=1P˜∑tifϵ∈[t,τmax]λn,qˆtP(λSˆζ(t):η(τmax)|Yt:τmax)where as defined earlier in Section 3.1,λn,qˆt=(μn+qˆt,Σn)with λn=(μn, Σn) being the gain-normalized white noise statistics, and the sum is taken over all longest matched noisy segmentsYt:τmaxthat contain the noise frame nϵ, withP˜being a normalization factor equalling the sum of the posterior probabilities associated with the segments included in the average. Note that as an estimate of nonstationary noise, the smoothed noise statistics estimateλ˜nϵcan change with time on a frame-by-frame basis, as shown in Fig. 2. The same expression (10) can be used to obtain a smoothed gain estimate for the speech frame in each noisy frame, by replacingλn,qˆtwith the segment-based speech gain estimategˆt. Letg˜ϵrepresent the smoothed estimate, for the gain of the speech frame in the noisy frame yϵ. This estimate will be used later for updating the corpus speech model.After obtaining the smoothed channel, noise and speech gain estimatesh˜,λ˜nϵandg˜ϵ, we consider how to incorporate them into the LMS system for a new search for the longest matched corpus speech segments, with the aim of improving the speech estimate. The smoothed channel and noise estimates can be used to modify the wideband, clean corpus speech model to reduce the mismatch against the noisy measurement, or used to reduce the level of distortion in the noisy measurement, thereby reducing the error in segment matching. In the following, we describe an algorithm to add compensation into the corpus speech model, and an iterative LMS algorithm for estimating the underlying speech.As described, we model each corpus speech utterance by using a sequence of frame-based, maximum-likelihood Gaussians taken from the corpus GMM. By introducing channel and noise compensation into the corpus GMM, we therefore introduce the compensation into all the corpus utterances built on the GMM used for finding the matched segments. Let {λm=(μm, Σm):m=1, 2, …, M} represent the corpus GMM with M Gaussian density functions λm(note that earlier for clarity we have addressed a corpus Gaussian by using the corpus speech frame it models, e.g.,λst; but it should be understood thatλst∈{λm}). We useλm(h˜,λ˜nϵ)to represent the modified λmwhich includes appropriate compensations for the channel distortionh˜and additive noise with statisticsλ˜nϵ. In the new search for the longest matching segments, we will model the corpus speech utterances/frames by replacing the clean Gaussians λmwith the corresponding channel and noise compensatedλm(h˜,λ˜nϵ), for comparing with the noisy frame yϵfor ϵ=1, 2, …, T. Based on the log-normal approximation, the corrupted speech frames are approximately log normally distributed, and henceλm(h˜,λ˜nϵ)=(μm(h˜,λ˜nϵ),Σm(h˜,λ˜nϵ)), whereμm(h˜,λ˜nϵ)andΣm(h˜,λ˜nϵ)represent the mean vector and covariance matrix of the channel and noise compensated Gaussian, respectively. Assuming a diagonal covariance matrix, the k’th mean and variance element, denoted byμm(h˜k,λ˜nk,ϵ)andΣm(h˜k,λ˜nk,ϵ), can be expressed as (Gales and Young, 1993)(11)μm(h˜k,λ˜nk,ϵ)=ln(μ¯m(h˜k,λ˜nk,ϵ))−12lnΣ¯m(h˜k,λ˜nk,ϵ)μ¯m2(h˜k,λ˜nk,ϵ)+1(12)Σm(h˜k,λ˜nk,ϵ)=lnΣ¯m(h˜k,λ˜nk,ϵ)μ¯m2(h˜k,λ˜nk,ϵ)+1whereμ¯m(h˜k,λ˜nk,ϵ)andΣ¯m(h˜k,λ˜nk,ϵ)represent the linear-spectral domain statistics to form the channel and noise compensation for the clean Gaussian λm, which can be expressed as(13)μ¯m(h˜k,λ˜nk,ϵ)=eμm+Σm/2+h˜k+μ¯nk,ϵ(14)Σ¯m(h˜k,λ˜nk,ϵ)=e2μm+Σm+2h˜k(eΣm−1)+Σ¯nk,ϵThe statistics in (13) and (14) are each expressed in two terms: the first term shows the channel compensationh˜kand the second term shows the noise compensationμ¯nk,ϵandΣ¯nk,ϵto the clean corpus Gaussian λm; the noise compensation is calculated from the noise estimateλ˜nk,ϵ=(μ˜nk,ϵ,Σ˜nk,ϵ)with the well-known relation(15)μ¯nk,ϵ=eμ˜nk,ϵ+Σ˜nk,ϵ/2(16)Σ¯nk,ϵ=μ¯nk,ϵ2(eΣ˜nk,ϵ−1)After forming the new corpus GMM based on (11)–(12), we re-normalize the gain of the noisy utterance Y1:T, and the gain of the stationary white noise model λn, to the gain of the new corpus model, as described in Section 3.1. Hence, by replacing each corpus speech frame Gaussianλsw(ϵ)in (3) and (5) with the corresponding channel and noise compensated Gaussianλsw(ϵ)(h˜,λ˜nϵ), we can rerun the longest matching segment based estimation (8) to obtain new estimates of the matched corpus speech segments. In the new search, the to-be-determined channel change h and noise statistics λn,qin (3) and (5) model the residual channel change and additive noise in the noisy utterance as compared to the compensated corpus speech model, assuming that the residual channel change is fixed during the utterance and the residual noise is piecewise stationary. The above two processes of the longest matching segment based speech, channel and noise estimation, and the formation of the noise and channel compensated corpus speech model based on the smoothed channel and noise estimates, can be alternated to form an iterative algorithm. A new iteration starts with the update of the clean corpus Gaussians λmthrough (11)–(14) with the smoothed channel characteristic and noise statisticsh˜k,μ¯nk,ϵandΣ¯nk,ϵaccumulated over all previous iterations. The accumulated statistics for the i’th iteration, denoted byh˜k(i),μ¯nk,ϵ(i)andΣ¯nk,ϵ(i), can be computed using recursion(17)h˜k(i)=h˜k(i−1)+h˜k(18)μ¯nk,∈(i)=μ¯nk,∈(i−1)+μ¯nk,∈eg˜∈(19)∑¯nk,∈(i)=∑¯nk,∈(i−1)+∑¯nk,∈e2g˜∈whereh˜k,μ¯nk,ϵandΣ¯nk,ϵare the smoothed channel and noise estimates andg˜ϵis the smoothed corpus frame gain estimate, generated after the (i−1)’th iteration based on (9), (10), (15) and (16). The division of the noise estimates by the corpus speech gain estimate is needed to make the compensated corpus speech model have approximately the same SNR as the noisy utterance as indicated in the estimates. The complete iteration algorithm can be summarized as follows.Initialization:Set the iteration index i=0; set the accumulated channel characteristic and noise statisticshˆk(0),μ¯nk,ϵ(0),Σ¯nk,ϵ(0)to zero.Perform the LMS-based estimation (8). Stop, or go to Step 2 with i=i+1.Update the clean corpus GMM.–Obtain smoothed channel, noise and corpus frame gain estimates based on (9) and (10).Update the accumulated channel characteristic and noise statistics using the smoothed channel, noise and corpus frame gain estimates obtained above, based on (17)–(19).Update the clean corpus GMM using the accumulated channel characteristic and noise statistics obtained above, based on (11)–(14). Go to Step 1.In our experiments, for each test utterance in each iteration, we calculate the average length of the longest matched segment found over all the test frames. We stop the iterations when there is no significant change in this average segment length between successive iterations. For more discussion see Section 5.3.Based on the longest matched corpus speech segments found at each time t (i.e., (8)), there can be several ways to build the estimates of the underlying speech frames. In this paper, we consider two different applications of the above system: speech enhancement and feature extraction for speech recognition. We use the Aurora 4 database in our experiments, and have found the following methods produce the best results.As we estimate a matched corpus speech segment from each noisy frame, each underlying speech frame can be included in a number of adjacent matched corpus speech segments, each segment providing an estimate of the frame (Fig. 2 shows the same situation for the estimation of the underlying noise frames). We can obtain an estimate of the underlying speech frame at t by using the matched corpus speech frame chosen from the matched corpus speech segment that has the longest left and right contexts about t. We have considered other methods, including taking the average of the corresponding estimates from the different matched segments, but found that the estimates with the longest and most balanced left and right contexts demonstrate the desirable quality in terms of the individual frame sharpness and the cross-frame continuity (measured by several objective tests including speech recognition and PESQ, for example). Given the estimate of the matched corpus speech frame for each noisy frame, we can reconstruct the underlying clean speech frame by forming a Wiener filter as in Ming et al. (2011). However, we found that this may not be the best method for speech recognition, because of the likely mismatch of the enhanced speech features against the training data. An advantage of the corpus-based system is that it can effectively connect, through the corpus data, the often separately implemented speech recognition and speech enhancement tasks, to achieve joint optimization for reducing the training and testing data mismatch. In our experiments for speech recognition, we build the enhanced speech features by directly taking the matched corpus speech features as the enhanced features; the same corpus speech features are also used to train the speech recognizer, thereby achieving a degree of matched condition training and testing.For speech enhancement, while Wiener filtering based on the matched corpus speech frame, as described in Ming et al. (2011), can be used to suppress the additive noise, it is not effective for recovering speech from channel distortion. Therefore, in our speech enhancement experiments with the Aurora 4 database, we reconstruct the waveform for each underlying speech frame by using the magnitude spectrum of the matched corpus speech frame. A further advantage of corpus-based speech enhancement is that we have the option of using the phase spectra of the matched corpus speech signals to reconstruct the waveforms of the speech being estimated. Although the noisy measurements phase spectra have proven to be usable for speech enhancement from noise, we have experienced poor performance on the Aurora 4 database for reconstructing the speech waveforms with the noisy measurements phase spectra with both noise and channel distortion. One possible reason is that some channel distortion (e.g., bandwidth reduction) can significantly reduce the speech energies in certain frequency bands and hence cause the phase spectra in these bands to become unusable or dominated by noise. In our experiments when this becomes a problem, we take the phase spectra from the matched corpus speech frames as an alternative. This was found to give better performance for speech enhancement.The experiments were conducted on the Aurora 4 database (Hirsch, 2002), which contains speech data with additive noise and combined additive noise and channel distortion. Aurora 4 is generated from the test data set of the WSJ0 database for a 5K-word speaker-independent speech recognition task. Table 1summarizes the data used in our experiments. We built an HTK-based 5k-word speech recognition system following the HTK WSJ Training Recipe (Vertanen, 2006) using the training data shown in Table 1, and using a bigram language model, for speech recognition experiments. The recognition system used 13 static MFCC (mel-frequency cepstral coefficients) plus the first and second order derivatives as the feature vector for each frame. In a slight difference from the recipe system, in our system we dropped the zero’th cepstral coefficient (C0) to account for the variable gain changes of the reconstructed speech. Then, we used a subset of the WSJ0 training data set (SI-TR-S) as the wideband, clean speech corpus to build the proposed LMS enhancement system as a preprocessor for providing clean speech feature estimates for the recognition experiments, as well as for speech enhancement experiments. As mentioned earlier, for speech recognition, the enhanced speech features built on the training data reduces the training and testing data mismatch. We only considered the training and test speech data sampled at 16kHz.The corpus WSJ0 training set (SI-TR-S) we used to build the LMS speech enhancement system consists of 12776 utterances from 101 speakers (roughly balanced in gender) and was recorded with a Sennheiser microphone in quiet environments. In our LMS based speech enhancement experiments, for identifying matching speech segments, we divided speech signals into frames of 20ms with a frame period of 10ms, and then represented each frame using the Mel-frequency log filterbank power spectrum with 50 channels. We built the LMS enhancement system by first normalizing all the corpus utterances to a common gain, then using all the corpus utterances to train a GMM with 4096 Gaussian densities with diagonal covariance matrices, and finally obtaining a statistical model for each training utterance by associating each frame in the utterance with a Gaussian density chosen from the GMM which produces maximum likelihood for the frame, as described in Section 3.1.Aurora 4 consists of 330 test speech utterances from eight speakers not included in the training/corpus data set. Each test utterance is recorded with a Sennheiser microphone (and hence contains no channel distortion compared to the corpus data), and also with one of three other microphones each introducing a different type of channel distortion compared to the corpus data. Aurora 4 is divided into two parts. The first part contains test data with additive noise only, which is formed by adding noise to the test utterances recorded with a Sennheiser microphone. Six different types of noise are used: car, babble, restaurant, street, airport and train station, each being added to the 330 test utterances at a randomly chosen SNR between 5 and 15 dB for each test utterance. This forms six test sets of noisy speech plus one test set without noise corruption for experiments, each test set containing 330 utterances.The second part of Aurora 4 contains test data with both additive noise and channel distortion, compared to the clean corpus data recorded with a Sennheiser microphone. The test data are generated by adding the same types of noise, at a randomly chosen SNR between 5 and 15 dB, to the test utterances recorded with one of the three other microphones: a Shure SM91, a RadioShack Pro-Unidirectional Highball, and a AT&T 720 Handset. Like the first part of test data, the second part of test data includes six test sets with both noise and channel distortion, plus one test set without noise and with channel distortion only; each test set contains 330 utterances.As described in Section 3.1, we simulated the piecewise stationary noise by generating stationary zero-mean white noise with the same gain as the corpus speech data. Additionally, for each given test utterance, we used the first and last 20 frames of the signal to obtain a Gaussian density estimate for the noise. This new noise model was used as an alternative to the white noise model – in calculating the likelihood of the measurement in (3) and (5), the noise model of the two which produced a larger likelihood would be used. Given a noisy test utterance, we normalized its gain to the gain of the corpus data. Taking the gain-normalized noisy utterances as input, we considered a range of segment-level speech gain losses to account for the noise and channel effects in the segment, from 0dB (i.e., no gain loss) to −48dB divided uniformly into 25 levels. Based on our experiments, we found that modeling this range of gain losses was suitable for the Aurora 4 data with variable noise and channel distortions, and that the 25-level quantization offered a good balance between the modeling accuracy and the computational efficiency. From this range, we used the segment-level gain losses from 0 dB to -20dB to model the speech gain loss due to the existence of noise in the segment (corresponding to a local, or segment-level, SNR from +inf to about −20dB), and used the rest of the segment-level gain losses to model the channel distortion. This corresponds to the speech gain g in the maximization in (3) and (5) taking a value from G=[0.0, −0.2, −0.4, …, −2.0]×ln10, with a total of eleven levels. Given a speech gaing=G[v]wherevis the index of the gain value set G, the corresponding noise gain qkin (3) and (5) for each frequency band takes a value from the set[ln(1−exp(G[v′])):v′≤v]. This is subject to the constraint that the power of the model of speech plus noise should not exceed the power of the noisy measurement; the use of the speech gain resolution to quantize the noise gain range for the search reduces the amount of computation for (3) and (5).From each g that models the noise-caused gain loss which applies to all speech frequency bands, we further modeled the gain loss in each frequency band caused by the channel distortion, by selecting the channel characteristic hkfor each frequency band, in (3) and (5), from the current g (i.e., no channel distortion) to g−28dB with a 2-dB resolution, giving fourteen further levels. As shown in the above, in the implementation of the LMS enhancement system for the experiments, we computed only 25-level gain changes in each frequency band for the corpus speech segment model, and the corresponding number of gain variations for the noise segment model, to model a wide range of unknown noise and channel distortions in the measurement. Also note that the above full-range search for the gains of the matched corpus speech segments may only be needed in the initial estimation of the longest matching segments. In the subsequent iterations based on the previous estimates, we can reduce the search range accordingly to account for the reduced variations of the residual channel distortion and noise. This was implemented in our experiments, and caused no performance degradation. When dealing with the test speech without channel distortion, we set hk=0 for all frequency bands. When the longest matched corpus segments were found, the clean speech frames were reconstructed using the DFT magnitudes of the corresponding corpus speech frames.First, we evaluated the proposed LMS enhancement system by performing speech recognition experiments. In these experiments, the LMS system was used as a preprocessor for clearing the noise and channel distortion from the input signals before passing them for recognition by the HTK baseline recognition system described above. Table 2shows the word error rates (WER) produced by the HTK baseline recognition system when taking (a) the unprocessed noisy speech as input and (b) the reconstructed speech features from the LMS enhancement system as input, respectively. The effect of the channel distortion on the recognition accuracy can be clearly seen in Table 2, particularly for the “clean” speech recognition. For this wideband, clean speech trained baseline recognition system, the channel distortion alone had significantly increased the WER. We have studied the data, and found that some of the alternative microphones introduced not only spectral distortion, but also significant electrical noise, to the speech signal. As described earlier, we did not use the reconstructed waveforms from the LMS system to calculate the features for recognition (we found this produced poorer results, possibly due to the discontinuity of the adjacent frames which can cause some distortion in calculating the dynamic features for recognition). Instead, we took both the static and dynamic features directly from the matched corpus speech frames. This is found to be helpful in reducing the training and testing data mismatch.In Table 3, we compare the recognition results obtained above with the results obtained by some of the other systems performing speech recognition on Aurora 4 published recently in the literature, to show the effect of the LMS enhancement system as a preprocessor for feature extraction for robust speech recognition. The results in Table 3 show that, among the selected recognition systems, using the LMS enhancement system to extract the acoustic features for speech recognition has raised the baseline recognition system performance from last position to around third position. We see no reason not to suppose that the application of the LMS-based preprocessing for feature extraction would also help improve the performance of the other recognition systems.In obtaining the above recognition results with the LMS system, we performed four iterations of the LMS-based estimation for each test utterance with noise only, and six iterations of the estimation for each test utterance with both noise and channel distortion, based on the iterative algorithm described in Section 4.1. In each iteration, a new corpus model was formed based on the previous accumulated noise and channel estimates for a new search of the longest matching segments. In our experiments, we found that the iterations converged and always led to improved speech estimates in terms of improved speech recognition and speech enhancement performance compared to without iteration. Fig. 3shows the effect of the iterations on the speech recognition WER obtained on Aurora 4, averaged over the seven test conditions (six with noise, one noise free) with and without channel distortion. The iteration reduced the WER by absolute 4.7% for the test data without channel distortion (test conditions A and B combined), and by absolute 9.9% for the test data with channel distortion (test conditions C and D combined). After convergence, we did see further iterations might lead to random, but extremely small fluctuations in some performance measures. In our experiments, we have observed a strong correlation between the speech recognition accuracy and the enhanced speech ratings for the LMS algorithm. The results presented below for speech enhancement were produced based on the same number of iterations for each test utterance.Next, we evaluated the proposed LMS system for speech enhancement applications. Table 4shows the PESQ scores for the unprocessed noisy speech and for the reconstructed speech from the LMS enhancement system. Again, we see that the channel distortion alone had significantly degraded the speech quality, in comparison to the original wideband clean speech. We conducted experimental comparisons with other conventional speech enhancement algorithms. Since many of these algorithms do not include a component for processing channel distortion, we compare with these conventional algorithms only on the part of the Aurora 4 test data without channel distortion. Fig. 4shows the comparison of the PESQ scores between the LMS algorithm and four other enhancement algorithms, which we found produced better results among other algorithms. Two sets of scores are shown: one for the clean test data (A) and one for the noisy test data (B); for the latter, the scores are averaged over the six types of noise. As indicated in Fig. 4, for the clean speech test data, many of the conventional algorithms produced higher PESQ scores than LMS algorithm. This is because the LMS algorithm reconstructed the speech using different speech data from the corpus. However, for the noisy speech test data, the LMS algorithm performed rather better than all the other algorithms. Fig. 4 also shows the PESQ scores for the reconstructed speech from the test data with channel distortion (C) and with combined noise and channel distortion (D), obtained by the LMS algorithm compared to the PESQ scores for the unprocessed data. Further evaluation of the LMS-based speech enhancement performance was conducted using the objective measure segmental SNR, with the results presented in Table 5and Fig. 5. Table 5 shows the detailed segmental SNR ratings obtained by the LMS algorithm for all the test conditions and Fig. 5 shows the comparison of the average segmental SNR ratings between the LMS algorithm and other conventional enhancement algorithms. Based on the comparisons, we can draw similar conclusions for the LMS algorithm in comparison to other conventional algorithms.As mentioned earlier, the LMS enhancement system has the option to use the phase spectra of the matched corpus speech data to reconstruct the underlying speech waveform. This contributed to the better PESQ and segmental SNR scores for the LMS-based reconstruction, for dealing with the test data with both noise and channel distortion, in comparison to the reconstruction with the noisy measurements phase spectra. For example, to reconstruct the speech based on the measurements with both noise and channel distortion, the use of the matched corpus speech phase spectra resulted in an average PESQ score 2.8, as shown in Fig. 4 Condition D. However, the reconstruction with the noisy measurements phase spectra only produced an average PESQ score 2.0, which is lower than the average PESQ score 2.2 for the unprocessed noisy speech. Similar observations were also obtained for the segmental SNR measure. For the measurements with both noise and channel distortion, the reconstruction with the matched corpus speech phase spectra resulted in an average segmental SNR of about 0.7 dB, as shown in Fig. 5 Condition D; but the reconstruction with the noisy measurements phase spectra only produced an average segmental SNR −3.9dB.Fig. 6shows the histograms of the length of the longest matched segments found by the LMS algorithm as a function of the iteration index, with a total of six iterations performed for each test utterance with both noise and channel distortion (test condition D). In the initial iteration, the matched segments found are short because of the high noise level and potentially nonstationary noise content in the raw measurements, such that the measurement segments that can assume stationary noise could be short. The subsequent iterations each dealt with the residual noise from the previous estimation and compensation. We assume that the residual noise would have a reduced level than the initial noise to model, and that after the compensation for a nonstationary noise estimate (10), the residual noise could be more accurately modeled by a piecewise stationary noise model. Indeed, the algorithm was converging with the iteration by finding longer matched segments between the noisy measurements and the compensated corpus model, as shown in Fig. 6. For our test data, the histogram became largely stable after four iterations. In our experiments, we stopped the iterations for each test utterance if successive iterations produced longest matched segment estimates with similar average lengths. The mean length of the longest matched segments found after six iterations over all the test utterances is about nineteen frames.Finally, we returned to the speech recognition experiments. We compared the proposed LMS algorithm with the other conventional speech enhancement algorithms as a preprocessor for generating enhanced speech features from noisy signals for speech recognition. As above, we conducted the experiments on the Aurora 4 data without channel distortion, and in the experiments we optimized the word insertion/deletion penalties for each individual enhancement algorithm. Table 6shows the results. While the conventional enhancement algorithms produced significant improvement in the SNR (Fig. 5), they offered rather limited improvement in the recognition accuracy. A reason for this is the lack of joint optimization between the enhancement and recognition tasks, which creates the chance of mismatch between the training and test data for speech recognition.

@&#CONCLUSIONS@&#
This paper has focused on the modeling of the time variation differences between speech, noise and channel for speech estimation. We described a novel corpus-based, iterative LMS approach for extracting speech signals from slowly varying noise and channel distortion. The corpus speech signal segments provide examples for the time-varying speech signals to be estimated; finding the longest matched noisy segments, subject to the constraint of stationary noise and invariant channel effect in the segments (i.e., a model of the slowly varying noise and channel distortion), could lead to an estimate of the matched corpus speech segments with the least uncertainty. To further improve the estimation accuracy, the new approach uses iterations between the LMS-based estimation, and the estimation-based corpus model updating, to improve the modeling accuracy for the noise and channel distortion and thereby to derive an improved speech estimate. The new approach was evaluated on the Aurora 4 database for both speech recognition and speech enhancement experiments, with test data with combined additive noise and channel distortion. The use of our enhancement approach as a preprocessor for feature extraction significantly improved the performance of a baseline recognition system for dealing with noisy speech with additive noise, channel distortion, and their combination. In another comparison against conventional enhancement algorithms, both the PESQ and the segmental SNR ratings of the LMS algorithm were superior to the other methods for noisy speech enhancement.