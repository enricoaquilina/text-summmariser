@&#MAIN-TITLE@&#
The SEXTANT software: A tool for automating the comparative analysis of mental models of dynamic systems

@&#HIGHLIGHTS@&#
Most management situations are dynamic; and manager’s decisions are framed by their mental models.Mental models of dynamic systems (MMDS) have three levels of representation: elements, loops and the whole model.Distance ratios at each level reveal differences both between subjects and over time.SEXTANT calculates the distance ratios for elements, loops and the whole model for large samples of MMDS.SEXTANT enables researchers to strongly reduce the efforts to apply MMDS research.

@&#KEYPHRASES@&#
Education,Decision analysis,Problem structuring,Psychology of decision,Strategic planning,

@&#ABSTRACT@&#
The comparison of mental models of dynamic systems improves our understanding of how people comprehend, interpret, and subsequently influence dynamic management tasks. Approaches to comparing mental models currently used in managerial and organizational cognition research, especially the distance-ratio and the closeness-approaches, have been criticized for not considering essential characteristics of dynamic managerial situations. This paper builds on a recent analysis method developed to compare mental models of dynamics systems, and introduces this mathematical approach to management and organizational researchers by means of the SEXTANT software. It presents the process of mental model elicitation, analysis, comparison, and interpretation. An example with four elicited mental models illustrates the software’s features to analyze and present the results. Then, the software is compared with existing software to map and compare mental models. Our conclusion is that SEXTANT marks a significant step in enabling large-scale studies about mental models of dynamic systems.

@&#INTRODUCTION@&#
Managers have to make purposeful decisions and implement actions in dynamic environments. Scholars in the field of managerial and organizational cognition (MOC) have studied how managers make sense of influence, causality, and systems’ dynamics in such environments (e.g., Eden, Ackermann, & Cropper, 1992; Hodgkinson, 2002; Huff, 1990). Perception and the mental process resulting in decisions are framed by mental models—an internal representation of conceptual and causal interrelations among elements that people use to understand phenomena (Groesser, 2012). In general, such models are represented by some form of conceptual diagram or cause map.Cause mapping is an approach to representing managers’ mental models about a decision situation which has been used, for instance, to study the formation of beliefs (Markoczy, 1997), strategic change (Hodgkinson & Healey, 2008; Markoczy, 2000, 2001), top management team decisions (Ensley & Pearce, 2001), and team performance (Ellis, 2006; Mohammed & Dumville, 2001; Osborne, Stubbart, & Ramaprasad, 2001). Research about managerial and organizational cognition has also shown that more accurate mental models can lead to more successful and more sustainable decisions, especially since a dynamic approach to decision making has the potential to make significant contributions to how managers adopt strategies and how well they perform (e.g., Gary & Wood, 2011; Kunc & Morecroft, 2010; Paich & Sterman, 1993; Sterman, 2010).One of the latest developments in managerial and organizational cognition is the notion of mental models of dynamic systems (MMDS). Besides other common approaches to representing knowledge (e.g., Bougon, 1992; Brown, 1992; Clarke & Mackaness, 2001; Hodgkinson, 2002; Huff, 1990; Yukso & Goldstein, 2004), MMDS are useful for conceptualizing how humans think, reason, make sense, and learn about systems’ dynamics, that is, systems that change in response to decisions taken to influence them (Doyle, 1997; Doyle & Ford, 1999; Groesser, 2012; Groesser & Schaffernicht, 2012).Once MMDS have been elicited, they can be compared. There are two purposes for MMDS comparison. First, pre- and post-comparison, i.e., differences between MMDS of individuals at several instances of time, allows one to estimate the learning of individuals over time (Schaffernicht, 2006; Sterman, 2010). And second, computing differences between several individual MMDS at the same point in time, for instance between novices and experts, allows one to assess the current level of individuals’ comprehension of dynamic situations as compared to a reference model which typically represents the understanding of an expert.Groesser and Schaffernicht (2012) operationalize MMDS on three levels: elements, which contain variables and causal links, then individual feedback loops, and finally the complete model. This operational definition allows comparing MMDS to gain insights about the degree of similarity of such models. With this, research about MMDS can advance to levels which research about mental model of static systems has already achieved (Fiol & Huff, 1992; Hodgkinson & Healey, 2008; Tyler & Gnyawali, 2009). Since MMDS specifically address dynamic situations by means of conceptual components which traditional mental models do not take into account, a specific method for comparison has been developed by Schaffernicht and Groesser (2011). They define the levels of comparison as the Element Distance Ratio (EDR), a set of Loop Distance Ratios (LDRs), and a Model Distance Ratio (MDR). The measure of the dissimilarity of two MMDS is the distance between both. Existing mental model approaches and software tools do not operationalize the MMDS approach.2Please note that a MMDS is not a system dynamics simulation model; the latter are quantified models, where the causal links are represented by difference equations. A MMDS is a qualitative model built of components which are analogous to quantitative models of dynamic systems. However, the software tools used for developing system dynamics simulation models cannot analyze MMDS.2The paper advances MMDS research by introducing and applying a sequence of steps to be carried out in MMDS comparison along with the SEXTANT software tool—a set of algorithms which automates most of these steps. This constitutes a progress, enabling researchers to undertake large-scale studies by automating the extensive calculations required by such comparisons of MMDS. Large sample sizes quickly exceed the research resources available, with the result of stagnating research in this area. In addition, statistical inferences can be made regarding patterns of global model configuration. The name “SEXTANT” refers to the purpose of the tool: it should help to analyse characteristics from many articulated MMDS, just as the old orientation device helped seamen to navigate across wide oceans. With SEXTANT, we respond to a call to advance MOC-research beyond small-scale, inductive studies (Hodgkinson, 2002; Huff, 1997; Walsh, 1995).The paper is organized as follows: Section 2 introduces mental models and especially MMDS. Section 3 explains the process of comparing, analysing, and interpreting MMDSs. In addition, it presents the outlines of selected algorithms in the SEXTANT tool. Section 4 compares the SEXTANT software to other existing software tools. Section 5 discusses the benefits of our software for research and details its limitations. Statements about directions for future work conclude the article.The term “mental model” was first used by Kenneth Craik (1943) and generally refers to mental representation of causal factors and how they relate to one another. Research about mental representations in the social sciences often has involved human decision-making in dynamic systems. A dynamic system is one that reacts both to interventions of decision-makers and other influences (Edwards, 1962). Mental models as a specific scientific construct originated in psychological research (Johnson-Laird, 1983, 1999, 2001). Multiple cause mapping approaches exist to elicit mental models. Most of them account for variables and causal links (Eden, Ackermann, & Cropper, 1992; Laukkanen, 1994; Markóczy & Goldberg, 1995; Mohammed, Ferzandi, & Hamilton, 2010; Mohammed, Klimoski, & Rentsch, 2000).Research has shown that inaccurate mental representations of external dynamic systems can result in fatal consequences (Ellis, 2006; Salge & Milling, 2006; Sterman, 2008; Vaughan, 1990). Even more, evidence suggests that more accurate mental representations and causal attributions of such situations contribute to improved performance (e.g., Gary & Wood, 2011; Hodgkinson, 2002; Kunc, 2012; Osborne et al., 2001). In addition, the emphasis on system understanding, action, and perceived interrelationships between a challenging situation, its causes, and possible interventions as well as their consequences qualifies causal mapping approaches not only for rigorous research methods, but also as a basis for interventions in practical applications (Eden & Ackermann, 1998a; Eden & Huxham, 1996).In dynamic situations or for dynamic tasks, the requirements of representing the system’s causal structure and of reasoning about the system’s resulting behaviour are different from those in static situations. Existing studies in managerial and organisational cognition neither account for the causal attributions that individuals assign to elements in a dynamic situation, nor do they address the accuracy of the individuals’ articulated mental representations when compared to reference representations of an external dynamic system (Gonzalez, 2003; Gonzalez, Vanyukov, & Martin, 2005). The concept of MMDS intends to overcome this shortcoming of the previous approaches. It is well suited to representing human knowledge when dealing with dynamic systems (Doyle, 1997; Groesser & Schaffernicht, 2012). The approach of mental models for dynamic systems accounts not only for the description level of variables and causal links, but also for feedback loops, i.e., closed causal chains of elements and links, and the interaction of several feedback loops on the level of the complete model. This richness implies more complex, multi-level mental representations. Groesser and Schaffernicht define a MMDS as an “enduring and accessible, but limited, internal representation of an external dynamic system. The internal representation is analogous to the external system and contains, on a conceptual level, reinforcing and balancing feedback loops that consist of causally linked stocks, flows, and intermediary variables. The causal links are either positive or negative, are either linear or non-linear, and can be delayed.” (Groesser & Schaffernicht, 2012: 113).Unfortunately, however, the potential of these developments is presently being undermined by software limitations, even though several software products have been developed over the last two decades. For example, Clarkson and Hodgkinson (2005) introduced the Cognitizer software for cause mapping and map analysis. Pirnay-Dumer (2006) has created MITOCAR (“Model Inspection Trace of Concepts and Relations”) with an elicitation based on discourse analysis. Kopainsky, Pirnay-Dummerb, and Alessic (2012) assess individuals’ understanding of dynamic systems using MITOCAR, with a focus on concepts and relations. Decision Explorer has been developed for mapping individual and team mental models (Eden, 1988; Eden et al., 1992). However, as will be demonstrated later, neither of these dedicated programs is conceptually sufficient to compare MMDS: they refer to similarities or to differences between mental models where only variables and causal links are taken into account. The review shows that a significant gap exists for software capable of automatically comparing MMDS at three different levels, as has been introduced by Schaffernicht and Groesser (2011). The SEXANT software has been developed to address this gap. The next section illustrates essential aspects of the software.Earlier in this article, we identified several areas of application within the field of managerial and organizational cognition that would benefit from the application of MMDS elicitation and comparison in large scale studies. Previous investigations into organizational and business-related topics have demonstrated the potential of mental model approaches to further our understanding of organizational learning and the interplay between mental models, resource management, and company success (Gary & Wood, 2011; Kunc & Morecroft, 2010). The boom and bust phenomenon in business and economics has achieved great attention (Arquitt, Honggang, & Johnstone, 2005; Ford, 2002; Gary, Dosi, & Lovallo, 2008; Oliva, Sterman, & Giese, 2003; Paich & Sterman, 1993; Sterman, Henderson, Beinhocker, & Newman, 2007) due to its importance for decision making. But such studies, thus far, have not used cognitive mapping techniques to directly investigate the mental representations underpinning this situation.Our illustrative study is situated in the context of managing the growth of a company by executives (Forrester, 1975; Morecroft, 2007). To simplify the presentation, we use a sample of four exemplary MMDS to highlight differences and similarities at the level of variables and links, loops, and the complete models. These four MMDSs of individuals are compared to an expert model—our reference model. The overall question is: How similar are the individual MMDSs as compared to the expert MMDS? In the following, we show the complete process of eliciting, analysing, and comparing MMDS; while some of these tasks involve human judgment, the SEXTANT software automates the transformation of raw input data and calculates the respective distance ratios and some supporting indicators. A few screenshots from SEXTANT show the most relevant outputs. The high-level algorithms are explained and presented in pseudo-code; the paper’s Online Appendix provides the full documentation and software output. Our objective is to enable interested researchers to directly use the approach for their own research.We define the following terms so as to supply a basis for the remainder of the paper:•MMDS: articulated external representation of an individual’s internal mental model of a dynamic system. Note that even if the internal MMDS is accessible to the individual’s conscious mind, any articulated MMDS can only be a representation of an internal MMDS. To ease understanding, we use the term “MMDS” for articulated MMDS.Variable: attribute of an object with varying values at different time points. In a MMDS, a variable is usually represented by a noun (e.g., revenue which is measured in USD).Link: causal relation between two variables. The link includes the direction of the causal influence, its polarity, which can be either positive or negative. In addition, a link might include the notion of a delay between cause and effect (Sterman, 2000).(Feedback-)loop: this is a circular chain of links. Each feedback loop has a polarity and degree of delay according to its constituting links.Loop set: a set of loops such that each link which belongs to one or several loops is included at least once.Participant’s MMDS: this is a MMDS which has been elicited from an individual. We refer to them as MMD1, MMDS2, etc.Reference model: a reference model is an expert model to which each individual MMDS is compared. In this paper, MMDS0 indicates the reference model.Mental models of dynamic systems which have been articulated by individuals are represented as causal loop diagrams (CLD) which contain variables, causal links, link polarities, and feedback loops as well as the polarities of feedback loops (Groesser & Schaffernicht, 2012; Sterman, 2000). A CLD can be analysed with respect to two dimensions: its content or its structure. Content measures reflect domain-specific variables that an individual or a group perceives as being relevant to a particular domain of interest, and how these variables are thought to be related to one another (Langfield-Smith & Wirth, 1992). Structural measures reflect the complexity of a CLD and thereby provide a useful basis for testing hypotheses concerning differences between individuals or changes in an individual’s belief systems over time. Even though we focus especially on the content dimension, the SEXTANT software also provides several structural indicators.Within the past two decades, a number of researchers have sought to capitalize on the strengths of ideographic and nomothetic elicitation procedures, while reducing their associated weaknesses, through the development and use of hybrid techniques (Eden, 1990, 1992; Hodgkinson, Maule, & Bown, 2004). To elicit variables, Markóczy and Goldberg (1995) performed two steps. First, they used an adjusted Delphi process to construct a reference list of variables. Then, they conducted semi-structured interviews, where the participants had to select a subset of personally salient variables to be mapped from this list. Thereafter, causal links were elicited by comparing pairs of variables. We use a different method to enable individuals to express their specific MMDS. We present the individuals with the decision situation in which they have to make decisions. The elicitation situation is facilitated by a researcher. The interviews are recorded and transcribed. Then the MMDS are constructed from these interview transcripts. In case the individuals have experience with a mapping methodology, they can articulate their reasoning by means of a diagram or a combination of text and diagram.Researchers’ guidelines for good practice state that participants should be presented with the opportunity to validate their completed MMDS. Huff and Fletcher (1990), for instance, advocated that such opportunities should be provided in cases in which indirect elicitation procedures have been used to construct the maps. Others, however, in an attempt to ensure that participants do not try to rationalize their maps post hoc, avoid all procedures that permit participants to review their responses as the data collection process unfolds. We follow Sparrow (1998), who states that reliability problems can arise when participants are allowed to validate their resulting model. Fig. 1shows the four MMDSs of our illustrative case study. They are represented by the Vensim© software. This software is used often in management research (Goh, Love, Brown, & Spickett, 2012; Jay, 2013; Vancouver, Putka, & Scherbaum, 2005; Vancouver, Tamanini, & Yoder, 2008; Vancouver & Weinhardt, 2012). We use the graphical representations directly as input for our SEXTANT software which is based on Mathematica©.3We assume that the reader is familiar with standard procedures in Mathematica, such as file operations. The Mathematica manual provides further details.3The content of the reference model can be described as follows: a company strives to expand its customers’ demand; since demand leads to revenues, growing demand enables a larger amount of demand-stimulating actions (carried out by, e.g., increased sales staff, additional advertisements), which influences buyers and in turn produces new additional demand: this is a first feedback loops, and its polarity is positive. At the same time, a rise in demand with a given service capacity will diminish the company’s performance; the buyers will not mind instantly, but perception of the reduced performance will become stronger over time (there is a delay), and then the perceived sinking performance will cause demand to sink. Again, this is a loop, but this time it has negative polarity because an initial rise in demand yields a successive reduction.The company itself also will progressively perceive the performance lack (another delay), since as the perceived performance falls short of the performance standard, the perceived need to invest will rise. The ensuing additional investments will increase capacity and reverse the performance decrease; therefore this is also a negative feedback loop. However, the descending performance will also slowly reduce the performance standard (again a delay), thus decreasing the perceived need to invest; this fourth and positive (reinforcing) feedback loop thus hinders the third one in its attempts at balancing capacity with demand.Variables referring to the same content have to be spelled in the same way. SEXTANT compares the names of the variables of each MMDS to each other and to MMDS0 to check for synonyms. In the case example, MMDS0 contains several variables which can be expressed by synonyms. For instance, names such as firm, enterprise, or company indicate the same underlying meaning in our context. In such cases, we apply the following procedure to unify the set of variable names (Table 1):The reader might think that this is an obvious matter; however, in our experience, it is a tedious and error-prone process with exponentially increasing cognitive load for an increasing number of MMDS to compare. As long as there is no software to read in different models and automatically generate a variables thesaurus that has spell-checking, one practical way to proceed is to use the import routine of SEXTANT and inspect the variable list for repetition of individual variables.4Execute the Mathematica© command FileNameSetter[Dynamic[modelDirectory], “Directory”] allowing one to navigate to the target directory and then call the SEXTANT routine importElements[modelDirectory]. Table 5 stores each model’s variables list and can be inspected by the command MatrixForm[V]. VariablesList contains the combination of all variable lists. Writing mistakes are easy to detect by visual inspection of these two data series.4As part of the unification procedure, a copy is made of each of the MMDS with at least one synonym. In the copy, the synonym is replaced by the common label for the variable. Thus, the same label for a variable is used across all MMDSs; the original representations are saved in a backup folder.5Depending on the number of MMDS to be analyzed, the preparation to arrive at a coherent set of variable names and assure coherent spelling is time-intensive. In our experience, processing 30 pairs of MMDS in a pre- and post-test design will take between 6 and 8hours.5The researcher assigns identification numbers to the feedback loops which have been recognized by the participants. The numbers are tracked in an Excel spreadsheet (as illustrated in Fig. 2). For each recognized loop, the researcher has to note the polarity, number of links, and number of delays. A polarity can either be “1” for positive polarity or “−1” for negative polarity. Additionally, the researcher records the list of variables which constitute each loop, and the number of delayed links.For instance, loop #1 is of positive polarity (“1”), and has two links and no delays, whereas loop #3 is of negative polarity (“−1”), and has four links, two of which are delayed (Fig. 3).Since each of the MMDS will most likely have a different set of recognized feedback loops, there is an important question for the computation of the Loop Distance Ratio: Which loops are to be compared? Answering this question requires the researcher’s judgment for two reasons: First, the variables which form loops can be different in their wording, but are actually equivalent in their semantic content. Ackermann and Eden (2011) deal with the possibility of using the links of a variable to identify its meaning despite the appearance of different variable names (p. 306). And second, different individuals may deal with and articulate at different levels of aggregation. Eden (2004) describes that two paths from a variable A to a variable C – one direct, the other one via variable B – have the same causal implications and therefore can be summarized as A→C. Schaffernicht and Groesser (2013) come to the same conclusion using a different reasoning: Two elements of a MMDS which only differ in their level of detail should not be interpreted as completely different because the more aggregated element does not include all the variables which the more detailed element includes.This double challenge prevents automatic matching of the loops. To solve this challenge, the researcher records the identifiers of semantically equivalent feedback loops on the spreadsheet. Fig. 3 shows how two models are compared using MMDS0 and MMDS2. At the first glance, these MMDSs appear to be rather different—varying in degree of detail and in wording, for instance, action becomes instantiated as performance, revenues, and advertisements. However, the feedback loops actually refer to the same semantic meanings. Loop 1 refers to how the company expands demand; loop 2 represents the buyers’ reaction to the perceived delivery delay (performance of the company). Loop 3 describes how the company adjusts its capacity to ameliorate performance. Eventually, loop 4 articulates how the internal performance standards adapt and influence investment behaviour. Therefore, we can match the individual feedback loops.The attribution of semantic equivalence thus requires an act of insight into what each loop refers to. The human judgment involved in such attribution may appear to be of little influence in our example; however, some application cases may be more challenging. The Appendix “judging semantic equivalence of feedback loops” discusses a more complex case to illustrate some details of the process.In terms of content, a MMDS can be represented by its adjacency matrix. This matrix contains the links of a MMDS with direction, polarity, and – where appropriate – with a delay. Table 2shows the adjacency matrix for MMDS0. In the case of this MMDS, for instance, action and demand are linked by “1” indicating a link with a positive polarity without delay.6Transforming all non-null cells to “1” would allow one to compute the reachability matrix (Oliva, 2004) and apply other structure analysis procedures.6As another example, performance and perceived need to invest are linked by “−2” indicating a link with a negative polarity and a significant delay. Null cells, i.e., when variables are not connected, are grey. The adjacency matrix is the essential element which the SEXTANT software uses to perform the calculations.A variety of measures for the structural complexity have been advocated in the literature (Clarkson & Hodgkinson, 2005; Eden, 1992; Eden & Ackermann, 1998a; Markóczy & Goldberg, 1995). A wide selection of these measures is incorporated in the SEXTANT software, as follows:•the number of variables;the number of links;the total, mean, and standard deviation (SD) of link delay;the link-to-variable ratio (i.e., the proportion of links to variables within any given MMDS);the map density (i.e., the number of observed links divided by the total number of links theoretically possible, given the set of variables in the participants MMDS);the number of loops;the loop length (number of links in a loop);and the number of delays in the loop.Fig. 4shows the results for the measures of structural complexity for the four MMDS plus the reference model. SEXTANT produces this output by using the functiongeneralOutput:The following explanations and equations display three measures for comparing MMDSs which have been introduced by Schaffernicht and Groesser (2011). At the level of elements, the method examines variables and clausal links and then computes the Element Distance Ratio (EDR), which expresses the differences between two MMDS as the fraction of all possible differences between the two MMDS. Possible differences can result from divergent sets of variables, i.e., one MMDS may have a variable which is not part of the other one. But differences can also result from differences in the causal links, i.e., one MMDS may have a link between two variables which the other MMDS does not, or a link between two variables, which exists in both MMDS but differs in its polarity and/or the length of its delay. In Eq. (1), A and B are matrix representations of two MMDS, diff is the number of actual differences, the denominator is the number of possible differences according to the number of variables in A and B(vc), unique to A (vuA) or unique to B (vuB).Element Distance Ratio (EDR):(1)EDR(A,B)=∑i=1vars∑j=1varsdiff(i,j)4vc2+2(vc(vuA+vuB)+vuA2+vuB2)-2(vc+(vuA+vuB))Since we take into account two properties of each link (polarity and delay), for each variable there can be a maximum difference of “2”. However, one of the linked variables may be contained in only one of the compared MMDS, while the other variable may only belong to the other MMDS. Since there cannot be a link between such pairs of variables, it follows that there cannot be any difference.The second level of comparison deals with feedback loops. The description level of feedback loops is necessary to detect both changes in the capacity, to recognize feedback structures and to indicate if recognized feedback structures are different or similar to one another. Each pair m and n of loops the researcher has classified as semantically equivalent is compared. Loops may differ in their polarity, in the length of their delay, and/or in their variables and causal links. We compute the Loop Distance Ratio (LDR) which indicates the fraction of dissimilarity of two feedback loops (Eq. (2)).Loop Distance Ratio (LDR):(2)LDR(m,n)=η∗ldd(m,n)+ι∗lpod(m,n)+κ∗LEDR(m,n)This ratio is calculated for each pair of loops for the two MMDS which are compared. The “Loop Delay Distance” function ldd(m, n) yields “0” if both loops are equal in delays; otherwise, if there are less than three differences it will return “0.5” and “1”. The function lpold(m, n) returns the Loop Polarity Distance: if loops m and n have the same polarity, then lpold(m, n) = “0”, and “1” indicates a different polarity. LEDR refers to the Loop Element Distance Ratio between the two compared loops. The weights of the different terms are given by η, ι, κ; with η+ι+κ=“1”. The parameters allow for different weightings. We use equal weights since, based on our evaluations, any reasonable weight combination which sums up to “1” leads to similar LDRs.On the third level we compare complete models. The Model Distance Ratio (MDR) is the weighted average of all of the LDRs between the two MMDS (Eq. (3)):Model Distance Ratio (MDR):(3)MDR(A,B)=∑l=1nLDR(m,n)lnIn the following, we present only the general process of the software and an overview of the algorithms and data structures of the SEXTANT software, enabling the reader to replicate the steps. We apply a typographical convention: pseudo-code, variables, and their values will be printed in a sans serif type font, and global variables in italics; Mathematica’s integrated functions will have names with capitalized initial letters, while our own function and procedure names will appear in bold letters. Parentheses are used for function arguments and square brackets for array elements.7Most important is that we present here the steps necessary for modellers to replicate the analysis. The online appendix offers a more detailed presentation of the work process and the software itself. For interested readers, we also provide the original files.7After preparing the Vensim and Excel files as described above and storing them in one folder, SEXTANT imports the data and executes the computations, starting with storing the number of MMDSs (including the reference model) in modsCount. A main procedure iterates through each MMDS:doEDRsis a program that iterates through the models from MMDS1 to modsCount, calling the functiondetEDRin each of the iterations; since MMDS0 is the reference model, it is not compared to itself. It creates EDRs as a table with nulls in (modsCount-1) rows. Then, a procedure nameddetEDRis called for each of the MMDSs, passing the current MMDS’s row number as parameter to the procedure. In each iteration, MMDS0 is called A and the compared MMDS is called B.The routinedetEDR(outlined in Table 3) computes basic parameters which are required for computing the EDR between MMDS0 and any given MMDS, called “thisModel”:The “difference matrix” DM is the data structure used by the diff(i,j) function mentioned in Eq. (1) to store all actual differences between two MMDSs. Based upon the previous procedures, each comparison of MMDS at the element level generates the required matrices by extracting the respective planes from AdjMs:A= AdjMs[All,All,1] – the vXv extended adjacence matrix representing the reference modelB=AdjMs[All,All,thisModel] – the vXv extended adjacence matrix representing the compared MMDSThereafter, the two specialized sub-procedures are called to complete the computation.evalDiffABis responsible for setting the correct difference values in DM, according to the comparison of the two compared matrices A (MMDS0) and B (compared MMDS). The proceduresumDiffsimplements the ΣΣ part of Eq. (1): it sums up DM’s cell values and yields the total number of actual differences between the MMDS0 and the compared MMDS. The resulting EDRs are displayed in Table 4:Next is the computation of the LDRs and the MDRs. Since the logic has been explained in detail (Schaffernicht & Groesser, 2011), we show the results of the computations as screenshots (Table 5). As Eq. (2) indicates, the LDR depends on three parts: the lpold, Loop POLarity Distance, the ldel, Loop DELay distance, and the ledr, Loop Element Distance Ratio. In SEXTANT, detPOLD assigns a “1”, i.e., “100% different,” if, for a given feedback loop, there is no equivalent loop in the other model, or if the polarity of the equivalent feedback loop is the opposite to the polarity of reference feedback loop. Otherwise the software assigns a “0”, i.e., indicates that both feedback loops are “identical”.ThedetLDELfunction assigns “0” if the delay of two feedback loops are the same, “1” if the number of delayed links differs more than “2” between the feedback loops, and “0.5” otherwise. Then,detLEDRcomputes the EDR for the pair of feedback loops, considering them as sub-models. Processing the four MMDSs yields the results displayed in Table 5.The first column in Table 5 indicates which MMDS is compared to the MMDS0. Then, the respective numbers of the compared loops are shown; a “0”-value indicates that the compared MMDS has no equivalent feedback loop for one of the feedback loops in MMDS0, e.g., MMDS4 has two “0”-values. The next three columns to the right contain the degree of difference for polarity, delays, and the Element Distance Ratio on the level of feedback loops. In case a MMDS does not have a loop which can be said to be equivalent to a given loop in MMDS0, then all three columns will have “100%” as the value for the maximum difference. The last column shows the Loop Distance Ratio which is the average of the adjacent three values.The MDR is the most comprehensive measure of distance between MMDSs, which is expressed by the mean of the LDRs for each MMDS. The results are shown in Table 6:This step concludes the procedures performed by the software.8The online appendix contains two copies of the notebook: one with only the function calls and the output, the other including also the function codes. The researcher has the standard output data from Mathematica. More detailed data can be examined using the SEXTANT functions as well as standard functions in Mathematica. Interested researches can obtain a copy of the SEXTANT software from the authors.8The three different distance ratios allow one to compare the MMDS in a compact and meaningful way. Each of the ratios can be compared on the individual level, i.e., one MMDS is compared to the reference model, or on the level of the complete sample. Let us first concentrate on individual models and compare them to the reference model. The first two EDRs in Table 4 (89% and 86% respectively) are high because most of the variables in MMDS1 and MMDS2 are largely different from MMDS0 (Fig. 1). The third EDR is low (1%) because MMDS3 has the same set of variables as MMDS0, but not all the causal links. MMDS4 is identical to the reference model at the level of model elements; hence, its EDR is “0”. When calculating overall MMDS, we arrive at a mean EDR of 44% with a standard deviation of 50%. While these numbers may seem to confirm what visual inspection of the CLDs already suggested, inspecting the LDRs shows that some aspects are not obvious.The LDRs (Table 5) enable additional interpretations. MMDS1 and MMDS2 have an equivalent feedback loop for each of the four loops of MMDS0. MMDS3 has only three loops and MMDS4 has only two which have counterparts in MMDS0. The loop polarities of the equivalent loops in the MMDS match the loop polarities of MMDS0; of course, the lack of an equivalent loop in any MMDS means that the respective polarity differences are 100%. Referring to loop delays, differences are to be found only in MMDS2, where loops 2, 3 and 4 do not have any delay.Comparing the loop tables (details are in the Online Appendix), one sees the fact that an individual did not identify one or two of the four loops has a substantial impact on the LDRs. The LDRs may have high values for a variety of reasons. For instance, MMDS1 and MMDS2 have different sets of variables, and therefore the Loop EDRs (see explanations on previous page and Table 5) must be high for these two MMDSs (86% for the first two MMDS). But they need not be low for the other MMDSs, because missing an equivalent loop also means a loop EDR of 100% (MMDS3 and MMDS4). Overall, even though MMDS1 and MMDS2 are distant from MMDS0 when one considers their elements, they are quite similar at the level of feedback loops. Conversely, MMDS3 and MMDS4 are similar to MMDS0 at the level of elements, but distant at the level of feedback loops. Based on the analysis of distances between the feedback loops, we conclude that these MMDSs are mostly similar. This deemphasizes the importance of differences at the level of the elements.MDRs (Table 6) are the global measures of differences in MMDS. They compress the different attributes which belong to the LDR and balance the relevance of the element level and the loops level. MMDS1 and MMDS2 have high EDRs, moderate but unequal LDR values, with high Loop EDR components, low differences in polarity, and some delay differences for MMDS2. MMDS3 is almost identical to MMDS0, but lacks one of the loops (i.e., 25% of the loops in MMDS0) which lead to a MDR that is quite similar to those of MMDS1 and MMDS2. MMDS4 is identical to MMDS0 at the level of elements. In addition, the two recognized loops in MMDS4 have the same polarity and delay as the corresponding loops in MMDS0. However, since two loops are missing in MMDS4, the MDR is 50%. When the MDR is calculated overall MMDSs, we arrive at a mean of 33% with a standard deviation of 14%.When comparing EDRs and the respective MDRs for each MMDS in our sample (Table 7), it appears that using EDR to measure the distance between MMDSs would result in a different conclusion than using MDR. In the former case, one would conclude that MMDS1 and MMDS2 are largely dissimilar to MMDS0, whereas MMDS3 and MMDS4 are almost identical to MMDS0. For the latter, the MDRs indicate that these four MMDSs are relatively similar to the MMDS0; also, there are no substantive MDR differences between them.The average values of the four comparisons are similar for the EDRs (44%) and the MDRs (39%), which is in part the consequence of the compensation between MMDSs 1 and 2 (extremely high values) and the other two MMDS (low values). Accordingly, the standard deviation for the EDRs is higher than for the MDRs. The standard deviation of the EDRs suggests a high degree of heterogeneity amongst MMDSs, which cannot be found at the level of the complete model as indicated by the MDRs.The implications of the differences between the distances indicated at the three different levels reach beyond the quantification details. If one accepts the proposition that logically closed causal loops are at the basis of steering social systems (Forrester, 1968), then comparing mental models only on the level of elements may lead to insufficient yet well calculated data with only low levels of validity. It would be erroneous to consider MMDS1 and MMDS2 as largely different from MMDS0. However, comparing these mental models only on the level of feedback loops would also result in debateable conclusions, since for all their similarities indicated by the LDRs, the differences at the element level do mean that individuals think about the situation in quite different terms and degrees of detail. The MDR level suggests a homogeneity amongst MMDSs which does not allow one to see that they are different for different reasons: the level is insufficiently rich to support an adequate conclusion. Only by using all three levels of description can one inspect a sample of MMDSs and discover the characteristics that are meaningful and relevant from a feedback-system perspective.At a more practical level, the use of the SEXTANT software has two obvious advantages: speed and reliability. While the exemplary case was deliberately short, SEXTANT has been used for a study to compare 33 MMDS (Schaffernicht & Groesser, 2012). In such cases, the advantages of automatic execution, i.e., speed and reliability of repetition, become highly valuable.As was mentioned in the introduction, researchers have developed several software tools to support the elicitation and comparison of mental models. After explaining the principal features of SEXTANT by means of an illustrative example, in this section we briefly compare four well-known tools which deal with mental models to our SEXTANT software.9For the sake of brevity, we limit our discussion to the tools directly related to mental models. For instance, we exclude the structural analysis of models of dynamic systems Oliva, R. 2004. Model Structure Analysis Through Graph Theory: Partition Heuristics and Feedback Structure Decomposition.System Dynamics Review, 20(4): 313–336.9The objective is to summarize the main strengths and weaknesses of our software vis-à-vis these alternatives. We pay particular attention to the ability of these systems to compare MMDS and to the availability of the software for researchers in order to spread its application. We first briefly review MITOCAR, moving on to Decision Explorer, Ulysse, and Cognitizer. An overall summary of these comparisons is presented in Table 8.Pirnay-Dumer (2006) has developed MITOCAR (which stands for “Model Inspection Trace of Concepts and Relations”) with an elicitation based on discourse instead of diagrams, such as cause maps; discourse is preferred over diagrams because discourse is humans’ natural way to articulate ideas; by avoiding diagrams, one intermediate level of representation is avoided. It automates discourse analysis and generates association networks which represent the relevant concepts of a mental model and the connections between them. The advantage of discourse is that it is the intuitive way for humans to articulate ideas, and therefore the elicitation challenges induced by diagrams might be reduced. However, in the resulting networks, associations contain less information than causal links; specifically they neither have a direction nor a polarity. The networks also lack the concept of feedback loops, and thus the tool is not conceptually rich enough to allow comparing MMDS which incorporate such loops.Extensive research has been undertaken with regards to cognitive mapping (Eden, 1988, 1992). Cognitive mapping is a diagramming approach substantiated by personal construct theory (Kelly, 1955). Simply stated, a “construct” is a brief phrase containing an object or attribute and a verb. Eden and Ackermann (1998b) have developed a specific method for the analysis of strategic options which was operationalized by the software Decision Explorer (www.banxia.com/dexplore). By using a diagram language, users create a cause map which consists of nodes, i.e., constructs, and causal links between them. The software offers a set of structure-related analysis functions for identifying groups of closely related constructs, finding influential constructs and constructs with many links, depicting the hierarchy among the constructs, and exploring feedback loops.As far as the comparison of MMDS is concerned, the Decision Explorer software has several differences which result from differences between the method of cognitive mapping and system dynamics. First, the approach is used to elicit constructs, not variables. A construct is an action-oriented short phrase (Ackermann, Eden, & Brown, 2005). It contains an action word which implies an observed, desired, or feared behaviour of an object or a property. Searching to articulate and organize constructs has a series of advantages, which have also been recognized by system dynamicists (Howick, Ackermann, & Andersen, 2006). It is in general possible to extract variables from such constructs by searching for nouns. This would take a considerable amount of interpretation, analysis, and judgment by the analyst, as described by Eden (2004), but there is no general incommensurability. And second, comparisons of MMDS are not possible; the software does not support the calculation of distance ratios at the three levels (Groesser & Schaffernicht, 2012). As with ideographic methods more generally, the highly idiosyncratic nature of the data renders the systematic comparison of maps problematic, particularly where large numbers of maps are involved.Ulysse is a software tool for eliciting and analysing mental models (Desthieux, Joerin, & Lebreton, 2010). The software structures the elicitation process and the data manipulation to explore the individually and collectively articulated mental models. Ulysse uses several of the graph-theoretical algorithms, e.g., the adjacency matrix, the identification of hierarchical levels, and feedback loops, which have been developed by Oliva (2004). Ulysse uses these algorithms to support experts in exploring and understanding their own mental models with the objective of improving and consolidating the mental models of experts. This, however, raises key concerns in the context of larger-scale cross-sectional and longitudinal studies, where the purpose is to formally test statistical hypotheses. This purpose of action research to influence the current mental models is fundamentally different from the intention to elicit and compare MMDS while minimizing the researcher’s influence. Even though the largely different intentions—influence and improvement on the side of Ulysse and measurement and comparison on our side—can be useful at different stages of an inquiry, researchers interested in the measurement of MMDS are recommended to clearly distinguish this purpose from intervention (Grossler, 2004). In addition, the Ulysse-algorithms are not publicly documented and the software is not publicly available.Cognitizer (Clarkson & Hodgkinson, 2005) is software for cause mapping and map analysis (www.mandrake-technology.com). It supports elicitation and allows capturing cause maps as digraphs or directly as an adjacency matrix. The software contains a series of standard structure analysis functions such as number of constructs, number of links, link density, and mean link strength and its standard deviation are part of the software. In addition, the software supports the traditional distance-ratio methods (Langfield-Smith & Wirth, 1992; Markóczy & Goldberg, 1995), that is, at the elements level. Therefore, Cognitizer is the existing software that best satisfies our demand. However, as a general purpose tool it has not been conceived as a support for comparing MMDS, and therefore does not allow eliciting and comparing feedback loops.As can be seen from the summary in Table 8, none of the previously existing software can satisfy the methodological needs to compare MMDS using the three-level distance ratio approach (Schaffernicht & Groesser, 2011). SEXTANT fills this gap. Compared to all other approaches, only SEXTANT is available as a download for researchers to use and elaborate. By this means, open questions, challenges, new representational features, and analysis functions can be developed and integrated in the software. Because we utilize development software (Mathematica) with only minimal technical requirements for programming and which is available at affordable prices, there is a high chance that SEXTANT will be used across the different groups of researchers working in this field. In other words, SEXTANT enhances the possibility that studies may complement each other and that research about MMDS can prosper.

@&#CONCLUSIONS@&#
