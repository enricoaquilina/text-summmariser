@&#MAIN-TITLE@&#
Outranking under uncertainty using scenarios

@&#HIGHLIGHTS@&#
Describes a scenario-based outranking model for decisions under uncertainty.Relates the scenario model to existing outranking approaches.Conducts a simulation study to assess the accuracy of the scenario model.Finds that the scenario model can give reasonable accuracy in most cases.A critical aspect is good scenario construction covering possible futures.

@&#KEYPHRASES@&#
Decision analysis,Multi-criteria,Outranking,Uncertainty,Scenarios,

@&#ABSTRACT@&#
This paper considers the use of scenarios to treat uncertain attribute evaluations in the outranking methods. The scenario-based approach allows the decision maker to think deterministically about the problem by attaching causal links to a small number of potential outcomes, instead of using probability distributions. The scenario approach can be expressed as a simplified version of the comprehensive but practically complex “distributive” outranking method of d’Avignon and Vincke. Using a scenario approach has distinct practical advantages, but also presents the inherent danger that meaningful information is ignored. The extent of this danger is assessed using a simulation experiment, where it is found to be of a magnitude that is non-trivial but still potentially acceptable for certain decision contexts.

@&#INTRODUCTION@&#
Many decisions must be made in conditions where the consequences of some actions are unknown because they depend on future events. This is sometimes termed “external uncertainty” (e.g. Stewart, 2005, chap. 11) because it relates to uncertainty about environmental conditions lying beyond the control of the decision maker. Particularly in the realm of strategic decision problems, these uncertainties can be complex and interrelated, with the result that the elicitation of precise mathematical measures such as probabilities become operationally difficult for decision makers to comprehend, and for facilitators to validate. In light of these obstacles, an alternative approach is to construct a number of narratives that describe possible ways in which the future might unfold. Each of these possible futures is conventionally termed a ‘scenario’ and the use of scenarios for strategic planning known as ‘scenario planning’ (e.g. Van der Heijden, 1996; Wack, 1985a, 1985b).Advocates of scenario planning often prefer to avoid formal quantitative modeling (e.g. Schoemaker, 1995; Van der Heijden, 1996) and use informed but informal judgment – some examples can be found in Enserink (2000), Wollenberg, Edmunds, and Buck (2000), Cairns, Wright, Bradfield, van der Heijden, and Burt (2004). Nevertheless, efforts have been made to integrate decision analysis with the use of scenarios, as discussed for example in Goodwin and Wright (2009, chap.16) and Stewart (2005, chap. 11). The main objective of a scenario-based decision model is to use the philosophy of multi-criteria decision analysis (MCDA) to evaluate and compare the performances of alternatives in each scenario – given a decision problem, the approach considers that problem separately in each scenario before (possibly but not necessarily explicitly) combining this information to arrive at a final decision.The general integration of scenario planning and MCDA has been thoroughly described in a number of places (e.g. Durbach & Stewart, 2012; Stewart, 2005, chap. 11; Stewart, French, & Rios, 2013), and a number of practical applications (using value function methods) reported (Montibeller, Gummer, & Tumidei, 2006; Ram, Montibeller, & Morton, 2010; van der Pas, Walker, Marchau, Van Wee, & Agusdinata, 2010). The current paper is narrower in scope and focuses only on the use of scenarios in the outranking methods, with the specific aims of:1.Providing a formal description of the scenario-based outranking method;Showing how the scenario-based outranking model provides a natural simplification to the comprehensive but practically complex “distributive” outranking method of d’Avignon and Vincke (1988);Assessing the differences between results obtained using scenario-based and distributive methods, using a simulation experiment.A primary motivation for this paper is the simplification of the distributive outranking method of d’Avignon and Vincke, which despite its theoretically appeal has not been widely used (indeed I could find no reported applications in the literature). By ignoring some aspects of uncertainty while assigning to others additional qualitative information, the scenario-based model avoids using measures that are potentially difficult for decision makers to interpret and work with (for example, stochastic indices of preference). Of course ignoring information brings with it the risk of selecting demonstrably worse alternatives, the extent of which is evaluated using a simulation experiment.The remainder of the paper is structured as follows. In Section 2 notation is introduced and uncertainty modeling in the outranking methods reviewed. Section 3 describes the scenario-based outranking method and its relationship to the distributive method in d’Avignon and Vincke (1988). Section 4 clarifies the relationship with a brief numerical example. Sections 5 and 6 describe the simulation study and results respectively. A final section concludes the paper.Consider a decision problem consisting of I alternatives denoted by ai, i∈{1,…,I}, each evaluated on J criteria denoted by cj, j∈{1,…,J}. Let Zijbe the evaluation of aiin terms of criterion cj, according to some suitable performance measure. Our concern is with decision making situations in which the values of Zijfor each i are not known with certainty for all j, but are viewed as random variables with an associated multivariate probability distribution function Fiand probability density function fi. Let Fijand fijdenote the corresponding marginal cumulative distribution function and probability density function for criterion cjif alternative aiis selected.Although Fijand fijwill usually be continuous functions, the range of possible outcomes can be approximated to arbitrary accuracy by a large number of discrete “states” or realizations of the associated random variable. Let zij,mdenote realization rmof Zij, with m∈{1,…,Mij}. To explicitly differentiate between realizations/states and scenarios, performance in a particular scenario smis denoted byzij(m), with m∈{1,…,S}. The set of constructed scenarios is denotedS.Several outranking methods use stochastic dominance concepts to treat uncertain attribute evaluations. These categorize instances in which a pairwise comparison of the associated probability distributions is sufficient to confirm that one alternative is preferred to another (in the sense of maximizing expected utility) provided that certain constraints on the underlying utility function are satisfied. Well-known results (Bawa, 1975) show that the first-, second-, and third-degree stochastic dominance of aiover akon cjimplies that aiis preferred for, respectively: any increasing utility function on cj; any concave increasing utility function on cj; and any decreasingly risk averse, concave, increasing utility function on cj. Similar conditions have been provided for convex utility functions (Goovaerts, de Vylder, & Haezendonck, 1984).Zaras and Martel (1994) and Zaras (2001) use a simple weighted aggregation of indicator variables ζj(ai,ak) which equal 1 if aistochastically dominates akon criterion cjand are otherwise zero. This results in a concordance index as for Electre I. Martel, d’Avignon, and Couillard (1986) and Azondékon and Martel (1999) construct a preference index ζj(ai,ak) as a product of three functions each scaled between 0 and 1 that cause ζj(ai,ak) to decrease as dominance weakens from first- to third-degree. A similar threshold-based method is provided in Nowak (2004). Dominance-based methods have also been extended to make use of fuzzy numbers, and possibilistic and evidentiary evaluations (Amor, Jabeur, & Martel, 2007; Boujelben, Smet, Frikha, & Chabchoub, 2009; Zaras, 2004), allowing for different uncertainty formats to be included in the same decision problem.Other pairwise comparisons of probability distributions have been incorporated into stochastic outranking methods. Jacquet-Lagrèze (1977) allocates the part of fij(fkj) where there is a non-zero probability of ak(ai) occurring as evidence in support of indifference, and then uses the cumulative distributions to allocate the remaining probability mass as evidence either that alternative aiis preferred to ak, or vice versa. Aggregation proceeds as for Electre I. A second set of models (Dendrou, Dendrou, & Houstis, 1980; Fan, Liu, & Feng, 2010; Liu, Fan, & Zhang, 2011; Martel et al., 1986) construct a matrix Pjwhose entriesPikjdenote the probability that alternative aiis superior to alternative akon criterion cj. The models differ with respect to the subsequent exploitation of these probabilities. Dendrou et al. (1980) and Liu et al. (2011) aggregate thePikjusing a weighted sum over attributes. Martel and d’Avignon (1982), Martel et al. (1986) use much the same approach but incorporate indifference and preference thresholds. Fan et al. (2010) compute joint probabilities associated with each of 2Jpossible permutations of binary indicators denoting (attribute-specific) outranking between a pair of alternatives. Each of these is taken as evidence in favor of the ‘superiority’, ‘inferiority’, or ‘indifference’ of airelative to ak, based on a user-defined threshold.In all of the outranking models above the distributional aspect of the problem is fully absorbed into the problem at an early stage of the process through the definition of thePikjor stochastic dominance relations. In contrast, (d’Avignon & Vincke, 1988) use the uncertain attribute evaluations to form a stochastic (or “distributive”) outranking degree indicating the probability of attaining various degrees of outranking, rather than summarizing the stochastic evaluations directly asPikj.The first step involves defining a preference index Ij(zij,m,zkj,n) denoting the degree of preference for zij,mover zkj,n. This index is scaled between 0 and wj, where wjis the normalized weight attached to criterion cj. A random variable Hj(ai,ak) can then be defined over the possible values of Ij(zij,m,zkj,n), with the probability corresponding to each Hj(ai,ak) given by(1)Pr[Hj(ai,ak)=h]=∑{m,n:Ij(zij,m,zkj,n)=h}fij(zij,m)fkj(zkj,n)where fij(zij,m) denotes the probability of obtaining zij,mfor alternative aion criterion cj.The Hj(ai,ak) can be aggregated into a distributive outranking degree S(ai,ak) by addition over criteria a la Electre III or Promethee i.e.S(ai,ak)=∑j=1JHj(ai,ak). Further random variables indicating average ‘strengths’ S(ai) and ‘weaknesses’ W(ai) are formed using unweighted averages (of S(ai,ak) and S(ak,ai) respectively, ∀k≠i). The final exploitation of these measures to obtain a partial preference order is again complicated by the stochastic nature of the problem, with several suggestions (the simplest of which is to use the median of the distributions) proposed in d’Avignon and Vincke (1988).Another distributive approach is to use Monte Carlo simulation from probability distributions. This is the approach followed by the outranking variant of the stochastic multi-criteria acceptability analysis (SMAA) method, SMAA-3 (Hokkanen, Lahdelma, Miettinen, & Salminen, 1998). SMAA is a family of inverse-preference methods that provides information about the types of preferences, if any, that would lead to the selection of each alternative. The approach simulates a large number of realizations from probability distributions governing (a) the uncertain attribute evaluations, and (b) parameters of the outranking model, and records the proportion and distinguishing features of those weights which result in each alternative obtaining a particular rank (often the “best” rank). A recent review is provided in Lahdelma and Salminen (2012, chap. 10).The general scenario-based approach decomposes the decision problem under uncertainty into S deterministic decision problems, one in each scenario, before possibly but not necessarily aggregating the resulting information over scenarios to arrive at a final decision. The process can be represented using a value tree in which scenarios are placed in the second level of the hierarchy as parents to S structurally similar “within scenario” value trees. Although from a mathematical perspective scenarios can also be included at lower levels of the objectives hierarchy, this may be considered contrary to the philosophy of scenario planning.Most scenario planning texts deal explicitly with the question of scenario construction, so that desirable features of scenarios (Schoemaker, 1991; Wright & Goodwin, 1999) as well as techniques for their construction (Schoemaker, 1995; Van der Heijden, 1996) are well-established and therefore not considered here. Instead, focus is directed onto the generally quantitative construction of performance measures within each scenario that must be assessed as part of scenario-based outranking. Two general procedures are given in Stewart (2005, chap. 11). The first considers combinations of scenarios and attributes as SJ distinct ‘meta-attributes’ and evaluates the I alternatives in terms of each of these meta-attributes. This means that a marginal preference model is constructed for each of the SJ meta-attributes, following which performance would be aggregated over all meta-attributes (possibly first within each scenario and then over scenarios, if this is desired).Issues of interest are the definitions of the concordance and discordance measures, the assessment of weights, and the manner of aggregation. Concordance and discordance measures on each criterion can be computed separately for each scenario, although the precise nature of the computations will depend on which outranking method is employed. For example, Electre III measures are defined as(2)cjm(ai,ak)=1ifzij(m)+tjme⩾zkj(m),zkj(m)-(zij(m)+tjmp)tjme-tjmpifzij(m)+tjme⩽zkj(m)⩽zij(m)+tjmp,0ifzij(m)+tjmp⩽zkj(m).(3)djm(ai,ak)=0ifzkj(m)⩽zij(m)+tjmp,zkj(m)-(zij(m)+tjmp)tjmv-tjmpifzij(m)+tjmp⩾zkj(m)⩾zij(m)+tjmv,1ifzkj(m)⩾zij(m)+tjmv.wheretjme,tjmpandtjmvare the indifference, preference and veto thresholds respectively, so thattjme<tjmp<tjmv. The notion of discordance may prove more problematic in a scenario context. Specifically, discordance means that if performance is poor enough on a criterion, even in one scenario, then that outranking relation is vetoed. This implies a strict condition, so that care should be taken that the veto thresholds are consistent with such an interpretation. In particular, it appears that performances in a single scenario would need to be extreme in order to incur a veto. Weights for each scenario-attribute combination can be assessed directly (although this may be impractical for large numbers of attributes or scenarios) or by first establishing the weight of each criterion cjunder the assumption of a common scenario sm, denoted wj∣m, standardizing these to sum to one within each scenario, and then assessing the weight wmassociated with each scenario sm. The joint weights are given by the product wmwj∣m.The outranking method at this stage consists of a set of J concordance measures cjm(ai,ak) and J discordance measures djm(ai,ak) for each scenario, each considering the statement “alternative aioutranks alternative akon criterion cjin scenario sm”. The degree of further aggregation depends on the demands of the decision maker. If all that is required is an evaluation within each scenario, then the construction of an outranking relation in each of the S scenarios can proceed as for the deterministic outranking methods. That is, an aggregate concordance measure defined by(4)Cm(ai,ak)=∑j=1Jwjmcjm(ai,ak)∑j=1Jwjmcan be compared to the set of J discordance indices in that scenario in order to build the outranking relation for scenario sm. Without trivializing this often difficult step, we can refer to existing aggregation methods to construct the set of S outranking relations.Some applications have emphasized this interpretation of results within scenarios, and prefer not to aggregate evaluations over scenarios to arrive at a final global evaluation (Goodwin & Wright, 2001; Ram et al., 2010). While this is in line with the philosophy of scenario planning, which has strong ‘robustness’ views (Van der Heijden, 1996), it is not evident that unaided intuitive aggregation across scenarios will generate the most goal-directed solutions. Provided that the scenario weights have been incorporated into the earlier construction of the Cm(ai,ak), further aggregation over scenarios can take the form of the simple summation(5)C(ai,ak)=∑m=1SCm(ai,ak)while global discordance is given by(6)D(ai,ak)=∏j,mf(djm(ai,ak),C(ai,ak))where(7)f(djm(ai,ak),C(ai,ak))=1ifdjm(ai,ak)<C(ai,ak),1-djm(ai,ak)1-C(ai,ak)ifdjm(ai,ak)>C(ai,ak)Based on this super-outranking problem a final outranking relation can be constructed with respect to the chosen method. For example, the Electre III credibility index is again given by S(ai,ak)=C(ai,ak)D(ai,ak).Despite the apparent ease of the aggregation over scenarios, it must be noted that there is no current consensus on the best way to interpret the scenario weights wm(and hence the joint weights wjm). Proposed solutions (in value function methods) have used scenario probabilities, relative likelihoods, relative weights on performance, or equal weights. No approach has found broad acceptance to date, and the reader is referred to Durbach and Stewart (2012) for further discussion. It is also worth noting here that several authors (Kalaï, Lamboray, & Vanderpooten, 2012; Roy, 2010; Vincke, 1999) have considered the issue of achieving robust performance over scenarios (broadly defined in terms of both external and internal uncertainties), contributing various measures of robustness. These however tend to aggregate performance over scenarios relatively early in the decision process, for example by measuring performance on a criterion by the number of scenarios in which a benchmark is exceeded. As a result it remains unclear how best to reconcile these ideas with traditional scenario planning views or integrate them into scenario-based outranking methods.Another approach is to consider combinations of alternatives and scenarios as IS distinct outcomes or ‘meta-alternatives’ to be evaluated in terms of the J attributes (this was in fact the approach first advocated in Goodwin & Wright (2009)). One important feature of this second assessment procedure is that it does not allow preference information to vary between scenarios. One veto threshold is defined per criteria, to apply over all scenarios, for example. It remains an open question how often the detailed qualitative information gathered during the construction of the scenarios might cause scenario-dependent preferences, or at least an awareness of those preferences. One of the few applications of scenario-based MCDA (Montibeller et al., 2006) found that progress was only possible once preference information was allowed to vary across scenarios. Preference information is also allowed to vary across scenarios in Korhonen (2001) and Parnell, Jackson, Burk, Lehmkuhl, and Engelbrecht (1999). The other important point is that at no stage would one wish to directly compare alternative-scenario pairs involving different scenarios i.e. establish whether (ai,sm) outranks (ak,sn). Doing so is likely to result in unwelcome discordances where two alternatives veto each other because their performances are very different in two boom and bust scenarios. There thus appears to be little benefit to be gained by pursuing this assessment approach further.To show how the scenario approach can be obtained as a special case of the distributive method of d’Avignon and Vincke, first note that no dependence structure is implied in the definition of Hj(ai,ak) supplied in Section 2.2. That is, the preference index Ij(zij,m,zkj,n) is defined for all combinations (zij,m,zkj,n), for m∈{1,…,Mij} and n∈{1,…,Mkj}.If no underlying causal reasoning is ascribed to the states indexed by m and n, the index for each alternative may be reordered without altering the results. This can be interpreted as implying both independence of alternatives i.e. Pr[Zij=zij,m∣Zkj=zkj,m]=Pr[Zij=zij,m], and independence of criteria i.e. Pr[Zij=zij,m∣Ziδ=ziδ,m]=Pr[Zij=zij,m]. A different view of the problem views each m as representing a future state of the world determined by an internally consistent chain of causal reasoning i.e. a scenario. In such a case, the implied dependence structure of the attribute values is very different. Specifically, the mutual exclusivity of the scenarios implies that after observingzij(m), knowledge of the underlying conditions relating to scenario smdirectly implies the effects of these conditions on other alternatives i.e.PrZij=zij(m)|Zkj=zkj(m)=1as well as on other criteria within the same alternative i.e.PrZij=zij(m)|Ziδ=ziδ(m)=1.The scenario-based outranking method then proceeds as for the distributive method of d’Avignon and Vincke, although the use of this much stricter dependence structure greatly simplifies the ensuing computations. Because the Hj(ai,ak) are computed separately in each scenario sm, these are trivially obtained asHj(m)(ai,ak)=Ijzij(m),zkj(m). Within each scenario theHj(m)(ai,ak)are deterministic values rather than random variables, so that scenario-specific pairwise outranking degrees, and strengths and weaknesses, can be calculated by simply summing over criteria and relevant alternatives. Scenario-specific strengths and weaknesses can be exploited within the scenario, or aggregated over scenarios (if desired) using the approaches outlined in the previous section.To clarify the concepts discussed in the previous sections a scenario-based outranking approach is illustrated using the example used in d’Avignon and Vincke’s original paper. Four alternatives are evaluated in terms of three criteria each with possible values {0,1,2}, so that each Zijis represented by a discrete probability distribution defined over the attribute space {0,1,2}. These are given in Table 1.Assuming the independence of evaluations over alternatives and over criteria, this structure gives a total of 312=531,441 possible states. Let us now consider a simple use of scenario planning by assuming that three scenarios have been constructed: a ‘bad’ scenario containing the worst possible attribute values, an ‘average’ scenario containing the expected attribute values, and a ‘good’ scenario containing the best possible attribute values (assuming here that these scenarios are internally consistent). Table 2shows the attribute values in each scenario.The criteria are assumed to be equally weighted, and the preference index takes the formI(x,y)=max0,16(x-y). This means that the largest possible difference of 2 corresponds to the full weight available to a criterion i.e. 1/3. The values of the preference index associated with each scenario are given in Table 3.Due to the dependence structure assumed by the scenario planning methodology, the calculations of the S(ai,ak) are trivially obtained by summation across the criteria within each scenario. Each S(ai,ak) is a three-point random variable denoting the degree of outranking of alternative aiover alternative ak. This allows for a far simpler qualitative interpretation of results at this stage of the analysis, which arguably presents the decision maker with enhanced opportunity for reflection and learning. Moreover, the technical difficulties associated with calculating the distributive strength and weaknesses in the full distributive method are also avoided by using a scenario-based approach. By again exploiting the simple dependence structure implied by scenario planning, these measures are obtained by summation over the relevant set of alternatives. The average strength and weakness measures are presented in Table 4.The results clearly indicate that alternative a1 is first in the rank order, followed by alternative a2. Alternatives a3 and a4 are very similar, from the perspective both of the strengths and weaknesses. In terms of the strengths a3 outperforms a4, albeit weakly, in all three scenarios, while in terms of weaknesses the reverse is true. A tentative final rank order may be given by [a1,a2,(a3,a4)]. As a basis for comparison, the rank orders obtained by d’Avignon and Vincke using the full distribution method were [a1,a2,a3,a4] for strengths, [a1,a2,a3,a4] for weaknesses, and [a1,a2,(a3,a4)] overall.The example above is of course an extremely simple artificial one, and the results of the full distribution- and scenario-based approaches may differ more substantially in other contexts. In the following sections we attempt to gain some insight into the potential magnitude of these differences via a simulation study.Fig. 1shows an outline of a single simulation run. Dashed boxes have been used to indicate those parts of the simulation applied iteratively to each alternative and attribute.Most previous simulation studies use a single distribution for each Zij(e.g. Barron & Barrett, 1996; Salo & Hamalainen, 2001), but the current study uses multiple distributions because they allow a parsimonious simulated application of scenario models, following the approach is used in Durbach and Stewart (2012). Specifically each Zijis composed of L=10 normal distributions, denoted byNμijℓ,σijℓ2,ℓ∈{1,…,L}where μ and σ2 are mean and variance respectively. The index ℓis referred to as indexing the ‘future’ fℓ.Means for the realizations in each future are simulated by an algorithm that allows the variability of performances over futures to differ between alternatives and criteria (i.e. some alternatives may differ very little over the range of futures on a criterion while for other alternatives or criteria almost any performance may be possible). The algorithm proceeds as follows, for every alternative ai, attribute cj:1.Generate a lower bound for the mean realizations by drawing randomly from Uni[0,α], where α is a parameter of the simulation. In a ‘low variability’ condition α is set to 0.3, while in a ‘high variability’ condition α=1. Denote the generated lower bound byαijlower.Generate an upper bound for the mean realizations by drawing randomly fromUniαijlower,1. Denote the generated upper bound byαijupper.Generate L=10 mean realizations i.e. the μijℓ, by drawing randomly from Uni[αlowerij,αupperij]Alternatives are made Pareto optimal within each future by standardizing within each alternative aiso that∑jμijℓ=1.Within each future fℓ, the simulation then generates unstandardized realizations by:1.Generating a standard deviation σijℓ randomly on Uni[0.01,σ(d)], where σ(d) is a parameter of the simulation. In a ‘low variability’ condition σ(d) is set to 0.05, while in a ‘high variability’ condition σ(d)=0.1.Setting the number of realizations Mℓ to be generated for future fℓ. A total of K=400 realizations for each Zijis used i.e. over all futures. These realizations are distributed over futures either “uniformly” (all futures contain 40 realizations) or “non-uniformly” (four futures contain 60 realizations each, four contain 20 realizations, and two contain 40 realizations). The distribution of realizations over futures is denoted by M.Generating Mℓ independent realizations fromNμijℓ,σijℓ2. The 1×Mℓ vector of realizations generated in future fℓ is denotedzij(ℓ).Once realizations have been generated for each future, these are concatenated into a single 1×K vector containing all the realizations for Ziji.e.zij=zij(1),zij(2),…,zij(ℓ),…,zij(L). Realizations are scaled so that the largest realization on each attribute over all alternatives is one and the smallest is zero.Inputs to the simulated scenario model are generated in a two-step process. First, a sample of size S is randomly drawn from {f1,f2,…,fL} without replacement i.e. each future fℓ has an equal probability of selection, regardless of how many states comprise it. The mean μijℓ is used in each of the S selected futures, giving a ‘mean-scenario’ model. The number of scenarios used S is a parameter of the simulation, taking on the values 3, 5, and 10. These values must be interpreted in light of there being 10 possible futures – the simulation cannot, for instance, provide results on the general use of 3, 5 or 10 scenarios. For this reason the proportion of futures selected – which we term the ‘coverage’ provided by a scenario model – is often used when discussing the mean-scenario model.Although attribute generation is somewhat biased in favor of a mean-scenario model using all L futures, the concept of coverage captures in an idealized way the scenario planning aim of constructing scenarios that “bound the future” (e.g. Schoemaker, 1991); a scenario model with 100% coverage is practically unrealistic (since the number of possible futures will surely exceed four or five, the recommended number of scenarios given by many scenario planning texts e.g. Van der Heijden, 1996) but useful in giving an upper bound on accuracy. More realistic scenario models with less coverage (50% and 30%) are also simulated, and sensitivity to coverage and construction method are important results. To assess the effect of possible errors in the scenario construction process, a model which uses a set of S realizations drawn at random from the full set of K=400 realizations is also evaluated i.e. one without regard for the structure imposed by the ‘futures’. It is important to note that in this construction the proportion S/L cannot be interpreted as “coverage” of the future, since realizations are drawn without regard for futures and multiple realizations may be drawn from each future (indeed S may exceed L, which is not possible under the mean-scenario model).Following the procedure described in Section 2.2, a preference index Ij(zij,m,zkj,n) is constructed that denotes preferences for all possible combinations (zij,m,zkj,n) of the simulated stochastic evaluations. The preference index takes on its maximum value wjat the maximum possible difference of one, is zero below an indifference threshold, and is possibly non-linear between these two extremes i.e.(8)Ij(x,y)=wj(x-y-tje)βj(1-tje)βjifx-y⩾tje0ifx-y<tjpwheretjeis an indifference threshold for criteria cj, and βjdetermines the curvature of the preference function. Both of these are left as parameters of the simulation.Once the preference index Ij(zij,m,zkj,n) has been constructed for all possible combinations, a large number of values from Ij(zij,m,zkj,n) are randomly drawn to empirically generate the random variable Hj(ai,ak). Note that a simple random sample can be used here because each of the zij,mare by construction equally likely i.e. fij(zij,m)fkj(zkj,n) is constant ∀i, j, m, n.The generated values for Hj(ai,ak) are then aggregated into measures of overall strength and weakness for aiby summation over all criteria and all alternatives except aii.e. using the Promethee formulation. A final exploitation of these measures (and hence two partial preference orders) is obtained by taking the median of the strength and weakness distributions, as suggested by d’Avignon and Vincke (1988).The scenario-based outranking method is simulated as for the distributive method, except that the initial preference index Ij(·) is not computed for all possible combinations (zij,m,zkj,n), but only between alternatives in the same scenario i.e. for combinations(zij(m),zkj(m)), wherem∈S, the set of scenarios.The approximation of the distributive outranking model by the scenario model is assessed by comparing the strength and weakness rank orders obtained for these two models, using the following outcomes:•The average (strength or weakness) rank of the alternative selected by a scenario model in the (strength or weakness) rank order obtained using the distributive model. As rank measures can only be interpreted relative to the number of alternatives used, which is a parameter of the simulation, this and all other rank measures have been standardized to lie between 0 (the best alternative is selected) and 1 (the worst alternative is selected).The average (strength or weakness) rank of the alternative selected by the distributive model in the (strength or weakness) rank order obtained using a scenario model,The rank correlation between the scenario and distributive model’s (strength or weakness) rank orders. To make this measure comparable with the rank-based measures, which decrease with better performance, results are given for 1−R where R is Spearman’s rank correlation.Table 5summarizes the parameter values used in the simulation study. Although a large degree of subjectivity in specifying simulation parameters is inevitable, the values here have been chosen on the basis of preliminary investigations and have been used to compare scenario models in the context of value function methods in Durbach and Stewart (2012). A full factorial design is used, performing 100 simulation runs for each combination of parameters, giving standard errors of less than 0.001 for differences between groups formed by combinations of three factors. This is small enough for any differences discussed below to be statistically significant at the 1% level.Fig. 2summarizes the overall performance of the scenario-based outranking model. For each of the three outcome measures the effects of different combinations of scenario number and construction policy are shown. To gain some insight into how bad the approximations may be, the 90% quantiles of each outcome are shown with the averages.The results show that the scenario model can return results that accurately approximate, on average, those obtained using the distributive model, under the important condition that mean scenarios are selected. Even if only 30% of the futures are included, the alternative selected by the mean-scenario model appears in the top 15% of the distributive model’s rank order on average i.e. rank 1–2 (rank 2–3) in the 9-alternative (19-alternative) problem. Similar conclusions may be drawn from the other two outcome measures. However, if mean scenarios are not selected the scenario models perform considerably worse. If scenarios are selected at random the resulting preferred alternative drops approximately one rank (for I=9; two ranks for I=19).1At the suggestion of a reviewer an intermediate scenario model that selects a random realization within each of the chosen futures (rather than over all futures) was also simulated. The performance of this model lies between the mean-scenario and fully-random models, as would be expected, but also depends on the degree of coverage. At 30% coverage performance is similar to a fully-random (3-scenario) model: the intermediate model achieves 5–10% of the improvement of the mean-scenario model over the fully-random model. At 50% and 100% coverage the intermediate model captures around 20% and 40% of the improvement respectively. Its performance thus remains closer to the fully-random model over all coverage conditions, highlighting the importance of scenarios capturing within-future performance as well as between-future differences.1Interestingly, scenario selection errors seem to affect rank orders derived from strengths more than those derived from weaknesses – under the mean-scenario model performance is slightly better using strengths, but if scenarios are randomly selected then weaknesses perform better.Improving coverage substantially improves model performance, although with marginally decreasing returns to scale. Since it seems likely that coverage in practical problems is likely to be low, this suggests that it may well be worth the effort of including one or two extra scenarios where this does not unduly burden the decision maker.The 90% quantiles show that there is at least some danger of the scenario model returning a poor approximation. Where coverage is low there is effectively a 10% probability that the selection made by the mean-scenario model appears outside of the top 25% of the distributive model’s rank order. For many applications this may not be a tolerable risk. If in addition scenario construction is poor, the scenario model quality is only marginally better than a random guess. Fortunately at higher levels of coverage the scenario model becomes substantially more robust, and in fact with full coverage there is very little difference between the mean and 90% quantile. This too highlights the benefits to be gained from constructing a small number of additional scenarios to ensure an adequate ‘bounding’ of the future.Fig. 3shows the performance of the scenario-based outranking model for different combinations of preference function shapes, with or without errors in the construction of scenarios. Because the same conclusions are drawn from any of the outcome measures, results are shown only for the rank of the scenario selection in the distributive model rank order.The shape of the preference functions have a smaller but still substantive influence on the performance of the scenario model, which is best when there are no indifference thresholds and preference functions are concave. Performance worsens as preference functions become more convex, particularly if indifference thresholds are also present. An interesting result here is that the strength rank order is much less adversely affected by changes in the preference functions that the weakness rank order (see the last column of plots in Fig. 3). Other than this, the strength and weakness rank orders return very similar results.Fig. 4shows how the scenario-based outranking models are influenced by the number of alternatives and criteria present in a decision problem. The relative performance of the scenario model improves substantially with the larger set of alternatives. The net effect of this relative improvement (nearly 10%) is that the absolute rank returned by the scenario model remains fairly constant in both conditions. Increasing the number of criteria plays less of role but does cause a marginal deterioration in the performance of the scenario model, particularly in the strength rank orders.Fig. 5shows the effect of variability in the attribute evaluations and future likelihoods. Having some futures more likely than others causes a small deterioration in the performance of the scenario model: since relative likelihood does not enter into the construction of scenarios there is always a chance that the more likely futures are not selected. The variability of attribute evaluations plays a much larger role, and shows an important interaction with scenario coverage. Performance of the scenario models deteriorates appreciably with more variable attribute evaluations if scenario coverage is low, but has much less of a detrimental effect is all futures are included. Deteriorations caused by increasing attribute variability are also larger when scenarios are randomly selected than when mean scenarios are used. This is presumably because means within a future are relatively unaffected by increasing variability, but a random selection from that future will differ more from the mean, on average, when variability is high.

@&#CONCLUSIONS@&#
Many ways of treating uncertain attribute evaluations exist in the outranking methods. In most of these the uncertainty (or “distributional” aspect) is absorbed at a fairly early stage of the process, for example by defining probabilities that aioutranks akon criterion cjand then working with these. As a result it may be difficult for decision makers to gain much insight into the uncertainty part of the problem – for example, how variable a particular alternative’s performance is relative to others.The distributive method of d’Avignon and Vincke, by allowing preferences to be expressed over attribute values rather than over probabilities, takes an important step in remedying this problem. Nevertheless, for the decision maker to think about the concepts of outranking strengths and weaknesses in a probabilistic way requires a great cognitive effort. It some situations this effort may be a critical and unavoidable aspect of the problem; in others it may be a hindrance to the use of a multi-criteria approach. In such circumstances it may help to allow the decision maker to think about the problem in a deterministic context through the introduction of a small number of scenarios. The scenario-based outranking method described in this paper decomposes the uncertain decision problem into a small number of deterministic ones, one in each scenario. That structure can be integrated with any existing deterministic outranking method, and indeed with non-outranking methods too, making it a flexible tool for MCDA.Although it is a simplification the scenario-based outranking method still provides a great deal of information for decision makers to digest. Outranking indices will still need to be interpreted, now with the added dimension of multiple scenarios, so that it is important to at least acknowledge that whether the method is transparent enough to be practically useful or yield insights remains a question for future research. Reports from real decision makers on their experiences of using scenario-based approaches in practical situations would be valuable in addressing these questions.Whatever their positive practical impact, the simplifications made by the scenario-based approach must also come at some cost. By selecting a small number of scenarios for closer consideration many of the finer distributional details of the decision problem are ignored. A simulation study was designed to assess the extent of this cost, through a comparison with the distributional method of d’Avignon and Vincke. The main results from the simulation study were:1.On average the use of a scenario model results in a deterioration of around 15–20% in accuracy. That is, the alternative selected by the scenario model appeared in the 15–20th percentile of the rank order derived from a full distributional approach. This figure assumed that the set of constructed scenarios covered a modest 30% of possible futures. Whether this accuracy is acceptable or not will depend on problem context, but it does not seem unreasonable a priori given the extent of the simplification and the other benefits associated with scenario planning.The accuracy of the scenario models improves fairly quickly with increasing coverage of the future. In addition scenario models with greater coverage show less variability in their accuracy and are more robust to non-idealities in the decision process, particularly variability in attribute evaluations. It therefore seems advisable to ensure that constructed scenarios cover as much of the future as is realistically possible. Whether this is done by constructing more scenarios, or by defining them in a broad way, will depend on the context of a particular decision problem.These two results agree in broad terms with a similar simulation study conducted in the context of value function methods (Durbach & Stewart, 2012). In summary, the use of scenarios appears to be a viable option for decision aid, not just in the value function method where it has predominantly been applied to date, but also in the outranking methods and presumably other approaches to MCDA. A number of research questions remain about the best way to go about conducting scenario-based MCDA however. These include how best to aggregate performance over scenarios; the related interpretation of scenario weights; assessing robustness over scenarios; and assessing ways of constructing scenarios so that the problem context is sufficiently simplified while minimizing the deterioration of model accuracy, and still providing the decision maker with views of the world that challenge his or her own.