@&#MAIN-TITLE@&#
Semantic search for public opinions on urban affairs: A probabilistic topic modeling-based approach

@&#HIGHLIGHTS@&#
The explosion of online user-generated content (UGC) and the development of big data analysis provide a new opportunity and challenge to understand and respond to public opinions in the G2C e-government context.We proposed an approach based on the latent Dirichlet allocation (LDA) and designed a practical system to provide users with satisfying searching results and the longitudinal changing curves of related topics.Municipal administrators could better understand citizens’ online comments based on the proposed semantic search approach and could improve their decision-making process by considering public opinions.

@&#KEYPHRASES@&#
Probabilistic topic modeling,Public opinions,Big data analysis,Semantic search,Latent Dirichlet allocation (LDA),

@&#ABSTRACT@&#
The explosion of online user-generated content (UGC) and the development of big data analysis provide a new opportunity and challenge to understand and respond to public opinions in the G2C e-government context. To better understand semantic searching of public comments on an online platform for citizens’ opinions about urban affairs issues, this paper proposed an approach based on the latent Dirichlet allocation (LDA), a probabilistic topic modeling method, and designed a practical system to provide users—municipal administrators of B-city—with satisfying searching results and the longitudinal changing curves of related topics. The system is developed to respond to actual demand from B-city's local government, and the user evaluation experiment results show that a system based on the LDA method could provide information that is more helpful to relevant staff members. Municipal administrators could better understand citizens’ online comments based on the proposed semantic search approach and could improve their decision-making process by considering public opinions.

@&#INTRODUCTION@&#
The spread of Web 2.0 applications enables a new model for the use of the Internet (Burke, 2009). Users act not only as websites’ visitors but also as content creators (Ingawale, Dutta, Roy, & Seetharaman, 2013). User generated content (UGC) significantly enriches the amount of online information and makes it more difficult for users to comprehensively understand information through regular reading behavior (Li et al., 2012; Zhu, Mo, Wang, & Lu, 2011). However, it is obvious that to appreciate the significance of UGC, any use of the knowledge underlying UGC must be developed based on a comprehension of its content (Williams, Wiele, Iwaarden, & Eldridge, 2010).With respect to public administrators, the Web 2.0 environment with multiple exchanges provides a vital opportunity for enhancing interactions between the government and citizens (Horton, 2006). The explosion of online UGC and the development of big data analysis provides a new opportunity and challenge to understand and respond to public opinions (Rhoda & Norman, 2013; Yu-Che & Tsui-Chuan, 2014). In particular, when confronted with urban residents whose problems are comparatively homogeneous, local governments attempt to establish interactive platforms to collect citizens’ opinions and recommendations from various perspectives to serve as a foundation for evaluating the performance of the government and to guide future policy adjustment (Hong, 2013). However, as citizens’ enthusiasm for voicing their opinions on the Internet grows, how to understand the information both timely and effectively becomes more significant, which is directly related to whether issues that concern administrators can be addressed and whether the corresponding feedback could be timely offered to citizens (Linders, 2012). Semantic analysis and semantic search technology are likely to become effective tools for assisting the public administrators in the rapid and precise positioning of mass text information.This study focuses on a platform for acquiring online citizen opinions on urban public affairs issues. B-city has been honored as one of China's international metropolises. To reinforce communication between municipal administrators and citizens, to listen to citizens’ opinions and suggestions on urban public affairs in a timely fashion, and to facilitate public participation in urban construction and development, in 2005 B-city's municipal government installed an opinion acquisition module related to urban public affairs onto its official website; the city now receives around 30,000 text messages per year.Confronted with this non-structured text information, it is difficult to use simple statistical methods and traditional data processing tools to help officials better understand those comments. Before the start of the study, face to average more than 100 comments of daily public feedbacks, the office of the website, only sorts responsibility to the different departments manually, and oversees the feedbacks every day, without any historical data analysis tools. When sometimes need to summarize a period of public opinions, or analyze deeply around one case or one area, the office can only conduct keywords retrieval then manual read and summarize the retrieved results. Obviously, the current manual method is inefficient, even invalid for those tasks which unable to provide accurate keywords initially. Semantic analysis and semantic search could play significant roles in solving this problem.The paper describes a basic idea for designing a semantic search tool directed to special demands: a framework composed of two sets of procedures—namely, a user search process and a probabilistic topic modeling process—is proposed based on the systemization of literature related to semantic search and semantic analysis to characterize the actual requirements of the online citizen opinion platform. In other words, the foreground procedure facilitates obtaining keywords from search input, provides auxiliary keywords to help searchers determine the theme (when necessary), and derives not only search results but also longitudinal changing curves. The background procedure is a process of preprocessing subject clustering for the comment data based on a probabilistic topic modeling approach called Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003). Each theme that is generated based on probability subject modeling is appropriately viewed as the basic message block that waits constantly to be searched and invoked by the front-end search flow in the database. This study tries to make two aspects of contributions: (1) For this specific practice scenario mentioned in the paper, we provide a set of feasible solution to help the office search the public comments history data according to changeful requirements. In particular, the solution of the study is based on semantics, rather than merely based on keywords, which make the system “smarter” and could adapt to more complex requirements; (2) For academic area, we show a semantic search approach based on the LDA method. Compare to existing research involves the field of semantic modeling (Misra, Yvon, Cappé, & Jose, 2011; Xu, Zhang, & Wang, 2015), we try to show that the LDA could also play a role in the semantic search, and provide some key details of the techniques process including coordination between real-time search and non-real-time LDA calculation, keywords matching and suggestions in the user search process, determination of the number of latent topics.To validate the usefulness and effectiveness of our proposed semantic search method based on LDA, we conduct a user evaluation experiment to compare with a baseline method, the Keywords-Matching approach (KM). The paper also presents the implications of the results for municipal management.The idea of semantic search, which is understood as searching by meanings rather than literal strings of word and which aims to solve the limitations of keyword-based search models, has been the focus of a wide body of research in both the Semantic Web (SW) and the Information Retrieval (IR) communities (Fernández et al., 2011). An important aspect of semantic search approaches is that almost all of them use conceptual representations of content beyond mere keywords, and many of them also attempt to provide conceptual representations of user needs, as a method of enhancing traditional mainstream keyword-based search technologies.Early in 2005, Mäkelä (2005) described five of the most-used methodologies in semantic search: (1) Resource Description Framework (RDF) Path Traversal; (2) keyword to concept mapping; (3) graph patterns; (4) logics; and (5) fuzzy concepts, fuzzy relations, and fuzzy logics. Then, Mangold (2007) surveyed and compared 22 different semantic search approaches or projects based on seven dimensions or criteria: architecture, coupling, transparency, user context, query modification, ontology structure and ontology technology. Thereafter, Dong, Hussain, and Chang (2008) conducted a brief survey based on a list of semantic search technologies from six categories: semantic search engines, semantic search methods, hybrid semantic search engines, semantic XML search engines, semantic ontology search engines and semantic multimedia search engines.As described above, the core of semantic search technologies is the type and use of semantic knowledge representation. Accordingly, most of the semantic search methods in the literature can be distinguished according to the following three categories:(1)Linguistic conceptualization approaches are based on light conceptualizations (usually considering few types of relationships among concepts) and low information specificity levels. For instance, early in 1998, Word Net was used to enhance search performance by considering the semantic relationships among words or concepts (Mandala, Takenobu, & Hozumi, 1998). Urbain, Goharian, and Frieder (2008) have explored unsupervised learning techniques for extracting semantic information about biomedical concepts and topics and introduced a passage retrieval model based on Markov random fields for using these semantics in context to improve genomics literature searches.Ontology-based proposals consider a much more detailed and densely populated conceptual space in the form of ontology-based knowledge bases. For example, Chiang, Chua, and Storey (2001) have proposed a smart Web query (SWQ) method for the semantic search of Web data by utilizing domain semantics, which are represented as context ontologies to specify and formulate appropriate Web queries to search. Kim (2005) has designed and implemented an ontology-based Web retrieval system for semantic searches of the Web resources of international organizations such as the World Bank and the Organization for Economic Co-operation and Development (OECD). Moreover, the semantic Web has been widely used as an effective tool for ontology-based semantic search. For instance, Ding, Kolari, Ding, and Avancha (2007) have reviewed the methods and tools of using ontologies on the semantic Web, whereas Vandic, van Dam, and Frasincar (2012) have presented a platform for multifaceted product search using semantic Web technology. Although ontology-based semantic search approaches have been widely investigated and applied in recent years (Jiang & Tan, 2009; Johnson Lim, Liu, & Lee, 2010; Kara et al., 2012; Lee, Min, Oh, & Chung, 2014; Ruiz-Martínez, Valencia-García, Martínez-Béjar, & Hoffmann, 2012; Zheng, Chen, & Jiang, 2012), there are still various types of criticisms of their limitations, which primarily lie in the problem of knowledge incompleteness and obsolescence, along with the lack of established evaluation methodology for semantic search models (Blanco et al., 2013).Statistical approaches, such as LSA (Latent Semantic Analysis) (Papadimitriou, Tamaki, Raghavan, & Vempala, 1998), pLSI (Probabilistic Latent Sematic Index) (Hofmann, 1999b) and LDA (Blei, 2012; Blei et al., 2003), use statistical models to identify groups of words that commonly appear together and therefore may jointly describe a particular reality.In addition, topic modeling algorithms—primarily regarded as the core techniques in statistical semantic search approaches—are statistical methods that analyze the words of an original text to discover the themes or topics that run through it. They were originally proposed and applied in the field of information retrieval (Blei, 2012) and have been adopted and used in various domains such as recommender systems (Wang & Blei, 2011), social sciences (Ramage, Rosen, Chuang, Manning, & McFarland, 2009), social network analysis (Li et al., 2010), opinion mining (Rao, Li, Mao, & Wenyin, 2014), etc.Without directly addressing the topics inherent in the documents, the TF-IDF schema (Robertson & Jones, 1976) and Vector Space Model (VSM) (Salton, Wong, & Yang, 1975) have provided a rough solution to describing and modeling documents and their content or topic similarities, with the disadvantage that documents with a similar context but different vocabulary will not be associated with each other (Liu, 2007). To address this problem, Latent Semantic Analysis (LSA) has been proposed to convert documents’ high-dimensionality word-space representations into low-dimensionality vectors of the topics (Deerwester, Dumais, Furnas, Landauer, & Harshman, 1990), in which topics can be obtained using singular-value decomposition (SVD). Using such a technique yielded some improvement over a TF-IDF baseline.A closely related technique—pLSI (Hofmann, 1999a)—attempts to create a set of topics in a probabilistic framework (Papadimitriou et al., 1998). pLSA topics are probabilistic instead of the heuristic geometric distances in LSA. pLSA has been successfully used for information retrieval related to large collections because it does not need to run the expensive SVD operation.LDA is a popular topic modeling tool, designed to learn a set of topics (word distributions) and to infer mixtures of those topics to create low-dimensionality representations of documents (Blei et al., 2003), which further refines the pLSI model within a Bayesian framework. LDA has been the most popular topic extraction algorithm in recent years, and it has been successfully used to characterize the content topics of a document or a collection (Blei, 2012; Carterette & Chandar, 2009; Liu & Turtle, 2013).In recent years, LDA model has been applied in various aspects of domain of information retrieval and natural language processing, such as text segmentation (Misra et al., 2011), feature identification (Xu et al., 2015), text classification (Phan, Nguyen, & Horiguchi, 2008), etc. Meanwhile, there have been many research works based on various applications of the LDA model in other research fields, such as customer segmentation (Wu & Chou, 2011), telecommunications fraud detection (Olszewski, 2012; Xing & Girolami, 2007), software evolution and defect reporting (Linstead, Lopes, & Baldi, 2008; Somasundaram & Murphy, 2012; Thomas, Adams, Hassan, & Blostein, 2014) as well as bioinformatics domain of microarray expression profile analysis (Bicego et al., 2012), miRNA-mRNA interactions study (Liu et al., 2010), the classification of genomic sequences (La Rosa, Fiannaca, Rizzo, & Urso, 2014), etc. Moreover, the dynamic topic model has been proposed to discover the evolution of topics over time, whose assumption is that the topics are related to the time periods, and the meanings of the words may evolve over time (Blei & Lafferty, 2006). However, it remains difficult to find research papers applying the LDA method in the domain of public management and public affairs (Levy & Franklin, 2014). Furthermore, there is little work addressing stages of user interaction by utilizing and applying the results of LDA modeling. A detailed introduction to the LDA model will be presented in the following section.Before describing our proposed semantic search model based on probabilistic topic modeling, we first must introduce the fundamental principle of the LDA model. Latent probabilistic topic models are effective methods for extracting latent semantic information from text documents (Blei, 2012; Blei et al., 2003; Deerwester et al., 1990; Hofmann, 1999a). Among these models, LDA appears to be the most effective (Blei et al., 2003).The intuition underlying the LDA model is that documents have multiple topics (Blei, 2012). For instance, consider the paper in Fig. 1. This paper, recently published in Science Magazine and entitled “The Regulation of Online Social Network Studies”, explores the US government's regulation of scientific research on online social networks, especially research related to adolescents (Shapiro & Ossorio, 2013). We highlighted, by hand, several words used in this article. Words about scientific research, such as “research” and “experiment”, are highlighted in colora(i.e., yellow); words about regulations, such as “control” and “allow”, are highlighted in colorb(i.e., green); words about social networks, such as “SNS” and “Facebook”, are highlighted in colorc(i.e., red). Had we highlighted all of the words in the paper, we would have found that this paper blends scientific research, regulations and social networks in different proportions. Moreover, stop-words that make little topical sense—such as “and”, “but” or “if”—have been excluded from this process. Furthermore, knowing that this paper blends those topics would help us to situate it in a collection of scientific or policy articles.Before moving ahead, we first introduce the terminology and notations used in this model.•A wordw∈{1,…,V}is the most basic unit of discrete data. For clear notations, w is a V-dimensional unit-based vector. If w takes on the ith element in the vocabulary, thenwi=1andwj=0for all j ≠ i.A document is a sequence of N words denoted byw=(w1,w2,…,wN), where wnis the nth word in the sequence.A corpus is a collection of M documents denoted byD=(w1,w2,…,wM).A topicz∈{1,…,K}is a probability distribution over the vocabulary of V words. Topics model particular groups of words that frequently occur together in documents, which thus can be interpreted as “subjects” or “themes”. We denotez=(z1,z2,…,zN)as the sequence of topics across all words in a document.LDA is an unsupervised, generative statistical model that seeks to capture the above intuition and proposes a stochastic procedure whereby the words in a document are generated. LDA is also a model of dimensional reduction, which handles the sparsity of vocabulary in a way that the words are represented as probability distribution over several hidden topics. The original space of vocabulary is mapped to several topics, and it can better depict the semantic meaning of the documents. Given a collection of unlabeled text documents, the LDA model seeks to discover hidden topics as distributions over the words in a fixed vocabulary. For example, the regulations topic has a high probability of containing words about regulations, whereas the social networks topic has a high probability of containing words about social networks. Here, words are modeled as observed random variables and topics are treated as latent or hidden random variables. Like other generative probabilistic modeling algorithms, once the generative procedure has been established, LDA first defines a joint distribution over both the observed and the hidden random variables and then utilizes statistical inferences to compute the conditional distribution or posterior distribution of the hidden variables (i.e., topics), given the observed variables (i.e., words).In LDA, it is assumed that these topics are specified before any document has been generated. Thus, for any document in the corpus, the generative process contains two stages. First, a topic distribution vector modeled by a Dirichlet random variable (Balakrishnan & Nevzorov, 2005) has been chosen randomly to determine which topics are the most likely to appear in a document. Then, for each word that is to appear in the document, a single topic from the topic distribution vector is randomly selected. To actually generate the word, we then draw on the probability distribution conditioned on the chosen topic. Each word in a document is generated by a different, randomly selected topic. The above generative process is described more formally below (as shown in Fig. 2).For each document indexed bym∈{1,…,M}in a corpus:(1)Choose a K-dimensional topic distribution vector θmfrom the distributionp(θ|α)=Dirichlet(α).For each word indexed byn∈{1,…,N}in a document:(a)Choose a topiczn∈{1,…,K}from the multinomial distributionp(zn=k|θm)=θmk.Given the chosen topic zn, select a word wnfrom the conditional probability distributionp(wn=i|zn=k,β)=βik.In the above descriptions, the parameterαis a K-dimensional positive vector determining the Dirichlet distribution, which remains constant across all of the documents within a corpus. The Dirichlet distribution is given as follows:(1)p(θ|α)=Γ(∑iαi)∏iΓ(αi)∏iθαi−1where Γ(x) is the gamma function.The parameterβis a V × K matrix describing the word probabilities, which is also estimated from the data.The generative procedure given above defines a joint distribution for each document. Assuming that parametersαandβare given, the joint distribution of a topic mixture isθand both a set of N topics z and a set of N words w is given by:(2)p(θ,z,w|α,β)=p(θ|α)Πn=1Np(zn|θ)p(wn|zn,β)By integrating overθand summing over z, we can obtain the marginal distribution or likelihood of a document:(3)p(w|α,β)=∫p(θ|α)(Πn=1N∑znp(zn|θ)p(wn|zn,β))dθ=Γ(∑iαi)∏iΓ(αi)∫(Πi=1Kθiαi−1)(Πn=1N∑i=1KΠk=1V(θiβik)wnk)dθNormally, the optimal values of the parametersαandβare chosen to maximize the likelihood of all of the documents in the corpus:(4)l(α,β)=∑m=1Mlogp(wm|α,β)In practice, the expectation–maximization (EM) algorithm (Dempster, Laird, & Rubin, 1977; Hastie, Tibshirani, & Friedman, 2009) and the Gibbs-sampling-based algorithms (Casella & George, 1992; Porteous et al., 2008) are usually used to conduct the parameter estimation task.More specifically, to estimate document-specific topic distribution probability (i.e., p(zn|wm)) and topic-specific word distribution probability (i.e., p(wn|zn)), sample posterior distribution using Gibbs sampling is usually conducted. Givenz={zn=k,z¬n}andwm={wn=s,w¬n}, it is easy to derive Eq. (5) as follows:(5)p(zn=k|wm,z¬n,α,β)=p(z,wm|α,β)p(z¬n,wm|α,β)∝Csk+βk∑s=1N(Csk+βk)×Ckm+αk∑k=1K(Ckm+αk)wherez¬nis the topic vector only excluded from zn,w¬nis the word vector only excluded from wn,Cskis the number of times word s is assigned to topic k andCkmis the number of times topic k is assigned to document m during the Gibbs sampling process, respectively.Next, iterate Eq. (5) for all topics until it reaches the convergence for the entire document set. The final estimated results of p(zn|wm) and p(wn|zn) are listed as follows:•The document-specific topic distribution probability (i.e., p(zn|wm)) can be derived as:(6)p(zn|wm)=θmk=Ckm+αk∑k=1K(Ckm+αk)The topic-specific word distribution probability (i.e., p(wn|zn)) can be derived as:(7)p(wn|zn)=Csk+βk∑s=1N(Csk+βk)From the above analysis, it is noted that the LDA model does not require any prior annotations or labeling of the documents and that the topics emerge from the analysis of the original texts. The LDA model enables us to organize and summarize electronic archives on a scale that would be impossible using human annotation (Blei, 2012).Unlike the traditional keyword-based or ontology-based IR models, we propose to design a novel semantic search framework for public comments on urban affairs based on a probabilistic topic modeling approach. More specifically, we propose to use LDA (Blei et al., 2003), which was introduced in detail in the last subsection. In this framework, the LDA model is utilized to discover and annotate large archives of public-generated documents with semantic thematic information to determine how these themes are connected to each other and how they change over time. The relation or association between a semantic topic and a document is what we call annotation. As introduced in the above subsection, document-specific topic distribution probability (i.e., p(zn|wm)) can be derived by performing a Gibbs sampling.As noted above, users—typically administrative staff of the website or a related government department—may be more interested in the longitudinal changing tendencies of particular topics of discussions, rather than merely a ranked list of documents that may be semantically related to a particular query. Therefore, in our proposed framework, after a user poses a query (a list of keywords or a sentence), our approach will ultimately generate and provide the end user with two types of results (i.e., the picture of the longitudinal changing curves of semantically related topics and a ranked list of documents for each semantically related topic).In general, the overall semantic search model consists of the following two essential components (see Fig. 3), which are described in more detailed in the following subsections:(1)The probabilistic topic modeling process (i.e., the LDA modeling stage); andThe user search process.In this section, we will introduce detailed procedures for the probabilistic topic modeling stage by applying the LDA model.In general, the probabilistic topic modeling process is based on (Chinese) text information in website documents that are generated by the public to provide comments or suggestions on diverse aspects of urban affairs and stored in the back-end database. To further explore that content, it is necessary to perform several pre-processing operations on the text, such as word segmentation and stop-word removal.Because Chinese text does not encounter situations related to various syntactical forms (i.e., plural forms for nouns, gerund forms and past tense for verbs) (Liu, 2007) and the LDA modeling process below does not require the learning of each word's part-of-speech (Blei et al., 2003), it is noted that stemming and POS tagging are not necessary. To perform word segmentation on Chinese text, we choose the ICTCLAS (Institute of Computing Technology, Chinese Lexical Analysis System) system. The ICTCLAS Chinese word segmentation system is based on the Hierarchical Hidden Markov Model (HHMM) (Fine, Singer, & Tishby, 1998) and first proposed by Zhang, Yu, Xiong, and Liu (2003), which is one of the best and most widely used Chinese word-segmentation systems, reaching a segmentation precision of more than 97% (Du, Tan, Cheng, & Yun, 2010; Gao et al., 2013; Li, Ma, Zhang, Huang, & Kinshuk, 2013; Li, Zhu, & Zhou, 2014; Liu, He, Wang, Song, & Du, 2013; Rao et al., 2014; Shi & Nie, 2009). To complete the task of stop-word removal, we utilize a list of Chinese stop-words used in Apache Lucene's SmartChineseAnalyzer Class. Thereafter, each public comment document is represented as a vector of words.Next, the core task lies in using the LDA model to conduct probabilistic topic modeling. Specifically, LDA modeling applies training and inference to all of the text vectors and can discover any latent topics or themes inherent in those data (Blei, 2012). In this paper, we actually choose the default parameter settings for the Dirichlet priors, i.e., we use symmetric setting α = 50/K and β = 0.01, where K is the number of topics. Such settings are widely used in previous literature (Wei & Croft, 2006; Zhang & Sun, 2012). Moreover, the Dirichlet priors would not influence the performance of the model much (Griffiths & Steyvers, 2004; Wei & Croft, 2006). We exploit Gibbs Sampling method to infer the parameters, which is in the form of Monte Carlo Markov Chain (MCMC) (Porteous et al., 2008). MCMC can be proved to get converged in multiple iterations of sampling process. To assess the convergence of the model, we computed the log likelihood in each iteration and the convergence was achieved when (old_likelihood-likelihood)/old_likelihood < 10e−6.After the topic modeling, by conducting the Gibbs sampling process it is easy to obtain the following useful results related to the words, documents and topics in the document set:•Latent topics with the most likely words in each topic;A document-to-topic probability matrix.Theoretically, for any given topic, there is a corresponding distribution across all of the words in the vocabulary (i.e., p(wt|zn)), and the LDA modeling process could provide the most likely words with the highest probability with respect to each topic, whereby the number of the most likely words can be given in advance.In the document-to-topic probability matrix, each row corresponds to a document vector and each column represents a topic with p(zn|wm) in each cell, in which znis the topic k, wmrepresents text document m and p(zn|wm) denotes the probability wmbelonging or discussing zn.A crucial issue in LDA modeling is to determine the number of latent topics K, which may impose an impact on the modeling results. Although several researchers have already suggested using the value of p(w|K) (Griffiths & Steyvers, 2004) or perplexity (Blei et al., 2003) to evaluate the topic modeling, to find the optimal topic number, it is still argued that the evaluation process often suffers from an over-fitting problem (Blei, 2012; Blei et al., 2003). In practice, we recommend using the 10-fold cross-validation of perplexity to avoid the possible over-fitting problem. The perplexity used by convention in language modeling, is monotonically decreasing in the likelihood of the test data, and is algebraically equivalent to the inverse of the geometric mean per-word likelihood (Blei et al., 2003), which is illustrated as:(8)perplexity(Dtest)=exp{−∑m=1Mlogp(wm)∑m=1MNm}where wmdenotes text document m and Nmrepresents the number of words in document m. Moreover, a lower perplexity score indicates better generalization performance.Specifically, to conduct the 10-fold cross-validation process, the entire document dataset has been randomly partitioned into 10 equal sized subsamples. Of the 10 subsamples, a single subsample is retained as the test dataset for testing the model, and the remaining 9 subsamples are used as training dataset. The cross-validation process is then repeated 10 times (the folds), with each of the 10 subsamples used exactly once as the test dataset. The 10 results of perplexity from the folds can then be averaged to produce a single evaluation of modeling effectiveness. The optimal topic number can be determined by the lowest average value of perplexity.Furthermore, given the topic number K, the total number of documents M, the average number of words in each document N and the maximum number of iterations of sampling process T, the time complexity for LDA Modeling by adopting Gibbs sampling is O(TKMN) (Porteous et al., 2008). When referring to the vocabulary size V, though we need to sample the latent assignment for each word appearance (not the unique word) in each document in Gibbs Sampling implying V<M*N, it is easy to derive that the time complexity for LDA Modeling can also be represented as O(TKV).Obviously, with large values of T, M and V, the LDA modeling would be a relatively time-consuming process, which however would not pose great influence on user search process described in Section 3.4. Though the user search process must be conducted immediately after a user query is submitted, the LDA modeling process (including the determination of optimal topic number) can be conducted offline. By assuming that the topics of the public comments on urban affairs are relatively stable, we could perform the LDA modeling process each time after a fixed time period (e.g., each week) or after certain burst events appearing (e.g., new policy announced) when searching is idle, merely matching results when users conduct a real search, largely reducing the computation burden. During the period before new modeling conducted, the document-to-topic probability vectors of new generated public comments with relative small volume can be easily estimated by the offline inference process (Blei et al., 2003).Based on the results of the periodic performance of LDA modeling, once a user has made a query request, the user search process described here could be designed to provide the end user with the results in which he or she is interested. The sub-steps involved in the user search stage are as follows:Sub-step 1: Query preprocessing.Sub-step 2: Keyword matching.Sub-step 3: Keyword suggestion.Sub-step 4: Topic selection.Sub-step 5: Search results generation.Normally, a user query appears as a list of several keywords or a complete sentence. In the latter situation, our approach would conduct text pre-processing operations similar to those in the above section, such as word segmentation and stop-word removal. Moreover, similar to the ontology-based method, normally only the words that possess actual meaning, such as nouns and verbs, will pose an important influence on semantics or meaning. Therefore, the part-of-speech (POS) tagging technique is used to label these keywords; meaningless words are then filtered (Allan & Raghavan, 2002; Toutanova & Manning, 2000). Next, after the user query is processed, a list of meaningful keywords from the query are used to conduct keyword matching based on the latent topics with the most likely words in each topic, which are generated by the LDA modeling process. Specifically, in this sub-step, we need to find all of the latent topics with thematic keywords that contain at least one of the meaningful keywords from the query.After keyword matching, if none of the topics are selected, the user will be returned a notification of empty results and a suggestion for query modification. Otherwise, for each possible semantically related topic, the first several thematic keywords with the highest probabilities will be recommended to the user, respectively. Then, the user can read the recommended keywords for each possible semantically related topic and select the appropriate topic or topics of interest.With the user-selected topics, based on the document-to-topic probability matrix and user-specific time period, two types of semantic results will be generated and provided to the end user: the picture of changing trends of semantic-related topics and a ranked list of documents for each semantic-related topic.In more detail, after we obtain all of the user's topics of interest, certain related statistical information (such as related-topics discussion hotness per day) can be calculated and provided for the end user. Specifically, to show the discussion hotness of these semantically related topics each day, the overall relevance of the documents to the user-selected topics—i.e., the daily thematic cumulative relevance—can be calculated. In general, given the total document set Diin day diand the topics selected by end user zCT, we can find the daily thematic cumulative relevance CumRel(zCT, di) as follows. For each document wi, its relevance to zCTcan be regarded as the summation of the probability of wiwith respect to each of the selected topics, which can be easily derived from the topic-document distribution matrix:(9)Rel(zCT|wi)=∑zn∈zCTp(zn|wi)Next, the daily thematic cumulative relevance CumRel(zCT, di) is represented as the summation on the total document set Diin day di:(10)CumRel(zCT,di)=∑wi∈DiRel(zCT|wi)=∑wi∈Di∑zn∈zCTp(zn|wi)Furthermore, the end user can limit the period of the search results to a particular interval, such asTI, when he or she submits the semantic query request. Accordingly, when the above results are limited to periodTI, they can generate a picture of changing trends in semantically related topics.In addition, by ranking all of the documents in periodTIbased on their thematic relevance to zCT, i.e., Rel(zCT|wi) and by selecting the top k documents, we can also provide the end user with a ranked list of semantically related documents. In particular, we add the user interaction sub-step (Step 4) to enable users to express their search intentions more accurately and to achieve a more flexible semantic search.This study's practical setting is an online platform to collect citizens’ opinions on urban public affairs in B-city, one of China's international metropolises. To reinforce communication between municipal administrators and citizens, to pay timely attention to citizens’ opinions and suggestions on urban public affairs, and to facilitate public participation in urban construction and development, in 2005 B-city's municipal government installed an opinion acquisition module related to urban public affairs on its official website. Since 2010, the module has been accessible through mobile terminals, in light of the popularity of mobile communications technology. Citizens can voice their opinions and recommendations about urban public affairs in the form of online comments at any time. Moreover, they can present comments and inquiries related to urban public service procedures along with reports and complaints about improper statements and actions by either government offices or by specific civil servants. The official website of B-city's municipal government notes the existence of a dedicated team responsible for processing citizen comments in a timely fashion, for allocating the items to related government departments according to their contents, and for supervising handling processes. Since it first went online, the platform has witnessed eight years of development, and its 240,000 documents of public comments constitute a set of large-scale data. A document here is defined as the text content a citizen submitted to the official website of B-city's municipal government as his or her opinions and recommendations about urban public affairs once.Confronted with this non-structured text information, the platform's administrators believe that a large amount of valuable knowledge is contained therein—for example, the issues that are matters of concern for the general public and the variance in the hotness of those problems due to policy promulgation, department reorganization, or even environmental change. However, non-structured text information and ambiguous expressions render administrators unable to derive the rules underlying citizen comments. Although the department has established a five-person team to classify and generalize the comments, understanding them remains difficult. Semantic analysis and semantic search will play significant roles in solving this problem.Therefore, B-city's municipal government, platform administrators and end users of the related information require a specific search tool that could meet the following requirements: (1) they need a system to conduct topic searches by keyword and to rapidly search a specific topic from the mass of comments; (2) in view of the difference in language expressions and wordings among various citizens, such a search should be able to implement the association of the meanings of citizen comments, rather than merely comparing whether the keywords appear; (3) with a view to the various meanings of some words, the system should be able to help searchers quickly choose the topics of concern by providing other auxiliary keywords when multi-vocal keywords appear; (4) comments that are subordinate to the hot topics should be retrievable, but it is also important to understand the longitudinal changing curves based on statistical data to measure and evaluate the occurrence and governance of the city's public affairs issues, or even the effectiveness of associated government sectors; and (5) because searches are directed to specific text databases with limited scales, the abovementioned objective must be achieved using fewer computing resource expenditures to their greatest ability and at an appropriate search speed.In our practical system implementation, the whole process for a user's search can be illustrated with a sequence diagram as following in Fig. 4.In such a search system, there are two main actors: the user and the administrator, whose use-case diagrams are shown in Fig. 5. The activity of an information query is launched by a user and three actions of keywords inputting, thematic keywords selecting and query resetting are involved (see Fig. 5(a)). As for the administrator, a special management activity is topic modeling, in which, two important resources of topic-keywords table and doc-to-topic matrix need to be maintained regularly (see Fig. 5(b)).In our semantic search system, to find the “optimal” topic number for the 24,000 documents of public comments, we followed the 10-fold cross-validation process with topic number from 20 to 400 (each 20 as an interval) to obtain the curves of perplexity changes with number of topics, which is illustrated in Fig. 6. In Fig. 6, “Test 1” to “Test 10” represents the result when the corresponding partitioned subsample was treated as the test dataset and “Average” is the averaged result for the whole dataset. It is shown that in most cases, the values of perplexity for Test 1 to Test 10 reach relatively low scores when the topic number is in the range of 100–200. In addition, the Average curve gets its lowest point in the case that the number of topics is 140. Therefore, the “theoretical optimal” number of topics for LDA modeling of these 24,000 documents should be 140. In addition, the vocabulary size of this model, i.e., the number of unique words after data preprocessing, is 314,305.Fig. 7shows the interface of our search system based on the probabilistic topic modeling approach. When an end user submits a semantic query with the keyword “公交 (i.e., Public Transportation)” to the system, by matching the keyword “公交” with the thematic keywords of the latent topics extracted by the LDA model, seven latent topics, each with ten suggested keywords, have been recommended to the end user to ask him to select real interest subtopics semantically related to “公交”.Next, further assume that the end user is indeed interested in public comments and discussions about the topic of “bus IC card recharge” problem and that he selects Topic 5. Thus, our proposed approach would first provide a picture of the longitudinal changing curves of topic-discussion hotness for the corresponding topic (i.e., bus IC card recharge), which is shown in Fig. 8.Moreover, based on the semantic relevance of each public comment with respect to the user-selected topic, i.e., bus IC card recharge, our proposed method could also provide the end user with a ranked list of public comments semantically related to the user-selected topic. Due to limited space, Fig. 9only lists the title of each semantically related comment with its issue date and relevance score to corresponding topic.The above words and figures show a complete semantic search process using an example. For users, the tool could provide an easy search that helps them to focus on certain topics of interest and to understand the latest hot topics; it also summarizes longitudinal changing trends over time.To validate the usefulness and effectiveness of our proposed semantic search method based on LDA modeling (SS-LDA), we conduct a user evaluation experiment to compare with the Keywords-Matching approach (KM), which is one of the most commonly used search methods in the domain of public management and public affairs. There must to point out that KM is not the current methods used by the target sector, the office of the website engaged in related work in manual mode before this study.The performance of the contrasting KM method is based on two principles: First, any valid search results provided by the KM method should contain all of the query or search keywords; Second, the ranking of each valid search result is determined by the well-known Okapi BM25 score between the document and the query or search keywords (Robertson, Walker, Jones, Hancock-Beaulieu, & Gatford, 1994). The Okapi BM25 is a bag-of-words retrieval function (i.e., not semantic search) that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship of the query terms within a document (e.g., their relative proximity), which has been usually used as one of most effective keyword-based search models (An & Huang, 2013; Dang, Luk, & Allan, 2014; Feuer, Savev, & Aslam, 2009; Lee, Seo, Jeon, & Rim, 2011; Whissell & Clarke, 2013; Zareh Bidoki, Ghodsnia, Yazdani, & Oroumchian, 2010). Okapi BM25 is not a single function, but instead an entire family of scoring functions with slightly different components and parameters. One of the most prominent representations of the Okapi BM25 function is shown in Eq. (11).(11)BM25(Q,di)=∑j∈Qtfij(k1+1)tfij+k1((1−b)+bdliavgdl)log(nnj)where Q is a query consisting of a set of keywords, diis the ith document in the document set, tfijrepresents the term frequency of the jth query keyword in the ith document, dlirepresents the ith document's length (i.e., number of words in the document), avgdl is the average document length for documents in the collection, and n and njare the number of all of the documents and the number of documents containing the jth query keyword, respectively. Moreover, b and k1 are parameters that are tuned with k1 ≥ 0 and 0 ≤ b ≤ 1. Typical values for the BM25 parameters in document retrieval are b = 0.75 and k1 = 2.0, and previous BM25 related papers have all used these default values (see the above BM25 papers); we adopt the same values in this user experiment.To conduct the user experiment in the practical environment of the search system, we invited ten administrative staff members from the department responsible for B-city's online citizen opinions acquisition platform as the subjects, and divided them into two groups. The five members of the first group work on the team whose routine task is the manual classification and generalization of citizens’ comments on the platform. Members of this group are considered to better understand citizens’ comments (due to their own related experiences) than the other group, which consists of five staff members in the same department who conduct other tasks.We asked each subject to perform 10 search tasks independently. Each search task is based on a typical and popular topic selected from the public affairs that are issues in B-city (see Table 1). For each search task, the subject can submit any keywords related to the corresponding topic, and the experiment system will return the top 10 search results by either our proposed SS-LDA method or by the KM method, which is essentially random and is not shown to the subjects. We need only guarantee that the numbers of the results provided by the two search methods are equal (i.e., 50 for SS-LDA and 50 for KM). Moreover, to maintain consistent result styles, the topic discussion-hotness changing curves (similar to Fig. 5) generated by SS-LDA are not provided to the subjects. In addition, to make the experiment process simple, for both search methods only the top 10 search results (i.e., the first result page) are provided.For each search task, after the subject read the search results, he/she was asked to evaluate the results with respect to information accuracy and information richness. The two questions were measured using a seven-point Likert-type scale, ranging from “very dissatisfied” (1) to “very satisfied” (7). The accuracy in our user experiment means the results quality of being true or correct. In this dimension, the subjects should pay attention to each result shown in the system correctly match the task topic or not, and conduct an overall judgement. The richness in our user experiment means the results quality of being comprehensive or completed. In this dimension, the subjects should pay attention to all results shown in the system is enough or not for the task topic, and conduct an overall judgement. In the questionnaire, we provide the clear definitions and need explanations of the two concepts. Therefore, although only the one question for each factor, the accuracy and richness, was measured using a seven-point Likert-type scale, ranging from “very dissatisfied” (1) to “very satisfied” (7), those overall judgements could represent the subject's evaluation on the results. The subject was not asked to compare the results of two methods (LDA versus KM), but asked to compare the results he/she saw in the system to the best matching results in his/her mind. In fact, in order to avoid possible interference, the subject did not know that the judgment result is calculated with what kind of method when he/she participate in the experiment.As shown in Table 2, the user evaluations of the results of SS-LDA method are higher than those of KM method both on information accuracy and information richness. According to the equal variance assumed (EVA) test, the differences between the two methods are more significant for information richness. Another noteworthy phenomenon is the impact on the subjects’ relevant experiences is that the mean scores given by the first group are higher than those given by the second group, indicating that staff who are more familiar with comments have higher demands for search tool ability.Although the second group of users is unable to distinguish the differences between the two methods, the first group of users confirms the more significant advantages of SS-LDA, which also shows that the advantage of LDA-related methods is their in-depth analysis of the text. We put the subjects into two groups experienced and non-experienced at first because the team whose routine task is the manual classification and generalization of citizens’ comments on the platform only have five people (experienced users). In order to appropriately increase the sample size, we invited to five staff members in the same department who conduct other tasks (non-experienced users) join the experiment. The experiment results show the experienced users confirm the more significant advantages of SS-LDA than non-experienced. One possible explanation is that the superiority of the LDA method may be reflected in a more professional situation. The conjecture remains to be further validation in the on-going research.The test results show the value of the search system for the departments. This system will soon be delivered to the related departments to enhance their efficiency in dealing with citizen comments.

@&#CONCLUSIONS@&#
In addressing municipal administrative departments’ semantic search requirements related to the contents of an official online platform for citizens to comment on urban public affairs, this study establishes a search process framework that includes and coordinates the user search process and the probabilistic topic modeling process. This paper also describes the technical details of that framework and presents an approach of orienting the search system toward hot topics. Although such a semantic search tool is not yet routinely employed in relevant departments, the user evaluation experiment shows that potential users are more satisfied by the search results returned by SS-LDA than those returned by the KM method. The rapid search for hot topics and the analysis of variation tendency enable decision-makers to collect from the online platform information that is more complicated and that relates to policy adjustment and urban governance. However, this paper's proposed solution is only validated for a special application scenario, and a deeper discussion of its general applicability remains to be performed in subsequent work.Broadly speaking, this paper also attempts to offer a new method for semantic search researchers that explore search solutions for specific domains or contexts (i.e., vertical search engines), rather than pursuing the widespread applicability of universal search tools (e.g., Google) in cross-media and cross-platforms. We believe that this will become a new area of growth for the technological development of semantic search because research and practices related to semantic search cannot be separated from analysis on its specific contents. Furthermore, the precision of the content analysis is obviously associated with the scenario and the context. Development of specific requirements can definitely facilitate better satisfying the end-users with the expected objective at the same technological level. We expect this paper can also provide helpful insights for subsequent, related studies.For future work, we anticipate incorporating the dynamic topic model (Blei & Lafferty, 2006) to our system to better demonstrate evolution of topics over time. In addition, the way of choosing and visualization of thematic keywords and corresponding topics in our semantic search system can be improved by more effective measures and methods (Chuang, Manning, & Heer, 2012; Sievert & Shirley, 2014).