@&#MAIN-TITLE@&#
Getting more from automatic transcripts for semi-supervised language modeling

@&#HIGHLIGHTS@&#
We analyze why semi-supervised backoff language modeling performs poorly.We motivate MAP adaptation of a log-linear language model.We use automatic transcripts as a prior for language model estimation.We show consistent reduction in WER across a range of low-resource conditions.

@&#KEYPHRASES@&#
Language modeling,Automatic speech recognition,LVCSR,Low-resource,

@&#ABSTRACT@&#
Many under-resourced languages such as Arabic diglossia or Hindi sub-dialects do not have sufficient in-domain text to build strong language models for use with automatic speech recognition (ASR). Semi-supervised language modeling uses a speech-to-text system to produce automatic transcripts from a large amount of in-domain audio typically to augment a small amount of manual transcripts. In contrast to the success of semi-supervised acoustic modeling, conventional language modeling techniques have provided only modest gains. This paper first explains the limitations of back-off language models due to their dependence on long-span n-grams, which are difficult to accurately estimate from automatic transcripts. From this analysis, we motivate a more robust use of the automatic counts as a prior over the estimated parameters of a log-linear language model. We demonstrate consistent gains for semi-supervised language models across a range of low-resource conditions.

@&#INTRODUCTION@&#
Most automatic speech recognition tasks take language modeling training data for granted. Voice search benefits from trillions of tokens of domain matched web queries. Broadcast news is well matched to newswire text and closed captions. However, for many other languages or domains, such as Arabic or Hindi diglossia, the only useful source of training data is expensive and time consuming in-domain manual transcription. While electronic resources may exist in the language, the wide gap in both vocabulary and word frequency limits the use of available newswire or web text for these conversational languages. Deploying a large vocabulary continuous speech recognition (LVCSR) system for an under-resourced language will require large investments in human labor and cost.One hope for overcoming this deployment burden is semi-supervised estimation of the component models of an LVCSR system: a small amount of in-domain transcripts are used in conjunction with a (typically) large amount of unlabeled audio. We assume that for any task that requires automatic speech recognition, there must be an abundance of audio in need of transcription. This audio has the potential to usefully augment the small amount of in-domain transcripts. The success of semi-supervised acoustic modeling demonstrated that as little as 1h of manual transcripts was sufficient to deploy an effective ASR system in a new domain (Ma and Schwartz, 2008). Yet the other half of the speech equation, language modeling, has not significantly benefited from semi-supervised methods. This article will explain the cause for limited previous successes and propose a framework for better exploiting available audio.Ideally, training corpora for language model estimation should be in-domain, copious and accurate. When one of the three are absent, then we are tasked with low-resource language modeling. Initial language modeling research assumed a small amount of in-domain and accurate transcripts were available. For instance, the Brown corpus (Kucera and Francis, 1967) is “only” 1M tokens. These corpora led to improvements in smoothing of unseen events with techniques such as Kneser-Ney smoothing (Kneser and Ney, 1995) outperforming simple methods such as absolute discounting. When more electronic corpora became prevalent, language modeling work considered copious, accurate, but no longer in-domain corpora under the umbrella of domain adaptation. The final combination of the three desiderata is a copious amount of in-domain, but now inaccurate, transcripts.This resource condition can arise when an automatic speech recognizer (or in-expert human transcriber (Novotney and Callison-burch, 2010)) produces transcripts with high error. Trained on a small amount of in-domain transcripts, the LVCSR system can quickly and inexpensively produce a large amount of n-gram counts from freely available in-domain audio. However, at higher error rates, the majority of these n-grams (as measured by type and token) are incorrect. As Section 3 will explore, the smoothing techniques and back-off language models standard in the research community are ill equipped to deal with such errors. The methods require accurate counts of the highest order n-grams (typically trigram) and are unable to adapt to accurate lower-order statistics. In low resource setting such as colloquial dialects, such resources may be scarce. Section 4 will motivate the use of a log-linear (a.k.a. maximum entropy) language model and a probabilistic use of automatic transcripts as a prior for adaptation.In this article, we make the following conclusions: (I) Back-off language models are poorly suited for learning from noisy transcripts. (II) Accurate higher-order n-gram counts are critical for semi-supervised language modeling. (III) Using automatic transcripts as a prior for language model adaptation results in a robust model, which gives consistent reductions in WER even when the available corpus is small.Prior work on semi-supervised language model estimation initially considered only back-off language models. The first reference in the literature decoded 17h of call center data with an initial model built from voicemail transcripts (Bacchiani and Roark, 2003). Unweighted n-gram counts from the automatic transcripts (at 20% WER) were used for MAP adaptation of a back-off language model. This resulted in a 4% absolute reduction in WER. However, self-adaptation of the test data did not result in further gains.Other work also reported limited success for self-adaptation. Call center data was again the target domain for adaptation (Gretter and Riccardi, 2001). n-gram expected counts were estimated from lattice posteriors, with all posterior scores below a threshold mapped to a common “unknown” token. This form of count thresholding recovered 0.8% absolute WER of the 2.2% possible.Semi-supervised language modeling was successfully combined with active learning (Nakano and Hazen, 2003). Experiments with an interactive dialog corpus used confidence estimates to select the most accurate utterances for inclusion in language modeling estimation. The least likely utterances were then manually transcribed, with a net savings in manual transcription cost and modest reduction in WER.Work in discriminative language modeling has used ASR output to find confusion neighborhoods (Xu et al., 2009). Lattices from Broadcast news audio were collapsed into confusion bins, which were then used to create sets of word confusions. These were applied to newswire text to create artificial training for the discriminative model. Other work used unsupervised estimates of hypothesis error rates for discriminative updates (Dikici and Saraclar, 2014). This achieved half of the gain possible with fully supervised transcripts.Log-linear language models were originally introduced to the NLP community as a method of self-adaptation of low-resource models (Della Pietra et al., 1992). The target model was adapted to unigram counts from one-best output for dictation. More commonly known in the speech literature as maximum entropy models (Rosenfeld, 1996), log-linear models are a flexible framework for incorporating a wide variety of features beyond n-grams. The closest work to this paper has applied Bayesian techniques to log-linear models for low-resource language modeling. Experiments with Estonian broadcast news combined 185M tokens of newswire with 104K tokens of in-domain transcripts (Alumae and Kurimo, 2010). Instead of interpolating the two corpora, one log-linear model was jointly used through hierarchical adaptation (Finkel and Manning, 2009). This idea was extended to domain adaptation of multiple broadcast news corpora (Sethy et al., 2013), resulting in better ASR performance than model interpolation.Recent work used the same estimation technique to combine 3000h of automatic transcripts with 60h of manual transcripts for discriminative language modeling (Tam and Vozila, 2011). They achieved a 2% relative reduction in WER with hierarchical adaptation over using just the 60h alone. Most importantly, they compared joint inference of both corpora (automatic and manual) to using the automatic corpus as a fixed background model for adaptation and found a small gain for joint inference.Our work differs by focusing on how best to use degraded automatic transcripts that arise in low-resource language modeling. While previous papers have visited low-resource language modeling and others have explored Bayesian techniques for language model adaptation, this paper joins the two methods with extensive empirical comparisons. The three previous papers most closely related to this article (Alumae and Kurimo, 2010; Tam and Vozila, 2011; Sethy et al., 2013) use either accurate out of domain corpora such as text or large amounts of in-domain transcripts. We compare the benefit of adaptation across a range of resource conditions – both of the background and target domains – and place the value of automatic transcripts in context. We also contrast Bayesian estimation with traditional back-off language models and explain the limited success with a variety of diagnostic experiments. These will show that back-off models are poorly suited for semi-supervised estimation as they are reliant on accurate highest order n-grams. This will explain the limited success of previous work in low-resource semi-supervised language modeling.We conduct experiments with the English Fisher corpus (Cieri et al., 2004), which consists of conversations between strangers about an assigned topic. These transcripts were provided by the Fisher QuickTrans effort (Kimball et al., 2004) with negligible error rate and so this paper does not explicitly model the manual transcription errors. While the target application is under-resourced languages, other language corpora lack the large audio necessary for the success of semi-supervised methods (Ma and Schwartz, 2008). Critically, the English Fisher corpus is also transcribed, allowing us to compare semi-supervised methods to more expensive fully supervised transcription. This data will be decoded with the initial LVCSR system and used to generate expected n-gram counts from the domain. The amount of in-domain transcripts will vary in this paper from 2.5 to 40h of manually transcribed audio. 400h of Fisher audio will also be treated as the unlabeled corpus. Since in reality it is manually transcribed, we will also be able to measure the performance of manual transcription on this set, which will serve as an upper bound on the performance of the semi-supervised training methods. We used the 3h NIST HUB-5 Dev04 corpus for evaluation. The vocabulary is a fixed 75K list which covers the Fisher corpus but not the test set (with an out of vocabulary (OOV) rate is 0.14%).For recognition we used the BYBLOS LVCSR system. The acoustic model is a multi-pass system that uses state-clustered Gaussian tied-mixture models (Prasad et al., 2005) using maximum likelihood parameter estimates. We found that the gains in this paper hold across both semi-supervised and discriminatively trained acoustic models. The language model will vary depending on the algorithm and will be fully detailed in later sections. We used MFCCs with normalized means and variances as well as Vocal Tract Length normalization. Decoding requires three passes: a forward and backward pass using triphone models produces an n-best list for re-scoring using quinphone acoustic models. These three steps are repeated after unsupervised speaker adaptation using constrained maximum likelihood regression. It is possible to build semi-supervised acoustic models (Ma and Schwartz, 2008) (and real world deployments should), but to control for conflating variables, this article does not.In addition to reporting the standard metrics of perplexity (PPL) and Word Error Rate (WER), we also report Recovery, a measure of semi-supervised learning effectiveness. The semi-supervised methods (which require no additional labeling) are contrasted with that of manually labeling the entire corpus. A semi-supervised experiment thus has three performance measures:•WERI– The WER of the initial models trained before semi-supervised training.WERS– The WER of the semi-supervised models after semi-supervised training.WERT– The WER of the supervised models trained with full supervision.WERRecovery=WERI−WERSWERI−WERTA WER Recovery of 100% states that semi-supervised training is as effective as supervised training. We report WER Recovery in addition to absolute gains since it is a valuable indicator for the usefulness of the semi-supervised methods on future domains. For key experimental results that are close in WER, we report significance using Matched Pairs Sentence-Segment Word Error (MAPSSWE) Test (Gillick and Cox, 1989). This is a t-test for estimating the mean difference of normal distributions with unknown variances. Unlike other statistical tests, The MAPSSWE test varies the sample length to ensure the validity of the independence of assumption. Instead of comparing at the utterance or word level, the test constructs sub-utterance phrases bounded by words correctly recognized by both systems.Estimating a language model requires specifying the empirical counts of word sequences from the domain. Under the supervised scenario, these counts are drawn from in-domain text. Now, under the semi-supervised scenario, audio can be used to also produce feature counts through some method. To map from audio to n-gram counts, the audio was decoded using the LVCSR system described above. The acoustic and language models were trained on the available in-domain data (from 2.5 to 40h depending on the condition). The decoder produced lattices with unlikely paths pruned. These lattices are then collapsed down to the sufficient statistics for an n-gram language model: expected counts of word sequences up to length n. For a word sequencew1,…,wn, the expected occurrence in an audio utterance X under the model P is(1)EP[w1,…,wn|X]=∑HP(H|X)c(w1,…,wn∈H)where H is a complete utterance hypothesis, P is the posterior probability of the hypothesis provided by the LVCSR system andc(w1,…,wn∈H)is the count ofw1,…,wnin H. Summing over all hypotheses and computing P(H|X) is efficiently done by the forward-backward algorithm over the lattice.Besides the entire expected count, one can limit count estimation to the one-best output from the recognizer. After all, if the recognizer was perfect, one would ignore the second-best hypothesis. Of course, this is not true and so one can weight the one-best output by its posterior probability. Additionally, previous work (Ma and Schwartz, 2008) has successfully used a confidence model to improve semi-supervised acoustic modeling.A confidence model estimates the probability of a word token in the one-best output being correct, which is different than the posterior probability of a word as computed from the lattice itself. It improves over posterior estimation since complementary information can be used which may be difficult to directly incorporate in the decoder.Our confidence model (Siu et al., 1997) is a logistic regression model which takes as input a variety of features. The probability of correctness that a word tokenwˆequals the reference wordwis defined as(2)P(wˆ=w)=exp(∑i=1Nλi·xi)1+exp(∑i=1nλi·xi)where there are 137 real valued features x1, …, xnthat include measurements of the instancewˆas well as lexical features ofwˆ, each of which has an estimated weight λi. These include 37 features such as lattice posterior probabilities, duration, signal to noise ratio, the number of times the word appeared in acoustic training, the number of triphones that appeared in training and many others taken from (Siu et al., 1997). Additionally, the top 100 words appear as binary indicator features, allowing for a word-specific bias. The set of predictive features were found using a greedy search that maximized likelihood on a small development set and remained constant throughout these experiments.The parameters of the model, Λ, are estimated via maximum likelihood training on a held-out set. As expected, the lattice posterior probabilities are the most predictive feature, but other useful features include the frequency of the word in training, average triphone coverage and phonetic length. Other models, such as neural nets, and additional features have given modest only gains over this robust recipe and so this confidence model is used here without further improvements.Once all tokenswin the lattice have estimated confidencesχ(w), the confidence-weighted count of an n-gramw1nis(3)cˆ(w1n)=∏j=1nχi(wj)so that the confidence of each instance ofw1nis simply the product of the individual word confidences.This incorrectly assumes that the probability of a word being correct is independent of its neighbors. Furthermore, it heavily discounts longer n-grams since the average n-gram token is proportional to the meanχ¯n. However, experiments with directly modeling n-gram confidences, with a separate GLM for each order and additional features, showed very modest improvements (Novotney et al., 2009) in confidence accuracy or improved semi-supervised language modeling. Other combinations, such as geometric or arithmetic mean, max and min were also empirically out-performed by the simple multiplication of Eq. (3). The next section will compare the benefit for using word confidence estimates over the word posteriors.The language modeling literature typically conflates count smoothing with model formulation. As noted in earlier work (Kneser and Ney, 1995), most smoothing methods for the conditional probability of wordwifollowing historywi−n+1i−1can be expressed (albeit clumsily) as(4)P(wi|wi−n+1i−1)=α(wi|wi−n+1i−1)ifc(wi−n+1i)>0γ(wi−n+1i−1)Pˆ(wi|wi−n+1i−2)ifc(wi−n+1i−1)=0where n is the order of the language model (n=3 for trigram),c(wij)is the count of a word sequencewijin the training corpus,α(wi|wi−n+1i−1)is the probability estimate when the word sequencewi−n+1iwas seen in the training data andγ(wi−n+1i−1)is the normalization factor so that the back-off estimateP(wi|wi−n+2i−1)is scaled appropriately forP(w1|wi−n+1i−1)to sum to one over all wordsw.The role of smoothing is then to estimate α's and γ's given the observed n-gram counts in the training data. Smoothing methods from Good-Turing to modified Kneser-Ney provide analytic solutions to these functions, meaning that a speech researcher can finesse the resulting language model only through modifying n-gram counts.It is not immediately obvious that one should move to a more complex model. Back-off language models with n-gram features offer competitive predictive power combined with simplicity of estimation. Additionally, they are easily incorporated into forward and backward recognition passes of an LVCSR system while language models which require additional information (such as topic) may be limited to n-best or lattice re-scoring. Before abandoning these models in later sections, we first re-affirm the conclusions of the previous literature with the standard back-off language model.190h of Fisher audio was decoded with an acoustic and language model trained on 10h of high quality manual transcripts at WER of 41.8%. The extracted counts were used to build a separate LM which was then interpolated with the initial LM from the 10h (100k tokens) using equal weighting of the two corpora. All language models used trigrams. Due to the fractional counts from the automatic transcripts, we used Witten-Bell smoothing, which is competitive with modified Kneser-Ney smoothing as measured by WER. Interpolation between the automatic transcript counts and 100k manually transcribed tokens was tuned on a small subset manual Fisher Transcripts. The optimal weights for the two corpora tended to give equal weights to both the background and foreground corpora. 3h Hub5.Dev04 test set was decoded using a fixed 10h acoustic model and the new estimated language model. We also built a language model on the full 200h (10+190) of manually supervised data. This became the upper bound for semi-supervised estimation when compared against estimation from manual transcription. The remainder of this section details the methods of semi-supervised estimation, extensive analysis and proposed attempts to improve semi-supervised back-off language models.This section experiments with using the full expected counts, using unweighted one-best output and using confidence weighted one-best output in Table 1. The best fair result used confidence-weighted counts from the one-best output and improved performance by 0.5% WER from 41.8% to 41.3% – which is only 7% of the possible gain for manually transcribing the 190h. The value of confidence weighted counts over posterior weighting was limited. Perplexity for the language model trained from the confidence counts was 139 versus 141 when using only the posterior. WER was almost identical and not statistically different. The remainder of the paper will report confidence weighted counts as they are generated as part of our standard decoding pipeline, but they are not critical to the techniques in this paper.Disappointingly, using the expected counts performs significantly worse than any method based on the one-best. Ideally, one would want to capture the model's full belief in the space of word distributions represented by the complete lattice posteriors. However, the degradation indicates that the low-resource recognizer is a very poor model of English conversational speech. The gain for confidence weighted one-best transcripts are statistically significant (p<0.001), while the unweighted counts are not significant (p>0.1).One clue as to why is revealed by measuring the recall of the n-gram types. This is the percentage of unique n-gram types in the reference also seen in the automatic counts. 56.63% of the trigram types seen in the reference appear in the one-best. Adding millions of additional trigram types seen in the lattice only increases the recall to 57.75%. The true word sequences uttered by the speakers appear so far down the list of alternate hypotheses that they are pruned out. Increasing the lattice size leads to a huge influx of hallucinated n-gram types and a marginal increase of reference types.The quality of the lattices is so poor for two reasons. First, the vast majority of words in the decoding dictionary are over-counted. A 75,000 vocabulary was used despite only having training samples for 5000 word types. The choice of a large vocabulary in recognition ensures that new content words will be added to the language model. However, the vocabulary was not carefully pruned as it is a stand in for a typical low-resource vocabulary, possibly scraped from the web. This means that the remaining 70,000 word types appear with equal probability in the language model. Many of these words are misspellings, inaccurate transcriber marks and otherwise “invalid” and should never occur in conversational speech. The huge space of words combine to create a massive space of n-grams, all which crowd out the rare valid n-grams. Future work could intelligently reduce the vocabulary size through unsupervised means with the goal of dramatically reducing the space of possible n-grams. Second, the posteriors produced by the recognizer were not optimized for estimating n-gram counts from a speech corpus. Instead, they were designed to minimize one-best error rate. One could directly attempt to improve the quality of the posteriors instead of using the standard LVCSR recipe.We measured the impact of improved lattice quality by generating new automatic transcripts with a stronger recognizer. A separate 200h of supervised Fisher data were used to build an acoustic and language model following the same recipe as the 10h setup. The vocabulary remained the same 75,000 words. We then decoded the 190h of unlabeled Fisher and extracted n-gram count estimates for combination with the 10h of manual transcripts. The WER using this 200h AM and LM system was 27.5% compared to 41.8% using the 10h AM and LM. Although the 200h of supervised data is available, we purposefully do not use it as the baseline since we want to directly compare the gain for improved lattices, not report the lowest possible WER in this section. We will refer to the lattices decoded at 27.5% WER as stronger and those decoded at 41.8% as weaker.Table 2reports the gains for semi-supervised modeling using these improved lattices. The same conclusions of Table 1 hold, but with a smaller gap in performance. Using full counts from the lattices (up to trigram) degrades performance over just using the 10h baseline. WER increases from 41.8% to 42.2% (using the better lattices) or 42.5% (using the weaker lattices). Using confidence weighted counts from the one-best reduces WER to 40.9% (stronger lattices) or 41.3% (weaker lattices). The underlying gain comes from posterior weighting, as the impact of confidence estimation is negligible as shown in Table 1. Notice the “squashing” effect of using automatic output for parameter estimation. The automatic output of the 190h improved from 41.8% WER to 27.5% – a reduction of 14.3% absolute. When used to estimate a new language model, this improved the WER on the held-out test set from 41.3% to 40.9% – only 0.4% absolute. This behavior has shown in prior work with semi-supervised acoustic modeling as well, where improvements in automatic transcription do not result in great improvements in parameter estimation (Novotney et al., 2009).With the availability of development data, one could optimize the posteriors to match the empirical frequencies, perhaps by minimizing KL divergence of the learned and empirical distributions. The goal is no longer interested in transcription accuracy, but in frequency accuracy. It is desirable that the recognizer predicts unigram, bigram and higher order statistics at the same rate as some amount of truth. In contrast to previous work with confidence estimation, this is no longer a token decision on a per-sample basis, but instead a type estimate across an entire corpus. This raises the difficulty of estimation as one can no longer make independence assumptions for each sample. The next section will explore the potential gain to be had for improved posterior estimates.Given the lackluster performance of back-off models, where should one invest effort to improve performance? As described earlier, the art of estimation with a back-off language model is in setting n-gram counts, so we now explore various oracles for count estimation. The methods explored in this subsection are only diagnostic – in reality we will not know any of these finer-grained oracles. The upper bound for posterior estimation is perfect confidences – an oracle provides the true accuracy for each word token in the one best. We then compute the confidence oracle for an n-gram by simply multiplying these binary scores. The language model is then trained on only the correctly recognized n-grams in the one-best output. Note, however, that this oracle does not include n-gram types unseen in the one-best, as this is only a word accuracy oracle. Table 3shows that nearly 40% of the potential gain could be recovered through perfect accuracy estimation.Instead of by token, we can ask for a type oracle. The difference is that we ask the oracle not whether a word sequence occurred in a specific region of time, but information about that word sequence in a whole corpus.The space of all n-gram types (V3+V2+V) can be partitioned into three different sets depending on their occurrence in the one best and reference transcripts of the 190h corpus:•Seen – Those appearing in the one best and reference transcripts of the 190h.Missed – Those not appearing in the one best, but occurring in the 190h reference transcript.Unknown – Those not occurring in either the one best or reference.We have two types of oracle information for this set. The Type oracle is a binary bit of information which tells us whether an n-gram occurred in the reference transcripts at all. This oracle divides Seenn-grams into two bins: hits and hallucinations. The hallucinations have their counts set to zero while the counts of the hits are uncorrected. For Missedn-grams, their counts are set to one.The Token count is a stronger oracle which tells us the true occurrence of an n-gram in the reference. In addition to knowing whether an n-gram occurred, we learn the sufficient statistics for n-gram language model estimation. Knowing the Token count for both the Seen and Missedn-grams will result in the reference language model.Table 4shows the improvements made for increasingly stronger oracle knowledge. First, knowing the Type oracle for Seenn-grams removes all hallucinated counts and recovers one third of the potential gain. If we then correct the counts of the remaining n-grams, we arrive at the true counts of all n-grams in the one best. This gets us to half of the potential gain (third column). The remaining gap is covered by inducing the Missedn-grams that appeared in the reference and providing their correct counts. This result is discouraging as only 56% of the n-grams in the reference appear in the one best. Some method must be able to extract the misses from the huge pot of possible n-grams. Furthermore, it is insufficient to set their counts to one as there is a substantial gain for providing counts other than one.We further break down the results by n-gram order in Table 5. First, we fixed only the unigram counts using the same oracles as Table 4. Due to the nature of back-off language models, there is no gain for knowing the correct count of individual words. We then fixed the unigram and bigram counts for a modest 0.5% reduction in WER. It is only when we correct all three orders up to trigrams that we see the large improvements described before – the action is all in the highest order. This is due to the choice of back-off models. Since smoothing methods were designed under the assumption that observed n-grams actually occurred, the back-off penalty or interpolating weight with lower order n-grams does not take trustworthiness into account. Thus when a higher order n-gram occurs in the training data, the lower order estimates are ignored.The previous subsection demonstrated that over half of the potential gain could be recovered by correcting the counts of trigrams seen in the 1-best output. In particular, setting the counts of incorrect trigrams to zero is a big part of the solution. This task of count regression is to then predict the number of occurrences in the reference transcript of an n-gram type given features observable from the ASR output. We used the observed weighted count of an n-gram in a corpus as well as the averages of the confidence features from Section 2 and the average confidence of each type.As an initial test, we estimated our regression model on a separate 200h of Fisher English with reference transcripts. After decoding both sets with a 10h LVCSR system, all n-gram types seen in the 200h of training were used to train an artificial neural network (ANN). Each n-gram type was a training instance where the target output was the normalized frequency of the n-gram in the 200h of reference and the inputs were 37 features described in Section 2. The ANN had one hidden layer, with 100 hidden units found to be optimal and was trained with back-propagation to minimize cross entropy.Performance was evaluated by computing the mean squared error between the predicted log frequency and true log frequency on the held-out 200h set. Hallucinations were given some small epsilon target. As described in Table 6, the ANN effectively reduces root mean squared error from 9.46 to 3.08. However, these reductions in RMSE did not carry over to perplexity or WER. Using the modified counts increased test set perplexity from 237 to 241 and failed to improve WER. We hypothesize that the mismatch between training criterion and end performance is due to the large imbalance of hallucinated n-grams, whose “target” count is zero.Table 7shows that nearly 75% of the trigrams seen in the 1-best output should have a true count of zero. Unigram hallucination rates are much lower and an ANN trained on just unigrams had much smaller variance between hits and hallucinations. In contrast, the trigram ANN had much higher variance on RMSE: 8.03 for hits versus 1.62 for hallucinations. This indicates that the trigram net learned to set the hallucinations near zero at a cost of accuracy for actual trigrams. Unfortunately, the improved unigram neural net did not benefit task performance. As described in Section 3.2, a back-off LM is dependent upon accurate trigram counts, so the better performing unigram neural net did not impact language model performance.Nonetheless, the balanced model led to attempts at different methods of unigram regression. Attempts at using various models (linear regression, negative binomial and Gaussian process models) and many variants of the target function (log, binned, frequency, raw counts) saw no real success for word regression. This leads to the conclusion that current methods lack observable features necessary to estimate whether a particular word is over or under generated in a large corpus of unlabeled speech. While singletons as a class are over generated, or long words tend to be right, the within class variance is so great that the model cannot predict for a specific word what its true count should be.Also known as a maximum entropy model, the log-linear family easily incorporates features beyond n-grams in an interpretable, probabilistic framework. Originally introduced in the speech community as a way to incorporate trigger features (Lau et al., 1993), log linear models have nicely incorporated a variety of features from topic (Khudanpur and Wu, 1999) to word classes (Goodman, 2001). However, none of these additional features have established themselves in the standard language modeling recipe book due to the modest gains over n-gram features. The reason for this modest gain is straightforward: previous work added additional features on top of a resource rich training corpus with plenty of in-domain and accurate n-grams. In this section, we will first apply the similar semi-supervised techniques from earlier in a Bayesian estimation of log-linear models.Like non-parametric language models, a log-linear language model is also typically a conditional model since it does not estimate probabilities of entire sentences. Instead, the outcome spaceVis a fixed vocabulary plus the end of sentence marker. The probability of a wordw∈Vpreceded by historyh∈His(5)P(w|h)=exp(∑k=1Kfk(h,w)·θk)∑w′∈Vexp(∑k=1Kfk(h,w′)·θk)where K feature functionsfk:H×V→Rand estimated model parametersΘ={θ1,θ2,…,θk}∈RKdefine the model. The separate distributions overVfor each history inHare tied together by the feature functions and their associated model parameters. Defining the feature functions is the art of the language model researcher. Typically, they include one feature function for each n-gram seen in a training corpus from unigrams, bigrams and on up. These feature functions can then be extended to include more complex information such as part of speech tags or topic information. Feature functions are typically binary valued and, importantly, can overlap. An observed trigram would also fire the unigram and bigram features, resulting in three features, each with separate parameters.While the space of possible feature functions is unbounded, only those constrained actually appear in the model. The second job of the language modeling researcher is to specify the target constraints of each feature function. This is the expected frequency of the feature in a corpus, typically set to the empirical frequencies derived from some training corpus.Under the original approach of maximum entropy models, parameter estimation sought to match the expected frequencies while minimizing the KL divergence with the distribution and the uniform through the Minimum Discriminant Information (MDI) criterion (Della Pietra et al., 1992). Since MDI will exactly match the target constraints, various techniques such as box constraints smoothed frequencies to reduce over-fitting to the training data.An alternative view more common in the NLP literature foregos the maximum entropy motivation and simply assumes we wish to use a log-linear model and seek an estimate of the model parameters, now a random variable Θ. This places us in a Bayesian setting which offers a more interpretable probabilistic framework for preventing over-fitting as well as incorporating semi-supervised counts. Since full Bayesian inference is impractical for log-linear models, we seek to find the maximum a posteriori probability or MAP estimate of Θ conditioned on some training data D.We start by placing a Gaussian prior over Θ. Gaussian priors (and others from the exponential family) are well suited for efficient estimation and empirically out-perform other priors (Chen and Rosenfeld, 1999). They have a direct relation with an L2 regularizer over the parameter space and penalize parameters which deviate too far from the mean of the prior, typically zero. The MAP estimate of Θ is found through(6)argmaxΘlogP(D|Θ)︸datalklhd+logP(Θ)︸prior.Placing a prior over the parameters requires the specification of two hyper-parameters for each parameter θj∈Θ: the mean μjand varianceσj2of the prior. Thusθj∼N(μj,σj2). While in principle each parameter θjcan have a separate mean and variance, in practice, these hyper-parameters are typically tied across all features. We will significantly explore the setting of these parameters shortly. With a specified μjandσj2for all j=1, 2, …, K, the solutions for each θjsatisfy(7)EPΘ[fj]=EP˜[fj]−θj−μjσj2which unfortunately does not have a closed form solution. It is solvable with techniques such as Newton's method or stochastic gradient descent which proceed in a round-robin fashion, updating parameter weights until acceptable convergence of held-out likelihood or some other criterion. Estimation requires computing the expected count under the model for each feature fkwhich is defined as(8)EPΘ[fk]=∑w∑hPΘ(h,w)fk(h,w)where we must sum over all wordswin the vocabularyVand all histories h. The naive implementation isO(V3)for a simple trigram model per iteration and is impractically slow. The first key speed up (Pietra et al., 1996) is to approximate the joint probabilityPΘ(h,w)with the conditional likelihood under the model multiplied by the empirical frequency of the history:PΘ(w|h)P˜(h). This removes the sum over all histories (orderV2)and reduces it to the seen histories from training data (order |D|). Second, we use a hierarchical training technique which reduces the training time per iteration to that of a standard back-off language model (Wu and Khudanpur, 2000). Finally, we take advantage of the hierarchical nature of n-gram features to encode the log-linear model into ARPA format (Wu and Khudanpur, 2000). This allows us to use the log-linear model not just in n-best re-scoring, but in the full forward and backward passes of the LVCSR decode and benefit from the improved model throughout decoding.Log linear language models provide a Bayesian framework for using counts from semi-supervised training data. Typically, the Gaussian prior over the parameters is centered at zero. This penalizes parameter weights that become too large or equivalently that move the model too far away from the uniform distribution. However, the prior does not need to be centered around zero. If we instead had reason to prefer some other point in the K-dimensional space that Θ lives, then the Gaussian prior would penalize models when they diverge from this initial point. Previous work (Alumae and Kurimo, 2010) (Sethy et al., 2013) used manually transcribed text or transcript sources in a similar adaptation framework. This section differs in two ways. First, we are primarily interested in measuring the benefit of automatic transcripts, which have the benefit of being “free”, but the danger of being erroneous as described in the previous section. Second, we do not jointly estimate a model for both the background and target corpora as in (Finkel and Manning, 2009) due to the small difference in performance for hierarchical estimation versus straightforward MAP adaptation (Tam and Vozila, 2011).Under the semi-supervised learning regime, there may be a small amount of in-domain manual transcripts to use in conjunction with a large amount of automated transcripts of lower quality. Both sets of data are untrustworthy for different reasons: the first is accurate, but under-sampled, while the second is large, but inaccurate. However, one trusts that n-grams in the small amount of in-domain transcripts actually occurred – regardless of anything else, the model should match the constraints from that data.The goal of MAP estimation is not to match the empirical target frequencies exactly, but rely instead on the principle of maximum entropy. In the absence of other domain data, one should fall back on a uniform distribution – a Gaussian centered at zero. The MAP estimate provides a compromise between the empirical frequencies and the uniform distribution. But now we have weak domain knowledge in the form of expected counts from the automatic transcripts. Whether one should adapt from the expected counts or to them is an empirical question that depends on their quality compared to the available in-domain data.Table 8shows that log-linear models are competitive with state of the art smoothing techniques across a variety of resource conditions. First, we compare the semi-supervised setup of Section 3 with a back-off or log-linear language model. Results for the back-off language model come from Table 5. All decodes used 10h of acoustic modeling as well as initial language modeling. The 190h unsupervised set from Section 3 was decoded with the respective initial 10h (100k token) LM. While supervised performance is identical for both the back-off and log-linear models (compare columns 1 and 4), MAP adaptation of a log-linear model provides a 0.7% absolute gain over interpolation with back-off language models. The two models generate statistically different hypotheses (p<0.001) for both the 10h and semi-supervised condition and (p<0.01) for the 200h supervised condition. Adapting the models to the true unigram counts (a cheating experiment) shows the potential of log-linear models. The back-off language model cannot be adapted to these lower order counts and shows no gain. In contrast, a round of MAP adaptation to these counts provides a sizable 1.6% absolute reduction in WER.Figs. 1 and 2give a visual overview of MAP adaptation and the experimental setup to follow. We evaluated the effectiveness of using expected counts as priors along two dimensions. First, the amount of in-domain English Fisher transcripts ranged from 2.5 to 40h (34K to 515K tokens). Note the 10h of training data in this section is a different random subset of Fisher than that used in Section 3. Second, the semi-supervised expected counts are placed between two extremes of background models. We used three different background corpora in total:•400h of English Fisher decoded with the 10h baseline system. Confidence-weighted n-gram counts were extracted from the one-best.Four million words of Broadcast News transcripts was used to represent a lower bound for many conversational corpora. Tokens were evenly sampled from the HUB-4 corpus (LDC97T22) which consists of a range of news programs from ABC, CNN and NPR collected in 1996. No special effort was made to ensure a low OOV rate on English Fisher or to condition data collection based on the available transcripts. The text was normalized by removing all punctuation, standardizing abbreviations and converting all words to upper case.Four million tokens of web data were selected to be conversational like using the entirety of the manual transcripts of Fisher. The resulting corpus contains web chats, television show transcripts and more. This reflects an upper bound on out of domain resources – targeted web transcripts (Bulyiko et al., 2003).The three background corpora are treated as static priors and we make a piece-wise comparison between them. To map from data to parameter priors on Θ, we first train a log-linear model on the background corpus. This log-linear model contains all unigrams, bigrams and trigrams seen in the 4M tokens of training data (350K unique features) be it broadcast news (BN), web text (Web) or automatic transcripts from the unlabeled audio. A Gaussian prior is placed over these learned parameters centered at zero with tied variance. The variance is tuned on a held-out set of target Fisher data to obtain the models ΘBN, ΘWeb, Θunsup.In the second step, the parameter vectors of this learned model now serves as the mean of a Gaussian prior over the target Fisher model ΘFisher. The background data is thrown away as it is incorporated through the prior weights ΘBN, ΘWeb, Θunsup, respectively. Instead of adapting from the uniform prior (Θ=0), we now train a log-linear model on the target Fisher data adapting from these weights. Both models, the prior mean and the Fisher language model, constrain features that are seen in the in-domain data (125K features for 40h). However, the unconstrained features (n-grams) which appear in the background data but do not appear in the target data will still fire in the adapted model. In the case where a feature appears in both corpora, MAP estimation will ensure that its feature weight is closer to the target in-domain data. The models contain the union of all n-grams that appear in the Fisher and background corpora. As for the initial background models, one variance is used for all features during adaptation, which is tuned on a small corpus. The variance tended to be relatively stationary across corpora, with it increasing as the amount of available in-domain data increased.Notice that the quality of the expected counts improves with the size of in-domain transcription. To generate them, a language and acoustic model was first built on the in-domain data. The WER of these models improved from 55% to 35% as they go from 2.5 to 40h. Then, the confidence weighted expected counts were extracted (Section 2). So although the audio corpus is fixed across the experimental condition, the extracted expected counts are different. For each different amount of in-domain data (2.5–40h), we estimated the adapted log-linear language model, encoded it in ARPA format and decoded the held out test set. Each LM was paired with an acoustic model trained on just the in-domain data alone. Semi-supervised acoustic modeling was not run and the vocabulary was fixed through all experiments. Table 9and Fig. 3details the results.We measured statistical significance using the MAPSSWE test described in Section 1.2. For each of the adapted experiments, rows 5 through 7, we compared the one best hypothesis to the better of the two baseline models. For example, for the 5h condition, Web→Target, with a WER of 40.2%, was measured against Web Text, with a WER of 40.9% and not Target, with a WER of 46.2% since Web Text was the stronger baseline. Conversely, BN→Target, with a WER of 44.6% was compared against Target (WER of 46.2%) and not Broadcast News (WER of 56.5%).There are a number of conclusions to draw. First, the broadcast news language model ΘBN is a terrible model of conversational speech. A language model built on 4M tokens has 8% higher WER (absolute) than just the 35k tokens of in-domain transcripts of the 2.5h Fisher corpus. Nonetheless, it provides complementary information to the in-domain transcripts. MAP adaptation from ΘBN to the in-domain data ΘFisher provides a 1% to 2% absolute gain over the in-domain data alone.At the other extreme, 4M tokens of conversational web text is an excellent model of Fisher. It is over 3% better (absolute) than 40h (500k tokens) of in-domain transcripts. But adapting from this better model to the worse in-domain models still provides a consistent benefit. The tuned variance allows the models not to wander too far from an accurate prior. Under this standard domain adaptation setting, MAP estimation provides a robust procedure which gives the best of both worlds.Lying in between these two corpora are the semi-supervised results. Unlike the two previous text corpora, this corpus is not fixed as the amount of in-domain data increases along the x axis. Note again that the quality of the expected transcripts improves with more in-domain transcripts. The higher quality results in a better estimated semi-supervised model. Unlike the broadcast news corpus, which tapers off in value, the expected counts provide a consistent reduction in WER ranging from 2.2% to 1.1% even as the amount of labeled data grows from 2.5 to 40h. When lacking relevant out of domain text corpora, acoustic evidence is useful in improving a low-resource language model across a range of resource conditions.Lastly, we validated the conclusions of Section 3 by contrasting the performance of a maximum entropy model with a back-off model using Witten-Bell smoothing. As demonstrated in Table 8, MAP adaptation of a maximum entropy model achieved greater gains for incorporating counts from automatic transcripts than a back-off language model. Using 190h of confidence-weighted n-gram counts from automatic transcripts and 10h of manually transcribed n-grams, a back-off language model with Kneser-Ney smoothing achieved a WER of 41.3% versus 40.6% for a maximum entropy model.We repeated the experiments of Table 9 for the 5h condition in Table 10. With supervised corpora, the two language models achieve the same performance. Compare the WER when estimating a language model from broadcast news, web text or in-domain fisher. The performance of the two models is statistically insignificant. However, the maximum entropy model out-performs the back-off model when adapting from weak domains.When adapting from the broadcast news to the 5h of in-domain transcripts, decoding with the maximum entropy LM achieves a WER of 44.6% compared to 45.8% using the back-off language model. Likewise, adapting from automatic transcripts shows the same trends, with a WER of 44.4% with the maximum entropy LM versus 46.5% for the back-off LM. Both comparisons are statistically significant with p<0.05. Combining the automatic transcripts with the in-domain data with the backoff model actually degraded performance. Using just the target data for LM estimation had a WER of 45.1%, which degraded to 46.5% with the automatic transcripts. The poor quality of the language model estimated from the automatic transcripts alone, with a WER of 48.9%, hints at the difficulty in accurate estimation of backoff language models.This trend does not hold when the strong web background data is available. Both models give near identical results. This may be due to the minimal impact the additional 5h of language modeling text over the already strong web data. But when relevant text corpora are lacking for a speech domain, automatically generated transcripts are a useful source of language model training data. This highly error full data cannot be treated as simple text, but must incorporate uncertainty through the posterior counts. MAP adaptation of a maximum entropy language model is a principled way of combining this data source with limited amounts of in-domain data.

@&#CONCLUSIONS@&#
