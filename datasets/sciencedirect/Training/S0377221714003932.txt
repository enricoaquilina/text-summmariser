@&#MAIN-TITLE@&#
Exploring trust region method for the solution of logit-based stochastic user equilibrium problem

@&#HIGHLIGHTS@&#
Showing that the trust region Steihaug-Toint (TRST) algorithm is inappropriate.The proposal of the modified trust region (MTRN) algorithm.The proof of the convergence and superlinear convergence rate of MRTN.Implementation issue on computation of the trial step.The proposal of the basic route choice principle.

@&#KEYPHRASES@&#
(B) Transportation,Modified trust region Newton algorithm,Steihaug-Toint method,Stochastic user equilibrium,Basic route choice principle,

@&#ABSTRACT@&#
In this research paper, we explored using the trust region method to solve the logit-based SUE problem. We proposed a modified trust region Newton (MTRN) algorithm for this problem. When solving the trust region SUE subproblem, we showed that applying the well-known Steihaug-Toint method is inappropriate, since it may make the convergence rate of the major iteration very slow in the early stage of the computation. To overcome this drawback, a modified Steihaug-Toint method was proposed. We proved the convergence of our MTRN algorithm and showed its convergence rate is superlinear.For the implication of our algorithm, we proposed an important principle on how to select the basic route for each OD pair. We indicated that it is a crucial principle to accelerate the convergence rate of the minor iteration (i.e. trust region subproblem-solving iteration). In this study, other implication issues for the SUE problem are also considered, including the computation of the trial step and the strategy to ensure strict feasibility iteration point. We compared the MTRN algorithm with the Gradient Projection (GP) algorithm on the Sioux Falls network. Some results of numerical analysis are also reported.

@&#INTRODUCTION@&#
There has been considerable interest in recent years in stochastic user equilibrium (SUE) problem for traffic assignment. In the literature, SUE was proposed to relax the unrealistic assumption behind the user equilibrium (UE) problem, which states that people have perfect knowledge of network conditions. Daganzo and Sheffi (1977) first defined SUE principle. They assume that people have different perception errors when selecting routes, and at the SUE equilibrium, no traveler believes he can improve his travel time by unilaterally changing routes.The most widely studied SUE models are the logit model and the probit model. The logit model assumes that people’s perception error follows a logistic distribution (Akamatsu, 1996; Bell, 1995; Chen & Alfa, 1991; Dial, 1971; Fisk, 1980; Leurent, 1997). The probit model assumes that people’s perception error has a normal distribution (Daganzo & Sheffi, 1977; Sheffi & Powell, 1982). Theoretically, each model have advantages and disadvantages. Logit model has well-known weaknesses such as their inability to take proper account of overlapping or correlated paths. However, it has an advantage of analytical simplicity. Probit model is more behaviorally appealing than logit model, but suffers the disadvantage either of requiring Monte Carlo techniques or of complete path enumeration. In the literature, the logit model has enjoyed much greater attention than the probit model. It is widely used not only for theoretical investigation (Ghatee & Hashemi, 2009; Guo, Yang, & Liu, 2010; Zhou, Chen, & Wong, 2009), but also for practical implementation (Aros-Vera, Marianov, & Mitchell, 2012; García-Ródenas & Marín, 2009; Haase & Müller, 2014). In this study, we will concentrate on the logit model.Generally speaking, solution algorithms for the logit based SUE problem can be divided into two classes: link-based algorithms and path-based algorithms. Link-based algorithms do not need explicit path enumeration. They only assume an implicit path choice set, such as the use of all efficient paths (Dial, 2001; Maher, 1998), or all cyclic and acyclic paths (Akamatsu, 1996; Bell, 1995). On the other hand, path-based algorithms require explicit choosing a subset of feasible paths prior to the assignment. Therefore, a large variety of methods can be used to generate a more realistic path choice set. For different types of path choice set generation methods, we can refer to Azevedo, Santos Costa, Silvestre Madera, and Vieira Martins (1993), Bekhor, Ben-Akiva, and Ramming (2006), Ben-Akiva, Bergman, Daly, and Ramaswamy (1984), Cascetta, Nuzzolo, Russo, and Vitetta (1996), and De la Barra, Perez, and Anez (1993)This paper concentrates on the path-based algorithms for the logit SUE model. Bekhor and Toledo (2005) proposed using the Gradient Projection (GP) method (Bertsekas, 1999) to solve this problem. In their study, the gradient of the objective function is projected on a linear manifold of the equality constraints, with the scaling matrix being diagonal elements of the Hessian. Note that algorithm GP still retains a linear convergence rate, it could be slow as it is approaching the optimal solution. Hence, we wonder to know whether methods that enjoy superlinear convergence rate (such as the Newton type method) can further improve the computation efficiency. At first sight, this is obvious, since faster convergence rate undoubtedly results in less computation time. However, it is not the case. As we know, superlinear convergence rate is a local property, only if the iteration point lies within a neighborhood of the optimal solution can we obtain such rate of convergence. But we do not know the optimal SUE solution a priori, hence we cannot find an initial point that is near the solution before finishing the computation. Practical routine for choosing the initial point for the SUE problem is to obtain a logit type route flow pattern using free-flow travel times. Therefore, when this initial point is far from the optimal solution, the computation time may be high, and most of the computing time is consumed in the early stages of the iteration, at which the iteration point is far from the optimal solution.As we know, trust region method is an important and efficient method in non-linear optimization. In this study, we will propose a modified trust region Newton (MTRN) algorithm to solve the logit-based SUE problem. Logit-based SUE problem is in essence a linear equality constraint optimization problem.We will first use the variable reduction method to transform it into an unconstrained one, and then use the trust region Newton method to solve it. However, when solving the trust region SUE subproblem, the well-known Steihaug-Toint method may be sometimes inappropriate. The reason is that in the early stage of iteration, the trust region radius should be adjusted so that the next iteration point satisfies the non-negative constraint. Hence it is usually very small. For small trust region radius, the Steihaug-Toint method usually terminates at the first iteration, which makes the trial step it generates only along the Cauchy direction (i.e., the steepest descent direction). Therefore, in the early iteration stage, sometimes the Steihaug-Toint method may be very slow and should not be used. Alternatively, a modified Steihaug-Toint method is proposed. This method does not suffer from the disadvantage that Steihaug-Toint method retains. It is a very efficient method for the SUE problem.When applying the modified trust region Newton (MTRN) algorithm to the SUE problem, several practical issues are studied in our research, including the computation of the trial step, the strategy to ensure strict feasibility iteration point, and the principle of the choice of the basic route. These issues are very important to generate a fast and robust solution algorithm for the SUE problem.The rest of this paper is structured as follows: In Section 2, we briefly outline trust region Steihaug-Toint (TRST) algorithm for the linear equality constraint optimization problem. In Section 3, we propose the modified trust region Newton algorithm (MTRN) for logit-based SUE problem. In Section 4, the MTRN algorithm and the GP algorithm are tested and compared on the Sioux Falls network. In Section 4, we provide conclusions and suggestions for future work.Consider the problem in the form:(1)[P1]minimizef(x)(2)subjecttoAx=bwhere f is twice continuously differentiable, and A is an m×n matrix of full row rank. We further assume for convenience that f is strictly convex, which guarantees that there is a global minimizer of [P1].Letx¯0be any feasible point for [P1], and Z be an n×(n−m) basis matrix for the null space of A. It is well known that any other feasible point can be expressed as(3)x=x¯0+Zywhere y is an (n−m)-dimensional vector.Therefore, [P1] is equivalent to the following unconstrained problem:(4)[P2]minimizey∈Rn-mϕ(y)=f(x¯0+Zy)In this paper, we call f(x) the original objective function. Its argument x is the original variable. Correspondingly, ϕ(y) and its argument y are called the reduced objective function and reduced variable, respectively.If g≜∇f(x) and H≜∇2f(x) are the gradient and the Hessian matrix of f, we can define the reduced gradient and the reduced Hessian matrix of f by the following expression:(5)g̃≜g̃(y)=∇ϕ(y)=ZTf(x)(6)H∼≜H∼(y)=∇2ϕ(y)=ZT∇2f(x)ZBy assumption, the objective function f(x) is strictly convex, then its Hessian matrix H is positive definite. For any non-zero vector y, (Zy)TH(Zy)>0, implies yT(ZHZ)y>0. Therefore, the reduced Hessian matrixH∼=ZHZis also positive definite. As a result, we conclude that the reduced objective function ϕ(y) is strictly convex, and [P2] is also a convex optimization problem.We now briefly describe the trust region Steihaug-Toint (TRST) algorithm to solve [P2]. This algorithm consists of two phases. The major iteration phase applies the trust region framework to [P2], and creates a trust region subproblem. The minor iteration phase uses the Steihaug-Toint method to solve the subproblem approximately.Let ykbe the current iteration point, we first define a region around yk:(7)Bk=y∈Rn-m|‖y-yk‖k⩽Δkwhere Δkis the trust region radius, and‖·‖kis an iteration dependent norm.Then we build a quadratic model that approximate the objective function ϕ(y) around yk, and choose a step to be the approximate minimizer of the model within the trust-region. That is, we seek a solution of the following trust region subproblem:(8)minimizep∈Rn-mmk(p)=ϕ(yk)+g̃kTp+12pTH∼kp(9)subjectto‖p‖Mk⩽Δkwhere(10)‖p‖Mk=pTMkpand Mkis a symmetric positive definite matrix that depend on iteration number k.Define the ratio(11)ρk=ϕ(yk)-ϕ(yk+pk)mk(0)-mk(pk)In (11), the numerator is called the actual reduction, and the denominator is called the predicted reduction. This ratio measures the agreement between the actual reduction in ϕ and the predicted reduction in mk.If ρkis sufficient positive, the trial step is accepted and the trust region is expanded or kept the same for the next iteration. If ρkis not sufficient positive, the trial step is rejected and the trust region is contracted.Details of the major iteration of the TRST algorithm can be described as follows:Algorithm 1(major iteration: basic trust-region algorithm)Step 0. Given initial point y0, initial trust region radius Δ0, and constants ε>0, 0<η1⩽η2<1,0<γ2<1<γ1. Set k=0.Step 1. If ykis a sufficiently accurate approximation to the minimizer of [P2], stop.Step 2. Approximately solve the subproblem (8)–(10) for pk.Step 3. Compute ρkby (11), setyk+1=yk+pkifρk⩾η1ykotherwiseStep 4. SetΔk+1=γ1Δkifρk⩾η2,Δkifρk∈[η1,η2,γ2Δkifρk<η1.Step 5. Updateg̃kandH∼k, set k=k+1, and go to Step 1.To compute the model decrease mk(0)−mk(pk), it is unwise to compute both mk(0) and mk(pk) and then subtract them. Since when both values are very large but their difference is small, numerical rounding can cause significant cancelation errors. From (8) we recognize that(12)mk(pk)=mk(0)+g̃kTpk+12pkTH∼kpkwhereg̃kandH∼kare reduced gradient and the reduced Hessian at yk. Therefore, the model decrease can be computed directly by using (12).The most well known algorithm for large scale trust region subproblem is the Steihaug-Toint method (Steihaug, 1983; Toint, 1981). This method uses the preconditioned conjugate gradient method to generate a sequence of iteration points zjj=0, 1, …, and proposes three termination rules.When some iteration point satisfies one of the termination rules, the method terminates, and the corresponding trial step can be calculated. For the convex trust region subproblem (8)–(10), only two termination rules are possibly encountered:Firstly, if the inner iterate point zj+1 violates the trust region bound, the algorithm terminates. The trial step pkis obtained by intersecting the current search direction with the trust region boundary. This is done in Step 2 of Algorithm 2 (see below). Secondly, if zj+1 satisfies the convergence tolerance ηkand remains within the trust region, we terminate the algorithm and return the step pkobtained by zj+1. This is performed in Step 3 of Algorithm 2.In the following, we describe Algorithm 2, the Steihaug-Toint method. This method is the minor iteration of the TRST algorithm. It is used in Step 2 of Algorithm 1 for solving the trust-region subproblem. We use bjto denote the conjugate search direction and zjto denote the iterate that it generates.rj≜H∼kzj+g̃kis the gradient of (8) evaluated at each zj. αjand βjare scalars that are used to determine zjand bj, respectively.Algorithm 2(minor iteration: the Steihaug-Toint method for Step 2 of Algorithm 1) Given preconditioner Mk, trust region radius Δk, and tolerance ηkStep 1.Setz0=0,r0=g̃kSolve Mkv0=r0, for v0Set b0=−v0Set j=0.Step 2.Setαj=rjTvj/bjTH∼kbjSet zj+1=zj+αjbjIf‖zj+1‖Mk⩾Δk, compute σjas the positive root ofzj+σbjMk=Δk,return pk=zj+σjbj, and terminate.Step 3.Setrj+1=rj+αjH∼kbj,If‖rj+1‖/‖g̃k‖<εk, return pk=zj+1 and terminate.Solve Mkvj+1=rj+1 for vj+1Setβj+1=rj+1Tvj+1/rjTvjSet bj+1=−vj+1+βj+1bj.Set j=j+1Go to Step 2.1.The choice of the tolerance ηkat each call to Algorithm 2 is very important. Steihaug (1983) showed that by choosing ηk→0, the TRST algorithm can achieve superlinear rate of convergence. In this paper, the tolerance sequence are chosen to be (Nocedal & Wright, 2006, p. 168)(13)ηk=minρ,‖g̃k‖where ρ is a parameter that is usually less than 1. In Eq. (13), ρ controls the convergence rate in the early iteration stage,‖g̃k‖controls the convergence rate in the late iteration stage. Both of them are critical to the efficiency of the algorithm.In Step 2, when the norm of zj+1 violate the trust region radius, we should compute the positive root ofzj+σbjMk=Δk.Observe that‖zj+σbj‖Mk2=‖zj‖Mk2+2σ·zjTMkbj+α2‖bj‖Mk2Therefore, the positive root of‖zj+σbj‖Mk=Δkcan be obtained by(14)σj=-zjTMkbj+zjTMkbj2+‖bj‖Mk2Δk2-‖zj‖Mk2‖bj‖Mk2At each stage of the Steihaug-Toint method, the reduced Hessian matrixH∼kneed not be formed explicitly. The method requires only matrix–vector products, that is, products of the formH∼kbfor some vector b. In Section 4, we will use this feature to simplify the calculation process of the logit-based SUE problem.Consider a network G(N,A), where N is the set of nodes and A is the set of links in the network. The link travel time function ta(xa),a∈A, is assumed to be continuous and differentiable. Let W be the set of all OD pairs in the network, Rwbe the set of routes between OD pair w∈W and dwbe the travel demand between OD pair w∈W. We assume that ∣W∣=m and ∣Rw∣=nwfor all w∈W, where m and nware positive integers. Therefore, each OD pair w∈W can be indexed by a positive integer from {1, 2, …, m}, and each path r∈Rwcan be indexed by a positive integer from {1, 2, …, nw}.For a route r∈Rwconnecting OD pair w∈W, the route flow is denoted asfrw. Clearly, the route flow and the link flow has the relationshipxa=∑w=1m∑r=1nwfrwδarw,∀a∈A, where the route-link incidence indicatorδarw=1if route r between OD pair w uses link a andδarw=0otherwise.The logit-based SUE can be expressed as the following minimization problem (Fisk, 1980):(15)minZ(f)=∑a∈A∫0xata(x)dx+1θ∑w=1m∑r=1nwfrwlnfrw(16)∑r=1nwfrw=dw∀w∈{1,…,m}(17)frw⩾0∀r∈{1,2,…,nw},w∈1,2,…,m(18)xa=∑w=1m∑r=1nwfrwδarw∀a∈Awhere the parameter θ, reflects an aggregate measure of the people’s perception of travel costs. From Fisk (1980), the objective function Z is strictly convex with respect to f. Hence, equilibrium route flows are unique. It is well known that the logit model assigns strict positive flows to all paths in the choice set. Therefore, the non-negative constraint is not binding at the optimal solution. We can ignore the non-negative constraints and the logit-based SUE problem is essentially equivalent to the equality constraints minimization problem [P1].As we know, the TRST algorithm converges very fast when the initial iteration point is within a neighborhood of the optimal SUE solution. But in reality, we do not know the optimal solution a priori. Therefore, the initial iteration point is usually obtained by performing a logit assignment using the free-flow travel times. When this initial iteration point is far from the optimal SUE solution, in the early stage of iteration, the non-negative constraint (17) may be violated, and the trust region radius should be adjusted so that each iterate is strictly feasible. Hence, in the early iteration stage, the actual trust region radius is usually much smaller than needed.From Algorithm 2, we know that the Steihaug-Toint method is basically unconcerned with the trust region until it blunders into its boundary and stops. This is rather unfortunate, particularly when the trust region radius is small, since considerable experience has shown that the Steihaug-Toint method will be terminated in the first iteration. From Algorithm 2, we also know that if the Steihaug-Toint method terminates in the first iteration, the generated trial step is along the scaled steep descent direction (with the scaling matrix being the preconditioner), which only uses the gradient information. In some cases when the preconditioner is not very good (especially at the early iteration stage), this direction may make the algorithm very slow.To avoid this drawback, we propose a modified version of the Steihaug-Toint method for the minor iteration. When computing the trial step, our method will be more likely to take the curvature information into account. Therefore, it is usually faster than the Steihaug-Toint method in the early iteration stage.We now present the main idea of our algorithm. All the relevance is concerned with solving the trust region subproblem (8)–(10) at major iteration k.Firstly, we consider the trust region subproblem in the absence of the constraint, i.e., the following unconstrained convex quadratic problem.(19)minimizep∈Rn-mmk(p)=g̃kTp+12pTH∼kpwe use the preconditioned conjugate gradient method to solve the above problem. When far from the SUE solution, this problem is approximately solved. Only when the SUE solution is approached should we solve (19) more accurately. Let lkbe the corresponding approximate solution to the above problem, From Theorem 1 (see below), we know that lkalso indicates a descent direction that is gradient related tog̃k.Secondly, along the direction indicated by lk, we calculate the minimizer of problem (19). That is, we should solve the following problem:(20)ming̃kTp+12pTH∼kp:p∈[lk]where [lk] is the space spanned by lk.Let(21)p=tlkDefine(22)h(t)=tg̃kTlk+12t2lkTH∼klkthenh′(t)=g̃kTlk+tlkTH∼klkLet h′(t)=0, we obtaintmin=-g̃kTlklkTH∼klkand(23)pmin=-g̃kTlklkTH∼klklkFinally when lkand pmin have been calculated, we obtain the trial step pkby one of the following six ways:(24)1.If‖pmin‖Mk⩽‖lk‖Mk⩽Δk,setpk=lk.(25)2.If‖pmin‖Mk⩽Δk⩽‖lk‖Mk,setpk=pmin.(26)3.IfΔk⩽‖pmin‖Mk⩽‖lk‖Mk,setpk=Δk‖lk‖Mklk.(27)4.If‖lk‖Mk⩽‖pmin‖Mk⩽Δk,setpk=lk.(28)5.If‖lk‖Mk⩽Δk⩽‖pmin‖Mk,setpk=lk.(29)6.ifΔk⩽‖lk‖Mk⩽‖pmin‖Mk,setpk=Δk‖lk‖MklkWe will see from Theorem 2 that such choice of trial step will ensure the convergence of our algorithm.Details of the modified Steihaug-Toint method for Step 2 of Algorithm 1 are described in Algorithm 3. In this paper, we will refer to Algorithms 1 and 3 as the modified trust region Newton (MTRN) algorithm.Algorithm 3(minor iteration: the modified Steihaug-Toint method for Step 2 of Algorithm 1)Given preconditioner Mk, trust region radius Δk.Define toleranceηk=minρ,‖g̃k‖;Step 1.Setz0=0,r0=g̃kSolve Mkv0=r0 for v0Set b0=−v0Set j=0.Step 2.Setαj=rjTvj/bjTH∼kbjSet zj+1=zj+αjbjSetrj+1=rj+αjH∼kbjIf‖rj+1‖/‖g̃k‖<ηk,Set lk=zj+1, calculate pkby (24)–(29) and terminate.Else continue with Step 3.Step 3.Solve Mkvj+1=rj+1 for vj+1Setβj+1=rj+1Tvj+1/rjTvjSet bj+1=−vj+1+βj+1bjSet j=j+1Go to Step 2.In Steihaug-Toint method, each iteration point zjis generated and trialed at the same time, once zjencounters the trust region boundary, we terminate the method. The final trial step is given either by intersecting the current search direction (indicated by the current iteration point zj) with the trust-region boundary or by some iteration point that satisfies the convergence tolerance defined by ηk. In our modified Steihaug-Toint method, we first generate an iteration point to satisfy the convergence tolerance ηk. Then we obtain the final trial step along the direction indicated by this point.When near the optimal solution, the trust region bound becomes inactive, both TRST algorithm and MTRN algorithm reduce to the inexact Newton method. When far from the solution, as is analyzed above, the Steihaug-Toint method (minor iteration of the TRST) usually terminates in the first iteration, which only exploits the gradient information of the objective function. But our modified Steihaug-Toint (minor iteration of the MTRN) does not suffer from such drawback, since we first compute the search direction (indicated by some iteration point) with tolerance ηk, and this may be done by more than one iterations. Therefore, we believe that our MTRN algorithm utilizes more information about the curvature in the early stage of the iteration. This is very crucial when solving the logit-based SUE problem, since it can accelerate the convergence rate in the early iteration stage.The following three theorems show the basic convergence results of the MTRN algorithm. Theorem 1 demonstrates that the direction indicated by the approximate solution point of Algorithm 3 is gradient related. Theorem 2 establishes the global convergence behavior of MTRN. Theorem 3 shows that the convergence rate of MTRN is superlinear if the relative residualsrk/g̃in Algorithm 3 converge to zero. Details of the proof of these theorems are in Appendix A.Theorem 1Let lkbe the approximate solution point ofAlgorithm 3. Then there exists a constant c>0, such that(30)lkTg̃k<-c‖lk‖‖g̃k‖Let y∗be the unique optimal solution of [P2] and pkbe the trial step given by(24)–(29). Then the sequence of iterates{yk}generated byAlgorithm 1satisfieslimk→∞‖g̃k‖=0andlimk→∞yk=y∗.Let y∗be the unique optimal solution of [P2] and pkbe the trial step given by(24)–(29). Suppose that the sequenceykconverges to y∗. If the relative residual satisfies(31)‖rk‖=‖H∼klk+g̃k‖=o(‖g̃k‖)ask→∞then the trust-region bound Δkis bounded away from zero, and the sequence {yk} converges superlinearly to y∗.For each OD pair, we choose one route as the basic route. The rest of the routes are non-basic routes. In order to be consistent with the statement in Section 2, the flows on the basic routes are considered as the basic variables, the flows on non-basic routes are considered the reduced (non-basic) variables, and the original variable is composed of the basic variables and the reduced variables.We rearrange the route indices so that the last route of this OD pair is the basic route. That is, after rearranging, the basic route for OD pair w∈W is denoted by the positive number nw, and the non-basic routes are denoted by the positive numbers 1, 2, …, nw−1. Therefore, the basic variable can be denoted byfnww, w∈{1, 2, …, m}, the reduced (non-basic) variables are denoted byfrw,w∈1,2,…,m,∀r∈{1,2,…,nw-1}.After expressing the basic variable in terms of the reduced (non-basic) variables, the transformation of variables is given by (for notational convenience we introduce variableyrw):(32)frw=yrww=1,2,…,m,r=1,2,…,nw-1fnww=dw-∑r=1nw-1frw=dw-∑r=1nw-1yrww=1,2,…,mThen the reduced objective function can be written as:(33)minZ̃(y)=∑a∈A∫0xata(x)dx+1θ∑w=1m∑r=1nw-1yrwlnyrw+1θ∑w=1mdw-∑r=1nw-1yrwlndw-∑r=1nw-1yrw(34)yrw⩾0∀r∈1,2,…,nw-1,w∈{1,2,…,m}Elements of the reduced gradient are given by(35)∂Z̃∂yrw=∑a∈Ata(xa)·δarw-δanww+1θlnyrw-lndw-∑r=1nw-1yrwElements of the reduced Hessian matrix are:(36)∂2Z̃∂yrw∂yr̃w̃=∑a∈A∂ta∂xa·δar̃w̃-δanw̃w̃δarw-δanww+1θ1yrw·δrr̃ww̃+1dw-∑r=1nw-1yrw·δww̃Indicatorδrr̃ww̃is equal to 1 ifr=r̃,w=w̃, and 0 otherwise and indicatorδww̃is equal to 1 ifw=w̃, and 0 otherwise.As is mentioned before, for major iteration k, when solving the trust region subproblem, we only need to compute the matrix–vector product for some vector b(37)u=H∼k·bIn (37), b is any vector whose dimension equals the total number of the reduced path flow variables.The coordinates of the vector u can be calculated by:urw=∑a∈A∂ta∂xa∑w̃=1m∑r̃=1nw̃-1br̃w̃·δar̃w̃-∑w̃=1m∑r̃=1nw̃-1br̃w̃·δanw̃w̃δarw-δanww+1θ·1yrw·brw+1θ1dw-∑r=1nw-1yrw·∑r̃=1nw-1br̃w(38)∀r∈{1,2,…,nw-1},w∈{1,2,…,m}Remark 4DefineTa(xa,b)=∂ta∂xa∑w̃=1m∑r̃=1nw̃-1br̃w̃·δar̃w̃-∑w̃=1m∑r̃=1nw̃-1br̃w̃·δanw̃w̃. It is a function of link flow xaand the current vector b. Clearly, Ta(xa,b) can also be regarded as a kind of generalized link travel time function. Therefore, (38) can be further simplified to:urw=Lrw-Lnww+1θ·1yrw·brw+1θ1dw-∑r=1nw-1yrw·∑r̃=1nw-1br̃w(39)∀r∈{1,2,…,nw-1},w∈{1,2,…,m}whereLrwandLnwware the generalized travel time of route r and nw, respectively, given by:(40)Lrw=∑a∈ATa(xa,b)δarw(41)Lnww=∑a∈ATa(xa,b)δanwwThe diagonal elements of the reduced Hessian matrix are(42)∂2Z̃∂yrw2=∑a∈A∂ta∂xa·δarw-δanww2+1θ1yrw-1dw-∑r=1nw-1yrwThe preconditioner matrix can be formed by using the diagonal elements of the reduced Hessian matrix:(43)M̃k=∂2Z̃∂y1120…00∂2Z̃∂y212…0…………00…∂2Z̃∂ynm-1m2Obviously, withM̃kdefined in this way, it can be easily inverted. Details of the derivation of (35), (36), (38) and (42) are in Appendix B.Up to now, all the ingredients required in Algorithm 3 are available. The trial step can be obtained by iteratively performing Algorithm 3.It is deserved to mention that for the logit-based SUE problem, although the final SUE solution satisfies the non-negativity constraints, some of them may be violated in the iteration process. Therefore, the trial step pkshould also be adjusted so that the non-negativity constraints are satisfied, i.e., fk+1=fk+pkshould be feasible for all iterations. This can be done by examining whether the following two constraints are satisfied (for the sake of clarity, we omit the major iteration subscript k):(44)yrw+prw>0∀r∈{1,2,…,nw-1},w∈{1,2,…,m}(45)∑r=1nw-1(yrw+prw)<dw∀r∈{1,2,…,nw-1},w∈{1,2,…,m}If they are true, we turn to Step 3 and Step 4 of Algorithm 1. If any one of the above constraints is not satisfied, we can first contract the trust region radius Δkby a factor γ3, 0<γ3<1. That is, we setΔk′=γ3Δk. And then, we use (24)–(29) to recompute the trial step for this new trust region radius. The above procedure should be repeated until the trial step satisfies (44) and (45).The choice of the reduced variable plays a very crucial role in the solution of the trust region subproblem. Obviously, once the basic variable is determined, the reduced variable can also be determined. Therefore, in this subsection, we will equivalently focus on how to choose the basic variable, i.e., which route should be chosen as the basic route.Form Algorithm 3, we know that in each major iteration, the preconditioned conjugate gradient method is used to solve the unconstrained convex quadratic problem (19). As is stated in Nocedal and Wright (2006), the convergence behavior of the conjugate gradient method is strongly dependent on the conditioning ofH∼k. The larger the condition number, the slower the likely convergence of the method. For the logit-based SUE problem, it is obvious that a different choice of the basic route flow variables will result in a differentH∼k. Therefore, to make the condition number ofH∼kas small as possible, which route should be chosen as the basic route is a very important issue. Next, we give a simple example to illustrate how to select the basic route.Example 1Consider the grid network in Fig. 1. There are 9 nodes and 12 links in the network, with only one OD pair between node 1 and node 9. The OD demand is 100. There are six different routes connecting O and D.The link travel time function is defined as follows:(46)ta(va)=ta01+βvaCanwhereta0,Ca,ta(va), respectively, are link a’s free-flow travel time, capacity and travel time with flow va. Further, β,n are deterministic parameters. In this example, we set β=0.6, n=4. We assume that Ca=100 for all links. For links (4,5), (5,6) and (7,8),ta0=1, and for the remaining links,ta0=2.In this example, there are 6 route flow variables. The initial flow is given byf¯=[16.67, 16.67, 16.67, 16.67, 16.67, 16.67]T, and 8 iterations are needed to get the optimal solution. For each iteration point, if route i (i=1, …, 6) is chosen as the basic route, the reduced Hessian matrix corresponding to this route can be formed, and its condition number can be calculated. For abbreviation, we use “the condition number for route i” to express the above meaning. Table 1shows the condition number for each route in each iteration.Inspection of Table 1 reveals that at the iteration point whose elements are all bounded away from zero (e.g. x1, x2), the condition number for each route is comparable. For example, at x2, each routes’ condition number is 12.29, 53.85, 17.84, 18.13, 17.44, 12.45. Clearly, these condition numbers have the same order of magnitude. At the iteration point who has nearly zero elements (e.g. x4, x5, x6), the condition number for the nearly zero flow route is much larger than that of the non-zero zero flow route. For example, at x6, the flow on route 2 is 0.12, which is nearly zero. The condition number for route 2 is 397.28, which is much larger than the condition number of other routes.Therefore, we conclude that in order to make the convergence of the minor iteration faster, at each major iteration, we should choose the basic route as the one whose flow is bounded away from zero. Note that this conclusion is based on the simple network in Example 1. For other more complicated networks, our numerical experiments indicate that similar conclusion can also be reached.Followed from the findings of Example 1, we now give a practical principle for choosing basic variables in the logit-based SUE problem.Basic route choice principle. We first let θ be a large enough constant, and use the path-based MSA (Sheffi & Powell, 1982) algorithm to give an approximate warm-up solution to the SUE problem. Then for whatever value of θ, the basic route can be set as the one that corresponds to the maximum flow route of each OD pair at the warm-up MSA solution. Note that in this principle, once the basic routes are selected, they are unchanged in all the major iterations.In other words, letfrw|w=1,2,…,m,r=1,2,…,nwMSAbe the warm-up MSA solution point, the basic route indices nwshould satisfy(47)nw=argmaxr=1,2,…,nwfrwMSA,w=1,2,…,mWe now describe the motivation behind this basic route choice principle. When the current iteration point is far from the optimal solution, a reasonable approximate solution to the unconstrained convex quadratic problem (19) is enough. Hence, only a few minor iterations are needed. When the current iteration point is near the optimal solution, problem (19) should be solved more accurately to make the convergence rate superlinear. Therefore, many more minor iterations should be performed. Note that the superlinear convergent property can only be attained within a neighborhood of the optimal solution, and many more minor iterations should be performed within this neighborhood. Hence, we will only focus on the iteration point that is near the optimal SUE solution.Furthermore, from Example 1, we know that at each iteration point when selecting the basic route, we should avoid selecting the route whose flow is nearly zero. Note that fkconverges to the SUE solutionfSUEwhen k→∞ (i.e.,limk→∞fk=fSUE). If at the SUE solution, the flow on some route is far from zero, then at the point that is near the SUE solution, the flow on this route is far from zero, too. Based on the above analysis, we will finally concentrate only on the SUE solution point. At this point, for each OD pair, we can select any route whose flow is bounded away from zero, and define this route as the basic route. Such choice of basic route can make the condition number of the reduced Hessian matrix as small as possible, for each iteration point that near the optimal SUE solution.However, we do not know the SUE solution a priori, so we do not know which route eventually has non-zero flow. Fortunately, with the help of the warm-up MSA solution, we can find such route easily. Suppose that we select the basic route to be the one that corresponds to the maximum flow route at the warm-up MSA solution. When solving a logit-based SUE problem, we are possibly encounter three cases: (1) θ→0+, (2) θ→+∞, (3) θ∈(0, +∞). Note that the parameter θ is used to calibrate people’s perception of travel cost. We now consider the three cases separately:Case 1:θ→+∞. This indicates that people’s perception error is very small, and they will tend to choose minimum cost routes. Therefore, the SUE solution is analogous to that of the warm-up MSA solution. Following from the basic route choice principle, at the SUE solution, for OD pair w∈{1, 2, …, m}, the flow on the basic route isfbasic1w≜fnww≈maxfrwMSA. Clearly, This flow is bounded away from zero.θ→0+. This indicates that people’s perception error is very large. They will choose routes with considerable larger actual costs than the least cost ones. In this case, the SUE solution is analogous to the case that all route flows for the same OD pair are the same, in other words, for OD pair w∈{1, 2, …, m}, the flow on the basic route isfbasic2w≜fnww≈dw/nw, which is non-zero too.θ∈(0, +∞). Assume that θ increases from 0 to +∞, thus people’s perception error decreases, and more people will transfer to the least cost routes. By definition, the basic route is one of the least cost routes when people’s perception is accurate. Therefore, as θ increases from 0 to +∞, for OD pair w∈{1, 2, …, m}, the final SUE flow on the basic route will increase fromfbasic2wtofbasic1w, which is also away from zero. It is worth noting that our numerical experiences show that the final flow on the basic route is not increasing monotonically with θ, but the global trend will be increasing.In all three cases, the basic route choice principle can guarantee the flow on the basic route at the SUE solution is bounded away from zero. So it is a proper principle for selecting the basic route.Remark 51. In practice, the path-based MSA algorithm is very easy to implement. It can give the approximate SUE solution in only a few iterations, and such an approximate solution is good enough to determine the basic route for each OD pair.2. The basic route choice principle can only ensure the conditioning of the problem (19) is good within a neighborhood of the optimal solution. If the current iteration point is far from the optimal solution, this cannot be guaranteed. In some cases, this drawback may deteriorate the performance of the algorithm. We should solve (19) with more accurately level to alleviate this drawback.In this section, we present some performance comparison between algorithm MTRN, TRST and GP. The three algorithms are tested on the well-known Sioux Falls network. This network is taken from Bar-Gera (2012). It consists of 76 links, 24 nodes and 528 OD pairs. The routes are generated prior to the traffic assignment. We use a combination of the link elimination method (Azevedo et al., 1993) and link penalty method (De la Barra et al., 1993) to generate the working route set. For the Sioux Falls network, the average number of generated routes is 7.3 per OD pair, and the maximum number of generated routes is 11 for any OD pair. Our computer programs are coded in MATLAB and executed on a notebook computer.For all of the computation instances in this section, the initial point is obtained by performing a logit assignment using the free-flow travel times.To specify the convergence criterion, for Algorithm 1, we use the norm of the reduced gradient to evaluate whether a feasible route flow pattern is close to the global minimum of [P2]. That is, if Step 1 of Algorithm 1 satisfies‖g̃k‖<ε, then it is terminated. We will specify ε=10−5 for numerical experiment. For Algorithms 2 and 3, as is seen before, we terminate them if the residual rj+1 satisfiesrj+1/‖g̃k‖<ηk, whereηk=minρ,‖g̃k‖. The parameter ρ controls the convergence rate in early iteration stage, it is critical to the efficiency of the algorithm. Therefore, different values of ρ should be used in order to make comparison. In the following experiment, we will useρ=0.1,0.2,0.3,0.4and ρ=1/k (k is the major iteration number) to test algorithm MTRN and TRST.To evaluate the performance of different algorithms, we use the following convergence measure proposed by Leurent (1997):(48)lnZkZ∗-1where Zkis the value of the objective function at the kth iteration, and Z∗ is the optimal value of the objective function.We now compare the convergence characteristics of algorithm MTRN, TRST and GP. Fig. 2compares the performance of MTRN and GP in terms of iteration numbers and CPU times. Fig. 3compares the performance of TRST and GP with the same measure. In this test case, the dispersion parameter θ is assumed to be 0.7. From Figs. 2 and 3, the following findings can be made:Firstly, we should note that Figs. 2 and 3 are only concerned with the major iteration number. The minor iteration number for each major iteration is not included because of space limitation. An algorithm with fewer major iterations does not necessary mean it spends less CPU times. For example, in Fig. 2(a), the major iteration number of MTRN for ρ=1/k is less than that for ρ=0.4, but in Fig. 2(b), the CPU times for ρ=1/k are higher than that for ρ=0.4. This is because there may be a chance that in an iterative procedure, the major iteration number is small, but the minor iteration number is quite large, which makes the computation time very high. Therefore, to assess the algorithm performance, CPU time is a more appropriate measure than iteration number. Later, we will mainly use CPU times to compare the three algorithms.Secondly, we can see that when near the optimal solution, the slope of the curve for algorithm MTRN and TRST are quite similar, and both are much steeper than that of GP. This is because when approaching the optimal solution,‖g̃k‖will control the convergence of the minor iteration. Since‖g̃k‖→0as k→∞, both MTRN and TRST are locally superlinear convergent. On the other hand, the convergence rate for GP is only linear. Therefore, MTRN and TRST will perform much better than GP within a neighborhood of the optimal solution.Now, we compare the three algorithms in terms of the CPU times. As is illustrated in Fig. 2(b), on the whole, the performance of MTRN and GP are comparable in the early stage of iteration, but MTRN performs much faster than GP in the late iteration stage. However, from Fig. 3(b), another phenomenon can be observed: TRST is much inferior to GP no matter what ρ is chosen, since it converges very slowly in the early stage of iteration. The reason for this phenomenon has already been discussed in Section 3.2. Therefore, for the logit-based SUE problem, MTRN is more appropriate than TRST.To give sensitivity tests, algorithm MTRN and GP are compared with the following two factors: (1) the level of demand. (2) the dispersion parameter. As analyzed above, in some cases algorithm TRST may be unacceptably slow. Therefore, it is not a robust algorithm and we will omit it from the comparison. In the following, we mainly use CPU times to compare algorithm MTRN and GP. For completeness, we still report the analysis results with respect to the iteration numbers.The iteration numbers and CPU times by different demand levels for algorithm GP and MTRN are illustrated Fig. 4. As is presented by this figure, generally speaking, there is a global trend that the required number of iterations and CPU times increase as the demand factor increases.We first compare MTRN with GP. From Fig. 4(b), we can observe that when the demand factor is small, GP is more efficient than MTRN. When the demand factor is large, MTRN is superior to GP when ρ=0.1, 0.2 and 0.3. This indicates that algorithm GP is suitable for less congested cases, while algorithm MTRN is preferable for heavily congested cases. Note that more heavily congested problems are harder to solve than less congested problems. Our MTRN algorithm is able to handle these difficult problems well.We now show the impact of ρ on the performance of MTRN in detail. From Fig. 4(b), we see that when the demand factor is small, the performance of MTRN for all ρ are similar. When the demand factor is large, MTRN with ρ=0.1, 0.2, 0.3 performs better than that with 1/k, and we can easily find that ρ=0.4 is the most inefficient parameter.The main reason for this phenomenon is that if the congestion level is high, the initial iteration point is much farther from the optimal solution than the case when the congestion level is low. Therefore, the parameter ρ will be more critical for the performance of the algorithm. For algorithm MTRN, in the early iteration stage, sometimes the conditioning of problem (19) may be poor. Therefore, we need to solve (19) in a relatively precise manner (e.g. if we let ρ=0.1, 0.2 and 0.3), so as to obtain a more suitable search direction. Although solving (19) approximately (e.g. if we let ρ=0.4) will save the computation time for each minor iteration, it will deteriorate the search direction, and hence the total CPU times may be very large.It should also be noted that solving the problem (19) with over precision (e.g. if we let ρ=1/k) in the early iteration stage can also be inefficient, especially when the demand is high. This is because ρ=1/k means solving (19) more exactly as k increases. If the iteration number is large, the accuracy level defined by 1/k is much higher than necessary, and hence the algorithm time will be long.Fig. 5presents the iteration numbers and CPU times required by different dispersion parameter θ for algorithm MTRN and GP. As is shown in Fig. 5(a), the iteration number increases with the value of θ. When comparing the two algorithms in terms of CPU times, we can observe from Fig. 5(b) that MTRN is superior to GP for each θ when ρ is chosen to be 0.1, 0.2, 0.3. However, when ρ=0.4 and θ⩾0.9 MTRN suddenly gets much worse than GP. When ρ=1/k and θ=1.5, MTRN is also a little inferior to GP. These two phenomena have already been observed in the demand sensitivity analysis, and the reasons are similar.Therefore, we conclude that MTRN can outperform GP for all values of θ if the parameter ρ is properly selected.

@&#CONCLUSIONS@&#
In this study, we investigated applying the trust region method to solve the logit-based SUE problem We showed that using the well-known trust region Steihaug-Toint (TRST) algorithm to solve this problem may be very slow in the early iteration stage, and proposed a modified trust region Newton (MTRN) algorithm to overcome this drawback.We compared the MTRN algorithm with the Gradient Projection (GP) algorithm on the Sioux Falls network. Numerical results showed that MTRN is especially suitable for the case when the congestion levels are high, or when people’s perception error is relatively small, which are both actually the most difficult cases to solve.In this paper, we considered using Newton type methods to solve the logit-based SUE problem. Although the convergence rate is very fast, the need to compute the Hessian matrix makes our algorithm a little complicated in implementation. On the other hand, quasi Newton type methods need not compute the Hessian, and can still maintain a fast rate of convergence. Therefore, we may use this type of methods to solve the logit-based SUE problem. This is a topic for further research.Another area for continued research is to apply our MTRN algorithm to the more general route choice models, such as the Cross-Nested Logit model and the generalized extreme value (GEV) family of models.