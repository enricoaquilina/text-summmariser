@&#MAIN-TITLE@&#
New approximate dynamic programming algorithms for large-scale undiscounted Markov decision processes and their application to optimize a production and distribution system

@&#HIGHLIGHTS@&#
We propose new approximate dynamic programming algorithms.These algorithms can solve large-scale undiscounted Markov decision processes.Optimal control problems for production systems with 35, 973, 840 states were solved.The kanban, base stock, CONWIP, hybrid and extended kanban systems are considered.We show numerical comparisons between optimal controls and optimized pull controls.

@&#KEYPHRASES@&#
Approximate dynamic programming algorithms,Undiscounted Markov decision processes,The curses of dimensionality,JIT-based production and distribution system,Optimal control,

@&#ABSTRACT@&#
Undiscounted Markov decision processes (UMDP's) can formulate optimal stochastic control problems that minimize the expected total cost per period for various systems. We propose new approximate dynamic programming (ADP) algorithms for large-scale UMDP's that can solve the curses of dimensionality. These algorithms, called simulation-based modified policy iteration (SBMPI) algorithms, are extensions of the simulation-based modified policy iteration method (SBMPIM) (Ohno, 2011) for optimal control problems of multistage JIT-based production and distribution systems with stochastic demand and production capacity. The main new concepts of the SBMPI algorithms are that the simulation-based policy evaluation step of the SBMPIM is replaced by the partial policy evaluation step of the modified policy iteration method (MPIM) and that the algorithms starts from the expected total cost per period and relative value estimated by simulating the system under a reasonable initial policy.For numerical comparisons, the optimal control problem of the three-stage JIT-based production and distribution system with stochastic demand and production capacity is formulated as a UMDP. The demand distribution is changed from a shifted binomial distribution in Ohno (2011) to a Poisson distribution and near-optimal policies of the optimal control problems with 35,973,840 states are computed by the SBMPI algorithms and the SBMPIM. The computational result shows that the SBMPI algorithms are at least 100 times faster than the SBMPIM in solving the numerical problems and are robust with respect to initial policies. Numerical examples are solved to show an effectiveness of the near optimal control utilizing the SBMPI algorithms compared with optimized pull systems with optimal parameters computed utilizing the SBOS (simulation-based optimal solutions) from Ohno (2011).

@&#INTRODUCTION@&#
Markov decision processes (MDP's) can formulate optimal stochastic control problems in various systems including production, inventory and distribution systems, supply chain systems, communication systems, risk management systems, queuing systems and so on. Puterman (1994) explained systematically all the exact algorithms of MDP's, that is, the value iteration method (VIM), policy iteration method (PIM), modified policy iteration method (MPIM) and linear programming (LP). However, the curses of dimensionality, as is well-known since Bellman (1957), have hindered computing an exact optimal policy for a large-scale MDP. This is because the computational requirements of the MDP are proportional to the number of elements in its state space and the state space grows exponentially in the number of state variables. In facts, Ohno and Ichiki (1987) numerically showed the relative efficiency of the MPIM for a controlled three-stage tandem queuing system with 3779 states but could not extend it to a four-stage system at that time.In view of the reliance on dynamic programming and neural network concepts, Bertsekas and Tsutsuklis (1996, p. 8) used the name “neuro-dynamic programming (NDP)” to describe collectively all the methods in their book to overcome the curses of dimensionality. The NDP may include the reinforcement learning (Sutton & Barto, 1998) and the approximate dynamic programming (ADP) (Powell, 2007; Si, Barto, Powell, & Wunsch, 2004). In the field of reinforcement learning, Das, Gosavi, Mahadevan, and Marchalleck (1999) proposed the semi-Markov average reward technique (SMART) algorithm for solving an undiscounted semi-Markov decision process (USMDP) and applied it to a maintenance problem. Moreover, He, Fu, and Marcus (2000) proposed the simulation-based policy iteration (SBPI) algorithm for solving an undiscounted Markov decision process (UMDP) and applied it to an inventory problem, where the UMDP minimizes the expected total cost per period. However, Ohno, Yashima, and Ito (2003) showed that it is not possible to apply these two algorithms to find an optimal policy for a single-stage just-in-time (JIT)-based production system with 144 states that the MPIM can solve easily. Ohno (2011) applied those algorithms and λ-SMART algorithm (Gosavi, Bandla, & Das, 2002) for a similar single-stage JIT-based production system with 108 states and indicated that they found an optimal action in no more than one of the three ergodic, or recurrent states of the JIT-based production system under the optimal policy obtained utilizing the MPIM. In addition, it was shown that the kanban system is superior to SMART, λ-SMART and SBPI more than 3 times when set with optimal numbers of kanbans. It is to be noted that the kanban system has the ability to control stochastic systems as stated by Rule 5 of Kanban Rules (Monden, 2012, pp. 45–51).Since the above-mentioned NDP algorithms couldn't find the optimal policy of the single-stage JIT-based production system, Ohno et al. (2003) proposed the simulation-based modified policy iteration method (SBMPIM) for finding a near optimal policy of the multi-stage JIT-based production system with stochastic capacity and demand and showed that the SBMPIM can compute the exact optimal policy for the single-stage system with 144 states and also a near optimal policy for a two-stage system with 12,150 states which is superior by more than 7 percent to the optimally set kanban system. Ohno and Ito (2004) improved the first version of the SBMPIM so that it could solve a three-stage system with 1,428,840 states and made numerical comparisons between near optimal controls computed using the SBMPIM and four types of pull systems whose parameters are set optimally. These pull systems are kanban (Monden, 2012), base stock (Clark & Scarf, 1960), CONWIP (Spearman, Woodruff, & Hopp, 1990) and hybrid (Bonvik, Couch, & Gershwin, 1997) systems. Ohno (2011) extended the second version of the SBMPIM so that it could solve a three-stage JIT-based production and distribution system with 26,091,520 states, where the number 42,398,720 in Ohno (2011) was wrong and proposed the simulation-based optimal setting (SBOS) algorithm for finding the optimal parameters of pull systems and conducted numerical comparisons between near optimal controls computed using the SBMPIM and the five types of pull systems adding the extended kanban system (Dallery & Liberpoulos, 2000) with optimal parameters computed using the SBOS algorithm. It was shown that the near optimal controls by the SBMPIM are, for example, superior by from 3 to 15 percent to the optimally set base stock systems for five different cases.In recent years, Cao (2007), Chang, Fu, Hu, and Marcus (2007), Gosavi (2003), Powell (2007), Si et al. (2004) and Buşoniu, Babuška, Schutter, and Ernst (2010) have published technical books about ADP or NDP algorithms for solving the curses of dimensionality. Powell and Ma (2011) also reviewed ADP algorithms with continuous state space. Most of the ADP or NDP algorithms explained in those books and the review, however, are not for UMDP's but for finite-horizon or discounted MDP's. Katanyukul, Duff, and Chong (2011) compared the learning-based and simulation-based approximate dynamic programming methods for finite-horizon inventory problems with demand forecasts and concluded that the simulation-based rollout method provides better performance. Desai, Farias, and Moallemi (2012) proposed the smoothed approximate linear program for discounted MDP's and applied to the game of Tetris and a queuing network control problem. Moreover, Bertsekas (2010, 2011) reported that the approximate policy iteration is hampered by oscillations between poor policies for some numerical examples.The purpose of this paper is to generalize and improve the SBMPIM (Ohno, 2011) for the optimal control problems of multistage JIT-based production and distribution systems by developing new ADP algorithms, called the simulation-based modified policy iteration (SBMPI) algorithms, for large-scale UMDP's that can overcome the curses of dimensionality. The main new concept of the SBMPI algorithms is that the algorithms start from the expected total cost per period and relative value estimated by simulating the system under a reasonable initial policy such as the optimized kanban policy and that the simulation-based policy evaluation step of the SBMPIM is improved.The paper is organized as follows. In Section 2, the authors explain the UMDP and MPIM and describe the SBMPIM that overcomes the curses of dimensionality. In Section 3, the SBMPIM for the JIT-based production and distribution systems is generalized and improved upon by the new ADP algorithm, the SBMPI, for large-scale UMDP's. In Section 4, a three-stage JIT-based production and distribution system with stochastic demand and finite production capacity is formulated as a UMDP. In Section 5, near-optimal policies for the optimal control problems with 35,973,840 states are computed by the SBMPI and the SBMPIM algorithms for numerical comparisons. In these problems, the demand distribution is changed from a shifted binomial distribution in Ohno (2011) to a Poisson distribution. Finally, in Section 6, numerical comparisons are made between the near optimal controls computed utilizing the SBMPI algorithms and the optimized pull control system computed by the SBOS algorithm (Ohno, 2011) in order to show the effectiveness of the SBMPI algorithms proposed in this research.Consider a discrete-time stochastic dynamic system with a finite state space S = {1,2, … , |S|}. When the system is in state sn∈ S at the beginning of period n (= 0,1,2, …), a decision maker selects an action a from a finite set K(sn) to minimize the expected total cost per period of the system. Usually the undiscounted Markov decision process (UMDP) maximizes the expected total reward per period but in this paper the expected total cost per period is minimized, as in Bertsekas and Tsutsuklis (1996). The Cartesian product of all K(s) (s ∈ S) is called the action space, denoted by K, and the decision rule to select action a ∈ K(s) for each state s ∈ S is called a policy denoted by f. The choice of action a in state sndetermines the expected immediate cost r(sn,a) during period n and the transition probability p(sn, sn+1,a) that the system moves from snto sn+1 under action a. Let R and P be {r(s,a): a ∈ K(s),s ∈ S} and {p(s, s′, a):a ∈ K(s)  , s, s′ ∈ S}, respectively. The UMDP is described by the quadruplet (S, K, R, P) associated with the problem of determining an optimal stationary policy f*that minimizes the expected total cost per period of the system.Suppose that the UMDP is weakly communicating (Puterman, 1994, Section 8.3). That is, there exists a randomized stationary policy which induces the Markov chain with a single closed irreducible class and a set of states which is transient under all stationary policies. Under this assumption, there exists an optimal stationary policy with constant expected total cost per period and the optimality equation in the UMDP becomes(1)g*+h*(s)=mina∈K(s){r(s,a)+∑s′∈Sp(s,s′,a)h*(s′)},s∈S,where g*is the minimum expected total cost per period and h*(s) denotes the relative value when the system starts from state s (Puterman, 1994, p. 354). The optimal stationary policy f*is determined as a*that minimizes the right-hand side of Eq. (1) for each state s ∈ S.The standard algorithm of the UMDP is the policy iteration method (PIM), which consists of the policy evaluation step and the policy improvement step (Puterman, 1994, p. 378). The former step requires solving a system of |S| linear equations. The modified policy iteration method (MPIM) replaces the sweep-out method for solving the system of |S| linear equations in the PIM by finite steps of an iteration method called the partial policy evaluation step (Puterman, 1994, p. 386). It is shown in Theorem 8.7.1 in Puterman (1994) that under a weak condition the MPIM converges to an ε-optimal policy in a finite step, where a policy with the expected total cost per period g satisfyingg−g*<ɛis called an ε-optimal policy. Since the SBMPI is developed based on the modified policy iteration method (MPIM) (Ohno, 1985; Puterman, 1994, p. 386) as well as the SBMPIM, the both algorithms are given as follows in detail:The modified policy iteration method (MPIM)Step 1. (Initialization)Choose an initial policy f and an initial relative value h0(s), s ∈ S, satisfying h0(sr)  = 0 for a reference state sr∈ S. Specify ε > 0, and set a positive integer m and n = 0.Step 2. (Policy improvement)For s ∈ S, compute(2)w0(s)=mina∈K(s){r(s,a)+∑s′∈Sp(s,s′,a)hn(s′)}.If f(s) does not attain w0(s), then improve f(s) as an arbitrary action to attain w0(s).Step 3. (Partial policy evaluation)3.1. Compute fork=0,1,…m−1,(3)wk+1(s)=r(s,f(s))+∑s′∈Sp(s,s′,f(s))wk(s′),s∈S,and set(4)gn+1=wm(sr)−wm−1(sr)and(5)hn+1(s)=wm(s)−wm(sr),s∈S.3.2. If|hn+1(s)−hn(s)|<ε for all s ∈ S, then policy f is an ε-optimal policy with an approximate value gn+1 of the minimum expected total cost per period and stop. Otherwise, setn=n+1and go to Step 2.As is clear from the above steps of the MPIM, Eqs. (2) and (3) must be computed for all states in the state space S. The main new concept of the simulation-based modified policy iteration method (SBMPIM) was that space S is confined to only the following three sets of states:(1)set St: the set of states that appear in a trajectory of simulation with length m in each iteration.set Sv: the set of states that have appeared in trajectories of simulation up to the present iteration.set Su: the set of states s′ whose relative values are unknown but required to compute Eq. (2).This solves the classical, first curse of dimensionality (Powell, 2007, p. 93). Moreover, the number of possible actions in K(s) also grows exponentially in the number of state variables for many real systems such as the multi-stage production and distribution system. Therefore, the action set K(s) in Eq. (2) is also restricted to a neighborhood of the present action f(s), denoted by N(s, f(s)), on K(s) in the policy improvement step. This solves the third curse of dimensionality (Powell, 2007, p. 93).In addition, a frequently observed state in the real system is set as the initial state, s0, and an initial policy f(s) is assumed to take a feasible action in s that targets a desirable state, denoted by s*. Finally, initial values of h(s′) and w(s′) for s′ ∈ Suare set as r(s′, f(s′)) – r(s0, f(s0)) and r(s′, f(s′)) for the initial action f(s′), respectively.The SBMPIM adopts the aperiodicity transformation (Puterman, 1994, p. 371). That is, the transition probabilities and costs are transferred top˜(s,s′,a)andr˜(s,a)given by(6)p˜(s,s′,a)=τp(s,s′,a)+(1−τ)δs,s′and(7)r˜(s,a)=τr(s,a),whereδs,s′=1ifs=s′;=0, otherwise, and a positive number, τ, satisfies(8)0<τ<mins∈S,a∈K(s),p(s,s,a)<1{(1/1−p(s,s,a))}.Then the simulation-based modified policy iteration method (SBMPIM) (Ohno, 2011) is given as follows:The simulation-based modified policy iteration method (SBMPIM)Step 1. (Initialization)Set states s0 and s*, a simulation trajectory length, m, and λ (0  ≤   λ < 1). Choose positive integers, Q, R(<Q), and positive numbers, ε, ε1 > 0, for termination. Set Sv= St= {s0} andSu=ϕ, where ϕ is an empty set. Set TC = 0, s = s0, n = 1, g(0) = g = 0, q = 0, f(s0) as an initial policy in state s0,w(s0)=r˜(s0,f(s0))and the visit count to s0 as 1; that is, v(s0) = 1, where TC represents the total cost of each simulation.Step 2. (Simulation of a trajectory with length m)Simulate action f(s) in s and find the next state, s′. Update TC = TC +r˜(s, f(s)) and s = s′. Update sets St, Svand Su, as follows; if s ∉ Svand s ∉ Su, then set Sv= Sv∪{s}, St= St∪{s} and v(s) = 1. Moreover, take f(s) as an initial policy in s and set w(s) =r˜(s,f(s)). If s ∉ Svand s  ∈  Su, then set Sv= Sv∪{s}, Su= Su– {s}, St= St∪{s} and v(s) = 1. If s ∈ Svand s∉St, then set St= St∪{s} and v(s) = 1. If s  ∈  Svand s  ∈  St, then set v(s) = v(s) + 1. Repeat these steps m times.Step 3. (Evaluation of g)Estimate the expected average costs, g(n) and g, utilizing(9)g(n)=TC/mandg=(qg+g(n))/(q+1),respectively. Find the most frequently visited state, sr; that is,(10)sr=argmaxs∈Stv(s).Step 4. (Evaluation of h(s)) Compute(11)h(s0)=(1−λnv(s0)/m)w(s0)+(λnv(s0)/m)r˜(s0,f(s0))−g.For s ( ≠ s0)∈  St, compute(12)h(s)=(1−λnv(s)/m)w(s)+(λnv(s)/m)r˜(s,f(s))−g−h(s0).For s∈(Sv∪Su)−St, compute(13)h(s)=w(s)−g−h(s0),and set h(s0) = 0.Step 5. (Policy improvement)5.1. For s ∈ Sv, compute(14)w(s)=mina∈N(s,f(s)){r˜(s,a)+∑s′∈Sp˜(s,s′,a)h(s′)},where N(s, f(s)) is, as noted in the above, a neighborhood of the present action f(s) on K(s) and for s′ ∉ (Sv∪Su)  that satisfiesp˜(s,s′,a)> 0, set Su = Su∪{s′}, take f(s′) as an initial policy in s′, set(15)w(s′)=r˜(s′,f(s′)),h(s′)=r˜(s′,f(s′))−r˜(s0,f(s0))and compute w(s) using h(s′). If f(s) does not attain w(s), then improve f(s) as an arbitrary action to attain w(s).5.2. For s ∈ Su, compute(16)w(s)=mina∈N(s,f(s)){r˜(s,a)+∑s′∈Sp˜(s,s′,a)h(s′)},where for s′ ∉ (Sv∪Su)  that satisfiesp˜(s,s′,a)>0, set h(s′) as in Eq. (15) and compute w(s). If f(s) does not attain w(s), then improve f(s) as an arbitrary action to attain w(s).Step 6. (Convergence test)6.1. If |g(n) − g(n − 1)| < ε1, then set q=q + 1 and go to Step 6.2. Otherwise, set q = 0 and go to Step 6.4.6.2. If q = R, then set s0 = srand go to Step 6.4. Otherwise, if q < Q, then go to Step 6.4, or if q ≥ Q, then go to Step 6.3.6.3. Calculate the estimator, S2, of the sample variance of {g(n − q)/τ,…, g(n)/τ} utilizing the mean, g/τ, and the confidence interval of the approximate minimum expected cost g*utilizing the 1 − α/2 quantile,tq,1−α/2, of the t distribution with q degrees of freedom. If its half-width is less than ε, that is,tq,1−α/2S/tq,1−α/2Sq+1q+1<ɛ, then stop. The approximate optimal policy is {f(s), s ∈ (Sv∪Su) }. Otherwise, go to Step 6.4.6.4. Set s = s0,St={s0}, TC = 0, n = n + 1 and go back to Step 2.In the SBMPIM, the initial policy f(s) is set as a feasible action in s that targets the desirable state, s*and the initial value of w(s) is set asr˜(s,f(s)). In many real systems, however, reasonable and practical policies exist. A typical example is the kanban policy with optimal numbers of kanbans determined by the simulation-based optimal setting (SBOS) (Ohno, 2011) algorithm for the JIT-based production and distribution system. The new ADP, called the simulation-based modified policy iteration (SBMPI) algorithms, intend to improve the SBMPIM in that the SBMPI algorithms start from the expected cost and the relative value of such a reasonable initial policy.Furthermore, the evaluation step of the relative value in Step 4 of the SBMPIM cannot be utilized for the reasonable initial policy because of temporal difference learning and the policy improvement in Step 5 of the SBMPIM, which is based on the temporal relative values. Consequently, in some cases the relative values may not be the same as values exactly evaluated for the initial policy, and the relative values computed in the later steps of the SBMPIM algorithm may not coincide with exact values of the current policies. On the other hand, Cao (2007), Cooper, Henderson, and Lewis (2003), He et al. (2000) and others have dealt with the simulation-based evaluation techniques of the relative value. In the SBMPI algorithms, the relative values of the initial policy in Step 2 as well as of all policies later in Steps 5 and 6 are evaluated in the same way as Step 3 in the MPIM.Furthermore, the unknown relative value of states in Eq. (15), which is required to compute Eq. (14) in the policy improvement step of the SBMPIM, is incorporated into Eq. (23) in the SBMPI algorithms. Eq. (23) shows that the new state is first treated as an undesirable state in the policy improvement process and subsequently, w(s) is estimated by Eq. (24). This leads to the faster convergence to the near-optimal policy.The SBMPI algorithms also adopt the aperiodicity transformation given in Eqs. (6)–(8). Thus, the basic version of the simulation-based modified policy iteration (SBMPI) algorithms, which is abbreviated to the SBMPI-Svalgorithm, is as follows:The SBMPI-SvalgorithmStep 1. (Initialization)Set a reasonable initial policy f, an initial state s0, a positive number τ for the aperiodicity transformation, simulation trajectory lengths, M0, M′ and iteration numbers N0, N′ for computation of the relative value. Choose positive numbers, ε1, ε2 and β1, β2 (0≤  β1, β2≤  1) for convergence test and a positive number W and μ1, μ2 (0 < μ1, μ2 < 1) for initial values of unknown relative values. Choose a positive integer γ for change of the initial state and probability α, simulation length m and sample number Q for estimation of the approximate minimum expected cost. Set g(0) = 0 and n = 1.Step 2. (Evaluation of g and h(s) of the initial policy)2.1. Set St= {s0}, #oldSt= 1, v(s0) = 1, TC = 0, s = s0, M = M0, N = N0 and k = 1.2.2. Simulate action f(s) in s and find the next state, s′. Update TC = TC +r˜(s, f(s)) and s = s′. If s∉Stthen set St= St∪{s} and v(s) = 1. Otherwise, set v(s) = v(s) + 1.2.3. If k≥  M, then set #Stas the number of states in set St, that is, #St= |St| and go to Step 2.4. Otherwise, set k = k + 1 and go to Step 2.2.2.4. If #St> #oldSt, then set #oldSt= #St, M = M + M′, k = k + 1 and go to Step 2.2. Otherwise, set Stis the set of states reachable from s0 under the initial policy and go to Step 2.5.2.5. (Evaluation of g)Estimate the expected average cost, g(n) by g(n) = TC/k and find the most frequently visited state, sr, by Eq. (10).2.6. (Evaluation of h(s))2.6.1. For s( ≠ s0) ∈ St, compute(17)h0(s)=r˜(s,f(s))−r˜(s0,f(s0))and seth0(s0)=0and k = 0.2.6.2. For s ∈ St, compute(18)wk+1(s)=r˜(s,f(s))+∑s′∈Sp˜(s,s′,f(s))hk(s′),where for s′ ∉ Stthat satisfiesp˜(s,s′,f(s))>0, set hk(s′) as in Eq. (15) and compute wk+1(s). Moreover, for s( ≠ s0) ∈ St, compute(19)hk+1(s)=wk+1(s)−wk+1(s0)and sethk+1(s0)=0.2.6.3. Set k = k + 1 and if k > N then go to Step 2.6.4. Otherwise, go to Step 2.6.2.2.6.4. (Convergence test)For s ∈ St, compute wk+1(s) by Eq. (18). Then for s ∈ Stthat satisfies v(s) ≥ β1v(s0), compute(20)Δ(s)=|wk+1(s)−g(n)−hk(s)|.If maxsΔ(s) > ɛ1, then set N = N + N′ and for s( ≠ s0) ∈ St, compute hk+1(s) by Eq. (19) and sethk+1(s0)=0, k = k + 1 and go to Step 2.6.2. Otherwise, set Sv= Stand for s ∈ Sv, set w(s) = wk+1(s). For s( ≠ s0) ∈ Sv, compute(21)h(s)=w(s)−w(s0)and seth(s0)=0, s0 = sr, Su= ϕ, n = n + 1 and go to Step 3.Step 3. (Policy improvement)3.1. For s ∈ Sv, compute(22)w(s)=mina∈N(s,f(s)){r˜(s,a)+∑s′∈Sp˜(s,s′,a)h(s′)},where N(s, f(s)) is, as noted in the preceding section, a neighborhood of the present action f(s) on K(s) and for s′ ∉ (Sv∪Su)  that satisfiesp˜(s,s′,a)>0, set Su= Su∪{s′}, take f(s′) as the initial policy in s′, set h(s′) as(23)h(s′)=max{h(s),μ1nW}and compute w(s) using h(s′). If f(s) does not attain w(s), then improve f(s) as an arbitrary action to attain w(s). For s( ≠ s0) ∈ Sv, compute h(s) by Eq. (21) and seth(s0)=0.3.2. (Convergence test)If |g(n) − g(n − 1)| < ε2 and f(s) is not improved for all s ∈  Stthat satisfy v(s) ≥ β2v(sr), then policy f(s) is an approximate optimal policy and go to Step 7. Otherwise, go to Step 3.3.3.3. For s ∈ Su, compute(24)w(s)=mina∈N(s,f(s)){r˜(s,a)+∑s′∈Sp˜(s,s′,a)h(s′)},where for s′ ∉ (Sv∪Su) that satisfiesp˜(s,s′,a)>0, set h(s′) as in Eq. (23) and compute w(s). If f(s) does not attain w(s), then improve f(s) as an arbitrary action to attain w(s). For s ∈ Su, compute h(s) by Eq. (21).Step 4. (Simulation of the improved policy)4.1. Set St= {s0}, #oldSt= 1, v(s0) = 1, TC = 0, s = s0, M = M0, N = N0 and k = 1.4.2. Simulate action f(s) in s and find the next state, s′. Update TC = TC +r˜(s, f(s)) and s = s′.4.3. If s∉Svand s∉Su, then set Sv= Sv∪{s}, St= St∪{s} and v(s) = 1. Take f(s) as the initial policy in s and seth(s)=r˜(s,f(s))−r˜(s0,f(s0)).4.4. If s∉Svand s ∈ Su, then set Sv= Sv∪{s}, Su= Su − {s}, St= St∪{s} and v(s) = 1.4.5. If s ∈ Svand s∉St, then set St= St∪{s} and v(s) = 1.4.6. If s ∈ Svand s ∈ St, then set v(s) = v(s) + 1.4.7. If k≥  M then set #St= |St| and go to Step 4.8. Otherwise, set k = k + 1 and go to Step 4.2.4.8. If #St> #oldSt, then set #oldSt= #St, M = M + M′, k = k + 1 and go to Step 4.2. Otherwise, set Stis the set of reachable states from s0 under the improved policy and go to Step 5.Step 5. (Evaluation of g)Estimate the expected average cost, g(n) by g(n) = TC/k and find the most frequently visited state, sr, by Eq. (10). If v(s0) > γ, then go to Step 6. Otherwise, compute(25)w(s0)=r˜(s0,f(s0))+∑s′∈Sp˜(s0,s′,f(s0))h(s′),(26)w(sr)=r˜(sr,f(sr))+∑s′∈Sp˜(sr,s′,f(sr))h(s′),where for s′ ∉ (Sv∪Su)  that satisfiesp˜(s0,s′,f(s0))>0orp˜(sr,s′,f(sr))> 0, set h(s′) as in Eq. (15) for the initial policy f in s′, and compute w(s) using h(s′). For s( ≠ sr) ∈ (Sv∪Su) , compute(27)h(s)=h(s)+w(s0)−w(sr)and set h(sr) = 0 and s0 = sr.Step 6. (Evaluation of h(s))6.1. For s( ≠ s0 ) ∈ (Sv∪Su) , set h0(s) = h(s) and set h0(s0) = 0 and k = 0.6.2. For s ∈ St, compute(28)wk+1(s)=r˜(s,f(s))+∑s′∈Sp˜(s,s′,f(s))hk(s′),where for s′∉ (Sv∪Su) that satisfiesp˜(s,s′,f(s))>0, set hk(s′) as in Eq. (15) for the initial policy f in s′ and compute wk+1(s). Moreover, for s( ≠ s0) ∈ St, compute hk+1(s) by Eq. (19) and sethk+1(s0)=0.6.3. Set k = k + 1 and if k > N then go to Step 6.4. Otherwise, go to Step 6.2.6.4. (Convergence test of the relative value)For s ∈  St, compute wk+1(s) by Eq. (28). Then for s ∈  Stthat satisfies v(s) ≥ β1v(sr), computeΔ(s) by Eq. (20). If maxsΔ(s) > μ2ɛ1, then set N = N + N′ and for s( ≠ s0) ∈ St, compute hk+1(s) by Eq. (19) and sethk+1(s0)=0, k = k + 1 and go to Step 6.2. Otherwise, for s( ≠ s0) ∈ St, set(29)h(s)=wk+1(s)−wk+1(s0)and seth(s0)=0, n = n + 1 and go to Step 3.Step 7. (Estimation of the approximate minimum expected cost g*)7.1. The approximate optimal policy and the relative values are given by {f(s), h(s); s ∈ (Sv∪Su) }. The following steps estimate the confidence interval of the approximate minimum expected cost g*, which is conform to the confidence interval in Step 6.3 of the SBMPIM.7.2. Set TC = 0, s = s0, q = 1 and k = 1.7.3. Simulate action f(s) in s and find the next state, s′. Update TC = TC +r˜(s, f(s)) and s = s′. Ifs∉ (Sv∪Su) , then take f(s) as the initial policy in s.7.4. Set k = k + 1 and if k > m then go to Step 7.5. Otherwise, go to Step 7.3.7.5. Set g(q) = TC/m and q = q + 1. If q > Q then go to Step 7.6. Otherwise, set TC = 0, s = s0 and k = 1 and go to Step 7.3.7.6. Calculate the meang=∑q=1Q(g(q)/τ)/Qand the varianceS2=∑q=1Q(g(q)/τ−g)2/(Q−1)of the simulation output {g(1)/τ, ..., g(Q)/τ}. Utilizing the 1 − α/2 quantile,tQ−1,1−α/2, of the t distribution with (Q – 1) degrees of freedom, the 100(1 − α) percent confidence interval of the approximate minimum expected cost g*is given by(30)[g−tQ−1,1−α/2S/Q,g+tQ−1,1−α/2S/Q].In the SBMPI-Svalgorithm, the policy is improved by Eqs. (22) and (23) for all states of set Svthat is inherited from the SBMPIM. The relative values of the improved policy, however, are evaluated by Eq. (28) for all states of set St, which is the set of states reachable from s0 under the improved policy. This is because the policy given by actions for all states of set Svmay have multiple recurrent classes. Consequently, the policy improvement step can be confined to set St. This simplified version of the SBMPI-Svalgorithm is abbreviated to the SBMPI-Stalgorithm, which is as follows:The SBMPI-StalgorithmOnly Step 3.1 of the SBMPI-Sv algorithm is replaced by3.1. For s∈  St, computew(s)=mina∈N(s,f(s)){r˜(s,a)+∑s′∈Sp˜(s,s′,a)h(s′)},where for s′ ∉ (Sv∪Su) that satisfiesp˜(s,s′,a)>0, set Su= Su∪{s′}, take f(s′) as the initial policy in s′, set h(s′) as in Eq. (23) and compute w(s). If f(s) does not attain w(s), then improve f(s) as an arbitrary action to attain w(s). For s( ≠ s0) ∈ St, compute h(s) by Eq. (21) and seth(s0)=0.In the following of this paper, the SBMPI algorithms, which are proposed as new ADP algorithms for UMDP's in order to overcome the curses of dimensionality, are applied to an optimal control problem of a three-stage JIT-based production and distribution system with stochastic capacity and demand. First, the system and the UMDP are explained in this section. The three-stage JIT-based production and distribution system is shown in Fig. 1. It is assumed that a supplier, stage 0, replenishes the parts used in stage 1. Let Imax,iand Jmax,ibe the maximum quantities of the parts input and product output at stage i (i = 1,2,3). The lead time for the delivery of the parts that stage i orders from the preceding stage, i − 1, is given by Li(≥1), which includes the transportation time, Ti(≥0). It is assumed that the demand for the final product in period n (n = 1, 2,…), D(n), is independent and identically distributed (i.i.d.) with a mean of D. In addition, Dmin and Dmax denote the minimum and maximum values of the demand, respectively. Assume that excess demand is backlogged until the maximum backlog, Bmax, and the amount over Bmax is lost. This assumption is essential for limiting the number of possible states. It is also assumed that the production capacity of stage i in period n, Ci(n), is a discrete i.i.d. random variable due to machine breakdown and repair, maintenance or absenteeism. The maximum and minimum values of the production capacity in each period of stage i are denoted by Cmax,iand Cmin,i, respectively. The following notations and cost parameters are utilized for i = 1,2,3:Ii(n)= on-hand inventory of the parts in stage i at the beginning of period n,= net inventory (on-hand inventory minus backorders) of the products in stage i at the beginning of period n,= order quantity from stage i to stage i − 1 determined at the beginning of period n,= production quantity of stage i determined at the beginning of period n,= quantity produced at stage i in period n,= quantity delivered from stage i − 1 to stage i at the beginning of period n,= holding cost of one part per period in stage i,= holding cost of one product per period in stage i,= holding cost of one part per period when delivered to stage i,= backlogged cost of one product per period at stage i,= backlog occurrence cost per time at stage i,= lost sales cost from one product over Bmax at stage 3.It is assumed that all events take place at the end of period n − 1 and beginning of period n in the following order:1.The stochastic demand D(n − 1) and stochastic production capacity Ci(n − 1) of each stage i are known.The quantity produced, Pi′(n − 1), at each stage i is known and the quantity delivered, Qi(n − Ti), arrives at each stage i.The inventory levels, Ii(n) and Ji(n), and quantity delivered, Qi(n), are known for each stage i.The order quantity, Oi(n), and production quantity, Pi(n), for all stages are determined based on the known state.The holding and backlogged costs for all stages and the lost sales cost at stage 3 in period n are incurred.The optimal control problem is formulated into the UMDP to determine the optimal ordering and production policy that minimizes the expected average cost per period. The state at the beginning of period n, sn, is denoted by the vector whose components are the ordered quantities of stage i (=1,2,3) from period (n −  Li+ Ti+ 1) to (n − 1), the quantities delivered to stage i from period (n− Ti+ 1) to n and the inventory levels of the parts and products at each stage at the beginning of period n. Consequently,(31)sn=(O1(n−L1+T1+1),…,O1(n−1),Q1(n−T1+1),…,Q1(n),O2(n−L2+T2+1),…,O2(n−1),Q2(n−T2+1),…Q2(n),O3(n−L3+T3+1),…,O3(n−1),Q3(n−T3+1),…,Q3(n),I1(n),J1(n),I2(n),J2(n),I3(n),J3(n)).Let KiO(sn) be the set of possible order quantities and KiP(sn) be the set of possible production quantities in sn, respectively. Then, the following relations hold:(32)KiO(sn)={0,…,Imax,i−Ii(n)−∑l=1Li−Ti−1Oi(n−l)−[−Ji−1(n)]+−∑l=0Ti−1Qi(n−l)},i=1,2,3,where[x]+=max(0,x)andJ0(n)=0,(33)KiP(sn)={0,…,min{Ii(n),Cmax,i,Jmax,i−Ji(n)}},i=1,2and for stage 3,(34)K3P(sn)={0,…,min{I3(n),Cmax,3,Jmax,3−J3(n)+Dmin}}.Then, a possible action in snis given bya=(O1(n),P1(n),O2(n),P2(n),O3(n),P3(n)), whereOi(n)∈KiO(sn),Pi(n)∈KiP(sn), i =1,2,3. Moreover, the Cartesian product of the possible order and production sets in snis denoted by K(sn). Then, a ∈ K(sn) and policy f is the set of possible actions in s, f(s); that is, { f(s) ∈ K(s) ;  s ∈ S}. Under this policy, the inventory transitions are given by(35)Ii(n+1)=Ii(n)+Qi(n−Ti+1)−Pi′(n),i=1,2,3(36)Ji(n+1)=Ji(n)+Pi′(n)−Oi+1(n−Li+1+Ti+1+1),i=1,2(37)J3(n+1)=max{J3(n)+P′3(n)−D(n),−Bmax},where Pi′(n) is given by(38)Pi′(n)=min{Pi(n),Ci(n)}.On the other hand, the quantity delivered from stage i − 1 to stage i at the beginning of period n is given by(39)Qi(n)=min{Oi(n−Li+Ti)+[−Ji−1(n−1)]+,P′i−1(n−1)+[Ji−1(n−1)]+},i=1,2,3,where J0(n) = 0 and P0′(n) = Imax,1. Then, the immediate cost in period n under action a in state snis given by(40)r(sn,a)=∑i=13{CiIIi(n)+CiJ[Ji(n)]++CiQ∑l=0Ti−1Qi(n−l)+CiB[−Ji(n)]++BiH(Ji(n)<0)}+CL∑c3=Cmin,3Cmax,3∑d=DminDmaxPr{C3(n)=c3,D(n)=d}×[d−Bmax−J3(n)−min{P3(n),c3}]+,where H(e) is the indicator function of event e; that is, H(e) = 1, if event e occurs, and 0, otherwise.Finally, the transition probability from snto sn+1 under action a in snis given by(41)p(sn,sn+1,a)={Pr[Ci(n)=ci,i=1,2,3,D(n)=d],ifsn+1=(O1(n−L1+T1+2),…,O1(n),Q1(n−T1+2),…,O2(n−L2+T2+2),…,O3(n−L3+T3+2),…,O3(n),Q3(n−T3+2),…,min{O3(n−L3+T3+1)+[−J2(n)]+,min{P2(n),c2}+[J2(n)]+},I1(n)+Q1(n−T1+1)−min{P1(n),c1},J1(n)+min{P1(n),c1}−O2(n−L2+T2+1),…,I3(n)+Q3(n−T3+1)−min{P3(n),c3},max{J3(n)+min{P3(n),c3}−d,−Bmax},(Cmin,i≤ci≤Cmax,i,i=1,2,3,Dmin≤d≤Dmax)0,otherwise.The optimal order and production policy, f,*is determined as a*that minimizes the right-hand side of the optimality Eq. (1) for each state.Ohno (2011) discussed the above optimal control problem and made numerical comparisons between near-optimal controls computed using the SBMPIM and five types of optimized pull systems, as noted in Section 1. These numerical comparisons assumed the demand distribution of a shifted binomial distribution with D = 2. In this section, the demand distribution is changed to a Poisson distribution with D = 2 and near-optimal policies of the optimal control problems are computed by the SBMPI-Stand -Svalgorithms and the SBMPIM.In the following numerical results, it is assumed that L1 = L2 = 1, L3 = 2, T1 = T2 = 0, T3 = 1, Imax,1 = Imax,2 = 6, Imax,3 = 19, Jmax,1 = 12, Jmax,2 = 3, Jmax,3 = 15 and Bmax = 5. Thus Eq. (31) is reduced to(42)sn=(Q3(n),I1(n),J1(n),I2(n),J2(n),I3(n),J3(n)).Since the Q3(n) varies from 0 to Jmax,2, for i = 1,2,3, Ii(n) varies from 0 to Imax,ifor i = 1,2,3, Ji(n) takes from−Imax,i+1to Jmax,ifor i = 1,2 and J3(n) takes from−Bmaxto Jmax,3, the number of states, |S|, becomes 35,973,840, which is computed by (Jmax,2 + 1)(Imax,1 + 1) (Imax,2 + 1) (Imax,3 + 1)(Jmax,1 + Imax,2 + 1) (Jmax,2 + Imax,3 + 1)(Jmax,3 + Bmax + 1). (Incidentally, the number of states in Ohno (2011) was 26,091,520.)As noted above, the demand distribution is the Poisson distribution with D = 2 and is truncated at 10. That is, Dmin = 0 and Dmax = 10. Incidentally, Dmin = 1 and Dmax = 3 in Ohno (2011). Following this paper, the cost parameters are set as(C1I,C2I,C3I)=(1,3,6),(C1J,C2J,C3J)=(3,6,12),(C1B,C2B,C3B)=(0,0,80),C3Q=6,(B1,B2,B3)=(0,0,120)andCL=1000. In addition, we consider the same production capacity distributions (i.e., A, B and C) and the same five cases (i.e., AAA, BBB, CCC, ABC and CBA),where for example, ABC denotes that the production capacity distributions of the first, second and third stages are A, B and C, respectively:A: P3 = 1: mean production capacity is 3,B: P3 = 0.7, P2 = 0.2 and P1 = 0.1: mean production capacity is 2.6,C: P3 = 0.7, P1 = 0.2 and P0 = 0.1: mean production capacity is 2.3.Since the mean demand is 2, each stage with production capacity distribution A, B and C has a high traffic intensity of 0.667, 0.769 and 0.870, respectively, where the traffic intensity is defined by D/(mean production capacity).First, the optimal control problem for each case of production capacity distribution in this three-stage JIT-based production and distribution system was solved using the SBMPI and the SBMPIM algorithms. The corresponding parameters, except the initial policy stated below, of the SBMPI-Stalgorithm for the case AAA were set as s0 =  (0, 6,12,6,3,19,15), τ= 0.99, M0 = 1,000,000, M′ = 500,000, N0 = 100, N′ = 10, ε1 = 0.3, ε2 = 0.01, β1 = 0.1, β2 = 0.01, W = 3,000, μ1 = 0.99, μ2 = 0.9, γ = 5, α= 0.05, m = 5,000,000 and Q = 20. Those parameters for the case CCC were set to the same values as the above, except M0 = 500,000, M′ = 50,000, ε1 = ε2 = 2, β1 = 0.6, β2 = 0.7, W = 45,000, μ1 = 0.995, μ2 = 0.99. All of algorithms were programmed in C++ and the computations were performed on a Windows Vista small work station with an Intel Xeon X5492 3.40 gigahertz CPU and 64 gigabyte memory.The 95 percent minimum average cost confidence intervals computed by the SBMPI-Stand SBMPI-Svalgorithms and the SBMPIM are shown in the second through forth rows, respectively, of Table 1. The computation time in seconds, iteration number and the numbers of the states of sets, |Sv|, |Su| and |St| are also shown from the fifth row of the table. The SBMPIM could not solve the CCC problem due to the memory shortage. The results of the minimum average cost and the computation time show that the SBMPI-Stalgorithm attains lower minimum average cost than the SBMPIM with about 200 times shorter computation time.The kanban policy with optimal numbers of kanbans determined by the SBOS algorithm (Ohno, 2011) is employed as the initial policy of the SBMPI algorithms. Let Miand Nibe the numbers of the withdrawal and production-ordering kanbans in stage i (=1,2,3), respectively. Then, the order quantity, Oi(n), and production quantity, Pi(n) are given by(43)Oi(n)=Mi−Ii(n)−[−Ji−1(n)]+−∑l=1Li−Ti−1Oi(n−l)−∑l=0Ti−1Qi(n−l),and(44)Pi(n)=min{Ni−[Ji(n)]+,Ii(n),Cmax,i},where it is assumed that(45)Ii(1)=Mi,Ji(1)=Ni.Moreover, the SBOS algorithm programmed in VB determined the optimal numbers of kanbans (M1, N1, M2, N2, M3, N3) as (6,3,6,3,9,5) with computation time 1945 seconds for the AAA case and as (6,12,6,3,19,15) with computation time 4836 seconds for the CCC case, respectively. Therefore, we have to know the effects of initial policies on the SBMPI algorithms. Table 2 shows how the minimum expected cost, computation time, iteration number and so on of the SBMPI-St algorithm change as its initial kanban policy is away from the optimal kanban policy, where the initial kanban policy “AAA (or CCC) +(−)j” for j = 1,2,3 means that the numbers of all kanbans are +(−)j away from the optimal kanban policy for the case AAA or CCC. It is to be noted that the shifted numbers of kanbans must satisfy the stability conditions (Ohno, Nakashima, & Kojima, 1995): Mi> (Li+ 1)D and Ni> D for i (=1,2,3). In fact, the second row of Table 2 shows the corresponding numbers of all kanbans. This table shows that the minimum expected cost and computation time are not so different except the case “AAA-2”. Consequently, the SBMPI algorithms can be applied for large-scale UMDP's with ordinary initial policies.In this section, the optimized pull systems are numerically compared with the near optimal controls computed utilizing the SBMPI algorithms in order to clarify the effectiveness of the latter near-optimal controls. The pull systems are kanban (Monden, 2012), base stock (Clark & Scarf, 1960), CONWIP (Spearman et al., 1990), hybrid (Bonvik et al., 1997) and extended kanban (Dallery & Liberpoulos, 2000) systems. In particular, the extended kanban system is a hybrid of the kanban and base stock systems and for given Bi, Mi, Ni, Riand Si(i = 1,2,3), the order quantity, Oi(n), and production quantity, Pi(n) are given by(46)Ii(1)=Si,Ji(1)=Ri,Bi(1)=Bi,(47)Oi(n)=min{D(n−1)+Bi(n−1),Mi−Ii(n)−[−Ji−1(n)]+−∑l=1Li−Ti−1Oi(n−l)−∑l=0Ti−1Qi(n−l)}(48)Bi(n)=Bi(n−1)+D(n−1)−Oi(n)(49)Pi(n)=min{Ni−[Ji(n)]+,Ii(n),Ci}.It is to be noted that the backlogged demand, Bi(n), is essential as an additional state in the extended kanban control system. Moreover, when the extended kanban control system has Bi= Ri= 0 and Mi= Ni= ∞, it is reduced to the base stock system, and if Bi= ∞, Ri= Niand Si= Mi, then it is reduced to the kanban system. The expressions for the base stock, CONWIP and hybrid systems are also explained in Ohno (2011).Table 3shows numerical comparisons for the Poisson distributed demand among optimized pull systems based on the near optimal controls computed by the SBMPI-Sv algorithm, because this algorithm gives a little better solution than the SBMPI-Stalgorithm. The optimal parameters of each pull system were computed by the SBOS algorithm written in VB.The third row in Table 3 shows the minimum average cost and optimal parameters of this optimized kanban system. The fourth row through the seventh row show the minimum average cost and optimal parameters of the base stock, CONWIP, hybrid and extended kanban control systems, respectively. It is to be noted that the parameters Biin the extended kanban control system were fixed at zero. The numbers (percent) in each column of AAA through CBA represent the possible minimum rate of increase compared to the the SBMPI-Svalgorithm; that is, [rate of increase (percent)] = [[lower boundary of the pull system]−[upper boundary of the SBMPI-Sv]]/[upper boundary of the SBMPI-Sv] (percent).As a conclusion, this table shows that the near-optimal policy obtained by the SBMPI-Svalgorithm dominates the above well-known five optimized pull systems. In particular, in the CCC case, the near-optimal policy is much better than the five optimized pull systems. This shows that in the uncertain production systems the SBMPI algorithm improves the performance of the production systems very significantly.

@&#CONCLUSIONS@&#
