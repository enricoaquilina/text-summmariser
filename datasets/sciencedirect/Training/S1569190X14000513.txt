@&#MAIN-TITLE@&#
Insight Maker: A general-purpose tool for web-based modeling & simulation

@&#HIGHLIGHTS@&#
A general-purpose web-based simulation and modeling tool is developed.The tool provides a unified, multi-paradigm modeling environment.A “drag and drop”, accessible modeling interface is developed in client-side code.Pure, client-side simulation code and a domain specific language are developed.The tool meets a need for a web-based, general-purpose, modeling environment.

@&#KEYPHRASES@&#
Modeling,Simulation,Web-based technologies,System Dynamics,Agent-Based Modeling,

@&#ABSTRACT@&#
A web-based, general-purpose simulation and modeling tool is presented in this paper. The tool, Insight Maker, has been designed to make modeling and simulation accessible to a wider audience of users. Insight Maker integrates three general modeling approaches – System Dynamics, Agent-Based Modeling, and imperative programming – in a unified modeling framework. The environment provides a graphical model construction interface that is implemented purely in client-side code that runs on users’ machines. Advanced features, such as model scripting and an optimization tool, are also described. Insight Maker, under development for several years, has gained significant adoption with currently more than 20,000 registered users. In addition to detailing the tool and its guiding philosophy, this first paper on Insight Maker describes lessons learned from the development of a complex web-based simulation and modeling tool.

@&#INTRODUCTION@&#
The field of modeling and simulation tools is diverse and emergent. General-purpose modeling tools (e.g. MATLAB’s Simulink or the Modelica language [1]) sit beside highly focused and domain-specific applications (e.g. [2] for modeling network control systems, [3] for simulating the behavior of wireless network routing protocols, or [4] for the simulation and control of turbines). Interest in and published works on such tools has grown over time. The ISI Web of Knowledge reports a substantial growth in papers published on modeling or simulation tools with 299 such papers published in the span of 1985–1989, 1482 published from 1995 to 1999, and 3727 published from 2005 to 2009.1The exact search query used was ‘”modeling tool” OR “simulation tool”’ in the Topic field.1For end-users, simulation and modeling tools are generally designed as executables to be run on a consumer operating system such as Windows or Mac OS X. With the expansion of the Internet and the ubiquity of the World Wide Web, new possibilities to harness this communication technology and platform to develop rich collaborative modeling tools have become available. One major review of the opportunities and issues faced when utilizing web-based technologies as part of a simulation or modeling application has been [5]. However, technology has advanced rapidly in the period since the publication of that review. Competition among technology companies such as Apple, Microsoft and Google has led to a rapid increase in the availability of advanced features in web browsers and significant performance enhancements in these same browsers (these improvements are documented at sites such as http://AreWeFastYet.com and http://html5test.com). Such improvements have changed what is effectively possible in the browser. Where [5] note that web-based technologies were not well suited to “local simulation and visualization” – running all simulation code on an end-user’s machine – that is no longer the case as the work presented here demonstrates.Other work on the intersection between simulation and modeling tools and web-based technologies includes but is not limited to, [6] who develop a simulation–optimization platform for industrial applications, [7] who propose a framework for online collaborative modeling and simulation an approach to environment construction adaptable across multiple domains, [8] who study the execution of simulations in cloud-based platforms, and [9] who explore collaborative model integration in a distributed environment. What is missing from earlier work, however, is a realization of a general-purpose simulation environment developed using web-based technologies targeted first at a general audience.Web-based technologies are especially beneficial to non-experts as they can significantly reduce the costs required for a new user to experiment with and learn about a simulation and modeling tool. Simulation and modeling tools have been shown to be useful instructional devices (e.g. [10] who used a simulation tool in the economics classroom or [11] for examples of work in primary schools). By developing web-based simulation and modeling tools, the accessibility of these tools can be increased both within the classroom and outside it. A layperson considering modeling may be adverse to downloading and installing modeling and simulation software on a personal computer (due to fear of viruses and other concerns). On the other hand, the same person may be more willing to open a simple web page that contains a simulation model or embedded model construction interface. Lowering barriers to entry make it possible to engage more people in modeling and simulation, potentially leading to societal benefits as a result of an increased usage and understanding of modeling.The rest of this paper presents the simulation and modeling tool Insight Maker (available at http://InsightMaker.com). It is a free, open-source modeling and simulation tool developed using web-based technologies and supports graphical model construction using multiple paradigms. The software is designed primarily to be as accessible to the layperson as possible, but also contains significant advanced modeling tools such as embedded scripting capabilities and an optimization toolset. Insight Maker was made public at the end of 2009 and has undergone continual development and evolution since then. Since its release, the tool has gain over 20,000 registered users. The feedback from these users has helped set the course for the direction and development of the tool.The remaining sections of this paper first describe the philosophy and design principles of the simulation and modeling tool. Next the architecture of the tool is detailed. As a web-based technology, the tool includes both server-side and client-side components and this architecture is discussed. Particular attention is paid to the benefits offered and compromises required to successfully implement a complex tool such as Insight Maker in a web-based environment. Finally, given that the tool has undergone multiple years of iteration and development based on user feedback and benchmarks of user adoption, conclusions and lessons learned from this evolution are also presented.It can be argued that three basic criteria should be used when assessing a tool or environment used to develop simulation and models: performance, features, and accessibility. A high-performing environment is one that executes simulations quickly with minimal resource requirements. Features are capabilities and high-level functionality in the simulation and modeling tool. Accessibility indicates how easy it is to learn or use an environment. There are generally tradeoffs among these criteria in the face of a fixed amount of development resources:•Performance versus features: Adding features to the simulation tool may often result in a negative impact on performance. In addition to software “bloat”, certain features may require computational overhead during simulations or make it more difficult to implement some types of optimizations.Performance versus accessibility: Making a simulation program accessible to users often requires utilizing higher-level concepts divorced from the underlying machine hardware. An environment’s ability to convert higher-level concepts to efficient machine code vary, but in general, accessible higher-level environments will have reduced performance when compared to less accessible, lower-level environments.Accessibility versus features: A baseline of features is important to make a simulation program accessible for users (e.g. a Graphical User Interface). However, many features added to simulation software are targeted at specific subsets of users and offer little value to the average user. Excess features that lead to “bloat” may result in user frustration or the feeling of being overwhelmed [12].Performance versus accuracy: One final tradeoff is between performance and accuracy, as they are often inversely correlated. This tradeoff may often be exposed directly to users allowing them to determine how to balance these competing goals.Although tradeoffs may be mitigated through increased development time or resources, the choices developers make about these tradeoffs are key in defining their simulation programs and environments (Fig. 1). As an example, models and simulations may be developed in raw assembly as machine processor instructions. Such models should have excellent performance, but their development would be highly difficult as assembly is inaccessible and has a minimal built-in feature set. At the cost of some performance, higher-level development environments such as C++ are alternatives. Higher-level tools offer increased accessibility and features, but in exchange are often not able to match the performance of a highly optimized, lower-level alternative.Insight Maker prioritizes tradeoffs among performance, accessibility and features in the following way. Accessibility is given primary priority, followed by features, with performance assigned the lowest priority. Accessibility is the primary goal of Insight Maker and is central to the design of the tool from the perspective of the overall program architecture (namely, the choice of implementation as a web application to increase application availability), to small language design choices (such as the use of case-insensitivity in the language), to cosmetic choices.The inclusion of a rich feature set is Insight Maker’s second priority. Many tools and capabilities are included in the program such as a built-in optimizer and scripting language. In adding features, attempts are made not to significantly increase the apparent complexity of the software; this has meant making those features available for users who desire them but highlighting a basic set of functionality for newer users who are not immediately interested or able to use the more complex features. An instance of this approach is Insight Maker’s support for measurement units. The simulation engine contains the ability to associate units with each number in the simulation that are then automatically checked and validated when the simulation runs. This useful capability can catch a class of equation errors and improve the reliability of models. However, it can also decrease the accessibility of the software as it requires that users specify the units upfront when constructing a model, which can be an unnecessarily complex burden for many applications. Thus, the simulator makes units a purely optional feature in model construction, which the user can ignore or fully engage with depending on their individual skill level and needs.The priority that is least focused on in Insight Maker is performance. Significant work has gone into optimizing the application. Nevertheless, choices made as a result of prioritizing accessibility first and features second make it impossible to achieve speeds that would be possible if performance had been the primary priority. For instance, the choice of a web application architecture imposes significant limitations on the application’s performance. As a pure web application, the simulator must use ECMAScript as its primary programming language2Other languages such as Java and Flash are often used in the browser. However, usage of these technologies require that users have plug-ins installed to support them. Many users may not.2which has good, but not excellent, performance characteristics, being roughly 2–8 times slower than C++ for numerical work [13]. Similarly, specific features may have a performance cost. For instance, the support of units in Insight Maker requires an aggregate data object to be maintained representing the results of calculations. This data object has significant performance costs compared to the usage of generic numbers.Insight Maker as a web application means that users run it by accessing a URL through their web browser. The application provides a number of different services that both mirror what would be found in a traditional desktop application – model construction and model simulation – but also extend beyond into areas more specific to a web environment – user account management as well as model searching and sharing. Although the latter services are important to the proper functioning of the application, they serve a purely bookkeeping role and are not intellectually interesting in the context of modeling and simulation. Instead, it is the novel simulation and modeling aspects of the application that are the focus of this section and paper.Fig. 2illustrates the primary model construction interface of Insight Maker running within the Google Chrome web browser. The interface is divided into three primary components: the model diagram, the toolbar and the configuration panel. The model diagram is an interactive illustration of the current model’s structure along with additional UI features such as textual descriptions, pictures and interactive buttons. Users may select, position and resize elements in the model diagram using an input device such as a mouse or, on touch devices such as tablets, their finger. Each item in the model diagram has a set of attributes associated with it that control its behaviors. The attributes can be configured in the configuration panel. The toolbar at the top of the window provides a number of functions. It contains a selector to add different types of objects to the model diagram (both for simulation and for descriptive purposes), standard tools for document editing (undo/redo, copy and paste, etc.), styling options to adjust the appearance of objects in the diagram (coloring and font characteristics), tools to interface with the simulation, and buttons to save the completed model or start a simulation of the model.The entire model building and simulation aspect of the application runs within a single web browser window. When a simulation completes, results are displayed within the same browser window. A number of different display types for results are available including time series charts, tables, histograms, scatterplots, and two-dimensional maps (Fig. 3). For offline analysis, users can export results to CSV files. In addition to the display of results, a number of other windows and configuration dialogues open within this main window. By way of example, dialogues to configure an object’s mathematical equation, set simulation time settings, and carry out optimization runs or sensitivity tests all open within this primary window. The entire contents of this window are defined using standard web technologies such as Hypertext Markup Language (HTML) and Cascading Style Sheets (CSS).The application uses a client–server architecture (Fig. 4), where clients (user computing devices) are connected to the server over the Internet. Users may use the web browser on their machine to connect to the server and load the model construction and simulation environment. The server-side components are responsible for managing user accounts, storing user data and models, and providing search infrastructure. The client-side components provide model-editing capabilities along with model simulation and analysis functionalities. Clients do not communicate with one another directly and instead all communication happens through the centralized server.It is important to note that the client-side code runs the simulation. An alternative would be to place the simulation logic and code on the server and simply use the client for model construction and the reporting of results. This latter structure was, in fact, how the application was originally designed. There are tradeoffs between these two approaches. In general, optimized server-side simulations should be faster than client-side, ECMAScript-powered simulations. For small models, however, the latency of sending data back and forth to the server may result in a server-side simulation effectively being slower than client-side simulations. Other trade-offs between server-side and client-side simulations involve issues with proprietary models (client-side simulation gives the client access to the model, making it impossible to release a model without also releasing access to the model details) and the effects of software popularity (server-side simulations can become costly and require the purchasing of additional servers, if the software experiences significant demand; client side simulation reduces this pressure). Ideally, both server-side and client-side simulation options would be available for the user to choose between. Unfortunately, this would most likely require maintaining two simulation code bases, which is currently beyond the scope of the resources available to this project.In terms of technological choices, the tool uses standard open-source technologies and runs on a generic Linux/Unix server. As much as possible, existing open-source technologies and solutions were used to minimize costs and increase the portability of the system. Data are stored on the server in the open-source MySQL database, while the open-source language PHP and the open-source content management system Drupal are used to store data and implement server-side application logic. The open-source application Lighttpd is used as the actual server software.The simulator portion of Insight Maker is architected modularly in multiple layers, where the higher-level layers depend on functionality provided by the lower levels (Fig. 5). The layers may be grouped into separate tiers that represent broad classes of functionality. Tier 0, the lowest level tier that provides the foundation for the simulator, is the ECMAScript programming language (colloquially referred to as “JavaScript”). ECMAScript is a dynamic programming language with high performance implementations available in every modern browser. The simulator uses ECMAScript 5 or higher and is supported in all modern browsers.Tier 1 provides mathematics and equation language engines to evaluate equations entered by users. The engines themselves are generic, computational tools and do not provide simulation and modeling specific features. Such features are provided by the higher tiers that will build on this tier.The mathematics engine parses and evaluates mathematical statements. There are two distinct underlying numerical libraries that the simulator may use. One generic calculation engine is provided by the ECMAScript environment itself. This is a purely double precision floating-point library with no integer, fractional or decimal data types. The second library is an arbitrary precision mathematics library implemented in ECMAScript. This second library supports precise decimal mathematics and numbers of arbitrary size with no numerical errors or imprecisions. The tradeoff between the two libraries is one of performance versus accuracy. The arbitrary precision library requires a large performance penalty over the native ECMAScript mathematics. On the other hand, the native library will have small errors due to the inherent inaccuracies in floating point numbers.3An illustration of this lack of precision in double precision floating point math engines (utilizing the IEEE 754 standard [14]) is that the summation of “0.1” and “0.2” will evaluate to “0.30000000000000004”. Many environments (such as Insight Maker) will attempt to round off the imprecisions when displaying the results thus hiding the issue from the user. Applications that require high precision, however, render such imprecisions potentially problematic.3The open-source simulator can be configured to use either library. For the deployment of the Insight Maker code base on InsightMaker.com, only the native ECMAScript numeric library is made available to users in order to maximize the performance of user models (at the cost of some precision).Additionally, the mathematics engine supports tagging numbers with arbitrary measurement units (e.g., instead of stating the length of an object in the model as “5”, it can be declared as “5 meters”). The engine enables the transparent conversion between equivalent units and also the raising of exceptions when incompatible unit operations are attempted. This is primarily useful for end-users in that it provides an additional check that equations are entered correctly.The simulator includes a general equation engine language that sits above the mathematical engine. This equation engine language is a general-purpose programming language with domain specific features targeted toward modeling and simulation utilization. Language features include: the usage and creation of block-scoped variables; the usage and creation of functions; flow control, including If-Then-Else statements, While loops and For loops; and the construction of vectors (arrays).Although the equation engine has a rich feature set, in most practical cases it serves as a thin layer over the mathematics engine in order to allow users to enter simple mathematical expressions. The more complex features are generally only required when constructing complicated Agent-Based Models. An example of the equation engine used for a simple recursive calculation is shown in Table 1and provides the reader a feel for the language characteristics. Again it is important to underscore that the equation engine language is implemented purely in ECMAScript (including the equation engine language’s tokenizer, parser and interpreter) and can be run in a web browser.Tier 2 of the simulator is a multi-paradigm modeling environment that supports several different approaches to modeling (Table 2). Each approach has a different tradeoff between flexibility and ease-of-use. System Dynamics is highly abstract and aggregate. Agent-Based Modeling is often much more granular, allowing increased control over the system but at the potential cost of decreased performance and increased modeling effort. Imperative programming, on the other hand, has no modeling-specific constraints and can be flexible to address any modeling task but potentially at a high cost in development time. The simulator allows for the seamless integration of the three approaches within a single model, enabling different resolutions and methods for different portions of the model as required.System Dynamics is a modeling paradigm developed in the 1950s to study industrial systems [15]. The technique is general and has since been applied to a range of different systems including, notably, the development of urban systems [16] and forecasting world wide trends [17]. Mathematically, System Dynamics models are often sets of nonlinear differential equations. What sets System Dynamics apart from the standard analytical analysis of differential equations is that the System Dynamics community focuses on easy-to-use graphical tools and numerical analysis of simulation results. System Dynamics modelers are also primarily focused on feedback loops and the roles they play in the evolution of a system.System Dynamics models are built graphically out of primitives: building blocks each with a unique function. The basic paradigm is that of a stock-and-flow model (referred to in some fields as a compartmental model), where material is moved by flow primitives between stock primitives. This maps conceptually and directly onto standard differential equation or dynamical systems models used in many fields. In this mapping, stocks represent state variables and flows represent the derivatives (or rates of change) of these state variables. The key primitives provided by the simulator are described in Table 3.Although not strictly formalized, the System Dynamics community has evolved conventions for functions and patterns of model construction. These numerous conventions are generally shared among different System Dynamics simulation software tools along with System Dynamics education resources. The conventions are followed within the simulator’s System Dynamics environment by including standard System Dynamics functions, such as those for generating canonical inputs (e.g. RAMP, STEP, and PULSE input functions) and functions for causing various forms of delays and feedback (e.g. DELAY1 and DELAY3, being first and second order exponential delays respectively). Such conventions facilitate the movement of models and ideas between System Dynamics software environments and they are supported in the implementation of the simulator.The simulator includes two numerical solvers to estimate solutions to user-defined differential equations underlying the System Dynamics models: Euler’s method and a 4th Order Runge–Kutta method. Generally, the 4th Order Runge–Kutta method should be the preferred choice over Euler’s method due to its increased accuracy per unit of computational effort. Nonetheless, Euler’s method may be preferred in two cases – first, when there are a number of discontinuities in the rates of change for the systems state variables. In this case, the averaging process of the Runge–Kutta method might not be desirable. The second usage case for Euler’s method is for learning and educational environments, as it is conceptually simpler and easier to understand as compared to the 4th Order Runge–Kutta method.Agent-Based Modeling simulates the individual actors or agents in a model [18]. Each agent has the potential ability to independently modify its state, based on rules or algorithms. Agents may modify their state in relation to other agents, forming systems and developing complex system-level behaviors [19,20]. This represents a significant difference from System Dynamics modeling, which addresses aggregated behavior and results. Simulating individual agents allows for finer grained models and greater flexibility, but it comes at a cost of increased computational effort and potentially leads to results that are more difficult to interpret.The simulator includes an approach to constructing Agent-Based Models that, in terms of user-interface elements, is very similar to that used to construct System Dynamics models. As in System Dynamics models, Agent-Based Models are built graphically using a set of primitives, each of which fulfills a specific modeling function. Whereas in the System Dynamics modeling paradigm the primary product was the stock and flow diagram, in Agent-Based Modeling it is a state transition chart where an agent may occupy one or more states and transition between them as a function of mechanistic or stochastic rules. Table 4summarizes the primitives used to construct Agent-Based Models in the simulator.Although Agent-Based Modeling can be conceived as distinct from the System Dynamics paradigm, they are in fact fully integrated in the simulator and the distinction between them is made solely to facilitate discussion in regards to existing paradigms. System Dynamics primitives can readily be included in primarily Agent-Based Models (for instance, a stock and flow model can be created within an individual agent) and vice versa. Thus, while states only represent binary values, stocks can be included within agents to track continuous state variables.A number of functions included in the simulator are specifically targeted at building Agent-Based Models. One category of functions deals with selecting agents matching certain criteria from a population. For instance, the FINDSTATE function will return a vector of all the agents in a model that have activated a given state. Find statements can be nested enabling binary AND logic, and the results from separate find-statements may be merged enabling binary OR logic.The Agent-Based Modeling in the simulator supports two forms of geographic relationships between agents: two-dimensional geography and network geography. Agents may be given a physical position within the plane. Their position may be queried or used as a selector (e.g. the FINDNEAREST or FINDFURTHEST functions to select agents that are, respectively, close to or far from a given target) and the position may also be modified (e.g. the MOVE or MOVETOWARDS functions). With regard to network geometry, each agent may have one or more connections to other agents. Each connection is binary and may be queried or modified as the simulation progresses (e.g. with the CONNECT or FINDCONNECTED functions).The equation engine language is an imperative programming language. This language will generally be used to write elementary mathematical equations. It can also be used to implement algorithms or model logic that does not fit within the System Dynamics or Agent-Based Modeling paradigms. These custom functions and code developed using the imperative environment can augment the System Dynamics or Agent-Based Modeling environments by providing additional functionality within them. For instance, a function could be defined in the imperative environment to carry out some numerical algorithm and that function will automatically become available for use by equations in the other environments.The third tier consists of the model itself along with tools and services that operate on a given model. Three primary tools are offered by the simulator: an optimizer that maximizes or minimizes a goal, a sensitivity testing tool, and an API that can be used to script model behavior or analyze model results.The simulator includes an optimizer to determine the set of parameter values that minimize or maximize an objective function. The user specifies the objective function, the parameters that should be adjusted, and the respective ranges of these parameters. The optimizer then explores the specified parameter space in search of the parameter set that achieves the objective. The study of optimization methods is a wide, multi-discipline field with numerous techniques and approaches available. Optimizations of the arbitrary user specified models in the simulator are, however, constrained by three primary features:•Non-convex objective functions: Although some models and associated optimization tasks may result in convex optimization problems, in general it should be assumed that the parameter space for a given model has multiple local minima.Non-smooth objective functions: Given the use of logical functions in a model (e.g. if-then-else statements), it is quite possible for the objective function to have sharp discontinuities. Even without the usage of such functions, the numerical limits of the floating-point mathematics engine could create discontinuities on a fine scale.No analytic derivatives: Although the derivatives or Hessian matrix at a given point in the parameter space may be estimated numerically, it is generally impossible to calculate them directly.A wide class of techniques used to address such optimization problems are known as “Direct Search” methods [21–23]. This class includes methods which do not rely upon differentials, such as the classic Nelder-Mead Simplex search [24], genetic algorithms [25] and simulated annealing [26]. Published comparisons between techniques exist (e.g. [27]); however, the results can often be context-dependent and vary across applications. For the simulator, a form of direct search based on Powell’s method [28] is implemented.Briefly, Powell’s method takes an initial starting point and a set of search vectors that span the parameter space. The objective function is evaluated at the starting point. Then for each search vector the objective is in turn evaluated at a new position that is a given search step size in the positive direction along that vector and, if an improved solution is not found, subsequently in the negative direction. As soon as an improved solution is identified, the center of the search is relocated along that search vector to a distance twice the search step size (the optimistic assumption being that if an improvement is found in a direction, the solution will continue to improve if the optimizer continues in that direction). After a full iteration through all the search vectors is completed, a combined search vector is constructed as a composite from all the moves that were taken in that iteration. This composite search vector then replaces one of the existing vectors in the set of search vectors. If no move is made, the step size is reduced or the search can be terminated according to a stopping condition.Note that the search algorithm as described is purely deterministic. The algorithm will find a single minimum value that, if multiple minima exist, may or may not be the global minimum. Many optimization techniques address this issue by introducing stochasticity into the optimization process (e.g. simulated annealing or genetic algorithms). Given convex problems, such stochasticity may, however, reduce efficiency in finding the minimum. As such, the simulator uses the deterministic algorithm as described, but deals with local minima by offering random starting locations for the search pattern. For instance, given an optimization problem where local minima are known or expected to exist, the optimizer could be run ten times, each time starting at a different random location. If all ten optimization sequences converge to the same minimum, it is highly likely the global minimum has been found. If all ten optimization sequences converge to separate minima, then little evidence is there to conclude that the global minimum has been located.The simulator’s sensitivity testing tool repeatedly runs a simulation and aggregates results. It can display the aggregated result by plotting results for each run or by averaging the runs and calculating their distribution. The sensitivity-testing tool has several general use cases:•Average responses and variances for stochastic models: Given a modification to a model’s parameter values, most models will exhibit a changed trajectory of results. For stochastic models, it can be difficult to ascertain when changes in the trajectory were caused by the parameter modifications and when changes were caused by the model’s inherent stochasticity. The sensitivity-testing tool allows the simulation to be run many times and the results averaged to numerically estimate a response independent of the stochastic variations.Parameter sweeps: Parameters sweeps can determine the model’s response to a range of different parameter values. They can be useful for exploring model behavior, and the sensitivity-testing tool can automate the process of this exploration.Uncertainty in parameter values: Model parameter values are often known with limited certainty. The data used to estimate them may be limited or in some cases non-existent. The simulator’s sensitivity-testing tool allows the formulation of a probability model to define the uncertainty for one or more parameters and then obtain a numerical estimate of the resulting distribution of outcomes given this uncertainty. This form of analysis is recommended as part of the model validation and verification process to determine whether model results and conclusions are robust to parametric uncertainty [29,30]. The simulator includes a set of functions to generate random numbers according to a range of common distributions (e.g. the normal distribution, the uniform distribution, and the log-normal distribution, among others) to model the uncertainty of knowledge about a parameter. The simulation is then repeated, each time sampling a set of a parameter values from their distributions. This Monte Carlo method enables the propagation of the uncertainty from the parameter values to the results.The simulator includes an API that can be used to programmatically build models and analyze their results. The API is implemented in ECMAScript and there are several mechanisms by which it can be accessed. The most basic is through a special primitive called a Button that can be added to the model. When a user presses a button primitive, the associated code and API commands are executed. Another mechanism that can be used to access the API is the browser’s console that allows users to directly enter API commands in an interactive manner.The range of API functions includes those for model construction (e.g. CREATEPRIMITIVE or SETVALUE), those for styling the model (e.g. SETLINECOLOR or SETSIZE), those for running the model (RUNMODEL), and those commands for input and output (e.g. SHOWDATA or PROMPT). As the commands are implemented in ECMAScript, any standard ECMAScript data analysis or communication functions may, of course, also be used. These capabilities allow model scripts to, for instance, download parameter data from an external server on the fly, run the model, analyze the results, and/or upload the results summary to another server.Given the usage of standard web technologies (HTML, CSS, and ECMAScript) by the simulator and model construction interfaces, these aspects of the application are portable and may be deployed in any environment that supports rendering and displaying web pages. The most common usage for this capability is to deploy content within other web pages. Models built within Insight Maker can be embedded within external web pages and can retain full interactivity within these external environments. Users embedding models can also harness the simulator’s API to develop a custom user interface for a model and simulation. Such custom user interfaces can be used to develop “serious games” (e.g. [31], a game exploring the Israel-Palestine conflict) or “flight simulators” (e.g. [32,33] for discussions of flight simulators in relation to management education and training).Although the application’s development process does not follow testing approaches such as Test Driven Development [34], an automated suite of internal tests has a key role in ensuring proper simulation behavior and the prevention of regressions. An extensive suite of over 1000 individual tests is part of the simulator that comprehensively evaluates the correctness of all aspects of the simulator’s behavior. The tests range from the trivial (does the equation engine correctly evaluate that 2+2 equals 4?) to higher-level aggregate tests (does a given model containing multiple equations and primitives evaluate to the correct result?). Tests are written as new functionality is added to the program and also in response to reports of issues from users. When an issue is found, the issue is first corrected and then a test for the issue is written to ensure that it does not reappear in a later regression. The development of this test suite has proven invaluable in ensuring the proper operation of the tool.

@&#CONCLUSIONS@&#
This study has been a complex and long-running one that accomplished four primary goals to date:1.A general-purpose simulation and modeling tool was built using web-based technologies that supports multi-paradigm modeling using a graphical model construction interface.The simulation and modeling tool combined both features targeted at laypeople and individuals new to the modeling field (e.g. a graphical interface) with those targeted at more advanced users (e.g. API and scripting, the optimization toolset) within a unified interface.The tool was iterated based on user feedback and its direction was shaped by the needs and problems faced by its users. This led to an evolution of the software that, though now somewhat different from the original vision, better met the needs of the majority of users interested in using a general web-based modeling tool.Usage of the tool continues to grow, indicating the existence of a need and desire for a general web-based modeling tool and validating the web as a platform for developing complex simulation and modeling tools. As of February 2014, the software has some 20,000 registered users with over 12,000 models constructed and saved in its database.Along the way, challenges and points of resistance from users were encountered in the development of the application that stemmed from its nature utilizing web-based technologies. Four key challenges have been:1.Security of intellectual property: Models are stored on a centralized server not controlled by the users.6When InsightMaker.com is used, all models must be saved on the centralized server. The Insight Maker software is open-source and users can download it and set it up on their own computers. But doing so may be quite difficult for users who are not highly technically proficient.6For most users this is acceptable (and they have the ability to back-up the model to their local machines). However, for certain users – especially those within a corporate environment – lack of control over the model may be viewed as a security risk. Although Insight Maker incorporated fine-grained settings to control model access, the fact that the model is stored on an external server made it inappropriate for some potential users.Performance: Web-based technologies are fundamentally limited in the performance they can obtain compared to their counterparts developed in more traditional languages. That said, the performance of Insight Maker is quite good in practice and more than adequate for its main target usage cases.Unfamiliarity of web-based environments: Many users at present are not comfortable using a simulation and modeling tool within their web browser. It is expected that this will change as people become more familiar with using rich applications within browsers. Such a trend was seen historically with email clients where once Desktop clients were the norm, but now web-based clients are ubiquitous.Perceived application quality: A final challenge centers on a perception that web-based applications are inferior to their desktop counterparts. In the case of the modeling and simulation tool presented here, it is in some ways superior in capabilities and features to even commercial counterparts. However, the stigma and experience of the restrictive web applications that users may have come into contact with in the past is difficult to overcome. Again, this is a difficulty that is expected to diminish over time as users become more familiar with high-quality interactive web-based applications.Despite the challenges, modeling and simulation tools developed using web-based technologies have a distinctly positive future. The application presented in this paper demonstrates the possibilities to create extensive modeling and simulation tools in a web-based environment. Given the steady improvement in web technologies, the possibilities available to build such applications will only grow over time. The benefits offered to developers using web-based technologies – increased accessibility, ability to push updates to users immediately, a rich toolkit of open-source libraries, etc. – are starting to outweigh the costs of web-based technologies in cases other than those where performance is the key factor. It is not hard to imagine a future where web-based technologies are the primary platform for developing simulation and modeling tools outside that of performance critical applications.