@&#MAIN-TITLE@&#
Focus-aided scene segmentation

@&#HIGHLIGHTS@&#
Focus has been typically exploited in computer vision as depth cue.A new focus-based perceptual cue is introduced: the focus signal.The focus signal corresponds to the change in focus level as a function of time.The focus signal can be integrated with classical cues for image segmentation.The proposed focus-aided methodology yields improved scene segmentation.

@&#KEYPHRASES@&#
Image sequences,Focus measure,Segmentation,Defocus,Visual cue integration,Focus cue,

@&#ABSTRACT@&#
Classical image segmentation techniques in computer vision exploit visual cues such as image edges, lines, color and texture. Due to the complexity of real scenarios, the main challenge is achieving meaningful segmentation of the imaged scene since real objects have substantial discontinuities in these visual cues. In this paper, a new focus-based perceptual cue is introduced: the focus signal. The focus signal captures the variations of the focus level of every image pixel as a function of time and is directly related to the geometry of the scene. In a practical application, a sequence of images corresponding to an autofocus sequence is processed in order to infer geometric information of the imaged scene using the focus signal. This information is integrated with the segmentation obtained using classical cues, such as color and texture, in order to yield an improved scene segmentation. Experiments have been performed using different off-the-shelf cameras including a webcam, a compact digital photography camera and a surveillance camera. Obtained results using Dice’s similarity coefficient and the pixel labeling error show that a significant improvement in the final segmentation can be achieved by incorporating the information obtained from the focus signal in the segmentation process.

@&#INTRODUCTION@&#
The visual system is fundamental for humans in order to perceive the surrounding environment. At the lowest level of scene understanding, the task of distinguishing different objects is fundamental due to its key role in the interaction with the perceived world. In particular, segmenting a scene into different objects can be understood as finding the spatial relationship between them and their distance or depth from the observer. In his seminal work on physiological optics, von Helmholtz distinguishes two main sources of visual information or visual cues[1]: the first source relies on experience and some familiarity with the nature of the perceived scene. Some mechanisms corresponding to this source are the determination of distance by means of the relative size of objects, their perspective, texture patterns and shading. The second source involves and actual perception of depth, such as vergence, depth perception by motion parallax (e.g., by moving the head), stereopsis (binocular or stereo vision), and accommodation (or focusing).Albeit each visual cue can be regarded as an independent source of information, at a higher complexity level, object segmentation is a complex task that involves the combination and interaction of different cues. As a result, in order to yield a deeper understanding of the human visual system, it is critical not only to understand each individual cue, its principles, limitations and advantages, but also to understand their integration and interaction.The focus cue in both human and computer vision is an increasingly important research field. Interestingly enough, the literature concerning the integration of the focus cue with classical perceptual cues, such as color, texture, shading, etc., is scarce. In this work, a framework for the efficient integration of the focus cue with other low-level monocular cues in order to yield an improved scene segmentation is proposed. Specifically, this paper presents a new interpretation for the autofocusing process by exploiting the implicit information about the scene geometry found in the variations of the focus level yielding and improved image segmentation. The claim is that, being autofocus an unavoidable part of systems with limited depth-of-field, it is possible to take advantage of this process in order to infer useful information about the imaged scene.Based on an analysis of the image formation process of a defocused image, we propose to model autofocus as a time-variant interaction between the capturing device and the observed scene, showing that each imaged point generates a pattern, or focus signal, that mainly depends on the configuration of the lens-camera system and the scene geometry. Since at every instant, the lens-camera system has the same configuration for all imaged points, the scene geometry (in particular the separation of objects in space) can be estimated as a function of the different focus signals, where the focus signals correspond to the focus measure as a function of time.For illustration, Fig. 1shows an image stack of a video sequence recorded while a camera is autofocusing on a real scene. The scene is divided into a discrete number of local regions of interest. An initial focus-based segmentation of the scene is recovered by clustering the focus signals extracted from each region of interest. The signal clusters depicted in Fig. 1 can be interpreted as a segmentation process through which the image is segmented into disjoint regions by taking into account the geometry of the scene rather than the color or texture features typically used in segmentation tasks. Finally, the segmentation is refined by incorporating texture and color cues by means of classical segmentation schemes.This paper is organized as follows. Section 2 reviews previous related work. Fundamental concepts for the proposed approach, namely focus measure and the focus signal, are introduced in Sections 3 and 4. The methodology for exploiting the focus signal in order to yield an improved scene segmentation is presented in Section 5. The proposed approach is experimentally evaluated in Section 6. Section 7 is a final discussion.Since the work by von Helmholtz [1], the role of accommodation and defocus in visual perception has extensively been assessed in the literature of human vision. More recently, it has been experimentally shown that, in fact, the observed amount of defocus blur is an independent human pictorial depth cue by itself [2,3]. These results suggest that, in addition to classical cues (stereopsis, shading, texture, perspective, etc.), the perceived blur is exploited for retrieving information about the structure of the scene, such as the depth of objects and occlusions.In computer vision, different researchers have tackled the problem of integrating different visual cues. Early efforts to integrate different low-level visual cues can be traced back to [4], where the joint effect of stereo, shading and texture was analyzed in depth perception. In this scope, the existing approaches have mostly dealt with the integration of low- and high-level perceptual cues not related to focus, such as contours, optical flow and image features [5,6]. More recently, Sabatini et al. integrated optical flow, contrast orientation and binocular disparity for depth perception [7]. In [8], Ogale and Aloimonos proposed a framework for the cooperative integration of disparity with basic visual models, such as segmentation, shape and depth estimation, and occlusion detection. Some researchers have devoted their efforts on assessing the integration of visual cues, such as perspective and disparity [9,10] or texture and shading [11] for specific tasks, such as detecting surface orientation. Similarly, Pang et al. integrated disparity, color and shape for real-time object tracking [12]. Recent approaches are aimed at the integration of multi-sensor data for performing basic visual tasks such as visual tracking and simultaneous location and mapping [13,14]. In cooperative cue integration for improved perception, perceptual grouping and stereo disparity have also been studied [15].Most research involving the focus cue has been devoted to depth estimation through shape-from-focus (SFF) and shape-from-defocus (SFD). SFF consists in capturing an ordered sequence of images of the same scene with different camera settings in order to change the degree of focus of every imaged scene point. A depth-map can be estimated by measuring the focus level of each point of the scene whereas the position at which each point presents maximum focus will determine its position with respect to the camera [16,17]. In SFF, it is assumed that the lens-camera configuration (in terms of the in-focus position) is known for each captured scene. SFF has been applied in microelectronics, fracture analysis, polymeric texture analysis and the reconstruction of microscopic objects [18–22].SFD is based on the principle that, in diffraction limited optics, depth can be estimated by measuring the amount of defocus [23–25]. SFD aims at determining depth as a function of the relative defocus between observations. There have been different approaches in both the frequency domain [26] and the spatial domain of images [25,27] for estimating depth from two or more images. More recently, Zhuo and Zim have proposed a method to obtain a defocus map from a single image [28]. SFD, as well as SFF, perform poorly in scenes with low texture content and have often been applied in controlled environments.If the camera’s hardware is allowed to be modified and manipulated, different focus-based alternatives that have provided good results in focus-based depth estimation are the coded aperture [29,30] and the plenoptic imaging [31] approaches. With an rigurous control of the camera’s optics and configuration (namely, the lens focal length, aperture and exposure time), light-efficient photography has been also studied [32]. From the very beginnings of the shape-from-defocus framework, Engelhardt and Knop exploited active illumination in order to perform real-time focus-based depth estimation [33]. Subsequently, active depth recovery by projection of light patterns has been exploited in [34] and, more recently, in [35].The literature specifically regarding the combination of focus with other perceptual cues is relatively scarce. In particular, [36] proposed a hybrid system that combines stereo and shape-from-focus in order to estimate depth. To the best of our knowledge, the problem of integrating the focus cue with classical visual cues such as texture and color for scene segmentation has not been tackled previously.In order to obtain a deeper understanding of the focus cue, it is necessary to analyze how focus and defocus are perceived by artificial vision systems, such as digital cameras. In particular, the method proposed here for the integration of the focus cue for scene segmentation is based on two assumptions:1.Objects at different depths from the acquisition device generate different focus signals during autofocus.Two observed focus measures corresponding to different camera configurations (different focus setting) are statistically independent.The rationale behind these assumptions is explained as follows.When a point object P is observed through a lens, its radiance spreads into a defocused pattern, I. As illustrated in Fig. 2, that pattern can be modeled as the convolution of P with a Gaussian point spread function (PSF) of radiusρ. The radius of the blurring Gaussian depends on the position of the real object, u, and the separation between the imaging device (the sensor in Fig. 2) and the lens,δ[37]:(1)ρ=πDδ21δ+1u+1f,where f and D are the lens focal length and its aperture diameter, respectively.This defocus model has two important implications: first, the value of the focus measure for a particular point is determined by the relationship between the scene geometry (in terms of its depth u) and the configuration of the capturing device (the lens positionδand parameters f and D). Second, the energy of the blured image I depends on amount of defocus of the system which, in turn, is given byρ.In different focus-related techniques, a focus measure is defined in order to estimate the degree of focus of each pixel of an imageI(x,y). This is achieved by applying a transformation to I usually referred to as a focus measure operator[38]. Several focus measure operators have been proposed in the literature, such as the modified Laplacian, the Tenenbaum algorithm, the gray-level variance, and the sum of wavelet coefficients, among many others [16,38–40]. In practice, a large image sub-region must be analyzed in order to provide a high signal-to-noise ratio (SNR) and to be able to accurately detect any change in the focus level. In other words, the energy or average of the transformed image over a region of interest (ROI) is used as an estimator of the focus level. Thus, the focus measureφcorresponding to a ROIΩis defined as:(2)φ=1S(Ω)∫(x,y)∈ΩIt(x,y)dΩ,whereItis the transformed image after the application of the focus measure operator andS(Ω)is the area ofΩ.In [39], Subbarao et al. showed that the expected value of the focus measure in a real noisy image is the sum of the focus measure of the ideal noiseless image plus a constant value proportional to the noise variance. According to the image formation process of digital images, noise is independent of the lens position. It actually depends on the camera’s sensor [41,42]. Therefore, two observed focus measures corresponding to different lens configurations are statistically independent random variables [39,43]. In this work, it is further assumed that these variables are normally distributed.The focus signal is defined as the focus measure value as a function of time,φvs. t, corresponding to a particular image pixel (or image region). As previously stated in Section 3, the focus level is a function of the target position, u, and the internal configuration of the camera,δ. As a result, by assuming that the only parameter of the acquisition device that changes during autofocusing is the lens-sensor distance,δ, the focus signal can then be expressed as:(3)φ(t)=f(u,δ(t)),wheref(·)is a given function andδ(t)is the variation of the camera’s focus as a function of time. Notice thatδ(·)is explicitly stated as a function of time since the focus setting of the camera is assumed to change during autofocusing.1Functionf(·)relates the focus setting of the camera with the estimated focus level of an imaged point and is referred to as focus profile. For a review of different focus profile models, the reader is referred to [17].1Since the focus of the camera,δ(t), is the same for all the scene points at each time instant, objects at different positions can be univocally distinguished from their corresponding focus signals. In other words, two objects at different target positions,u1andu2, will yield different focus signals,φ1(t)≠φ2(t). This particular approach has the advantage of not relying on the absolute or relative degree of focus (which, in turn, depends on the image content, image noise and the focus measure operator) and requires no knowledge about the camera or the acquisition parameters. This fact is illustrated in the following experiment: Fig. 3(a) shows a highly-textured target in front of a camera at a fixed distance,u1. An image sequence is then captured by moving the focus of the camera back and forward between +0.5m and −0.5m around the target position, thus covering a range of 1m. The experiment is then repeated by moving the target to a new position,u2(withu2≠u1). Fig. 3(b) plots the focus signals for the first target (φ1) and the second target (φ2).Notice that for both target positions in the previous experiments, the information of the focus position is lost since the focus signals are a function of time. The internal parameters of the camera, namely the lens focal length, aperture and lens-sensor distance, are assumed to be unknown. In addition, since the only difference between both targets is their distance from the camera, they cannot be differentiated by means of the focus level or image content (color, texture, etc). Remarkably, it is evident from Fig. 3(b) that targets at different positions clearly yield different focus signals. Notwithstanding, this is only valid for the ideal noiseless, aberration-free and highly-textured case. Further considerations must be taken into account in order to apply it to complex scenes in real imaging conditions, since the effects of noise, camera movement during autofocusing, and vibration, among others, make it harder to distinguish the focus signals corresponding to objects at different positions.Let us consider the simplest case of a fronto planar object at a distance u from the camera, partitioned into a finite set of N non-overlapping ROIs:{Ωn|n=1,2,…,N}. In the limit case, when the area of each ROI tends to 0, eachΩncorresponds to an image pixel. The focus signal corresponding to the n-th ROI,xn, is defined as the sequence of values of the focus measure overΩnat subsequent time instants. From this perspective, the autofocusing process yields a set of N time-varying focus signals of finite duration. Specifically, for the case of discrete-time signals, the autofocusing process yields a set of N vectors (signals) such that the n-th vector hasτelements,xn=[φn(t1),φn(t2),…,φn(tτ)]T, withτbeing the number of frames of the autofocus sequence.In real imaging conditions, Subbarao and Tian showed that the expected valueμof the focus measureφin a real noisy image is the sum of the focus measure of the ideal noiseless image plus a constant value proportional to the noise variance [39]. As previously stated in Section 3, noise is independent of the lens position. It actually depends on the camera’s sensor. Therefore, two observed focus measures,φ(t1)andφ(t2), corresponding to different lens configurations are statistically independent, normally distributed random variables. Therefore, each focus measure value can be treated as a random variable with a Gaussian probability density function (PDF). Thus, as long as the focus signals correspond to ROIs at the same distance from the camera, they can be considered to be samples from a stochastic Gaussian process whose probability density function can be modeled as a multivariate normal distribution [44]:x∼Nx;μ,Σ,x∈x1,x2,…,xM, whereNx;μ,Σis given by (4).(4)Nx;μ,Σ=(2π)-τ/2|Σ|-1/2×exp-12(x-μ)TΣ-1(x-μ).In (4), the meanμis the vector[μ1,μ2,…,μτ]Tcorresponding to the ensemble average, that is, the average of all the focus signals for each time instant, andΣis a diagonal correlation matrix:diag(σ1,σ2,…,στ). The diagonality ofΣcomes from the statistical independence of eachφm(t).The algorithm to retrieve the focus-aided segmentation of a scene from the images captured during autofocus can be divided into three stages: first, an initial scene segmentation is obtained by clustering the focus signals according to the formulated model. Then, a refinement step is carried out in order to improve the initial segmentation, yielding a focus-based segmentation of the imaged scene. Finally, the boundaries of the coarse approximation are improved by incorporating the information obtained from texture and color cues. These stages are described below.Eq. (4) corresponds to the PDF that models the focus signals originated from a single planar object perpendicular to the camera. However, in complex scenes with many different objects at different positions, a more flexible model is required. For this purpose, a Gaussian mixture model (GMM) is introduced. GMMs are a popular tool with application to statistical signal processing [45], speech recognition [46] and biomedical signal processing [47], among many others.In particular, let us partition the scene into N non-overlapping ROIs,Ω1,Ω2,…,ΩN, of fixed size. In this work, ROI sizes greater than one pixel are used for two reasons: first, to provide a high SNR for a reliable measurement of the focus level and, second, to reduce the number of sample data, thus improving the processing time. The sample data X is the set of the N focus signals corresponding to the N ROIs:X=x1,x2,…,xN. The aim is to find a probability density functionΓ(x,θ),x∈X, corresponding to a family of multivariate Gaussian distributions that is most likely to have generated the sample data:(5)Γ(x,θ)=∑m=1MωmNm(x;μm,Σm),where M is the number of Gaussians in the model,N(·)is defined as in (4),ωmis the weight of the m-th Gaussian function (mixing probability) andθis the set of parameters of the model:θ=ωm,μm,Σm|m=1,2,…,M. SinceΓ(·)is a probability density function, the weightsωmmust add to one:∑m=1Mωm=1. For the sake of simplicity,Nm(x;μm,Σm)will be abbreviated asNm(x)in the sequel.The problem now reduces to finding the parameter setθˆthat maximizes the data log-likelihood:(6)θˆ=argmaxθlogP(X|θ),where,P(X|θ)=∏n=1NΓ(xn,θ).There is no closed-form solution for (6), although it is differentiable. Therefore, any general purpose non-linear optimizer can be used to solve it. Notwithstanding, the expectation–maximization (EM) algorithm provides a convenient solution for the case of Gaussian mixtures. The EM algorithm iteratively generates a sequence of estimates,θk, from an initial guessθ0. In the literature, the initial parameters are often calculated by applying a supervised clustering algorithm to the data (e.g., k-means) when the number of Gaussians is known. In this work, the x-means algorithm [48] has been used for automatically finding the number of clusters M and initializing the parameter vector. This allows performing a non-supervised clustering, thus avoiding the need for previous training stages. A detailed description of the EM algorithm for Gaussian mixtures can be found in [45,49].Once the model in (5) has been found, the sample data can be clustered by assigning each focus signal to the Gaussian with the highest posterior probability for that signal. Thus, the n-th focus signal (and its corresponding ROIΩn) will be assigned to clusterCnaccording to the following rule:(7)Cn=argmaxmPxn|ωmNm(x)Eq. (7) merges the ROIs that likely correspond to the same Gaussian of the mixture.According to the model proposed in Section 4, objects at different depths in a scene can be differentiated by analyzing the focus signals. For illustration, Fig. 4(a) shows three ROIs manually selected from a real scene (Ω1,Ω2andΩ3) and their corresponding focus signals (Fig. 4(b)). Clearly, focus signals drawn from the same object correspond to similar patterns. Therefore, the signals from the same object are likely to correspond to the same Gaussian in (5).Fig. 4(c) shows the three final signal clusters (C1,C2andC3) obtained after applying the signal clustering procedure described previously. Clearly, the selected clusters correspond to different objects in the scene. This shows that objects at different positions from the camera generate focus signals with different patterns. The proposed method clusters the scene regions accordingly. Remarkably, this focus-based segmentation has been generated based solely on the focus signals and has not explicitly incorporated color and texture cues so far.In order to be able to detect different objects by means of the focus signals, it is assumed that the relationship between the camera-lens system and the scene changes during autofocus. That is, the lens moves yielding a detectable change in the focus level of the imaged points. In our experiments the signal clustering stage was successfully executed for autofocus sequences from 300ms up to 3.4s of duration.By dividing the scene into ROIs of finite size greater than one pixel, the robustness of the focus signals is improved at the cost of spatial resolution. Therefore, the clustering obtained by (7) is further refined using an iterative procedure as described below.The initial segmentation is obtained by only using the information in the focus signals without any assumptions about the spatial relationship with the scene ROIs. In order to refine the boundaries between regions, a quad-tree subdivision is proposed. The latter is carried out by dividing each sub-window belonging to the boundaries among two or more different regions into four quadrants of equal size. For each quadrantQi⊂Ωn|i=1,2,3,4a focus signalxiis computed using the corresponding scene region. Each quadrantQiis then compared against the clusters in its neighborhood and reassigned to the clusterCnwith the highest likelihood:(8)Cn=argmaxj∈Nh(Qi)P(xi|ωjNj(x)),whereNh(Qi)is the set of clusters in the neighborhood ofQi.The quad-tree subdivision process is recursively iterated until the smallest integer ROI size is reached. The re-assignment rule of (8) is parameter-free and consistent with the model developed in the previous section. In order to compensate for variations in the focus measure due to changes in illumination and the amount of texture in the scenes, the focus signals are standardized to have zero mean and a standard deviation of one.In the first stage (signal clustering), the number of Gaussians has been selected using x-means, a general purpose unsupervised clustering algorithm not designed for this particular application. The non-optimal selection of the number of components of the GMM often leads to an over-segmented scene. Therefore, a consistency test is applied in order to fuse neighboring clusters that are unlikely to correspond to different objects. In order to be consistent with the proposed autofocus model, that fusion should take into account the similarity between the distributions that correspond to each cluster. In this work, two clusters a and b are fused if they satisfy:(9)||μa-μb||<min||σa||,||σb||,whereσaandσbare vectors of the form[σ1,…,στ]Tcorresponding to the traces ofΣaandΣb, respectively.Eq. (9) aims at reducing the number of clusters (over-segmentation) and merges two clusters whenever the inter-mean distance lies within a hyper-sphere of radius||σ||. The scene refinement in a real scene is illustrated in Fig. 5. Fig. 5(a) shows the initial segmentation obtained by clustering the focus signals using (7). The cluster’s boundaries are further refined using (8), as shown in Fig. 5(b). The final segmentation shown in Fig. 5(c) is obtained by merging similar clusters by means of (9).The focus-based segmentation of the scene can be interpreted as an object-oriented scene segmentation: that is, the scene is segmented according to geometrically discontinuous objects that generated different focus signals. On the one hand, this representation of the scene is quite meaningful in the sense that it provides a simple description of the real geometry of the scene by clustering objects according to their depth and spatial location with respect to the camera. On the other hand, the coarseness of the obtained segmentation - more specifically, its contours- is an undesired artifact that should be minimized in order to improve the accuracy of the geometric description of the scene. However, it is important to recall that, so far, these results have been obtained exclusively based on the information extracted from the focus signals, with classical image perceptual cues, such as color, texture, local brightness and edges not having been explicitly exploited. As a result, the coarse focus-based can be further improved by incorporating the information of these cues as shown below.This stage takes advantage of the state-of-the-art in image segmentation in order to post-process and improve the coarse segmentation obtained by the methodology described in the previous sections. In particular, the statistical region merging approach proposed by Nock and Nielsen has been selected due to a combination of reasonable performance, efficiency, publicly available implementation and low parametrization [50]. The proposed perceptual cue integration is simply carried out in two steps as described below.1.The imaged scene is divided into P super-pixels{χp|p=1,2,…,P}using a segmentation algorithm (e.g., statistical region merging [50]).Each generated super-pixel,χp, is assigned to a cluster of the coarse approximation of the scene. Specifically, the super-pixelχpis assigned to theñ-th cluster that maximizes the overlap:(10)ñ=argmaxn∑(i,j)On(i,j),where,(11)On(x,y)=1ifχp(x,y)∈Cn,0otherwise.The visual cue integration framework is illustrated in Fig. 6. Fig. 6(a) and (b) respectively correspond to the focus-based segmentation and the segmentation obtained using a classical texture and color-based segmentation approach, the later being the statistical region merging algorithm proposed in [50]. From Fig. 6(b) it is evident that the image segmentation algorithm yields an over-segmented result, arguably due to the complexity of the particular scene. In contrast, the result shown in Fig. 6(c) that integrates the focus cue is more accurate in terms of the real scene geometry.The proposed method for scene segmentation is evaluated in this section. Test sequences have been obtained through different off-the-shelf cameras: a compact digital photographic camera Sony DSC-HX5 (Cam1), a Logitech Orbit AF (Cam2) and a surveillance camera Sony SNC-RZ50P (Cam3). The autofocus sequences were obtained by simply pointing each camera to the scene and activating the autofocus mechanism. Each autofocus sequence was recorded on video and then converted to separate image frames in order to process them. Details about the capturing device, number of frames and image size of each sequence are summarized in Table 1. In this table, the sequence number is in correspondence with the row number in Fig. 9. The sequences have been selected in order to include realistic indoor scenarios with complex and cluttered backgrounds at different image resolutions and qualities.This section describes the results obtained using the proposed approach for focus-aided scene segmentation from the image frames of real autofocus sequences. Each row of Fig. 7corresponds to a different image sequence. These sequences have been selected for discussion since they represent challenging scenarios with different imaging and acquisition conditions.In particular, the first row of Fig. 7 corresponds to a video sequence of approximately 0.3s captured with a photographic camera. This particular scene has several challenging features for both classical segmentation approaches and focus-based depth estimation techniques, such as highly-textured and weakly-textured areas, objects with multiple colors and texture patterns, low-contrast regions, reflective surfaces, and a complex background, among others. The coarse segmentation obtained using the proposed signal clustering approach without the integration of color or texture cues (second column) provides a meaningful description of the scene in terms of the scene’s geometry: the two foreground objects are clearly isolated from the background. However, the objects’ boundaries are coarse and inaccurate. Conversely, the segmentation shown in the third column of this figure provides an accurate description of the scene in terms of region boundaries and both color and texture consistency. However, the regions of the segmented image are difficult to interpret in terms of the scene geometry. The proposed focus-based segmentation (last column) shows a meaningful description of the scene’s geometry with improved region boundaries. Different regions corresponding to the same object are successfully merged despite the differences in texture and color content.The second and third rows of Fig. 7 correspond to image sequences captured with a webcam (Cam2) and a surveillance camera (Cam3). In addition to the complexity of the imaged scene, the particular characteristics of the acquisition device, such as vignetting, distortion and image shift, pose an additional challenge. Notwithstanding, the obtained segmentations are meaningful in terms of both the region boundaries and the layout of the objects with respect to the camera.The proposed methodology is compatible with any camera with autofocus mechanism (not limited to contrast-based autofocus), as long as a video sequence can be captured during the focusing process.For comparison purposes, the all-in-focus image of the autofocus sequences was used in order to segment the corresponding scene using two classical segmentation approaches: the widely known graph-based algorithm proposed by Felzeswalb (Seg1 [51]) and the statistical region merging algorithm (Seg2 [50]). In order to measure the performance of each algorithm quantitatively, two quality measures have been utilized: Dice’s similarity coefficient (DSC) and the labeling error (Err). DSC is commonly used as a statistical validation measure in order to compare the spatial overlap between two images [52,53]:(12)DSC(A,B)=2|A∩B||A|+|B|,where A and B represent the pixels of the ground truth and the corresponding segmentation, respectively. Since each scene is composed by several objects, DSC is computed for each object of the ground truth with the cluster that shows maximum overlap in the segmented scene and the average of all objects is used as quality measure for that particular scene. Following a similar approach, the labeling error is simply measured as the average of the percentage of mis-classified pixels corresponding to each object of the ground truth. The ground truths have been generated manually based on previous knowledge about the position of the objects in each scene. Notice that, although the ground truths have been carefully generated taking into account the layout of the real scene, image segmentation is a subjective task that can bias the quantitative results.The obtained results are summarized in Fig. 8and Fig. 9. From Fig. 8 it is clear that the proposed defocus-aided segmentation consistently yields a higher similarity coefficient (DSC) and a lower labeling error. In addition, Fig. 9 clearly shows that the proposed approach significantly reduces the over-segmentation of the scene and groups objects meaningfully according to their spatial relationship.The inherent complexity of the segmentation problem is well illustrated in the scenes shown in the fourth and last rows of Fig. 9, where the background is segmented in multiple regions due to differences in texture and color patterns. These scenes consist of simple scenarios with a single foreground object and a complex background. The main qualitative advantage of the segmentation generated with the proposed approach is avoiding over-segmenting thus yielding simple yet meaningful segmentation.In contrast to previous focus-based approaches, the proposed method does not require:1.Knowledge or estimation of the parameters of the acquisition device, nor accurate control of the camera’s optics or calibration as in SFD, SFF and light-efficient photography [17,27,32].User interaction (e.g., the photomontage step in light-efficient photography [32]).Special hardware, as in coded aperture and plenoptic cameras [29,31].The main limitation of the proposed approach is the requirement of an appreciable change in the focus level during the autofocus process so that the focus cue can be positively contribute to the segmentation. This implies that, if either the focus of the camera remains static or the imaged objects are too close to one another, different objects can be merged together since the differences in their focus signal is negligible. This effect can be partially appreciated in the fifth row of Fig. 9. In addition, the requirement of an autofocus sequence, limits the application of this approach to prospective studies, since previously existing datasets do not provide these sequences. However, it is important to remark that camera autofocus is often seen as an undesirable yet unavoidable part of image acquisition. The focus-aided scene segmentation proposed here allows to leverage the information contained in the focus signals in order to improve image segmentation. In future work, this approach could be improved by incorporating concepts from shape-from-defocus in order to generate the focus-based segmentation based on fewer images.The algorithm’s parameter, that is, the initial ROI size, has been determined experimentally. The selection of the ROI size is a tradeoff between accuracy in the detection of the focus level and spatial resolution. A large ROI increases the SNR, thus making the estimation of the focus level more reliable. On the other hand, a small ROI will reveal small features in the scene. For all the results shown in this paper, the ROI size has been set to1/20th of the total frame size. For instance, for images of640×480pixels, a ROI of32×24pixels have been used.In the present work, the sum of wavelet coefficients has been used as a focus measure operator:It=|WLH|+|WHL|+|WHH|, whereWLH,WHLandWHHare the coefficients of the three detail sub-bands of the over-complete discrete wavelet transform (DWT) of I. Following [40,19], the 1-level DWT with Daubechies-6 filters has been used in this work. This operator was selected based on experimentation and since it can be efficiently implemented in the multi-scale framework of Section 5.1.As for the segmentation algorithm used in the cue-integration stage, the parameter of the statistical region merging approach has been fixed toQ=64for all the experiments. For a detailed description of this parameter and its meaning, the reader is referred to the corresponding reference [50].The proposed approach has been compared with traditional shape-from-focus (SFF) [16,17] and diffusion-based shape-from-defocus (SFD) [37]. Although SFF and SFD are used for focus-based depth estimation, whereas the proposed approach is used for segmentation, this experiment has been provided in order to illustrate the differences between these techniques. In order to apply SFD, two frames were manually selected from each autofocus sequence and the algorithm proposed in [37] was used. This procedure was repeated with different image pairs. The best result was kept. Since the lens moves alternatively backwards and forward during autofocus, the frames of each sequence were manually ordered before applying SFF. Fig. 10compares the obtained depth-maps using SFF and SFD with the segmentation obtained with the proposed approach. Since none of the methods provides absolute depth measures, the depth-maps obtained with SFF and SFD have been normalized in the interval[0,1], with 1 being represented with the brightest color.As shown in Fig. 10, the main drawback of SFF is its sensitivity to the lack of texture, whereas the depth estimation becomes unreliable in low-textured areas, generating a noisy depth-map. In contrast, SFD yields an over-smoothed depth-map, making it difficult to distinguish among objects at different depths. In addition, SFD is also sensitive to complex backgrounds (e.g., the scenes at the bottom row). Although the proposed approach does not guarantee the estimation of the depth of the imaged objects -unless the focus position is accurately known for each camera setting- it provides a meaningful description of the scene in term of object discontinuities.

@&#CONCLUSIONS@&#
A new focus-based segmentation cue has been introduced. The focus measures obtained during the focusing process of a camera, namely the focus signals, have been used in order to identify objects at different distances from the camera. The proposed method works by clustering the focus signals without any knowledge of the parameters of the lens during the focusing process. A practical application of the proposed algorithm has been developed by processing autofocus video sequences for obtaining a focus-aided segmentation of the imaged scene. Experiments using different cameras and complex scenes show the potential of the proposed approach.Due to limitations in the depth-of-field, autofocus is currently an important feature of most off-the-shelf digital cameras. The results in this work show experimental evidence that the autofocus sequences can be successfully exploited in order to improve segmentation of tasks. Although the proposed approach does not provide information about the absolute depth or real layout of objects in the scene, the obtained results are potentially useful for scene understanding tasks, such as object segmentation and recognition and depth ordering [54,55], as well as computational photography applications, such as digital refocusing and defocusing [56,57].In computer vision, the integration of different perceptual cues is an increasingly important research field. In this scope, the proposed approach provides a new framework for the integration of classical perceptual cues, such as texture and color, with the focus cue. In the field of pattern recognition, the proposed concept of the focus signal paves the way for future research exploiting this cue as a novel source of information for scene analysis and understanding. In addition, digital signal processing approaches have the potential to improve the quality of the focus signals in real time and, ultimately, yield improved segmentation results.