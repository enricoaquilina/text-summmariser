@&#MAIN-TITLE@&#
Take it to the limit: Innovative CVaR applications to extreme credit risk measurement

@&#HIGHLIGHTS@&#
Four innovative credit models are developed that apply CVaR metrics to credit risk.Traditional credit models understated risk by about three times during the GFC.Rating models respond slowly to economic changes as compared to our models.Our models measure capital buffers needed by banks in dynamic economic conditions.

@&#KEYPHRASES@&#
Uncertainty modeling,Credit risk,Conditional Value at Risk,Conditional probability of default,Capital buffers,

@&#ABSTRACT@&#
The Global Financial Crisis (GFC) demonstrated the devastating impact of extreme credit risk on global economic stability. We develop four credit models to better measure credit risk in extreme economic circumstances, by applying innovative Conditional Value at Risk (CVaR) techniques to structural models (called Xtreme-S), transition models (Xtreme-T), quantile regression models (Xtreme-Q), and the author's unique iTransition model (Xtreme-i) which incorporates industry factors into transition matrices. We find the Xtreme-S and Xtreme-Q models to be the most responsive to changing market conditions. The paper also demonstrates how the models can be used to determine capital buffers required to deal with extreme credit risk.

@&#INTRODUCTION@&#
The Global Financial Crisis (GFC) raised widespread spread concern about the ability of banks to accurately measure and provide for credit risk during extreme downturns. Prevailing widely used credit models were generally designed to predict credit risk on the basis of ‘average’ credit risks over time, or in the case of Value at Risk (VaR) models on the basis of risks falling below a pre-determined threshold at a selected level of confidence, such as 95 percent or 99 percent. The problem with these models is that they are not designed to measure the most extreme losses, i.e. those in the tail of the credit loss distribution. It is precisely during these extreme circumstances when firms are most likely to fail, and it is exactly these situations that the models in this study are designed to capture.Although the use of VaR (which measures potential losses over a given time period at a pre-determined confidence) is widespread, particularly since its adaptation as a primary market risk measure in the Basel Accords, it is not without criticism. Critics include Standard and Poor's analysts (Samanta, Azarchs, & Hill, 2005) due to inconsistency of VaR application across institutions and lack of tail risk assessment. McAleer (2009) finds that the internal modeling VaR approach as contained in the Basel II Accord (now to be superseded by Basel III with a greater focus on cyclical risk) seemed to encourage excessive risk taking at the expense of providing accurate measures and forecasts of risk. VaR has also been criticized by Artzner, Delbaen, Eben and Heath (1999) as it does not satisfy mathematical properties such as subadditivity. Embrechts, Puccetti, Rüschendorf, Wang, and Beleraj (2014) summarize the weaknesses of VaR as being threefold. Firstly, it says nothing concerning the what-if question: “Given we encounter a high loss, what can be said about its magnitude?” Secondly, for high confidence levels, e.g. 95 percent and beyond, the statistical quantity VaR can only be estimated with considerable statistical and model uncertainty, i.e. forecasts can become more uncertain and unstable at higher confidence levels. Thirdly is the subadditive problem.Conditional Value at Risk (CVaR) is a measure initially used in the insurance industry for determining extreme returns (those beyond VaR). The metric has been shown by Pflug (2000) to be a coherent risk measure without the undesirable properties exhibited by VaR. In terms of the three VaR shortfalls mentioned above by Embrechts et al. (2014), CVaR partly corrects the first problem in that it addresses losses of high magnitude, and corrects the sub-additive problem. The authors state, in line with findings by McNeil, Frey, and Embrechts (2005), that the problem of being able to accurately estimate single risk measures at high confidence levels still remains. Kaut, Wallace, Vladimirou, and Zenios (2007) find that accuracy and stability of CVaR forecasts based on historical data can be impacted through mis-specification of the underlying distribution and through insufficient scenarios. CVaR has been applied to portfolio optimization problems by Uryasev and Rockafellar (2000), Rockafeller and Uryasev (2002), Andersson, Mausser, Rosen, and Uryasev (2000), Alexander, Coleman, and Li (2003), Rockafellar, Uryasev, and Zabarankin (2006), Birbil, Frenk, Kaynar, and Noyan (2009), Menoncin (2009), Dupačová and Kopa (2014), and Mansini, Ogryzak, and Speranza (2014). CVaR has also been explored as a measure of sectoral market and credit risk by Allen and Powell (2009), Powell and Allen (2009), but compared to VaR, CVaR studies in a credit context are still in their infancy.Given the importance of understanding and measuring extreme credit risk (which we define as the risk in tail of a credit risk distribution, i.e. that risk beyond a specified threshold such as the CVaR thresholds used in this article), the first aim of this study is to show how CVaR techniques can be applied to prevailing models to measure this tail risk, using a US dataset which includes 380 US companies, mixed between investment and speculative entities. Of course CVaR, by definition, must always be higher than VaR but the extent needs to be quantified to understand the level of risk that is being ignored by VaR measures and we do this as part of our first aim. The second aim is to show how the CVaR measures can be used by banks to measure capital buffers required to deal with volatility in credit risk. A link can be drawn between the volatility of the market asset values of banks (as measured by models like the Merton Distance to Default (DD) model explained in detail in this paper) and capital adequacy, as illustrated by the Bank of England (BOE, 2008). BOE reports that in 2008 UK banks had equity ratios of around 3.3 percent, and assuming volatility in market value of assets of 1.5 percent, this gives a Probability of Default (PD) of around 1 percent (per DD and PD equations (1) and (2)). If volatility doubles, then PD increases substantially to 15 percent. As bank PDs increase with deteriorating market conditions, so too does the chance of the assets needing to be liquidated at market prices. Therefore as PDs rose during the GFC, market participants changed the way they assessed underlying bank assets, placing a greater weight on mark to market asset values, implying lower asset values and higher potential capital needs for banks. Thus BOE sees the mark to market approach of a bank's assets as providing a measure of how much capital needs to be raised to restore market confidence in the bank's capitalization. Other prominent bodies who have promoted monitoring the DD of banks include the European Central Bank (ECB) who sees a reducing DD as a useful measure of bank distress, and the International Monetary Fund (Otsu, 2010) who sees Distance to Default in a bank context as “Distance to Capital” (DC), which indicates when capital has been eroded and needs to be restored. In line with this thinking by the BOE, ECB and IMF, we use the CVaR based volatility metrics in this study to determine what capital buffers are required to restore market confidence in volatile times. This focuses on capital buffers is consistent with Basel III capital adequacy requirements (Bank for International Settlements, 2012), whereby banks are required to hold countercyclical capital buffers to protect them in downturn times.To ensure a thorough examination of CVaR metrics we use a range of models (four in total), as well as apply two techniques (Historical and Monte Carlo Simulation) to each model. The Monte Carlo method generates multiple random scenarios, with the key advantage being that thousands of potential scenarios can be generated and considered, as opposed to just a few discrete observations. This is especially advantageous with CVaR, where historical observations are only limited to a small number of observations in the tail of the distribution.The third aim of this study is to ascertain which of the models are best able to measure credit risk in the different economic circumstances of the pre-GFC, GFC and post-GFC periods by correlating the model outputs with actual measures of credit risk, including Credit Default Swap (CDS) spreads, delinquent loans and charge-offs.Our four models are based around CVaR type modifications to some of the most widely used existing credit models. The Merton (1974) structural model uses a combination of asset value fluctuations and balance sheet characteristics to measure Distance to Default (DD) and Probability of Default (PD). Moody's KMV model (with KMV standing for Kealhofer, McQuown and Vasicek, a credit analysis business acquired by Moody's) is a modified version of the Merton model (with modifications summarized in Section 2.2). Moody's KMV (2010) report use of their products by more than 2000 leading financial institutions in over 80 countries, including most of the 100 largest financial institutions in the world. Auvray and Brossard (2012) report that DD can be a good lead indicator of bank distress, due to its market component, when closely monitored by the shareholders. Our first model (Xtreme-S) applies CVaR techniques to this structural model by measuring the tail asset value fluctuations (those beyond VaR). Our second model (Xtreme-Q) applies quantile regression to the Merton structural model, by dividing the dataset of asset value fluctuations into parts (quantiles), allowing the selected quantile (in our case based on tail observations) to be isolated and measured. Our third model (Xtreme-T) applies CVaR techniques to the CreditMetrics Transition model, which measures VaR and is the credit equivalent of the RiskMetrics model ofMorgan and Reuters (1996) who introduced and popularized VaR. The CreditMetrics model incorporates credit ratings and calculates VaR based on the probability of transitioning from one rating to another (including to a default rating). Traditionally, transition models have been primarily used to measure corporate credit risk, but have also been used to measure consumer credit risk (Malik & Thomas, 2012) and for even wider applications such as the spread of infectious disease (Yaesoubi & Cohen, 2011). Our fourth model (Xtreme-i) applies CVaR techniques to our own iTransition model which is a transition model modified to incorporate market derived sectoral risk weightings. Whilst these credit models all have different outputs (for example, VaR as compared to DD), this is not a major concern for our study as we are interested in relative changes in measurements in each of our selected periods, rather than absolute measures.The remainder of the paper is structured as follows: Section 2 describes data and the methodology (both Historical and Monte Carlo) used for each of the four models; Section 3 discusses results and implications for capital adequacy; Section 4 concludes.In order to examine our models over different economic circumstances, data is divided into three periods: pre-GFC, GFC, and post-GFC. For each of the four models we generate separate measurements for each of these three periods. We also generate an annual measure for each model for each of the 13 years in the dataset. Our pre-GFC period includes the 7 years from 2000 to 2006. US banks were not regulated according to Basel Accord advanced model credit risk requirements at this time, but in terms of providing a useful benchmark, we note that this 7 year period aligns with the Basel Accord advanced model credit risk requirements. Our GFC period includes 2007–2009 which was the height of the GFC, and the post-GFC period is 2010–2012. Although our pre-GFC period is longer than the other two periods, we have opted for periods of different economic circumstances rather than periods of equal length. The 7 years prior to 2007 were a period of growth, followed by a crisis period until 2009, followed by a period of recovery, and our split represents these three different sets of circumstances. We also checked to see if there was any major difference between using a 3 year pre-GFC period (2004–2006) and a 7 year pre-GFC period (2000–2006) and found no significant difference between the VaR, CVaR, DD and CDD outcomes for these two period lengths and so we retained the 7 year pre-GFC period.For our Merton/KMV based models (Xtreme-S and Extreme-Q) which require equity prices, we obtain daily prices from Datastream (approximately 250 observations × 13 years = 3250 observations per company). Required balance sheet data for the structural model, which includes asset and debt values, is also obtained from Datastream. To ensure a mix of investment and speculative entities, we include the following categories in the data we obtain from Datastream: firstly entities comprising the U.S. S&P 500 index; secondly entities included in Moody's Speculative Grade Liquidity Ratings list (Moody's Investor Services, 2012). In both cases we only include rated entities, for which equity prices and Worldscope balance sheet data are available in Datastream. Entities with less than 12 months data in any of the periods are excluded. This results in 378 entities consisting of 208 S&P 500 companies and 170 speculative companies.The transition based models (Xtreme-T and Xtreme-i) require credit ratings and transition probability matrices (as discussed in Sections 2.4 and 2.5) for each period, which are not available on Datastream. Therefore credit ratings are obtained from Moody's Default and Recovery Database. We use Standard and Poor's (2013) US transition probability matrices for each year in our study. While there could be some differences between agencies such as S&P and Moody's, the Bank for International Settlements (2014) who co-ordinates the global regulations for banks, have provided a mapping process which they deem suitable for the measurement of credit risk and allocation of capital by global banks, and we thus use this mapping process for mapping Moody's ratings to the ratings in the Standard & Poor's (S&P) transition matrix.Annual delinquent loans and charge-off rates are obtained from the U.S. Federal Reserve Bank (2013). Annual CDS figures for US Corporates are extracted from Datastream by credit rating, and weighted according to the dollar value of debt for each credit rating category in our data sample.These different types of default data that we use from the different sources mentioned above all measure different aspects of credit risk as explained further in Section 3 (delinquent loans are loans 3 months in arrear, charge-offs are written-off losses and CDS spreads are risk premiums), and thus there is no duplication. Correlating our model outcomes with these three different default measures allows a more comprehensive analysis of our model results than if we had used one only.We use the Merton/KMV approach to estimating default, and then modify this calculation to incorporate a CVaR component (which we term CPD as the model uses Probability of Default as opposed to VaR). The detailed workings of the Merton/KMV approach are well documented and can be located in Crosbie and Bohn (2003) and Bharath and Shumway (2009) and hence we will not include the detail here, other than to provide a summary of the key components to facilitate the reader's understanding. The structural model measures market asset value fluctuations and the point of default is where the firm's debt exceeds asset values. KMV (Crosbie & Bohn, 2003), in modeling defaults using their extensive worldwide database, which includes over 250,000 company-years of data and over 4700 incidents of default, find that in general firms do not default when asset values reach total liability book values. Many continue to trade and service their debts at this point as the long-term nature of some of their liabilities provides some breathing space. KMV find that the default point, the asset value at which the firm will default, generally lies somewhere between total liabilities and current, or short-term, liabilities (modeling evidence from their extensive database shows approximately half way). Thus KMV use current debt plus half of long term debt as the default point. Distance to Default (DD) and Probability of Default (PD) are measured as(1)DD=ln(V/F)+(μ−0.5σV2)TσVT(2)PD=N(−DD)whereV = market value of firm's assetsσV= standard deviation of asset returnsF = face value of firm's debt (in line with KMV, this is defined as current liabilities plus one half of long term debt)µ = an estimate of the annual return (drift) of the firm's assetsN = cumulative standard normal distribution functionT = time horizon (PD is usually measured at a future T of 1 year, and in line with standard practice, we set T to 1).It should be noted that KMV find the PD values arising from the normal distribution are very small, and hence use their own extensive database of defaulting entities to derive an Estimated Default Frequency (EDF) from DD values, which we do not have access to. For this reason, we will report DD values only (both KMV and Merton use similar DD values) as opposed to PD values. This has little impact on our study as we are concerned with changes from period to period rather than with absolute measures.For our historical approach, we obtain daily equity returns for each entity, and calculate the standard deviation of the logarithm of price relatives. Following the estimation, iteration and convergence procedure outlined by KMV (2008), Bharath and Shumway (2009), and Allen and Powell (2009), we obtain asset values and daily asset returns. These figures are then applied to the DD and PD calculations in Eqs. (1) and (2). We measure µ as the mean of the change in ln V as per Vassalou and Xing (2004). Following KMV, debt is measured as current liabilities plus one half of long term liabilities. It should be noted that the above procedure for asset returns provides the returns for each company individually. The weighted average of DDs will provide an undiversified portfolio DD which, while useful for comparing individual companies to, does not take account of correlations between returns. Correlations for the historical asset return series are, however, naturally embedded in the historical weighted average return series of the portfolio entities, from which a diversified (correlated) σVcan be calculated and hence a correlation factor (undiversified σV/diversified σV) can then be applied to Eq. (1) to calculate a diversified DD.CVaR is the risk beyond VaR, and thus if VaR is measured at a 95 percent level of confidence CVaR is the average of the lowest 5 percent returns. CVaR is normally measured at high levels of confidence, often 95 percent or 99 percent. As this study is a comparative study, the important thing is consistency of significance level between the models rather than which exact level is used. The 99 percent level provides a very low number of observations (1 percent of the data which equates to only 2.5 annual observations based on 250 working/trading days when using daily asset returns as we use in this study) and we thus use a 95 percent level in this study in order to provide a reasonable number of observations to compare between the models. Conditional Distance to Default (CDD) is DD based the worst 5 percent of asset returns. We term the standard deviation of the worst 5 percent of returns for each period as CStdev, which we then substitute into Eq. (1) to obtain CDD:(3)CDD=ln(V/F)+(μ−0.5σV2)TCStdevVTOur Monte Carlo approach generates 20,000 simulated asset returns for every company in our dataset, separately for each of the pre-GFC, GFC and post-GFC periods. A high number of simulations is desirable to estimate VaR an CVaR with precision and 20,000 is the same number used by CreditMetrics (Gupton, Finger, & Bhatia, 1997) and other key studies, including Andersson, Mausser, Rosen, and Uryasev (2000). The advantage of a Monte Carlo approach is that it randomizes the selection of variables, and provides a much larger and richer set of potential observations than the historical simulation approach. While Monte Carlo is often undertaken using a normal distribution based on the first two moments or some other fitted distribution, we do not use this approach, as credit losses are not normally distributed. Instead we generate 20,000 random numbers between 1 and 100 (with two decimal places) and draw the corresponding percentile number from the historical dataset. For example, number 28.14 draws the 28.14th percentile number from the dataset in the relevant period. Unlike re-sampling, which only draws from existing numbers in an historical dataset, this method allows the creation of a much richer dataset, as it can draw far more possible outcomes from the historical distribution, than just the re-sampled numbers. For example, in the dataset (9, 2, 4, 6, 8, 8), the 28.14th percentile number is 4.81, not seen in the historical dataset, but which could potentially occur. This larger set of possible outcomes is particularly important in modeling CVaR, where for example, in a 3 year historical sample of 750 trading days, the maximum number of observations from which CVaR can be calculated is limited to 37.5 (the lowest 5 percent of returns), whereas our Monte Carlo method yields 1000 possible observations (5 percent of 20,000) from a much greater range of possible numbers, which still approximate the historical distribution. We then follow the same approach as for the Historical model, applying the standard deviation of all simulated returns to Eq. (1) (modified by the correlation factor calculated in the historical model) to measure DD, and the standard deviation of the worst 5 percent of simulated returns to Eq. (3) to measure CDD.Quantile regression is a technique for dividing a dataset into parts. Again, this method is well documented with details in Koenker and Basset (1978) and Koenker and Hallock (2001), and therefore we will not repeat these details here, other than a summary of key components to facilitate reader understanding. Minimizing the sum of symmetrically weighted absolute residuals yields the median where 50 percent of observations fall either side. Similarly, other quantile functions are yielded by minimizing the sum of asymmetrically weighted residuals. Let yibe a real valued random number in a discrete sample y1, …, yn. The rth sample quantile, 0 < r < 1, may be defined as any solution to the minimization problem:(4)=argminb∈R∑pr(yi−br)where pr(⋅) is the absolute value function, providing the rth sample quantile with its solution, with the objective function for the rth sample quantile expanded as follows:(5)=argminb∈R{∑i∈{yi≥br}r|yi−br|+∑i∈{yi<br}(1−r)|yi−br|}Fig. 1 illustrates the quantile regression technique. The x and y axes represent any two variables being compared (such as age and height; or market returns and individual asset returns). The 50 percent quantile (middle line) is the median, where 50 percent of observations fall below the line and 50 percent above. Similarly, the 90 percent quantile (top line) is where half the values in the 90 percent quantile lie above the line, and the 10 percent quantile (bottom line) has half of the quantile observations below the line. The intercept and slope are obtained by minimizing the sum of the asymmetrically weighted residuals for each line. Quantile regression allows direct modeling of the tails of a distribution rather than ‘average’ based techniques such as ordinary least squares or credit models which focus on ‘average’ losses over a period of time. Financial applications include the study of the Capital Asset Pricing Model (CAPM) and stock market returns (Barnes & Hughes, 2002), modeling distribution of profit and loss (Whittaker & Somers, 2007), sales forecasting (Taylor, 2007) the measurement of VaR (Engle & Manganelli, 2004; Taylor, 2008), and the estimation of bank efficiency scores (Behr, 2010).In a stock market context, beta (β) under CAPM measures the systematic risk of an individual security relative to the market. The lower and upper extremes of the distribution are often not well fitted by OLS. Allen, Gerrans, Singh, and Powell (2009), using quantile regression, show large and sometimes significant differences between returns and β, both across quantiles and through time. These extremes of a distribution are especially important to credit risk measurement as it at these times when failure is most likely. We therefore expand these quantile techniques to credit risk by measuring fluctuating asset values for each company in our dataset (using the Merton model as described in Section 2.2), which we then divide into quantiles as described above and shown in Fig. 1. Similar to the way CAPM (Lintner, 1965; Sharpe, 1964) works, whereby β is obtained by measuring the risk of an asset relative to the market, we obtain a β for each quantile, but we measured it as σq/σb. Here σq is the volatility of the daily asset values for the quantile being measured (standard deviation of the datapoints in the quantile from the mean of the entire dataset) and σb is the volatility benchmark, for which we use the volatility of the entire dataset (all asset) returns. Thus βq = 2 means that particular quantile has double the volatility of the dataset as a whole. βq > 1, shows risk (volatility) higher than the norm and βq < 1 shows a relatively low volatility. As volatility is the numerator of the DD equation (Eq. (1)), a doubling of asset volatility means a halving of DD. Thus this method allows us to see how much closer a loan (or portfolio of loans) comes to default at various points in a time period. This is illustrated in the graph below for a 95 percent quantile, with an assumed σb = 0.4 and assumed benchmark DD (DDb) = 2. In this example, quantile risk on the y axis (σq0.95 = 0.8) is double that of our benchmark risk on the x axis (σb = 0.4), i.e. βq0.95 = 2. Therefore, using Eq. (1), the quantile DD is half that of DDb (DDq0.95 = 1). Diversified (correlated) portfolio DD can be calculated in the same way as for the Xtreme-S model. (See Fig. 2).The Historical approach uses actual past asset fluctuations to measure σ, whereas the Monte Carlo approach generates 20,000 simulated asset returns in the same manner as for Xtreme-S.This model is based upon obtaining the probability (ρ) of a bank customer transitioning from one grade to another as shown for the following BBB example:BBBρAAAρAAρAρBBBρBBρBρCCC/CρDSo, in the above, a BBB obligor might, for example, have a 90 percent probability of remaining at BBB, a 1 percent probability of improving to an A rating, a 3 percent probability of deteriorating to a BB rating, and so on, with a different probability of transitioning to each of the different rating categories, including the probability of transitioning to the default category D. The sum of all probabilities must be equal to 1.The CreditMetrics model, which we use here, is a well known credit transition model and is the credit equivalent of the RiskMetrics VaR model. As the model is well documented by CreditMetrics (Gupton, Finger, & Bhatia, 1997), we will only summarize the key features here to facilitate the reader's understanding.External raters such as Moody's and Standard & Poor's (S&P) provide transition probabilities for each grading and we use the S&P US transition probabilities. We exclude non-rated categories and adjust remaining categories on a pro-rata basis as is the practice of CreditMetrics.Following CreditMetrics methodology the model obtains forward zero curves for each rating category (based on risk free rates) expected to exist in a year's time. Using the zero curves, the model calculates the market value (V) of the loan, including the coupon, at the one year risk horizon. Effectively, this means estimating the change in credit spread that results from ratings migrating from one rating category to another, then calculating the present value of the loan at the new yield to estimate the new value. The following example values a 5 year loan, paying a coupon of 6 percent, where r is the risk free rate (the rate on government bonds) and s is the spread between a government bond and corporate bonds of a particular category, say AA (see CreditMetrics (Gupton, Finger, & Bhatia, 1997)).(5)V=6+6(1+r1+s1)+6(1+r2+s2)2+6(1+r2+s2)3+106(1+r2+s2)4The above is calculated for each rating category (yields for government and corporate bonds are obtained from Datastream for each rating category for each year in the sample, and are weighted according to debt (F) for each entity in our data sample). Probabilities in the S&P (Standard and Poor's, 2013) transition tables are multiplied by F for each rating category to obtain a weighted probability of returns. Based on the revised probability table, Historical VaR is obtained by calculating the probability weighted portfolio variance and standard deviation (σ), and then calculating Historical VaR using a normal distribution (for example 1.645σ for a 95 percent confidence level). We use the CreditMetrics method of calculating correlations, in order to determine portfolio VaR. As this correlation method is fully explained in Gupton, Finger, and Bhatia (1997), we will not detail it here, suffice to say that it involves calculating joint probabilities for each rating with each other rating and then using these to calculate co-variances, correlations and portfolio VaR. Again, by comparing the weighted average VaRs of the assets in the portfolio with the diversified (correlated portfolio), we are able to obtain a correlation factor.It is common practice for modelers of transition matrices to use the average historical transition probabilities over the time period being modeled (i.e. a through-the-cycle approach as opposed to varying the probabilities year by year, i.e. a point-in-time approach). The through-the-cycle approach has the advantage of remaining relatively stable in business cycles, as ratings should have considered stressed scenarios. However, credit ratings change only periodically and, especially over periods like the GFC, this will not be effective in predicting actual changes in VaR using the transition model, as it ignores the impact of volatility in the default probabilities associated with the ratings. S&P provide annual probability matrices as well as historical averages over various extended time periods. When correlating our VaR and CVaR outcomes to CDS spreads and bank defaults and charge-offs (in Section 3) we examine both approaches – one which uses fluctuating probabilities and one which uses the average for each of our three periods. We extend this Historical VaR methodology (Gupton, Finger, & Bhatia, 1997) to calculate Historical CVaR by using only the lowest 5 percent of ratings returns.CreditMetrics (see also Allen & Powell, 2009) use Monte Carlo modeling as an alternate approach to estimating VaR. We begin here by describing the CreditMetrics Monte Carlo approach and then explain how we modify the approach to measure CVaR. Under CreditMetrics, transition probabilities and a normal distribution assumption are used to calculate asset thresholds (Z) for each rating category as follows:(6)Pr(Default)=Φ(ZDef/σ)Pr(CCC)=Φ(ZCCC/σ)−Φ(ZDef/σ)and so on, where Φ denotes the cumulative normal distribution, and(7)ZDef=Φ−1σCreditMetrics then generate scenarios of asset returns using a normal distribution assumption. These returns are mapped to ratings using the asset thresholds, with a return falling between thresholds corresponding to the rating above it, and then VaR is calculated as for historical VaR.It should be noted that the Creditmetrics normal factor is used merely to set the thresholds. Given the thresholds are set to match the desired probabilities, the same result could be achieved using any other distribution where the inverse of the default factor was used to determine the corresponding threshold. For consistency of approach we generate 20,000 returns for each firm using the same approach as outlined for our Xtreme-S model, which will achieve the same returns as CreditMetrics. Our thresholds are derived from the actual S&P transition probabilities. For example, if the probability table shows that there is a 10 percent probability of a BB rated loan moving to BBB, an 80 percent chance of it remaining BBB, and a 10 percent chance of it moving to B, then any of our random numbers falling between 0 and 10 would be allocated to B (and the return associated with B), those >90 to BB, and the remainder to BBB. The worst 5 percent of the 20,000 returns form our Monte Carlo CVaR portfolio. We can then calculate returns associated with each rating scenario by using the forward values described for the historical approach, with VaR being the actual 95th percentile return for each asset (rather than the normal distribution approach of CreditMetrics which uses the standard deviation multiplied by the normal standard variable, i.e. 1.645σ for a 95 percent confidence level). The portfolio VaR can be calculated as the weighted average sum of VaRs for all the assets in the portfolio, adjusted by the correlation factor.CreditMetrics advise that, although they make use of mean and standard deviation to calculate VaR, these may not be the best measures of risk and that percentiles may be better measured directly from the scenarios, as we have done for VaR. We also infer CVaR directly from the scenarios, with CVaR being the average of those simulations beyond VaR (using the actual 95th percentile VaR, rather than a normal distribution inferred VaR).CreditPortfolioView (Wilson, 1998) varies the transition model by adjusting transition probabilities with industry factors calculated from macroeconomic variables, thus recognizing that customers of equal credit rating may transition differently depending on their industry risk. Other studies have also linked macroeconomic/business cycle conditions to consumer credit risk (Bellotti & Crook, 2012; Crook & Bellotti, 2010) and to transition matrices (Kim, 1999; Nickell, Perraudin, & Varotto, 2000; Trück, 2010; Wei, 2003). The value of macroeconomic factors in measuring consumer credit risk is also recognized by Leow and Mues (2012), who see it as a future extension to either one or both of their probability of repossession model and their haircut model. All these studies illustrate the importance of macroeconomic variables in credit risk modeling. Inclusion of these variables, does however, add additional complexity. APRA (1999) showed that banks did not favor using macroeconomic factors due to these complexities. To overcome this, and yet still incorporate the important industry market factors, our own iTransition model (Allen & Powell, 2009) uses the same framework as CreditPortfolioView, but (on the basis that differences in industry risk will be captured in share prices) incorporates market VaR (fluctuations in the share prices of industries) instead of macroeconomic variables to derive industry adjustment factors. This is done by calculating market VaR for each industry, then calculating the relationship between market VaR and credit risk (as per the Merton model) for each industry. We classify data into sectors using Global Industry Codes (GICS), which are energy, materials, industrials, consumer discretionary, consumer staples, financials, health care, retail, information technologies, telecommunications and utilities. These factors (i) are used to adjust the Xtreme-T model as follows using a BBB rated loan example:BBBρAAAiρAAiρAiρBBBiρBBiρBiρCCC/CiρDiAssume a transition matrix showed that a BBB obligor had a 3 percent probability of deteriorating to a BB rating. For an undiversified matrix such as that shown in Section 2.4, this 3 percent probability would be applied to all customers in a portfolio, irrespective of their industry. However assume this transition probability was 3 percent for an Energy obligor and 2 percent for a retailer, our industry adjustment factor (i) for BB would be higher for a debt portfolio of predominantly energy obligors than for a portfolio of predominantly retailers.Other than the industry adjustments, our Xtreme-i Historical and Monte Carlo VaR and CVaR calculations follow the same process as Xtreme-T.Having outlined our four models, we now go on to present the results of our unconditional measures (VaR and DD) and conditional measures (CVaR and CDD) of credit risk.DD in Table 1(measured by number of standard deviations) is calculated using Eq. (1). CDD is based on the worst 5 percent of asset returns in each using Eq. (3). As mentioned in Section 2.1 we use daily asset values. So, for example in the GFC period of 3 years, there are approximately 750 asset value observations (approximately 250 annual trading days), so the denominator of Eq. (3) is the based on the standard deviation of these 750 observations. VaR (95 percent confidence level) would be based on the 95th worst observation (out of 750 observations, for example, in the GFC period) and CVaR is the average of the daily losses beyond VaR. The daily VaR and CVaR figures can be annualized by multiplying by the square root of 250, being the approximate number of annual trading days (Table 2).To give an idea of the standard errors from our historical approach compared to the Monte Carlo approach, we present the errors for the asset standard deviations σ (which are used in both the structural based models, Xtreme-S and Xtreme-Q). The standard deviations of asset values are what underpin the fluctuations in the DD and CDD calculations. The historical standard errors are calculated asσ/n. In this case, for uniformity of standard error calculation, we used equal size samples of 3 years each (approximately 750 observations) for the pre-GFC, GFC and post-GFC periods. For Monte Carlo, in line with the CreditMetrics Monte Carlo approach we generated 20,000 samples of 750 each and measured the standard error as the standard deviation of the sample means.We note is that the standard error increased in line with the increased volatility of the GFC, reducing thereafter, and that the standard errors of the Monte Carlo were slightly lower than the historical ones.In line with the points mentioned by Embrechts et al. (2014) and Kaut, Wallace, Vladimirou, and Zenios (2007) that measure of high confidence are subjected to inaccuracy and stability, we undertook some stability tests on our models. Kaut, Wallace, Vladimirou, and Zenios (2007) hold that one of the aspects of stability is that the solutions (i.e., CVaR in our case) should not vary (as measured by standard error of simulations compared to the historical standard error) across scenario sets of the same size. Embrechts et al. (2014) show that there can be large variations in the CVaR/VaR ratio depending on the distribution used (for example, a lower ratio at high confidence levels for a fat decaying tail like the normal case, as compared to a long tailed Pareto case).To test the stability Monte Carlo of solutions compared to the historical ones, we used a similar procedure to Kaut, Wallace, Vladimirou, and Zenios (2007) we generated multiple sets (1000) of simulations, with varying numbers of simulations (1000, 5000 and 10,000 and 20,000) in each set, i.e. 1000 sets of 250 simulations, then 1000 sets of 1000 simulations and so on, to see how many simulations it took for the 1000 sets of simulations to achieve stability of CVaR. We measure stability at a 95 percent confidence level (i.e. stability is obtained when 95 percent of the simulated CVaR's fall within the historical CVaR plus 1.96 times the standard error of the historical CVaR).The sets of 250 and 1000 simulations fell well outside the criteria, i.e., the historical standard error did not match the Monte Carlo standard error at 95 percent confidence level. VaR almost met the criteria at 5000 simulations while CVaR was still well short (due to the lesser number of observations in the tail), and both metrics met the criteria at 10,000. This means stability can be achieved at 10,000 simulations, well within the 20,000 we used in this study.While of course, CVaR must always be higher than VaR, it is the extent of the difference and its relation to the historical model that we are interested in. Table 3shows the spread between the standard measure and the conditional measure. As DD is based on standard deviation and CCD is based on CVaR, the measure for our structural models is CVaR/σ. For the transition models, the standard measure is based on VaR and the extreme measure on CVaR, so the measure is CVaR/VaR (same ratio as for the Embrechts et al.'s study). We also compare our Monte Carlo outcomes to the expected outcomes for a normal distribution, to see if a normal distribution approach would have achieved the same result.From an accuracy/stability perspective the ratios for the historical model of each of the above as compared to their Monte Carlo version are fairly close to each other for all the models in each of the periods. Let us compare this to a normal distribution. On a normal distribution, 95 percent VaR = 1.645σ and CVaR = approximately 2σ, with CVaR/VaR = approximately 1.2. The historical ratios are far higher than this, showing high risk in the tails, and therefore a Monte Carlo based normal distribution would thus would severely understate the risk, as compared to our distribution.All Historical models show the conditional measures (CDD and CVaR) being approximately three times higher risk than the unconditional measures (VaR and DD) during the pre-GFC period. This gap remains fairly consistent for the market based models (Xtreme-S and Xtreme-Q) over each period, but narrows for the transition based models (Xtreme-T and Xtreme-i) over the GFC and post-GFC periods, as companies in the upper 95 percent category were downrated to a larger extent than the lowest 5 percent (which were already poorly rated), thus causing VaR to move closer to CVaR. The Monte Carlo models show similar trends to their corresponding Historical models, although there are some differences between the absolute Historical and Monte Carlo figures, particularly with CVaR where the Historical model generates only a small number of CVaR observations (5 percent of historical observations) compared to the large number of Monte Carlo CVaR observations (5 percent of 20,000 observations).Although there are some differences between the models in the extent of the variation between the quantiles, the difference between VaR and CVaR (or DD and CDD) is nonetheless significant for all models at the 99 percent level using F tests for changes in volatility. This has significant implications for banks. Provisions and capital calculated on below the threshold measurements will clearly not be adequate during periods of extreme downturn, as is illustrated in Fig. 3.For illustrative purposes, Fig. 3 only shows the results of the Xtreme-Q Model, but a similar approach could be applied to any of the four models to show the relative differences between the unconditional (VaR or DD) and conditional measurements (CVaR or CDD) during various periods. The figure shows the 50 percent and 95 percent quantiles for the pre-GFC and GFC periods. For simplicity and to avoid cluttering the graph, we just compare two of our three periods to illustrate the implications for capital. Also for simplicity, we only show the Historical values on the graph, and note that the Monte Carlo results in Table 1 would yield a similar spread. The y axis is calculated on the asset fluctuations (σ), using the Merton model, for the quantile in question. The x axis is the median σ for the entire 10 year period (pre-GFC plus GFC). Thus the beta (β) for the 50 percent quantile for the 10 year period is one. Where σ for a particular quantile is less (greater) than the median for the 10 year period, β < (>) 1, and DD increases (reduces) accordingly.The above graph shows that the ‘median’ DD (based on how the standard Merton structural model calculates DD) over the 10 year pre-GFC plus GFC period is 6.02 for US banks with an asset value standard deviation (σ) of 0.0078. As asset value σ is the denominator of the DD equation (Eq. (1)), as σ increases (reduces) from one level to another (i.e., from σ1 to σ2) DD reduces (increases) by the same proportion. Thus the numerator of the equation (a measure of capital: the distance between assets and liabilities) needs to increase to restore DD back to the same level (i.e., as per the BOE observation in Section 1, capital (K) will need to increase by the same proportion to restore market confidence in the banks’ capital). Thus, a given level of capital (K) will need to be increased (to K*) in line with by the relative shift in asset value fluctuations:(8)K*=K×σ2σ1Based on Fig. 3, during the extreme fluctuations of the GFC (as measured by the 95 percent quantile) US banks needed in excess of three times more capital than during ‘median’ circumstances. Whilst we have used the Xtreme-Q model to illustrate this, the same principle applies to all the models – a trebling of VaR or DD requires treble capital to deal with it. Thus a bank with 5 percent capital during ‘normal’ times would need 15 percent during extreme times.An analysis such as the above would assist a bank to identify the capital required to ensure capital adequacy over various economic periods. Rather than achieving this on a procyclical basis (i.e. continually adjust DD to ensure it remains constant, and to set capital accordingly), this could be achieved on a countercyclical basis, raising capital in the good times and releasing it in downturn times. Pro-cyclicality of capital has been extensively discussed in the literature, with several downsides. The pro-cyclical approach includes the need for extreme increases in capital requirements in recessions, and resulting severe macroeconomic effects through credit crunches in recessions thus worsening the economic downturn (Catarineu-Rabell, Jackson, & Tsomocos, 2005). In downturns, as losses mount across the banking system, equity levels fall, banks have difficulty in raising new capital, and cannot meet their capital requirements if they make new loans, and hence the supply of new loans decreases (Kowalik, 2011). The countercyclical way is to use the methods shown in Fig. 3 to estimate capital buffers required to ensure capital adequacy over different economic cycles, and then raise these in good times (thus increasing DD above the desired benchmark) to ensure sufficient capital is held to absorb losses in downturns. Countercyclical buffers can help reduce credit growth during booms and reduce the credit contraction once it is released (Drehmann, Borio, Gambacorto, Jiménez, & Trucharte, 2010; Drehmann, Borio, & Tsatsaronis, 2011; Drehmann & Gambacorta, 2012). The Drehmann studies focus on using indicators for the accumulation and release of capital. They propose the gap between the ratio of credit-GDP and its long-term backward-looking trend as an indicator for the accumulation of capital, with indicators of tightening credit conditions such as credit spreads providing signals which trigger the release. In a similar manner, our model of a market based credit indicator to a benchmark (current DD to benchmark DD), applied over a long-term range of economic cycles can identify the capital buffers required in downturn times, which can be built up in good times, and released in downturns. As the DD fluctuates daily with market circumstances, it also provides an early warning indicator of stressed circumstances.When comparing different volatility models, it is important to consider how well their relative outcomes compare to actual credit risk volatility experienced by US banks. Using 10 years of annual data, we correlate our measures for the four models to three measures of actual credit risk. The first of these three measures is Credit Default Swap (CDS) spreads, which is a measure of the premium the market is prepared to pay for increased credit risk. The second is delinquent loans as reported by the US Federal Reserve and are loans past 30 days or more and still accruing interest as well as those in non-accrual status, measured as a percentage of end-of period loans. The third is charge-off rates, also reported by the US Federal Reserve, which is the value of loans removed from the books and charged against loss reserves, measured net of recoveries as a percentage of average loans. These correlations are reported in Table 4. Various lags are tested, with most correlations being most significant with no lag and some correlations (the shaded areas of Table 4) being most significant with a 1 year lag (e.g. a 2009 measurement for actual risk compared to a 2008 model measurement). To avoid over-reporting of figures, we show only the results of the Historical model, but the Monte Carlo models produce very similar outcomes.The table correlates the Historical model metrics produced by each of our four models for the 13 years in our data sample with three measures of actual credit risk of US banks, being CDS spreads, delinquent loans, and charge-off rates. Level of significance is measured by a t-test, with * denoting 95 percent significance and ** denoting 99 percent significance. Non shaded areas are where highest correlation is experienced with no lag, and the shaded areas with a 1 year lag.The structural based models (Xtreme-S and Xtreme Q) show a much higher correlation with CDS spreads than the other models. This is because CDS spreads change daily with market conditions, and so does the asset value component of the structural model. The transition based models (Xtreme-T and Xtreme-i) which largely depend on ratings (more sluggish than CDS spreads as ratings are often updated only annually) show no significant correlation in the same year, but a higher correlation when using a 1 year lag. This dominance of market factors over firm specific factors in volatile times has some parallels with the findings of Angelini, Nobili, and Picollo (2011), who found that volatilities of spreads in interbank interest rates during the Global Financial Crisis largely reflected aggregate market factors rather than firm-specific ones. All four of our models show highly significant (99 percent confidence) correlation with delinquent loans, meaning that the metrics of all the models are a good indicator of actual defaults. There is very high significance shown by the transition based models’ correlations with charge-off rates. This is explained by the timeline in Fig. 4, which shows how both CDS spreads and the structural model respond quickly to market events, resulting in high correlation between these two items, whereas ratings (and thus transition models) react slower to market events and thus have a higher correlation with actual write-offs which usually occur sometime after initial market deterioration.Of note is that there is very little difference in the correlation significance levels for VaR (DD) as compared to CVaR (CDD). This means that, although CVaR (CDD) are at much higher levels than VaR as previously discussed, the trend (percentage increase or decrease from year to year) is similar for both VaR (DD) and CVaR (CDD).To avoid over-detailed reporting, we have not split our correlation tables into each of the three periods. However, in summary, when we looked at the three periods individually we found that the correlations between the structural based models (Xtreme-S and Xtreme Q) and all the other default metrics (CDS, delinquent, and charge-offs) remained fairly constant across all three periods. However, there was no relationship at all between the transition models and the default metrics in any of these shorter periods, because the rating changes were happening in different periods to the changes in default. For example, during the GFC when defaults were increasing, the ratings were only increasing in response in the post-GFC period, at which point the default indicators such as delinquencies and charge-offs were starting to reduce.It should be noted that transition probabilities used in Table 1 to calculate VaR and CVaR for Xtreme-T and Xtreme-i have been updated each year according to the annual probability matrices provided by S&P, as opposed to using a long term historical average (for example the S&P probability of a B rated loan defaulting in 2008 is given as 3.82 percent compared to 0.64 percent in 2006, even though the underlying rating had not changed). We mentioned in Section 2.4 that there are two main transition approaches that could be used with ratings. The first is a through-the-cycle (TTC) approach which models transition matrices with long term probability averages, thus varying only the ratings, and not the changing probabilities associated with the ratings. The second is the point-in-time (PIT) approach which varies the ratings and their probabilities (these changed dramatically over the GFC, as shown by the changing S&P probabilities), which we used and showed in Table 1, and in the correlations in Table 4. We modeled both approaches, a PIT one which allowed the probabilities associated with each rating to vary in line with the annual changes to S&P probabilities (per Table 1) and a TTC one which used an average probability over the 13 years in or dataset. We found no significant correlation at all between the VaRs and CVaRs produced using the TTC method with any of the three risk variables (CDS spreads, defaults, or charge-offs) and therefore do not report them in Table 1 or Table 4. The TTC transition model has only periodic changes to ratings, which do not ratchet up or down with changes to economic conditions. As this is a comparative study which compares transition to structural models, and the structural model is very much a PIT model, there is little point in comparing the TTC transition model to the PIT structural model. There is ample literature on the relative merits of PIT and TTC (see for example, Breeden, Parker & Steinebach, 2012; Carlehed & Petrov, 2012; Crook & Bellotti, 2010; Löffler, 2004, 2012; Vallés, 2006). In summary, TTC ratings have greater stability but low correlation with actual default probabilities, whereas point in time ratings have lower stability but are usually based on actual default probabilities over a specified time horizon. Ratings agencies tend to rate on a TTC basis, which means that ratings are very stable, but do not provide a good indicator of changing economic circumstances. In practice, banks do not necessarily rely on external ratings and many use an internally constructed ratings system, which may be updated at frequencies different from the external ratings. If the ratings and associated default probabilities are frequently updated, then Table 4 shows that there can be reasonable correlation with defaults, using a 1 year lag, particularly for the Xtreme-i model.

@&#CONCLUSIONS@&#
This paper has shown how CVaR type metrics can be applied to credit risk models to measure extreme risk. A comprehensive study was undertaken by generating and comparing four Xtreme models and by applying Historical as well as Monte Carlo metrics to each. In addition the models are applied to pre-GFC, GFC and post-GFC data to capture different economic circumstances.All four models show highly significant differences between VaR (DD) and CVaR (CDD). Increased volatility requires capital buffers to deal with the increased risk and the paper demonstrates this buffer requirement can be measured. There are no significant differences in outcomes between Xtreme-S and Xtreme-Q, nor between Xtreme-T and Xtreme-i. There are significant differences between the structural based models (Xtreme-S and Extreme-Q) as compared to the transition based models (Xtreme-T and Xtreme-i). The changes in risk measured by the structural based models are more consistent with changes in CDS spreads than those shown by the transition based models, because both structural models and CDS spreads respond very rapidly to market conditions. The opposite is true of charge-offs where the transition based models show much greater correlation than the structural based models, as there is generally a delay between defaults and charge-offs, and credit ratings also often respond slower (often annually) to market conditions than the structural models. All models show a significant correlation with delinquent loans.This provides important information to banks and regulators, demonstrating that, in times of high volatility, the structural models of can be of greater assistance than the ratings based transition models in the early detection and measurement of shifting credit risk and potential changes or releases to the capital buffers required to counter this.