@&#MAIN-TITLE@&#
Graphics processing unit (GPU) accelerated fast multipole BEM with level-skip M2L for 3D elasticity problems

@&#HIGHLIGHTS@&#
GPU parallel algorithm of the fast multipole BEM with level-skip M2L is presented.A rigid body motion method for the fast multipole BEM is given.Different M2L schemes are compared and discussed in detail.Engineering examples demonstrate the efficiency and accuracy of the algorithm.

@&#KEYPHRASES@&#
Fast multipole method,Boundary element method,Parallel computing,Graphics processing unit,3D elasticity,Level-skip M2L,

@&#ABSTRACT@&#
In order to accelerate fast multipole boundary element method (FMBEM), in terms of the intrinsic parallelism of boundary elements and the FMBEM tree structure, a series of CUDA based GPU parallel algorithms for different parts of FMBEM with level-skip M2L for 3D elasticity are presented. A rigid body motion method (RBMM) for the FMBEM is proposed based on special displacement boundary conditions to deal with strongly singular integration and free term coefficients. The numerical example results show that our parallel algorithms obviously accelerates the FMBEM and can be used in large scale engineering problems with wide applications in the future.

@&#INTRODUCTION@&#
The boundary element method (BEM), due to its high accuracy and dimension reduction characteristics, has been widely applied in a variety of engineering areas, e.g. acoustics [1], flow [2] and elastostatics [3]. However, the advantages of the BEM are compensated by its dense and nonsymmetric coefficient matrix which needs O(N2) operations to compute the coefficients and O(N3) or O(N2) operations to solve the equation system by direct or iterative solvers (N is the number of degrees of freedom (DOFs)). It is necessary to develop the BEM of faster computing and less memory usage for the purpose of dealing with large scales problems. One of the most common strategies to achieve this is combining the BEM with the fast multipole method (FMM), which leads to fast multipole BEM (FMBEM) whose complexities of both computational time and memory space are O(N).The FMM was presented in the late 1980s by Rokhlin and Greengard [4] in the context of N-particle (N-body) simulations. Based on the hierarchical tree structure as the O(NlogN) algorithm used in [5], two concepts are introduced in the FMM: (1) “multipole expansion (ME)” used to compute the moments of particle groups and (2) “local expansion (LE)” used to evaluate the contribution from particle groups. With the help of these two expansions, the FMM reduces the complexity of N-body problems to O(N). At the beginning, the FMM utilized full tree structure to store data, but it was inefficient if the particles were distributed nonuniformly in the domain. In 1988, Carrier et al. [6] introduced adaptive tree structure into the FMM, and then presented the adaptive FMM which was efficient for both uniform and non-uniform particle distribution. In the FMM, the multipole-to-local (M2L) translation costs high computational expanse because of its high complexity. In order to optimize the M2L translation, the exponential expansion was used to convert the M2L translation into a series of low complexity translation and the new version FMM was proposed by Greengard and Rokhlin [7], but this method increases memory cost and results in acceleration only if the number of expansion terms is large. In recent years, Gumerov and Duraiswami [8] presented a child-to-parent level-skip M2L translation, which may apparently accelerate all expansion terms almost without memory increment. Later, Bapat and Liu [9] presented an more efficient parent-to-child level-skip M2L translation, but it loses the accuracy.All the FMMs stated above can be introduced into BEMs to set up FMBEMs, and the advent of FMBEMs makes the BEMs obtain ability to efficiently solve large-scale engineering and scientific problems without losing its intrinsic advantages [10–13]. In the FMBEM, the final boundary integral equations are assembled from the unknowns at the nodes but the boundary integrals are element-based, while using high-order elements. Therefore, some approaches, e.g. “global-node approach” [14] and “node-patch approach” [15], need to be used if the tree structure of FMM only contains node information. An alternative better approach using high-order element in the FMBEM is modifying the tree structure to include information of both nodes and elements [16], which will be also used in this work. The major difference between the FMBEM and the BEM is that most of the coefficients in the matrix are not computed explicitly in the FMBEM, which makes the rigid body motion method (RBMM) [17] unavailable in the computation of the singular integrals and the free-term coefficients. Therefore, for 3D elastic problems, the O(1/r2) singular integrals need to be computed by analytical integral method [18] or Cauchy principal value integral method [19] and the free-term coefficients have to be computed by the solid angle method [20]. Compared to the RBMM, the latter methods are more complicated and cut off the relationship between the diagonal block coefficients and the non-diagonal coefficients, which may influence the behaviors of the coefficient matrix. Figuring out an treatment of the singular integrals that is suitable for FMBEM is meaningful and that is also an important aspect of this paper.Although the FMBEM has successfully solved large scale problems within a relatively short time, serial performance has not been improving, and parallel computing on CPUs becomes one of the efficient approaches [21–23]. In recent years, with a more important role the graphics processing unit (GPU) parallel computing playing in scientific computing, the GPU parallel FMMs or FMBEMs have been studied by researchers. In 2008, Gumerov et al. [8] pioneered an GPU parallel FMM for 3D Laplace problems and achieved 30–60 times speedup. Darve et al. [24] discussed the parallel FMM on different parallel architectures including computer clusters, multicore processors and GPU. Lashuk et al. [25] researched parallel adaptive FMM on heterogeneous architectures, and obtained 30 times speedup over a single core CPU and 7 times speedup over a multicore CPU implementation. Yokota et al. [26] and Nguyen et al. [27] used GPU clusters to accelerate FMM to solve large-scale problems. Hamada [28] used two GTX295 GPUs to accelerate indirect FMBEM for voxel model analysis with double precision, compared to the Intel Core i7-975 CPU (4 cores), and the speedups achieved 5–8 times. Takahashi et al. [29] presented four M2L GPU parallel schemes and analyzed them in detail in the case of a uniform tree, and the speedups to multicore parallelized CPU are 4–8 times. Yokota et al. [30] implemented parallel FMBEM on a GPU cluster of 512 GPUs to solve billion-scale biomolecular electrostatics problem within one minute.The outline of this paper is organized as follows: Section 2 describes the theory and procedures of FMBEM; Section 3 presents level-skip M2L and FMBEM-suitable RBMM approaches; Section 4 proposes our GPU parallel strategies of different parts in FMBEM; Section 5 analyzes and discusses large scale engineering examples; in Section 6 we present our conclusions.For a 3D physical domain Ω with boundary Γ, in the absence of body force, the discrete form boundary integral equation (BIE) for linear elasticity [31] is(1)cij(P)uj(P)=∑e=1M∫ΓeUij(P,Q)tj(Q)dS(Q)-∑e=1M∫ΓeTij(P,Q)uj(Q)dS(Q)=∑e=1M∑α=1Ntjα(Q)∫ΓeUij(P,Q)Nα(Q)dS(Q)-∑e=1M∑α=1Nujα(Q)∫ΓeTij(P,Q)Nα(Q)dS(Q),wherePis the source point,Qis the field point, M is the number of elements, Γeis the area of the element e,Nα(Q)is the shape function of α-th node at pointQ,ujα(Q)andtjα(Q)are the j-th component of displacements and tractions at the α-th node of the element whereQis, Cij(P) is free term coefficient depending on the boundary geometry at pointP, andUij(P,Q)andTij(P,Q)are the fundamental solution kernels for 3D elasticity which are given as follows(2)Uij(P,Q)=116πμ(1-v)r[(3-4v)δij+r,ir,j],(3)Tij(P,Q)=-18π(1-v)r2∂r∂n[(1-2v)δij+3r,ir,j]-(1-2v)(r,inj-r,jni),where v is Poisson’s ratio,μis shear modulus,δijis Kronecker delta, niis the i-th component of the unit outward normal, r is the distance between the pointPand pointQ, and r,iis partial derivative in the direction i of coordinates.Collocating source points on the boundary and applying Eq. (1) at each source point, the equation system is assembled as(4)Hu=Gt.Reassembling Eq. (4) by assigning all unknown displacement/traction variables and their coefficients to the left hand side and other known ones to the right hand side, the equation system may be written as(5)Ax=b,and all the unknown variables will be obtained by solving Eq. (5).The biggest bottleneck limiting application of conventional BEM in large scale problems is the dense and non-symmetric matrix A in Eq. (5), which requires O(N2) operations to compute the coefficients and solve the equation system. To solve this problem, the FMM is introduced into the BEM to construct the FMBEM. The formulations of FMBEM for 3D elasticity are listed briefly here and the detail formulation derivation could refer to [32,33]. The formulations are divided into five parts: (1) Multipole Expansions (ME), (2) Multipole-to-Multipole (M2M) translation, (3) Local Expansion (LE), (4) Multipole-to-Local (M2L) translation, and (5) Local-to-Local (L2L) translation.ME uses a expansion pointQcnear the field point to converts the integrals ofUij(P,Q)andTij(P,Q)in Eq. (1) as follows(6)∫ΓeUij(P,Q)tj(Q)dΓ(Q)≅18πμ∑n=0p∑m=-nn[Fij,n,mS(QcP→)Mj,n,m1‾(Qc)+Gi,n,mS(QcP→)Mn,m1‾(Qc)],(7)∫ΓeTij(P,Q)uj(Q)dΓ(Q)≅18πμ∑n=0p∑m=-nn[Fij,n,mS(QcP→)Mj,n,m2‾(Qc)+Gi,n,mS(QcP→)Mn,m2‾(Qc)],where p is the number of expansion terms,(QcP→)is a vector fromQctoP,Mn,mk‾(Qc)andMj,n,mk‾(Qc)are the complex conjugates ofMn,mk(Qc)andMj,n,mk(Qc)(k=1 or 2), and the formulations of ME moments and their coefficientsFij,n,mS(QcP→)andGi,n,mS(QcP→)are given in Appendix A.The ME center may be moved from Qcto Qc′ by M2M translations(8)Mn,mk(Qc′)=∑n′=0n∑m′=-n′n′Rn′,m′(Qc′Qc→)Mn-n′,m-m′k(Qc)+(Qc′Qc→)jMj,n-n′,m-m′k(Qc),(9)Mj,n,mk(Qc′)=∑n′=0n∑m′=-n′n′Rn′,m′(Qc′Qc→)Mj,n-n′,m-m′k(Qc).LE is similar to ME, which expands the integrals ofUij(P,Q)andTij(P,Q)in Eq. (1) as(10)∫ΓeUij(P,Q)tj(Q)dΓ(Q)≅18πμ∑n=0p∑m=-nn[Fij,n,mR(PcP→)Lj,n,m1(Pc)+Gi,n,mR(PcP→)Ln,m1(Pc)],(11)∫ΓeTij(P,Q)uj(Q)dΓ(Q)≅18πμ∑n=0p∑m=-nnFij,n,mR(PcP→)Lj,n,m2(Pc)+Gi,n,mR(PcP→)Ln,m2(Pc),whereGi,n,mRandFij,n,mRare obtained from Eqs. (A.1) and (A.2) with Sn,mreplaced with Rn,m, and the LE moments are obtained by the M2L translations(12)Ln,mk(Pc)≅(-1)n∑n′=0p∑m′=-n′n′Sn+n′,m+m′‾(QcPc→)Mn′,m′k(Qc)-(QcPc→)jMj,n′,m′k(Qc),(13)Lj,n,mk(Pc)≅(-1)n∑n′=0p∑m′=-n′n′Sn+n′,m+m′‾(QcPc→)Mj,n′,m′k(Qc).When the LE point is moved from Pcto Pc′, we need to use the following L2L translations(14)Ln,mk(Pc′)≅∑n′=0p∑m′=-n′n′Rn′-n,m′-m(PcPc′→)Ln′,m′k(Pc)-(PcPc′→)jLj,n′,m′k(Pc),(15)Lj,n,mk(Pc′)≅∑n′=0p∑m′=-n′n′Rn′-n,m′-m(PcPc′→)Lj,n′,m′k(Pc),The whole procedure of the FMBEM can be divided into four parts: (1) Tree structure construction, (2) Upward, (3) Downward and (4) Solution, where (2) and (3) are the core computation parts.The prerequisite of the FMM implementation is using the tree structure to store data. There are two versions of the tree structures in the FMM: the full tree structure [4,7] and the adaptive tree structure [6,34], and the later one is widely used nowadays. In the FMBEM, the final boundary integral equations are assembled from the unknowns of nodes but the boundary integrals are element-based, so the tree structure of the FMM cannot be used directly while high-order elements are used. In order to deal with this, a dual-information tree structure that contains both element and node information is constructed to satisfy the computational requirement of the FMBEM.The tree structure is constructed by dividing the bounding box domain of problem into smaller box sub-domains recursively until the number of nodes in each box sub-domain is less than a prescribed number. When each sub-domain box is generated, whether elements are classified in the domain or not is determined by whether the center of the element in or out of the box (see Fig. 1). At the level 0, a cubic bounding box containing the whole domain is defined. Boxes on level l+1 are obtained by subdividing each box on level l into 8 equal child boxes, and then the empty child boxes are trimmed off. To implement the adaptive scheme, four lists associated with box b are defined as follows:(1)List 1 (only for childless box b), denoted by L1(b), is the set consisting of b and all childless boxes adjacent to b.List 2, denoted by L2(b), is the set consisting of all children of the adjacent boxes of b’s parent that are well separated from b.List 3 (only for childless box b), denoted by L3(b), is the set consisting of all descendants of b’s adjacent boxes that are not adjacent to b.List 4, denoted by L4(b), is the counterpart of List 3, if b.∈L3(c) then c∈L4(b). Note that all boxes in L4(b) are childless.Fig. 2shows the four lists for a box b in 2D where f represents the far field boxes of b, and the lists of 3D problems can be obtained in the same way by replacing squares with cubes.Upward pass is divided into two steps: (1) compute the multipole moment of each childless box b at its center by accumulating all the element integrals in b; and (2) compute the multipole moments of other boxes by M2M translation from the penultimate level to level 2. All the multipole moments will be obtained except the boxes belonging to level 0 and level 1. An example of ME and M2M is shown in Fig. 3.Downward pass is divided into following steps:(1)From level 2 to the lowest level, loop over each box b of level l, use M2L to translate the multipole moments of L2(b) to the local moment of b and add them together. Then, from level 3 to the lowest level, loop over each box b of level l, use L2L translation to translate local moment of b’s parent to b and add to the original local moment of b. Finally, use LE to compute the U/T integrals at each node of each childless box b. An example of M2L and L2L is shown in Fig. 4.Loop over each childless box b (level independent), use direct computation as conventional BEM to compute the U/T integrals from elements of each box in L1(b) to each node in b, and accumulate the integrals.Loop over each childless box b (level independent), use ME (the number of nodes in b is larger than the square of expansion term p2) or direct computation as conventional BEM (the number of nodes in b is not larger than p2) to compute the U/T integrals from elements of each box in L3(b) to each node in b, and accumulate the integrals.From level 3 to the lowest level, loop over each box b of level l, use direct computation as conventional BEM to compute the U/T integrals from elements of each box in L4(b) to each node in b, and accumulate the integrals.The coefficient matrix of the BEM equation system Eq. (5) is dense and nonsymmetric. Iterative solvers are more efficient than direct solvers in large-scale equations. Generalized minimum residual (GMRES) method is one of most efficient iterative methods and widely used in BEM equation solutions [35], which is also used in this paper. In the GMRES solution, the most time-consuming part is matrix–vector multiplication (Aλ) whose complexity is O(N2). The core idea of the FMBEM solution is to use the FMM to accelerate the matrix–vector multiplication in GMRES, without forming the entire matrix A explicitly. In order to accelerate the convergence of the iterative solutions, the leaf based preconditioner [36] is used in this work.In the FMBEM, M2L is one of the most time-consuming parts and will be the most expensive when the expansion term p is large enough. Therefore, improving M2L can obviously improve the efficiency of the FMBEM, which attracts researchers to work on it.Greengard and Rokhlin [7] used the exponential expansion to accelerate M2L. The key concept of the exponential expansion method is using less complex expansions to replace M2L as shown in Fig. 5, but it will increase memory cost to store the exponential moments and does not result in acceleration when the number of expansion terms is small. Besides, the exponential expansions are restricted to the boxes in+X3 direction of a box b as Fig. 6, and extra rotation operations need to be applied to the boxes in other directions. More details of exponential expansion can be found in [33].Unlike the exponential expansion method, the level-skip methods accelerate M2L translation by reducing the number of M2L translations. Later, Bapat and Liu [9] presented an efficient parent-to-child level-skip M2L translation, but it sacrifices accuracy for efficiency. Gumerov and Duraiswami [8] presented an child-to-parent level-skip M2L translation to accelerate all expansion terms almost without memory increase, but they did not describe the detailed implementation. In this section, we will detailedly present an available method of the child-to-parent level-skip M2L translation.For an arbitrary box b, all boxes in L2(b) may be subdivided into two sublists L2,1(b) and L2,2(b), where L2,1(b) represents the boxes in L2(b) which are near the center of b’s parent box, and L2,2(b) represents the boxes in L2(b) which are far from the center of b’s parent box (see Fig. 7). Assuming that the edge length of box b is l and the distance from the center of a box in L2(b) to the center of b’s parent box is r, and then one of the classified rules for arbitrary boxc∈L2(b)may be concluded as(16)32l⩽r⩽352l,c∈L2,1(b)432l⩽r⩽532l,c∈L2,2(b).The boxes in L2,2(b) can do M2L translation to b’s parent box instead of box b itself, while still satisfying the specified error bound. This M2L skips from child level to parent level, so it can be called child-to-parent level-skip M2L translation in this paper. By using this technique for the M2L translation, the max number of operations per box is reduced from 189 to 119.For a box b, in tree construction, the only change is splitting the L2(b) into L2,1(b) and L2,2(b) according to the box position; and in downward, the M2L translation needs to be computed by two parts: one is for the boxes in L2,1(b) and the other is for those in L2,2(b). It is noted that the local moments of boxes at level 1 is no longer equal to 0 (original M2L does not perform at level 1, so the local moments is 0), and therefore the L2L translation in downward should start from level 2 instead of level 3. The computational cost of the L2L in level 2 is so small that it can be neglected.In 3D elasticity, the diagonal block coefficients of matrix H in Eq. (4) contain strongly singular integrals and free-term coefficients which are commonly evaluated by the rigid body motion method (RBMM) in the conventional BEM as follows(17)HijPQ=(δPQ-1)∑Q=1NHijPQ,where N is the number of nodes,δPQis the Kronecker delta, the subscripts i and j range from 1 to 3, and the superscripts P and Q refer to the nodal number.However, the RBMM cannot be used in the FMBEM because most of the coefficients of matrix H are not generated explicitly. Although the analytical integration [18] or the CPV integration [19] can be used to evaluate the strongly singular integrals, the free-term coefficients have to be evaluated by other methods, and the linear-dependence relationship between the diagonal and non-diagonal coefficients is cut off, which can influence the matrix condition because of the errors between analytical and numerical computations.In this section, we will present a RBMM that can evaluate the diagonal block coefficients of matrix H by the FMBEM. The core idea of our method is using particular boundary conditions to construct equations and solve them to get the diagonal block coefficients of matrix H. For 3D problems, set free term coefficients and traction of each node to 0, and assign the displacement coordinate components of each node to be {−1,0,0}, {0, −1,0} and {0, 0, −1} respectively, and then the following three separate equation systems can be obtained(18)H′u1=0,(19)H′u2=0,(20)H′u3=0,where H′ is matrix H of Eq. (4) with 0 free term coefficients, and u1={−1,0,0,…,−1,0,0}T, u2={0,−1,0,…,0,−1,0}T, u3={0,0,−1,…,0,0,−1}T.The FMBEM is used to compute H′ui. When the diagonal block coefficients of matrix H′ (the coefficients of each node to itself in) is temporarily set to 0 in the near field direct integration (NFDI) of downward, the equations become(21)H′u1=b1,(22)H′u2=b2,(23)H′u3=b3,where b1, b2 and b3 are the vectors consisting of diagonal block coefficients of matrix H as shown in Fig. 8.The idea of our method is the same as RBMM, and successfully used in FMBEM without computing all coefficients of matrix H. All the diagonal block coefficients of matrix H only need to be evaluated and stored once, so it will not influence the whole efficiency of the FMBEM.GPUs were originally used for graphic processing but are increasingly being applied to scientific computations due to their outstanding computational power, especially since NVIDIA released the Compute Unified Device Architecture (CUDA) in 2007 [37].Under CUDA, a program is composed of both host part (CPU) and device part (GPU), where host part is in charge of serial parts, and device part is in charge of parallel parts. The functions defined in device part are called kernel functions. Threads are the units of execution on the GPU. A certain number of threads bundled together form a thread block. Within a block, threads are executed in groups called warps (32 threads) which always run together on a processor (core). There are several memories that can be used by programmers to achieve high efficiency. Global memory, shared by the whole GPU which has a noticeable latency, usually stores input and output data; registers are allocated to individual threads, and each thread can only access its own registers (access speed is very high); shared memory can be accessed by all threads in a block almost as fast as registers; constant memory is cached, where host can write and read but device can only read; local memory which is allocated to each thread usually stores data when register is used out; and texture memory has some special functions (e.g. texture rendering).NVIDIA GTX 580 GPU is used to run our parallel FMBEM program, which contains 16 streaming multiprocessors and 512 CUDA cores. Global memory occupies most of the available memory (1.5GB). The 32 processors in a multiprocessor share 64KB of RAM with a configurable partitioning of shared memory and L1 cache, and all multiprocessors share 64KB constant memory.In this section, A CUDA based GPU parallel algorithm of the adaptive FMBEM with level-skip M2L is presented according to the intrinsic parallelism of boundary elements and levels of the adaptive tree, which accelerates ME, M2M translation, M2L translation, L2L translation and NFDI in the FMBEM.The MEs of childless boxes are independent so they can be computed in parallel in different levels of the adaptive FMBEM. From Eqs. (A.1)–(A.4), it can be obtained that the subscript (n,m) computations of the momentsMn,mkandMj,n,mkare independent. A parallel strategy is presented as follows: one block completes the computations of one childless box (one-block-one-box) as Fig. 9, and each thread in the block completes the moment component computation corresponding to subscript (n, m). Assume the expansion term is p, the subscript (n, m) satisfies(24)0⩽n⩽p-n⩽m⩽n(n,m∈integers),where only the moment components whose subscriptm⩾0need to be computed and the others can be obtained from the complex conjugate relationship as Eqs. (A.7) and (A.8).One dimension arrays are used in graphic memory for ME coefficients, and thereforeMn,mkare stored in an array, andMj,n,mkare stored in three arrays depending on the value of j. The location of (n, m) in an array is(25)loctn,m=n(n+1)/2+m.One thread is assigned to compute one moment component corresponding to subscript (n, m). Then the number of threads in one block is (p+1)(p+2)/2, and the thread to (n, m) mapping relationship is shown in Fig. 10.In this parallel mode, all the multipole moments are stored in shared memory that can avoid the access latency of global memory, and the loads between each thread are balanced. When the number of leaves is large enough, the computing capability of GPU can be fully utilized. The values of multipole moments are copied from shared memory to global memory when all the moment components are obtained. The parallel GPU ME algorithm is demonstrated as followsAlgorithm 1: GPU parallel ME// Number of blocks numblock is equal to the number of childless boxes// Number of threads in a block is p(p+1)/2+p+1// Variable bid is block ID and tid is local thread ID in bidFor bid=0,1,…,numblocksin parallel doIf tid=0 thenGet the number of a childless box b and the coordinates of its centerEnd ifGet ME moment components through Eqs. (A.1)–(A.4), where tid computes the component of the tid-th location.End forRemark: In Algorithm 1, the “For …numblocksin parallel do” structure is not a real loop structure, which is just used to describe the parallel process. This representation method will be also used in the following algorithms.The M2M translations are from the boxes of the lowest level to the boxes at level 2 recursively. It means that the M2M translations of different levels cannot be parallelized; however, all M2M translations at the same level can be parallelized. There are two parallel strategies as shown in Fig. 11.(1)Parallelization of child level boxes (Fig. 11(a)). The advantage of this strategy is that the number of the child level boxes is large (the parallel scale is large), but it has a disadvantage that the M2M translations from different boxes of the child level need to be accumulated to their parent boxes, which causes the accumulative process unable to be parallelized because of data writing conflict.Parallelization of parent level boxes (Fig. 11(b)). Although the parallel scale of this strategy is smaller than the parallelization of child level boxes, it can avoid data writing conflict in the process of M2M translations. Therefore, this strategy is used in the M2M translation in this work.According to Eqs. (8) and (9), it is easy to find that each component represented by subscript (n, m) can be computed in parallel, so the parallelization strategy of subscript (n, m) in the ME can also be applied to the M2M translation. The GPU parallel algorithm of the M2M translations is described as followsAlgorithm 2: GPU parallel M2M translation// Number of tree levels is nlevel, Number of blocks required at level i is numblocks_i// Number of threads in a block is p(p+1)/2+p+1// Variable bid is block ID which represents box bbidand tid is local thread ID in bidFor level l=nlevel-1, nlevel-2,…,2 doForbid=0,1,…,numblocks_l-1in parallel doFor each box b∈child boxes of box bbiddoEach thread tid parallel translates the ME moment to the subscript (n, m) component of ME moment of box bbidby Eqs. (8) and (9), then adds it to the counterpart of the ME moment component of box bbidend forcend forbidend forlIn general, the M2L translations are evaluated level by level. In fact, the M2L translations of different levels are independent, which means they can be evaluated in parallel. Therefore, in the GPU parallel computing, we can use one block to charge the M2L translations of one box, and use one thread to evaluate the LE component of each subscript pair (n, m).When the level-skip M2L translation approach is used, List 2 of a box b, denoted by L2(b), is divided into the near sublist L2,1(b) and far sublist L2,2(b) as Fig. 7. In the parallel M2L translation process, data writing conflict will occur. As shown in Fig. 12, box b1 and box b2 share the same parent box fb, and box c1 and box c2 belong to L2,2(b1) and L2,2(b2) respectively, when the level-skip M2L translations of boxes in L2,2(b1) and L2,2(b2) are computed in parallel, the translations from box b1 and box b2 may happen at the same time that results in data writing conflict in the moment summation process of box fb.In order to solve this problem, we change the tree structure and useL2,2′(fb)to replace L2,2(fb) for each box fbfrom the lowest level (level nlevel) to level 1 as(26)L2,2′(fb)=L2,2(b1)∪L2,2(b2),…,∪L2,2(bi).It can be seen thatL2,2′(fb)is a set of all L2,2(bi) of box fb’s child box biand the data conflict can be avoided. Meanwhile, the M2L translations at level 1 need to be computed, but the computational cost of the M2L translations at level 1 is very small. The GPU parallel algorithm of the M2L translations is demonstrated as followsAlgorithm 3: GPU parallel M2L translation// Number of tree levels is nlevel, Ciis the ith level of the tree structure// The total number of boxes from C1 to Cnlevelis N1, and the corresponding number of blocks is numblocks1=N1// The total number of boxes from C2 to Cnlevelis N2, and the corresponding number of blocks is numblocks2=N2// Number of threads in a block is p(p+1)/2+p+1// Variable bid be block ID which represents box bbidand tid is local thread ID in bid// Steps (1) and (2) need to call the kernel function respectively(1) Forbid=0, 1,…,numblocks2-1in parallel doFor each box c∈L2,1(bbid) doEach thread tid parallel translates ME moment to the subscript (n,m) component of LE moment of box bbidby Eqs. (12) and (13), then adds it to the counterpart of the LE moment component of box bbidtogetherend forcend forbid(2) Forbid=0, 1,…,numblocks1-1in parallel doFor each box c∈L2,2(bbid) doEach thread tid parallel translates ME moment to the subscript (n,m) component of LE moment of box bbidby Eqs. (12) and (13), then adds it to the counterpart of the LE moment component of box bbidtogetherend forcend forbidIn the L2L translation, similar to the M2M translation, only the translations at the same level can be parallelized, but it needs to be evaluated from level 2 to the lowest level which is opposite to the M2M translation. It means that the data writing conflict will not occur in the L2L translation, so we utilize the strategy of parallelization of child level boxes (Fig. 11(a)) to the L2L translation. The GPU parallel algorithm of the L2L translations is demonstrated as followsAlgorithm 4: GPU parallel L2L translation// Number of tree levels is nlevel, Number of blocks required at level i is numblocks_i// Number of threads in a block is p(p+1)/2+p+1// Variable bid is block ID which represents box bbidand tid is local thread ID in bidFor level l=2, 3,…,nleveldoForbid=0, 1,…,numblocks_l-1 in parallel doEach thread tid parallel translates the LE moment of box bbid’s parent box to the subscript (n,m) component of the LE moment of box bbidby Eqs. (14) and (15), then adds it to the counterpart of the LE moment component of box bbidend forbidend forlIn the FMBEM, each node needs to use direct integration to complete the element integrals which are near itself, and this direct integration is called near field direct integration (NFDI). The NFDI may be used in the computations of List 1, List 3 and List 4 of each childless box.Element integrals are box independent, and thus they can be computed in one-block-one-box parallel mode. We compute integrals of one element in a field box to all nodes in a source box (Fig. 13(b)) in parallel, and then loop the elements in the field box to complete the box computation. In this way, the data writing conflict of the summation of element integrals to one node as shown in Fig. 13(a) is avoided.According to our parallel strategy, the number of threads in a block is set to the maximum number of nodes allowed in a leaf, and the shared memory is used to store the integral results. The parallel CUDA NFDI algorithm is described as followsAlgorithm 5: GPU parallel NFDI// Number of blocks numblock is equal to the number of childless boxes// Number of threads in a block x is the max number of nodes allowed in a box// Use Nn_i and Ne_i to represent the number of nodes and the number of elements in box birespectively// Variable bid is block ID which represents box bbidand tid is local thread ID in bidForbid=0, 1,…,numblocks-1in parallel doFor each box c∈L1(bbid)∪L3(bbid)∪L4(bbid) doIftid<Nn_bidthenFor element e∈box cdoEach thread tid parallel does the integral computation of element e to all nodes in box bbid, and then accumulates to the original integral of the nodesend foreend ifend forcend forbidThe GMRES method which we used in the FMBEM is an iterative method for the numerical solution of a system of linear equations as Ax=b. Each iterative process is dependent and cannot be parallelized, but more than 90% computational cost is spent in linear algebra operations which can be computed in parallel.The linear algebra operations in the GMRES can be divided into five types [38]: (1) matrix vector multiplication; (2) vector dot product; (3) vector Euclidean norm; (4) vector real multiplication and (5) vector multiply–add operation. The most complicated operation is matrix vector multiplication which is completed by the FMBEM. The rest of the four types can be easily completed by assigning suitable blocks and threads according to the computational scale, which will not be discussed in detail here.The FMBEM programs are executed on a desktop computer: CPU is Intel Core i7-2600k 3.4GHz (1 core is used), GPU is NVIDIA GTX 580 GPU, OS is Windows 7 Professional, RAM is DDR3 SDRAM (8GB), the compiler for CPU code is Microsoft Visual Studio 2008, and the compiler for GPU codes is NVIDIA CUDA 4.0 (C/C++ language).The sizes and boundary conditions of the bracket model are shown in Fig. 14. Elastic modulus is 200GPa, Possion’s ratio is 0.3, and the load is 20MPa (−Z direction). The surfaces of the model are meshed with 6646, 14,172 and 55,560 linear triangular elements, respectively, and the degrees of freedom (DOFs) are 9945, 21,234 and 83,316. The number of expansion term p is 8, and the maximum number of nodes allowed in a childless box is set to 20. The convergence tolerance of GMRES solution is set to 10−3.In this paper, the level-skip M2L method is used to accelerate the M2L translations. Therefore, we compare the level-skip M2L with the exponential-expansion based M2L and the conventional M2L, which are distinguished by M2L_S, M2L_E and M2L in the following of this section. The bracket model with 9945 DOEs is used here, and the number of expansion term p is chosen 6, 8, 10, 12 for the multipole expansion and location expansion, respectively. The number of expansion term g is chosen 8 and 17 for the exponential expansion, which contains 3 and 6 digital accuracy, respectively [34].In order to avoid the error introduced by solution, we use the right hand side vector b of equation system Ax=b to compare the accuracy and efficiency of different M2L translation methods. The vector b obtained from conventional BEM is used as reference, so the relative error of vector b is defined as(27)ε=‖bBEM-bFMBEM‖2‖bBEM‖2,where bBEMand bFMBEMare the right hand side vectors of the conventional BEM and the FMBEM, respectively.Table 1lists the relative errors of vector b using different M2L translations. The accuracies of both M2L and M2L_S increase with the number of expansion term p. The accuracy of M2L_S is a little lower than that of M2L, but still keeps in the same order of magnitude. The accuracy of M2L_E is not only associated with expansion term p but also with the expansion term g, and high accuracy cannot be obtained if either of them is small.Table 2lists the times costs in the M2L translations using different M2L methods. The rising trends of time costs of M2L and M2L_S are consistent when the number of expansion term p increases, and acceleration of M2L_S is irrelevant to the number of expansion term. The M2L_E accelerates the M2L translation only when p equals to 6, 8 and 10 and g equals to 8, but its rising speed of time cost is slower than that of other methods when the number of expansion term p increases. This means that M2L_E makes full use of its advantages only in the situation that p is very large.The memory costs of the FMBEM using different M2L methods are shown in Table 3, where it can be found that the M2L_E method needs to use more memory than other methods. The memory costs between M2L and M2L_S are almost the same.Comprehensively comparing the accuracy, efficiency and memory cost, the M2L_S performs the best, that is why it is used in this paper. In the following sections, all the M2L translations uses the M2L_S methods if no explicit explanation is made.Before discussing the GPU-FMBEM, the accuracy of the CPU-FMBEM is analyzed, which is the basis to demonstrate the accuracy of the GPU-FMBEM. We pick up 4 points along the load direction (see Fig. 14) as the sample points to compare the displacement results between the FMBEM, conventional BEM using 10−5 convergence tolerance and the finite element software ANSYS [39] using 16,041 quadratic tetrahedral elements with 74,688 DOFs. Another two meshes with 1932 and 3678 elements (DOFs are 2874 and 5493) are also discussed to compare the accuracy.Table 4lists the relative errors of displacements at the sample points, where F/B and F/A represent the relative errors of FMBEM/BEM and FMBEM/ANSYS, respectively. The symbol “——” represents that the problem scale is too large to be solved by the desktop using the conventional BEM. It can be found that the relative errors between the FMBEM and the BEM are less than 0.6% except point 1, and the max error at point 1 is only 2.1%. The relative errors between the FMBEM and ANSYS are less than 2% except the case that the FMBEM with coarse elements. These demonstrate the accuracy of the FMBEM. The reason for that the displacement of point 1 is less accurate is that the displacement value is small which is more sensitive to the computational errors.We use the right hand side vector b of equation system Ax=b and displacement result vector u to analyze the accuracy of the FMBEM. The relative errors of vector b and u are defined as(28)εb=‖bFMBEM_C-bFMBEM_G‖2‖bFMBEM_C‖2,(29)εu=‖uFMBEM_C-uFMBEM_G‖2‖uFMBEM_C‖2,where subscript FMBEM_C represents the results of CPU (double precision) and FMBEM_G represents the results of GPU (single precision). We can respectively obtain the errors between double precision and single precision in a FMM process and iterative solution by the comparisons of vector b and vector u.In Table 5, it can be found that errors exist between GPU and CPU, but the errors are small. In this example, the magnitude of the max error is 10−3 which is acceptable by engineering computation. Comparing error εband error εu, we can find out that εuis larger than εbbut still in the same order of magnitude, which means that the errors accumulate in the iterative processes but the speed of error increase is slow. Fig. 15shows the errors in load direction of randomly selected nodes, and the scale of the max error is 10−3, which is consistent with that of Table 5.Table 6shows the number of iterations, time and memory of different problem sizes. Although in some cases, using single precision increases iterative number of solution [35], this phenomenon does not occur in the example. From the total time in Table 6, it can be seen that GPU computing has obviously accelerated the FMBEM, and the speedups of 9945, 21,234 and 83,316 DOFs are 8.5, 9.8 and 10.4 respectively. The speedups increase with the solving scales because large-scale computation can fully utilize the computational capability of GPU, but the speedups will not increase when the solving scale reach a certain degree. The reason why memory of GPU is less than that of CPU is that GPU uses single precision data which occupy less memory than double precision data used by CPU.Table 7shows the time spent in different parts of one upward–downward process, and Table 8gives the corresponding GPU/CPU speedups. GPU greatly accelerates the FMBEM with different solving scales, especially for the M2L whose speedup is more than 30, which is comparable to the 4–8 times speedup between the GPU and multi-core parallelized CPU in [29] and the 4–32 times GPU/CPU speedup of the FMM downward pass in [8]. The acceleration effects of different parts is summarized as: M2L>ME>NFDI>L2L≈M2M. ME, M2L and NFDI spend most of time (more than 95% of the total time in the FMBEM process), especially the M2L and NFDI. In the GPU computing, the NFDI is the most time-consuming part because its speedup is much smaller than that of M2L, so this part will be further optimized in the future.The major dimensional sizes, boundary conditions and mesh model of the machine model are shown in Fig. 16. Elastic modulus is 260GPa, Possion’s ratio is 0.3, the load value of the spindle surface is 15MPa (Z direction), and the load of the worktable is 5Mpa (−Z direction). The surfaces of the machine tool are meshed with 168,028 linear triangular elements, and the degrees of freedom (DOFs) are 251,988. The number of expansion term p is 6, and the maximum number of nodes allowed in a childless box is 32.The convergence tolerance of GMRES solution is set to 10−3. In order to verify the results of the FMBEM, the software ANSYS is used, which used linear and quadratic tetrahedral elements (the same edge size as those in the FMBEM) to discretize the machine tool model and the DOFs is 493,919 and 3,699,943, respectively.Computational parameters of machine tool adopting different methods are shown in Table 9, where FMBEM_C represents the CPU serial solution, FMBEM_G represents the GPU parallel solution, and FEM_1 and FEM_2 represent ANSYS solution with linear and quadratic elements, respectively. The memory data of ANSYS are separated by symbol ‘||’, where the front one is memory allocated for solver and the other one is the memory required for in-core mode solution (only uses RAM). ANSYS used out-of-core mode to solve the machine tool model because the in-core required memory has exceeded the available RAM of the computer.From the computational time, it can be found that FMBEM_C is slower than FEM_1 but a little faster than FEM_2, and FMBEM_G is faster than all the others. The speedup of FMBEM_G/FMBEM_C reaches 11.2 times, and the speedups of FMBEM_G/FEM_1 and FMBEM_G/FEM_2 are 12.2 and 1.9 times. Note that the efficiency comparison is just given as a reference which cannot represent the algorithm comparison of the FMBEM and FEM, because the FMBEM codes are just written for research without professional optimization. From the memory cost, we can find that FMBEM_C and FMBEM_G consume much less memory than ANSYS, which means that the FMBEM can solve large problems using limit memory resource.Figs. 17 and 18show the displacement and von Mises stress of machine tool model respectively. The results among FMBEM_C, FMBEM_G, FEM_1 and FEM_2 are consistent. The displacement error of FMBEM/FEM_2 is less than 8% which is smaller than that of FEM_1/FEM2, and the stress error (less than 20%) of FMBEM/FEM_2 is also smaller than the error of FEM_1/FEM_2. The reason for this is that linear triangular elements used in the FMBEM have C1 continuity in displacement but only have C0 continuity in stress, while the quadratic tetrahedron elements used in ANSYS have C2 and C1 continuity in displacement and stress respectively. Therefore, the quadratic triangular elements is planned to be used in the FMBEM to improve the accuracy in the following research. One thing should be noted that these error are smaller than those between linear and quadratic elements in ANSYS which demonstrates the accuracy of the FMBEM.

@&#CONCLUSIONS@&#
In this paper, we presented GPU parallel strategies for different parts of the FMBEM with level-skip M2L in 3D elasticity. We first briefly introduced the theory and procedure of the adaptive FMBEM whose tree structure contains both node and element information. In order to accelerate the M2L translation, an available child-to-parent level-skip M2L translation was presented in detail. Meanwhile, a RBMM suitable for FMBEM was proposed to avoid the CPV integration and the computation of free term coefficients. After that, the GPU parallel algorithms of the adaptive FMBEM are presented according to the intrinsic parallelism of boundary elements and boxes of the adaptive tree, which accelerates ME, M2M translation, M2L translation, L2L translation and NFDI in the FMBEM. Two examples were used to verify our algorithms, and the results showed that the theories and algorithms in this paper have advantages of high computing efficiency, large solving scale and strong adaptability, which have a promising future in engineering.In the future, we will further improve our FMBEM algorithms, especially in computational accuracy improvement and computational speed acceleration. In the computational accuracy improvement, quadratic elements will be used and the nearly singular integrals will be dealt with by some especial techniques like subdivision of the integration region [40] or semi-analytical algorithm [41]. In the speed acceleration part, we plan to use the reusable intrinsic sample point algorithm [42] to accelerate the NFDI in the FMBEM. Besides that, we will expand our FMBEM to other engineering problems, such as thermodynamics, fluid mechanics and acoustics.