@&#MAIN-TITLE@&#
A new feature extraction framework based on wavelets for breast cancer diagnosis

@&#HIGHLIGHTS@&#
All healthy mammographic patches are correctly classified by a proposed framework.The LCP descriptor is very successful on both microcalcifications and masses.LCP parameters are directly computed on frequency domain representation of images.Proposed framework is effective on separation of malignant cases from benign ones.The new framework can be developed to aid radiologists for more accurate diagnosis.

@&#KEYPHRASES@&#
Breast cancer,Feature extraction,Digital mammography,Computer aided diagnosis,

@&#ABSTRACT@&#
This paper investigates a pattern recognition framework in order to determine and classify breast cancer cases. Initially, a two-class separation study classifying normal and abnormal (cancerous) breast tissues is achieved. The Histogram of Oriented Gradients (HOG), Dense Scale Invariant Feature Transform (DSIFT), and Local Configuration Pattern (LCP) methods are used to extract the rotation- and scale-invariant features for all tissue patches. A classification is made utilizing Support Vector Machine (SVM), k-Nearest Neighborhood (k-NN), Decision Tree, and Fisher Linear Discriminant Analysis (FLDA) via 10-fold cross validation. Then, a three-class study (normal, benign, and malignant cancerous cases) is carried out using similar procedures in a two-class case; however, the attained classification accuracies are not sufficiently satisfied. Therefore, a new feature extraction framework is proposed. The feature vectors are again extracted with this new framework, and more satisfactory results are obtained. Our new framework achieved a remarkable increase in recognition performance for the three-class study.

@&#INTRODUCTION@&#
Breast cancer is a type of cancer that commonly occurs in the milk ducts or the lobules lying within breast tissue [31]. Breast cancer cases represent 30% of all cancer cases. Pal et al. [27] declared that, annually, more than a million women have breast cancer, and 400 thousand of those cases lead to death. This fatal disease affects not only developing countries but also developed countries. It is quite important to diagnose breast cancer in its early phases to prevent death. Cheng et al. [8] state that early diagnosis should not only include breast cancer detection but also specify whether the breast cancer is benign or malignant. It is very complicated to identify lesion (tissue disorder) type via a mammogram image, so even radiologists miss cancerous regions 10–30% of the time. Therefore, Computer-Aided Diagnosis (CAD) applications are necessary to aid specialists in carrying out more accurate diagnoses.Despite all of the technological advancements in medical electronics, it is still hard to diagnose breast cancer circumstances on digitized mammograms because of the lack of distinction between benign and malignant tissue structures on mammograms. Rabottino et al. [28] mention that if more than one radiologist participates in the diagnosis process, the possibility of diagnosing an abnormal tissue could be increased by up to 10%. Generally, mammographic images have slight information about cancerous regions because there is only a small difference between normal and abnormal tissues due to X-ray permeability. It is especially difficult to understand cancerous regions for young women because of the high density of their tissues. Additionally, the X-ray attenuation over microcalcifications results in a low-contrast region on a mammogram, so especially small lesion areas can be harder to identify.The classification of mammographic images is negatively affected by the presence of noise. X-ray dose and other medical equipment make the mammograms noisy, and this noise can be modeled as Gaussian additive noise, Poisson noise and multiplicative quantum noise. Naveed et al. [24] state that there could be up to a 21% reduction in mammogram quality due to noise. This decrease in quality reduces microcalcification diagnosis rate from 89% to 67%, and lesion presence detection rate from 93% to 79%. Romualdo et al. [29] used the Wiener filter following the Anscombe Transform to suppress quantum and Poisson noise. In addition, a neuro-fuzzy filter can be utilized to address same noise types, and this filter scheme is an effective way to denoise if the noise presence on a mammogram image is low [3]. However, if the amount of noise is high, this filter unfortunately discards critical information of the image. Eng and Ma [11] propose a Noise Adaptive Soft Switching Median filter, which is another spatial filter technique to preserve small details of images. Naveed et al. [24] has used the combination of the Frost Filter, Wiener Filter and Non-Local Means (NLM) Filter in order to eliminate noise on mammograms. Continuous Wavelet Transform (CWT) has also been used for the visual enhancement of microcalcifications [17]. Another proposed filtering methodology is an adaptive method comprising the noise matching and Wavelet shrinkage operations [18]. Only horizontal and vertical decompositions of Wavelets have been used, and Wavelet coefficients have been reproduced according to various scale-space constraints by this method. Although it is a flexible method, some parameters need to be determined by the user.Image restoration, which is a crucial part of classification for mammogram images, includes noise filtering, the reduction of blurred areas and the improvement of the contrast of the image. This subject can be separated into two categories: conventional and blind restoration [20]. The conventional approach requires foreknowledge about the image. The amount of blur presence and distortion can be described by the conventional methods. However, it is not easy to have foreknowledge of images in practice; therefore, the blind restoration techniques are generally preferred. The main purpose of these methods is to save vital information of images by discarding unnecessary information.The success of restoration and filtering is not enough to make an efficient classification of mammogram images. Feature extraction is the most important step of breast cancer diagnosis over CAD framework. This process can only be performed if the suspicious structures, tumor areas of benign and malignant lesions, are properly described. In addition to a convenient representation, the extracted features must also be affordable in size for feasibility and computational efficiency. Feature extraction of mammographic images needs space transformations to maintain those requirements. The Gabor Wavelet [5], Discrete Wavelet Transform (DWT) [13], Principal Component Analysis (PCA) [4], and Local Binary Pattern (LBP) [32] are some examples of the feature extraction methods of digitized mammograms. [1] have used samples from the Digital Database for Screening Mammography (DDSM) and Overcomplete Wavelet Transform (OWT) with Haar Wavelets to identify masses on mammograms, and they have achieved a 90% classification rate. Martins et al. [23] have suggested the spatial features as contrast, homogeneity, inverse-difference moment, entropy and energy from co-occurrence matrices to discriminate cancerous regions. Without a feature extraction step, the mammogram images have been directly classified with 80% accuracy using a cascaded Support Vector Machine (SVM) classifier on DDSM [7].In this paper, initially, a two-class classification study separating normal and abnormal (cancerous) breast tissues is conducted. The rotational- and scale-invariant features are extracted by the HOG, DSIFT, and LCP descriptors for all tissue patches. A classification is made utilizing SVM, k-NN, Decision Tree, and FLDA classifiers via 10-fold cross validation. Then, a three-class (normal, benign, and malignant cancerous cases) study is achieved using similar procedures in a two-class case; however, the obtained classification accuracies are not sufficiently satisfied. Therefore, a new framework for the feature extraction phase is proposed. In this framework, an NLM filter [9] is first applied to all of the mammographic patches. Then, the feature vectors are extracted using HOG, DSIFT, and LCP descriptors. Finally, the newly constructed feature vectors are classified with SVM, k-NN, Decision Tree, and FLDA classifiers via 10-fold cross validation, and more satisfactory results are attained. Our new feature extraction framework achieved a noticeable increase the recognition rates. The rest of the paper is organized as follows. All feature extraction techniques and classifiers used in this study are explained in the second and third sections, respectively. Experimental works are included in the fourth section, while all discussions on the obtained experimental results are given in the fifth section. The last section comprises of conclusions.The Histogram of Oriented Gradients (HOG) descriptor [39] is based on the distribution of a differential intensity histogram of an image. Each image is divided into non-overlapping uniform cells, as shown inFig. 1. Intensity variations, which belong to differentials for a desired orientation, are calculated for different directions in each cell.In HOG computation, each block creates the density of its intensity gradients. The 2-dimensional gradients,(∂f(x,y))/∂xand(∂f(x,y)/∂y), are calculated by filtering the image with the filters [−1, 0, 1] and [−1, 0, 1]T, respectively (Eqs. (1) and (2)).(1)Gx=∂f(x,y)∂x=f(x+1,y)−f(x−1,y)(x+1)−(x−1)(2)Gy=∂f(x,y)∂y=f(x,y+1)−f(x,y−1)(y+1)−(y−1)The weighting, (Δf(x,y)), and orientation, (θ(x,y)), for an image pixel can be calculated as(3)Δf(x,y)=Gx2+Gy2(4)θ(x,y)=tan−1(Gy2Gx2)The weighting, (Δf(x,y)), is the magnitude of the vectorial sum of 2-D (x and y) directional derivatives. The orientation, (θ(x,y)), is the angle of the gradient vector with the positive x-axis. The orientation is uniformly distributed, and it is in the interval of either [0, π] or [0, 2π]. The usage of the HOG descriptor is an efficient way to describe textures and deformable objects [12]. Moreover, the HOG descriptor is also a convenient technique for identifying abnormal tissues in medical imaging [32]. A more effective descriptor could be achieved by enhancing each cell in contrast.The Scale Invariant Feature Transform (SIFT) descriptor [42] is a feature extraction method that calculates the gradient vectors of an image. The evaluation of the SIFT descriptor [22] can be investigated in three parts.Firstly, some key points are to be searched over all of the image regions. An efficient approach, Difference of Gaussian (DOG), is necessary for determining the rotation- and scale-invariant possible key points. A scale-space function is then created utilizing a Gaussian function under some assumptions [22], as shown in Eq. (5).(5)L(x,y,σ)=G(x,y,σ)×I(x,y)I(x,y)is the input image, whileG(x,y,σ)is a Gaussian function with different variances.(6)G(x,y,σ)=12πσ2e−(x2+y2)/2σ2The scale-space function in Eq. (5) can be more efficiently calculated with the changes in Eq. (7).(7)D(x,y,σ)=[G(x,y,kσ)−G(x,y,σ)]2×I(x,y)=L(x,y,kσ)−L(x,y,σ)The functionD(x,y,σ)is substantially sensitive to edges and noises of an image. For this reason, unstable key points are to be removed among the possible key points. When the DOG is calculated over an edge, the horizontal response is very high, whereas the vertical one is too small. A Hessian matrix is used in order to overcome this problem.(8)H=[DxxDyxDxyDyy]Dxx,Dyx,Dxy, andDyyare the partial derivatives for the directions of x and y. The threat of contrast differences can be prevented with a vector normalization process.The SIFT descriptor uses a Gaussian smoothing function for each key point. The local region of any key point is supposed to be scale-invariant by the nature of the SIFT function. Assume thatL(x,y)refers to the x and y coordinates of the current scale;m(x,y), the magnitudes of gradients over the current region, andθ(x,y), the rotation of the gradients, can be calculated as(9)m(x,y)=[L(x+1,y)−L(x−1,y)]2+[L(x,y+1)−L(x,y−1)]2(10)θ(x,y)=tan−1([L(x,y+1)−L(x,y−1)][L(x+1,y)−L(x−1,y)])An orientation histogram is then calculated by the locations around the determined key points. There are 36 cells belonging to 360° in the orientation histogram. An additional key point calculation may be made for every magnitude peak value, or 80% of the magnitude peak values, in the orientation histogram. Consequently, rotation-invariant features are obtained. Sample DOG-based gradients evaluated using the SIFT descriptor are illustrated inFig. 2.Later, feature vectors are constructed using each orientation histogram of each cell, and all feature vectors are normalized.The Dense Scale Invariant Feature Transform (DSIFT) descriptor [38] is a variant of SIFT that presents both scale and rotation invariant features. The difference between SIFT and DSIFT is their method of key point selection. DSIFT uses a given radius value, R, to uniformly overlap cells in the entire image. The procedure steps are the same as for the SIFT descriptor. Because a dense sampling can be achieved over an image, DSIFT is more effective than SIFT for feature extraction when foreknowledge about an image is limited.The Local Configuration Pattern (LCP) descriptor [14] is a method to describe an area in an image with local information and a circularly shifted histogram of pattern occurrences, which makes the image rotationally invariant. LCP features can be used to evaluate both the microscopic features and local features represented by pattern occurrences, as illustrated inFig. 3.Pattern occurrences, referring to the local structure of an image, are calculated using Local Binary Pattern (LBP) [32], which labels the pixels of a gray level intensity image by utilizing the circular neighborhood of each pixel. The LBP method uses a circular neighborhood for a given radius, R, from each pixel (Eq. (11)).(11)LBP(P,R)=∑i=0P−1u(gi−gc)2iP is the number of pixels.giis the intensity of the ith pixel, whilstgcis the intensity of the center pixel. Though LBP methods are superior when illumination differences are considered, the center pixel values are not too far from nearby pixels if the local structure of an image is a smooth area. In this circumstance, nearly the same vector for all pixels that are equidistant from the center pixel is obtained. The variance (VAR) is to be calculated for the creation of the histogram in order to get rid of this problem [14].(12)VAR=1P∑i=0P−1(gi−µ)2µ refers to the average intensity of nearby pixels from the center pixel. Consequently, this method combines the microscopic configuration that represents the textural property of an image with the pattern occurrences.Two Dimensional (2D) Gabor filters are frequently used in computer vision applications and medical image processing [5,41]. A Gabor filter has the linearity property because the impulse response of a Gabor filter is a multiplication of a Gauss distribution by a harmonic. A 2D Gabor function can be formed as(13)GF(x,y)=exp(−x2+γ2y22σ2)cos(2xx′λ+φ)(14)x′=xcosθ+ysinθ(15)y′=−xsinθ+ycosθx and y are the pixel coordinates of an image. The Gabor function has the following parameters: Wavelength (λ), orientation (θ), phase shift (φ), standard deviation (σ) and spatial aspect ratio (γ). The ׳σ׳ value was selected as ׳2׳ in this paper. The size of the data created over a Gabor filter is equal to the image size.The Gabor wavelet method, the complex form of a Gabor filter, is based on the mathematical modeling of brain cells that provide the sense of sight [34,2]. The Gabor wavelet is rotationally invariant and appropriate for tissue classification applications. It is calculated by multiplying a complex sinusoidal with a Gaussian kernel as(16)Ψk(z)=k2σ2exp(k22σ2z2)[exp(ikz)−exp(−σ22)]z(x,y) is the variable in the spatial domain and k is the frequency vector, which determines the orientations (µ) and the scales (v) of the Gabor kernels. The frequency vector, k, is composed of different Gabor filter frequencies and computed using Eq. (17).(17)kv=2−v+22πwhere v refers to the Gabor filter frequency. The orientation is calculated as(18)φμ=μπ8Fig. 4 includes a Gabor wavelet transform with 2 different scales (v=0, 1) and 4 different orientations (µ=0, 1, 2, 3) of a sample image [43].The k-Nearest Neighborhood (k-NN) classifier is one of the basic classification techniques. k-NN classifies a test sample by considering the closest training samples in the feature space [19]. In other words, a test (unknown) feature vector is assigned to the class that is the most common among its k closest neighbors. The value of k is empirically determined. It can be determined by considering the classification error on the training samples [35]. When there are only two classes, k must be an odd integer. Moreover, the k value must not be the same as the number of classes. The illustration of the k-NN classification procedure is given explicitly inFig. 5.The most common distance function for k-NN is Euclidean distance. For two feature vectors of n dimensions,x⇀=(x1,x2,…,xn)Tandy⇀=(y1,y2,…,yn)T, the Euclidean distance is computed as [33](19)Euclideandistance(x⇀,y⇀)=(x1−y1)2+(x2−y2)2+…+(xn−yn)2An important benefit of the k-NN classifier is that it is efficacious in multi-class cases because its decision criterion is simple. Therefore, the k-NN algorithm can provide a satisfactory degree of accuracy even if the number of classes is high.One of the well-known and widely used classifier is the Decision Tree [40]. In this classifier, the training phase performs a recursive operation on the training data samples until a stopping criterion is met. The Decision Tree includes a root node, internal nodes and terminal nodes. Terminal nodes are associated with one of the class labels. The construction of a decision tree begins with placing a feature at the root node, and then they branch from top to bottom to allocate internal and terminal nodes. During the construction of the tree, it is intended to maximize the purity of outputs at each node, and this can be achieved by minimizing the impurity. Impurity is a measure that indicates how well classes are separated. Entropy is a widely used tool for the impurity measure. A decision tree stops when the highest decrease in node impurity is less than a certain threshold. Unknown samples can be classified via branching along the tree from top to bottom with respect to the decisions at each node. Then, an unknown sample is assigned to a class with regard to the terminal node it reached.Fisher Linear Discriminant Analysis (FLDA) [15] is a considerably important method in pattern recognition applications. This method differs from other methods since it considers both within-class similarities and between-class differences. Fisher used an LDA that maximizes the rate of between-class scatter over the within-class scatter. Fisher׳s maximization criterion can be defined as(20)J(W)=Tr{(WTSWW)−1(WTSBW)}whereWis the projection matrix, andSWandSBrefer to within-class and between-class scatter matrices, respectively. In this optimization criterion, the dimension of the obtained subspace is one less than the number of classes [21]. The eigenvalues and eigenvectors of(SW−1×SB)are calculated to maximizeJ(W). Then, some eigenvectors corresponding to the largest eigenvalues are chosen. Finally, all data vectors can be represented in a lower-dimensional space by projecting them onto the eigenvectors corresponding to the largest eigenvalues.Support Vector Machine (SVM) aims to obtain a maximum-margin hyperplane in a transformed feature space [35]. It finds the optimal hyperplane, which maximizes the distance between the optimal hyperplane and the nearest sample to this hyperplane [36]. For this reason, SVM is also known as the maximum margin classifier. Support vectors correspond to the data samples that are nearest to the optimal hyperplane [6]. For a two-class approach, the training set is denoted as TS={(x1, L1), (x2, L2), …, (xM, LM)}. There are M data samples in this set, and xi(i=1, 2, …, M) is the data sample. The Li, (Li∈{−1,1}) represents the class label (either a negative or positive class). The class type of the unknown test sample,xtest, is determined by the decision function(21)f(xtest)=∑i=1M{αiLi(xiTxtest)+b}whereαi(i=1, 2, …, M) are the nonzero coefficients that are the solution of the quadratic programming problem,(|b|/||w||)is the orthogonal distance from the optimal hyperplane to the origin andwis the normal vector of the hyperplane. The sign of this decision function gives the label of the class to which the test data (xtest) is assigned. It is possible to construct “S(S−1)/2” classifiers [30] to address multi-class problems with S classes. The SVM classifier is illustrated for a two-class pattern recognition problem inFig. 6. Although SVM works very well with high- and low-dimensional feature vectors, it is disadvantageous due to its slow computational speed.Upon accepting a license agreement, the Image Retrieval in Medical Applications (IRMA) project database [10], which has three different databases, is retrieved. The IRMA project categorizes lesion cases and tissue densities according to Breast Imaging-Reporting and Data System (BI-RADS) classes [26]. The datasets in the IRMA project have 3 different cases of breast cancer: Normal (no cancer), benign and malignant. The following subtitles explicitly explain the properties of the images in the corresponding datasets included in the IRMA project.The Digital Database for Screening Mammography (DDSM) has mammogram images with two different views, Cranio-Caudal (CC) and MedioLateral (ML), for each breast. The resolution of X-ray scanners has an interval of 42–50μm/pixel to shoot the X-ray roentgen photos of breasts. The photos are then converted into lossless JPEG (Joint Pictures Expert Group) format. The metadata information of the images provides different lesion classes according to the American College Radiology (ACR) system. In the preparation of the IRMA project, the patches of normal, benign and malignant cases are extracted from all images and converted into 16-bit Portable Network Graphics (PNG) format. Each patch size is 128×128 in this final format.The IRMA database in the content of the IRMA project includes digital mammogram images of selected patients from the Radiological Diagnosis Department of the University of Aachen [26]. The X-ray roentgen photos of breasts are acquired by a General Electric Senographe with an interval of low beam energies between 26kV and 32kV. The images have metadata information comprising different lesion classes. The patches of these lesion classes are exscinded from the images, and each patch has a size of 128×128.The Mammographic Image Analysis Society (MIAS) database contains mammography images that have a resolution of 50μm/pixel [26]. The images are resized into 1024×1024, which makes their resolution 200μm/pixel. The images are in Portable Gray Map (PGM) format and have additional ground-truth information about right/left breasts, including the class of a lesion and the location of the corresponding lesion. The IRMA project promises the patches of normal, benign and malignant images according to this information. The size of each patch is 128×128.The study of two-class classification includes a separation of normal and abnormal mammogram patches. The entire set is categorized throughout a ground-truth information [10]. Sample patches of this experiment are shown inFig. 7. The first, second and third rows in this figure include five different samples of normal, benign and malignant lesions, respectively. Each patch has a size of (128×128).200 normal and 200 abnormal (benign or malignant) lesion patches are used in this two-class study. Firstly, a histogram equalization procedure is applied on all mammographic patches as a pre-processing step. Then, the rotational- and scale-invariant features are extracted by the HOG, DSIFT, and LCP descriptors for all patches. Finally, a classification is carried out using SVM, k-NN, Decision Tree, and FLDA classifiers via 10-fold cross validation. The flow of the two-class classification system is summarized inFig. 8.The HOG feature extraction algorithm is applied on the mammogram patches with 18 different orientations for a block size of 16×16. The HOG variant utilized for this extraction process is UoCTTI (University of Chicago Toyota Technological Institute) [12], and tensors with (8×8×58) size are obtained for each tissue patch. These tensors are then reshaped into the matrices with a size of (8×464). Six different time-domain features (energy, mean, standard deviation, maximum, skewness, and kurtosis) [16] are extracted from the columns of these matrices so that matrices with a size of (8×6) are attained. Finally, these matrices are converted into column vectors with a dimension of (48×1), and they are used as the feature vectors for mammogram patches.DSIFT feature extraction presents rotation- and scale-invariant features by implementing a differential process on the overlapping image blocks based on selected radius and step size [37]. For a given radius of 5 and a step size of 4, the feature matrices with a size of (128×400) are extracted from the mammographic patches. Six time-domain features are again extracted from the columns of those matrices so that more affordable sized feature matrices (128×6) are attained. These feature matrices are converted into column vectors (768×1), which are used in the classification process.The LCP algorithm is achieved with the neighborhood value of “8” in this manuscript. In other words, the pattern occurrences per angle of 4.5° around each patch were calculated. Thus, the LCP feature extraction technique constructs (81×1) dimensional feature vectors for each mammographic patch. This technique is rotationally invariant because the last element of the vector, which belongs to the most repeated pattern occurrence on the patch, is circularly shifted to the last position on the feature vector. The last element is then discarded to avoid a superimposition effect over the other elements before the classification is performed. Thus, all of the feature vectors are (80×1) in dimension.Four different pattern recognition classifiers are used to obtain the accuracies of the two-class classification study for normal and cancerous cases. SVM classifiers are applied with two different kernels, linear and quadratic. All recognition accuracies are presented, along with their standard deviation values calculated from 10-fold cross validation scores, in theTable 1. The best results (100%) are attained with a Decision Tree and k-NN with five nearest neighbors. In Table 1, SVM(1) is the SVM classifier with a linear kernel, whereas SVM(2) is the SVM classifier with a 2nd degree quadratic kernel. In addition to Table 1, a graph is also drawn to illustrate the average 10-fold cross validation results to show the performance of each technique (Fig. 9).This study presents a classification scheme to construct a Computer Aided Diagnosis (CAD) framework for the three cases of mammogram images, normal, benign, and malignant. The database utilized in this study is retrieved from the IRMA project and includes mammogram patches that are categorized into three classes according to ground-truth information [10]. The dataset consists of 200 normal, 200 benign and 200 malignant patches. A histogram equalization process is again employed on all mammographic patches as a pre-processing step, and HOG, DSIFT, LCP features are extracted from these patches using the respective procedures explicitly explained for the two-class classification study (Subsection 4.2). Next, a 10-fold cross validation is also applied on four different classifiers for recognition performance, but the obtained classification accuracies are not sufficiently satisfied. All accuracies are given, along with their standard deviation values evaluated from 10-fold cross validation, inTable 2. Another recognition accuracy graph is drawn to illustrate the average 10-fold cross validation results for the three-class study (Fig. 10).Because the recognition accuracies of the three-class study are not sufficient, the authors suggest a new framework for the feature extraction phase. In this framework, a Non-Local Means (NLM) filter [9] is first applied to all mammographic patches. NLM is a commonly used filter in medical image processing [24]. The size of an NLM filter window changes depending on the local intensities in an image. The edges remain preserved due to the differentiation over small windows by a weighting function. Assuming thatI(q)is a two-dimensional image, NLM is performed as(22)NLM[I(p)]=∑∀q∈ηw(p,q)I(q)(23)0≤w(p,q)≤1,∑∀q∈ηw(p,q)=1p is the pixel to be filtered, while q is the pixel in a search window neighboring of η. The weighting functionw(p,q)is computed as(24)w(p,q)=1/[Z(p)]e−[(d(p,q)/h2)]Z(p)is the normalization parameter given in Eq. (25)(25)Z(p)=∑∀qe−[(d(p,q)/h2)]where h is the exponential decay control parameter and d is the Euclidian distance of a pixel from the neighboring pixels. In this paper, the NLM filter is chosen to enhance possibly degraded mammographic patches before the feature extraction phase to achieve a more robust classification.Fig. 11 shows how the NLM filter comes up with a result on a malignant lesion patch.Secondly, the feature vectors are extracted using HOG, DSIFT, and LCP methods for all patches after the NLM filtering process. The feature extraction procedures are different than the abovementioned studies, so they are rewritten in the following subsections.Initially, the image enhancement techniques, histogram equalization and NLM, are applied. Then, a mammographic patch with a size of (128×128) is decomposed into four subbands, LL (low–low), LH (low–high), HL (high–low), and HH (high–high), by a one-level Two-Dimensional Discrete Wavelet Transform (2D-DWT) using the variant Daubechies-I [13]. The HOG algorithm is applied on each subband so that the tensors with (4×4×58) size for each subband. These tensors are then reshaped into matrices with a size of (4×232). Six time-domain features are again extracted from the columns of those matrices so that matrices with a size of (4×6) are computed. Finally, these matrices are converted into column vectors (24×1) for each subband. Some weighting coefficients are then multiplied with each subband according to the rule (Eq. (26)) so that feature vectors of patches are constructed.(26)Featurevector=(2×LL)+(2×LH)+(2×HL)The name code for this feature extraction process is ‘HOG 2-1-1-X’ in Table 4.The original mammogram patches (128×128) are again decomposed into 4 subbands by 2D-DWT using the variant Daubechies-I after histogram equalization and NLM are performed. For a given radius of 5 and a step size of 4, the feature matrices (64×200) are extracted for each subband. Six time-domain features are extracted from the columns of those matrices so that the feature matrices (64×6) are computed. These features matrices are converted into vectors (384×1). The same weighting coefficients are multiplied with each subband so that feature vectors of mammogram patches are obtained. The name code for this feature extraction process is ‘DSIFT (r=5)’ in Table 4. There are three different r values (3, 4, and 5) that were compared in the experiments.As was done for the HOG and DSIFT methods, the mammogram patches are decomposed into 4 subbands after histogram equalization and NLM are performed. In this manuscript, the LCP algorithm is performed with the neighborhood value of ‘8’. In other words, the pattern occurrences per angle of 4.5° around each subband were taken into consideration. Thus, the LCP technique returns (81×1) dimensional feature vectors for each wavelet subband. The last element is again discarded in the feature vectors so that all feature vectors are (80×1) in dimension for each subband. Finally, each subband is then multiplied with various different weights, which can be seen inTable 3, to discover their effects on classification accuracy. For the experiment named ‘LCP (histeq)’, the NLM filter is not used.In addition, another flowchart is given inFig. 12 for the proposed feature extraction framework for the three-class classification study to make it easier for the reader to understand the entire proposed framework.Four different pattern recognition classifiers are used to obtain the accuracies of the proposed feature extraction framework for the three-class classification study for normal, benign, and cancerous cases. All recognition accuracies are presented with their standard deviations calculated from 10-fold cross validation scores inTable 4. The best results (90.60%) are attained by SVM classifier with a linear kernel using the feature vectors named ‘LCP 1,4-1’. Moreover, all computational times for each feature extraction method are comparatively presented inTable 5. The unit for these durations is seconds, and each time duration indicates the total elapsed time for the 10-fold cross validation steps. The time durations in Table 5 were measured by using a personal computer which has an Intel Core i7-2640M processor and 4GB Random Access Memory (RAM).

@&#CONCLUSIONS@&#
