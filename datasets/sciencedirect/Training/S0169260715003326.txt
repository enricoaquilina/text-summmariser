@&#MAIN-TITLE@&#
Machine vision application in animal trajectory tracking

@&#HIGHLIGHTS@&#
Five methods used for animal trajectory tracking, which were proposed with the usage of visual system.All the algorithms are proposed for poor-quality images (non-homogenous lightning, presence of grid, etc.).All methods are compared and their effectiveness is evaluated.Proposed methods are compared with standard used methods for animal tracking as GPS tracking or RFID tracking. They bring several advantages such as user-friendly and simple interface appropriate for medical (non-technical) specialists, too.

@&#KEYPHRASES@&#
Animal tracking,Differential methods,Thresholding-based methods,Color matching,

@&#ABSTRACT@&#
This article was motivated by the doctors’ demand to make a technical support in pathologies of gastrointestinal tract research [10], which would be based on machine vision tools. Proposed solution should be less expensive alternative to already existing RF (radio frequency) methods. The objective of whole experiment was to evaluate the amount of animal motion dependent on degree of pathology (gastric ulcer). In the theoretical part of the article, several methods of animal trajectory tracking are presented: two differential methods based on background subtraction, the thresholding methods based on global and local threshold and the last method used for animal tracking was the color matching with a chosen template containing a searched spectrum of colors. The methods were tested offline on five video samples. Each sample contained situation with moving guinea pig locked in a cage under various lighting conditions.

@&#INTRODUCTION@&#
Laboratory animals are widely used in medical research and their importance is undoubtedly significant, especially in various drugs testing. Laboratory animals’ behavior is a determining factor in evaluation of influences and effectiveness of a medical treatment (e.g. animal suffering from any kind of pathology is less vital than healthy one). The effects of different medications are monitored with high interest of researchers. Worldwide-performed experiments search for differences in laboratory animals’ behavior between healthy subjects and those with some kind of disorder or defects. Therefore, it is necessary to use appropriate methods for animals’ monitoring. One of the factors which best describe individuals’ behavior is their ability to move. Thus, the length of the trajectory they passed needs to be tracked and analyzed [14].Nowadays various methods for animal tracking could be used. Wide variety of commercial sensors and Global Positioning Systems (GPS) [8] are useful when tracking objects and larger animals [13], however they are not suitable for applications with laboratory animals due to their minor size and area of movement restricted only to a small cage. Radio frequency identification (RFID) is an appropriate solution for tracking smaller animals in cages. The whole RFID hardware, as well as the control software, can be opportunely tailored for the particular application. Novel RFID-based approaches work on basis of a near-field (NF) RFID multiantenna system, which operates in the ultra-high frequency (UHF) bandwidth. Such system needs to be placed below the animal's cage and then is able to accurately identify the NF RFID tags implanted into laboratory animals [4,5].Although RFID systems work reliably and precise enough, quite high costs belong to their main disadvantages. Consequently, there exist an ambition to develop cheaper methods that would be sufficiently reliable and comparably accurate to RFID. Such methods are based on recording the animal's cage via a video camera and processing the obtained sequence of images [6,11,12]. When the subjects and their background are in contrast, then the objects can be easily differed, for instance simply by background subtraction [18].From the technical point of view, RFID animal tracking used in research mentioned above [10] was only able to decide if the guinea pig was moving or not – number of detected movements was counted. Our proposed methods based on visual system enable not only to detect whether there was a movement in the scene or not, but also to detect animal's trajectory with high precision and measure its length.Low contrast images and those containing considerable amount of noise are the challenging part of this issue. Non-homogenous lighting or presence of upper covering grid of animal's cage could be problematic when occurring in the image, too. The aim of our research was to find appropriate algorithms for processing such poor-quality images. Several methods of laboratory animal tracking using video sequences dealing with real problematic images are presented in this paper. All of them are described, compared and their effectiveness is evaluated by machine vision applications. The approach of software tools utilization offers the opportunity to lower the costs for animals’ tracking research and brings several advantages such as user-friendly and simply understandable evaluation interface, appropriate for medical (non-technical) specialists, too.The article is organized as follows: Section 1 is the introduction into solved problematic, Section 2 in its first part describes the scanned video scene and the other parts offer the brief description of used methodologies together with their mathematical background. Section 3 deals with obtained results and the last part of this study, Section 4 summarizes the applicability of each method for selected video samples.Laboratory animals (guinea pigs) are continually recorded by inspection color camera during the day and night. Additional lighting is needed during the day (white light) and also during the night (IR lighting). Animals are placed in the cages: one animal only for one cage. Each cage is controlled by one camera. Only white guinea pigs are used for experiments. The cage also contains a dark bowl for food, drinking place (made of plastic bottle) and plastic tube in the function of a nest (Fig. 1). The covering grid is not removed from the cage's top to verify the robustness of the motion detection algorithms. The lighting is also non-homogenous through entire scene for the same purposes: to verify the motion detection even if the shadows or light reflections are presented (especially for thresholding-based methods).This paper represents the more recent approach, where the video sequences are continually stored on the storage medium and the algorithms are verified in offline mode. Suitable algorithms will be then chosen, optimized and used in real time for motion detection.We propose several approaches for laboratory animal tracking using digital image processing in this paper. Inputting signal is a video sequence from inspection camera capturing the frames in day and night mode (IR range). Proposed methods can be divided into three basic categories: differential methods, thresholding-based methods and color matching method. Resulting segmented images of video sequences are further enhanced (by additional thresholding and morphology) and trajectory of laboratory animal is constructed from boundary box centroids representing the animal position in each particular image.The main purpose of all mentioned methods is to find selected (and moving) object – laboratory animal in a video sequence. Such methods of motion detection in a video can be divided into several types (based on different approaches): methods, which are based on detection of the changes in the scene (more video frames needed), thresholding methods (only one frame is needed in particular time) and a method of object's detection based on its features. Selected methods described in this article represent each group of tracking methods mentioned above. Results for every method are evaluated and compared in conclusion part.The first category (differential methods) integrates basic and advanced methods for motion detection using frame subtraction or static background removal [1]. Detection of laboratory animal motion can be defined as a change in position of the animal relative to its surroundings. Area with significant motion (changes) between adjacent video frames is represented in a resulting image as an area with higher intensity levels. Such an area is then transformed to the binary object in order to calculate the centroid (position of the animal). There is only one single animal in the cage (scene) and the background is static. This fact brings a tremendous advantage for this method, which means that the resulting differential image is composed from one huge object representing animal and isolated small particles generated by noise processes. These small particles can be removed by morphological operators.The simplest differential method (in results marked as Differential method – 2) is based on subtraction of following frames in the video sequence or subtracting a scene without animal (background reference) from each video frame. Using of such method anticipates processing of video sequences captured by a stationary video camera with constant lighting source. Method of background reference subtracting is suitable only for shorter analysis, because in the longer periods this reference can change (the bowl for food can be replaced by the animal or the food can be added to the bowl). The principle is relatively simple and is based on a comparison of corresponding pixels between two sequential images (Fig. 2). Arithmetic subtraction of two following images follows the formula:d(i;j)=fn(i;j)−fn+1(i;j)where d(i; j) is a pixel of resulting image, fnand fn+1 are following frames of video sequence. However, negative pixel values can be produced by this formula, so we can use absolute difference operator:d(i;j)=|fn(i;j)−fn+1(i;j)|where intensity value in differential image correspond to the absolute change between coinciding pixels in following frames.This method is useful in clearly distinguishable object with respect to the background. By differential image d(i; j) a presence of motion in a video sequence can be detected. However, the information about the direction of detected movement cannot be obtained from differential image. If such information is required, thus the cumulative differential image is more appropriate to be used. The cumulative differential image can be obtained by performing the difference of the reference image with all the other images in the sequence. The resulting cumulative image is then given by sum of all differential images and can be described as follows:di;j=∑l=1nal⋅dlwhere alrepresents the weight of each image, n is the number of frames and dlrepresents the corresponding differential image.The algorithm of background subtraction uses two inputs – background image (static, not moving part of the scene) and video. A current frame is subtracted from the background model and it is determined which changes belong to the moving object for each frame. The background model is simultaneously updated in parts of the scene where no movement occurs. Used algorithm can be described in following steps:1.Create background model from the whole video or part of the video. There are many ways how to create such model. On a basic level, it can be simple average of all frames – each pixel in background model corresponds to average value of that pixel in all video frames. More advanced methods are able to extract static background from whole video or from sequence of last n-frames [18,17].Compute the difference image by subtracting current video frame from background model (Fig. 3a). After the subtraction, the difference image is scaled down by factor 4 and then it is scaled back to its former size in order to remove unnecessary particles from the image. The difference image can be also blurred to remove even more unnecessary details.Threshold the difference image by Otsu method [16]. After thresholding, small details are filtered out using erosion and dilation (Fig. 3b). If total number of white pixels in the image is greater than MAX value or lower than MIN value, it is considered as false detection and the current frame is discarded. Position coordinates of the object in this frame are then set to predefined value or value from previous frame.Detect contours in binary image, filter possible animal positions. As animal moves, it may change the structure of background surface (Fig. 4a). In such case, the difference image will contain multiple areas where animal can be, as shown in Fig. 4b. Purple dot corresponds to animal position, green dot is the center of changed background. To detect closed areas in binary image, a contour detection provided by OpenCV library was used. Also many other strategies to choose the area which truly corresponds to moving animal could be used. Possible criteria are areas shape, size or weight of the area – average value of all pixels in the area. In this experiment, choice of area, which contains global maximum from difference image, appeared to be sufficient (Fig. 4c – maximum highlighted by red dot).In image processing lots of other combinations of subtracting two images are known. Some of the differential methods are quite simple and usually they are sensitive to the noise and ghosting [19]. Method of Collins et al. [7] was chosen (in results marked as Differential method – 1), because this method erases the ghosting effect. Ghosting effect appears when receiving a distorted or multipath input signal and this happens when subtracting two images. Collins et al. uses three images, which are called previous frame (PF), current frame (CF) and next frame. Collins method subtracts the previous and next image and after that it subtracts the current and next image. Then the logical operation AND between the both partial results is performed:DF=(PF−NF)&(CF−NF)and differential image (DF) is further thresholded to obtain a binary image. Significant changes in the image will be indicated by white dots in the image. When motion is detected (white dots appear), minimal and maximal positions are used to compute the bounding rectangle. Then the centroid of bounding rectangle can be determined. By the comparison of two consequent centroids, the motion of animal can be expressed by the amount of pixels. If scene consists of only single animal, this method has a high efficiency (Fig. 5).However, the method generates lots of false positive small objects due to noise processes and also in cases when the wrong threshold has been chosen. Finding the right threshold is crucial and motion detection algorithms select a dynamic or adaptive threshold. A dynamic threshold can still fail and give false positives either. Although robust motion detection algorithms tackle these problems with ease, they need a lot of extra CPU power to succeed which is not always available.Moreover, there is a simple trick how to erase the false positives. This trick assumes that the motion only occur in a sequence of images larger than one. Parameter, which can be used to neglect false positives, is the standard deviation. The standard deviation informs us about the distribution of motion. When motion is specific at a single pixel, the standard deviation will be near to zero. When a lot of motion is detected and it is distributed over the entire image, then the standard deviation will be very high. However, huge distribution mostly indicates no real motion. Used algorithm consists of following steps:1.Create the sequence of images: previous, current and next frame of video sequence. Actually, the next frame is currently captured frame from the video sequence. Current frame is the last frame from the previous sequence, etc.Apply method of Collins et al. with the application of Differential images method.Choose appropriate threshold and operations of mathematical morphology (neglect the false positives). Threshold is used for indication of changes in the image and therefore any changes with the nature of noise must be suppressed. Usually binary threshold with the result as white dots is used:d(i;j)=255d(i,j)>thresh0otherwiseThe determination of threshold value depends mostly on the light conditions. However, in this case, the light conditions are constant and that is why the constant intensity value (equal to 35) was used.Evaluate the number of changes.Neglect the high motion on the basis of standard deviation, which is calculated:mean=∑i,jd(i;j)Nstdev(i;j)=∑i,j(d(i;j)−mean)2Nwhere N is the number of pixels in image. When motion is specific at a single point the stdev will be near to zero. When a lot of motion is detected and it is distributed over the entire image, the stdev will be high. This indicates no real motion and it can be caused by noise or false measurement. That is why there is another threshold for the elimination of such high motion. This threshold for maximal deviation was determined empirically to value 20.Determine the bounding rectangle of the motion and the position of rectangle centroid.Compare the position of centroid with the position of previous one and calculate the difference in pixels.Add the result to the total amount of motion.Go to the step 1 until entire video sequence is processed.Compared to differential methods, thresholding-based methods do not use the subtraction of neighboring frames in the video sequence but they work with frames separately. These methods suppose that the object is well separable from background (because of color or intensity). To make the methods more accurate, we tested threshold algorithms with interactive ROI (Region of Interest) builder, which constricts the areas where the animal could not be (Fig. 6). With this constriction, we neglected false positive segmentations typically caused by light reflections on the items on the scene. Such items, e.g. drinking place and cage side barriers, also represents obstacles in animal's motion.First, we converted RGB frames to HSL model. HSL or related models are often used in computer vision and image analysis for feature detection or image segmentation. The applications of such tools include object detection for instance in robot vision, object recognition and analysis of medical images. Each color component is separately passed through the same algorithm, because the features of interest can be distinguished in used color dimensions [2].HSL is a simple transformation of RGB that preserve symmetries in the RGB cube unrelated to human perception, such that its R, G, and B corners are equidistant from the neutral axis, and equally spaced around it (Fig. 7).After this conversion step, color thresholding or single threshold taken on 8-bit image layer could be used.In order to threshold a monochromatic 8-bit image (image layer) many approaches based on statistical analysis of image histogram can be used. Finding the threshold value is then changed to maximize or minimize the key parameter describing the relations between or inside the regions in histogram corresponding with object of interest and background. Many of discussed methods come from minimizing the total miss-classifying error of pixels belonging to background or objects. Many methods for global thresholding are known, such as Clustering, Metric, Moments, Interclass Variance (Otsu Method) or Entropy. In the case of non-homogenous lighting, there are also some methods for local thresholding: Niblack algorithm or Background Correction algorithm [9].Global thresholding based on entropy (Entropy method) is a method where the threshold value is obtained by applying information theory to the histogram data (Fig. 8). In information theory, the entropy of the histogram signifies the amount of information associated with the histogram [3]. Letp(i)=n(i)∑i=0N−1n(i)represent the probability of occurrence of the gray level i in image. The entropy of a histogram of an image with gray levels in the range [0; N−1] is given by:H=−∑i=0N−1p(i)log2p(i)If k is the value of the threshold, then the two entropiesHb=−∑i=0kPb(i)⋅log2Pb(i)Hw=−∑i=k+1N−1Pw(i)⋅log2Pw(i)represent the measures of the entropy (information) associated with the black and white pixels in the image after thresholding. Pb(i) is the probability of the background and Pw(i) is the probability of the object.The optimal threshold value is gray level k that maximizes the entropy in the thresholded image given byHb+HwSimplified, the threshold value is the pixel value k at which the following expression is maximized:−1∑i=0kn(i)∑i=0klog2[n(i)+1]n(i)−1∑i=k+1N−1n(i)∑i=k+1N−1log2[n(i)+1]n(i)+log2∑i=0kn(i)∑i=k+1N−1n(i)Local thresholding, also known as locally adaptive thresholding, is similar to global grayscale thresholding as both create a binary image by segmenting a grayscale image into a particle region and a background region. Unlike global grayscale thresholding, which categorizes a pixel as part of a particle or the background based on a single threshold value derived from the intensity statistics of the entire image, local thresholding categorizes a pixel based on the intensity statistics of its neighboring pixels.Non-uniform lighting changes, such as those resulting from a strong illumination gradient or shadows, often make global thresholding ineffective [15].The local threshoding algorithm calculates local pixel intensity statistics – such as range, variance, surface fitting parameters, or their logical combinations – for each pixel in an inputting image. The result of this calculation is the local threshold value for the pixel under consideration. The algorithm compares the original intensity value of the pixel under consideration to its local threshold value and determines whether the pixel belongs to a particle or to the background.Local thresholding algorithms use standard window of 32×32 pixels to determine statistical parameters of local image area. Primarily, non-optimized algorithms of local thresholding require a large amount of computational time, so they are not suitable for real-time processing. However, many development systems (such as IMAQ library in LabVIEW) use optimized versions of algorithms non-independent on the window size.Background correction algorithm combines local and global thresholding concepts for image segmentation (Fig. 9). Background corrected images B(i; j) are computed as:B(i;j)=I(i;j)−m(i;j)where I(i; j) is an actual image pixel at position (i; j) and m(i; j) is a local mean in image window.Background corrected image is then thresholded by Interclass Variance method (Fig. 10).In Niblack algorithm, local threshold T(i; j) for actual image window is computed as:T(i;j)=m(i;j)+k⋅σ(i;j)where m(i; j) is local mean, σ(i; j) is local variance and k is deviation factor. All the pixels in actual window with values higher or equal to T(i; j) are considered as particle. The Niblack algorithm is sensitive to the window size and produces noisy segmentation results in areas of the image with a large, uniform background. To solve this problem, the NI Vision local thresholding function (LabVIEW IMAQ library) computes a deviation factor that the algorithm used to categorize the pixels correctly.The color spectrum represents the 3D color information associated with an image or a region of an image in the HSL space. If the input image is in RGB format, the image is first converted to HSL format and the color spectrum is computed from the HSL space. Colors represented in the HSL model space are easy for humans to quantify. However, the hue and saturation planes cannot be used to represent the black and white colors that often comprise the background colors in many machine vision applications.The generating of color spectrum is done by this procedure. Each element in the color spectrum array corresponds to a bin of colors in the HSL space. The last two elements of the array represent black and white colors, respectively. Fig. 11illustrates how the HSL color space is divided into bins. The hue space is divided into a number of equal sectors, and each sector is further divided into two parts: one part representing high saturation values and another part representing low saturation values. Each of these parts corresponds to a color bin – an element in the color spectrum array.The color sensitivity parameter determines the number of sectors the hue space is divided into. Fig. 11a shows the hue color space when luminance is equal to 128. Fig. 11b shows the hue space divided into a number of sectors, depending on the desired color sensitivity. Fig. 11c shows each sector divided further into a high saturation bin and a low saturation bin. The saturation threshold determines the radius of the inner circle that separates each sector into bins [15].Fig. 12shows the correspondence between the color spectrum elements and the bins in the color space. The first element in the color spectrum array represents the high saturation part in the first sector, the second element represents the low saturation part, the third element the high saturation part of the second sector, etc. If there are n bins in the color space, then the color spectrum array contains n+2 elements. The last two components in the color spectrum represent the black and white color, respectively.The value of each array element in the color spectrum indicates the percentage of image pixels in each color bin. When the number of bins is set according to the color sensitivity parameter, the machine vision software scans the image, counts the number of pixels that fall into each bin, and stores the ratio of the count and total number of pixels in the image in the appropriate element within the color spectrum array.The value of array element corresponds to the percentage of pixels in the image that fall into the corresponding bin. The color spectrum contains useful information about the color distribution in the image. The color spectrum gets information such as the most dominant color in the image, which is the element with the highest value in the color spectrum. The color spectrum array also can be used to directly analyze the color distribution and for color matching applications.Color matching can be used to compare the color content of an image or regions within an image to a reference color information. If the color matching method is used, the reference image must be used for color information comparison. The color information in the image may consist of one or more colors. The algorithm then learns the color information in the image and represents this information as a color spectrum. Algorithm compares the color information in the entire image or regions in the image to the learned color spectrum, calculating a score for each region. The score relates how closely the color information in the image region matches the information represented by the color spectrum.Color matching is performed in two steps. In the first step, the algorithm learns a reference color distribution. In the second step, the algorithm compares color information from other images to the reference image and returns a score as an indicator of similarity. The algorithm learns a color distribution by generating a color spectrum. The color spectrum becomes the basis of comparison during the matching phase.During the matching phase, the color spectrum obtained from the target image or region in the target image is compared to the reference color spectrum taken during the learning step. A match score is computed based on the similarity between these two color spectrums using the Manhattan distance between two vectors. A fuzzy membership weighting function is applied to both the color spectrums before computing the distance between them. The weighting function compensates for some errors that may occur during the binning process in the color space. The fuzzy color comparison approach provides a robust and accurate quantitative match score. The match score, ranging from 0 to 1000, defines the similarity between the color spectrums. A score of zero represents no similarity between the color spectrums, whereas a score of 1000 represents a perfect match. Fig. 13illustrates the comparison process.

@&#CONCLUSIONS@&#
