@&#MAIN-TITLE@&#
From pose to activity: Surveying datasets and introducing CONVERSE

@&#HIGHLIGHTS@&#
We provide a comprehensive review of current appearance and pose modality action recognition datasets.Analysis shows pose based HAR requires data for the subtle interaction problem.The CONVERSE dataset of pose based conversational interactions is presented.CONVERSE presents interactions that are not semantically defined by pose.Benchmark results are provided for use of CONVERSE in pose HAR method evaluation.

@&#KEYPHRASES@&#
Human pose,Action,Interaction,Activity,Dataset,

@&#ABSTRACT@&#
We present a review on the current state of publicly available datasets within the human action recognition community; highlighting the revival of pose based methods and recent progress of understanding person–person interaction modeling. We categorize datasets regarding several key properties for usage as a benchmark dataset; including the number of class labels, ground truths provided, and application domain they occupy. We also consider the level of abstraction of each dataset; grouping those that present actions, interactions and higher level semantic activities. The survey identifies key appearance and pose based datasets, noting a tendency for simplistic, emphasized, or scripted action classes that are often readily definable by a stable collection of sub-action gestures. There is a clear lack of datasets that provide closely related actions, those that are not implicitly identified via a series of poses and gestures, but rather a dynamic set of interactions. We therefore propose a novel dataset that represents complex conversational interactions between two individuals via 3D pose. 8 pairwise interactions describing 7 separate conversation based scenarios were collected using two Kinect depth sensors. The intention is to provide events that are constructed from numerous primitive actions, interactions and motions, over a period of time; providing a set of subtle action classes that are more representative of the real world, and a challenge to currently developed recognition methodologies. We believe this is among one of the first datasets devoted to conversational interaction classification using 3D pose features and the attributed papers show this task is indeed possible. The full dataset is made publicly available to the research community at [1].

@&#INTRODUCTION@&#
Recent advances in human motion capture and action recognition have a range of applications including surveillance, synthesis of computer generated imagery, and human–computer interfaces . Despite this progress there are still several problems that require solving, including the understanding of complex classes and maintaining accuracy rates on significantly large datasets. The field has moved fluidly between the use of both appearance and pose based features since its conception, with datasets being produced for both modalities that can be used for cross comparison between developed methods. The release of a commercial depth sensor has revived the use of pose based features in recent years, however the datasets have yet to represent the complexity of classes that are provided by appearance based sets. We therefore intend to highlight datasets within the field and then introduce the proposed dataset to build on the current state.In 1973 and 1975, the authors in [2,3] presented a model for the representation of the human form that closely followed the biological interpretation of human movement, the human skeleton representational model, based in Gestalt principles that provide key interest points in the movement. Model representations were then expanded by the authors in [4–9] to develop systems that are able to identify human walking actions. A review of the field was reported in [10], focusing on the recognition of the articulated movement by the human body and acknowledging the benefit of a priori shape models in Human Action Recognition (HAR). Campbell and Bobick [11] used 3D coordinates of 14 joints to perform event recognition from a continuous sequence of ballet moves. In following years the use of pose estimation was reduced in favor of video sequence analysis, due in part to their ease of acquisition and relatively lower cost compared to the use of marker capture systems at the time. Aggarwal and Cai [12] formed another review of the field, discussing the use of both body part representation and the global motion of the body, recognizing the need for accurate tracking of body parts when undertaking 3D estimation from 2D projections, noting the difficulty in estimating the position of joints in the scene when using appearance based pose extraction methods. The review then draws light on the use of tracking motion without needing to directly identify body parts; making use of image processing methods for appearance based tracking such as bounding box locality [13] and mesh features [14–18]. This use of motion lead to the use of appearance features in the recognition of activities, with use of image features including motion fields [19,20], motion histories [20] and space-time interest points [21–23]. Around 2004/2005 the KTH and Weizmann action recognition datasets were publicly released to the field, providing a collection of sequences with which to evaluate developed methodologies [22,24]. Despite their huge success as a comparison dataset, both sets were representative of the time of their release, containing single camera recordings of individual subjects performing discrete actions. Since the release of the KTH and Weizmann action sets, recent appearance based HAR has moved towards understanding complex interactions between multiple individuals. Contextual understanding of the scene as a whole has been explored in recent years, with Choi et al. [25] utilizing the behaviors of multiple subjects in the scene to help obtain accurate classification of a given individual’s action. Further appearance datasets are reviewed in [26] with identification of sets that provide classes for specific domains and describing complex scenarios; including meta-source sets, multiview recordings, and repositories of long observations.The uses of low-level and high-level features in HAR have been established. Low-level features typically limit recognition of actions and interactions to those of distinct or exaggerated classes which can be distinguished via strong spatio-temporal gestures or poses; such as the jumping jack, handshake and high-five. The use of higher level temporal tracking can often out-perform low-level features in HAR, and Yao et al. [27] suggested the consideration of 3D pose features as a benefit over lower level appearance features, acknowledging previous difficulties in obtaining accurate 3D pose features. The recent advances within pose capture and estimation methodologies has helped to reduce the difficulty in collecting 3D human pose from an observed scene, thus increasing the prominence of 3D pose in HAR. Various features have been developed from the body pose domain, including joint–joint/joint–plane distances, motion velocities, and histograms of joint orientations [27–29]. Recent work has moved into the application of fusing multiple modalities for recognition, with particular highlight on the benefit of audio–visual fusion [30–32]. With this resurgence of 3D pose it is worthwhile reviewing the datasets that are available to the HAR community in order to facilitate comparable evaluation of research methods. Discussing these datasets in terms of their reflectance of real world scenarios and ability to provide challenges to a rapidly moving field highlights the difference between the appearance and pose based areas of the community, with challenges that have been explored in the image modality being relatively untouched in the pose domain.Methods in classifying individual actions have been well studied in both the image processing and depth based methods. Relatively simplistic pose rich actions such as waving, walking and clapping have been the focus of research for decades, with numerous datasets providing standard benchmarks with which evaluate the performance of new methodologies. HAR has often focused on the analysis of spatio-temporal features that are extracted from data collected in the raw domain. Schuldt et al. [22] make use of local space-time features to identify key interest points of motion; these points are then used to develop a vocabulary of action primitives that train a Support Vector Machine (SVM) classifier. Blank et al. [24] presented the action event as an XYT volume, extracting local saliency and orientation combined with global space-time features to perform spectral clustering based classification. Methods designed for action representation, segmentation and recognition via appearance information has been reported in [33], identifying the spatial features, temporal model, temporal segmentation, and view invariance provided by each method for appearance based recognition. For pose based recognition the depth sensor has become an efficient method of tracking and extracting human skeletal model representations of subjects during experimental recordings [34–36]. This has led to a renaissance in pose estimation techniques [27,37–39], and to the production of numerous public datasets for pose estimation method validation [37,40,41]. In addition, many recognition methods have been developed which are more generic in their ability to use both appearance and skeletal model derived features, focusing on the learning of similar representative sub-action primitives, which are then verified using both appearance and skeletal features [42–45].Over the development of the field some main problems have revealed themselves, namely, variation in execution style and appearance. Appearance effects are reduced by considering the individual as a human skeleton model, removing all external stimuli except for pose articulation. Despite the benefit of removing anecdotal image domain information by considering pose, it is argued that this lack of appearance data may remove higher level contextual information [27]. Temporal execution variation has a large impact on the ability to recognize events, not only in execution speed but also the order in which action primitives are executed. Some actions can be subject to more variation than others; some even have a definitive order in which primitives must be executed in order to fulfill the higher level contextual semantics of the action, often described as a sequence of key poses [36,46–48]. Execution length variance became a large part of the HAR problem, with actions being executed at differing speeds. As such, Dynamic Time Warping (DTW) has been used to align two sequences of actions, adapted from [49] in [50,51]. This method of sequence alignment has since been used to compare sequences of differing execution length [44,52–54]; however the use of DTW has also been criticized, especially when aligning highly periodic actions [41] or actions where the time taken to execute of a key feature, such as walking and running [48]. Exemplar based methods make use of key poses almost as a series of checkpoints frames which make up an action, and therefore are believed to not require a time warping alignment phase [55,56]. These developed methodologies seem to provide reliable accuracy for the publicly available datasets on which they are often validated, despite their variance in execution rates and styles.Another issue in the community is the lack of methods which are able to extend beyond the recognition of simplistic action classes. Weinland et al. [33] reports upon the predictive accuracy of methods that are evaluated on the KTH, Weizmann and IXMAS datasets; showing that in recent years the level of accuracy can often reach over 90%. State of the art performance accuracy is also reported within [57], with older datasets often reporting the highest number of correct classifications. Hassner [57] also shows that those datasets which are more representative of real world observations tend to challenge the current methods within the community; such as Hollywood1/2, HMDB51 and Olympic Sports. This suggests that current HAR methods are able to easily classify the relatively simplistic classes presented in established datasets, but that the community requires challenging with complex scenarios.This study aims to first consider a large selection of the current datasets that are available for human action recognition, evaluating properties that facilitate comparable testing of developed HAR methods. The survey identifies the growth of the field from consideration of generic emphasized actions towards the understanding of interactions between numerous individuals. Datasets are analyzed based on a variety of key properties that influences their use for various HAR techniques, including number of action classes, complexity of events and their application domain. Differing levels of abstraction within the understanding of human behavior are described, detailing the nature between pose, gesture, action, interaction, and activity. Despite the progression of the field towards higher levels of behavior abstraction there is still need for a dataset that provides interaction classes that contain complex person to person activities, representing actions and interactions that are not readily identifiable by the presence of a given gesture or pose. The second part of this study then aims to help occupy this gap in the community with a dataset describing subtle conversational interaction classes. CONVERSE provides a collection of interactions in which the activity develops over a long period of time, with realistic representations of behaviors that have contextual differences that are difficult to define by motion.The rest of this paper will outline the current state of data available to the community and outline the requirement for a novel conversational set. In Section 2 we present the evaluation of a plethora of available datasets, evaluating each one based on the provided set of criteria and highlighting the need for a dataset which introduces subtle interactions between individuals. Section 3 then describes the surveyed sets, providing key information regarding their composition and usage. Section 4 then draws on these findings to present our novel dataset, describing the composition of the data and its usage in HAR, and providing a baseline set of classification results for comparison.Numerous HAR datasets have been produced and publicly released in the last decade for the purpose of detecting and identifying action events in an observed scene. Many of these sets have the added benefit of allowing cross-verification of methodologies developed in the field of computer vision, specifically those of action detection and classification. Available datasets contain a variety of traits which require consideration when deciding upon their appropriate usage. Sets differ in the data collection modality, including RGB videos, depth maps, accelerometers and marker based motion capture. They also differ in the actions carried out, including simple gestures, discrete actions, and continuous sequences of actions, multi-user interactions and person–object interactions. Some datasets make use of original data collection, allowing a degree of control over certain parameters within the data collection methodologies. Others use meta-data collected from video clips that are publicly available from media such as films and online video clips; these tend to have large amounts of variation between individual sequences, however they are also among the largest of the datasets, with some meta-sets containing thousands of sequences [87,131]. Numerous sets have ground truth labels for an entire sequence; however many are either manually segmented out of a continuous sequence of multiple actions, or are left for users to perform labeling before their use. Ground truth labeling on a frame-by-frame basis is rare, due to the complexity in determining the exact frame at which an action begins.Datasets, such as KTH, Weizmann and MSR Action3D [22,24,40], provide the common examples of well annotated and discrete action executions, including kicking, walking, and shaking. Others, such as the CMU Motion Capture set [74], expand the complexity further by containing sequences of multiple actions executed in a continuous manner. Recently, sets have moved towards recognizing interaction between two people, including SBU Kinect, BIT-Interaction and K3HI [37,64,101]; however, these sets still provide interactions using the classic simplistic actions of pushing, punching and kicking. A few studies, including MSR DailyActivity3D and the TUM Kitchen [41,129], have made steps towards the recognition of so-called ‘daily activities’, natural actions which may be more representative of the real world executions.Despite this abundance of datasets, there is still a lack of sets that make use of subtle interaction classes, representing loosely defined actions such as those in natural conversational styles, or in context dependent situations. With [76,77], we have presented methodology, using a dataset of subtle conversational interactions, which is able to classify such subtle action events, based upon 3D pose features.The following section will evaluate the public datasets detailed within Section 3 and summarized in Table 1, identifying key features for their usage in the HAR community. Several parameters that require consideration when developing and evaluating action recognition methodologies using publicly available data are identified; including the modality of data acquisition, data provided by the set, and consistent training and testing subsets. The complexity of each dataset is also evaluated, based upon the number of individual classes they present, the number of samples provided, and the presence of complex and realistic class scenarios. Summaries are provided in Tables 1, 2, and 4–9. The proposed CONVERSE dataset [1] is included within the evaluations to highlight the necessity for such a set and identify where it resides amongst the currently available data. A detailed explanation of the proposed dataset is given in Section 4.In Table 2 we cluster the datasets based on their method of data capture; from video, depth maps, skeletal tracking, Motion Capture (MoCap) marker tracking, IMU, and audio. The majority of sets in HAR make use of vision; however recent progress has been made towards the use of 3D pose estimation via depth sensors; therefore understanding the modality provided by a dataset will often impact on the choice of features used to describe each sequence.Appearance based HAR makes use of datasets that are often collected via still images or video, as cameras can provide a relatively cost effective method of obtaining both real-world and staged execution samples from both a laboratory or real-world environment. In Table 1 it can be seen that all of the datasets presented contain some form of video or appearance based data (except CMU MoCap, K3HI and UCF iPhone); therefore in Table 2 we omit the video data. The quality of the recordings varies greatly between sets, with some specializing in evaluating action detection and recognition in low quality or small scale recordings. High intra-set and inter-sequence variation in image quality, camera motion, scale and viewpoint are common in meta-data sets that collect observations from multiple sources, such as UCF101, UCF50, UCF11, Hollywood, Hollywood-2 and HMDB51, and these pose a more realistic problem to the community. Visual based HAR can provide an intuitive representation of the scene; however there can often be superfluous information contained within an observation that negatively impacts on the reliable global recognition of a given action; therefore, appearance based modalities can often make use of subject localization and background removal, coupled with the extraction of descriptors such as Space-Time Interest Point (STIP)s, Histogram of Oriented Gradients (HOG), Histograms of Optical Flow (HOF) or local regions of motion features to enable the global recognition of actions regardless of background information or subject-specific appearance. Many depth based datasets also provide simultaneously captured video representations of their data; this appearance data can either be omitted from the learning, or combined to form a multi-modal system. Of the appearance based datasets, the KTH and Weizmann datasets have been cited the most for single action recognition method evaluation. For appearance based interaction recognition the CAVIAR, Hollywood and UT Interaction datasets have been used frequently by the community.Motion capture concerns the recording of numerous markers placed upon the body by multi-camera systems, providing accurate tracking of the markers within a volume over time. MoCap often provides a method of capturing a spatial ground truth for the marker locations within the scene, being used as a stand-alone modality or augmenting datasets captured through other methods. MoCap systems are often calibrated using built in software and a calibration tool, allowing all cameras to be spatially and temporally synchronized, increasing confidence in the marker tracking. Placement of the markers varies between datasets and as such datasets which make use of MoCap provide details of the marker placement on the body, allowing semantic affordance to be applied to each marker. MoCap can be seen as a cost-expensive method of data collection, often requiring dedicated systems; however the generation of a special ground truth and reliable pose tracking method is of great benefit when developing pose from appearance or pose based action recognition methodologies. Despite this, an implementation of marker based MoCap systems in a real world environment is impractical, requiring individuals to wear a motion capture suit to be detected by the system would provide little benefit to the user; as such there has been some effort that has also been made to produce human skeletal tracking without the use of markers from simple RGB image recording [129] and from depth maps [40].Of the HAR datasets that utilize MoCap, the HumanEVA, Berkeley Multimodal Human Action Database (MHAD) and Carnegie Mellon University (CMU) MoCap datasets are most commonly used. The HumanEVA dataset provides a set of evaluation metrics for the purpose of action recognition, Berkeley MHAD provides a detailed dataset containing multiple modalities for fusion based action recognition, and the CMU MoCap dataset contains a vast number of continuous sequences which can be used for action detection and sequence segmentation.The production of a consumer level depth sensor, most notably the Microsoft Kinect, coupled with efficient and accurate joint tracking software has provided the HAR community with an inexpensive method of collecting 3D poses of a subject performing actions within a scene [34–36]. This has allowed for the development of methods that represent the action as a series of key poses or bag of words model [46,47,77], extracting the key frames that describe the overall action event. Datasets such as 50 Salads, Berkeley MHAD, CAD120, CAD60, G3D, G3Di, K3HI, LIRIS, MSR Action3D, MSR DA3D, MSR Gesture3D, and SBU Kinect Interaction all make use of the Kinect depth sensor to collect data providing the depth map of the scene. The Hollywood3D set utilizes commercial films that have been recorded using a 3D stereo camera system to provide depth maps. By obtaining a 3D pose estimation of the subjects within the scene users are able to, given accurate tracking, generate pose, scale, and appearance invariant features for the purpose of HAR that include joint trajectories, joint–joint distances, joint–plane distances, and joint motion histories. Many of the depth datasets captured using the Kinect provide the associated estimated skeleton representation of the individual, tracking a number of joints across the scene. The number of joints tracked and the position of the provided markers often depends on the method used to extract the skeleton; those using the Microsoft Kinect SDK often provide 20 points, whilst those using the OpenNI standard track 15 joints on the body. The selection of joints often aligns with the major joints of the human body, and so provides an estimation of limb motion. Currently the use of depth sensors are limited to a viewpoint that is in a roughly front-on position due to the method of estimating depth, using distortions of infra-red projections into the scene which is then captured by a receiving sensor. This method has little ability to handle scene occlusions which can cause shadowed regions in the depth map, resulting in lost or noisy tracking in the extracted skeletons.The most prominent depth datasets for single person actions include those presented by the Microsoft Research group, namely the Action3D and DA3D datasets. Despite the small number of samples and action classes provided by the MSR Action3D dataset there has been a vast number of citations for its use as an evaluation dataset. For person–person interactions there are few datasets available which make use of depth based data; the K3HI and SBU Kinect Interaction datasets provide sequences of single executions of a given interaction, analogous to those provided by the BIT Interaction and UT Interaction appearance datasets; however their recent release may reflect their low citation and usage for evaluation of pose based methods.Various other methods of data capture have been used for HAR purposes, including the use of audio recordings [30,151] and IMUs [30,152,153]. These methods can provide reasonable classification results on their own, however they are often used in a multi-modality system to improve the accuracy rates of single modality methods. These datasets are beyond the scope of this survey and omitted for brevity.Human behaviors are often a set of events with differing levels of abstraction and complexity; therefore to aid comparison between HAR class types we shall first define assumptions made about terminology we wish to use. Many class labels provided within HAR datasets can often be re-labeled to fit within a different level of abstraction; however we attempt to use common terminology found across the community, with an overview provided in Fig. 1and a summary of the datasets in Table 4. Example images from datasets that describe differing levels of abstraction are given in Table 3.Pose: An atomic observation of the spatial arrangement of a human body at a single temporal instance, e.g. ‘Arm above head’.Gesture: A temporal series of poses on a sub-action scale, sometimes described as action primitives e.g. ‘Arm moves left’.Action: A series of gestures which form a contextual event, e.g. repeated gestures of arm moving left and then right can be contextual described as an ‘overhead wave action’. These are the most commonly used class labels found within current datasets, describing single actions executed by a subject including ‘run’, ‘jump’, and ‘wave’.Interaction: A pairwise or reciprocal action is committed by two entities on each other. Each entity therefore has a single action that reflects its state compared to the other entity, i.e. consider the action of person A shaking the hand of person B; A executes the action of shaking the hand of B, B executes the action of having their hand shaken by A, together this pairwise action execution can be described as that of a ‘handshake’ interaction. For the purpose of action recognition interactions are often further divided into differing interaction types based on if the entities include people, objects or groups. For this study we have omitted group interaction datasets due to space limitations.Person-person: An action is committed directly by one individual upon another. This definition does not include crowded scenes in which an individual performs a single person action with other subjects in the environment. The class labels in a P–P interaction treats the interaction as a single entity, rather than two separate single person actions, e.g. we consider the class ‘punching’ as an interaction between person A, the puncher, and person B, the individual being punched.Person-object: An action is committed directly by one individual upon an object. This includes the manipulation of objects. We consider class labels such as ‘lift chair’ and ‘open box’ as person–object interactions as the actions ‘lift’ and ‘open’ are performed on the objects ‘chair’ and ‘box’ respectively.Groups: Characterized as interactions carried out between a collected entity of more than two individuals. Group interactions can include inter- and intra-group behaviors and the interaction of the group on other objects, individuals, or even other groups. These often form their own subsets of group behaviors.Activity:A collection of actions and/or interactions that compound to describe a high level event. These are common within the sets that describe daily behaviors, e.g. ‘cook a meal’ and ‘tidy room’ can often include numerous actions and interactions that are executed. Each action and interaction can therefore be thought of a sub-activity event in such scenarios. Activity is also used to describe the daily activities, a more realistic observation execution than the exaggerated instances such as ‘punch’ and ‘kick’.The choice of classes that are performed by the actors is a key motivation in the generation and usage of the proposed dataset. Often the actions executed are those of a visually definable nature, comprising single executions of a discrete action which contain key poses and gestures. The complexity of the problem can then be increased by observing multiple executions of actions in a sequence, either with distinct boundaries between the classes or with a natural flow between different classes. These are all complex issues that are the focus of the community, with segmentation methods often utilized to separate out actions from a continuous sequence. Judging the difference in complexity between two classes can be subjective, depending upon the subtlety of gestures, the context of any interactions, and the spatio-temporal rigidity of the executions; subtle gestures, for example, may well present a more complex recognition problem than the simplest of activity classes. We can however make some generalized assumptions about the complexity within the different abstraction levels. Lower levels of abstraction such as pose and gesture should provide less challenges to the field in its current state, while higher levels of abstraction, especially those involving interactions between two or more entities, still remain a challenging issue.Obviously with the definitions of the action types presented there can be some overlap in how to handle events in which an entity is not only interacted with, but also pivotal to the context of the label. Consider the class label ‘smoking’, this event can fit both into the definition of a singular action in which the object is explicit to the action, a person–object interaction between the person and cigarette, and also into its own activity class in which smoking is the task executed. Consider also the class label of ‘pushing’, this may be a class label that can be readily classified as a single action, person–person interaction, or person–object interaction depending upon the entities present, and also as an activity if there is a contextual background to the event. This highlights the complexity in describing class labels and requires the careful consideration of overlaps that appear to be presented between datasets with similar action classes. To further this point, we ask should the community consider an interaction as its own complete class, or should the system understand the states occupied by all entities within the interaction, i.e. the class label of ‘pushing’ may be deconstructed into sub-classes that describe the action of the instigator and the reaction of the recipient. Many interaction datasets handle the class labeling as a single complete unit of interaction, often reliant on the action committed by the instigator, e.g. K3HI, SBU Kinect Interaction, and UT Interaction. However the TUM Kitchen, 50 Salads and MPII Composite sets explicitly annotate the states of both entities to define the person–object interactions for the purpose of activity recognition. The use of a single interaction class that encompasses all sub-divisions of that interaction may provide learning that is broad and resistant to variation of intra-class behaviors; however by learning the sub-divisions of an interaction class, considering the different actions and reactions as their own states, there may be an ability to learn more effective boundaries for execution variations. For this study we have considered and evaluated upon the class labels provided by the original datasets; however we invite the community towards potentially defining multi-scale class labeling for the purpose of action and activity recognition.The size of a dataset, not just in the number of sequences but also in the range of different action classes and participants, can impact on its suitability for method evaluation. Testing on a small-scale dataset can provide misleading results during analysis which may not be replicated when introducing more class labels or observations, due in part to the highly variable nature of inter- and intra-instance executions. Contrarily there are implications in the usage of large datasets, not only the collection and storage of data, but also in the processing of features, class learning and validation. Due to the inherent issues in obtaining a large number of participants, action classes, and sequences, the largest sets tend to be meta-sets, which collect action sequences from various sources, such as YouTube and films, containing large variation between sequences; this often makes meta-sets highly variable and challenging problems to be solved. A summary of dataset sizes is given in Table 5Datasets with a small number of action classes, such as MSR Action-I, MSR Action-II, and Drinking/Smoking, can often provide strong recognition results in part due to the low number of partitions needed to divide the actions provided within the set. Those sets that contain a large number of action classes, namely HMDB51, UCF101, and UCF50, provide a difficult challenge to HAR methods due to the need to find partitioning information within each class that allows for inter-class partitioning, while preserving intra-class similarity. Due to the inconceivable number of possible actions and interactions that can exist in the real world it can be beneficial to evaluate methodologies on datasets with a large number of distinct action classes.Datasets that are able to provide more individual subjects performing an action are able to portray the variability in inter- and intra-subject execution of a given class. Observations of the same action class can often differ greatly in both their temporal rate and spatial occupancy, leading to complexity in learning the action for recognition purposes. Methods that are able to provide subject invariant action recognition should provide consistent results on a dataset which contains a large number of subjects. Again, the meta-sets tend to provide the highest number of subjects, almost capturing a new subject per sequence, representing a large range of inter-subject variation.The number of observations per class can impact on the ability of a system to suitable learn a given class. A low number of observed instances of a class can result in weak recognition of unobserved instances of the same class. HMDB51 provides over 100 instances of each action class it contains, providing a range of observations across differing viewpoints, quality and executions, as such it can provide a useful benchmark for the recognition of actions from a subject and observation invariant methodology. Current pose based datasets contain few repeated instances of an action class, often with 3–5 repetitions per subject per class. To increase the number of instances per class it is possible to segment those datasets which contain continuous recordings of multiple executions into discrete single execution clips, this includes the KTH dataset.The total number of sequences within a dataset should be a factor of the number of subjects, classes, and number of class executions, and as such can impact on the reliability of the results produced. Larger datasets can provide larger testing sets for which to evaluate a system, allowing for more confidence in the results of the validation. Size alone however is only one parameter in the selection of evaluation benchmark, with domain, class complexity and modality impacting on the application of methodologies to real world implementations.The intended application domain of a dataset can provide certain intrinsic features in the data collection methodology and action classes captured, from low resolution images of CCTV surveillance footage to more complex action sequences of daily living. Some actions are representative of the domain from which they are intended; for example the UCF-Sports dataset, [137], makes use of numerous actions from various sports, such as javelin throws and long jumps. We classify the datasets into 4 action class domains; generic actions, daily living, surveillance, and sport. Generic action datasets have no overall theme, instead providing classes that are pan-domain; these include the classes ‘running’, ‘jumping’, ‘punching’, and also more complex interactions such as ‘handshake’ or ‘play guitar’. Daily living datasets often include actions and activities that are more natural in their execution and environment, this includes classes based on assisted living and household tasks. Surveillance datasets often make use of elevated view points and lower resolution images, mirroring the common camera setups in the security industry [145,154]. Sports based action recognition often makes use of previously captured data from multiple sources, often containing varying image quality and varying levels of camera motion. A summary of the domains for each of the datasets is provided in Table 6.Many action recognition datasets often contain generic action classes that are observable in numerous domains. The intention is to cover a wide variety of actions to allow domain invariant action recognition, with generic datasets being the most widely used for validation purposes, including the KTH [22], Weizmann [24] and MSR Action3D [41] sets. Many generic datasets are collected in a laboratory environment; with static cameras, static backgrounds and calibrated data-capture setups, including Berkeley MHAD and CMU MoCap. Others may be collected outdoors with a controlled clutter free setting, such as Weizmann and KTH. Others are collected within cluttered environments, featuring non-participatory subjects that complicate the scene, such as MSR Action-I and Action-II. Pose based datasets which make use of a depth sensor and the pose estimation technique of extracting the 3D skeleton are often captured in a relatively clutter free scene due to the limitations of the skeletal tracking methodology used.Daily living sets are designed to closely represent the natural world in both the environmental surroundings and the natural style of action classes executed. The Tum Kitchen [129], MSR DA3D [40,41], MPII Cooking [155], and Rochester AoDL [156] sets are commonly used for the analysis of methodology in the recognition of day-to-day activities. Activities include ‘having a conversation’, ‘phone calls’, ‘laying down’, ‘drinking’ and ‘eating’, but may also include sub-actions within a higher level task, such as ‘setting a table’ or ‘cooking a meal’. The executions may be allowed to occur naturally as in the 50 Salads, MPII Cooking, and MPII Composite datasets; or the observations may be more scripted, such as in the POETICON and the robotic class of the TUM Kitchen set [59,121,129]. By understanding the actions and interactions within a daily activity dataset the field is moving towards learning higher level semantics of human behavior via natural representations.Surveillance is a domain concerned with detecting and identifying activity within a continuous observation of a scene, often making use of video-based action recognition samples that are taken from a distance, prone to crowding, and contain poor resolution recordings. A surveillance domain sequence may contain more frames of empty or redundant information, sporadically interspersed with temporally short regions of interest. Datasets such as UT-Interaction, CASIA, and BEHAVE make use of surveillance style setups to capture emphasized person–person interaction classes such as ‘come together’ and ‘fight’. The CAVIAR, ETISEO, and VIRAT datasets all make use of detailed ground truth annotations to provide information regarding persons and objects within the scene, enabling the evaluation of methods in detecting varies entities and their interactions within a scene for higher semantic understanding of the events.The UCF-Sports [137] and Olympic Sports [119] datasets are focused explicitly on sports related action examples. These sets contain samples that are collected from various sources of TV and online recordings, providing samples that vary in their recording quality and containing both static and dynamic camera movements. As such these can often be challenging datasets. In both cases the intent of the dataset is to be able to recognize the sport being performed, this can be more challenging than in the case of learning sports related actions, such as in the case of ‘tennis serve’ and ‘boxing’ from some of the generic action datasets. A sport as a high level class can contain numerous action and interaction actions that make up the overall activity and learning a sporting class may require learning vastly different observations that belong to the same class. 3D pose based HAR in the sports domain has few datasets due to the complexity in capturing a large volume in which the activity can be played. The G3Di dataset provides interactions between two people in the context of a sporting game played through a console, however we treat the provided classes as being generic actions rather than true sporting based actions.Table 7outlines various ground truths provided with each dataset, both for spatial ground truths and labeling of action classes. Providing consistent ground truth with which to evaluate results is important for developing benchmarks against which to test developed methodologies, aiding in the generation of a metric score that can be used to compare implementations.Class label ground truths and scene annotations of a dataset can provide a clear benchmark for quantifying the performance of a developed methodology. Some datasets provide frame-by-frame labeling of the scene, whilst others label an entire sequence as containing a given class label. These annotations allow quantification of results obtained from various methodologies, with predicted class labels and detections being compared against the ground truth. The collection of the class ground truth can be either manually annotated by the author or produced via some form of machine learning. Manual annotation can provide detailed descriptions of the entire scene, with locations and affordances being given to persons and objects within the scene, as can be seen with the ETISEO and HMDB51 datasets. These can be extremely useful when tracking the states of multiple entities within the scene, or for the understanding of a high level abstracted class; however the manual labeling of individual frames can produce observation bias into the dataset, requiring strict objective criterion to gain consistent ground truths. Machine based annotations can combined machine learning with data labeling to rapidly provide ground truths to large datasets, e.g. the Hollywood and Hollywood-2 datasets are partially annotated by learning textual descriptions within the film’s scripts. An automated ground truth annotation may require subsequent manual verification to ensure the false labeling is minimized. The simplest form of ground truth labeling provided by HAR datasets is by attributing the entire sequence to a specific label, acknowledging that a given action occurs at some point within the observation, as is the case with CASIA, CMU MMAC, MSR Action3D, and many more. Having simplistic whole sequence labeling can make it hard to use such datasets for detection purposes, as evaluating the beginning and end frames of an action can be problematic to determine manually. For action recognition purposes the learning of background frames from a sequence may also provide some level of noise to the partitioning of that class.Spatial truth can be provided by explicitly locating the subjects and objects within the environment or by highlighting regions of interest in which the subject, object or event resides by using bounding boxes or silhouette masks. Calibrated ground truth methods can be used to determine the spatial locations of the subjects within a scene, often using motion capture suits and markers to explicitly track the body through a capture volume, providing either a raw point cloud or the predicted skeletal frame of the body. The accuracy of motion capture systems can vary from method to method; however the resolution accuracy is often within a range of a few millimeters, providing superior body tracking than using machine learning based pose extraction. Marker based motion capture systems, such as those used in CMU MoCap and Berkeley MHAD, require the application of each marker to the individual at certain predetermined locations, and variation in placement of the markers on the body from sequence to sequence can introduce small errors in obtaining truly explicit spatial truths. The use of depth maps to extract an estimated 3D pose of the subject in the scene has become a prominent inclusion in depth based HAR datasets such as MSR Action3D, K3HI, SBU Kinect Interaction, CAD120, and CAD60. The observation is fed into a skeleton extractor, such as the OpenNI, Microsoft Kinect SDK softwares, or custom methods [157–159], in which a subject is located and a human skeleton model is fitted, predicting the 3D coordinates for a number of joints. Although an approximation of true 3D spatial orientation of the joints, depth sensors and joint tracking has been shown to be relatively accurate in the tracking of humans [34,36]. The use of bounding boxes to describe regions of interest in a scene are common within appearance based datasets, such as BEHAVE, CAVIAR, ETISEO and MSR Action, especially those that consider person–object interactions or belong to the surveillance domain. They simply provide an area of focus that contains relevant annotated information, such as object and subject location. The use of silhouette masks also provide a region of interest, whilst simultaneously removing external and internal appearance information, representing the subject as a binary classification as either belonging to the background or foreground. These regions of interest can also be utilized to validate action detection and localization methodologies, removing the unwanted information from the overall observation.Camera based methods can also make use of various viewpoints, from single camera to multi-camera simultaneous viewpoint capture. Viewpoints can also differ greatly, capturing events from roughly a parallel plane with the ground, elevated above head height, or from an almost top-down viewpoint. Often events are captured from a viewpoint that is roughly parallel to the ground, producing observations that are almost representative of a human-eye view of the event, examples can be found in MSR Action3D, K3HI, and CMU MoCap. A summary of dataset viewpoint representation is given in Table 8. Sets such as BEHAVE, UT Interaction and CASIA contain events recorded from an elevated angle; these viewpoints are common within the surveillance domain due to the positioning of surveillance cameras for capturing a large scene at once. Recently there has been work towards the recognition of actions from a first person perspective, with data captured from the viewpoint of the observer [99,160,161]. This field is often working towards the understanding of interactions by robots for the purpose of human–robot interaction. Such a viewpoint is believed to provide more meaningful information when the observer has an active role in the interaction rather than simply observing a scene, as is the case in human–robotics interactions. There are also datasets which attempt to capture simultaneous multi-camera views of an event for the purpose of evaluating supposedly pose-invariant methodologies. Sets such as WVU MultiView, Berkeley MHAD and TUM Kitchen all contain numerous cameras located in differing positions capturing the same scene. Depth based data, such as tracked skeletons and motion capture marker coordinates, can be orientated arbitrarily about its three axes to develop multi-view methodology, with some pose alignment used to reduce the effect of orientation discrepancies, [162]. However this is dependent upon accurate pose estimation in order to provide data which has confident tracking. Due to the nature of extracting pose estimation from depth based methods there are limited numbers of datasets that utilize multiple depth sensors; however Berkeley MHAD provides multiple Kinect recordings alongside its vast number of appearance views, with the sensors located in positions from which the infrared sensors are not causing occlusions.Popularity of a dataset within the community can be difficult to evaluate, however here we attempt to identify the number of citations that are made to the dataset’s description publication via Google Scholar. Using this count as a measure of how well adopted a given dataset has become, we rank each set in Table 9. Note that older sets can often show higher citation due in part to their steady accumulation of references over time. Similarly, the number of citations made may not explicitly reflect the use of dataset as a benchmark, as often the datasets are published in parallel with a novel methodology which may accrue its own citations. It can be seen from Table 9 that the pose based datasets show considerably fewer citations, most likely due to the relative age of the rapidly growing field.The following section will now detail the datasets evaluated above, describing the composition of each dataset and a brief discussion of their usage in literature. We also report on some of the accuracy rates achieved using each dataset; however due to the multitude of evaluation criteria these are used as an indicative measure of the dataset complexity as opposed to a definitive survey of state-of-the-art results obtained. It would be unfair to directly compare results obtained between datasets, or even within datasets for differing purposes. Such a survey would require extensive analysis to ensure that cross comparison between results are fair and reflective of their achievement.The section is divided into the appearance and pose based datasets, with further grouping into their respective abstraction levels as described by Fig. 1.Even though we wish to examine datasets that utilize pose estimation techniques for action recognition, we will briefly discuss availability and impact of video based sets. Video provides a relatively cheap method of obtaining sample sequences, with both real-world and staged executions being obtained. Collection methods can make use of single or multi-camera setups. Actions can be performed from a singular viewpoint, most often face-on, or from differing angles.The Drinking/Smoking dataset [78], Fig. 2, contains 308 sequences of either drinking or smoking actions taken from 3 sources (two movies and one custom recorded set). The dataset can be used for detection, recognition and localization evaluation. There are 159 instances of drinking and 149 of smoking, from either a front or side viewpoint. Instances are taken from two movies and some custom lab recordings. The dataset provides the training and testing samples that were utilized for method evaluation in [79], allowing for direct comparison to the original methodology. The authors in [79] used singular key frames, coupled with a space-time action classifier to detect the events in a given scene. The dataset has been utilized for validation several times, notably in the evaluation of modeling person–object interactions via the object trajectory [163] and the recognition of an action class based on a single observation training instance [164].The HMDB51 [86], Fig. 3, is a dataset of 6849 video sequences of 51 different actions with a minimum of 101 executions per label. Videos are taken from a mixture of online clips, movies and television. The actions encompass 5 perceived top level classes; facial expressions, facial object interaction, body movement, body object interaction and person-to-person interaction. The dataset also provides detailed labeling of video quality, number of people in scene, viewpoint, visible body parts and camera motion. Instances of the same class can vary greatly in terms of the execution style, the subject appearance, the quality of the images and the camera view. As such the HMDB51 dataset is one of the more challenging appearance datasets for use as an evaluation tool. The original publication attempts to use the HOG/HOF feature combination to recognize action events within the scene [87], developing a collection of visual words to train an SVM classifier. It has since been used for the recognition of actions within natural settings and loosely controlled parameters [135,165,166].The Hollywood dataset [88], Fig.4, intends to provide realistic human behaviors from unconstrained videos, namely those produced for purposes other than HAR, e.g. films and television. The set provides 5 action classes: answer phone, get out of car, sit down, sit up, and stand up and 3 interaction classes: handshake, hug, and kiss. The sequences are automatically annotated by forming alignments with the script, subtitle and time stamps of the sequence. A subsample of these have been manually corrected to provide ‘clean’ training and testing sets. In the associated publication [89] the videos are represented by STIPs at multiple spatio-temporal scales. Each STIP is then used to generate a set of HOG and HOF features which are then used to train a non-linear Support Vector Machine (SVM) for event classification. The Hollywood dataset and its sister dataset Hollywood2 (see below), are often used for the validation of methods under realistic conditions; due to the high variation in the quality and examples of behaviors observed.The Hollywood-2 dataset [90], Fig. 5, is an extension on the Hollywood dataset, greatly increasing the number of observed sequences and action classes. The set contains 3669 sequences of 8 single person actions and 4 interactions. There is a large overlap with the Hollywood dataset in terms of the action classes provided, including answer phone, get out of car, handshake, hug, kiss, sit down, sit up, and stand up. The set also introduces 4 new classes; drive car, eat, fight person, and run. To explore the relationship between an action and the scene it occurs within the dataset provides 10 scenario locations, with a large focus on interior environments. The set takes scenes from 69 movies and automatically annotates them using the same script synchronization as with the Hollywood dataset. The set is divided into a training and testing set, selecting given films for each set. There is some intersection between the Hollywood and Hollywood-2 sets, with some films being included in the training set for Hollywood and the testing set for Hollywood-2, thus the two sets should be used independently of each other to avoid issues in training on samples that may be duplicated in the training sets. Marszalek [91] utilizes the set for the learning of both actions and scenes; locating space-time salient motion with a 3D-Harris detector, and static salient areas using 2D-Harris regions. They compute HOG/HOF descriptors from the 3D-Harris, and Scale Invariant Feature Transform (SIFT) descriptors from the 2D-Harris points. These features provide a vocabulary for a bag of words representation of the scene and action. Hollywood-2 has been used as an evaluation set for a number of studies, including multi-modal fusion of audio–visual cues for action recognition [31] and the use of action primitives for classification [167].The INRIA Xmas Motion Acquisition Sequences (IXMAS) dataset [96], Fig. 6, is a multi-view dataset designed for view invariant HAR. 5 cameras capture simultaneous views of 12 actors performing 13 actions with 3 repetitions; check watch, cross arms, scratch head, sit down, get up, turn around, walk, wave, punch, kick, point, pick up, and throw. There is an additional labeling for the action class ‘nothing’, and the throwing action is divided into an over-head and underarm subclass. The ground truth is provided in the form of frame-by-frame annotation of the action class label present within the scene, subject silhouettes, and reconstructed volumes. The dataset is initially used for the recognition of actions regardless of viewpoint [168] and history volumes [97]. It has since been used widely to evaluate methodology on view-invariant recognition [169–171].Presented in 2004 by Schuldt et al. [22], the KTH dataset [102], Fig. 7, consists 25 subjects performing 6 actions in 4 scenario types, recorded via a static camera. Actions are performed with single subjects visible in a frame, with multiple executions of an action in a sequence. Actions performed were walking, jogging, running, boxing, hand waving, and hand clapping. Scenarios covered involved outdoor, scale variations, clothing variation and indoor recordings. For the original study, the 600 continuous recordings are divided to provide 2391 single execution sequences. Despite the simplistic nature of the actions performed, the set has become prominent within the appearance based HAR community, with hundreds of citations making use of the dataset for validation. The original study [22] used the dataset to extract local space-time features from the observation for classification. Of the vast number of subsequent uses of the KTH dataset there have been uses of individual sequences for classification methodologies [172–175], while several sequences are often appended to test segmentation methods [44].The Microsoft Research group have provided a number of appearance based HAR datasets, including MSR Action-I and Action-II, Fig. 8,. These sets are readily available to the research community at [112] and include actions, daily activities and gestures. The Microsoft Research (MSR) Action I dataset [112] contains 16 video sequences of 10 subjects performing 3 different action classes: clapping, waving and boxing. Each sequence contains continuous recording of different actions being carried out in series, often in a cluttered outdoor environment or with multiple subjects in the observation. Manually provided ground truth labeling is given as a spatio-temporal bounding box over for each frame in which the action is present, and in [113] correct detection is determined by the overlap of ground truth and prediction areas. The dataset is used within [113] for the purpose of action detection and localization within the scene; it has since been used for evaluating a variety of HAR recognition and detection methods [176,177] MSR Action II [112] is an expansion on the previous set, containing the same 3 action classes; clapping, waving and boxing. The set includes 54 continuous video sequences recorded in crowded environments, including multiple subjects and non-subject individuals. Both the MSR Action I and Action II datasets contain action classes that allow them to be overlapped with the KTH dataset, intending to promote cross-dataset action detection evaluation. The dataset has been used to validate methods in action detection, localization and recognition [114,178,179]The Multicamera Human Action Video (MuHAVi) dataset [116], Fig. 9, is a large scale multi-view action recognition set, capturing 17 action classes performed by 14 subjects. The action classes are performed within the capture area and include punch, kick, run and stop, walk and turn, collapse, pull object, pick up and throw, walk and fall, look in car, crawl, wave, draw graffiti, jump over fence, drunk walk, climb ladder, smash object, and jump over gap. Many of these classes contain sub-action primitive action classes themselves, which can either be handled separately or as a compound action. The project is ongoing, and provides ground truth silhouette masks for a number of sequences and ground truth frame annotation for all sequences. A large number of publications use the MuHAVi set for evaluation of action recognition and view invariant methods, most of which are detailed in [116].The Stanford Olympic Sports dataset [118], Fig. 10, contains video clips of 16 sport actions taken from YouTube; high-jump, long-jump, triple-jump, pole-vault, basketball, bowling, tennis-serve, platform, discus, hammer, javelin, shot put, springboard, snatch, clean and jerk, and vault. The clips include cluttered scenes, dynamic camera movement, varying scales and execution styles. [119] uses the dataset to evaluate their method of modeling temporal structure motion for action recognition. The suggested testing and training split of the 50 video sequences is provided at [118] and the ground truth is provided as simple whole sequence labels.The Stanford 40 Actions dataset [180], Fig. 11, is a collection of 9532 still images that represent naturally executed actions including riding a horse, rowing a boat, fishing, applauding, and smoking. There are between 180 and 300 images per action class and the dataset provides bounding box annotation for the subject in the observation for the purpose of action localization and recognition. The challenge of understanding human action from a singular instance is explored in [127], learning the context between actions and the objects contained within the image, with further study into the use of still image understanding being evaluated on the dataset [181,182].The UCF action datasets are a collection that make use of video to represent action sequences. UCF-11, UCF-50 and UCF-101 are all video sets taken from YouTube designed to provide an action recognition problem that focuses on the accurate recognition of observations in which there are highly variable training observation samples.The UCF-11 dataset [132], Fig. 12, was produced to enable the evaluation of recognition methods upon unconstrained observations of an action class. The collection provides 1168 sequences from 11 different action classes, with over 100 instances in each action class. The actions presented include basketball shooting, cycling, diving, golf swinging, horseback riding, soccer juggling, swinging, tennis swinging, trampoline jumping, volleyball spiking, and walking with a dog. Liu [133] presents this dataset as an evaluation set for the understanding of action classes from natural observations that were produced for reasons other than for HAR, providing little knowledge about camera quality, viewpoint and motion. The samples are grouped into 25 categories, with each category containing numerous instances of the same action from similar scenarios.UCF-50 [134], Fig. 13, extends upon the UCF-11 dataset by introducing yet more action classes, increasing the total count to 50, including baseball pitch, basketball shooting, bench press, biking, billiards shot, breaststroke, clean and jerk, diving, drumming, fencing, golf swing, playing guitar, high jump, horse race, horse riding, hula hoop, javelin throw, juggling balls, jump rope, jumping jack, kayaking, lunges, military parade, mixing batter, nunchucks, playing piano, pizza tossing, pole vault, pommel horse, pull ups, punch, push ups, rock climbing indoor, rope climbing, rowing, salsa spins, skate boarding, skiing, skijet, soccer juggling, swing, playing tabla, tai chi, tennis swing, trampoline jumping, playing violin, volleyball spiking, walking with a dog, and yo yo. Again the initial use of the dataset is in the recognition of actions from an unconstrained set of recordings [135]. This dataset has since been superseded by the UCF-101 dataset.UCF-101 [130], Fig. 14, is the latest extension of the UCF appearance based action datasets, containing 101 separate action classes collected from various sources, which are grouped into 5 activity types; Human–Object Interaction, Body-Motion Only, Human–Human Interaction, Playing Musical Instruments and Sports. The sub-activity actions include apply eye makeup, apply lipstick, archery, baby crawling, balance beam, band marching, baseball pitch, basketball shooting, basketball dunk, bench press, biking, billiards shot, blow dry hair, blowing candles, body weight squats, bowling, boxing punching bag, boxing speed bag, breaststroke, brushing teeth, clean and jerk, cliff diving, cricket bowling, cricket shot, cutting in kitchen, diving, drumming, fencing, field hockey penalty, floor gymnastics, frisbee catch, front crawl, golf swing, haircut, hammer throw, hammering, handstand pushups, handstand walking, head massage, high jump, horse race, horse riding, hula hoop, ice dancing, javelin throw, juggling balls, jump rope, jumping jack, kayaking, knitting, long jump, lunges, military parade, mixing batter, mopping floor, nunchucks, parallel bars, pizza tossing, playing guitar, playing piano, playing tabla, playing violin, playing cello, playing daf, playing dhol, playing flute, playing sitar, pole vault, pommel horse, pull ups, punch, push ups, rafting, rock climbing indoor, rope climbing, rowing, salsa spins, shaving beard, shotput, skate boarding, skiing, skijet, sky diving, soccer juggling, soccer penalty, still rings, sumo wrestling, surfing, swing, table tennis shot, tai chi, tennis swing, throw discus, trampoline jumping, typing, uneven bars, volleyball spiking, walking with a dog, wall pushups, writing on board, and yo yo. Over 13,320 sequences are collected to provide over 100 instances of each action class, with each sequence containing variation in subject, scenario and camera parameters. The original publication [131] provides a baseline recognition score by extracting Harris3D corners from a clip and representing them via HOG/HOF descriptors. These descriptors were then used to generate a histogram of video words, utilizing the training and testing splits provided at [130] to evaluate the performance of an SVM developed on the histogram vectors. These baseline results allow for the evaluation of novel methods on the previously developed methods by utilizing the benchmark splits.Overall the use of the UCF Action datasets for method evaluation has been reported within numerous publications, especially in the recognition of action classes from observations that contain little similarity in regards to the camera positioning and quality [165,166,183].The UCF Sport dataset [136], Fig. 15, is similar in construction to the previous UCF Action sets, with sequences being collected from previously recorded events. The main difference is that the focal domain of the dataset is within the recognition of sporting activity domain, providing class labels such as diving, golf swinging, kicking, lifting, horseback riding, running, skating, swinging, and walking. The dataset collects 200 sequences and contains the same unconstrained camera parameters as the previous action sets. The dataset has since been used for the evaluation of action recognition methodologies both within the generic and sports specific domain, including [174,184,185].The Virtual Human Action Silhouette (ViHASi) dataset [142], Fig. 16, provides synthetic silhouette masks that have been produced by using 20 actions performed by 9 virtual actors. The use of a virtual environment has allowed for the generation of 40 virtual viewpoints from which the silhouettes are produced, creating a dataset that provides evaluation of view invariant silhouette based action recognition. The 20 action performances are generated using the same motion capture sequences, ensuring that each virtual actual performs the same action execution. Classes include hang on bar, jump on bar, jump over object, run and pull object, run and push object, run and turn left, run and turn right, hero smash, hero door slam, knockout spin, knockout, grenade, collapse, stand and look, punch, jump kick, walk, walk and turn back, and run. Differing subjects were developed that included not only differences in body proportions but also variation in clothing which impact upon the silhouettes produced. Despite being a niche dataset there have been several works that make use of the ViHASi dataset, evaluating the use of silhouette pose projection for action recognition [143,186–188].One of the three main appearance based action recognition datasets, the Weizmann dataset [24,146], Fig. 17, provides RGB recordings of 10 actions performed by 9 subjects, captured at 50 fps. Actions performed were running, walking, skipping, jumping-jacks, jump forwards, jump in place, sideways gallop, two-handed wave, one-handed wave and bend. Each recording uses a static camera to capture multiple executions of the same action against a solid wall background. Tangential to the main dataset are a series of samples deemed for method robustness testing; with recordings including occlusion, abnormal execution style and carrying objects. Despite providing readily definable action classes, the Weizmann dataset has been used repeatedly for HAR method validation since its creation [48,173,174,189–192].The WVU MultiView dataset [148], Fig. 18, is comprised of two sets; with WVU MultiView-I containing sequences of a single action execution from one of 12 action classes, and WVU MultiView-II describing continuous combinations of 9 available actions in an interleaved fashion. The intention is to utilize the first dataset to perform action recognition, while the second requires detection and segmentation. WVU MultiView makes use of 8 cameras to collect data that can be used to test view-invariant methods. The classes included in the first dataset are standing still, nodding head, clapping, waving one hand, waving two hands, punching, jogging, jumping jack, kicking, picking up, throwing and bowling. In WVU-II the actions show slight overlap; including clapping hands, waving one arm, waving two arms, punching, jogging in place, jumping in place, kicking, bending, and underarm bowling. The two datasets are often used as a validation method for multiple distributed camera action recognition [149,150,193].BEHAVE, Fig. 19, is a human action recognition project that is comprised of two separate datasets, the Multiagent Interaction dataset [60], featuring multi-person interactions captured from an elevated viewpoint, and the Optical Flow set [194], containing recordings of human flow in a train station exit under three scenarios. The Multiagent Interaction set contains 10 interaction classes being performed by multiple individuals in an outdoors environment, recorded using static RGB cameras from 2 non-simultaneous viewpoints. The set contains bounding box annotations of the individual and their action class, including; InGroup, Approach, WalkTogether, Meet, Split, Ignore, Chase, Fight, RunTogether, Following. There are 163 instances with varying number of instances per class. The class WalkTogether contains 43 instances at a total of 6694 frames, while the classes Meet and Following only contain a single instance each, comprising of less than 100 frames each. The majority of the sequences are annotated by providing the ground truth individual bounding box locations. Action labels are then provided along with start and end frames for the event; this is coupled with the identification labels for each individual involved in the event. Ref. [60] also provides image pixel position measurements for the computation of the ground plane homography. One viewpoint of the set is captured from inside a building, filmed through a window, and as such contains a large amount of noise in the illumination of the scene due to reflections from the glass. The BEHAVE Optical Flow dataset has been used several times for event detection in crowded scenes [195,196], while the Multiagent Interaction set has been utilized for the recognition of actions and identification of individuals within the surveillance domain [197–200].The Beijing Institute of Technology (BIT)-Interaction dataset [63], Fig. 20, consists of 400 AVI video clips capturing 8 interaction events with 50 videos per class. The dataset provides a further level of complexity by introducing varying occlusions, appearances, temporal and spatial scale. The classes include these which are definable by their respective poses, such as bow, boxing, handshake, high-five, hug, kick, pat, and push. The presence of pedestrians, occlusions and variable views results in a dataset that can be used for detection, localization and recognition. The original publication [64] utilized the BIT-Interaction set to develop a set of high-level phrases which describe the interaction in terms of the interdependencies of lower-level attributes belonging to each individual in the interaction.The Context Aware Vision using Image-based Active Recognition (CAVIAR) project [70], Fig. 21, provides action recognition sets for the purpose of determining if local image descriptors guided by contextual knowledge of the scene can improve image-based action recognition. The project provides two RGB sets, one from an entrance lobby of the Institut National de Recherche en Informatique et en Automatique (INRIA) labs, the second utilizes 2 simultaneous views from within a shopping center. The INRIA subset provides 4 single person actions: walk, browse, rest/slump, and leave bag unattended; the set also provides 2 interaction classes: meet and fight. The shopping center subset provides the remaining 3 action classes enter shop, window shop, and leave shop. Both subsets provide the ground plane homography measurements for the scene. The ground truth labeling in the sets are XML hand labeled bounding boxes for each image in the sequence. The CAVIAR dataset is one of the most utilized appearance based datasets for human action recognition alongside KTH and Weizmann, being utilized for evaluation of numerous methodologies, including tracking, recognition and segmentation [201–203].The ETISEO dataset [80], Fig. 22, provides a methodology and accompanying dataset for video surveillance evaluation. 5 main scenarios are presented, containing instances of 15 action classes. 10 are single person actions: walk, run, sit, lying, crouching, holding, jumping, pick up, put down, and tailgate. 5 are person–person interactions: push, fight, meet, exchange, and queue. The dataset is annotated with bounding boxes detailing both events and physical objects within the frame, and the project provides evaluation metrics on a variety of problems including object detection, object localization, object tracking, object classification, and event recognition. The project is a multi-institute venture into developing a benchmark for evaluating security and surveillance domain observations, containing detailed information regarding its use as a validation tool and the data structures provided [81]. ETISEO has been used as a benchmark for the detection, localization, and recognition of both pedestrians and behavioral actions in a surveillance domain [204–206].The Jet Propulsion Laboratory (JPL), Fig. 23, at the California Institute of Technology provide a first person viewpoint dataset into person–person interactions [98]. The project captures a mixture of positive and negative interactions from a camera positioned on a non-static subject as they traverse an office environment. During the sequences the subject encounters 7 interactions which are recorded from their perspective of recipient; including handshake, petting subject, wave at subject, conversation with pointing at subject, punching subject, and throwing objects at subject. In the associated publication [99] the use of local motion descriptors across space-time provides a bag of visual words representation for recognition of first person recipient view interactions. The egocentric domain is often used for determining the actions of the observer and of the subject observed [99,207], with complications arising from the motion and perspective captured by the camera’s location on the body [208,209].The UT-Interaction dataset [140], Fig. 24, contains 20 continuous static camera recordings of multiple subjects performing multiple interactions within a scene. Each recording captures all action classes recorded from an elevated angle. The interactions between two individuals including handshake, hug, kick, point, punch, and push. Several subsets are present within the dataset; with static backgrounds, dynamic backgrounds, multiple events in the scene and crowded scenes. Ground truths are provided in terms of bounding box frame-by-frame annotation. The UT-Interaction set has often been used for evaluating interaction recognition within a surveillance domain [141,210,211].The VIRAT dataset [144], Fig. 25, provides action classes expected within a surveillance domain, describing natural scenes and interactions between individuals and the environment. The sequences within the collection are annotated with a high level of detail, providing information via bounding box annotations regarding the people, objects, vehicles and the interactions that occur between them. 12 activity classes are provided, including loading an object on a vehicle, unloading an object from a vehicle, opening the trunk of a vehicle, closing the trunk of a vehicle, getting in to a vehicle, getting out of a vehicle, gesturing, digging, carrying an object, running, entering a facility, and exiting a facility. The challenge of this dataset is in the natural executions of the interactions, and also in the cluttered scene that is observed, a common problem task for real world surveillance domain. Common use of the VIRAT dataset is in the analysis of surveillance domain action detection and localisation [212–215].The CASIA action database [68] provides a multi-view action and interaction dataset containing 8 single person actions and 7 person–person interactions. The single person actions include walk, run, bend, jump, crouch, faint, wander, and punch car. The interactions include rob, fight, follow, follow and gather, meet and part, meet and together, and overtake. The scene is captured from three simultaneous static viewpoints; horizontal/side on, top down and angle, although global locations of the cameras are not provided. The choice of viewpoint provides a surveillance style dataset with simultaneous viewing allowing the evaluation of view-invariant methods. Each AVI sequence is annotated as a whole clip by filename; detailing the viewing angle, action class, subject ID, and action repetition number. The CASIA dataset has been used to evaluate view-independent, surveillance based action recognition [69,216–218].The Max Planck Institut Informatik (MPII) Cooking datasets, Fig. 26, are a pair of closely related datasets that concern the daily living activities of cooking and the action and interactions that are compounded into the higher level semantic classes. The MPII Cooking Activities dataset [108] contains 44 continuous recordings of naturally executed daily cooking activities, with 12 participants completing activities that included any number of 65 potential activities, such as chopping and pouring. These fine grained activities are recorded as part of a higher level semantic activity, such as preparing a salad or cake, allowing flow between each action to be natural. The focus of [109] is to detect and recognize the execution of the lower level actions within an activity, with the dataset providing detailed frame annotation to facilitate evaluation. The MPII Composite set [110] is an expansion upon the MPII Cooking set, introducing more detailed information regarding the higher level activity classes. 14 activities, such as cake, omelet, mashed potato, and pancake, are provided as composite activities which are built from the finer actions provided by the MPII Cooking Activities set. These two sets provide a method of evaluating methods that are able to recognize events at differing levels of abstraction. The overall activity can be decomposed into a set of lower level actions and interactions.The Rochester Activities of Daily Living (AoDL), Fig. 27, dataset contains 10 natural action classes performed 3 times by five subjects in front of a waist height desk. Actions performed include answering a phone, dialing a phone, looking up a phone number in a telephone directory, writing a phone number on a white-board, drinking a glass of water, eating snack chips, peeling a banana, eating a banana, chopping a banana, and eating food with silverware. The intention of the project is to perform HAR on more realistic executions of behavior classes, with [123,219–221] using tracked key point trajectories for action recognition and [222] considering the pairwise spatio-temporal relationships of the interest points in the scene.MoCap has allowed for highly accurate localization of body positioning, using markers to identify joints and bones in coordinates of a volume space. Motion capture techniques often utilize the pose of an individual during an action’s execution. There are several purely MoCap datasets available; however most now use MoCap techniques as part of a multimodal collection. In recent years community focus has moved from traditional motion capture techniques to the collection of joint positioning via commercial depth sensors, such as the Microsoft Kinect. Depth data has become prolific in the community since the release of the Microsoft Kinect depth sensor; mostly due to its ability to accurately track a human, and provide a skeleton representation in 3D. Most depth sets also provide their corresponding skeleton representations so that the same skeletons are also part of the standard training and testing methods. It is not only the visual information that is used to identify action events. Often there can be use of accelerometers and gyroscopes to capture the kinematics of the body during the performance of an action. Sometimes these additional depth based modalities are captured in parallel with more conventional methods, sometimes they are the sole modality under focus.The Berkeley MHAD [62], Fig. 28, contains 660 sequences of 12 participants performing 11 actions, recorded using RGB video, depth sensors, marker based motion capture, accelerometers and microphones. Action classes include jumping jacks, bend, punch, two handed wave, one handed wave, clap hands, throw, sit down and stand, sit, and stand. Each class was recorded 5 times, with jumping jacks, bend, punch, two handed wave, one handed wave, clap hands, sit down and stand containing 5 continuous repetitions per recording. 3D coordinates for 43 markers were recorded via 8 MoCap cameras. 12 RGB cameras were grouped into 2 stereo vision clusters and 2 4-camera multi-view clusters. Two Kinect sensors captured RGB-D data. 6 tri-axial accelerometers were affixed to the wrist, ankles and hips to record limb dynamics during an action. Sensor recordings are geometrically and temporally synchronized to allow multimodal HAR. The motion capture system is first calibrated, with RGB and Kinect sensors being calibrated for both intrinsic and extrinsic parameters, referencing all sequences to a the motion capture world coordinate system. Due to the vast amount of data provided across numerous modalities the MHAD dataset has been used to evaluated methods that make use of modality fusion [39,223], motion capture data [224,225] and RGB-D joint tracking information for the purpose of action recognition [225,226]The CMU Graphics Lab action dataset [74], Fig. 29, contains marker-based MoCap sequences of subjects performing a large variety of actions. Sequences are grouped in 6 types; Human Interaction, Interaction with Environment, Locomotion, Physical Activities & Sports, Situations & Scenarios and Test Motions. Sequences can contain multiple action executions and can include person-to-person interactions. The set uses 40–60 markers to capture the full human skeleton of 109 subjects in 2605 sequences. The C3D format markers are not consistent from sequence to sequence, and thus require the user to determine the marker locations beforehand when using the C3D data. However the use of the AMC formatted joint angles are consistent between sequences. Evaluation on the CMU MoCap dataset often utilizes a subset of the overall dataset, due to the large number of sequences and action classes [41,42,227].The G3D dataset [82], Fig. 30, is an action set that focuses on the recognition of actions designed for gaming and computer interaction. 10 subjects perform 20 game based actions, with up to 3 repetitions, in front of a stationary Kinect sensor, capturing synchronized RGB-D data and 20 joint skeletons. Actions recorded by the dataset include punch right, punch left, kick right, kick left, defend, golf swing, tennis swing forehand, tennis swing backhand, tennis serve, throw bowling ball, aim and fire gun, walk, run, jump, climb, crouch, steer a car, wave, flap and clap. The purpose of the dataset is to develop a framework for the real time recognition of actions within a observed scene [83].The Hollywood3D dataset [92], Fig. 31, contains similar action classes to that of the appearance based Hollywood and Hollywood-2 datasets; null, run, punch, kick, shoot, eat, drive, use phone, kiss, hug, stand up, sit down, swim, and dance. However the data modality in this dataset consists of depth maps obtained from the production of commercial 3D movies. The purpose of [93] is to expand the complexity of using non-HAR based recordings for the purpose of action recognition by representing the observations as depth data. The dataset provided a significant challenge to the community, describing natural observations of action classes as depth information [228–230].The HumanEVA-I dataset [95,231] contains video sequences synchronized with motion capture poses, capturing 6 actions performed by 4 subjects. Actions include walking, jogging, gesturing, throwing & catching, boxing, and a combo action. Actions were repeated 3 times, once with MoCap and then twice with a combination of MoCap and video. The set contains separate training, testing and validation sets, detailed in [95], allowing comparative results. The MoCap markers were tracked using 6 ViconPeak cameras, while the video data was collected using 3 RGB cameras and 4 grayscale cameras. The grayscale cameras are located in the corners of the capture space, with the color cameras positioned to the front, left, and right viewpoints of the subject.HumanEVA-II, Fig. 32, then expands on the previous set by having 2 subjects perform combinations of the previous actions to develop a secondary testing set that is ten times smaller than that of HumanEva-I. The dataset is designed as a testing set for the methods developed on the HumanEva-I dataset, providing only complex continuous sequences; starting with walking a path, then jogging, concluding with the subject alternating balancing on each foot. The intent is to use the HumanEva-I set to train and validate the system, with testing be executed on the HumanEva-II set. The MoCap markers are tracked using 12 ViconPeak cameras, twice as many as the original dataset, and the video data is collected by 4 color cameras located in the corners of the capture space. The HumanEva sets have been used repeatedly to evaluate the performance of pose estimation and sequence segmentation algorithms due to its continuous series of multiple action classes performed in the combo-action observations and the motion capture ground truth [232,233].The MSR Action3D dataset [40,41,112], Fig. 33, provides the first example of a public depth map dataset for HAR, capturing both the depth data for 20 gaming related actions performed by 10 subjects, with up to 3 executions of an action by each subject. Action classes include high arm wave, horizontal arm wave, hammer, hand catch, forward punch, high throw, draw x, draw tick, draw circle, hand clap, two hand wave, side boxing, bend, forward kick, side kick, jogging, tennis swing, tennis serve, pickup and throw, and golf swing. In recent years the dataset has been expanded to include the tracked joints in screen and real world coordinates, each skeleton consists of 20 joints captured with a device similar to the Kinect; head, shoulder center, left shoulder, right shoulder, left elbow, right elbow, left wrist, right wrist, left hand, right hand, spine, hip center, left hip, right hip, left knee, right knee, left ankle, right ankle, left foot, and right foot. Due to the computation involved in learning the 20 total actions the total dataset is divided into 3 subsets, each containing 8 of the 20 possible actions, dubbed Action Sets. Action Set 1 contains similar action classes such as high throw and tennis serve. Action Set 2 contains actions that involve subtle actions with the arms and hands, including draw tick and draw circle. Action Set 3 then aims to group complicated actions together, including the sporting actions. 10 samples are considered to be too noisy and are omitted from the use of the dataset for evaluation [112]. The MSR Action3D dataset is one of the most prominent action recognition depth based datasets available, with numerous action recognition methods utilizing the set for evaluation purposes [28,39,45,162,234].The MSR Gesture3D dataset, Fig. 34, contains 336 sequences of American Sign Language gestures. 10 subjects remain in a seated position and perform 12 different dynamic sign language gestures in up to three repetitions. The dataset provides the depth maps for each frame in the sequence. In the associated publication [115] it was possible to develop a real-time system to recognize input to the Kinect sensor at 10 fps. The dataset is captured from a front-on view, with the lower half of the subject obscured by a table, with focus being on the body, head, arms and hands. Due to its focus on the behavior displayed by the hands the MSR Gesture3D dataset has often been used in the hand pose estimation and action recognition community [235,236].The MPI08 dataset [105], Fig. 35, collects motion capture recordings of subjects performing tasks for the purpose of multi-modal body tracking fusion. Despite this primary purpose it provides several sequences of highly accurate spatial tracking whilst the subjects execute their actions. The use of modality fusion within this dataset could be exploited for the purpose of action recognition, utilizing the frame labeling of the files for action recognition [106,107].The University of Dundee 50 Salads dataset [58], Fig. 36, is a collection of birds-eye-view recordings of food preparations using an RGB-D sensor and accelerometers for the purpose of recognizing gestures and person–object interactions. 25 participants prepared 2 salads each, utilizing a variety of tools and ingredients, resulting in a total of 966 observed action instances, with an average of over 55 observations per lower level class. The sequences are annotated with a label being assigned to all frames between a given start and stop frame. There are two tiers of labeling, the first describing the higher level action as one of 3 tasks; cut and mix ingredients, prepare dressing, and serve salad. The second tier describes the frames in terms of a lower level actions such as peel cucumber, cut cucumber, place cucumber into bowl, cut tomato, place tomato into bowl, cut cheese, place cheese into bowl, cut lettuce, place lettuce into bowl, mix ingredients, add oil, add vinegar, add salt, add pepper, mix dressing, serve salad onto plate, and add dressing. Each of the lower level action labels are given a suffix of being either prep, core or post the action. Tri-axis accelerometer recordings are provided for 7 tools used in the sequences. Stein and McKenna [58] provide the RGB video recordings, depth maps, accelerometer sequences and the synchronization of all sequences. The sequences begin with an assistant making 4–5 sharp knocks to an IMU in the scene, allowing synchronization to that point. The 50 Salads set has been used to explore the impact of learning differing levels of abstraction, focusing on the information gained between higher and lower level behaviors [237] and the understanding of complex scenarios [238].A progression on G3D, the G3Di [84,85] dataset makes use of a single Kinect depth sensor to track two individuals interacting within the scene. This dataset captures 6 pairs of subjects performing actions taken from 6 sports, with 14 action classes. The top level sports are boxing, volleyball, football, table tennis, sprint, and hurdles. The primitive actions include right punch, left punch, defend, overhand hit, underhand hit, jump hit, kick, block, save, serve, forehand hit, backhand hit, run, and jump. The action classes run and serve span two classes, whereas the remainder are top level action specific. The G3Di dataset presents interactions between two individuals who are side by side with both subjects facing the sensor. This makes it possible to separately recognize an individual’s action, before then compounding this knowledge to recognize an interaction [85].The K3HI dataset [101], Fig. 37, contains 8 pairwise person–person interactions performed by 15 individuals. Each of the 320 sequences captures a single execution of one of the 8 action classes; approaching, departing, kicking, punching, pointing, pushing, exchanging an object, and shaking hands. Both individuals in the scene are tracked using the Kinect sensor, capturing the 15 joints of each subject. The OpenNI skeleton representation was extracted, tracking the head, neck, left shoulder, right shoulder, left elbow, right elbow, left hand, right hand, torso, left hip, right hip, left knee, right knee, left foot, and right foot. The use of approaching and departing classes are dismissed for method evaluation in [101], due to their simplistic nature and high recognition accuracy rates. [101] uses a 4-fold cross validation method for their initially reported experimentation on the dataset; however the training/testing splits are not provided in the dataset itself. The dataset has been used to evaluate positive action recognition in [101].The SBU Kinect Interaction dataset [37,124], Fig. 38, presents person–person interaction recorded via synchronized video, depth maps and skeletal models of both actors. 7 individuals, in 21 pairings, performed 8 types of interaction. The interactions between the two individuals are captured from a side-on view and include approaching, departing, pushing, kicking, punching, exchanging objects, hugging, and shaking hands. These interactions provide several classes that involve similar gestures in the arms, namely pushing, punching, shaking hands and exchanging objects. The dataset provides the RGB video and depth maps, alongside the OpenNI 15 joint skeleton tracking. The skeleton joints are head, neck, torso, left shoulder, right shoulder, left elbow, right elbow, left hand, right hand, left hip, right hip, left knee, right knee, left foot, right foot. In [37] the dataset was used to identify joint distance and velocity features that are coupled between the individuals in the scene.The Utrecht Multi-Person Benchmark (UMPM) [138,239], Fig. 39, is a synchronized video and marker-based MoCap set that includes numerous subjects interacting and occluding one another. The set intends to provide a ground truth labeled standard dataset for the recognition of dense scenes, at risk of inter- and intra-subject occlusions. The dataset records 9 different scenarios using 4 RGB cameras and a 37 marker MoCap system; (1) walk,jog or run, (2) walk along circle or triangle shape, (3) walk around while another person hangs or sits on chair, (4) sit, lie, hang or stand on table, (5) grab object on table, (6) conversation with gestures, (7) throw or pass ball while moving, (8) stand still and (9) move around. These are all complex activities that contain actions, interactions and higher level activities that require recognition. This is coupled with a multi-view approach and the occurrence of a cluttered scene. UMPM has been used to explore tracking and action recognition that occurs in a scene that contains numerous complex occlusions [239].The CMU Multimodal Activity dataset, [73]Fig. 40, commonly known as the CMU MMAC, aims to understand recognition of complicated human daily actions. The dataset utilizes RGB video, marker-based motion capture, audio, accelerometers and gyroscopes for the capture of 5 differing cooking recipes by 43 subjects. 6 RGB camera viewpoints record the scene with a variety of spatial and temporal resolutions, including a first person view from a head-mounted camera. 63 markers are tracked using a Vicon motion capture system of 12 cameras which provide the spatial ground truth for the body tracking. The CMU MMAC database has been used to evaluate the temporal segmentation of complex activities from a first person perspective [240], and the segmentation of joint gestures for classification [241].The CAD-60 dataset [65], Fig. 41, provides 60 RGB-D recordings of 4 subjects performing 12 activities across 5 different environments. Participants were captured executing more natural actions, including rinsing mouth, brushing teeth, wearing contact lens, talking on phone, drinking water, opening container, chopping, stirring, talking on couch, relaxing on couch, writing on white-board, and working on computer. The dataset is provided as a collection of RGB images, depth maps and the corresponding 15-joint tracked skeletons. [66] first introduced the dataset to classify unstructured human activity by constructing a graph of sub-activities that compound into the top level activities.The CAD-120 set [65], Fig. 42, focuses on the execution of long daily activities, capturing high and low level actions. 4 participants provide 120 sequences capturing 10 high level activities, which are each comprised of a number of 10 potential sub-activities. The compound actions include making cereal, taking medicine, stacking objects, unstacking objects, microwaving food, picking objects, cleaning objects, taking food, arranging objects, and having a meal. Gestures are labeled as reaching, moving, pouring, eating, drinking, opening, placing, closing, scrubbing and null.The LIRIS Human Activities dataset [104], Fig. 43, provides RGB-D recordings of 21 subjects completing 10 behavioral classes; discussion, giving an item, picking up or putting down, entering or leave room, unsuccessful attempt to enter room, unlock room and enter, leaving an unattended bag, handshake, typing on a keyboard, and talking on a telephone. The dataset attempts to be purposefully difficult by introducing very little constraint in the execution of the behavior, relying more on the semantics of the behavior. To introduce a more realistic representation the use of different contexts and tools within an action class is provided, i.e. different types of phone are used, and discussions can occur seated or standing. Two different semi-independent sets are provided, the first represents the depth maps captured from a Kinect mounted on a joystick controlled robot, the other is a stationary mounted RGB camcorder.Using the Kinect sensor, 10 subjects were recorded performing 16 natural daily actions, [41], Fig. 44; drink, eat, read book, call cellphone, write on paper, use laptop, use vacuum cleaner, cheer up, sit still, toss paper, play game, lie down on sofa, walk, play guitar, stand up, sit down. The actions were recorded, were possible, in both standing and seated positions in a living room environment. The majority of the action classes involve person–object interactions, such as toss paper and write on paper, thus provide a more real-world set of observations for the purpose of HAR. As with the MSR Action3D dataset, the RGB and depth map sequences are provided alongside the tracked 20-joint skeletons. No standard training/testing splits are provided either within the description paper [41] or the web location [112]. MSR Daily Activities 3D has provided evaluation for numerous methods in activity recognition via pose features, [29,242,243]The POETICON corpus [120] is a collection of scripted scenarios in which two individuals perform a daily living task such as cleaning the kitchen. The subjects are tracked using motion capture suits and recorded using 5 RGB camcorders. Certain tools and objects within the environment were also identified using marker based tracking. 4 pairs of actors learnt the associated script with 6 different high level scenarios, performing each activity in 3 repetitions per pair. Wallraven and Schultze [121] apply the dataset to identifying actions at differing levels of abstraction and granularity.The Technische Universität München (TUM) Kitchen dataset [128,129], Fig. 45, provides a daily living set that describes the preparation of a table setting within a smart kitchen. The set makes use of video, marker-less skeletal tracking, RFID tagged objects and magnetic sensors to provide detailed information on the human action in the kitchen. The subjects collect items from cupboards and use them to set a place on the table. Actions were performed in one of two styles; natural or robotic, in which subjects could only carry one object at a time. There are also recordings of certain actions being repeatedly executed in order to train a classifier for recognition. The TUM Kitchen is designed to facilitate the learning of action events are differing levels of abstraction; recognizing not only the low level gestures and actions, but also the overall tasks completed. The TUM Kitchen has often been used for recognition of activities at various abstraction levels [129], and also for the detection, localization and segmentation problem [244,245].In the following section we draw upon the findings from the survey to present our own novel dataset for the recognition of complex conversational interactions between two individuals. We outline the necessity for the production of the set, the structure of the dataset and report on several previous publications that have utilized the dataset.As can be seen from the previous sections, datasets that are able to capture human action using appearance based modalities, such as RGB videos, have developed from representing non-realistic emphasized actions to considering more complex interactions between individuals and their surrounding environment. The field has moved from actions which are easily distinguishable in the visual domain, e.g. ‘waving’ and ‘jumping’, to those of interactions, although still recognizable, e.g. ‘hug’ and ‘kiss’ [23,246]. Due to the availability of these datasets many methods have been produced and evaluated for the purpose of action recognition and detection, including the use of SIFT [247], temporal Harris corner features [248] or STIPs [89].Meanwhile the depth based methodology which has risen to prominence over the past decade has far fewer publicly available datasets which consider the problem of person–person interactions, with most considering either emphasized actions or interactions. As such we believe that the publication of a dataset that represents highly complex person–person interactions is timely. We have chosen to capture conversational interactions between two individuals using the Kinect depth sensor, posing the challenge of recognizing subtle interaction classes.The primitive action provided by many of the available datasets can be decomposed into a series of definable gestures and atomic poses. However we argue that real-world social interactions contain more complex and subtle class partitioning, being a product of multiple actions, semantics and the interplay between those involved. We therefore propose the problem of recognizing interactions in which the distinguishing features are containing within the temporal dynamics of the total event, such as that of a verbal interaction. We provide a dataset in which the interaction is labeled as a whole, rather than describing the event based on the primitive gestures within the scene. By providing such a dataset we hope to move the field towards the recognition of scenarios in which the defining descriptors are highly complex and context specific.In this work, we choose seven conversational action categories and use a two-Kinect setup to capture 3D human pose during the interaction between two individuals. The collection environment consisted of a cleared space within a boardroom (Fig. 46); in order to keep the dataset complex, no effort was made to homogenize the environment by use of any backdrops. Two Kinect sensors were located at opposite ends of the room, approximately 2 m away from a marker on which a subject would be loosely located. Each person was recorded using a single Kinect Sensor at 30 fps. The Kinect was offset to the front right of the subject in order to avoid occlusion from the opposing subject, which could occur if taking a frontal recording of the subject. Subjects were placed approximately one meter apart but not limited in their movement. Two PAL cameras (B cameras in Fig. 46) were located to capture the full body of a single participant, with a third camera (M in Fig. 46) located to capture the entire recording scene. These recordings are purely for the monitoring of the experiment and synchronization, thus are not provided within the dataset published in [1]. Cameras were also located to capture the face of each participant (F cameras in Fig. 46), these provide the RGB recordings used to generate the gaze estimation provided. The recording devices were not located in the same place, and as such there is orientation variance between the depth maps and the RGB recordings.Participants were required to complete 7 different conversational tasks, outlined in Table 10. There was no time limitation on the execution of each task, and some tasks took naturally longer than others. Several tasks were given revealed to the participants before collection, to allow preparation; the actions that required preparation were describing work, story telling, debate, discussion and jokes (Figs. 47and 48). If the participants were given the problem or subjective question before the study then there may have been a reduction in interaction between the individuals.Each task was performed and then there was a small break while the participants were reminded of the next task to carry out. The first task was to discuss an area of their current work. The second task was to prepare an interesting story to tell their partner, such as a holiday experience. The third task was to jointly find the answer to a problem. The fourth task was a debate, where the participants were asked to prepare arguments from opposing view points on an issue we gave to them. In the fifth task they were asked to discuss the issues surrounding a particular statement and come to agreement whether they believe the statement is true or not. The participants were asked to reach an agreement through discussion; hence, it is different to the debate task, which was based on conflicting views. The sixth task was to answer a subjective question, and the seventh task was to take it in turn telling jokes to one another.16 subjects responded to a call for participants to take part in dataset collection and provided their consent for the collection. Participants were then organized into 8 pairs to record the person–person interaction during the following series of conversational styles. Interested individuals were asked to prepare for tasks ‘Describing Work’, ‘Story Telling’, ‘Debate’ and ‘Joke’ in advance, while the topics for ‘Problem Solving’, ‘Discussion’ and ‘Subjective Question’ were provided during collection. Participants were not subjected to time limitations or any execution styles.The main data in the collection is the skeletons extracted using the Microsoft Kinect SDK, providing the 20 tracked joints and the confidence of the tracking at each frame in the sequence. The raw depth and RGB recordings from the Kinect are also available alongside the RGB recordings from the separate camcorder. We also provide facial tracking features used for the tracking of gaze and facial dynamics which have been used for feature fusion in [249]. Despite the benefit that audio provides to action classification [30,250–252], the audio has been stripped from all recordings due to the private natures of the conversations that occurred during the interactions. This allows the conversations to be natural, providing a more realistic representation of the scenarios than if each subject was given a script. Although this may be disappointing to those wishing to carry out audio–visual feature fusion, we believe that CONVERSE provides a more complex challenge to be solved when occluding the audio cues of conversation.To provide insight into the use of CONVERSE for interaction recognition we provide baseline results achieved using various state of the art methods for subject-specific classification, with results reported in Tables 11and 12. To achieve this level of accuracy we followed the methods outlined in [249], utilizing pose, face and head orientation features to provide a visual vocabulary of words and topics. Discriminative classifiers, SVM and Random Forest (RF), were trained to classify each CONVERSE task based on the discriminative power of the features. K-Nearest Neighbor (KNN) was selected as a baseline classification technique for comparison. First a Gaussian Mixture Model (GMM) was fitted to low level features (joint–joint/joint–plane distances and joint velocity) in order to obtain a vocabulary of 740 visual words consisting of the Gaussian components taken from 5 s clips, 370 words from facial features and 370 from pose features. Sequences were also sub-sampled into 20 s segments and Latent Dirichlet Allocation performed to obtain the 25 visual topics that made up each document. Both visual words and topics were used as temporal feature descriptors for each class. All sequences from the CONVERSE set were utilized, with 10 fold cross validation used to evaluate the performance. The RF classifier was produced using 100 trees with random sampling with replacement. The SVM was trained using a radial basis function kernel on the same training set.It was found that visual topics provide a generalization of the classes which benefit SVM and RF performance (Table 12), while KNN produced more accurate classification on data at the visual words level (Table 11). The importance of each feature was identified via novel use of particle swarm optimization (PSO) to generate a Ranked Feature SVM (SVM-R) classifier, reducing the dimensionality of the feature space and simultaneously performing optimal SVM model selection. The PSO method locates the optimal hyper-parameters that are used to subsequently train the SVM-R classifier by selecting towards correct identification of training samples, removal of redundant features, and the selection of compact feature vectors. This method significantly improved over the previous methods due to the selection of key partitioning features, increasing the accuracy on both visual word and topic feature sets. SVM-R optimization achieved 89.1% and 87.3% accuracy for word and topic respective levels of generalization due to its optimized feature set pruning. More detail regarding the use of the SVM-R classifier can be found in [249,253].Although these accuracy rates are relatively high, the results have been obtained on subject specific classification utilizing features extracted from long temporal segments of the observation. The main challenge we propose with CONVERSE is for the role of global recognition across multiple subjects for these complex interaction classes.

@&#CONCLUSIONS@&#
