@&#MAIN-TITLE@&#
Solving stochastic resource-constrained project scheduling problems by closed-loop approximate dynamic programming

@&#HIGHLIGHTS@&#
A computationally tractable closed-loop algorithm for stochastic resource-constrained project scheduling (SRCPSP).Theoretical results of the rollout policy for SRCPSP, including the sequential improvement property.Enhanced base policy by embedding constraint programming in the rollout framework.Integrated look-back and look-ahead approximation architectures to improve effectiveness and efficiency of the algorithm.

@&#KEYPHRASES@&#
Resource-constrained project scheduling,Uncertain task durations,Stochastic scheduling,Approximate dynamic programming,Simulation,

@&#ABSTRACT@&#
Project scheduling problems with both resource constraints and uncertain task durations have applications in a variety of industries. While the existing research literature has been focusing on finding an a priori open-loop task sequence that minimizes the expected makespan, finding a dynamic and adaptive closed-loop policy has been regarded as being computationally intractable. In this research, we develop effective and efficient approximate dynamic programming (ADP) algorithms based on the rollout policy for this category of stochastic scheduling problems. To enhance performance of the rollout algorithm, we employ constraint programming (CP) to improve the performance of base policy offered by a priority-rule heuristic. We further devise a hybrid ADP framework that integrates both the look-back and look-ahead approximation architectures, to simultaneously achieve both the quality of a rollout (look-ahead) policy to sequentially improve a task sequence, and the efficiency of a lookup table (look-back) approach. Computational results on the benchmark instances show that our hybrid ADP algorithm is able to obtain competitive solutions with the state-of-the-art algorithms in reasonable computational time. It performs particularly well for instances with non-symmetric probability distribution of task durations.

@&#INTRODUCTION@&#
Given a set of tasks and resources, a typical resource-constrained project scheduling problem (RCPSP) involves finding a time- and resource-feasible schedule of tasks, such that the project completion time (makespan) is minimized. It includes various shop scheduling problems such as job shop, flow shop, and open shop as special cases (Brucker, 2002), and has a wide range of applications in construction, manufacturing, R&D, personnel scheduling and military operations. The deterministic RCPSP is well-known to be NP-complete (Bartusch, Mohring, & Randermacher, 1988). Its solution methods have been extensively studied in the operations research literature. See Demeulemeester and Herroelen (2002), Kolisch and Hartmann (2006) and Debels and Vanhoucke (2007) for the state-of-the-art algorithms to solve deterministic RCPSPs.In the real-world scheduling environment, the exact task duration is often unknown at the point when the scheduling decision is made, giving rise to the stochastic RCPSP (SRCPSP; Demeulemeester & Herroelen, 2002). For instance, construction projects can often be delayed by unexpected weather and/or disruption of logistics; accurate task processing time in the engineering-to-order (ETO) or made-to-order (MTO) settings can be difficult to obtain due to the uniqueness of order and learning; task durations in an R&D project are often uncertain due to the unforeseeable outcome of a predecessor task; planning for a military mission or campaign is often subject to uncertain task durations.The classical approach to deal with uncertainty of task duration in project management is the well-known PERT analysis (Malcolm, Roseboom, Clark, & Fazar, 1959). PERT estimates the expected project makespan and its variation assuming given probability distribution of task durations, such as the commonly used beta-distribution. A limitation of the PERT method and its variants (cf. Dodin, 2006; Slyke & Richard, 1963) is the lack of decision-support. That is, these methods focus on understanding the statistical properties of project makespan, but they do not provide optimal start times, nor identify which path(s) will likely be critical, or the longest path. Other researchers have attempted to overcome this limitation of the PERT methodology (cf. Dodin, 1984; Elmaghraby, Ferreira, & Tavares, 2000). However, this line of research does not explicitly consider resource constraints: it is assumed that unlimited resources are available for project execution.A successful approach to address exogenous disruptions on project task duration and resource availability, known as robust project scheduling, aims to construct a robust or stable project schedule in a resource-constrained environment. One may proactively construct a robust schedule to minimize the expected deviation from the baseline schedule, which is often achieved by properly inserting time buffers in the schedule (cf. Goldratt, 1997; Herroelen & Leus, 2004); or reactively revise/re-optimize the schedule during project execution to obtain a feasible schedule with respect to the newly available information, that minimizes the deviation from the original baseline schedule (Van de Vonder, Ballestin, Demeulemeester, & Herroelen, 2007). Several researchers developed combined proactive–reactive procedures (cf. Demeulemeester, Herroelen, & Leus, 2008; Van de Vonder, Demeulemeester, Herroelen, & Leus, 2006). Recent work by Deblaere, Demeulemeester and Herroelen (2011) developed a dynamic proactive project execution policy to minimize the weighted activity starting time deviations plus the penalty or bonus for late or early project completion. Bruni, Beraldi, Guerrero, and Pinto (2011) proposed a decomposition-based heuristic method with the use of joint probabilistic constraints to obtain a feasible baseline schedule with the ability to hedge against variations of activity duration.The SRCPSP studied in this paper minimizes the expected project makespan while considering both limited resource availability and uncertain task duration. The goal is to obtain a time- and resource-feasible task sequence that gives minimum expected project makespan. A deterministic schedule, in the form of task start times, is unable to provide an implementable solution to SRCPSP, as it can easily become time- or resource-infeasible due to random task durations. An implementable solution to the addressed problem requires a policy-type decision specifying which task(s) to start at each decision point (cf. Igelmund & Radermacher, 1983a; Mohring & Stork, 2000).The fundamental difference between SRCPSP and the robust scheduling problems is the underlying scheduling setting and environment. In robust scheduling, the decision-maker must obtain a baseline schedule for the entire project upfront, but he/she does not have complete flexibility of revising it during project execution. Therefore, any deviation from the baseline schedule will be penalized, and its goal is to minimize total expected deviation penalty through: either obtaining a proactive baseline schedule to start with, and/or re-optimizing the schedule during execution in a reactive way. In the SRCPSP, we assume that project scheduling decisions are executed sequentially without the need of an a priori baseline schedule. The SRCPSP also differs from the other stream of research on resource allocation for projects, which focuses on optimizing the time-cost tradeoffs under uncertainty (cf. Gutjahr, Strauss & Wagner, 2000; Keller & Bayraksan, 2010; Shen, Smith & Ahmad, 2010).The simplest and most popular policy-type solutions are various priority-based policies, in which all tasks are ranked according to a predefined priority rule, and started in the order specified by the priority. Although easy to implement and fast to execute, they suffer the so-called Graham's anomalies (Graham, 1966), and there are instances for which no priority-based policy generates an optimal schedule (Demeulemeester & Herroelen, 2002). Stochastic branch-and-bound procedures have been proposed by Igelmund and Radermacher (1983b) and Stork (2001). Recent research efforts have been focusing on the integrated simulation-optimization (Sim-Opt) algorithms, in which either a greedy heuristic (Golenko-Ginzburg & Gonik, 1997) or some metaheuristic (Glover & Kochenberger, 2005) is used to search the global solution space; while simulation is employed to evaluate a candidate or neighbor solution. Various metaheuristics have been implemented in such frameworks including a genetic algorithm (GA) by Ballestin (2007), tabu search (TS) by Tsai and Gemmill (1998), and greedy randomized adaptive search procedure (GRASP) by Ballestin and Leus (2009). Notably, Ashtiani, Leus, and Aryanezhad (2011) developed a new pre-processing procedure with a two-phase GA to obtain currently best results for SRCPSP in the literature. These approaches attempt to find a sequence of all tasks at time zero, without observing durations of early tasks. Using the terminology of the optimal control theory, they correspond to an open-loop policy. Such solutions are static in nature, and are not updated during real-time execution.An alternative solution approach to SRCPSP is the closed-loop policy, in which scheduling decisions are made in a sequential fashion through the methodology of dynamic programming (DP; Bertsekas, 2007). Instead of optimizing the entire task sequence prior to project execution, a closed-loop policy seeks to find an optimal decision rule (policy) for selecting the task(s) to start at each decision-point, given the information a decision-maker knows about the current system. It is dynamic and adaptive in nature, which makes it possible to take advantage of information that becomes available between decision-points. Thus, in principle, a closed-loop policy is more flexible than an open-loop policy. We refer to Dreyfus and Law (1977) and Bertsekas (2007) for a systematic treatment of DP and closed-loop policies.Although theoretically attractive, optimal closed-loop policies for SRCPSP have generally been perceived as being computationally intractable. There are only a handful of papers that attempt to offer closed-loop policies. Fernandez (1995) describes SRCPSP as a multi-stage sequential decision problem and proposes a decision-tree approach, which is computationally intractable for even a small number of tasks and scenarios. Fernandez and Armacost (1996) and Fernandez, Armacost, and Pet-Edwards (1998) discuss the advantage of modeling SRCPSP as a sequential decision problem and clarifies the importance of an implementability or non-anticipativity constraint in the formulation, without providing any computational results. Choi, Realff, and Lee (2004) consider a simplified job-shop version of SRCPSP with fixed sequence of tasks in each project, and present a DP algorithm with a heuristically confined state space. They show computational results for an example of project with 17 tasks.The objective of this paper is to develop computationally tractable near-optimal closed-loop algorithms for solving reasonably large SRCPSPs. To tackle the curse-of-dimensionality, we have devised several schemes to approximately solve the Bellman equation (Bellman, 1957) in the Markov decision process model (MDP; Puterman, 2005) for SRCPSP. Our approximate dynamic programming (ADP) algorithm is built upon three core techniques. First, a sub-problem is constructed in each decision stage through an approximation of the exact recursive cost-to-go function in DP. Second, a forward iteration procedure is employed through sample paths generated by Monte Carlo (MC) simulation, which avoids the need of enumerating all possible states via the transition function in classical DP. Third, some deterministic scheduling methods are employed to handle the sub-problem in each ADP iteration.Given the combinatorial nature of SRCPSP, we embed some scheduling heuristics in a rollout framework (Bertsekas, Tsitsiklis, & Wu, 1997) to sequentially improve a closed-loop solution. Such a rollout policy can be viewed as a look-ahead strategy that evaluates the expected cost of a state-action pair using the MC sample paths for all future stages. Our design of the rollout algorithm enhances its basic version in two ways. Only a small subset of random scenarios (features) are generated, using the idea of limited simulation proposed by Bertsekas and Castanon (1999), to reduce the burden of a pure MC simulation. In addition, since the quality of a closed-loop solution depends heavily upon the ability to solve the sub-problem in each iteration, we replace the simple priority-rule based heuristic by the more effective constraint programming (CP; Baptiste, Le Pape, & Nuijten, 2001) techniques to handle the scheduling sub-problem.We then design a new approximation architecture integrating techniques in the artificial intelligence (AI) area to approximate the cost-go-go function. Rather than directly working on the optimization problem associated with the recursive cost-to-go function, the AI community has developed a suite of reinforcement learning (RL; Sutton & Barto, 1998) techniques to learn the value of a state-decision pair through sequential decision-environment interactions. Take the well-known lookup table approach for example: there, a record for the expected value of each state-decision pair is maintained and updated using MC simulation. In contrast to the rollout look-ahead policy, the lookup table approach can be viewed as look-back strategy that approximately evaluates the value of a state-decision pair based on what happens in the history simulated by MC samples. One advantage of the lookup table approach is its simplicity and speed to implement. Its main drawback, however, is that it is often not possible to visit every state-decision pair in a lookup table. Therefore, a pure lookup table approach usually works best well for problems with state and decision spaces of low dimension.The complementary strengths of lookup table (look-back) and rollout (look-ahead) approaches have motivated us to integrate the two. On one hand, the lookup table directly associates the function evaluation with a state-action pair, which significantly reduces the computational burden of pure MC simulation in the rollout look-ahead approach. On the other hand, for those state-action pairs that have not been visited/evaluated in the lookup table, the rollout look-ahead policy can be used to evaluate them. Therefore, a hybrid look-back and look-ahead (HBA) approximation architecture is expected to offer more effective and efficient solutions than either the look-back or look-ahead approach alone. Comprehensive computational studies are conducted to examine the behavior and performance of the proposed ADP algorithm with HBA, called ADP–HBA. Computational results on benchmark instances show that our closed-loop ADP–HBA algorithm is competitive with the state-of-the-art algorithm and performs particularly well for instances with non-symmetric probability distribution of task durations.The remainder of the paper is organized as follows. Section 2 formally describes the SRCPSP and models it as an MDP. The basic rollout algorithm and its enhanced versions are developed in Section 3. Section 4 presents the hybrid ADP–HBA algorithm integrating both the rollout look-ahead and lookup table look-back approximation schemes. Computational results on benchmark instances are provided in Section 5. Section 6 draws conclusion and discusses future research.The problem setting and assumptions of SRCPSP are presented first, followed by its MDP formulation, which lays the foundation for the ADP algorithms to be developed in the succeeding sections.Consider an activity-on-node (AON) project network described by G(V, E), whereV={0,1,…,n,n+1}denotes a set of activities in the project. Activities 0 andn+1are the dummy start and end of the project, respectively. E represents a set of precedence relationships among activities, i.e. for (i, j) ∈ E it is required that j cannot start before i is finished. A setK={1,2,…,m}of resources are needed for the project to execute. Each resource k ∈ K has a limited capacity Rkavailable in each time period. An activity j requires rjkunits of resource k during its execution. Duration of an activity j is represented by a random variable Dj, which follows a probability distribution known to the decision-maker. The activity duration is observed only when the activity is completed. The goal is to find a time- and resource-feasible schedule that minimizes the expected completion time (makespan) of the project. The AON network is assumed to be acyclic and an activity cannot be interrupted once started.We model the SRCPSP as an MDP with the following components.A decision stage is defined as a time point when an activity is completed. Clearly, the number of stages L is finite and bounded by n. The time point associated with stagei=1,…,Lis recorded by ti.The state at stage i is define asSi={Ci,Ai,d¯i,τi}, where Cidenotes the set of completed activities, Aiis the set of active activities,di¯is defined as the vector of activity durations that have been executed and observed at stage i, andτiis the vector of start times of activities inCi∪Aiat stage i.Sicontains all the information needed to make decision at the current stage. The initial state can be described asS0={∅,∅,d¯0,τ0}, whered¯0is initialized as a vector of zeros, andτ0 initialized as a vector of −1. The terminating state isSL={V,∅,d¯L,τL}, whered¯LandτLis the vector of realized durations and start times of all activities, respectively.LetX(Si)be the set of activities eligible to start at stage i, andUi=V∖(Ci∪Ai)denote the set of unscheduled activities at stage i.X(Si)can be described as follows:(1)X(Si)={∑j∈Ai00x∈Ui:j∈Ci,∀j|(j,x)∈E(2)rxk+∑j∈Airjk≤Rk,∀k∈K}An unscheduled activity x ∈ Uiis eligible to start if it satisfies two conditions. Condition (1) requires that every predecessor j of x must be completed. Condition (2) requires that for each resource type k the sum of resource requirements of all active activities plus the resource requirement of the candidate activity x does not exceed the capacity Rk. We nameX(Si)the eligible set givenSiat stage i. Then the decision made at stage i is a subsetxiof activities feasible to be started, i.e.xi⊂X(Si)and its start does not violate the available resource capacity.Let SM( · ) be the transition function from the current stage i to the next stagei+1:(3)Si+1=SM(Si,xi,Dj:j∈Ai),whereDj:j∈Aiis the set of realized random durations of active activities in Ai. Equation (3) means that the stateSi+1at the next stagei+1depends only on the current stateSi, the decisionxitaken, and the random duration Djrealized for an active activity j ∈ Ai, but not the history of the decision process. This is known as the Markov property. Given the time point tiat stage i, The transition function SM( · ) can be written as follows:(4){d¯i+1[j]:=ti+1−τi[j],∀j∈AiCi+1:=Ci∪{j∈Ai:d¯i+1[j]=Dj}Ai+1:=Ai∖{j∈Ai:d¯i+1[j]=Dj}∪xiτi+1[j]:=ti,∀j∈xiThe executed durationd¯i+1[j]of an active activity j at stagei+1is updated as the difference between the time pointti+1at stagei+1and activity j’s start timeτi[j]. The setCi+1of completed activities is the union of Ciand the set of activities that are completed at stagei+1. The setAi+1of active activities is Aiexcluding the set of activities that are completed at stagei+1, and including the setxiof started activities. Finally, for each activity j scheduled to start at state i we update its start time to be ti. Due to randomness of activity durations,Sitransits toSi+1with some probability. For a reasonably large project, precisely calculating such transition probabilities will be computationally intractable, and one of our main solution strategies is to use simulation to handle such high-dimension uncertainty space. This will be elaborated in Sections 3.1 and 3.2.The system dynamics of the MDP can be depicted by Fig. 1. A decision stage is triggered by the completion of an activity. At each decision stage i with the corresponding stateSi, a setxiof activities is scheduled to start. When any activity is completed, the exogenous random durations of active activities are observed, and the system transits to the next stagei+1with stateSi+1, described by an updated set of completed activitiesCi+1, set of active activitiesAi+1, vectord¯i+1of realized durations of active activities, and vectorτi+1of activity start times. Next the decisionxi+1is made given the stateSi+1. The process continues until the terminating stateSLhas been reached.We let the time point associated with stage i be ti, and the one-stage cost functiong(Si,xi,Si+1)denote the increment of project execution time if the set of activities,xi, is started at stateSi, and the system transits to stateSi+1. Precisely,g(Si,xi,Si+1)=ti+1−ti. The goal is to choose the best policy π among the set of policies Π, to minimize the expected project makespan over a finite number of stagesi={0,1,…,L}. The objective function or cost-to-go function of policy π starting from a state-stage pair(Si,i)can be written as:(5)Ji(Si)=E{∑j=iLg(Si,xiπ,Si+1)}That is, at any stage i one minimizes the expected project makespan, which is the completion time of the setxiπof activities just started at stage i following policy π, plus all the time taken to complete the unscheduled activities in the future stages. The cost-to-go function (5) can be computed through the following well-known recursion function of Bellman (1957):(6)xiπ=argminx⊂X(Si)E{g(Si,xi,Si+1)+Ji+1(SM(Si,x,Dj:j∈Ai))}Equation (6) states that the optimal policyxiπ, i.e. the optimal set of activities started at stage i, is a subset of the eligible set that minimizes the one-stage cost plus the cost-to-go function of the next stagei+1. Solving the exact recursion of (6) suffers the curse-of-dimensionality due to large state space and large combinatorial solution space for sequencing decisions. Several methods to obtain near-optimal solution to (6) based on ADP are developed next.The essence of ADP for solving an MDP is to replace the exact cost-to-go function with an approximation. ADP has its roots in neuro-dynamic programming (NPD) of Bertsekas and Tsitsiklis (1996) and RL of Sutton and Barto (1998). We refer to Si, Barto, Powell, and Wunsch (2004) and Powell (2007) for a comprehensive coverage on ADP and its applications in various settings.Two general approximation paradigms have been successful for solving high-dimensional MDPs. The first is to work directly on the cost-to-go function, and to replace it with an alternative functional form that is computationally tractable. The resulting sub-problem to be solved in each stage reduces to a deterministic optimization problem with the approximated objective function and the set of constraints corresponding only to the current stage. This paradigm works well for problems having a special structure amenable to math programming methods such as linear programming, integer programming or network optimization. For instance, linear and piece-wise linear approximation architectures have been used to solve the class of dynamic resource allocation problem (cf. Topaloglu & Powell, 2006; Simao, Day, George, Gifford, Nienow, & Powell, 2009), for which the deterministic sub-problem at each stage has the unimodularity property, and thus can be well handled by network optimization methods.A second approximation paradigm is the rollout policy proposed by Bertsekas et al. (1997) for combinatorial optimization, either deterministic or stochastic. At each state, a rollout policy replaces the exact cost-to-go function by some heuristic base policy, and is used to make a decision at the current state. A rollout policy can be viewed as a heuristic version of the policy iteration algorithm for DP. We refer to Bertsekas et al. (1997) and Bertsekas and Castanon (1999) for a formal description of general rollout framework, and to Bertsekas (2013) for an updated coverage. The rollout algorithms are attractive for problems that do not possess a special structure, such as a wide range of NP-hard combinatorial optimization problems, for which effective and efficient solutions are often offered by problem-specific heuristics or metaheuristics (Glover & Kochenberger, 2005). The rollout policy based algorithms have been successfully applied to a variety of stochastic optimization problems including quiz scheduling (Bertsekas & Castanon, 1999), stochastic vehicle routing (Secomandi, 2001) and revenue management (Bertsimas & Popescu, 2003).For the SRCPSP considered in this paper, the rollout policy is our choice because it is well-known that a deterministic RCPSP is well handled by a variety of heuristic methods (Kolisch & Hartmann, 2006). Promising results for the deterministic RCPSP have been reported by Xu, McKee, Nozick, and Ufomata (2008) and Li (2009). Next we establish the theoretical soundness of the rollout policy for SRCPSP and describe a basic version of the rollout algorithm.We replace the optimal cost-to-go function valueJi+1(·)in (6) with an approximationJ¯i+1(·)provided by some heuristic base policy, then a heuristic decisionx¯iassociated with each stage is obtained by solving:(7)x¯i=argminx⊂X(Si)E{g(Si,xi,Si+1)+J¯i+1(SM(Si,x,Dj:j∈Ai))}Bertsekas et al. (1997) have shown that when the underlying base policy is sequential consistent, the rollout algorithm generates solutions with quality no worse than the base policy. This is known as the sequential improvement property. Following the general definition of sequential consistent in Bertsekas et al. (1997), we state its definition in the context of SRCPSP.Definition 1A heuristic is sequential consistent for SRCPSP if whenever it generates a feasible initial activity sequence(j,j1,…,jn+1)starting at j, it also generates the feasible sequence(j1,…,jn+1)starting at activity j1.It has been noted that many greedy algorithms with an inherent sequential feature are often sequential consistent (Bertsekas et al., 1997). We identify a class of priority-rule based heuristic that are sequential consistent for SRCPSP. Let the priority value of activity j be given by a scoring function h(j), based on certain priority rules, e.g. shortest processing time, minimum slack, etc. We now define a static priority rule as in Kolisch (1996).Definition 2A priority rule is static if h(j) does not change during the heuristic procedure.A priority rule can be embedded in a serial generation scheme (SGS; Kolisch, 1996), which iteratively schedules an activity to start in the order specified by the priority rule. We show that SGS with a static priority rule is sequential consistent for SRCPSP. However, many metaheuristic approaches for SRCPSP may not be sequential consistent.Lemma 1The serial-generation-scheme (SGS) procedure with a static priority rule is sequential consistent for the SRCPSP.Suppose that if SGS generates a feasible sequence(j,j1,…,jn+1)starting at j, it does not generate the feasible sequence(j1,…,jn+1)starting at activity j1. Since the priority values of all activities remain the same, this can only happen when the sequence(j1,…,jn+1)is time- or resource-infeasible, a contradiction.□LetRHhdenote the rollout algorithm (policy) based on the priority-rule heuristicHhwith scoring function h ( · ). The rollout policy for SRCPSP can be expressed as:(8)x¯i=argminx⊂X(Si)E{g(Si,xi,Si+1)+RHh(Si+1)},whereRHh(Si+1)denotes the cost-to-go function value at stateSi+1following heuristic policyRHhforward. At each stage, the base heuristicHhis used to evaluate candidate activities in the eligible setX(Si). The one(s) resulting in minimum expected project makespan, i.e. time to completexplus all the remaining unscheduled activities, is (are) selected to start.Before establishing the sequential improvement property of rollout policyRHhfor the SRCPSP, we need to introduce the terminating property ofRHh(Bertsekas & Castanon, 1999).Definition 3A rollout algorithm for SRCPSP is terminating if it is guaranteed to generate a complete and feasible sequence of activities starting from any activity.A rollout algorithmRHhfor SRCPSP is terminating.Since the underlying network is acyclic with a finite number of nodes, an activity is never repeated in a feasible sequence generated by the priority-rule heuristicHh. And the length of a feasible sequence is always equal to the number of activities in the project. Therefore,RHhfor SRCPSP is terminating.□We state and prove the sequential improvement property ofRHhfor SRCPSP.Proposition 1LetgRH(S0,Si)denote the random completion time of the activities in Ci when applying the rollout policy in (8) starting at the initial stateS0forward to stateSi. The following inequalities hold fori=1,…,L:RHh(S0)≥E{gRH(S0,S1)+RHh(S1)}⋯≥E{gRH(S0,Si)+RHh(Si)}⋯≥E{gRH(S0,SL)}Since the base heuristicHhuses a static priority-rule, it is sequential consistent (Lemma 1). AlsoRHh(S0)≥minx⊂X(Si)E{g(S0,x,S1)+RHh(S1)}=E{gRH(S0,S1)+RHh(S1)}, according to (8) and the definition ofgRH(S0,Si). Thus the proposition holds fori=1.By induction, assuming that it holds for i > 1, i.e.RHh(S0)≥⋯≥E{gRH(S0,Si)+RHh(Si)}, we need to show that it also holds fori+1. Following (8) again,RHh(Si)≥E{g(Si,x,Si+1)+RHh(Si+1)}. Then we have:RHh(S0)≥⋯≥E{gRH(S0,Si)+E{g(Si,x,Si+1)+RHh(Si+1)}}=E{gRH(S0,Si+1)+RHh(Si+1)}Thus the proposition holds fori+1. Since the rollout policyRHhis terminating (Lemma 2), we haveRHh(S0)≥⋯≥E{gRH(S0,SL)+RHh(SL)}. Note thatRHh(SL)at the terminal statesSLis zero as it involves starting the dummy end activity. Therefore, the entire series of inequalities hold.□Skipping the intermediate terms in the series of inequalities of Proposition 1, we haveRHh(S0)≥E{gRH(S0,SL)}, which establishes the following Corollary on the sequential improvement property of the rollout policy for SRCPSP.Corollary 1.1The expected makespan of schedule generated by the rollout policyRHhbased on a static priority-rule heuristicHhis no larger than that obtained byHhalone for SRCPSP.Due to high-dimensional nature of the SRCPSP, an exact evaluation of the expectation in (8) is computationally intractable. Therefore, we employ MC simulation to approximately evaluate the expected project makespan resulting from starting a setxof activities. This approach has been successfully applied in several open-loop Sim-Opt algorithms for SRCPSP (cf. Ballestin & Leus, 2009; Tsai & Gemmill, 1998). We let Ω be a set of MC sample activity durations. Each scenario ω ∈ Ω is a sample realization of activity durations, i.e.ω=[d0,d1,…,dn+1]. Thus the evaluation of the expectation becomes a deterministic procedure of applying the base heuristicHhto obtain project makespan for all |Ω| samples. LetHh(ω)be the makespan associated with the ωth scenario, then the expected makespan can be computed as∑ω∈ΩHh(ω)/|Ω|, which is an unbiased and consistent estimator of the true mean makespan, when the sample size |Ω| is large enough based on the law of large numbers. According to the computational study in Ballestin and Leus (2009), an accuracy level smaller than 1% of the standard deviation of the percentage error can be achieved with 1000 MC samples for the 120-task instances. The authors reported that a better solution quality can be achieved by using a sample size of 10 since more schedules can be scanned with the same budget of computational effort. The basic rollout algorithm to solve the MDP model of SRCPSP can be sketched in Fig. 2.The algorithm starts with initializing a stage counteri:=0, time countert:=0, initial completion set C0, initial active set A0, and the initial vectorsd¯iandτi. A sample Ω of activity durations is generated in Step 1.3 via MC simulation to evaluate a candidate set of activitiesx⊂X(Si). In principle, Step 1.4 involves examining every feasible subset ofX(Si). In our implementation, we examine each eligible activity one at a time, until no more activity can be started. One may also examine two- or multi-element subsets ofX(Si), which can be computationally intensive. In the basic rollout algorithm version, the evaluation is performed in Steps 1.4.1 and 1.4.2 by some priority rule based SGS heuristicHh. The best set of eligible activitiesx*, i.e. the one resulting the smallest expected makespan is updated in Step 1.4.3, and implemented in Step 1.5. The solution procedure reaches a new state when any activity is completed in Step 1.7. The rollout algorithm is terminated when all activities have been completed, o.w. it repeats Step 1.1.The entire rollout algorithm consists of three loops. The outmost main loop has the number of iterations bounded by n. The second loop in Step 1.4 executes δ iterations in general, where δ is bounded by the number of feasible subsets of the corresponding eligible set. The third loop in Step 1.4.1 is executed |Ω| times. The overall complexity of the basic rollout algorithm is given in Proposition 2.Proposition 2The time complexity of the basic rollout algorithm for SRCPSP is O(n3mδ|Ω|).It is well-known that the time complexity of an SGS priority rule heuristic is O(n2m), which will be executed a maximum of |Ω| times in Step 1.4.1. Since the outer loop has at most n iterations and the loop in Step 1.4 is executed at most δ times, the result holds.□Proposition 2 establishes the tractability of the rollout algorithm for SRCPSP. The basic version of the rollout algorithm in Fig. 2 can be improved in two ways. First, efficiency of the algorithm can be improved by reducing the number of MC samples. A sample size reduction method is presented in Section 3.2 to improve the speed of the basic rollout algorithm. Second, the priority-rule based SGS heuristic in Step 1.4.1 can be replaced by some more effective solution methods to enhance the quality of the overall algorithm. An augmented rollout algorithm with CP serving as the base heuristic is described in Section 3.3.We implement the limited simulation approach suggested by Bertsekas and Castanon (1999) to reduce the MC sample size in the rollout algorithm. It is similar to the well-known Latin hypercube sampling method (Tang, 1993) in that the entire sample space is sampled evenly by choosing an equal number of sample points for each random variable. The difference between the two is that an additional step in limited simulation is needed to calibrate the weights (relative importance) of each sample point, because now the small number of samples may not carry the same weight as in the typical MC simulation. The details of our implementation are elaborated below.We generatef=1,2,…,Fsamples of activity durations, where F is significantly less than |Ω| in the pure MC simulation. We assume that the cost-to-go function value can be estimated by the following linear regression architecture:(9)J˜i(Si,r)=r0+∑f=1FrfHf(Si),whereHf(Si)is the makespan obtained by executing the base heuristicHunder the sample f starting fromSi, andr=[r0,r1,…,rF]is a vector of the weights that encodes the aggregate effect of uncertain disturbances of f. To estimate the vectorrof coefficients, we record the dependent variableJ˜i(Si,r)as the certainty-equivalent project makespan solved by heuristicHat stateSi, and independent variablesHf(Si)assuming scenario f of durations for the unscheduled activities. Thenr^can be estimated by the least-square estimates in the standard multi-variable linear regression.To calibrate the sample size F of limited simulation, we randomly generated small size instances with fewer than 14 activities, for which optimal deterministic solutions can be easily found by CP. The solution quality can be evaluated using the gap of the ADP solution between the average makespan of deterministic solution of each scenarios. Details of this experiment have been documented in Li (2011). Fig. 3shows that the average gap decreases as the number of features in (9) increases. The curve tends to flatten with increased features, suggesting diminishing returns to additional features. To balance quality and computational effort, we have chosen F to be 3 in our implementation of ADP with limited simulation.It is well-known that a single-pass of priority-rule based heuristic may often not perform well for a deterministic RCPSP. To enhance the quality of the base policy (heuristic), we employ CP to handle each sub-problem during the ADP algorithm. Specifically, the priority rule base heuristicHhis now replaced by a CP procedure in Step 1.4.1 of Fig. 2 to evaluate each candidate eligible activity.CP is a solution methodology originated in the AI area (Marriott & Stuckey, 1998). With its efficient domain reduction (constraint propagation) for scheduling problems (Baptiste et al., 2001), CP has proved to be successful for scheduling problems including RCPSP (cf. Dorndorf, Pesch, & Phan-Huy, 2000; Li & Womer, 2009). Thus it is a natural choice for serving as the base heuristic in the rollout framework.We employ the time-table and disjunctive constraint propagation of Baptiste et al. (2001) and Brucker (2002) to reduce the domain of activity starting times whenever the domain of related activities is modified. Let [ESj, LSj] be the time window of j, where ESjand LSjrepresent the earliest and latest start of j, respectively. The time-table constraint propagation repeatedly modifies [ESj, LSj] by maintaining the following inequality:(10)∑j∈V:j.start≤t<j.endrjk≤Rk∀t,kThe disjunctive constraint propagation introduces new disjunctive relationships for any pair of activities (i, j) whose requirement for resource k exceeds its available capacity, Rk. That is, for any (i, j) and k such thatrik+rjk>Rk, the following disjunctive constraints are imposed:(11)i.end≤j.startorj.end≤i.startOur computational experience shows that (10) and (11) are able to achieve satisfying domain reductions with reasonable computational efforts. Since constraint propagation alone is often not able to reduce the domain of each decision variable to a singleton, search is needed in CP. LetXbe the set of eligible activities whose start and end times have not been fixed. A depth-first search used in our implementation can be sketched in Fig. 4.Two activity selection rules are considered at Step 3.1., similar to those used in Dorndorf et al. (2000).Rule-1: Among the eligible activities in Γ having the minimal earliest start times, it chooses one having the minimal earliest end time.Rule-2: Among the eligible activities in Γ having the minimal earliest start times, it chooses one having the maximal earliest end time.To balance solution quality and computational effort, some search limit, e.g., number of nodes in the search tree, can be imposed for the CP search.The main drawback of a pure rollout look-ahead ADP algorithm is that the computational burden of the embedded pure MC simulation can be prohibitively high. Although some sample reduction techniques such the one presented in Section 3.2 may alleviate this issue, its performance often depends on quality of the limited simulation.In this section, we exploit an alternative approximation scheme and devise a new approximation architecture. Consider the lookup table approach widely used in RL (Sutton & Barto, 1998), where the value of a state-decision pair(S,x)is learned through randomly generated MC samples in the following way:(12)J˜(S,x)=∑f=1N(S,x)Hf(S,x)N(S,x)The relationship in (12) states that the expected makespanJ˜(S,x)of starting activity x at state S can be estimated as the average of makespanHf(S,x)obtained by some base heuristicHexecuted in scenario f. The denominatorN(S,x)is the number of times (scenarios) that the state-decision pair(S,x)has been visited. According to the law of large numbers, asN(S,x)increases the estimateJ˜(S,x)approaches its true value. A lookup table is maintained to keep a record of evaluation for each pair, which is then used in the solution process to evaluate a state-decision pair at each stage. Approximation through the lookup table can be viewed as a look-back approach, as the estimates are based on the experience learned and the history simulated through MC samples.Empirically, the magnitude ofN(S,x)depends upon the project network structure and random activity durations, which may or may not satisfy the law of large numbers. To address the issue of potentially low number of visits, we have aggregated the states in the lookup table, such that a state record is identified by the set of completed activities C plus the set of active activities A, without information about the executed activity durationsd¯, or the activity start timesτ. Our computational experiment shows that such aggregation scheme performs well for the tested benchmark instances.The use of a lookup table in the solution process is extremely efficient, as all one has to do is to check the memory for a(S,x)pair in each stage. When both the state and solution spaces are of low dimension, every state-decision pair can be evaluated a sufficient number of times in the lookup table through a large number of MC samples. For our MDP model of SRCPSP with high-dimensional state and solution spaces, it is often not possible to visit every state-decision pair, or the computational effort of doing so is prohibitively high.This has motivated us to integrate the look-back and look-ahead methods to approximate the value of state-decision pairs. The theoretical foundation of this integration shares the idea of employing multiple base heuristics, or parallel rollout, proposed by Bertsekas (2013). The approximated valueJ¯i+1(S,x)now becomes:(13)J¯i+1(S,x)={Hh(S,x),ifJ˜(S,x)=+∞J˜(S,x),o.w.If the pair(S,x)has not been visited in the lookup table with its look-back approximationJ˜(S,x)being+∞,J¯i+1(S,x)relies on the approximation through the look-ahead heuristicHh; o.w.J¯i+1(S,x)directly retrievesJ˜(S,x)in the lookup table, assuming it offers an accurate estimate of the true value of(S,x)as in (12). We name the new algorithm framework, the ADP with HBA approximation architecture, or simply ADP–HBA. Fig. 5 sketches the ADP–HBA algorithm.An off-line training procedure is conducted in Step 1 to generate the lookup table. This step is only required one time for the instance at hand. Once the trained lookup table is obtained, it can be used in the main solution process. Compared with the basic rollout algorithm in Fig. 2, the burden of a pure MC simulation to evaluate a state-decision pair has been alleviated through the use of lookup table approximation in Step 3.3.2. On the other hand, the rollout procedure in Step 3.3.3 overcomes the issue of missing state-decision pair in the lookup table.The offline training phase consists of solving scenarios generated by MC simulation. Each instance is solved by a rollout procedure for the deterministic RCPSP developed by Li (2009). During the solution process, a record of each visited state-decision pair is maintained and updated. Fig. 6 elaborates the offline training procedure called by ADP–HBA in Step 1.Note that CP based methods are again used as the base policy in Step 2.3.1 of the training procedure. It has been shown that a rollout algorithm embedding CP as the base policy provides a significant improvement over the one with a priority-rule based policy (Li, 2009).The purpose of this computational study is to examine the performance of our ADP algorithms in both solution quality and speed, and to draw insights about the algorithm behavior in different problem domains. In Section 5.1 we first consider a numerical example, one of the 110 Patterson RCPSP benchmark instances (Patterson, 1973) used by Tsai and Gemmill (1998), to demonstrate how a closed-loop ADP solution functions for SRCPSP. Then in Sections 5.2 and 5.3, we present results on the PSPLIB benchmark instances (Kolisch & Sprecher, 1997).Consider an SRCPSP instance of 11 activities (with 2 dummies) and 3 resources with its network structure depicted in Fig. 7. The number above a node represents its mean activity duration (m), and the tuple below a node specifies its resource requirement of three resources A, B, C. For instance, Activity 3 has a mean duration of 6 and requires 3 units of type A, 1 unit of type B and 2 units of type C resource. The availability of type A, type B and type C resources is 6, 7, and 6 units, respectively. Following Tsai & Gemmill (1998), it is assumed that each activity's optimistic duration (a) is 80% of its mean duration, and its pessimistic duration (b) is 150% of its mean duration. Then the random duration of each activity is assumed to follow a beta-distribution approximated by the three PERT estimates a, m and b (Badiru, 1991).The lookup table obtained from the training phase on 100 MC training samples is shown in Table 1. It contains a total of 52 records, each of which shows the expected value (makespan) of a state-decision pair and the number of times each pair has been visited during the MC simulation. For instance, for the state in which activity 3 (for simplicity we call it A3) finishes and A8 is started, the value (cost) of starting A4 is 20. This state-decision has been visited 12 times, i.e. the expected value 20 is the average of 12 evaluations. The average number of visits for all state-decision pairs is about 22, with a standard deviation of 27. Let one random scenario be described by the following vector of activity durations [3, 4, 7, 2, 4, 2, 4, 4, 4, 1, 3]. For this scenario, the HBA–ADP algorithm is able to obtain an optimal solution with a makespan of 20.The solution process can be illustrated by Fig. 8. At time 0, both A1 and A2 are started. When A1 finishes (and A2 is active) at time 3, both A3 and A4 are eligible, but only one of them can start due to resource constraints. The lookup table is checked first to evaluate A3 and A4, and a match is found to give each an evaluation of 19 (highlighted in bold in both Fig. 8 and Table 1). We arbitrarily choose A3 to start. When A2 finishes (and A3 is active) at time 4, only A7 is eligible to start. Next when A7 finishes (and A3 is still active) at time 8, A8 is the only one to start. When A3 finishes at time 10 (with A8 being active), both A4 and A9 are eligible to start. The lookup table provides an evaluated cost of 20 for A4 and 22 for A9, therefore, A4 is chosen to start. When both A4 and A8 finish at time 12, A5, A6 and A9 are all eligible to start. Since no match is found in the lookup table, a rollout procedure is employed to provide an evaluated cost of 19 for A5, 21 for A6 and 19 for A9. Thus both A5 and A9 are started, but A6 is delayed. When both A5 and A9 finish at time 16, both A6 and A10 can start. Finally when A10 finishes at 17, the last activity A11 starts and the whole project completes at time 20. Since the instance is small enough, we can easily verify that 20 is the optimal makespan.Our ADP–HBA algorithm is executed to solve 100 randomly generated scenarios using MC simulation, the same sample size as in Tsai and Gemmill (1998). As shown in Table 2, its solution has an average makespan of 20.13 with a standard deviation of 1.31, which is an improvement of 7.26% over the solution (21.706) found by the TS based open-loop approach with about 3 s on average (Tsai & Gemmill, 1998). After 6.54 s of training phase, it takes the ADP–HBA 0.08 s on average (with standard deviation of 0.17 s) to solve an instance. Each solution process evaluates nine state-decision pairs on average. Over 90% of the evaluations are performed by lookup table (look-back).We test the ADP–HBA algorithm on three sets of PSPLIB instances with 30, 60, and 120 tasks. The assumptions about stochastic task durations follow those in the literature (cf. Ballestin & Leus, 2009; Stork, 2001). Table 3summarizes the type, support, variance and shape of the probability distributions assumed.d¯refers to the deterministic task duration. The exponential distribution (Exp) has the largest variance, while U1 and B1 have the same smallest variance, and U2 and B2 have the same intermediate variance. The assumption for beta-distribution always results its shape to be right-skewed.Based on the empirical findings of Li (2009), the embedded CP procedures are configured with Rule-2 described in Section 3.3 for activity selection. To balance solution quality and computational effort of ADP–HBA, we set the training phase to consist of 100 training samples for the 480 30-task instances, 50 samples for the 480 60-task instances and 25 samples for the 600 120-task instances; the rollout look-ahead procedure in its solution process has 50 evaluation samples for the 30-task instances and 10 samples for both the 60-task and 120-task instances. The quality of makespan found for each instance is evaluated by computing the gap between the critical path method (CPM) based lower bound, or LB-CPM. The LB-CPM is obtained by finding the makespan of the deterministic instance without considering the resource constraint. This approach has been used by Ballestin and Leus (2009) among others to evaluate solution quality. All computations are performed on a PC with Intel 12 Core i7 3.2 GHz CPU and 16 G RAM.To examine the benefit of using CP as the base heuristic, Fig. 9compares the performance of our ADP–HBA with CP and the shortest-processing-time (SPT) SGS procedure as the base heuristic for solving the 480 30-task PSLIB instances. It is evident that the CP base heuristic version consistently outperforms the SPT base heuristic for all five probability distributions. The training phase of the CP base heuristic version is able to visit more state-decision pairs, which makes the training more effective. In all our subsequent computational experiments, we only test our best configured ADP–HBA algorithm with CP being the base heuristic.The overall performance of ADP–HBA is summarized in Table 4. More variance in task durations makes an instance have higher average gap. For each set of test instances, the Exp distribution always results in the largest gap given its large variance. Notably, the ADP–HBA solutions appear to have smaller gap for B1 and B2 distributions. The ADP–HBA is also computational tractable. For the largest 120-task instances, it spends about an hour on average to train the lookup table and an average of less than 6 min to solve an instance.Additional information about the solution process of ADP–HBA is collected and presented in Table 5. The total number of records in the lookup table grows rapidly with the problem size, and so does the total number of state-decision evaluations per instance. As problem size becomes large, ADP–HBA relies more on the rollout approximation than the lookup table approximation, since there is less chance to match a state-decision pair when the problem size becomes large.We also consider the situation where all five probability distributions co-exist in one scenario for the 480 30-task and 480 60-task instances, and report the results in Table 6. Each task in an instance is randomly assigned with one of the five probability distributions. Interestingly, solution quality measured by the average gap is somewhere between the lowest and highest gap in the single-pdf case in Table 4. The mixed pdfs do not necessarily make an instance harder to solve, because the computational time (training and solving) and the measures shown in Table 6 are all comparable with the single-pdf case.To further understand the behavior of the ADP–HBA algorithm, we examine the impact of several problem and solution characteristics on the algorithm performance in terms of quality and speed. The following four factors are considered in our analysis.x1—The percentage gap between the best upper bound (UB) and the best lower bound (LB) for the corresponding deterministic instance. This measure accurately quantifies the difficulty of solving the deterministic counterpart of an SRCPSP instance. Since our ADP algorithms rely heavily on solving a deterministic sub-problem in each iteration, we hypothesize that a more difficult deterministic counterpart is associated with a more challenging stochastic instance.x2—The total number of records in the lookup table per instance. This measure can serve as an indicator of the training phase effectiveness.x3—The total number of evaluations during the solution process for each instance. This measure may indicate the complexity of a stochastic instance.x4—The percentage of evaluations obtained by rollout look-ahead approximation.A simple linear regression model below is used for the analysis:(14)y=b0+b1x1+b2x2+b3x3+b4x4Three dependent variables are considered for y: the average gap between the ADP–HBA solution and LB-CPM, measuring the quality of ADP–HBA solution, the average training phase time in CPU, and the average solution process time in CPU.The regression results on a subset of 120-task instances with the U1 probability distribution are summarized in Table 7. The numbers in parenthesis are standard errors. All four factors are significant at the 0.01% level in all three regressions. The quality of ADP–HBA solution can be well explained by the four factors with an adjusted R-square of 79.8%. It is positively affected by the difficulty of solving the deterministic counterpart (x1). This finding justifies the need for using more effective deterministic solution methods. A more effective training phase (x2) yields better solution quality. The gap increases if more state-decision pair evaluations (x3) are needed for solving each stochastic instance. More rollout evaluations in the solution process tends to improve solution quality, indicating that the quality of rollout evaluations in ADP–HBA outperforms that of the look-up table evaluations.The regression results show an even higher adjusted R-square (over 90%) for the computational speed. The difficulty of solving the deterministic counterpart significantly increases the time of both the training phase and the solution process. Visiting more state-decision pairs requires more time for the training phase, but less time for the solving process. More rollout evaluations in the solution process requires more time to solve an instance.Wefirst compare our two versions of ADP algorithms with the GRASP algorithm of Ballestin and Leus (2009) in Table 8. ADP-Rollout-LS stands for the ADP with a pure rollout look-ahead policy and limited simulation. The GRASP algorithm with 25,000 evaluations of task sequence always finds better solutions than the one with 5000 evaluations. Our ADP-Rollout-LS algorithm outperforms both versions of GRASP by a significant margin for the Exp and B2 distributions, but is not as good as the GRASP-25000 for U1, U2 and B1. The ADP–HBA algorithm performs better than ADP-Rollout-LS, which shows that the look-up table offers better quality of approximation than that of sample reduction through limited simulation. ADP–HBA also achieves the best solution quality for four out of five cases, and is very competitive with GRASP-25000 on the U2 case. Notably, for instances with the Exp distribution (the one having the largest variance) ADP–HBA has an average gap of 30% lower than GRASP-25000. It also performs better on average for two other non-symmetric pdfs: B1 and B2. A direct comparison with GRASP in computational time is not available as only the number of evaluations has been reported by Ballestin and Leus (2009).Table 9 provides a more comprehensive comparison with the current best algorithm in the literature, the pre-processing with two-phase GA (PPGA) algorithm of Ashtiani et al. (2011). PPGA-5000 and PPGA-25000 stand for the PPGA algorithm with 5000 and 25,000 evaluations, respectively. The lowest average gap between LB-CPM for each category of instances is highlighted in bold. Our ADP–HBA algorithm finds the lowest gap for all U1, B1 and B2 pdfs. The PPGA-25000 performs consistently better on the U2 pdf. A mixed result is observed for the Exp pdf: ADP–HBA performs better for the 30-task and 60-task instances with Exp pdf, where PPGA-25000 performs better on the 120-task instances with Exp pdf. To test whether the solution quality difference is statistically significant, we perform a paired-sample t-test and report the test statistic in the last column of Table 9. All the tests are statistically significant at confidence level over 99.9%. As for computational speed, the PPGA algorithm spends significantly less time than ADP–HBA, especially for the largest 120-task instances.The results in Table 9 suggest a tradeoff between solution quality and efficiency. With the iterative sequential solution paradigm, our ADP–HBA appears to perform particularly well for situations where the random activity duration follows non-symmetric probability distributions with higher variance, at the price of more computational effort.In this paper, we have developed computationally tractable closed-loop algorithms to obtain near-optimal solutions for the project scheduling problems with both resource constraints and stochastic task durations, called SRCPSP. We show that a static priority-rule based scheduling heuristic is sequential consistent for the addressed problem, which lays the theoretical foundation for a rollout policy to effectively approximate the cost-to-go function.To reduce the burden of a pure Monte Carlo simulation in the rollout framework, we have integrated a lookup table approach with the rollout policy to form a new approximation architecture in ADP. While the rollout policy is a look-ahead approach that evaluates a state-decision pair from the current stage into the future, the lookup table approach looks back into the history simulated by MC samples. This HBA architecture is superior to the lookup table approach alone as some state-decision pairs may not be visited due to high-dimensional state and solution spaces; it is also more efficient than the pure rollout approach, since for state-decision pairs that have been visited in the lookup table it is much faster to estimate their value. To improve the quality of the base policy, hence the quality of the whole rollout procedure, we build in CP to serve as the base policy for handling the scheduling sub-problem in each algorithm iteration.A comprehensive computational study has been conducted to examine the quality and efficiency of the hybrid ADP–HBA algorithm. Computational results on the 120-task PSPLIB instances show that the ADP–HBA is competitive with the current best algorithms in the research literature for solving SRCPSP. Due to its dynamic and adaptive nature, ADP–HBA performs particularly well for instances with non-symmetric probability distribution of task durations.Our work offers several insights to project management practitioners. First, the ADP solution paradigm is applicable for scheduling large-scale projects in production, construction, service and military industries, which often last for months and even years. In such scheduling environments, project schedules are often being made in a dynamic and adaptive fashion, and tasks are executed sequentially, which fits the ADP paradigm. At the initiation of the project, project managers will collect historical data to come up with some probability distribution of project tasks. Then the training phase will evaluate state-decision pairs in the MDP model for SRCPSP. Due to the long-term nature of the project in interest, it is usually acceptable to run the one-time training phase in hours. After the project is started, whenever a task is completed a decision point (stage) in the MDP model is reached, which triggers the decision of which tasks to start next. Second, an advantage of using such sequential and dynamic scheduling paradigm is the ability to incorporate information observed during project execution in scheduling decision. This is especially beneficial when facing task durations with non-symmetric and high variation probability distributions. Third, integration of solution methods in scheduling, simulation and artificial intelligence will enhance the solution quality and efficiency of an ADP approach.Our ADP–HBA algorithm establishes a new computational framework to integrate techniques in optimization and artificial intelligence for tackling SRCPSP. It also opens an avenue of research opportunities to further improve its performance. Our current implementation of the lookup table is quite fundamental. There are a number of techniques in machine learning to improve the effectiveness of lookup table. Another direction of research may focus on improving the efficiency of ADP–HBA, so that it may also be attractive for projects having very short makespans, where computational speed is more important. Finally, from the application perspective, our ADP–HBA framework is general and flexible enough to handle other stochastic scheduling problems, e.g. the job shop, flow shop problems in machine scheduling, that are either special cases or variants of the SRCPSP.

@&#CONCLUSIONS@&#
