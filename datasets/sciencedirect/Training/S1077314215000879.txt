@&#MAIN-TITLE@&#
Reweighted sparse subspace clustering

@&#HIGHLIGHTS@&#
We address the problem of subspace clustering.A reweighted scheme is introduced to the existing SSC algorithm.The proposed RSSC algorithm achieves lower clustering error than LSR, SMR, LRR and SSC algorithms.

@&#KEYPHRASES@&#
Subspace clustering,Sparse representation,ℓ1 minimization,Compressed sensing,Iterative weighting,Convex programming,Spectral clustering,Motion segmentation,Non-rigid motions,Human face clustering,

@&#ABSTRACT@&#
Motion segmentation and human face clustering are two fundamental problems in computer vision. The state-of-the-art algorithms employ the subspace clustering scheme when processing the two problems. Among these algorithms, sparse subspace clustering (SSC) achieves the state-of-the-art clustering performance via solving a ℓ1 minimization problem and employing the spectral clustering technique for clustering data points into different subspaces. In this paper, we propose an iterative weighting (reweighted) ℓ1 minimization framework which largely improves the performance of the traditional ℓ1 minimization framework. The reweighted ℓ1 minimization framework makes a better approximation to the ℓ0 minimization than tradition ℓ1 minimization framework. Following the reweighted ℓ1 minimization framework, we propose a new subspace clustering algorithm, namely, reweighted sparse subspace clustering (RSSC). Through an extensive evaluation on three benchmark datasets, we demonstrate that the proposed RSSC algorithm significantly reduces the clustering errors over the SSC algorithm while the additional reweighted step has a moderate impact on the computational cost. The proposed RSSC also achieves lowest clustering errors among recently proposed algorithms. On the other hand, as majority of the algorithms were evaluated on the Hopkins155 dataset, which is insufficient of non-rigid motion sequences, the dataset can hardly reflect the ability of the existing algorithms on processing non-rigid motion segmentation. Therefore, we evaluate the performance of the proposed RSSC and state-of-the-art algorithms on the Freiburg-Berkeley Motion Segmentation Dataset, which mainly contains non-rigid motion sequences. The performance of these state-of-the-art algorithms, as well as RSSC, will drop dramatically on this dataset with mostly non-rigid motion sequences. Though the proposed RSSC achieves the better performance than other algorithms, the results suggest that novel algorithms that focus on segmentation of non-rigid motions are still in need.

@&#INTRODUCTION@&#
In many real applications, high-dimensional data in several classes or categories can be respectively represented by corresponding low-dimensional subspaces. For example, motion trajectories of multiple rigidly moving objects in a video [1], face images of different subjects under varying illumination [2] all lie in low-dimensional subspaces of the ambient high-dimensional space. Subspace clustering refers to the task of separating the high-dimensional data into multiple low-dimensional subspaces according to their latent common patterns being recognized. Specifically, for a collection of{yi}i=1npoints inRm,lying in a union of L subspaces,{Sj}j=1Lof dimensions{dj}j=1L,while which points belong to which subspaces are unknown. The goal of subspace clustering is to identify the clustering of data so that points in the same cluster belong to the same subspace and find the parameters of each subspace. Such a model is an extension of the single subspace model found in many papers [3,4]. A more detailed definition of subspace clustering problem can be found in [5].Subspace clustering has numerous applications in computer vision and image processing, e.g., motion segmentation [6,7] and face clustering [8,9]. The motion segmentation problem refers to segmenting the motion trajectories of different objects from tracked points in video sequences which are captured by a static or moving camera [10,11]. The face clustering problem refers to clustering the face images of multiple subjects according to their face images acquired with a fixed pose but varying illumination. Recently, the subspace clustering problem has drawn attention of researchers in compressed sensing, which is a hot research area in information science [12,13].Numerous subspace clustering algorithms have been proposed in the past [6,9,10,14–29]. According to the mathematical framework they employ, existing subspace clustering algorithms can be divided into three main categories: algebraic, statistical, and spectral clustering [9].The algebraic algorithms solve the subspace clustering problem by modeling a subspace as a gradient of a polynomial [11,14]. These methods do not require prior information of each subspace, and can enforce structural restriction on the subspaces. Shape interaction matrix (SIM) [6] and generalized principal component analysis (GPCA) [14] are two classical methods which belong to this category. Though having a lot of advantages, the algebraic algorithms are hard to avoid exponentially expensive computations due to the polynomial fitting. Furthermore, these algorithms are generally sensitive to noise and outliers and unable to resolve the difficulty of clustering points near the intersection of subspaces, and have exponentially complex computation with respect number and dimensions of subspaces [9,22].The statistical algorithms probabilistically model each subspace as a Gaussian distribution, and consider the clustering problem as an estimation of mixture of Gaussian. Specific algorithms include agglomerative lossy compression (ALC) [10], mixture of probabilistic PCA (MPPCA) [15], multi-stage learning (MSL) [16], and the robust method known as RANSAC [17]. These algorithms typically require prior information of the subspaces, such as the number of subspaces and their dimensions. The computational complexity of the above mentioned algorithms is also exponential with respect to the number of subspaces and their dimensions [9,22].The spectral clustering algorithms use local information around each data point to build a similarity between pairs of points. The clustering of data points is achieved by applying spectral clustering to the affinity matrix. Local subspace affinity (LSA) [18], spectral local best-fit flats (SLBF) [19], locally linear manifold clustering (LLMC) [20], and spectral curvature clustering (SCC) [21] are methods of this class. They cannot deal well with points near the intersection of two subspaces if the neighborhood of this point contains points from different subspaces. Inspired by the emerging field of compressed sensing (CS) [12,13], the sparse subspace clustering (SSC) algorithm [9,22] solves the clustering problem by seeking a sparse representation of data points used as a dictionary. Hence, by resolving all the sparse representations for all data points and constructing an affinity graph, SSC automatically finds different subspaces as well as their dimensions from a union of subspaces. Finally, the subspace clustering is performed by spectral clustering [30]. In addition, as ℓ1-norm minimization is convex and needs at most polynomial time in complexity, SSC deals well with the data. A robust version of SSC to deal with noise and corruptions or missing observations is also given in [9,22]. Instead of finding a sparse representation, the low-rank representation (LRR) algorithm [23,24] poses the subspace clustering problem as finding a low-rank representation of the data over the data itself. Then, Lu et al. proposed a method based on least squares regression(LSR) [28] which takes advantage of data correlation and groups highly correlated data together. The grouping information can be used to construct an affinity matrix which is block diagonal and can be used for subspace clustering through spectral clustering algorithms. Recently, Lin et al. analyzed the grouping effect deeply and proposed the smooth representation framework (SMR) [29] which also achieves state-of-the-art performance in subspace clustering problem. Different from SSC, the LRR, LSR and SMR algorithms use normalized cuts [31] in the spectral clustering step.Currently, SSC, LRR, LSR and SMR achieve state-of-the-art performance on subspace clustering problem than the other methods [6,10,14–21,25–27]. This can be demonstrated by the clustering performance of these methods on benchmark datasets such as the Hopkins155 dataset [7] and Extended Yale B dataset [8]. However, the performance of these four state-of-the-art algorithms [9,22–24,28,29] on motion segmentation of non-rigid moving objects is not thoroughly revealed as the Hopkins155 dataset [7] has its bias [32]. This dataset only includes a few non-rigid motion sequences. On the other hand, the performance of these algorithms on face clustering problem still has a large space to improve.In our work, we will first theoretically demonstrate the advancement of reweighted ℓ1-norm minimization framework over ℓ1 minimization. From recent work [33–37] in the compressed sensing field, we speculate that the performance of SSC [9,22] can be largely improved if we use iterative weighting (i.e., reweighted) ℓ1 minimization framework instead of ℓ1-norm minimization. The improvements can be applied into many subspace clustering problems (motion segmentation, face clustering, and face recognition [9,22–24,38]). Our main contributions are twofold. First, through a series of experiments on the Hopkins155 dataset [7] and Extended Yale B dataset [8], we demonstrate that our method largely reduces the clustering errors with little additional computational cost. Second, we found the limitation of the representation-based methods in subspace clustering, especially in non-rigid motion segmentation. We test the state-of-the-art algorithms [9,18,21,24,28,29]] and our proposed RSSC on a different motion segmentation dataset: the Freiburg-Berkeley Motion Segmentation Dataset [32,39] (for more details, please see the experiments section). We divide this dataset into two parts: rigid motion and non-rigid motion. All these algorithms will get good performance on the part of rigid motions while achieve high clustering errors on the part of non-rigid motions, though our method will achieve the lowest clustering error on this dataset.The paper is organized as follows: Section 2 briefly introduces the ℓ1 minimization framework and reweighted ℓ1 minimization framework. Section 3 introduces the SSC algorithm and the proposed RSSC algorithm. Section 4 presents our experimental results on the Hopkins155 dataset [7], the Freiburg-Berkeley Motion Segmentation Dataset [32,39], and Extended Yale B database [8]. Section 5 concludes this paper and discuss some future work.The use of ℓ1 minimization can be traced back to the year 1973 [40]. It is first applied in reflection seismology [40–42]. After that, the nature of sparsity of the ℓ1 minimization was confirmed and it began to be used in signal recovery [43,44] and image processing [45]. The famous LASSO algorithm [46] and Basis Pursuit [47] are just two of numerous applications of the ℓ1 minimization. Due to its wide applicability, the ℓ1 regularization is considered the “modern least squares”. For more details of the progress of the ℓ1 minimization, please refer to [37].In mathematics, many problems in signal recovery and image processing need to identify the sparsest solution of an underdetermined linear system. For example, given an m × n matrixAwith m ≤ n and a nonzero vectorb∈Rm,to find a sparsest solution ofAx=bequals to solving the ℓ0 minimization problem(1)(P0)minx∈Rn∥x∥0s.t.Ax=b,where ‖x‖0 is the number of nonzero components ofx. Since the problem (P0) is a nonconvex and NP hard optimization problem [48], approximate solutions are considered instead. In the past decade, several greedy pursuit algorithms have been proposed, such as the Matching Pursuit (MP) [49] and the Orthogonal Matching Pursuit (OMP) algorithms [50]. Another pursuit algorithm is the Basis Pursuit (BP) [47] which suggests a convexification of the problems by replacing the ℓ0-norm with ℓ1-norm. The focal underdetermined system solver (FOCUSS) [51] shares a very similar idea. It uses the ℓp-norm (p ≤ 1) as a replacement for the ℓ0-norm. Lagrange multipliers are used to convert the constraints into a penalty term and an iterative method is derived based on the idea of iterated reweighed least squares that handles the ℓp-norm as a weighted ℓ2-norm.By performing a convex analysis, it is reasonable to minimize the convex envelope of ‖x‖0 instead of ‖x‖0. In [52], Jojic et al. had proved that the convex envelope of ‖x‖0 is exactly the ℓ1 norm, so we can consider the convex problem(2)(P1)minx∈Rn∥x∥1s.t.Ax=b,where∥x∥1=∑i=1n|xi|. The ℓ1 minimization is convex and can be solved efficiently via convex programming tools [53,54]. You may ask that, whether there exists an alternative to the ℓ1 minimization which is also convex and finds the correct solution but its performance is better than the ℓ1 minimization’s? The answer is yes.In [37], Candès et al. proved via plenty of experiments that, by wisely weighting the ℓ1 norm and iteratively updating the weights, the recovery performance of the ℓ1 minimization framework can be largely enhanced. How this happens? Let us look into the key difference between the ℓ1 norm and ℓ0 norm. By the definitions of the ℓ1 norm and ℓ0 norm, larger coefficients are penalized more heavily than smaller coefficients in ℓ1 norm while the ℓ0 norm treats them equally. It is the dependence on the magnitude of the coefficients that explains the nature difference between the ℓ1 norm and the ℓ0 norm [37]. To eliminate the dependence, we need to discuss a weighted ℓ1 minimization problem which penalizes more impartially than the standard ℓ1 norm(3)(PW1)minx∈Rn∥Wx∥1s.t.Ax=b,whereW∈Rn×nis the diagonal matrix with weights on the diagonal and zeros elsewhere. If the weights are w1, …, wnand the elements in vectorxare x1, …, xn, the problem (PW1) equals to(4)(PW1)minx∈Rn∑i=1nwi|xi|s.t.Ax=b.The above problem (PW1) can be viewed as a relaxation of the weighted ℓ0 minimization problem(5)(PW0)minx∈Rn∥Wx∥0s.t.Ax=b.If the weights do not have any zeros, the solution of the problem (P0) is also the solution of the problem (PW0). However, the solutions of the relaxed problems (P1) and (PW1) are different in general. The key difference between the ℓ1 minimization and the weighted ℓ1 minimization is the weighting matrix, i.e.,W.Take a simple example for illustration, assumeb=Ax=[131313]Tand(6)A=[100−130101300113].The solution of the problem (P0) isx0=[23001]T,while the problem (P1) finds a wrong solutionx1=[1313130]T. If we introduce a weighting matrixW= diag([1 3 3 1]T), the problem (PW1) and (PW0) coincidently find the correct solutionxW1=xW0=x0=[23001]T. In fact, the weighting matrix is not unique, for any positive matrix, the problems (PW1) and (PW0) will find the correct solution as long asw1+3w4<w2+w3.So we can imagine that, if we set theWwisely, the solution of the problem (PW1) is more sparse than the solution of (P1), and therefore results in a solution that is more approximate to the solution of the problem (P0), the optimal one is that we seeks for. However, this raises another question: how to set the weighting matrix so that it improves the sparsity of the solution of the underdetermined linear equationAx=b? By intuition, we know that larger wiwill discourage the penalty magnitude of xito be smaller or even zero while smaller wiwill encourage the penalty magnitude of xito be larger and nonzero. As a rough rule of thumb, the absolute value of weights should be inversely proportional to the value of the corresponding elements of solution [37]. But if we do not know the exact solution ofx, how can we know the suited weighting matrix? Meanwhile, if we cannot set a suited weighting matrix, how can we obtain the correct solution ofx? This is really a chicken-and-egg situation.The reweighted framework has found applications in several research projects. As earlier in [51], Gorodnitsky and Rao proposed an iterative algorithm, i.e., FOCUSS, for finding sparse solutions of underdetermined systems. FOCUSS solves a reweighted ℓ2 minimization problem at each iteration. Harikumar and Bresler [55] proposed an iterative algorithm that can be viewed as a generalization of FOCUSS. At each iteration, the algorithm solves a convex optimization problem with a reweighted ℓ2 penalty function that encourages sparse solutions. In [33,56], Fazel et al. proposed a log-det heuristic method for minimizing the rank of matrices and examined the vector case as a special case, where the log-det heuristic method reduces to a log-sum heuristic method for reweighted ℓ1 minimization. These heuristics were used in minimum-order system realization [33]. In [57], Fazel et al. used the reweighted ℓ1 minimization as a heuristic algorithm for portfolio optimization.For the first iteration, the weighting matrix in reweighted framework is not hard. At the beginning, since we do not know the exact solution of the problem (PW1), we can set the weighting matrix to be the identity matrix, just as [33,35–37] did. Then we can estimatex0, the solution of (PW1). For the second iteration, we can update the weighting matrix according tox0. Subsequently, we still solve the problem (PW1) with the updated weighting matrix and generate a solutionx1. The iterations can be repeated until terminal condition is satisfied. Now let us discuss how to calculate the weighting matrix when the solution of the problem (PW1) is given.The iteration of the weighting matrix in problem (PW1) can be solved naturally if we look into the log-sum heuristic method used in the problem of cardinality minimization [33], which is a special case of matrix rank minimization problem. Consider the following problem:(7)minx∈Rn∑i=1nlog(|xi|+ϵ)s.t.Ax=b,where ε > 0. This is equivalent to(8)minx,u∈Rn∑i=1nlog(ui+ϵ)s.t.Ax=b,|xi|≤ui,i=1,…,n.In other words, ifx* is a solution of (7), (x*,u*) is a solution of (8) and vice versa. Though the surrogate function∑i=1nlog(ui+ϵ)is concave, it is smooth on any convex constraints, e.g.,Ax=b. So this function can be locally minimized via an iterative linearization method [33]. Letxkdenote the kth iteration of the optimization variablex, and we know that the gradient of log(ui+ ε) is (ui+ ε)−1. So the first-order Taylor series expansion of the function∑i=1nlog(ui+ϵ)with respect toxkis(9)∑i=1nlog(ui+ϵ)≈∑i=1nlog(uik+ϵ)+∑i=1n1uik+ϵ(ui−uik).The iterative linearization of the concave surrogate function gives the following heuristic method for cardinality minimization(10)(xk+1,uk+1)=argmin∑i=1nuiuik+ϵs.t.Ax=b,|xi|≤ui,i=1,…,n,which is equivalent to(11)xk+1=argmin∑i=1n|xi||xik|+ϵs.t.Ax=b.Note that in each iteration, we solve a weighting ℓ1 minimization problem and hence we can implement our solution via existing tools [53,54]. Specifically, if we choosex0 = [1, 1, …, 1], the solution ofx1 is the minimizer of∑i=1n|xi|,i.e., the ℓ1 norm ofx. Based on (11), we finds that the ith element of the weighting matrix in the kth iteration iswik=(|xik|+ϵ)−1. This again shows that the weights are inverse proportional to the solution.In a nut shell, the iterative weighted (i.e., reweighted) framework finds a local minimum of a concave penalty function that more closely resembles the ℓ0 norm than the convex ℓ1 norm. The resemblance stems from the usage of the log-sum surrogate function for reweighted ℓ1 minimization and can be illuminated via a simple example. We first define three penalty functions:•f0(x) = 1{x ≠ 0};f1(x) = |x|;flog, ε(x) = log(|x| + ε).The penalty function f0(x) has slope of positive infinity as x → 0+ (negative infinity as x → 0−), while its convex relaxation, the penalty function f1(x) has slope of 1 as x → 0+ (−1 as x → 0−). On the other hand, the slope of the function f0(x) at x ≠ 0 is always 0, while its convex relaxation, the function f1(x) has slope of 1 when x ≥ 0 (−1 as x ≤ 0). However, when x → 0+, the slope of the concave penalty function flog, ε(x) grows to positive infinite as 1/ε when ε → 0 and when x → 0−, the slope of the function flog, ε(x) grows to negative infinity as − 1/ε when ε → 0. This is one way why the function flog, ε(x) better approximates the function f0(x). What is more, when |x| grows to infinity, the slope of the concave function flog, ε(x) approaches to 0. This is another way why the function flog, ε(x) better approximates the function f0(x) (see Fig. 1). Similar to the ℓ0 norm, this allows a relatively large penalty to be placed on small nonzero coefficients and encourages to set the small nonzero coefficients as 0. In fact, flog, ε(x) tends to f0(x) as ε → 0. So if the ε is set wisely, i.e., sufficiently small, the log-sum penalty function flog, ε(x) would closely resemble the ℓ0 norm penalty function.In [55], Harikumar and Bresler proved that, for a given choice of reweighted schedule, the FOCUSS algorithm converges to a local minimum of some concave objective function (similar to the log-sum function∑i=1nlog(|xi|+ϵ)). In [33,56,57], especially in [57], Fazel et al. proved that{∑i=1nlog(|xik|+ϵ)}k=0∞will converge to a local minimum of the concave penalty function∑i=1nlog(|xi|+ϵ). Since this log-sum heuristic function is concave, it does not always have a global minimum [33]. In [37], Candès et al. proposed a question about the convergence conditions of their reweighted ℓ1 minimization algorithm. In [58], Zhao and Li provided a unified convergence analysis for existing reweighted ℓ1 minimization algorithms, including the algorithm proposed in [37]. In this paper, they first introduced a new concept called “merit function” which possesses some properties. Then they proved that if the merit function is properly chosen, the reweighted ℓ1 minimization algorithms can find a sparse solution of underdetermined systems. For more details, please refer to [58]. After that, the convergence of the reweighted ℓ1 minimization problem is proved in [59]. It must be noted that the log-sum penalty function of the reweighted ℓ1 minimization algorithms only produces a local minimum.In this section, we first briefly introduce the Sparse Subspace Clustering algorithm [9,22]. In Section 3.2, we introduce our reweighted ℓ1 minimization framework and formulate the problem and propose an algorithm to solve it.In this subsection, we briefly introduce the SSC algorithms [9,22]. This algorithm employs the sparse representation scheme, which is first introduced in compressed sensing, to construct a similarity graph or affinity matrix for clustering a collection of data points in multi-subspace.Consider a set of n data pointsY= [y1,y2, …,yn], whereyi∈Rmis the ith point potentially lying in a low dimensional subspace of the ambient space. SSC seeks the self-similarity among these data points by solving the following optimization problem(12)min∥C∥1s.t.Y=YC,diag(C)=0.Here,C∈Rn×nis the sparse representation matrix of data pointsYover the dictionaryYitself. Eventually, the sparse representation matrix C is employed for segmentation of a high-dimensional space into multiple low-dimensional spaces.In SSC, a post-processing step is to perform spectral clustering [30]. The SSC [9] can be described in Algorithm 3.1:As Liu et al. [23] point out, the subspaces are linear as the affine space case can be simply treated by decreasing the dimension of ambient space by one. Therefore, we only discuss the affine case for simplicity. For more general case with noise and sparse outliners, SSC solves the following ℓ1 minimization problem(13)min(C,E,Z)∥C∥1+λe∥E∥1+λz2∥Z∥F2s.t.Y=YC+E+Z,1TC=1T,diag(C)=0.Here, the matrix variableEmodels the sparse outlying observations in the data and the matrix variableZmodels the noise or corruption observations in the data. The Frobenius norm promotes having small entries in the columns ofZ. The two parameters λe> 0 and λz> 0 balance the three terms in the objective function and are discussed in [9]. Note that the optimization program in (13) is convex with respect to the optimization variables, hence, can be solved efficiently using convex programming tools (refer to [9] for more details).In ideal case, we would seek cij= 0 for all j∉Slas in (12), but the SSC algorithm may not be able to identify the ideal solution for real data. We can replace the ℓ1 minimization framework with reweighted ℓ1 minimization framework that we have discussed in Section 2, which potentially enhances the performance of the subspace clustering. As discussed, the updating of weighting matrix is based on the solution of (PW1) in previous iteration. In the experiment section, we will demonstrate through plenty of experiments that the performance of the SSC algorithm can be largely enhanced if we introduce the reweighted scheme into this algorithm.The following is our reweighted SSC framework:(14)min(C,E,Z)∥W⊙C∥1+λe∥E∥1+λz2∥Z∥F2s.t.Y=YC+E+Z,CT1=1,diag(C)=0.Here, ⊙ denotes the element-wise product between two matrices. In affine case, the above proposed framework can be described as follows:(15)min(C,E,A)∥W⊙C∥1+λe∥E∥1+λz2∥Y−YA−E∥F2s.t.AT1=1,A=C−diag(C).Here,Ais an auxiliary matrix that helps to obtain faster updates through hard-thresholding on the optimization variables. Initially, we set all the elements of the weighting matrix to be one. Hence, the above framework is a simple ℓ1 minimization problem. It is convex and can be solved efficiently via convex programming algorithm [53,54]. Following [9,22], we use Alternating Direction Method of Multipliers (ADMM) algorithm [54] to solve the weighting ℓ1 minimization problem. The ADMM algorithm is an iterative algorithm which solves the ℓ1 minimization problem with a global solution. However, as discussed in Section 3, we should update the weighting matrix after we obtain the solution of the sparse representation matrix in each iteration. To facilitate this, we add a reweighting step in the ADMM algorithm [54]. For the detailed derivations of ADMM, please refer to Appendix A.The convergency of the direct extension of ADMM with more than two convex functions in its objective is still an open problem, not only for non-convex optimization problems like RSSC, but also even for convex problems. Specifically, the ADMM can be used when there are two variables in its objective function while will fail if there are three or more variables. Recently, Lin et al. proposed a method combining linearized ADMM with parallel splitting and adaptive penalty (LADMPSAP) to solve multiple-variable convex minimization problems and get efficient results with conditioned convergence [60]. Later, Chen et al.[61] give an example showing that the direct extension of ADMM is not necessarily convergent. They also give a sufficient condition ensuing the convergence of the direct extension of ADMM with three or more separate convex functions. However, we can set a maximum iteration number as well as some convergence conditions to ensure that our algorithm can converge while achieve good performance.In this section, we will evaluate the performance of the state-of-the-art subspace clustering algorithms, i.e., LRR [23,24],11Matlab Codes available at http://sites.google.com/site/guangcanliu/.LSR [28],22Matlab Codes available at https://sites.google.com/site/canyilu/.and SMR [29],33Matlab Codes available at https://sites.google.com/site/hanhushomepage/pyu.SSC algorithm [9,22]44Matlab Codes available at http://www.eecs.berkeley.edu/~ehsan.elhamifar/code.htm.and our proposed reweighted sparse subspace clustering (RSSC) algorithm55Codes will be available when this paper will be published.on two real world computer vision problems, including video motion segmentation and human face clustering. Besides, for the segmentation of non-rigid objects, we evaluate more algorithms, such as LSA [18],66Matlab Codes available at http://vision.jhu.edu/code/.and SCC [21],77Matlab Codes available at http://www.math.duke.edu/~glchen/scc.html.to test the performance of more subspace clustering frameworks. We will first describe the two computer vision problems.Given a video sequence containing multiple moving objects, we can track and extract a set of feature points, e.g.,{xfn∈R2×1,n=1,…,N,f=1,…,F},which describe the X-coordinate and Y-coordinate of N points through F frames of the recorded video sequence. Each data point, e.g.,yn=[x1nT,…,xFnT]Tis a motion trajectory vector of dimension 2F. Motion segmentation is to segment the motion trajectories according to their motions. Under affine projection assumption, the motion trajectories of a rigid motion lie in a linear subspace which is at most of 4-dimension [1] while under the linear combination model, the motion trajectories of a non-rigid motion lie in a linear subspace which is at most of 3K-dimension [62,63], where K is the number of rigid motion parts or bases of a non-rigid motion. Hence, motion trajectories of multiple rigid or non-rigid motions lie in a union of corresponding low-dimensional subspaces of the ambient spaceR2F. Hence, the motion segmentation problem can be viewed as the problem of subspace clustering of data points in a union of subspaces. For the motion segmentation problem, we test our RSSC algorithm and the state-of-the-art algorithms including SSC on the Hopkins155 dataset [7],88Dataset is available at http://vision.jhu.edu/data/.and the Freiburg-Berkeley Motion Segmentation Dataset [32,39].99Dataset is available at http://lmb.informatik.uni-freiburg.de/resources/datasets/.The detailed description of these datasets can be found in Sections 4.1 and 4.2, respectively.Face images of a subject with a fixed pose and varying illumination lie close to a linear subspace of 9 dimensions under Lambertian assumption [64]. Hence, the collection of face images of multiple subjects lies close to a union of subspaces of 9 dimensions. The face image clustering problem can be modeled as the problem of clustering multiple subspaces. For the human face clustering problem, we consider the Extended Yale B dataset [8],1010The dataset we use in this work is included in the code of SSC.whose face images are size of 192 × 168. Specifically, we consider the dataset which is downsampled to 48 × 42 by the authors of [9] and already included in the code of SSC.Implementation detail: For all these algorithms, we use the matlab code provided by their authors. All the parameters are chosen when the final average clustering errors are lowest. All experiments are carried out using Matlab2014b on a machine with Intel(R) Core(TM) i7-4770K CPU at 3.50 GHz and 12 GB RAM. For the fairness of the comparement of the clustering accuracy as well as the computational time, we use the same implementations of data projection used in [29], which is more efficient than the standard SVD used in [9]. For more details about the parameters, please refer to the corresponding paper.For the Hopkins155 dataset: In LRR, we use λ = 4 and achieves the same results published in [9]. In LSR,1111The original published code of LSR uses the erroneous function ‘compacc.m’ for computing the clustering error. This error has also been mentioned in [9]. We correct the code for computing the clustering error and as a result, the reported results for LSR1 and LSR2 are different from the published results in [28].the author uses two different solutions in the LSR problem and obtains two methods named LSR1 and LSR2. The best performance is achieved when λ = 0.0048 for LSR1 and λ = 0.0046 for LSR2, which are in accordance with the parameters used in [28]. In SMR, we use α = 20 and get the same results published in [29]. In SSC, we use α = 800 and get the same results published in [9]. In our proposed RSSC, we use α = 800. Besides, we use ε1 = 0.001 for numerical stability and use ε2 = 0.02.For the Freiburg-Berkeley Motion Segmentation Dataset: In LRR, we use λ = 2. In LSR, we use λ = 0.006 for both LSR1 and LSR2. In SMR, we use α = 22. In SSC, we use α = 753. In our RSSC, we use α = 753. Besides, we use ε1 = 0.0002 for numerical stability and use ε2 = 0.0014. In LSA, we use K = 30 nearest neighbors and dimension d = 3. In SCC, since the optimal value of the parameter σ is inferred automatically from the data itself, we need not to tune the parameter. On the other hand, in order to reduce the randomness effect due to initial sampling procedure, we repeat the experiment 100 times and record only the average misclassification rate, just the same as [21] did.For the downsampled Extended Yale B dataset: In LRR, we use λ = 0.15, at which LRR can get better performance compared with the results published in [9]. In LSR1 and LSR2, we use λ = 0.004. We found that varying the parameter α from 10−6 to 220 will get similar results. The performance will drop dramatically when α ≥ 221. In SMR, we use the default parameters, i.e., k = 4 for constructing k-nn graph and α = 215. We found that varying the parameter α from 2 to 235 will get similar results. The performance will drop dramatically when α ≤ 1 or α ≥ 236. Just as LSR and SMR did in [28,29], we project the original data of 2016-dimension into 6n dimension through PCA [65], where n is the number of the clusters of faces. But for LRR, SSC, and RSSC, we do not perform dimension reduction.In this section, we evaluate the performance of the algorithms mentioned above on the Hopkins155 dataset [7]. This dataset consists of 155 video sequences, 120 of which contain two moving objects and 35 of which contain three moving objects, corresponding to 2 or 3 low-dimensional subspaces of the ambient space. Some sequences are taken from other datasets. For example, the sequence three-cars in checkerboard category is taken from [66], the sequences kanatani1 and kanatani2 in Traffic category are taken from [67], the sequence kanatani3 is taken from [67], the sequences head and two_cranes are taken from [18]. For all the remaining sequences, the motion trajectories of points are extracted by using a tool based on a tracking algorithm implemented in OpenCV [7]. Fig. 2shows some samples in the Hopkins155 database.All of these motion sequences can be divided into three main categories: checkerboard, traffic, and articulated or non-rigid motion sequences. Among these motion sequences in checkerboard category, 100 motion sequences are constructed as follows: 25 of which contain three moving objects. Without loss of generality, we just name them Object A, Object B, and Object C. Each sequence containing three moving objects (A, B, and C) can be subdivided into three sequences which contain two moving objects: Object A and Object B, Object A and Object C, Object B and Object C, respectively. Therefore, we obtain the remaining 75 sequences. Among the 155 sequences, only 5 motion video sequences are indeed non-rigid [7] and the remaining are rigid motion sequences. Furthermore, these motion video sequences are very short, the time lasts from less than 1 s to several seconds.In our experiments, when we use the original 2F-dimensional feature trajectories in our experiment, the results are shown in Table 1. In Table 2, the results are obtained when we project the original 2F-dimensional feature trajectories onto a 4n-dimensional subspace via PCA [65] where n is the number of moving objects. We use the same parameters described in Implementation detail for both the 2F and 4n cases. We have the following results and conclusions.First of all, in both 2F and 4n cases, all these algorithms achieve low average clustering errors (lower than 5%). However, the recently proposed SMR and our RSSC achieve lower clustering errors, around 1%, when compared with the remaining methods. The results prove that RSSC algorithm has a strong capability in motion segmentation problem. In both 2F and 4n cases, the LSR1 performs better than LSR2, which is in accordance with the results published in [28]. Moreover, all these algorithms achieve higher clustering errors in the 4n case than in the 2F case, which is due to the information loss in PCA process as the Hopkins155 dataset contains relatively slight corruptions.Secondly, with the case of the original 2F-dimensional motion trajectories, the clustering errors are reduced from 1.53% to 0.64% for the 2-motions case and from 4.4% to 2.01% for the 3-motions case respectively, by employing our algorithm. The average clustering errors on the whole Hopkins155 dataset are 2.18% for SSC and 0.95% for RSSC. For the case of the projected 4n-dimensional feature trajectories, the clustering errors of 2-motions and 3-motions cases are separately reduced from 1.69% and 4.38% to 0.83% and 2.50% by utilizing the RSSC algorithm. The average clustering errors on the whole Hopkins155 dataset are 2.29% for SSC and 1.21% for RSSC. The results implies that the RSSC algorithm largely enhances the performance of SSC on subspace clustering problems through reweighted ℓ1 minimization.Thirdly, the clustering performances of different algorithms at the 2F and 4n cases are very close. This again proves the fact that the motion trajectories of n motions in a video almost lie in a 4n-dimensional linear subspace of the 2F-dimensional ambient space. This result is in accordance with the results published in [9].Fourth, we have validated the sensitivity of the RSSC algorithm to the choice of the two parameters, ε1 and ε2. We set the value of ε1 as 10−3 and 2 × 10−3 respectively and tune the value of ε2, see Figs. 3 and 4. We note that RSSC improves over SSC in a large range of ε2.Finally, we compare the running time of RSSC and several leading algorithms, see Tables 3 and 4. Note that LSR is more efficient than other methods while the LSR2 is more efficient than LSR1. This result is in accordance with the results published in [28,29]. For LSR algorithms, the computation time of solving representation matrix is relatively small when compared to the time of spectral clustering stage. For SMR, the computation time of solving representation matrix is comparable to time of spectral clustering procedure. However, for SSC, LRR, and RSSC, the computation time of solving the representation matrix is significantly larger than the time for spectral clustering procedure. We note that the adding of reweighted iterations has a moderate impact on computation time of RSSC algorithm, i.e., additional 0.4 s (2 motions case) and 0.8 s (3 motions case) are costs for reweighted iterations.Just as Brox and Malik [32] pointed out, the Hopkins155 dataset is too special for factorization and algebraic methods. In fact, the Hopkins155 dataset only contains 5 non-rigid motion sequences. In order to evaluate the performance of the-state-of-the-art algorithms and our RSSC algorithms over non-rigid motion segmentation problem, we introduce the Freiburg-Berkeley Motion Segmentation Dataset [32,39]. This dataset is obtained through tracking technique of optical flow [68]. It contains 59 motion videos with pixel-accurate segmentation annotation of moving objects. Forty-two of the 59 videos describe animal’s moving, which is clearly non-rigid. Eleven videos describe vehicle’s moving and the last 6 videos describe people’s motions, such as walking, talking, and playing tennis, which are also non-rigid. Twelve of the 59 sequences (including 10 sequences describing cars moving, which are rigid motions and 2 sequences describing people walking) are taken from the Hopkins155 dataset [7]. Since the tracked feature points in a video sequence consist of multiple frames such as 10 frames, 50 frames, and 200 frames, we can get more than 59 motion sequences. For more details, please refer to [32,39,68,69]. Finally, we get 137 motion sequences, 22 of which are rigid motion sequences and 115 of which are non-rigid motion sequences.In our experiments, we only consider the video sequences which contain two motions for simplicity. In non-rigid motion videos, only one rigid motion, i.e., the motion of the background as well as the camera, is recorded whose feature points occupy the majority of the video. Some videos consist of hundreds of frames. Therefore, not all feature points appear in all frames, that is, many feature points vanish at certain frames. In our experiments, we randomly choose 200 points instead of all feature points, which could be thousands for each motion. We note that the motions in Hopkins155 dataset [7] choose 133 feature points on average. Hence, we believe 200 tracked points are sufficient for representing the pattern of motions for a moving object. For instance, Figs. 5and 6are some sample frames of tennis and marple6 from the Freiburg-Berkeley Motion Segmentation Dataset [32,39]. Similar to the evaluation protocol on the Hopkins155 dataset [7], the results of the SSC and RSSC algorithms on the Freiburg-Berkeley Motion Segmentation Dataset [32] are showed in Tables 5 and 6. And we have the following conclusions:First of all, all these algorithms have a good performance when performing segmentation on rigid motions, see Tables 5 and 6. The LSR algorithm achieves the lowest clustering errors, i.e., 0.59% on the 2F and 0.57% on the 4n cases. Our RSSC achieves the second lowest clustering errors, i.e., 1.20% on the 2F case and 1.24% on the 4n case. However, all algorithms achieve high clustering errors, i.e., higher than 5%, on non-rigid motion segmentations. The lowest clustering errors are achieved by RSSC in the 4n case, which is 6.68%. RSSC also achieves the lowest average clustering error on the entire dataset (the 4n case), i.e., 5.81%. The performance of SCC and LSR algorithms is inferior to the RSSC algorithm but outperforms the remaining algorithms, i.e., LSA, LRR, and SMR. The comparison between the performance of SSC and RSSC confirms the improvement of reweighted ℓ1 minimization framework over simple ℓ1 minimization scheme when performing subspace clustering.Secondly, the performance on non-rigid motion segmentation problem discloses the limitation of all these state-of-the-art algorithms. In our opinion, the gap between the clustering error rates on rigid and non-rigid motions is due to the Self-Expressive Property (SEP) [22], i.e., each feature trajectories in the same subspace can be represented as a linear combination of other points in the same subspace, which is commonly used in representation based methods such as SSC, LRR, LSR and SMR. However, when dealing with the non-rigid motions, the SEP is no longer perfectly satisfied since the non-rigid motions are not lying in a linear space. We believe this is the main reason why these algorithms fail in clustering non-rigid motions. However, we note that RSSC still outperforms the remaining algorithms on the non-rigid motions.Thirdly, we have validated the sensitivity of the RSSC algorithm to the choice of the two parameters, ε1 and ε2. We set the value of ε1 as 10−4 and 2 × 10−4 respectively and tune the value of ε2, see Figs. 7and 8. We note that RSSC improves over SSC in a large range of ε2.Finally, we compare the running time of the representation-based algorithms, i.e., LSR, SMR, and LRR, SSC, RSSC, see Tables 7 and 8. The results are similar to the running time on Hopkins155 dataset. Note that the computation time of these algorithms on the Freiburg-Berkeley Motion Segmentation Dataset is higher than the time on the Hopkins155 dataset, because more feature points are employed in the former dataset than in the latter one. We note that the adding of reweighted iterations has a moderate impact on computation time of RSSC algorithm, i.e., additional 0.8 s (2F-dimensional data)and 0.6 s (4n-dimensional data) are costs for reweighted iterations.In this subsection, we perform evaluation on the clustering performance of the RSSC and SSC algorithms on the Extended Yale B dataset [8], which contains face images of 38 subjects. Each subject consists of 64 frontal face images taken under various illuminating conditions including cast shadows and specularities. The face images are not distributed in linear subspaces perfectly as they are corrupted by errors [9]. All of the images are cropped and of 192 × 168 pixels. In order to reduce the computation cost, we reduce the size of the image to 48 × 42 pixels. Hence, each face image or data point is a vector of 2016-dimension. For the evaluation of the impact of the number of subjects on the clustering performance of the SSC algorithm, study in [9] divides the 38 subjects into 4 groups: 1 to 10, 11 to 20, 21 to 30, and 31 to 38. To simulate the face recognition problem, images of multiple subjects are combined. For the first three groups, the authors of [9] have considered the combination of n ∈ {2, 3, 5, 8, 10} subjects while they considered the combination of n ∈ {2, 3, 5, 8} subjects for the last group. For example, if we perform combination of 2 subjects in the first group which has 10 subjects, we have 45 combinations of trials. This is much fewer than the number of combinations by choosing n out of 38 subjects. Therefore, this strategy significantly reduces the number of combinations. Similar but not limited to [9], we consider all choice of n ∈ {2, 3, 4, 5, 6, 7, 8, 9, 10} subjects for each of the first three groups and consider all choice of n ∈ {2, 3, 4, 5, 6, 7, 8} subjects for the fourth group. The results of our experiments on the Extended Yale B dataset [8] are listed in Table 9. Based on the numbers, we have the following conclusions.First of all, the performance of SSC is in accordance with the results published in [9]. The performance of LSR1 and LSR2 on clustering 10 subjects is similar to the results published in [28]. For LRR, it achieves better performance in this study than the results published in [9,23]. The difference between the performance of SMR in this study and the results published in [29] is due to the choice of the 10 subjects. Ref. [29] uses the first 10 subjects in all 38 subjects, while we use the three subject groups which contains subjects 1–10, 11–20, and 21–30, respectively.Secondly, among all these algorithms, the SSC, SMR, and RSSC algorithms achieve good performance on the Extended Yale B dataset. However, RSSC algorithm obtains the lowest clustering errors, i.e., less than 5%, on different number of subjects when compared with other state-of-the-art algorithms. This again confirms the capability of RSSC algorithm in processing subspace clustering problems. The comparison between SSC and RSSC demonstrates a larger improvement of reweighted ℓ1 minimization framework over ℓ1 minimization scheme when performing signal recovery.Thirdly, we note that the average clustering errors go up with the increase of the number of subjects for all algorithms. However, the RSSC algorithm is more robust to the number of subjects than the remaining algorithms. Since the face images of this dataset are corrupted by errors, the results also indicate that sparse representation has a strong ability in processing error corruptions.Finally, we have compared the average computational time of these algorithms on Extended Yale B dataset. The results are demonstrated in Table 10. For LSR and SMR, we perform dimension reduction via PCA just as the authors suggested in [28,29]. Note that LSR is still the most efficient algorithm which is consistent with the results on Freiburg-Berkeley Motion Segmentation dataset and the Hopkins155 dataset. For SSC, LRR, and RSSC, the computation time of solving the representation matrix is significantly larger than the time for spectral clustering procedure. We note that the adding of reweighted iterations has a little impact on the computation time of RSSC algorithm, i.e., less than 0.5 s, occupying 3% of computational time for solving the representation or affinity matrix, is cost for reweighted iterations.

@&#CONCLUSIONS@&#
