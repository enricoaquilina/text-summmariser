@&#MAIN-TITLE@&#
Automatic annotation of tennis games: An integration of audio, vision, and learning

@&#HIGHLIGHTS@&#
Fully automatic annotation of real-world tennis videoState-of-the-art tennis ball tracking algorithmAn integration of computer vision, machine listening, and machine learningThe only system that can annotate tennis game at such a detailed level

@&#KEYPHRASES@&#
Tennis annotation,Object tracking,Audio event classification,Sequence labelling,Structured output learning,Hidden Markov model,

@&#ABSTRACT@&#
Fully automatic annotation of tennis game using broadcast video is a task with a great potential but with enormous challenges. In this paper we describe our approach to this task, which integrates computer vision, machine listening, and machine learning. At the low level processing, we improve upon our previously proposed state-of-the-art tennis ball tracking algorithm and employ audio signal processing techniques to detect key events and construct features for classifying the events. At high level analysis, we model event classification as a sequence labelling problem, and investigate four machine learning techniques using simulated event sequences. Finally, we evaluate our proposed approach on three real world tennis games, and discuss the interplay between audio, vision and learning. To the best of our knowledge, our system is the only one that can annotate tennis game at such a detailed level.

@&#INTRODUCTION@&#
The rapid growth of sports video databases demands effective and efficient tools for automatic annotation. Owing to advances in computer vision, signal processing, and machine learning, building such tools has become possible [1–3]. Such annotation systems have many potential applications, e.g., content-based video retrieval, enhanced broadcast, summarisation, object-based video encoding, and automatic analysis of player tactics, to name a few.Much of the effort in sports video annotation has been devoted to court games such as tennis and badminton, not only due to their popularity, but also to the fact that court games have well structured rules. A court game usually involves two (or two groups of) players hitting a ball alternately. A point is awarded when the ball fails to travel over a net or lands outside a court area. The task of court game annotation then consists in following the evolution of a game in terms of a sequence of key events, such as serve, ball bouncing on the ground, player hitting the ball, and ball hitting the net.On the other hand, building a fully automatic annotation system for broadcast tennis video is an extremely challenging task. Unlike existing commercial systems such as the Hawk-Eye [4], which uses multiple calibrated high-speed cameras, broadcast video archives recorded with a monocular camera pose great difficulties to the annotation. These difficulties include: video encoding artefacts, frame-dropping due to transmission problems, illumination changes in outdoor games, acoustic mismatch between tournaments, frequent switching between different types of shots, and special effects and banners/logos inserted by the broadcaster, to name a few. As a result of the challenges, most existing tennis applications focus only on a specific aspect of the annotation problem, e.g., ball tracking [5,6], action recognition [7]; or only annotate at a crude level, e.g., highlight detection [3], shot type classification [1]. Moreover, they are typically evaluated on small datasets with a few thousands of frames [5–7].In this paper, we propose a comprehensive approach to automatic annotation of tennis games, by integrating computer vision, audio signal processing, and machine learning. We define the problem that our system tackles as follows:•Input: a broadcast tennis video without any manual preprocessing and pre-filtering, that is, the video typically contains various types of shots, e.g. play, close-up, crowd, and commercial;Output: ball event detection: 3D (row and column of frame+frame number) coordinates of where the ball changes its motion; and ball event classification: the nature of detected ball events in terms of five distinct event labels: serve, hit, bounce, net, and null, which corresponds to erroneous event detection.To the best of our knowledge, our system is the only one that can annotate at such a detailed level.To achieve the goal defined above, at the feature level, we improve upon our previous work and propose a ball tracking algorithm that works in the more cluttered and therefore more challenging tennis doubles games. The identified ball trajectories are used for event detection and as one feature for event classification. A second feature for classification is extracted by audio signal processing. At the learning level, we model event classification as a sequence labelling problem. We investigate four representative learning techniques and identify their advantages on simulated event sequences. Finally, our approach is evaluated on three real world broadcast tennis videos containing hundreds of thousands of frames. Discussions on the interplay between audio, vision, and learning are also provided. Note that this paper extends our preliminary work [8] by including the construction of visual and audio features, the integration of visual and audio modalities at the learning level, and a more comprehensive investigation of learning techniques.The rest of this paper is organised as follows. Section 2 gives an overview of the proposed approach. The construction of features, including visual and audio features, is described in Section 3. Four learning techniques are then reviewed and compared on simulated event sequences in Section 4. Results on real world tennis games and discussions on the results are provided in Section 5. Finally Section 6 concludes the paper.A diagram of our proposed system is illustrated in Fig. 1. We assume a tennis video recorded with a monocular and static camera, e.g., a broadcast tennis video. If the video is interlaced, its frames are first de-interlaced into fields, in order to alleviate the effects of temporal aliasing. For the sake of simplicity, in the remainder of this paper, we will use “frames” to refer to both frames of progressive videos and fields of interlaced videos. After de-interlacing, the geometric distortion of camera lens is corrected. De-interlacing and geometric correction are considered “pre-processing” and omitted from Fig. 1.A broadcast tennis video is typically composed of different types of shots, such as play, close-up, crowd, and commercial. In the “shot analysis” block of Fig. 1, shot boundaries are detected using colour histogram intersection between adjacent frames. Shots are then classified into appropriate types using a combination of colour histogram mode and corner point continuity [9]. An example of the composition of a broadcast tennis video is shown in Fig. 2, where two examples of typical sequences of events in tennis are also given. The first example corresponds to a failed serve: the serve is followed by a net, then by two bounces under the net. The second example contains a short rally, producing a sequence of alternate bounces and hits.For a play shot, the ball is tracked using a combination of computer vision and data association techniques, which we will describe in more detail in Section 3.1. By examining the tracked ball trajectories, motion discontinuity points are detected as “key events”. Two examples of ball tracking and event detection results are shown in Fig. 3, where each key event is denoted by a red square. The detected events are then classified into five types: serve, bounce, hit, net, and “null”, which are caused by erroneous event detection. Two features are exploited for this classification task: information extracted from ball trajectories, i.e. location, velocity and acceleration around the events (Section 3.1); and audio event likelihoods from audio processing (Section 3.2).In addition to the features, the temporal correlations induced by tennis rules should also be exploited for classifying the events. For instance, a serve is likely to be followed by a bounce or a net, while a net almost certainly by a bounce. The focus of the “event classification” block of Fig. 1 is combining observations (features) and temporal correlations to achieve optimal classification accuracy. We model event classification as a sequence labelling problem, and provide an evaluation of several learning techniques on simulated event sequences in Section 4.In this section, we first introduce a ball tracking algorithm which improves upon our previous work. We sacrifice completeness for conciseness, and give an outline of the complete algorithm and discuss in detail only the modifications. Interested readers are referred to [10] for details of the complete algorithm. The tracked ball trajectories are used for event detection and also as a feature for event classification. In the second half of this section, we describe the second feature for event classification that is based on audio processing.Ball trajectories carry rich semantic information and play a central role in court game understanding. However, tracking a ball in broadcast video is an extremely challenging task. In fact, most of the existing court game annotation systems avoid ball tracking and rely only on audio and player information [11–13,3,14]. In broadcast videos the ball can occupy as few as only 5pixels; it can travel at very high speed and blur into the background; the ball is also subject to occlusion and sudden change of motion direction. Furthermore, motion blur, occlusion, and abrupt motion change tend to occur together when the ball is close to one of the players. Example images demonstrating the challenges in tennis ball tracking are shown in Fig. 4.To tackle these difficulties, we improve upon a ball tracking algorithm we proposed previously [10]. The operations of the algorithm in [10] can be summarised as follows11A video file “ball-tracking.avi” is submitted with this manuscript to demonstrate this algorithm.:1.The camera position is assumed fixed, and the global transformation between frames is assumed to be a homography [15]. The homography is found by: tracking corners through the sequence; applying RANSAC to the corners to find a robust estimate of the homography; and finally, applying a Levenberg–Marquardt optimiser [9,16].The global motion between frames is compensated for using the estimated homography. Foreground moving blobs are found by temporal differencing of successive frames, followed by a morphological opening operator which helps remove noise. Two example images of the output of this step are shown in Fig. 5.Moving blobs are then classified as ball candidates and not-ball candidates using their size, shape and gradient direction at blob boundary.A temporal sliding window is considered which centres at frame i and spans frame i−V to frame i+V. For each candidate in frame i, we search in a small ellipsoid around it in the column–row–time space for one candidate from frame i−1 and one candidate from frame i+1. If such candidates exist, we call the three candidates inside the ellipsoid a “seed triplet”, and fit a constant acceleration dynamic model.Candidates within the sliding window that are consistent with the fitted model are identified as “supports” of the model, and the model is refined recursively using new triplets selected from the supports. This is done in a greedy fashion until convergence, i.e., when the “cost” λ of the model does not decrease anymore. λ is defined as:(1)λ=∑j=i−Vi+V∑kρpjkwith the per-candidate cost:(2)ρpjk=d2p^jpjkifdp^jpjk<dthdth2ifdp^jpjk≤dthwhere pjkis the observed position of the kth ball candidate in frame j,p^jis the estimated ball position in frame j as given by the current model, d(.,.) is the Euclidean distance, and dthis a predefined threshold. A converged dynamic model together with its supports is called a “tracklet” and corresponds to an interval when the ball is in free flight. An example of the model fitting/optimisation process is given in Fig. 6.As the sliding window moves, a sequence of tracklets is generated. These tracklets may have originated from the ball or from clutter. A weighted and directed graph is constructed, where each node is a tracklet, and the edge weight between two nodes is defined according to the “compatibility” of the two tracklets. The ball trajectories are obtained by computing the shortest paths between all pairs of nodes (tracklets) and analysing the paths.The tracking algorithm summarised above works well in real world broadcast tennis videos of singles games. On the other hand, doubles games are considerably more challenging: the doubled number of players means more clutter, more occlusion, and more abrupt motion change. In order to cope with the increased challenges, we modified steps 4 and 5 of the previous algorithm to get more accurate tracklets.First, we modify the way a dynamic model is computed. In [10] a constant acceleration model is solved exactly for three candidates in three frames: in the first iteration for the seed triplet, and in subsequent iterations for the three supports that are temporally maximally apart from each other. This scheme has two disadvantages: 1) It assumes the three observed ball candidate positions are noise-free, which is never the case in reality. 2) It uses only three candidates for computing the model, ignoring a potentially large number of supports. To remedy these problems, we assume that the observed candidate positions are corrupted by a zero mean Gaussian noise, and jointly estimate the true positions and solve the dynamic model with a least squares (LS) estimator.More specifically, letJbe a set of frame numbers such that for eachj∈Ja candidate in frame j is used for computing the dynamic model. For example, in the first iteration,Jcontains the numbers of frames from which the seed triplet is drawn. Letpj=rjcjTj∈Jbe the set of observed positions of candidates in terms of row and column,pj=r^jc^jTj∈Jbe the set of corresponding estimated true positions. Also letp^ibe the estimated position in frame i (middle frame of the sliding window), letv^ibe the estimated velocity of the ball in frame i, and leta^be the estimated acceleration, which is assumed constant for the dynamic model. Then we have:(3)p^j=p^i+j−iv^i+12j−i2a^⊙a^∀j∈Jwhere ⊙ denotes the element-wise multiplication.p^i,v^i, anda^can then be estimated by solving the LS problem:(4)minp^i,v^i,a^∑j∈J||pj−p^j||2.Compared to the model fitting scheme in [10], the LS estimator in Eq. (4) does not assume noise-free observations, and does not impose the constraint that|J|=3. As a result, it leads to a more stable estimation of dynamic models.We also modify the way candidates for model computation are selected in each iteration. In [10],|J|is set to 3, and the three candidates that are in the support set of the current model and are temporally maximally apartAlgorithm 1Recursively refine the dynamic model for a seed triplet.To remedy this, we introduce a randomised process inside each iteration. This randomisation greatly increases the chance of escaping from local minima, and as a result leads to more accurate model computation. Details of the improved algorithm for refining a dynamic model are provided in Algorithm 1. Note that the size of each random sample in Algorithm 1,|J|, is not limited to 3 anymore, thanks to the new LS formulation in Eq. (4).In our experiments, the number of inner random samples is set to R=100, which is larger than the number of iterations taken for the greedy search to converge (typically less than 10). The LS problem in Eq. (4) is also more expensive than the exact solution in [10]. However, in practice we observe that the two modifications do not significantly slow down the model fitting/refining process. With a frame rate at 25 frames per second, model fitting/refining runs comfortably in real time.After ball tracking, key events are identified by detecting motion discontinuity in the ball trajectory [10]. Recall that each red square in Fig. 3 corresponds to one detected key event. The task of tennis annotation then consists in classifying the key events into five types: serve, bounce, hit, net, and “null” which corresponds to false positives in event detection. The ball dynamics around a detected event are used as one feature for event classification. More specifically, for a detected event in frame i, letp^i−1,p^i,p^i+1be the estimated ball positions in frames i−1,i,i+1,v^i−1,v^ibe the estimated velocities, anda^ibe the estimated acceleration. We also compute the magnitude and orientationa^i†ofa^i. The 14 dimensional feature that is based on ball trajectory is finally defined as:(5)xtraj=p^i−1Tp^iTp^i+1Tv^i−1Tv^iTa^iTa^i†TT.In order to extract audio features for event classification, seven types of audio events are defined [17], as summarised in Table 1. Note that the set of audio events does not completely overlap with the four events for annotation (serve, bounce, hit, and net), as some of the events for annotation e.g. bounce does not produce a characteristic and clearly audible sound, especially in the presence of crowd noise.A men's singles game from Wimbledon Open 2008 is used for building models of the audio events. We first segment the sound track into audio frames of 30ms in length with 20ms overlap between consecutive frames. As a result, the audio frame rate is at 100 frames per second, which is higher than the video frame rate (50 for interlaced videos and 25 for progressive ones). Extra care is taken to ensure synchronisation between audio and video frames. The audio frames are labelled in terms of the seven audio events. We compute 39 dimensional Mel-frequency cepstral coefficients (MFCCs) for each frame and build a Gaussian mixture model for each audio event type.Once the generative models are built, a test audio frame could be classified using straightforward maximum likelihood (ML). However, characteristics of audio events may vary across games and tournaments, significantly degrading the performance of the ML estimator. In order to reduce the impact of acoustic mismatches between training and test data, we employ a confidence measure. Let Lijbe the log likelihood of audio frame i being the audio event j, where j=1,…,7 correspond to the seven audio events, and Lijis given by the jth Gaussian mixture model. The confidence measure of audio frame i being audio event j is then defined as:(6)Dij=Lij−maxk≠jLik.Using the difference between log likelihoods provides some immunity to mismatch between training and test distributions: the mismatch will, to a certain extent, be cancelled by the differencing operation.For a detected event in audio frame i, we wish to compute some statistics of the confidence measures in a neighbourhood of i and use it as a feature for event classification. In practice, we find that the max operator performs the best. Because our interest is in the “hit” audio event, we use only the confidence measure associated with this event. As a result, the audio-based feature for event classification is one dimensional:(7)xaudio=maxi−W≤l≤i+WDl3where the superscript j=3 corresponds to audio event “hit”, and W is set to 10 in our experiments. Note that event detection and the computation of the trajectory based features Eq. (5) are both done in terms of video frames. To ensure synchronisation, one could “map” audio frames to video frames by dividing i,l,W in Eq. (5) by 2 (for interlaced video) or 4 (for progressive video).In the previous section, we have detected key events, and extracted features for each detected event. The two types of features extracted can be combined e.g. using vector concatenation x=(xtrajT,xaudioT)T. For a given play shot, suppose a sequence of o events are detected, i.e., there are o “tokens” in the sequence. Let ys,s=1,…,o be the (micro-)label of the sth event. yscan take any value in the set of {serve,bounce,hit,net,null}, where null is the label for false positives in event detection. The overall (structured) label of the play shot is then composed of a sequence of micro-labels:(8)y=y1→y2→…→yoSimilarly let xs,s=1,…,o be the concatenated feature for the sth event. The overall (structured) feature of the play shot is then:(9)x=x1→x2→…→xoIn Eqs. (8) and (9) we have used “→” to indicate the sequential nature of the label y and the feature x. It is now clear that we have a structured learning problem: given a training set of feature/label pairs {xi,yi}i=1m, wherexi=xi1→xi2→…→xioi,yi=yi1→yi2→…→yioi, and oiare the number of events (or tokens) of play shot i in the training set, we want to learn a structured classifier that can assign a label y to a test pattern x.For such a sequence labelling task, we could ignore the internal structure of the labels and learn non-structured classifiers such as naive Bayes or SVM to assign micro-labels to individual events. On the other hand, as structured methods hidden Markov model (HMM) and structured SVM (S-SVM) can exploit the dependency of micro-labels. Among the four learning methods mentioned, naive Bayes, SVM and HMM are very well known. In the following we briefly review structured SVM. We will then evaluate the four methods on simulated event sequences. Through the simulation, the relative advantages of the different learning techniques are identified. A taxonomy of four learning techniques is given in Table 2.In a nutshell, structured output learning (SOL) jointly embeds input–output pairs (xi,yi) into a feature space, and applies linear classifiers in the feature space. In the case of hinge loss, a max-margin hyperplane is sought, and the resulting learning machine can be thought of as structured SVM (S-SVM) [18]. More specifically, we assume for any training example (xi,yi),wTϕxiyi−wTϕxiy≥Δyiy,∀y∈Y\yiwhere ϕ(⋅,⋅) is the joint embedding, Δ(⋅,⋅) is a label loss function measuring the distance between two labels, andYis the set of all possible structured labels. Introducing regularisation and slack variables ξi, the max-margin hyperplane w⁎ is found by solving:(10)minw12||w||2+C∑i=1mξis.t.wTϕxiyi−wTϕxiy≥Δyiy−ξi,∀y∈Y\yi,ξi≥0.The prediction of a test example x is then given byy∗=argmaxy∈Yw∗Tϕxy.As the labels are structured,|Y|is often prohibitively large, making Eq. (10) intractable with standard SVM solvers. Iterative cutting-plane algorithms have been developed [18,19], where the “most violated” constraints are identified by repeatedly solving the so-called separation oracley˜i=argmaxy∈YΔyiy+w˜Tϕxiy, and are added to the constraint set. These greedy algorithms admit polynomial training time, and are general in the sense that a large class of SOL problems (including sequence labelling) can be solved provided: 1) a joint feature mapping is defined, either explicitly as ϕ(xi,yi), or implicitly through a joint kernel function J((xi,yi),(xj,yj))=<ϕ(xi,yi),ϕ(xj,yj)>; 2) a label loss function Δ(yi,y) is specified; and 3) an efficient algorithm for the separation oracle is available.Now consider the event sequence labelling problem in court game annotation. Let (xi,yi) and (xj,yj) be the two training examples with length (number of events, or number of tokens) oiand ojrespectively. We implement first order Markovian assumption through a joint kernel function:Jxiyixjyj=∑s=2oi∑t=2ojyis−1=yjt−1yis=yjt+η∑s=1oi∑t=1ojyis=yjtKxisxjtwhere 〚⋅〛 is an indicator function, η is a kernel parameter controlling the trade-off between temporal correlation and observation, and K(xis,xjt) is a kernel defined for the observations [20,21].For the label loss function, we use the hamming loss between two competing labels:Δyiy=∑s=1oiyis≠ys. Finally, it is easy to show that the separation oracle for sequence labelling is the Viterbi decoding problem, for which efficient algorithms exist.Consider event sequences where the set of micro-labels is {serve,hit,bounce,net}. We employ two ways of simulating successive events. The first makes a first order Markovian assumption, while the second assumes no dependence between neighbouring tokens. For each token in each sequence a 10 dimensional observation is also simulated using one of four distributions corresponding to the four types of events. The separation of the distributions is controlled by varying their means through a parameter γ: the larger its value, the more separated. We consider two scenarios for observation distributions, namely, Gaussian and uniform.The combination of Markovian/non-Markovian and Gaussian/uniform results in a total of four scenarios. For each of them, 2000 sequences of micro-labels and associated observations are simulated. We use 1000 sequences for training, and the rest for testing. For the two generative methods, Gaussian distributions are assumed for the observations. The mean and standard deviation of the test errors of the four learning methods in the four simulated scenarios are shown in Fig. 7.The results in Fig. 7 demonstrate that when there is indeed a structure in the micro-labels, structured classification methods outperform non-structured ones; while when no structure is present, both methods perform similarly. On the other hand, when P(x,y) is estimated poorly, the performance of a generative approach is severely degraded. In contrast, discriminative approaches do not make assumptions of P(x,y), and as a result tend to be more robust [22]. Overall, the structured and discriminative S-SVM seems to be a safe choice: in all the scenarios considered, it produces either optimal or near optimal performance. We will test if this is still the case on real world data in the next section.

@&#CONCLUSIONS@&#
In this paper we have presented a solution to the challenging problem of automatic annotation of tennis game using broadcast tennis videos. The output of our system is key events with locations in the row–column–time space and labels, which can potentially lead to scores. To the best of our knowledge, our work is the only one that can annotate at such a detailed level. This is achieved by integrating computer vision, audio processing, and machine learning. For each of the disciplines involved we have designed our approach carefully so as to optimise the overall performance. The proposed method was evaluated on three real world tennis videos with positive results. Discussions on the interplay between vision, audio, and learning, and on future work plan were also provided.