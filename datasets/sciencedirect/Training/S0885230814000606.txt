@&#MAIN-TITLE@&#
Unsupervised language model adaptation using LDA-based mixture models and latent semantic marginals

@&#HIGHLIGHTS@&#
Unsupervised language model adaptation approaches for speech recognition.A hard-clustering approach is applied into latent Dirichlet allocation (LDA) model.Tri-gram topic models are created and mixed to form an adapted tri-gram model.The adapted tri-gram model is interpolated with a background tri-gram model.The above models are modified using unigram latent semantic marginals.

@&#KEYPHRASES@&#
Language model,Topic model,Mixture model,Speech recognition,Minimum discriminant information,

@&#ABSTRACT@&#
In this paper, we present unsupervised language model (LM) adaptation approaches using latent Dirichlet allocation (LDA) and latent semantic marginals (LSM). The LSM is the unigram probability distribution over words that are calculated using LDA-adapted unigram models. The LDA model is used to extract topic information from a training corpus in an unsupervised manner. The LDA model yields a document–topic matrix that describes the number of words assigned to topics for the documents. A hard-clustering method is applied on the document–topic matrix of the LDA model to form topics. An adapted model is created by using a weighted combination of the n-gram topic models. The stand-alone adapted model outperforms the background model. The interpolation of the background model and the adapted model gives further improvement. We modify the above models using the LSM. The LSM is used to form a new adapted model by using the minimum discriminant information (MDI) adaptation approach called unigram scaling, which minimizes the distance between the new adapted model and the other model. The unigram scaling of the adapted model using LSM yields better results over a conventional unigram scaling approach. The unigram scaling of the interpolation of the background and the adapted model using the LSM outperform the background model, the unigram scaling of the background model, the unigram scaling of the adapted model, and the interpolation of the background and the adapted models respectively. We perform experiments using the '87–89 Wall Street Journal (WSJ) corpus incorporating a multi-pass continuous speech recognition (CSR) system. In the first pass, we used the background n-gram language model for lattice generation and then we apply the LM adaptation approaches for lattice rescoring in the second pass.

@&#INTRODUCTION@&#
LM adaptation plays a vital role to improve a speech recognition system's performance. It is essential when the styles, topics or domains of the recognition tasks are mismatched with the training set. To compensate for this mismatch, LM adaptation helps to exploit specific, albeit limited, knowledge about the recognition task (Bellegarda, 2004). The idea of an unsupervised LM adaptation approach is to extract latent topics from the training set and then adapt topic-specific LMs with proper mixture weights, finally interpolated with a generic n-gram LM (Liu and Liu, 2007; Haidar and O'Shaughnessy, 2010).Statistical n-gram language models have been used successfully for speech recognition and other applications. They use local context information by modeling text as a Markovian sequence and capture only the local dependencies between words. They suffer from insufficiencies of the training data, which limit model generalization. Due to limitations of the amount of training data, statistical n-gram LMs encounter a data sparseness problem, which is handled by using backoff smoothing approaches with lower-order language models (Chen and Goodman, 1999). Moreover, n-gram models cannot capture the long-range information of natural language. Several methods have been investigated to overcome this weakness. A cache-based language model is an earlier approach that is based on the idea that if a word appeared previously in a document it is more likely to occur again. It helps to increase the probability of previously observed words in a document when predicting a future word (Kuhn and Mori, 1990). Recently, various techniques such as latent semantic analysis (LSA) (Deerwester et al., 1990; Bellegarda, 2000), probabilistic LSA (PLSA) (Gildea and Hofmann, 1999), and LDA (Blei et al., 2003) have been investigated to extract the latent topic information from a training corpus. All of these methods are based on a bag-of-words assumption, i.e., the word-order in a document can be ignored. In LSA, a word–document matrix is used to extract the semantic information. In PLSA, each document is modeled by its own mixture weights and there is no generative model for these weights. So, the number of parameters grows linearly when increasing the number of documents, which leads to an overfitting problem. Also, there is no method to assign probability for a document outside the training set. On the contrary, the LDA model was introduced where a Dirichlet distribution is applied on the topic mixture weights corresponding to the documents in the corpus. Therefore, the number of model parameters is dependent only on the number of topic mixtures and the vocabulary size. Thus, LDA is less prone to overfitting and can be used to compute the probabilities of unobserved test documents. However, the LDA model can be viewed as a set of unigram latent topic models. The LDA model has been used successfully in recent research work for LM adaptation (Tam and Schultz, 2005, 2006; Liu and Liu, 2007, 2008; Haidar and O'Shaughnessy, 2010, 2011, 2012b,a). In Tam and Schultz (2006), a unigram scaling approach is used for the LDA adapted unigram model to minimize the distance between the adapted model and the background model (Tam and Schultz, 2006). The LDA model is also used as a clustering algorithm to cluster training data into topics (Ramabhadran et al., 2007; Heidel and Lee, 2007). The LDA model can be merged with n-gram models and achieve perplexity reduction (Sethy and Ramabhadran, 2008). A non-stationary version of LDA can be developed for LM adaptation in speech recognition (Chueh and Chien, 2009). A topic-dependent LM, called topic dependent class (TDC) based n-gram LM, was proposed in Naptali et al. (2012), where the topic is decided in an unsupervised manner. Here, the LSA method was used to reveal latent topic information from noun–noun relations (Naptali et al., 2012).The simple technique to form a topic from an unlabeled corpus is to assign one topic label to a document (Iyer and Ostendorf, 1996). This hard-clustering strategy is used with leveraging LDA and named entity information to form topics (Liu and Liu, 2007, 2008). Here, topic-specific n-gram language models are created and joined with proper mixture weights for adaptation. The adapted model is then interpolated with the background model to capture the local lexical regularities. The component weights of the n-gram topic models were created by using the word counts of the latent topic of the LDA model. However, these counts are best suited for the LDA unigram topic models. A unigram count weighting approach (Haidar and O'Shaughnessy, 2010) for the topics generated by hard-clustering has shown better performance over the weighting approach described in Liu and Liu (2007, 2008). An extension of the unigram weighting approach (Haidar and O'Shaughnessy, 2010) was proposed in Haidar and O'Shaughnessy (2011) where the weights of the n-gram topic models are computed by using the n-gram count of the topics generated by a hard-clustering method. The adapted n-gram model is scaled by using the LDA-adapted unigram model called latent semantic marginals (LSM) (Tam and Schultz, 2006) and outperforms a traditional unigram scaling of the background model using the above marginals (Haidar and O'Shaughnessy, 2012a). Here, the unigram scaling technique (Kneser et al., 1997) is applied where a new adapted model is formed by using a minimum discriminant information (MDI) approach that minimizes the KL divergence between the new adapted model and the adapted n-gram model, subject to a constraint that the marginalized unigram distribution of the new adapted model is equal to the LSM. In this paper, we present an extension to the previous works (Haidar and O'Shaughnessy, 2011, 2012a) where we apply the unigram scaling technique to the interpolation of the background and the adapted n-gram model and note better results over the previous works. In addition, we perform all the experiments using different corpus sizes (’87 WSJ corpus (17 million words) and '87–89 WSJ corpus (37 million words)) instead of using only the 1 million words WSJ training transcription data used in Haidar and O'Shaughnessy (2011, 2012a) and using different test sets. Also, we use various topic sets instead of using a single topic set.The rest of this paper is organized as follows. Section 2 is used for describing the LDA model. The generation of the adapted model using n-gram weighting is explained in Section 3. The LM adaptation approaches using LSM are illustrated in Section 4. In Section 5, the experimental details are described. Finally the conclusion is described in Section 6.LDA is a three-level hierarchical Bayesian model. It is a generative probabilistic topic model for documents in a corpus. Documents are represented by the random latent topics,11Topics are unobserved in LDA.which are characterized by a distribution over words. The graphical representation of the LDA model is shown in Fig. 1. Here, we can see the three levels of LDA. The Dirichlet prior α and β are the corpus level parameters that are assumed to be sampled once in generating the corpus. The parameters θ are document-level variables and sampled once per document. The variables z andware word-level variables and sampled once for each word in each document (Blei et al., 2003).The LDA parameters (α, β) are estimated by maximizing the marginal likelihood of training documents. α={α1, …, αK} represents the Dirichlet parameter for K latent topics and β represents the Dirichlet parameter over the words and defined as a matrix with multinomial entryβkz,w=P(w|kz). The LDA model can be described in the following way. Each documentdl=[w1,…,wN](l=1, …, M) is generated as a mixture of unigram models, where the topic mixture vectorθdlis drawn from the Dirichlet distribution with parameter α. The corresponding topic sequence k=[k1, …, kN] is generated using the multinomial distributionθdl. Each wordwNis generated using the distributionP(wN|kN,β). The joint probability of dl, topic assignment k and topic mixture vectorθdlis given by:(1)P(dl,k,θdl|α,β)=P(θdl|α)∏i=1NP(ki|θdl)P(wi|ki,β)The probability of the document dlcan be estimated by marginalizing unobserved variablesθdland k as:(2)P(dl|α,β)=∫P(θdl|α)∏i=1N∑ki=1KP(ki|θdl)P(wi|ki,β)dθdlwhereθdlis a K-dimensional random variable that can take values in the (K−1)-simplex (a K-vectorθdllies in the (K−1)-simplex ifθdlki>0,∑ki=1Kθdlki=1), and has the following probability density on this simplex:(3)P(θdl|α)=Γ(∑ki=1Kαki)∏ki=1KΓ(αki)θdl1α1−1,…,θdlKαK−1where the parameter α is a K-vector with componentsαki>0, and Γ(x) is the Gamma function. The Dirichlet distribution is used as the prior for the multinomial distributions. This is because of the conjugate property of the Dirichlet distribution, which results in the posterior integrals in a Dirichlet distribution, which simplifies the model inference. The product of a Dirichlet prior with the multinomial likelihood will yield another Dirichlet distribution of a certain form (Blei et al., 2003).The parameters of the LDA model can be estimated using variational inference (Blei et al., 2003) or Gibbs sampling (Gilks and Richardson, 1996). The variational inference method uses the variational parameters and Jensen's inequality to obtain an adjustable lower bound on the likelihood. The optimizing values of the variational parameters are obtained by minimizing the KL divergence between the variational distribution and the true posteriorP(θdl,k|dl,α,β). For details of calculation, see Blei et al. (2003). In Gibbs sampling method, an algorithm is described for extracting a set of topics from a large corpus that uses Gibbs Sampling, which is a form of Monte Carlo Markov chain (Gilks and Richardson, 1996). It simulates a high-dimensional distribution by sampling on lower-dimensional subsets of variables where each subset is conditioned on the value of the others. The sampling is done sequentially and proceeds until the sampled values approximate the target distribution. For details, see Griffiths and Steyvers (2004).We have used the MATLAB topic modeling toolbox (Steyvers, 2013) for LDA analysis that uses a Gibbs sampler for parameter estimation. We have formed the word–topic matrix, WP, and the document–topic matrix, DP, using LDA analysis (Steyvers, 2013). In the WP matrix, an element WP(i, kz) shows the number of occurrences of wordwiin topic kzover the training set. In the DP matrix, an element DP(j, kz) contains the total number of occurrences of words in document djthat are from a topic kz(kz=1, 2, …, K).We have formed the topic set by applying a hard-clustering approach (Iyer and Ostendorf, 1996; Liu and Liu, 2007, 2008) to the DP matrix. Here, we assign a document djto a topic kzas:(4)kz=argmax1≤kz≤KDP(j,kz)i.e., a document is assigned to a topic from which it takes the maximum number of words. Therefore, all the training documents are assigned to K topics. The n-gram topic LM's for K topics are trained. The models are then combined with proper mixture weights to form an adapted model (see Section 3.2). The idea is portrayed in Fig. 2.A document is generated by a mixture of topics in the LDA model. So, for a test documentdt=w1,…,wN, a dynamically adapted n-gram model can be created by using a mixture of n-gram topic LMs as:(5)PA(wi|h)=∑kz=1KϕkzPkz(wi|h)wherePkz(wi|h)is the kzth n-gram topic model,ϕkzis the kzth mixture weight, and h is the preceding n−1 words of the current wordwi. To find topic mixture weightϕkz, the n-gram count of the topics, generated by Eq. (4), is used (Haidar and O'Shaughnessy, 2011). Therefore,(6)ϕkz=∑j=1NnP(kz|wjn,…,wj1)P(wjn,…,wj1|dt)with(7)P(kz|wjn,…,wj1)=Ckz(wjn,…,wj1,kz)∑kz=1KCkz(wjn,…,wj1,kz)(8)P(wjn,…,wj1|dt)=C(wjn,…,wj1)∑j=1NnC(wjn,…,wj1)whereCkz(wjn,…,wj1,kz)describes the number of times the n-gram(wjn,…,wj1)is seen in topic kz, which is created by Eq. (4).C(wjn,…,wj1)and Nnare the counts of the n-gram(wjn,…,wj1)and the number of different n-grams respectively in document dt.The adapted (A) n-gram model is then interpolated with the background (B) n-gram model to capture the local constraints using linear interpolation as:(9)PL(wi|h)=λPB(wi|h)+(1−λ)PA(wi|h),where λ is an interpolation weight.We computed the LSM by using the technique described in Tam and Schultz (2006). At first the automatic transcription (recognition results after first pass decoding) is treated as a single document (Tam and Schultz, 2006). Then, a Gibbs sampler is applied for the test document to estimate the posterior over the topic mixture weights (Heinrich, 2009). The LDA-adapted marginal is then computed as follows (Tam and Schultz, 2006):(10)Plda(wi)=∑kz=1KP(wi|kz,β).γkz∑kz=1Kγkz,whereγkzis the weight of topic kzfor the test document dtobtained after LDA inference and computed as:(11)γkz=DP(dt,kz)+αDP(dt,.)+Kα,where DP(dt, .) is the total occurrences of words in document dtin all topics. K, DP(dt, kz) and α are defined as above.P(wi|kz,β)is the probability of wordwifor topic kzobtained after applying LDA over the training set and is computed as (Griffiths and Steyvers, 2004; Heinrich, 2009):(12)P(wi|kz,β)=WP(wi,kz)+βWP(.,kz)+Vβ,where WP(., kz) is the total count of words in topic kzand V is the size of the vocabulary.WP(wi,kz)and β are defined as above.The unigram scaling technique (Kneser et al., 1997; Gildea et al., 1992) forms an adapted model by minimizing the KL-divergence between the background model and the adapted model subject to the marginalization constraint for each wordwiin the vocabulary (Tam and Schultz, 2006) as:(13)∑hPA1(h)·PA1(wi|h)=Plda(wi).The constraint optimization problem has close connection to the maximum entropy approach (Rosenfeld, 1996), which provides that the adapted model is a re-scaled version of the background model:(14)PA1(wi|h)=δ(wi)Z(h)·PB/A/L(wi|h)with(15)Z(h)=∑wiδ(wi)·PB/A/L(wi|h)wherePA1(wi|h)is defined as the new adapted model,PB/A/L(wi|h)is the background, adapted (Eq. (5)) or the interpolated (Eq. (9)) model. Z(h) is a normalization term, which guarantees that the total probability sums to unity, andδ(wi)is a scaling factor that is usually approximated as:(16)δ(wi)≈PA1(wi)PB/A/L(wi)μ,where μ is a tuning factor between 0 and 1. In this paper, we used μ=0.5 (Tam and Schultz, 2006; Haidar and O'Shaughnessy, 2012a). To compute the normalization term Z(h), we used the same procedure as Tam and Schultz (2006), Kneser et al. (1997) and Haidar and O'Shaughnessy (2012a). To accomplish this, an additional constraint is considered where the total probability of the seen transitions is unchanged:(17)∑wi:seen(h,wi)PA1(wi|h)=∑wi:seen(h,wi)PB/A/L(wi|h).The new adapted LM is then computed as:PA1(wi|h)=δ(wi)Zs(h)·PB/A/L(wi|h)if(h,wi)existsbw(h)·PA1(wi|hˆ)otherwisewhereZs(h)=∑wi:seen(h,wi)δ(wi)·PB/A/L(wi|h)∑wi:seen(h,wi)PB/A/L(wi|h)andbw(h)=1−∑wi:seen(h,wi)PB/A/L(wi|h)1−∑wi:seen(h,wi)PA1(wi|hˆ)Here, Zs(h) is used to compute the normalization similar to Eq. (15) except the summation is performed only on the seen alternative words with the same word history h in the LM (Tam and Schultz, 2006), bw(h) is the back-off weight of the context h to ensure thatPA1(wi|h)sums to unity andhˆis the reduced word history of h. The idea is described in Fig. 3.We evaluated the LM adaptation approaches using the Wall Street Journal (WSJ) corpus (Paul and Baker, 1992). The SRILM toolkit (Stolcke, 2002) and the HTK toolkit (Young et al., 2013) are used for generating the LMs and computing the WER respectively. We perform our experiments by varying data sizes to see the relative improvement. The '87 (17 million words) and '87–89 (37 million words) WSJ corpus is used to train the tri-gram background model and the tri-gram topic models using the back-off version of the Witten–Bell smoothing. The language models are closed-vocabulary language models, i.e., the models are generated using the n-gram counts without considering n-grams with unknown words. To reduce the computational cost, we incorporated the cutoffs 1 and 3 on the bi-gram and tri-gram counts respectively. The LDA and the language models are trained using the WSJ 20K non-verbalized punctuation closed vocabulary. We define the α and β for LDA analysis as 50/K and 0.01 respectively (Griffiths and Steyvers, 2004; Heinrich, 2009). The acoustic model from Vertanen (2013) is used in our experiments. The acoustic model is trained by using all WSJ and TIMIT (Garofolo et al., 1993) training data, the 40 phones set of the CMU dictionary (Anon., 2013), approximately 10,000 tied-states, 32 Gaussians per state and 64 Gaussians per silence state. The acoustic waveforms are parameterized into a 39-dimensional feature vector consisting of 12 cepstral coefficients plus the 0th cepstral, delta and delta delta coefficients, normalized using cepstral mean subtraction (MFCC0−D−A−Z). We evaluated the cross-word models. The values of the word insertion penalty, beam width, and the language model scale factor are −4.0, 350.0, and 15.0 respectively (Vertanen, 2013). The development test set is the si_dt_05.odd (248 sentences, 4074 words) and the evaluation test sets are the Nov’92 and Nov’93 test data from the November 1992 (330 sentences, 5353 words) and November 1993 (215 sentences, 3849 words) ARPA CSR benchmark test data respectively for 5K vocabularies (Paul and Baker, 1992; Woodland et al., 1994). The interpolation weights ϕ and λ are computed using the compute-best-mix program from the SRILM toolkit. They are tuned on the development test set. The latent semantic marginals (LSM) are created by the automatic transcription. Automatic transcription is the recognition result obtained after first-pass decoding of the evaluation data. The results of the experiments are noted on the evaluation test set. We evaluated the WER experiments using lattice rescoring. In the first pass, we used the back-off tri-gram background language model for lattice generation. In the second pass, we applied the interpolated model of the LM adaptation approaches for lattice rescoring. We record WERs (%) and error rate reductions (%) using different LMs for various topic and corpus sizes.The perplexities on the November 1993 and November 1992 test sets for different sizes of corpus are described in Tables 1 and 2respectively.From Tables 1 and 2, we can note that for the stand-alone adapted (A) model, only models with topic size 25 give better results than other topic sizes. This is due to the limitation of the SRILM toolkit that can mix only 10 models at a time and the lack of the local lexical regularities of the background model. However, the interpolation of the background and the adapted models (B+A) outperforms all the other above approaches for all topic and corpus sizes.The WER results of the experiments on different test sets for different corpus sizes are described in Tables 3 and 4respectively.From Tables 3 and 4, we can note that the stand-alone adapted (A) model for all topic and corpus sizes outperforms the background model for the November 1993 test set, and the best WERs are achieved for topic size 75 using the '87 corpus (9.8% (9.2–8.3%)) and topic size 25 using the '87–89 corpus (10.8% (8.3–7.4%)). For the November 1992 test set, the best result is obtained only for the '87–89 corpus using topic size 75 (6.5% (4.6–4.3%)). Here the WERs are over the background model. The interpolation of the background (B) and the adapted model (A) outperforms all the other above models (Haidar and O'Shaughnessy, 2011).In Haidar and O'Shaughnessy (2012a), the unigram scaling of the adapted (A) model through MDI adaptation using LSM was proposed, which outperforms the MDI adaptation of the background (B) model using LSM (Tam and Schultz, 2006). Here, we introduce the MDI adaptation to the (B+A) model using the LSM. The idea of MDI adaptation is to minimize the Kullback–Leibler (KL) distance between the adapted model and the other model (Kneser et al., 1997; Gildea et al., 1992). The perplexity results of the experiments using the November 1993 and November 1992 test sets for different sizes of corpus are explained in Tables 5 and 6respectively.From Tables 5 and 6, we can note that all the models outperform the background model except for the adapted model using the November 1992 test set. The unigram scaling of the interpolation of the background (B) and the adapted (A) models outperforms the unigram scaling of the background model (Tam and Schultz, 2006), the unigram scaling of the adapted model (Haidar and O'Shaughnessy, 2012a), and the (B+A) model (Haidar and O'Shaughnessy, 2011) for all topic and corpus sizes.The WER results of the experiments on different test sets for different corpus sizes are described in Tables 7 and 8.From Tables 3, 4, 7 and 8, we can note that the unigram scaling of the adapted (A) models using LSM outperforms the unigram scaling of the background (B) model using LSM for all topic sizes except for the November 1992 test set using the '87 corpus (Haidar and O'Shaughnessy, 2012a). For the '87 corpus, the proposed unigram scaling of the B+A model does not give any improvement over the n-gram-weighting (Haidar and O'Shaughnessy, 2011) adaptation approaches except for the November 1993 test set with topic size 50. However, the proposed unigram scaling of the (B+A) models outperforms all the other above approaches (Haidar and O'Shaughnessy, 2011, 2012a; Tam and Schultz, 2006) and the best results obtained for topic sizes 25 and 75 for the November 1993 and November 1992 test sets respectively using the '87–89 corpus. For the November 1993 test set using topic size 25 and the '87–89 corpus, it gives about 16.9% (8.3–6.9%), 13.7% (8.00–6.9%), 8.0% (7.5–6.9%), and 4.2% (7.2–6.9%) over the background model, the unigram scaling of the background model (Tam and Schultz, 2006), the unigram scaling of the adapted model (Haidar and O'Shaughnessy, 2012a), and the interpolation of the background and the adapted models (Haidar and O'Shaughnessy, 2011) respectively. For the November 1992 test set using topic size 75 and the '87–89 corpus, it gives about 19.6% (4.6–3.7%), 19.6% (4.6–3.7%), 15.9% (4.4–3.7%), and 5.1% (3.9–3.7%) over the background model, the unigram scaling of the background model (Tam and Schultz, 2006), the unigram scaling of the adapted model (Haidar and O'Shaughnessy, 2012a), and the interpolation of the background and the adapted models (Haidar and O'Shaughnessy, 2011) respectively. From the above experiments, we can note that adding more data we get better improvement for all topic sizes and test sets.The significance improvement in WER is done by using a matched-pair-test where the misrecognized words in each test utterance are counted. The p-values of the proposed unigram scaling of the (B+A) model are measured relative to the background model, the unigram scaling of the background model (Tam and Schultz, 2006), the unigram scaling of the adapted model (Haidar and O'Shaughnessy, 2012a) and the interpolation of the background model and the adapted models (Haidar and O'Shaughnessy, 2011), respectively. For the November 1993 test set using topic size 25 and the '87–89 corpus, the p-values are 4.0E−9, 0.00081, 7.4E−8, and 0.00175. For the November 1992 test set using topic size 75 and the '87–89 corpus, the p-values are 4.9E−6, 0.0071, 8.3E−7, and 0.00989. At a significance level of 0.01, the proposed approach is significantly better than the other models.Tables 9 and 10are used to describe the ASR results for deletion (D), substitution (S), and insertion (I) errors, and also the correctness (Corr) and accuracy (Acc) of the tri-gram language models. From the tables, we can note that the proposed unigram scaling of the B+A model reduces all types of errors, and improves correctness and accuracy relative to the background and other models (Tam and Schultz, 2006; Haidar and O'Shaughnessy, 2012a, 2011). Using the proposed approach, the deletion and insertion errors do not change much compared to the background and other models. Therefore, the substitution errors play an important role to improve the performance, i.e., more words can be recognized accurately using the proposed method than the background and other models. We can also note that the improvement of the A model can help to reduce the existing errors in the current approach.

@&#CONCLUSIONS@&#
We have proposed unsupervised language model adaptation approaches using LDA, LSM, and unigram scaling. A hard-clustering approach is applied on the document–topic matrix obtained in LDA analysis to form a topic set. Then, an n-gram weighting approach is used to compute the mixture weights of the component topic models. An adapted model is computed using the weighted combination of n-gram topic models. We have performed the experiments for various topic sizes. We have formed new adapted models by modifying the adapted models using unigram scaling, which minimizes the KL divergence between the new adapted models and the adapted models, subject to a constraint that the marginalized unigram probability distributions of the new adapted models are equal to the unigram probability distributions estimated by using the LDA models, called LSM. We created LSM for the automatic (recognition results after first-pass decoding of the evaluation test set) transcriptions. We have interpolated the background model with the adapted model to capture the local lexical regularities and scaled the interpolated model using the above LSM. We performed the experiments using the WSJ corpus with varying sizes on two different test sets and used the LM adaptation approaches in the second pass of decoding. We compared our approaches with traditional MDI adaptation approaches. We have seen that our proposed approach gives significant reductions in perplexity and WER over the traditional approaches used in the literature. Moreover, we have found that adding more data helps to get better improvement.