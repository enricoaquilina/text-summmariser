@&#MAIN-TITLE@&#
A modified quantized kernel least mean square algorithm for prediction of chaotic time series

@&#HIGHLIGHTS@&#
We propose a new method to predict the chaotic time series.The gradient descent method is used in M-QKLMS to reduce the steady-state MSE.The modified quantization method is incorporated in M-QKLMS to reduce the network size.The energy conservation relation of M-QKLMS in RKHS is derived.A sufficient condition for mean square convergence of M-QKLMS is provided.

@&#KEYPHRASES@&#
Quantized kernel least-mean-square,Gradient descent method,Coefficient update,Chaotic time series,

@&#ABSTRACT@&#
A modified quantized kernel least mean square (M-QKLMS) algorithm is proposed in this paper, which is an improvement of quantized kernel least mean square (QKLMS) and the gradient descent method is used to update the coefficient of filter. Unlike the QKLMS method which only considers the prediction error, the M-QKLMS method uses both the new training data and the prediction error for coefficient adjustment of the closest center in the dictionary. Therefore, the proposed method completely utilizes the knowledge hidden in the new training data, and achieves a better accuracy. In addition, the energy conservation relation and a sufficient condition for mean-square convergence of the proposed method are obtained. Simulations on prediction of chaotic time series show that the M-QKLMS method outperforms the QKLMS method in terms of steady-state mean square errors.

@&#INTRODUCTION@&#
Chaotic time series prediction has been widely used in economic data processing, signal processing, automatic control and many other fields. A proper prediction method can improve the prediction accuracy. Commonly used methods include neural network based methods [1], least-mean-square (LMS) methods [2], support-vector machines (SVM) [3], etc. However, in many cases, these methods cannot meet the requirements of accuracy, or are too complicated to be implemented in practice. Therefore, an efficient prediction method is of practical importance.Over the past decades, kernel adaptive filter (KAF) has become an emerging topic of research. KAF maps the input data into a high dimensional feature space that is also called reproducing kernel Hilbert spaces (RKHS). In RKHS, inner products can be easily computed by the kernel trick [2]. The representative examples of KAF include kernel least-mean-square (KLMS) algorithms [4], kernel affine projection algorithms (KAPA) [5], kernel recursive least squares (KRLS) methods [6], extended kernel recursive least squares (EX-KRLS) methods [7], and kernel maximum correntropy (KMC) methods [8].Since KAFs build a linear growing radial basis function (RBF) network [9] by allocating a new kernel unit for every new data sample as a RBF center, a linear growing network structure becomes the main bottleneck of KAFs, leading to increasing computational costs and memory requirements. If the sample size is very large, the practicability is highly restricted [10]. In order to solve this problem, a lot of online sparsification criteria have been proposed, e.g., approximate linear dependency (ALD) criterion [6], novelty criterion (NC) [11], surprise criterion (SC) [12], prediction variance criterion (PVC) [13], and coherence criterion (CC) [14]. The ALD can be viewed as a special case of the surprise criterion [12,15]. According to these sparsification criteria, only part of the input data are preserved in the dictionary, thereby resulting in dramatic reduction of the network size.Recently, a very simple vector quantization (VQ) method [15] has been proposed for online sparsification. The VQ method is imposed on KLMS and KRLS to generate the quantized kernel least-mean-square (QKLMS) algorithm [15] and the quantized kernel recursive least squares (QKRLS) algorithm [10], respectively. A quantization size is required in the VQ method. The distance between the current input and the dictionary is calculated for comparison. If the distance is greater than the quantization size, the current input data is added to the dictionary as a new center. Otherwise, the current input is discarded, but the coefficient of the closest center is updated. The main drawback of the VQ method is that during the update of coefficients, only the prediction error is used, and the input data are omitted. Although the distance between the discarded input data and the dictionary is less than the quantization size, the information of the input data should be utilized to update the corresponding coefficient for accuracy improvement.To further improve the prediction accuracy of chaotic series, a modified quantized kernel least-mean-square (M-QKLMS) is proposed in this paper. In M-QKLMS, a gradient descent algorithm is used to update the coefficient, and in the process, both the prediction error and the input data are considered. Thus, the M-QKLMS method offers a better prediction accuracy than initial QKLMS.The Mercer kernel is a continuous, symmetric and positive definite functionk(⋅,⋅):U×U⟶R, which is widely used to compute the inner product of RKHS. When the input–output pair{u(n),d(n)}is given, the problem is to find a mapping f to reconstruct the corresponding outputf(u(n))=〈f(⋅),k(⋅,u(n))〉. Generally, there existsd(n)=f⁎(u(n))+v(n), wheref⁎(u(n))denotes the optimal estimate ofd(n), andv(n)stands for the disturbance noise of nonlinear mappings. According to the representer theorem [16], thef(⋅)can be written as a linear form in RKHS, i.e.,(1)f(⋅)=ΩTφ(⋅),whereφ(⋅)is a nonlinear function which transfers the input data from the initial space to RKHS; and Ω is the weight vector in RKHS, which can be expressed as a linear combination of the transformed inputs until the nth training iteration, namely,(2)Ω=∑j=1nwjφ(u(j)).According to the kernel trick, i.e.,k(⋅,⋅)=φ(⋅)Tφ(⋅), Eq. (1) can be rewritten as(3)f(⋅)=∑j=1nwjk(⋅,u(j)),wherewjis the kernel weight; andk(⋅,u(j))is a kernel function with the centeru(j).Generally, the Gaussian kernel is most commonly used kernel in KAF because of its universal approximation capability, desirable smoothness and numerical stability, which is defined as(4)k(u,u′)=exp⁡(−‖u−u′‖2/2h2),where the parameterh>0is the kernel bandwidth. In the rest of the paper, the Gaussian kernel is a default choice.The quantized kernel least-mean-square (QKLMS) algorithm [15] can be described as follows(5){f0=0e(n)=d(n)−fn−1(u(n))fn=fn−1+ηe(n)k(Q[u(n)],⋅),whereu(n)is the input vector of N dimensions, f is the mapping between the input and output, η is the step size, andQ[⋅]denotes quantization operation.LetDl(n−1)denote the lth center of the dictionaryD(n−1)at discrete timen−1, and‖⋅‖denote the Euclidean norm in input spaceU. The VQ method can be divided into the following three steps.(1) Choose the quantization sizeγ≥0, and initialize the dictionaryD(1)={u(1)}.(2) Compute the distance between the current inputu(n)and dictionaryD(n−1), which can be expressed asdis(u(n),D(n−1))=min1≤l≤size(D(n−1))∥u(n)−Dl(n−1)∥.(3) Compare the distancedis(u(n),D(n−1))with the quantization size γ. Ifdis(u(n),D(n−1))>γ, update the dictionary usingD(n)={D(n−1),u(n)}. Otherwise, the dictionary is unchanged, i.e.,D(n)=D(n−1)), andu(n)is quantized to the closest center of the dictionary, i.e.,uq(n)=Dl⁎(n−1), wherel⁎=arg⁡min1≤l≤size(D(n−1))∥u(n)−Dl(n−1)∥. The coefficient of the closest center is updated as follows(6)wl⁎(n)=wl⁎(n−1)+ηe(n).According to VQ, when the input isu(n), the outputy(n)of QKLMS can be expressed as(7)y(n)=∑l=1mwl(n−1)k(Dl(n−1),u(n))=w(n−1)kT(n),wherew(n−1)=[w1(n−1),w2(n−1),…,wm(n−1)]is the coefficient vector in RKHS,m=size(D(n−1))is the length of dictionary, andk(n)=[k(D1(n−1),u(n)),k(D2(n−1),u(n)),…,k(Dm(n−1),u(n))].The prediction error is defined ase(n)=d(n)−y(n), whered(n)is the desired signal. Without loss of generality, we define the following cost function:(8)J=12e2(n).In order to perform the operation of quantization, a quantization size γ is set in advance. The distancedis(u(n),D(n−1))between the current inputu(n)and the dictionaryD(n−1)is therefore computed for comparison. There are two cases in the process of filtering. Case I:dis(u(n),D(n−1))is less than the quantization size γ, and the current inputu(n)is not added into the dictionary. Case II:dis(u(n),D(n−1))is greater than the quantization size γ, and the current inputu(n)is added into the dictionary.Case I: The sizes of the coefficient vector and the dictionary are unchanged. We only update the coefficient of the closest centerwl⁎(n−1). Since the second-order derivative of the cost function in Eq. (8) is non-negative, the function of (8) is a convex function ofwl⁎(n−1). Using the gradient descent method, from equations (7) and (8), we have(9)wl⁎(n)=wl⁎(n−1)−η∇wl⁎(n−1)J=wl⁎(n−1)+η[0,…,1,…,0]kT(n)e(n)=wl⁎(n−1)+ηk(Dl⁎(n−1),u(n))e(n),where [0,…,1,…,0] is an m-dim vector with the corresponding element of the closest centerl⁎being 1 and the others being 0, and∇wl⁎(n−1)Jis the gradient of the cost function J with respect towl⁎(n−1). Therefore, the updated form of the coefficient vector can be rewritten as(10)w(n)=w(n−1)+η[0,…,k(Dl⁎(n−1),u(n)),…,0]e(n).For simplicity, definingk(l⁎)=[0,…,k(Dl⁎(n−1),u(n)),…,0]. Then, (10) can be simplified as(11)w(n)=w(n−1)+ηk(l⁎)e(n).Case II: The sizes of the coefficient vector and the dictionary increase. The updated form of the coefficient vector can be written as(12)w(n)=[w(n−1),0]+η[0,0,…,0︸m,k(u(n),u(n))]e(n)=[w(n−1),0]+η[0,0,…,0︸m,1]e(n)=[w(n−1),0]+η[0,1]e(n),where0=[0,0,…,0]is an m-dim zero vector.Combining Cases I and II, the proposed M-QKLMS can be summarized in Algorithm 1.Remark 1The gradient descent method is a powerful tool to solve the problem of optimization. Since the cost function J of Eq. (8) is a convex function regardingwl⁎(n−1), the proposed M-QKLMS method can avoid falling into a local minimum value.Remark 2The M-QKLMS algorithm updates the coefficient of the dictionary by using the gradient descent method, which is an improvement of QKLMS. From Eq. (6), QKLMS updates the coefficient only by the prediction error, which can be regarded as an approximation of the gradient descent method. Moreover, from Eq. (9), M-QKLMS incorporates both the prediction error and the new training data to update the coefficient. Hence, compared with QKLMS, M-QKLMS can achieve more accurate updated coefficients. In addition, the modified vector quantization can be easily extended to other KAFs, e.g., KRLS [6].The energy conservation relation is a powerful tool for the mean square convergence analysis. In the following, we derive the energy conservation relation of the M-QKLMS algorithm in RKHS. A sufficient condition for mean-square convergence is provided.The desired signald(n)is expressed as follows:(13)d(n)=w⁎kT(n)+v(n),wherew⁎is the optimal coefficient vector, andv(n)stands for the disturbance error.The prediction errore(n)denotes the difference between the desired signald(n)and the actual output signaly(n), which can reflect the prediction accuracy of a new algorithm. Combining (7) and (13),e(n)becomes(14)e(n)=d(n)−y(n)=(w⁎−w(n−1))kT(n)+v(n)=wˆ(n−1)kT(n)+v(n)=ea(n)+v(n),wherew˜(n−1)=w⁎−w(n−1)is the coefficient error vector andea(n)=w˜(n−1)kT(n)the a priori error at iteration n. The variable ofea(n)measures how close the actual outputy(n)is to the optimal estimate ofd(n), namelydˆ(n)=w⁎kT(n).According to the M-QKLMS algorithm, the energy conservation relation is discussed under the two cases.For case I, the size ofw⁎is unchanged. Subtractingw⁎from both sides of Eq. (11) yields(15)w˜(n)=w˜(n−1)−ηk(l⁎)e(n),wherew˜(n)=w⁎−w(n).Multiplying both sides of Eq. (15) bykT(n)from right generates(16)w˜(n)kT(n)=w˜(n−1)kT(n)−ηk(l⁎)e(n)kT(n).After some simple manipulations of Eq. (16), we get(17)ep(n)=ea(n)−ηe(n)k(l⁎)kT(n)=ea(n)−ηe(n)[0,…,k(Dl⁎(n−1),u(n)),…,0]kT(n)=ea(n)−ηe(n)k2(Dl⁎(n−1),u(n)),whereep(n)=w˜(n)kT(n)denotes the a posterior error. Different fromea(n), theep(n)measures the error in estimatingd(n)by usingw(n)kT(n), i.e., by using the new coefficient vector estimate.Combining Eqs. (15) and (17) and eliminating the prediction errore(n), we get(18)w˜(n)=w˜(n−1)+(ep(n)−ea(n))×[0,…,1k(Dl⁎(n−1),u(n)),…,0]=w˜(n−1)+(ep(n)−ea(n))g(l⁎),whereg(l⁎)=[0,…,1k(Dl⁎(n−1),u(n)),…,0].Squaring both sides of Eq. (18) gives(19)w˜(n)w˜T(n)=[w˜(n−1)+(ep(n)−ea(n))g(l⁎)][w˜(n−1)+(ep(n)−ea(n))g(l⁎)]T=w˜(n−1)w˜T(n−1)+(ep(n)−ea(n))2g(l⁎)gT(l⁎)+2(ep(n)−ea(n))w˜(n−1)gT(l⁎)=w˜(n−1)w˜T(n−1)+(ep(n)2−ea(n)2)g(l⁎)gT(l⁎)+2(ep(n)−ea(n))(w˜(n−1)gT(l⁎)−ea(n)g(l⁎)gT(l⁎)).It is obvious thatg(l⁎)gT(l⁎)=1/k2(Dl⁎(n−1),u(n)). Then, we have(20)‖w˜(n)‖F2+ea(n)2k2(Dl⁎(n−1),u(n))=‖w˜(n−1)‖F2+ep(n)2k2(Dl⁎(n−1),u(n))+β1,where‖w˜(n)‖F2=w˜(n)w˜T(n)denotes the norm in RKHS;β1is expressed as(21)β1=2(ep(n)−ea(n))(w˜(n−1)gT(l⁎)−ea(n)g(l⁎)gT(l⁎)).Eq. (20) is the energy conservation relation for case I.For case II, the size ofw⁎increases. Similarly, we can also consider that there exists an optimal coefficient vectorw⁎for every iteration. Subtractingw⁎from both sides of Eq. (12) gives(22)w˜(n)=[w˜(n−1),w⁎(n)]−η[0,1]e(n),wherew⁎(n)is the latest element ofw⁎.Multiplying both sides of Eq. (22) bykT(n)from right (notice that the size ofkT(n)also change), we have(23)ep(n)=ea(n)+w⁎(n)k(u(n),u(n))−ηk(u(n),u(n))e(n)=ea(n)+w⁎(n)−ηe(n),whereep(n)=w˜(n)kT(n)andea(n)=w˜(n−1)kT(n−1)denotes a posterior error and a priori error, respectively.Combining Eqs. (22) and (23), and eliminating the prediction errore(n), we get(24)w˜(n)=[w˜(n−1),w⁎(n)]+(ep(n)−ea(n)−w⁎(n))[0,1]=[w˜(n−1),w⁎(n)]+[0,(ep(n)−ea(n)−w⁎(n))].Squaring both sides of Eq. (24) gives(25)w˜(n)w˜T(n)=w˜(n−1)w˜T(n−1)+ep(n)2−ea(n)2−2ea(n)(ep(n)−ea(n)).Hence, the energy conservation relation for case II is(26)‖w˜(n)‖F2+ea(n)2=‖w˜(n−1)‖F2+ep(n)2+β2,whereβ2=2ea(n)(ep(n)−ea(n)).For case I, substitutingep(n)=ea(n)−ηe(n)k2(Dl⁎(n−1),u(n))into the energy conservation relation of Eq. (20). After some simple manipulations, we have(27)‖w˜(n)‖F2=‖w˜(n−1)‖F2+η2e2(n)k2(Dl⁎(n−1),u(n))−2ηe(n)w˜(n−1)kT(l⁎).Taking expectations of both sides of Eq. (27), we have(28)E[‖w˜(n)‖F2]=E[‖w˜(n−1)‖F2]+η2k2(Dl⁎(n−1),u(n))×E[e2(n)]−2ηE[e(n)w˜(n−1)kT(l⁎)].In order to analyze the mean square convergence, we make the following assumptions that will be used in the rest of this paper.A1: The noisev(n)is the zero-mean.A2: The noisev(n)is independent, identically distributed (i.i.d.), with varianceδv2=E[v(n)2].A3: The noisev(n)is independent of inputu(n).Asea(n)=w˜(n−1)kT(n), according to the relationship betweenu(n)andkT(n), we can find thatea(n)is independent ofv(n).In order to guarantee a convergence solution, the energy of the weight-error vector should decrease gradually, i.e.,(29)E[‖w˜(n)‖F2]≤E[‖w˜(n−1)‖F2].Then, Eq. (28) becomes(30)η2k2(Dl⁎(n−1),u(n))E[e2(n)]≤2ηE[e(n)w˜(n−1)kT(l⁎)].According to A1, A2 and A3, Eq. (30) can be rewritten as(31)ηk2(Dl⁎(n−1),u(n))(E[ea2(n)]+δv2)≤2E[ea(n)w˜(n−1)kT(l⁎)].Then, we get(32)η≤2E[ea(n)w˜(n−1)kT(l⁎)]k2(Dl⁎(n−1),u(n))(E[ea2(n)]+δv2).Hence, to guarantee the power of the weight-error vector being monotonically decreasing, the step size should satisfy(33)0<η≤2E[ea(n)w˜(n−1)kT(l⁎)]k2(Dl⁎(n−1),u(n))(E[ea2(n)]+δv2),∀n.There exists such a step size that requires(34)E[ea(n)w˜(n−1)kT(l⁎)]>0,∀n.Therefore, we can obtain the following sufficient condition for mean-square convergence:(35){E[ea(n)w˜(n−1)kT(l⁎)]>00<η≤2E[ea(n)w˜(n−1)kT(l⁎)]k2(Dl⁎(n−1),u(n))(E[ea2(n)]+δv2).For case II, substitutingep(n)=ea(n)+w⁎(n)−ηe(n)into the energy conservation relation Eq. (26). After some simple manipulations, we get(36)‖w˜(n)‖F2=‖w˜(n−1)‖F2+w⁎(n)2+η2e2(n)−2ηw⁎(n)e(n).Taking expectations of both sides of Eq. (36), we have(37)E[‖w˜(n)‖F2]=E[‖w˜(n−1)‖F2]+E[w⁎(n)2]+η2E[e2(n)]−2ηE[w⁎(n)e(n)].In order to guarantee a convergence solution, we should have(38)E[‖w˜(n)‖F2]≤E[‖w˜(n−1)‖F2].It is readily shown that(39)E[‖w˜(n)‖F2]≤E[‖w˜(n−1)‖F2]+E[w⁎(n)2].Similar to case I, we have(40){E[w⁎(n)ea(n)]>00<η≤2E[w⁎(n)ea(n)]E[ea2(n)]+δv2.Combining Eqs. (35) and (40), a sufficient condition for mean-square convergence of the M-QKLMS algorithm is summarized as follows:(41){min⁡{θ1,θ2}>00<η≤min⁡{η1,η2},whereθ1=E[ea(n)w˜(n−1)kT(l⁎)]andθ2=E[w⁎(n)ea(n)],η1=2E[ea(n)w˜(n−1)kT(l⁎)]k2(Dl⁎(n−1),u(n))(E[ea2(n)]+δv2)andη2=2E[w⁎(n)ea(n)]E[ea2(n)]+δv2.In this section, the proposed M-QKLMS method is used to perform prediction of chaotic time series. The presented examples include Chua's chaotic time series and Mackey–Glass chaotic time series. For all experiments, the kernel width of the Gaussian kernel is set ash=1, and the noise is Gaussian white noise with varianceδv2=0.01. In fact, the Signal-to-Noise Ratios (SNR) are 21 dB and 19 dB for Chua's chaotic time series and Mackey–Glass chaotic time series, respectively.Chua's circuit in a dimensionless form [17,18] is written as(42){dxdt=α[y−x−f(x)]dydt=x−y+zdzdt=−βy,where α> 0 and β> 0. Also,f(x)is a piecewise-linear function given by(43)f(x)={cx+c−d,x≤−1dx,−1<x<1cx+d−c,x≥1,whered<c<0.To make the system (42) be chaotic, we setα=10,β=15,c=−0.9,d=−1.7, and the initial conditions arex(0)=0.6,y(0)=−0.2,z(0)=0.3. The time series is discretized at a sampling period of 0.07 second. In the following, the second coordinate y is chosen for prediction. A segment of initial Chua's chaotic time series and its noise contained series are shown in Fig. 1.First, the choice of filter order in the prediction of Chua's time series is discussed. Fig. 2shows the steady-state mean-square error (MSE) of the M-QKLMS methods versus the filter order in the Chua's chaotic time series prediction, where the step sizeη=0.2. Herein, for each filter order, 30 independent Monte Carlo simulations are run with different segments of time series. In each segment, the training data size is 1000 and the test data size is 100. Also, the last 100 train iterations are used to compute the final steady-state MSE. It can be seen from Fig. 2 that, when the filter order is about 5, the steady-state MSE can reach the minimum value. Hence, five previous points are used as the input vector to predict the present oney(n)in Eq. (42), i.e.,d(n)=y(n). In the following, the step sizeη=0.2is used both in QKLMS and M-QKLMS methods.Then, we compare the steady-state MSE of the QKLMS and M-QKLMS methods versus the quantization size in Chua's chaotic time series prediction in Fig. 3. It can be seen from Fig. 3 that when the quantization size is greater than some fixed value, the steady-state MSE of the M-QKLMS method is smaller than that of the QKLMS method. In Fig. 4, we plot the MSE curves of the QKLMS and M-QKLMS methods when quantization size is set as 2.0, which is averaged over 100 simulations to minimize turbulence. From Fig. 4, we see that the steady-state MSE of the M-QKLMS method is smaller than that of QKLMS, which is consistent with the result in Fig. 3. Finally, the final network size versus the quantization size is plotted in Fig. 5. It can be seen from Fig. 5 that, with the increase of quantization size, the network size decreases as expected. The M-QKLMS method can also implement online sparsification of the network structure.The Mackey–Glass (MG) chaotic time series [2,19] displays characteristics of periodic and chaotic systems, which is generated by the following delayed differential equation.(44)dx(t)dt=−bx(t)+ax(t−τ)1+x(t−τ)p.whereb=0.1,a=0.2,p=10,τ=30. The time series is discretized using a sampling period of 6 seconds.In this experiment,u(n)=[x(n−7),x(n−6),…,x(n−1)]are used as the input to predict the current valuex(n)which is the desired signal. The relation between the quantization size and the prediction accuracy of QKLMS and M-QKLMS methods is plotted in Fig. 6, where the parameters are same as in the prediction of Chua's chaotic time series except that the step sizeη=0.1. Also, the MSE curves of QKLMS and M-QKLMS methods when quantization size is set as 1.5 is shown in Fig. 7. Herein, 200 Monte Carlo simulations are run to reduce the turbulence. We can see from Figs. 6 and 7 that the simulation results of the Mackey–Glass chaotic time series are coincident with those of Chua's chaotic time series. Thus, from the prediction of the two kinds of chaotic time series, the M-QKLMS method offers a better prediction accuracy than the QKLMS method.

@&#CONCLUSIONS@&#
Unlike the quantized kernel least-mean-square method that only uses the prediction error to adjust the coefficient of the closest center, the proposed modified algorithm utilizes gradient descent algorithm to update the coefficient. Both the prediction error and the new training data are incorporated in the proposed method. Hence, a more accurate updated coefficient is acquired, resulting in better prediction accuracy of chaotic time series. Furthermore, the modified method can reduce the network size, which is a significant improvement in practice. Simulations on prediction of chaotic times series show that, when the quantization size is greater than a fixed value, the proposed algorithm has a smaller steady-state mean-square error compared to the original quantized kernel least-mean-square method.