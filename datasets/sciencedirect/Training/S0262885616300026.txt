@&#MAIN-TITLE@&#
Enhancing feature tracking with gyro regularization

@&#HIGHLIGHTS@&#
A 3-axis gyro mounted to a video camera can frequently predict the main component of optical flow.The gyro-predicted flow can be used to regularize feature tracking to help track ambiguous features through bad imagery.Gyro regularization does not require all features to belong to a rigid scene.Gyro regularization adds very little computational cost to feature tracking.The common practice of using gyros to initialize trackers offers no advantage over careful optical-only initialization.

@&#KEYPHRASES@&#
Feature tracking,Optical flow,Inertial sensors,Gyroscopes,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Feature tracking is the task of determining and maintaining the location of one or more visually interesting points as they move about in motion video. This task is crucial in computer vision, where it is frequently used as a first step in solutions to important problems like simultaneous localization and mapping (SLAM) and structure from motion (SFM). A common solution is to characterize each feature using a template image, which is a small image centered on the feature, extracted from a recent frame. This template is updated periodically, and the feature's location in each new frame is determined by searching through the new imagery for the region that is most common to the template. The celebrated Kanade–Lucas–Tomasi (KLT) feature tracker [1–4] achieves this, for instance, using Gauss–Newton optimization to find the location in a new image that minimizes the mean-squared difference between the image and the template.For a given camera, the location of a feature in the image plane is a function of the location of the corresponding 3D world point relative to the coordinate frame of the camera. If we imagine a “world frame” fixed to the environment, then the motion of a feature in the image plane (which we will refer to as a feature's flow) can be decomposed into two quantities: motion of the corresponding 3D point relative to the world frame, and movement of the camera frame relative to the world frame. This decomposition is useful because motion of world points is often much slower when measured in the coordinate frame of their environment than when measured in the coordinate frame of the camera. If the motion of the camera relative to the environment can be measured independently, then the component of a feature's flow due to camera egomotion can be predicted a-priori, leaving only the smaller component due to motion through the environment to be determined. This is especially helpful in hand-held camera applications, where camera rotation can completely dominate other sources of flow.Independently measuring camera motion can be quite challenging. Position and orientation can often be determined by measuring external signals like GNSS (Global Navigation Satellite Signals). Such external signals are not always detectable or reliable however, and exploiting them can require additional undesirable hardware (like GNSS antennas). Additionally, for the purpose of predicting a feature's flow we actually only need to measure relative motion (from one frame to the next), instead of knowing our pose relative to an absolute reference frame. In this setting, inertial sensors (which measure accelerations due to specific forces and rotation rates) are a desirable alternative because they can offer excellent relative motion measurements over short time intervals without relying on external signals. Additionally, these sensors have become very low cost in recent years and have made their way into all kinds of consumer electronics (like smart phones and tablet computers). It is now the case that many devices one might use to collect video already happen to have inertial sensors built into them. This makes them a natural choice for integration with feature trackers.Inertial sensors can be used to estimate relative changes in camera orientation and/or position between two frames and predict the component of the optical flow field due to camera egomotion. The simplest way to exploit this information is to use the predicted flow field to initialize and bound the search for features in the new frame. This has been found to be a successful strategy by several authors (see Section 1.1). While this strategy can reduce the number of candidate locations for a feature in a new frame of video, it still relies entirely on the imagery for selecting the best candidate. We propose an additional level of integration where we regularize the tracking energy function to gently penalize deviation from our prior estimate of flow. Thus, in addition to reducing the number of candidate locations for a feature, our method can help differentiate between them when the imagery is not distinctive enough to reveal the location on its own.The rest of the paper is organized as follows. We review previous work in Section 1.1 and detail the contributions of this work in Section 1.2. Template-based feature tracking is reviewed in Section 2. In Section 3 we summarize how a prior estimate of optical flow can be computed using a gyroscope. In Section 4 we introduce our method of exploiting this prior flow estimate for feature tracking. In Section 5 we present results which demonstrate that our technique offers significant improvements in performance over conventional template-based tracking methods, and is in fact competitive with much more computationally expensive state-of-the-art trackers, but at a fraction of the cost. Section 6 explores possible areas for future research. Finally, Section 7 concludes this work.Gyroscopes have been used in conjunction with optical systems for multiple purposes in recent years. Perhaps the most ubiquitous application is image stabilization. This broadly refers to two different problems. The first is to reduce blur in individual images during long-exposure acquisitions (e.g., low-light photography). This can be done by detecting camera rotation during the acquisition period and driving actuators connected to the imager to mechanically compensate for the motion [5] (this is called optical image stabilization, or OIS), or it can be done by using the detected camera motion to deblur the image in post-processing (as in [6]). The second problem is to reduce the shakiness of video by measuring rotation during video acquisition and translating frames in the sequence to compensate for short, rapid motions, resulting in video that only reflects the smoother camera motion [7].Gyro-optical integration has also been used to improve feature tracking for computer vision applications. You et al. [8] used gyroscopes to predict feature motion in the image plane of a camera and restrict the search space for feature tracking. Feature trajectories were then used in turn to yield driftless attitude estimates for use in augmented reality systems. Hwangbo et al. [9] used gyroscopes to estimate relative changes in orientation to predict feature motion for initializing a KLT feature tracker (and to pre-warp templates), in order to provide more robust feature tracking for vision applications such as 3D reconstruction.Perhaps the most studied application of integrated optical and inertial sensors has been in aiding inertial navigation systems when conventional sources of navigation information (like GNSS) are unavailable or untrustworthy. Pachter and Mutlu [10] proposed tracking and geo-localizing unknown terrain features and using them as landmarks for self-localization when conventional aides are unavailable. Mourikis et al. [11] and Roumeliotis et al. [12] exploit feature trackers to generate measurements to aid inertial navigation systems for space travel and space vehicle descent. Jones and Soatto [13] use trajectories from a KLT feature tracker with an inertial navigation system in a modern framework for robust SLAM. While these methods employ loose integration between optical and inertial systems, other authors have proposed tighter integration (where the inertial system plays an important role in feature tracking). Hol et al. [14] used measured changes in both position and orientation to predict the motion of features in the image plane and bound the search space for those features. They located features by minimizing the sum-of-absolute-differences between imagery in the search region and feature template images. They used trajectories in turn to provide corrections to the navigation system via a Kalman filter. Veth and Raquet [15,16] implemented a similar optical–inertial system, while matching SIFT descriptors to locate features in the search region. Gray [17] extended this work to achieve deeper integration between the inertial and optical systems by predicting perturbations in feature descriptors that result from changes in system pose and directly compensating feature descriptors to achieve better matching performance. Predicting feature motion due to sensor translation is considerably more challenging than predicting motion due to rotation, as it requires estimating the range from the camera center to each tracked feature. To deeply integrate feature tracking with a full inertial navigation system one must employ some method of estimating feature range ([14] assumes that a 3D scene model is known, [15] and [16] use a binocular camera, and [17] presents results using binocular systems and monocular systems with range estimated online from motion). An alternative is to neglect the effect of camera translation on feature motion (when rotation is dominant). Diel et al. [18] used gyroscopes to measure relative changes in camera orientation and pre-warped imagery to compensate, thereby making feature tracking simpler. They used tracked features to derive epipolar constraints and applied them to an inertial navigation system through a Kalman filter.Our main contribution is a novel method for deeply integrating inertial sensors with template-based feature tracking. We directly modify the tracking energy function to exploit inertial measurements as a prior estimate of feature position. This modification aids the tracker in localizing features in directions where the imagery is ambiguous (directions where the unmodified energy function is relatively flat). Unlike our proposed method, previous works in integrating inertial sensors with feature trackers generally achieve lower levels of sensor integration. Many methods are very loosely integrated, where feature tracking is nearly independent of the inertial system [11,12] and realizes no benefit from these other sensors. Other methods exploit the inertial sensors (and sometimes additional sources of navigation information) to initialize and bound the search for features in the image plane [8,9,14–17]. Some techniques also use this information to pre-warp feature template images [9], or to correct feature descriptors to better match new imagery [17]. However, to our knowledge, ours is the first method to directly regularize the tracking energy function using gyro-predicted optical flow.Our proposed solution competes with state-of-the-art feature trackers in performance, but has only a fraction of the computational cost of some competing methods. In addition, we show that using gyro-predicted optical flow to merely initialize a feature tracker (like KLT) offers no advantage over a careful optical-only initialization method (see Eq. (17)). This suggests that in some of the advances reported by other authors the gyroscopes were not really needed to achieve the gains from better initialization. A deeper level of integration, like the one proposed here, may be necessary to realize a genuine tracking performance improvement from these inertial sensors.The goal of feature tracking is to determine the location of a small point or feature as it moves about in motion video. This work focuses on template-based feature tracking, where a feature is characterized by a small (usually square) template image that describes the expected appearance of the feature. This fundamentally distinguishes feature tracking from object tracking, where the target is potentially larger and can change significantly in appearance between frames, requiring more sophisticated techniques for target characterization. In template-based feature tracking, the goal is usually to locate a feature by minimizing the single-feature energy function:(1)c(x)=1n2∑u∈ΩψT(u)−I(u+x),where:Ttemplate image characterizing the featurewidth and height of template imageframe of video in which we are trying to locate the featureloss function. Typically ψ(y)=|y| or ψ(y)=|y|2the candidate location of the feature in the new framethe set of locations where T is defined: {(i,j) where i,j ∈{1,2,…,n}}.Intuitively, we are overlying our template image T on the video frame I in a location governed by the input x. Then, for each pixel in the template, we compute the difference in intensity between T and I and take the square or absolute value of the result. For each pixel in the template, this produces a measure of dissimilarity between the template and the image which is 0 when they are equal and positive otherwise. Our energy is the average of these values over all pixels in the template. Thus, when we minimize Eq. (1), we are effectively sliding our template around over the video frame, looking for the location where the image looks most similar to the template.Generally, there is a maximum distance which a feature is expected to move between two consecutive frames. Thus, the energy in Eq. (1) is minimized in a local neighborhood of its previous position. The minimization can be performed through exhaustive search, using 1st-order methods like gradient descent, or using higher-order schemes like Gauss–Newton minimization (as in the KLT tracker). Additionally, most feature trackers are implemented in a coarse-to-fine framework, where tracking is done in stages: first on lower-resolution imagery and then subsequently refined on higher-resolution imagery [19]. This is to increase the likelihood that in any given stage the initial estimate of a feature's position is within the region of convergence of the true minimum of Eq. (1).As an alternative to tracking each feature independently, some modern feature trackers have as their state variable the joint positions of a collection of many (or all) features in the scene. Hence, they iteratively refine the position estimates of each feature simultaneously, as opposed to one after another. This allows them to impose constraints on how features can move relative to one other. An example of this is [20], where features are encouraged, through a penalty term, to maintain trajectories over a short temporal window which lie in a low-dimensional subspace. We refer to such tracking methods as multi-feature trackers or in short multi-trackers.In the multi-feature tracking framework the state variable encodes the joint positions of a collection of many (or all) features in the scene. The main energy function for multi-feature tracking, which we call the multi-feature energy function, is formed by simply combining all of the single-feature energy functions of a collection of F tracked features:(2)The state variable x is a 2F-element vector, where elements (2f−1) and 2f form the position of feature f. The matrix Sfextracts these coordinates from x. The inner sum in Eq. (2) is nothing more than the standard single-feature energy function Eq. (1) for feature f. The outer sum adds up these energy functions for each feature being tracked. Notice though that each single-feature energy function depends on a different pair of coordinates in the state vector, x. Thus, minimizing Eq. (2) is equivalent to minimizing each single-feature energy function separately. It only becomes different if we impose constraints or add additional terms that introduce interactions between the locations of different features. As with single-feature trackers, there are many ways this energy can be minimized, and it is often implemented in a multi-resolution, coarse-to-fine framework.It is important to understand the limitations inherent in both single-feature and multi-feature tracking. In single-feature tracking, unless one pulls in additional sources of information, it is not generally possible to track arbitrary features. For instance, if you are trying to track a feature in a completely textureless, uniform portion of an image, then it can be readily verified that the energy function Eq. (1) becomes completely flat. When this happens, there is no unique minimizer and the flow of the feature is not observable in the imagery. A less extreme example would be tracking a feature along an edge in an image. In this case, sliding the template transverse to the edge results in an energy increase, but sliding the template along the edge does not change the energy. In this situation it is theoretically possible to partially resolve the flow of the feature (i.e. in the direction orthogonal to the edge). However, template-based trackers are not designed to exploit such partial information and will instead try to fully resolve the location of the feature. They will select what they see as the best minimum, even when there are several, nearly identical candidates. The result is that such features can wander and jump around along the edges to which they belong. Without additional sources of data or additional assumptions about how features interact, the multi-feature tracking framework suffers from the same problem because it essentially minimizes the single-feature energy functions for a set of features.One straightforward way to address this issue is to simply not attempt to track features which are less than ideal. For instance, [2] defines a criterion for a template that ensures that the corresponding feature is distinctive enough to make tracking possible and suggests discarding features that don’t meet the criterion. Many other criteria have been proposed in the literature, but all effectively try to select corner-like features and have the shared goal of screening out features that are likely to cause problems in tracking. This is a reasonable approach to the problem but it is not universally appropriate. For instance, when using trajectories for the purpose of 3D reconstruction, one wants long feature trajectories that survive long enough to allow individual features to be viewed from many different poses. Replacing features when they become difficult to track has the contrary effect of building a larger set of shorter-lived feature trajectories. Navigation applications also tend to prefer longer feature trajectories. Additionally, there are phenomena that can negatively impact many or all features in a scene together, such as poor lighting conditions. The ability to track through even short-duration events like this can be quite valuable in these applications.When trying to track a collection of less-than-ideal features the flow of each feature may not be independently observable in the imagery. Thus, some additional assumptions or sources of information must be brought in to make this possible. In [20] a low rank constraint is used as a way of sharing information between features to make low-quality features more trackable. In this work, we bring in an outside estimate of the flow of each feature, in particular, the (often dominant) component due to camera rotation. We rely on this to help locate features in directions where the imagery is not distinctive enough for a features location to be discerned from the imagery alone. Before we discuss how we use this outside estimate, we develop the equations used to produce the estimate using gyroscopes rigidly mounted to our camera.In our application we estimate the component of optical flow that is due to changes in camera orientation between frames to use as our prior estimate of flow in feature tracking. In many situations this is the dominant source of flow simply because cameras are often free to rotate much faster than they can move through their environments (or relative to other visible objects). For the purpose of illustration, consider a common cellular phone camera with 50° diagonal angle of view. In hand-held applications it is not uncommon for the camera to achieve rotation rates (in part due to unintentional camera “shaking”) of 350°/s. If images are being collected at 30fps, this camera rotation induces motion in the image plane of up to 23% of the image diameter between consecutive frames11The magnitude of the flow in pixels depends on the sensor resolution. If the sensor has a resolution of 600×800 (1000pixels across the diagonal), this would correspond to a maximum displacement of 230pixels.. For a non-rotating camera viewing an object 50ft. away, in order to generate a comparable flow in the image plane the object would need to be moving at least 199mph (320km/h). In many situations objects are not moving nearly this fast, and camera rotation is therefore the dominant source of flow.A typical camera can be modeled using the perspective model [21]. With this model, there is a calibration matrixK associated with the camera, depending only on the internal characteristics of the image sensor and optics, such that the image of a 3D point with coordinates X in the coordinate frame of the camera isx=KI,0X. Both X and x are expressed in homogeneous coordinates. The matrix K has the following form:(3)K=fx0a00fyb0001,where fx, fy, a0, and b0 are constants associated with the imaging sensor and optics. This model is relatively general since most camera non-idealities (e.g., radial distortion) can be compensated for, when necessary, explicitly in pre-processing.Next, we must identify what information we are able to extract from the gyroscopes. Ideally, they would allow us to measure the rotation of the camera between arbitrary instants in time. Unfortunately, however, gyro measurements are taken in the coordinate frame of the gyro sensor package, which is not generally aligned with the camera frame. We will assume that there is some fixed rotation matrix that takes vectors in the gyro sensors coordinate frame to the cameras internal coordinate frame (this assumption requires that the gyro be physically mounted to the camera so that they experience the same rotations).We will be considering the camera at two instants in time corresponding to acquisition times of two consecutive frames of video. We will refer to these instants as time 0 and time 1, respectively. We will let X0 represent the position of a feature at time 0 (in homogeneous coordinates), in a coordinate frame which we will refer to as the gyro frame. This frame has its origin at the camera center and has its axes parallel to those of the coordinate frame of the gyro sensor package. Similarly, we will let X1 represent the position of the feature at time 1 in the gyro frame. Let RGyroCam be the fixed rotation matrix that takes vectors in the gyro frame to the cameras internal coordinate frame (these coordinate frames share the same origin in space so there is no translational component to this transformation). Then, the images of a feature in the focal plane at times 0 and 1 are given by:(4)x0=KI,0RGyroCam001X0,andx1=KI,0RGyroCam001X1.We can rewrite these equations as:(5)x0=KRGyroCamI,0X0,andx1=KRGyroCamI,0X1.If we defineK˜=KRGyroCam, we get:(6)x0=K˜I,0X0,andx1=K˜I,0X1.Next, we will collect measurements from our 3-axis gyroscope between time 0 and time 1. What comes out of a gyro are measurements of the rotation rates of the gyro frame about its X, Y, and Z axes relative to inertial space, coordinatized in the gyro frame. We will denote these rates rx, ry, and rz, respectively. From these measurements we want to compute the 3×3 matrix describing the rotation of the gyro frame between times 0 and 1, which we will call R. This is a well-studied problem, and we use a common quaternion representation for rotations to help solve it. It can be shown (e.g., [22]) that the quaternion representing the orientation of the gyro frame obeys the ODE:Q̇=12QP, where P=0+rxi+ryj+rzk. We initialize our quaternion Q at time 0 as Q=1+0i+0j+0k, which represents the trivial rotation (by angle 0), and we integrate this ODE numerically from time 0 to time 1 and convert the final quaternion to a rotation matrix, R, as described in [22].Since X0 and X1 are defined relative to the gyro frame, they are related by:(7)X1=R001X0.Combining this with Eq. (6), we can write:(8)x1=K˜I,0X1=K˜I,0R001X0=K˜R,0X0=K˜RI,0X0.Next, observe thatK˜is invertible because it is the product of a rotation matrix and an invertible camera calibration matrix. We will insertK˜−1K˜=Iin the right spot on the right hand side of Eq. (8) to get:(9)x1=K˜RK˜−1K˜I,0X0=K˜RK˜−1x0.Hence, the images of the feature at times 0 and 1 in the image plane are related by:(10)x1=Hx0,whereH=K˜RK˜−1.Thus, if we are able to estimate the rotation of the gyro frame between two consecutive frames, R, and the constant 3×3 matrixK˜is known, then we can compute the matrix H (which is a homography between the two frames) and predict where each point observed in the first frame will appear in the second. Fig. 1illustrates the output of this process. It is important to observe that the range to a feature is immaterial in this development. Range only effects the component of flow due to a points translational motion relative to the camera from time 0 to time 1, which is assumed to be negligible in this development.The constant matrixK˜can be determined by collecting a video sequence of a static scene where the camera undergoes rotations about all 3 axes (but only negligible translations). Between each pair of consecutive frames the rotation R is computed from the gyroscope and H is estimated via optical image registration. Each consecutive pair of frames yields a set of 9 equations from Eq. (10), where the unknowns are the elements ofK˜. The system can be solved in a least-squares sense to yieldK˜.It should also be mentioned that using gyroscopes to predict optical flow requires some amount of calibration and integration between the gyros and the camera. For instance, the data from the gyros must be synchronized with the camera data. Also, gyros have slowly varying biases that must be compensated for. See Appendix A for a discussion of these details.The idea behind our proposed method is to regularize the registration energy function by adding a term that penalizes feature positions that deviate from our prior estimate of flow (in our case derived from a gyroscope). The term we add will be small enough to not interfere with the well-behaved energy functions of strong features, but it will be significant enough to dominate the energy functions of weak features in ambiguous directions. This allows us to extract all of the information that is present in the imagery,22For instance, we may be able to determine the horizontal position of a feature on a vertical edge very precisely from the imagery, but we may be unable to determine the same features vertical position along the edge. In this case there is still valuable information in the imagery that we do not want to ignore.but rely on our prior estimate of position when localizing in ambiguous directions. That is, we want the ability to “ride” our gyro measurements through poor imagery, until a feature can be re-acquired, but our implementation should only effect weak features and only in directions where the feature cannot be localized based purely on the imagery.When tracking corner-like features, the single-feature energy surface Eq. (1) is generally well-behaved in a small neighborhood of the global minimum. If we start minimization with an initial guess that is sufficiently close to the minimum we generally have no problems converging to it. However, when tracking features that are not corner-like, the energy surface is often not so well-behaved. In fact there may not even be a unique global minimum. In this situation, it is not possible to completely identify the correct location of a feature using template registration alone (this has nothing to do with the optimization scheme employed; it is a problem with the energy function itself). In Fig. 2we give sample template images and energy functions for corner-like and edge-like features. Notice that there is a line of global minimums for the energy function corresponding to the edge-like feature (we will call this a line of ambiguity). This is because sliding the template image along the edge does not result in a better or worse match between the image and the template. Of course, because the energy function is derived from real-world data there will be a single global minimum somewhere on the line of ambiguity, but the location of the minimizer on this line will be unstable and unpredictable. This is why edge-like features tend to wander (and sometime jump) along their lines of ambiguity during tracking. It is important to note that simply initializing (or even bounding) the search of such a feature using the prior estimate of flow (as in [9]) will have little effect here since the issue is the instability of the minimizer of the energy function, not the inadequacy of a given optimization scheme at finding it.In this work, we combine information about the flow of a feature from two very different sources. On one hand, we have imagery, which promises to yield an extremely accurate estimate of flow, provided the imagery is distinctive enough to reliably align with the feature's template. On the other hand, we have a flow estimate based on measurements of camera rotation. This estimate is more crude because it does not reflect flow due to translational motion of the feature relative to the camera, but it is not effected by ambiguity in the imagery. In summary, we have one source of information (the imagery) that can accurately capture all sources of flow (both due to camera rotation and translational motion of the feature), but which is susceptible to ambiguity in the imagery. Then we have another source of information (gyroscopes) that can only observe the often-dominant portion of flow, but which is not susceptible to ambiguity in the imagery. These properties make the two sources complimentary for the purposes of tracking.When bringing together information from complimentary sources such as these, there is a temptation to produce estimates of the flow from both sources of information and combine them in a filter, exploiting known error characteristics of both estimates. This approach is impractical in this situation, however, because the error of the gyro-derived flow estimate is difficult, if not impossible to accurately characterize because it is blind to translational motion of features. Depending on the subject matter of the video, it may not even be reasonable to assume that the gyro-derived flow estimate is unbiased (imagine video of traffic, where most features are moving in one direction, in no part due to camera motion). What can surely be said is this: When the imagery is distinctive enough to localize a feature in a given direction, that estimate should be preferred over any other estimate. When the imagery is not distinctive enough to localize a feature in some direction, then it makes sense to fall back on the gyro-based estimate of flow. This is precisely what is accomplished by our proposed method. By weakly regularizing the image-based registration energy function using the gyro-derived estimate of flow, we use the imagery as our primary instrument for localizing a feature. However, when the imagery is not distinct enough to localize a feature in some direction, our gyro-based estimate takes over to fill in the missing information.In Section 3 we covered how we can take a features location in one frame and predict its location in the next using gyro measurements. We will let xgyro denote this predicted position for a feature (2D position, in units of pixels). To achieve our goal in a single-feature tracking framework, we will add a term to Eq. (1) to penalize locations that differ from this prior prediction. The regularization term we add should be small enough to be completely dominated by a strong registration energy function. This must still be the case if the minimum of the registration energy function is relatively far from the prior estimate because there may be valid reasons for a significant discrepancy between true flow and predicted flow. In particular, since our prior is estimated using a gyroscope and only accounts for camera rotation, features on objects that are moving relative to the environment can have a significant component of flow that will not be reflected in the prior estimate. Thus, our regularization term must not grow too quickly as we move away from the prior estimate. It is also desirable, of course, for the regularization function to not contain local extrema. The penalty function we use is:(11)P(x)=λIn(α|x−xgyro|+1)/In(αxmax+1),where xmax is a constant (in units of pixels) reflecting the greatest expected deviation from the gyro prediction, the constant α controls the “pointiness” of the curve, and the constant λ controls the overall strength of the penalty (see Fig. 3). In this entire work, we keep α fixed at 0.5 and xmax fixed at 25pixels. We discuss in Section 5 how the parameter λ can be learned.After incorporating the above regularization term, we see that to track a feature when we have a prior estimate of flow we need to minimize the energy function:(12)csingle(x)=1n2∑u∈ΩψT(u)−I(u+x)+λln(α|x−xgyro|+1)ln(αxmax+1).In this work, we use the absolute-value loss function for ψ. Other variables, such as n, Ω, T, and I have the same meanings as in Section 2. In the above energy function, the sum on the left measures the dissimilarity between the template image for the feature and the equally-sized region of the video frame, centered about the point x. The term on the right measures the dissimilarity between the point x and the gyro-derived predicted location for the feature. The value of λ will be small, so as to put greater emphasis on the objective of matching the template with the energy. We caution against trying to impose a strict statistical interpretation on the combination of terms in Eq. (12). The sum on the left and the term on the right are not estimates of the location of the feature. Instead, they are energy functions which are, under suitable assumptions, minimized by the location of the feature. While our term on the right is derived from an estimate of the features location, little can be said about the error characteristics of that estimate. In general, as a result of translational motion of world points relative to the camera, the estimate may not even be unbiased. We suggest viewing Eq. (12) as a modification of the classical feature tracking energy function, where we add some additional shape to the energy function using a prior estimate of flow. This addition is very slight, and is only consequential when the classical energy function has an ambiguous minimizer (i.e. there are multiple candidate locations in the frame that appear to match the template).As was the case when tracking without a prior flow estimate, this energy function can be minimized in a number of ways. In our implementation we use gradient descent with fast line search in a coarse-to-fine framework. This is much faster than exhaustive search, while still providing reliable, consistent behavior, even with poorly conditioned energy functions. This is an iterative algorithm where in each step, the gradient of the energy function, ∇E, at the current location is computed and −∇E is taken as the search direction. Then, the energy is approximately minimized on a line segment starting at the current location and continuing in the search direction. These steps are repeated for a minimum number of steps and until a maximum number of steps is reached or a stop condition is met. In this work we have two stop conditions: (a) The gradient of the energy has sufficiently small magnitude (0.00001), or (b) the gradient magnitude is not sufficiently smaller than in the previous iteration (greater than 0.9999 times the magnitude from the previous iteration). This algorithm is detailed in Algorithm 1. The gradient of the energy function is differentiated numerically using the centered derivative approximation (we perturb the position of the feature by 0.25pixels in each direction). We initialize the tracker on a given feature to the gyro-predicted location for that feature:(13)x=xgyro.The source code for this tracker will be available on our supplemental web page.Algorithm 1Gradient Descent With Fast Line Search.In addition to the single-feature tracker of Section 4.1, our proposed method can also be used in a multi-feature tracking framework (see Section 2). To incorporate our gyro prior, we add a collection of terms to Eq. (2) which penalize each feature's deviation from its predicted position. Thus, we minimize the following regularized multi-feature energy function:(14)cmulti(x)=1n2∑f=1F∑u∈ΩψTf(u)−I(u+Sfx)+λ∑f=1Fln(α|Sfx−xf,gyro|+1)ln(αxmax+1),where xf,gyro denotes the prior estimate of position for feature f (2D position, in units of pixels). As with the single-feature implementation, we use the absolute-value loss function for ψ. This energy function can be minimized by whatever means would be used to minimize Eq. (2). We minimize this energy in a coarse-to-fine scheme using 4 pyramid levels, and we use a slightly modified version of the gradient descent method described in Algorithm 1. The only difference is that the search direction is modified to prevent strong features from completely controlling the search direction, causing weak features to be lost. The search direction computation is detailed in Algorithm 1, and is very similar to the method used in [20].Algorithm 2Search direction for multi-feature tracker with gyro prior.Like the single-feature tracker, we compute the gradients of the image-template fit terms numerically, using the centered gradient approximation and sampling 0.25pixels in each direction. However, rather than grouping together regularization terms with image-template fit terms, we evaluate the gradients for these explicitly. We will denote the sum of all of the gyro-prior terms in Eq. (14) using P:(15)P=λ∑f=1Fln(α|Sfx−xf,gyro|+1)ln(αxmax+1).Now, let xidenote the i’th entry of x, and let xi,gyro denote the gyro-derived prior estimate of xi. The gradient of P is given by:(16)dPdxi=λα(xi−xi,gyro)ln(αxmax+1)(αNi+1)Ni,where:Ni=(xi−xi,gyro)2+(xi+1−xi+1,gyro)2ifiis odd, andNi=(xi−1−xi−1,gyro)2+(xi−xi,gyro)2ifiis even.We also present results for our multi-feature tracker with gyro prior with an additional rank penalty from [20]. We use the rank penalty based on empirical dimension and the centered trackpoint matrix and we compute the gradient of the rank term using the method described in [20]. When minimizing the energy function with an additional rank penalty term, we use the same algorithm (including the search direction rule) as we use when there is no additional rank term. The source code for the multi-feature tracker will be available on our supplemental web page.

@&#CONCLUSIONS@&#
