@&#MAIN-TITLE@&#
Efficient and robust model fitting with unknown noise scale

@&#HIGHLIGHTS@&#
robust scale estimation without a breakdown pointinsensitive to inlier noise distributionefficient and practical application to many computer vision problems

@&#KEYPHRASES@&#
Robust estimation,RANSAC,Scale estimation,Structure from motion,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Data fitting (i.e., estimating the parameters of some hypothetical model that best explains a set of data measurements) is a critical task that arises in many disciplines. In general, the measurement set will contain some unknown fraction of outliers, and the good measurements will be subject to some unknown scale of noise, typically assumed to be Gaussian.Least squares (LS) estimators, which minimize the sum of squared residual errors, are efficient and optimal for Gaussian noise [1], but are highly sensitive to outliers [2]. The breakdown point of the LS estimator is 0% because the estimate may be arbitrarily skewed when the percentage of outliers is greater than 0% ([3], p.9). A more robust approach is the least median of squares (LMS) estimator [4], which minimizes the median of squared residuals, and has a breakdown point of 50%.Accumulator-based methods (e.g., the Hough transform [5,6] or variations such as the Randomized Hough Transform [7]) have no specific breakdown point, and are popular for simple line and curve detection. However, they are non-optimal, and are limited in their general applicability because they require discretization of a p-dimensional space for models with p parameters, which would result in prohibitively high time and space complexity for many problems.Perhaps the most well-known and generally applicable robust estimator without a breakdown point is RANdom SAmple Consensus (RANSAC) [8]. RANSAC uses a hypothesize-and-test framework by randomly sampling subsets of the measurement set, and retaining the model that maximizes the number of inliers according to some threshold. Because it does not require discretization of the search space, estimation of high-dimensional models is computationally feasible, and there is no breakdown point beyond the minimal fraction necessary to define a model.Due to its success, there have been many popular RANSAC variations. To summarize, robust M-estimators [9] were used for model evaluation with MSAC and MLESAC [10], the inner optimization from LO-RANSAC [11] attempts to compensate for the unrealistic assumption that all models estimated from uncontaminated (albeit noisy) data are good, and explicit testing for degenerate configurations has been incorporated in DEGENSAC [12] and QDEGSAC [13]. Other improvements have focused on performance optimizations by using heuristic bail-out tests [14–16] or guided sampling as in PROSAC [17], Preemptive RANSAC [18], and ARSSAC [19]. See [19] for a more thorough survey.All of these aforementioned RANSAC variations implicitly require accurate a priori knowledge of the scale of inlier noise in order to choose the threshold. In many cases, it is possible to choose a reasonable threshold based on domain knowledge, but the sensitivity to this choice is undesirable and can sometimes result in instability. Indeed, it has been often noted that given a bad choice of threshold, RANSAC will completely break down [2,20].One of the first approaches attempting to overcome this limitation was to first make a robust estimate of the model using LMS and then make a robust estimate of scale using the median squared residual [3], as proposed in [21]. However, because LMS and the median scale estimate both have 50% breakdown points, this method cannot be applied to data sets with more than 50% outliers, as is often the case.More recent RANSAC variations have attempted to incorporate scale estimation with model estimation. For example, ASSC [2] modified the RANSAC objective by maximizing the inlier count divided by a robust estimate of scale. However, there is no statistical support for this modified objective, and it does not always detect the correct scale. ASSC also retains adaptive sampling from RANSAC, but not in a statistically valid way, and this can lead to premature convergence at grossly over-estimated scales when the outlier ratio is high. A similar approach was taken in [22], who suggest modifying the objective function to seek the model which minimizes their proposed weighted median absolute deviation (WMAD) estimate of scale.Projection-based M-estimators (pbM-estimators) were used in the ‘projection pursuit’ approach of [23], and some performance enhancements were proposed in [24–26]. The most recent and best-performing technique along these lines is ASKC [27].ASKC is an improvement upon the original ASSC algorithm, with a more statistically motivated objective function. The basic idea is to choose the random model hypothesis that maximizes a kernel density estimate (KDE) in residual space centered at the origin. ASKC also abandons the earlier attempt at adaptive sampling from ASSC, and instead uses a fixed number of samples. The recognition that adaptive sampling does not work in this context is a significant limitation in comparison to the original RANSAC algorithm, because using a fixed number of samples either prevents good performance in the presence of high inlier ratios (excessive sampling), or induces a breakdown point in the presence of low inlier ratios due to not enough sampling to find the structure within the data.A subtle theoretical problem with ASKC is that the method was derived based on the assumption that the residual distribution should be normal and hence have a mode at the origin, but in their experiments is often applied to the distribution of squared fitting errors which has, in general, a scaled χk2-distribution with a non-central mode that depends on σ for k>2 (Section 3).Ultimately, the greatest limitation of both ASSC and ASKC is that they attempt to estimate the scale directly from the fully contaminated set of residuals, which is an inherently difficult problem to solve under high outlier ratios. The proposed Two-Step Scale Estimator (TSSE) from [2], which is used by both methods, relies on a KDE of the residual error distribution and is capable of functioning under high outlier ratios, but only if the kernel bandwidth is chosen properly. Automatic methods for choosing the bandwidth rely on an accurate estimate of scale. Thus, it is somewhat of a ‘chicken-and-egg’ problem.Wang et al. propose obtaining the initial estimate using the k-th order statistic, which we find sometimes works and sometimes does not. Outlier contamination can lead to over-estimated bandwidth, in turn leading to oversmoothing of the KDE, and finally poor scale estimation with TSSE. To counteract this oversmoothing they propose using a fraction ch∈(0,1) of the automatically derived bandwidth, but we find that there is no ‘one size fits all’ value of this parameter, because it depends largely on the scale and distribution of outliers. In summary, the overall sensitivity of ASKC to the scale estimator leaves us unsatisfied.Another recent approach to automatic scale estimation is based on the recognition that RANSAC tends to exhibit the greatest consistency in the discovered models when the threshold is set near the true scale level. This observation was first exploited in StaRSaC [28], which performs a brute force search across a wide range of logarithmically spaced scales, repeating RANSAC at least 30 times at each level, in order to identify the scale at which the Variance of the estimated model Parameters (VoP) is minimized.A notable disadvantage of this approach is high computational cost: even with a modest granularity of 100 scales, this would require running RANSAC about 3000 times. One must also consider that running RANSAC with too small a scale imposes a near-zero inlier ratio, which requires an exponentially larger number of samples for the adaptive convergence criterion. This problem can be partially avoided by using an artificial limit on the number of iterations, although such a limit might prevent the true structure from being found if the outlier ratio is high.Another more subtle problem is a dependence on model parameterization, because the algorithm assumes that the VoP is indicative of ‘structural variation’ of the model. Variance is completely meaningless for over-parameterized models (such as homogeneous entities), and even after projecting into a minimal parameterization (e.g., by performing a homogeneous division), variance is still not an accurate reflection of structural variation. For example, if one projects the homogeneous equation of a line into the familiar form of y=mx+b, one is likely to observe extremely high variance in the b parameter for near-vertical lines, which is much greater than the variance would be for a set of nearly horizontal lines of equal angular variation. Thus, in order to obtain good results for any particular problem, one may need to spend a great deal of effort into finding a parameterization in which the VoP corresponds well to structural model variation. This alone makes it unsuitable as a generic estimation routine.A related problem is that the algorithm requires comparing models to find the largest-scale model that is ‘consistent’ with the model at the scale that minimizes the VoP. In their implementation, model consistency is assessed by using the Frobenius norm of model parameters with some unspecified threshold, but the Frobenius norm of model parameters is generally not an accurate measure of ‘structural difference’ between models. Furthermore, choosing this threshold automatically is implicitly related to the noise level, and is arguably no simpler than choosing the original RANSAC threshold.Lastly, the scale cannot be estimated more finely than the search discretization, and while it is generally true that the lowest variance occurs around the true scale, this is not always the case. For example, if one is fitting lines to point data inR2, and there are two large outliers outside of a data set, then any threshold large enough to connect these two outliers will consistently result in the bad line connecting those outliers. Also, whenever there is a low outlier ratio, the scale encompassing all data points will be preferred over the true scale.A more recent approach in the same spirit as StaRSaC is RECON [29], which also attempts to determine scale based on the recognition that model variance is low around the true scale, but with one major difference: rather than explicitly looking for low variance in the model parameters, RECON looks for models with low variance in the sort-order of residuals (or fitting errors).RECON forms model hypothesis from randomly selected minimal subsets until K≥3 models with mutually α-consistent residual sets have been found. The α-consistency test searches for the smallest n-value such that the data points associated with the n smallest residuals have more than α2 percent overlap, with α=0.95. It is assumed that this n-value represents the separation between inliers and outliers. Although this test is statistically inspired, there are a number of practically occurring situations in which it fails.First, it rests heavily on the implicit assumption that the fitting errors for inliers between any two good models will occur in random order. However, if one compares two identical models, then the fitting errors must have the exact same sort-order, and thus α-consistency would pass at any value, such as the minimal value of n=1, meaning that none of the inliers are detected.In the presence of noise, it is unlikely to find two identical models, but the similarity of sort-order can still be expected to be similar for similar models. This is mostly a problem in the fourth step of RECON, which calls for making M=30 over-determined model estimates from outlier free data, and then taking the minimal n-value that passes the α-consistency test between all pairs. Because these estimates are over-determined and outlier free, it should be expected that some of these models are very accurate and hence very similar, and hence it would not be surprising if there were some pair of models with a very similar sort order, leading to a greatly under-estimated n-value. Because RECON then re-estimates the final model from this minimal number of points, it would destroy the model estimate.Another problem is that, for very small values of n, the α-consistency test can easily pass for inconsistent models by pure chance. For example, consider a line fitting problem with two perpendicular intersecting line models that both have the smallest fitting error to the same point nearby their intersection. In this case, the normalized overlap θ1i,j=1, and thus the two models will be deemed as α-consistent, despite that these models are not at all consistent in their support regions. Although the individual probability of this for a single trial is low, given a sufficiently large number of α-consistency trials, the probability of this problem occurring at least once becomes very large.This problem can occur for arbitrarily large n values, and may be exacerbated by the distribution of data points, because the only condition is that two inconsistent models happen to share some of their lowest residuals in common order (e.g., the models ‘pivot’ around a similar point in the data). Thus, if the data distribution inherently supports a region of common points that are likely to be shared by many different models (such as a bow-tie distribution for line fitting), then the same problem may be common for larger values of n. In the case of F-matrix estimation, a large number of points on a common plane could create this problem, even if the data also contains a significant number of off-plane points as well.The runtime performance of RECON can be prohibitive, because for each RECON hypothesis, one must test for α-consistency with all prior RECON hypothesis — and each test for consistency requires a brute force search through the residuals at all possible scales. Thus, despite that the overall number of samples is low, this high time complexity coupled with the large number of consistency checks required to find a set of mutually consistent models can quickly result in excessive runtime for low outlier ratios.RECON also has difficulty with data sets that may contain multiple structures, because it returns the first significant structure that is found, which is not necessarily the most dominant structure in the data.In the context of multiple model fitting, a number of methods have been proposed that also incorporate automatic scale selection [30–32]. However, [32] have already developed ASSC and ASKC which are optimized for single-model estimation, [31] effectively generalizes the principle used by RECON to the multiple model fitting problem, and the method of [30] does not work for single-model estimation problems. Thus, we will not consider these more complex and computationally intensive multiple-model fitting methods further.To summarize, it is hard to justify the greatly increased computational cost or reduction in reliability that is associated with using any of the aforementioned RANSAC variations that incorporate automatic scale selection, especially for computer vision problems, where the errors are generally measured in image space, and one can often assume a sub-optimal threshold that works ‘acceptably well’ based on the assumption that image correspondence error is on the order of a pixel or two, as is done with current state of the art Structure from Motion systems like Bundler [33,34].This is not always the case, as one might be using a subpixel matching algorithm for low-baseline pairs leading to subpixel errors, or one might be performing wide-baseline matching with multi-scale features that have significantly larger errors, or tracking points across multiple frames resulting in the accumulation of errors, or dealing with images of varying sizes and qualities leading to unpredictable error levels. Thus, automatic scale estimation is still preferable, if it could be done reliably and efficiently.Our recognition is that it is usually not difficult to specify a conservative maximum scale, and doing so permits the development of a new approach to the scale estimation problem without the large sacrifices in efficiency or reliability that are associated with completely unbiased searching through scale space. This is the motivation behind the proposed Simultaneous Fitting and Scale Estimation (SIMFIT) algorithm.Like the original RANSAC algorithm, SIMFIT is simple to implement, is applicable to arbitrarily-high dimensional data, has no specific breakdown point, is independent of model parameterization, and uses the same statistical convergence criterion to adapt the number of iterations. It does not require any additional parameters, and does not significantly increase computational cost or reduce reliability. Furthermore, SIMFIT is designed to be fully general, and not just limited to computer vision problems.We begin by introducing the theoretical background for classifying inliers (Section 2) and estimating scale from the residuals or fitting errors (Section 3). We then introduce the SIMFIT algorithm (Section 4), clarify the parameters of algorithms compared (Section 5), and present our experimental results (Section 6), starting with a validation of the assumed noise distribution (Section 6.1), followed by an empirical comparison of accuracy and performance on line fitting and homography estimation (Section 6.2) and finally a real-data experiment with fundamental matrix estimation (Section 6.3). Our results show that SIMFIT produces a model estimate with greater likelihood, more accurately estimated scale, and lower computational cost.For some implicit p-dimensional model defined by parametersθ∈Rp, let the functionfx|θ:Rn→Rrbe a mapping from data measurements to residual errors, where n is the dimension of the measurement space and r is the number of residual errors per datum. Thus, for a set of N data measurementsxi∈Rn,i=1…N, the function f(xi|θ)=dimaps the ith datum to di, a vector of residual errors associated with the datum. We call ‖di‖2 the squared fitting error of xiwith respect to the model θ, which is equal to zero only when xiis perfectly consistent with θ.When fitting an m-dimensional surface in an n-dimensional space there are k=n – m degrees of freedom in defining a surface normal, called the codimension[35]. If measurement noise is independent and normally distributed with standard deviation σ, and given a reasonable model estimateθ^≈θ, then residual errors in the codimension will be distributed approximately the same as measurement errors (normally). Thus, squared fitting errors will be distributed according to a scaled χ2-distribution with k degrees of freedom [36].Once the scale σ is known, a threshold may be calculated as τ2=σ2Fk−1(α) ([37], p.119), where α is the desired percentile (e.g., α=0.95) and Fk−1 is the standard inverse cumulative χk2-distribution function. Given an estimated modelθ^and threshold τ, a datum may then be classified as an inlier when the squared fitting error is below the threshold; that is, ||di||2<τ2.Given an existing model estimateθ^, a robust scale estimator attempts to estimate the true scale of measurement noise from the distribution of residuals or fitting errors relative to the model estimate. The maximum likelihood (ML) estimate of σ is simply given by the sample standard deviation from the combined set of residuals. In the special case where the number of χ2 degrees of freedom is equal to the number of residuals per datum (r=k), this can be written in terms of the fitting errors as(1)σ^ML=1Nk∑j=1Nkrj2=1Nk∑i=1Ndi2,where rjis the jth residual out of the combined set. However, it is well known that the ML estimate is not robust to outliers, and we expect that the data does contain outliers. A robust alternative comes from the median squared residual [21,3]. Assuming that residual errors are distributed as R~(0, σ2), then from the definition of the median, we have(2)0.5=PR2<medR2=PR<medR(3)=PZ<medR/σ,where Z=R/σ is a standard normal random variable (RV). Because the distribution of Z is symmetric, the above is equivalent to(4)0.75=PZ<medR/σ,which implies(5)Φ−10.75=medR/σ(6)σ=medR/Φ−10.75,where Φ is the cumulative distribution function of the standard normal distribution. Therefore, a robust and asymptotically consistent estimator for σ from the residuals is given by(7)σ^MED=medjrjΦ−10.75=medjrj2Φ−10.75.In practice, when k>1, one does not always have a residual vector, and it is tempting to use (7) to estimate σ from the fitting errors instead; however, this would be incorrect because the squared fitting errors have a non-central χ2 distribution. The median absolute deviation (MAD) is often used to compensate for this non-centrality, but this is not an asymptotically consistent estimator.The correct estimator can be derived in the same fashion as (7) (see also ([38], Appendix C, p.102)). Specifically, if D~ χ2 (σ, k), then from the definition of the median we have(8)0.5=PD<medD,which implies(9)0.5=FkmedD|σ(10)medD=σ2Fk−10.5(11)σ=medD/Fk−10.5.Thus, a robust and asymptotically consistent estimator for σ, analogous to (7) but computed from the fitting errors and hence valid for all cases, is given by(12)σ^′MED=medidi2Fk−10.5.It should be noted that these median-based estimators have 50% breakdown points, which is ineffective for most previous scale estimation algorithms where the breakdown point of the scale estimator induces an equivalent breakdown point in the overall estimation routine [2,27,22,29].This has led to the development of scale estimators with increased tolerance to outliers, such as the Compressed Histogram [39], the k-th order statistic [40,41], the WMAD [22], TSSE [2], IKOSE [32] and others.However, the breakdown point of the scale estimator is not a significant concern for SIMFIT, because the scale estimate is only used to obtain an over-estimate anyway. Thus, we will preferσ^′MEDfor its simplicity and reliability.The sensitivity to threshold choice τ in RANSAC is revealed by the fact that, as τ→∞, all constraints on the estimated model vanish, giving a purely random result. In contrast, we notice that MSAC [10], a modification of RANSAC that minimizes a robust M-estimator [9], becomes equivalent to the method of least absolute deviations (LAD) [42] as τ→∞.LAD is already a fairly robust method, and by using any τ<∞, one may obtain far more robust results without a specific breakdown point. Thus, MSAC is quite robust to over-estimated scales, and this is the core concept we exploit in the algorithm outline below:1.Starting from any initial overestimate of σ, the corresponding optimal threshold τ may be derived, and used to estimate a model with associated inliers using MSAC.From the residuals of the inlier set, a robust estimate of σ may be computed usingσ^′MED. Because it was estimated from a more restricted set of inliers, the newly estimated σ will usually be less than the previous.If there is no significant change in the estimate of σ, then all the outliers must have been removed, and hence the model, inliers, and scale should all be accurate. Otherwise, one may repeat MSAC from step 1 using the newly reduced estimate of σ.To clarify the algorithm details, we give pseudo-code in Algorithm 1, and proceed here with some analysis. First, the initial estimate of σ is used to calculate a corresponding over-estimate of τ (line 1), and all the data points are added to the potential inlier set (line 5).On each iteration, MSAC is used to compute a robust estimate of the model θ (line 9) from within the potential inlier set (our modified version of MSAC that works with a shrinking inlier set is given in Algorithm 2). The set of inliers is reduced (line 10) and used to compute a new robust estimate of scale (line 16) and associated threshold (line 17).In general, each new estimate of scale will be lower than the previous until convergence. However, this is not guaranteed, and in some very rare cases a cycle might be entered. Therefore, we perform explicit cycle prevention by computing a quick hash (e.g., the MurmurHash3 [49]) of the inlier indices and current scale estimate (rounded to nearest integer), and break out of the loop if a repeated state would be entered (line 24).Normally, convergence is detected when the reduction in σ becomes insignificant, as detected by a difference less than some threshold ϵ (line 24). However, we note that one may ignore this parameter by setting ϵ=0 here, which merely delays convergence until no further improvement is possible.Additionally, one may add some optional convergence criteria to improve best and worst case performance: (a) If the found solution uses nearly all of the potential inliers (i.e., if the number of inliers reduced from the current iteration is an insignificant fraction); (b) If the found solution uses such a small number of inliers that further reduction of the inlier set would be pointless (i.e., the size of the current inlier set is less than 2 times the minimal number of points necessary to define a model).In most cases, we do not expect to need more than 1–3 iterations of MSAC to converge to the correct scale. Moreover, because MSAC is run within a reduced inlier set (similar to LO-RANSAC [11]), subsequent runs of MSAC become computationally trivial, as the inlier ratio will be near to 1, requiring only a few random samples to meet the statistical convergence criterion of [8].After convergence to the proper scale, we transition into a final (optional) model refinement stage (line 25), which we refer to as the model-shift procedure, because it is actually a generalization of the well-known mean-shift procedure [43], where the threshold is effectively the mean-shift bandwidth with a uniform kernel, and we generalize the sample mean from mean-shift with the over-determined estimate of the model. The only actual difference from mean-shift is that we only allow inliers to be added (and not removed) from the potential inlier set, which guarantees convergence by preventing cycles.SIMFIT is usually quite robust to the choice of σMAX. For example, if one chooses σMAX=∞, then the first iteration would reduce σMAXdown toσ^′MEDfrom an all-data fit. This is often sufficient to converge to the proper scale, although when the outlier points come from some distribution that also has finite variance, then the all-data fit may yield a stable model that is a false attractor. Thus, one should chooseσMAX<σ^′MEDif possible.In this section we identify previous algorithms that we compare to SIMFIT in our experimental results for their ability to do robust estimation with simultaneous scale detection. We also clarify the choice of free parameters and algorithm details when necessary. In general, we set τ so as to capture α=0.99 percent of inliers, and we let the number of samples for RANSAC/MSAC be determined adaptively with pFail=1×10−3, and a maximum of 10000.Although RANSAC does not include scale estimation, we use RANSAC with an optimally derived threshold based on the true σ for the purposes of performance comparison.Because we expect image noise on the order of a pixel or so, we use a conservative over-estimate of σMAX=15. We use the same value in synthetic tests. We use three conditions for early-termination of the main loop: (a) change in σ less than ϵ=0.5, (b) reduction in the inlier set is less than 1% from the previous iteration, (c) size of the inlier set is reduced to less than 2 times the minimal number of points to define a model.The first comparative algorithm is the procedure of [21], where LMS is used to obtain an initial estimate of the model, followed by robust scale estimation usingσ^MED, and finally RANSAC with a threshold based onσ^MED. We have upgraded this method by replacing RANSAC with MSAC from [10] because it is strictly superior to RANSAC in this context. LMS is implemented (as per usual) by random sampling, and the number of iterations is determined based upon the assumption that the data may contain up to 50% outliers (because this is the breakdown point of LMS).The second algorithm that merits comparison is ASSC [2], which modifies the RANSAC criterion to maximize the number of inliers divided by a robust scale estimate, as well as the newly developed ASKC [27]. ASSC uses adaptive sampling, but ASKC uses a fixed number of samples, and we chose M=1000 samples.Both ASSC and ASKC estimate scale using TSSE, which uses a KDE of the residual error, where the bandwidth for the KDE is chosen automatically using a rule of thumb multiplied based on an initial scale estimate, and then multiplied by an unspecified tuning parameter ch∈(0,1) to compensate for oversmoothing.For the initial scale estimate, we use the k-th order static with k=0.2, as described in [27]. We use the default value of ch=1 because we are testing generic performance and do not permit tuning the algorithms for particular noise distributions.Several comparable algorithms were proposed in [22]: RANSAC-MAD was essentially the same as ASSC but used the MAD to estimate scale instead of TSSC, RANSAC-EIS modified the objective by calculating a weight vector using Ensemble Inlier Sets (EIS) and then using WMAD instead of MAD, and finally RANSAC-EIS-Metropolis incorporated weighted sampling. The latter was found to be the superior version, so we only compare against this one. We note that their algorithm calls for some number of unspecified fixed iterations, so we use the same probabilistic argument from RANSAC and LMS to calculate the number of required iterations assuming there are 50% outliers.STARSAC requires a minimum and maximum scale, we used σMIN=1×10−5 and σMAX=1000, and tested at 20 scales logarithmically spaced within this range, performing the recommended 30 runs of RANSAC at each scale. Due to the large number of RANSAC iterations, and because the required number of RANSAC samples grows exponentially for under-estimated scales, we found it computationally necessary to impose a maximum of 100 samples for each run of RANSAC.When dealing with homogeneous models, the set of model parameters was taken as the set of real parameters after performing homogeneous division. Two models were deemed ‘consistent’ if the maximum relative difference between model parameters was less than 0.2.The version of RECON that we use has a few improvements over the algorithm described in [29]. First, we clarify that the termination condition of finding ‘K mutually α-consistent models’ requires clustering models into mutually α-consistent groups. These clusters are efficiently maintained using the union-find structure.Because the initial α-consistency test will always pass for inconsistent models at some large scale encompassing all (or most of) data, RECON explicitly tests forσ^MED<σMAX, or alternatively tests the KS-test for distribution equality. We opted to use the scale-based test in our experiments because it is faster, more comparable to SIMFIT, and because the KS-test will often accept the null-hypothesis of distribution equality for an all-data fit when the outlier distribution has finite variance, inducing a breakdown point as a set of α-consistent models are more likely to be found at maximum scale for low outlier ratios.However, when the outlier ratio is less than 50%, the robustness of theσ^MEDestimator may cause a low value of σ to be estimated even when n is large enough to be an all-data fit. This issue is corrected by instead testing against the sigma implied by the residual error of the nth residual. That is,rn2/Fk−10.5<σMAX.Another issue is the assumption that residuals will always occur in random sort order. As discussed in the introduction, this assumption is invalid for over-determined models, and can lead to α-consistency check passing at an incorrect small n-value. This can be problematic in the fourth step of the original algorithm, which takes the minimum n value across all pairs between the M over-determined models. This issue is corrected by recomputing the scale from the final model usingσ^MED, and then reclassifying inliers according to a threshold derived from this final scale estimate.We use σMAX=15 in all experiments. Additionally, we use K=3 and α=0.95 as recommended by the authors. The authors recommend M=30, but we found this to be computationally limiting, and reduced it to M=5. To generate the final non-minimal models, we always use a sample size equal to twice the number of minimal points. Due to problems with the α-consistency test for small n-values, we only consider n>10.

@&#CONCLUSIONS@&#
RANSAC has proven to be an effective technique for overcoming large outlier ratios when a good threshold can be chosen, but it is sensitive to this choice. Several methods for augmenting RANSAC with automatic scale estimation have been previously proposed, but these methods tend to break down, or are too slow for many practical applications.To overcome these limitations we have proposed the novel SIMFIT algorithm, which efficiently and reliably performs simultaneous scale estimation and model estimation without a breakdown point. SIMFIT is simple to implement, requires no new parameters (other than optional parameters for early termination), is reliable, and allows for adaptive sampling, keeping the runtime on-par with RANSAC for low as well as high outlier ratios.Because SIMFIT is designed as a drop-in replacement for RANSAC, it can also be incorporated into other algorithms that use RANSAC as a subroutine. For example, the QDEGSAC [13] algorithm was designed to cope with quasi-degenerate data sets, and works by first running RANSAC to find a model that explains the data, and then estimating the codimension of the found model from the found inliers, and finally searching the remaining data for additional inliers that may provide the constraints necessary to make the model non-degenerate, if necessary. Thus, the initial RANSAC step can simply be replaced by SIMFIT to remove the need for a priori knowledge of scale.Although we have demonstrated SIMFIT on some specific vision-related problems, in addition to basic line fitting, we would like to stress that it is not specifically designed just for vision-related tasks. We feel that the simplicity and generality of the method make it applicable to robust estimation in many other fields of science.Finally, we note that there is room for future improvement in deriving a more advanced version of the χ2-distribution that accounts for increased peakedness due to over-fitting. Although the difference would be less than negligible for any normal model fitting problem (where errors are measured relative to a quantity that becomes increasingly over-determined from additional measurements), this would permit more accurate scale estimation for the special case of fundamental matrix estimation.