@&#MAIN-TITLE@&#
Fusion of hard and soft information in nonparametric density estimation

@&#HIGHLIGHTS@&#
We address how to fuse hard and soft information for probability density estimation.We formulate the problem as a stochastic optimization model.We examine convexity, consistency, and asymptotics.We illustrate the approach in a variety of settings.

@&#KEYPHRASES@&#
density estimation,data analytics,data fusion,epi-splines,

@&#ABSTRACT@&#
This paper discusses univariate density estimation in situations when the sample (hard information) is supplemented by “soft” information about the random phenomenon. These situations arise broadly in operations research and management science where practical and computational reasons severely limit the sample size, but problem structure and past experiences could be brought in. In particular, density estimation is needed for generation of input densities to simulation and stochastic optimization models, in analysis of simulation output, and when instantiating probability models. We adopt a constrained maximum likelihood estimator that incorporates any, possibly random, soft information through an arbitrary collection of constraints. We illustrate the breadth of possibilities by discussing soft information about shape, support, continuity, smoothness, slope, location of modes, symmetry, density values, neighborhood of known density, moments, and distribution functions. The maximization takes place over spaces of extended real-valued semicontinuous functions and therefore allows us to consider essentially any conceivable density as well as convenient exponential transformations. The infinite dimensionality of the optimization problem is overcome by approximating splines tailored to these spaces. To facilitate the treatment of small samples, the construction of these splines is decoupled from the sample. We discuss existence and uniqueness of the estimator, examine consistency under increasing hard and soft information, and give rates of convergence. Numerical examples illustrate the value of soft information, the ability to generate a family of diverse densities, and the effect of misspecification of soft information.

@&#INTRODUCTION@&#
It is recognized that statistical estimates can be improved greatly by including contextual information to supplement the information derived from data. We refer to the contextual information as soft information, in contrast to hard information derived from observations (data). In this paper, we consider univariate probability density estimation exploiting, in concert, hard and soft information. Although our development, theoretical and numerical, makes no distinction based on sample size, not surprisingly, it is when the sample size is small that this fusion of hard and soft information plays a crucial role in producing quality estimates. We limit the scope to densities of random variables with distributions that are absolutely continuous with respect to the Lebesgue measure on a bounded interval.The need for estimating probability density functions is prevalent across operations research and management science. For example, an essential step in simulation analysis and stochastic optimization is the generation of probability densities for input random variables; see for example Barton, Nelson, and Xie (2010); Chick (2001); Freimer and Schruben (2002). Density estimation is also needed when populating probability models and when analyzing simulation output beyond their typical first and second moments. In all these situations, however, the sample available is typically extremely small due to practical and computational limitations. One is usually forced to restrict the attention to parametric families of densities. In this paper, we provide the theoretical foundations of an alternative approach that brings in soft information about problem structure and past experiences to obtain reasonable nonparametric density estimates even for very small sample sizes. The approach has been successfully applied in the context of simulation output analysis Singham, Royset, and Wets (2013), uncertainty quantification Royset, Sukumar, and Wets (2013), as well as estimation of errors in forecasts for commodity prices Wets and Rios (Under review) and electricity demand Feng, Gade, Ryan, Watson, Wets, and Woodruff (2013); see also Rios, Wets, and Woodruff (Under review).A natural and widely studied approach to density estimation is to adopt an M-estimator with additional constraints to account for soft information. We continue this tradition by defining an estimator that is an optimal solution of a constrained maximum likelihood problem. An appealing property of such estimators is that for any sample size, an estimate is the best possible within the class of allowable functions according to the given criterion (likelihood).We trace the consideration of soft information in terms of shape constraints at least back to Grenander (1956a), 1956b). More recent studies of univariate log-concave densities include Balabdaoui, Rufiback, and Wellner (2009); Dumbgen and Rufibach (2009); Groenenboom and Wellner (1992); Jongbloed (1998); Pal, Woodroofe, and Meyer (2007); Walther (2002), with computational comparisons in Rufiback (2007); see also the review Walther (2009) and, in the case of multivariate densities, e.g., Cule, Samworth, and Stewart (2010a), 2010b). Convexity and monotonicity restrictions are examined in Groenenboom, Jongbloed, and Wellner (2001); Meyer (2012b) and monotonicity, monotonicity and convexity, U-shape, as well as unimodality with known mode are studied in Meyer (2012b); Meyer and Habtzghib (2011). Unimodal functions are also covered in Hall and Kang (2005); Reboul (2005), with the former covering U-shape as well. Monotone, convex, and log-concave densities are dealt with in Birke (2009). Studies of k-monotone densities include Balabdaoui and Wellner (2007), 2010); Gao and Wellner (2009). Densities given as monotone transformations of convex functions are examined in Seregin and Wellner (2010). Convex formulation of a collection of shape restrictions is discussed in Papp (2011); Papp and Alizadeh (2014). We refer to the recent dissertation Doss (2013) and the discussion in Cule, Samworth, and Stewart (2010b) for a more comprehensive review and to Lim and Glynn (2012) for the related context of shape-restricted regression.Although these studies address important cases, there is no overarching framework that allows for a comprehensive description of soft information formulated by a large variety of constraints. Initial work in this direction is found in Wang (1996), which deals with parametric nonlinear least-squares regression subject to a finite number of smooth equality and inequality constraints. That paper examines the asymptotics of the least-squares estimator using the convergence theory of constrained optimization, specifically epi-convergence. In the context of constrained maximum likelihood estimation, Dong and Wets (2007) establishes consistency of an estimator through a functional law of large numbers and epi-convergence. The latter work is an immediate forerunner to the present paper.Having adopted a nonparametric constrained maximum likelihood framework, we face technical challenges along two axes. First, one needs to deal with constrained optimization problems. Of course, in principle, constraints can be handled through penalties and regularizations; see for example Good and Gaskin (1971); Klonias (1982); Leonard (1978); de Montricher, Tapia, and Thompson (1975); Silverman (1982); Thompson and Tapia (1990) and more recently Bühlmann and van de Geer (2011); Eggermont and LaRiccia (2001); Koenker and Mizera (2006), 2008), 2010); Meyer (2012a); Turlach (2005). However, the equivalence and interpretations of such reformulations depends on the successful selection of multipliers and penalty parameters which is far from trivial in practice, especially in the case of multiple constraints. In fact, poor selection of these multipliers and parameters may cause computational challenges due to ill-conditioning of the resulting optimization problem as well as significant deterioration of the quality of the resulting density estimate. Moreover, it becomes unclear in what sense, if any, an estimator is “best” when an otherwise natural criterion such as likelihood is mixed with nonzero penalty terms; see Dong and Wets (2007) for further discussion. It is also possible to devise specialized algorithms such as the iterative convex minorant algorithm Groenenboom and Wellner (1992); Jongbloed (1998) to account for certain constraints or modify “unconstrained” estimators such as those based on kernels; Hall and Kang (2005) handles unimodality, Birke (2009) considers monotonicity, convexity, and log-concavity, and Davies and Kovac (2004) aims to reduce the number of modes; see Racine (2015); Wolters (2012) for computational tools. Again, it is unclear in what sense, if any, such estimates are “best” in the case of finite samples. Moreover, it is challenging to generalize these approaches to handle other types of soft information. We direct the reader to Tsybakov (2009) and references therein for treatments of kernel estimators including a discussion of optimality.The second challenge with a nonparametric constrained maximum likelihood framework is the infinite-dimensionality of the resulting optimization problem. Naturally, there is a computational need to consider families of approximating densities characterized by a finite number of parameters. The method of sieves Chen (2007); Geman and Hwang (1982); Grenander (1981) provides a framework for constructing, typically, finite-dimensional approximating subsets that are gradually refined as the sample size grows and that in the limit is dense in a function space of interest. However, difficulties arise from three directions. First, with our focus on small sample sizes, the linkage between sample size and sieves becomes untenable. Second, in order to allow for the possibility of discontinuous densities and exponential transformations, we choose as underlying space the extended real-valued lower or upper semicontinous functions, but neither is a linear space. Consequently, the mathematically inbred tendency to obtain a finite-dimensional approximation by relying on a well-chosen finite basis is problematic; see for example Delecroix and Thomas-Agnan (2000); Meyer (2012a) for such an approach based on splines. Third, despite progress towards handling shape restrictions on sieves (see for example Dechevsky and Penev (1997); DeVore (1977a), 1977b); Papp (2011); Papp and Alizadeh (2014)), there is no straightforward way of handling a comprehensive set of soft information.In this paper, as in Dong and Wets (2007), we consider an arbitrarily constrained maximum likelihood estimator for densities. We appear to be the first to consider such general constraints (soft information) in the context of nonparametric density estimation. The soft information might even be random, i.e., the soft information may not be known a priori but is realized with the sample. We give concrete formulations of the constrained maximum likelihood problem in the case of soft information about support bounds, semicontinuity, continuity, smoothness, slope information and related quantities, monotonicity, log-concavity, unimodality, location of modes, symmetry, bounds on density values, neighborhood of known density, bounds on moments, and bounds on cumulative distribution functions. We allow for any combination of these, and essentially any other constraint too.We overcome the technical difficulty caused by constraints through the theory of constrained optimization, specifically epi-convergence, and therefore avoid tuning parameters related to penalties and regularization. With the exception of the preliminary work Dong and Wets (2007), this paper is the first to utilize epi-convergence to analyze constrained density estimators. We overcome the difficulty of infinite dimensionality through the use of a new class of splines, epi-splines Royset and Wets (2014), which are highly flexible, allow for discontinuities, and enable convenient exponential transformations. Here, for the first time, the theoretical foundations for using epi-splines in density estimation are laid out. In contrast to sieves, epi-splines can be constructed independently of the sample and therefore handles small sample sizes naturally. The precursor Dong and Wets (2007) relies on a finite approximation ofL2by Fourier coefficients. In this paper, we consider the spaces of extended real-valued semicontinuous functions, exponential transformations, and epi-spline approximations.The reliance on epi-convergence and epi-splines allow us to view the constrained maximum likelihood problem as an approximation of a limiting optimization problem involving the actual probability density, correct soft information, and the full space of semicontinuous functions; we refer Pflug and Wets (2013) for a related study in the context of regression utilizing graphical convergence. Consequently, we not only approximate a certain function space or deal with finite sample size, but study the approximation of the whole estimation process as formulated by the limiting optimization problem. The approach facilitates the examination of families of estimators such as those that are near-optimal solutions of a constrained maximum likelihood problem.Our primary motivation is to obtain reasonable estimates in situations with little hard information and we provide a consistency result as soft information is refined, quantify finite sample errors, and present a small computational study to motivate the estimator in that regard. Still, we also establish consistency and quantify asymptotic rates, as hard information is refined, under general constraints.We focus exclusively on univariate densities that vanish beyond a compact interval of the real line. Although most of the results extend to the unbounded case and higher dimensions, technical issues will then become prominent and obscure the treatment of arbitrary random constraints and the supporting epi-spline approximations. Moreover, with a small sample, tail behavior can only come in via soft information, which is easily handled by our framework but omitted here for simplicity; a few experimental results can be found in Sood and Wets (2011).The paper proceeds in Section 2 by defining the constrained maximum likelihood estimator, summarizing the underlying approximation theory, which is based on Royset and Wets (2014), and discussing existence and uniqueness. Section 3 exemplify the breadth of soft information that can be included and Section 4 provides consistency, asymptotics, and finite sample error results. A small collection of numerical examples are featured in Section 5. The paper is summarized in Section 6.This section formulates a constrained maximum likelihood problem and presents a finite-dimensional approximation. We discuss existence, uniqueness, and computations. The section also includes the prerequisite approximation results.We consider a random variable X, with−∞<l≤X≤u<∞a.s. and a distribution that is absolutely continuous with respect to the Lebesgue measure, an iid sampleX1,X2,⋯,Xn,and a possibly random set Fnthat accounts for soft information about the density of X; see Sections 3 and 5 for concrete examples. Realizations of Fnare subsets of allowable functions on [l, u]. The randomly constrained maximum likelihood problem takes the form(Pn):fn∈argmax∏i=1ne−f(Xi)suchthatf∈Fn⊂F,∫lue−f(x)dx=1,where “argmax” denotes the set of optimal solutions andFis the space of extended real-valued lower semicontinuous (lsc) functionsf:[l,u]→IR¯=IR∪{−∞,∞}excluding f ≡ ∞, or alternatively the space of extended real-valued upper semicontinuous (usc) functionsf:[l,u]→IR¯now excludingf≡−∞. The density estimator then takes the forme−fn(·),withfnasolutionof(Pn).These spaces of functions under considerations are large enough to capture essentially all densities on [l, u] including, of course, those with discontinuities. Moreover, the ability to handlef(x)=∞ensures thatexp(−f(x))=0and, therefore, the exponential transformation in (Pn) does not eliminate the possibility of vanishing densities at points in [l, u]. Iff(x)=−∞,thenexp(−f(x))=∞,which obviously can at most occur for x in a set of (Lebesgue) measure zero ifexp(−f)is a density. The exponential transformation (see Good and Gaskin (1971); de Montricher et al. (1975) for early use of such transformations and Seregin and Wellner (2010) for a broader treatment) automatically ensures thatexp(−f(·))is nonnegative and explicit constraints for that purpose are redundant. In addition, some types of soft information are more easily formulated for f than forh=exp(−f(·)); see examples in Section 3. Since we approximate lsc (usc) functions by the piecewise polynomial epi-splines, to be discussed shortly, further motivation for the exponential transformation is provided by the fact that many of the common densities are indeed exponential transformations of polynomials. We observe that the lsc and usc functions are measurable and consequently the integral in (Pn) is well-defined, but possibly infinite.It is clear that a solution fnof (Pn) generates a densityexp(−fn(·))that, regardless of the sample size, possesses the properties embedded in Fn, which presumably are the properties of the actual density (see Theorems 4.2 and 4.4 and Section 5.4 for a discussion of misspecification). Moreover, it will be a best possible density in terms of the maximum likelihood criterion and the set of allowable densities.In view of the above formulation and discussion, we are unable to build on the extensive literature on sieves and follow a different path. We instead introduce a new class of functions called exponential epi-splines from which we can construct approximations independently of the sample. They allow us to substitute for the infinite-dimensional (Pn), a finite-dimensional problem, guaranteed to generate a solution that approximates, to any desired level of accuracy, a solution of (Pn).We start by defining the central building block of our approximation framework; see Royset and Wets (2014) for details. A basic epi-spline is a function given in terms of an order p ∈ IN0 ≔ {0}∪IN, whereIN:={1,2,…}and a meshm:={mk}k=0N,withmk−1<mk,k=1,2,…,N,that partitions its domain [m0, mN] in N open subintervals, where on each subinterval the basic epi-spline is a polynomial of degree p.Our focus on small samples and the use of highly flexible candidate densities in (Pn) and its epi-spline-based approximations can easily lead to overfitting. This might give the impression that the mesh m will become an important tuning parameter. However, since tuning the mesh might be challenging and easily could have become the subject of arbitrary decisions, we take another approach. We recall that (Pn) is the actual problem of interest and the estimator isexp(−fn),with fnbeing one of its solutions. Since such a solution is not directly available, our effort is directed towards obtaining an approximation through a “discretization” of the spaceF. The mesh should therefore be selected fine enough to allow epi-splines to adequately approximate the underlying spaceFof lsc (usc) functions, or possibly subsets of continuous or continuously differentiable functions, if such restrictions are warranted. With this perspective, it becomes Fnthat needs to be appropriately defined to ensure that (Pn) has reasonable solutions that avoid overfitting, among other things. Since we allow for arbitrary constraints, there are usually several ways soft information can be brought in to ensure reasonable solutions, which leads to flexibility for the analyst; see Section 5 for examples. In most of the paper, we therefore assume that the mesh m is fixed and sufficiently fine.Obviously, basic epi-splines are structurally related to polynomial splines, widely used in engineering and statistical applications Delecroix and Thomas-Agnan (2000); Meyer (2012a); Wahba (1990), as both are piecewise polynomial functions. However, basic epi-splines are more flexible, with continuity not required at mesh points, and they can approximate any extended real-valued semicontinuous function (see Theorem 2.4 below). The formal definition is stated next.Definition 2.1(basic epi-spline and associated mesh). A (basic) epi-spline s: [m0, mN]⊂IR → IR with meshm={mk}k=0Nand mesh-grade|m|:=max1≤k≤N(mk−mk−1)is of order p ∈ IN0 if on each subinterval(mk−1,mk)fork=1,⋯,N,s is polynomial of degree p.The family of all such epi-splines is denoted by e-splp(m).Exponential transformations of epi-splines result in exponential epi-splines:Definition 2.2(basic exponential epi-spline). The family of (basic) exponential epi-splines of order p ∈ IN0 with meshm={mk}k=0N,denoted by x-splp(m), consists of functions h: [m0, mN] → IR of the formh=e−s,where s ∈ e-splp(m).Since this paper deals with basic epi-splines and exponential epi-splines exclusively, we systematically drop “basic” from now on. The approximation of (Pn), relying on (exponential) epi-splines then takes the following form (after the customary switch to log-likelihood)(Pp,mn):sn∈argmin1n∑i=1ns(Xi)suchthats∈Sn⊂e−splp(m),∫m0mNe−s(x)dx=1,where Snis the formulation and possibly approximation of soft information in terms of epi-splines. In this paper, we therefore examinehn:=e−sn(·),withsnasolutionof(Pp,mn),which is our approximation ofexp(−fn). We refer to hnas the exponential epi-spline estimator. Throughout the paper we make the assumption that the support [l, u] of the true density is a subset of [m0, mN].It is clear from the definition that every s ∈ e-splp(m), with meshm={mk}k=0N,is uniquely defined by(p+2)N+1parameters11There are N subintervals(mk−1,mk)each with a polynomial of degree p. This givesN(p+1)parameters. In addition, there areN+1mesh points on which an epi-spline is freely defined (unless continuity is imposed) as motivated in §2.3, which leads to an additionalN+1parameters. We note that the mesh m is fixed.. Consequently,(Pp,mn)is equivalent to a finite-dimensional optimization problem, usualy easily solved by standard algorithms. The next subsection provides the justification for approximating (Pn) by(Pp,mn). We note that this approximation is carried out for computational reasons, as a means to overcome the infinite dimensionality of (Pn). The hard and soft information are considered fixed.Approximations of extended real-valued semicontinuous functions by epi-splines rely on the refinement of the mesh as made precise in the next definition. This subsection is based on Royset and Wets (2014).Definition 2.3(infinite refinement). Given the interval [l, u], one refers to a sequence of meshes {mν}ν ∈ IN, withmν={l=m0ν,m1ν,⋯,mNνν=u},as an infinite refinement if their mesh-grade |mν| → 0.It is clear from classical spline theory that continuous functions can be approximated by polynomial splines. We need to go beyond continuous functions to extended real-valued semicontinuous functions. We rely on the epi-topology and hypo-topology (sometimes called the Attouch-Wets topologies), which are reviewed here for completeness; see (Rockafellar and Wets, 1998, §7.I) for details. For any l < u ∈ IR, we denote bylsc-fcns([l,u])the set of all lsc functionsf:[l,u]→IR¯excluding f ≡ ∞. For any two functions, f and g, in this space, the epi-distance dl, is defined bydl(f,g):=∫0∞dlρ(f,g)e−ρdρ,wheredlρ(f,g):=max∥xˇ∥≤ρ|d(xˇ,epif)−d(xˇ,epig)|andd(x,S):=infy∈S∥x−y∥forS⊂IR2,with∥z∥:=(∑izi2)1/2andepif:={xˇ=(x,α)∈IR2|f(x)≤α}being the epigraph of f and similarly forepig; see Fig. 1for an illustration. When the metric is defined in terms of the epi-distance, it generates the epi-topology onlsc-fcns([l,u]):(lsc-fcns([l,u]),dl)is a Polish (complete separable metric) space (Rockafellar and Wets, 1998, Theorem 7.58), (Attouch, Lucchetti, and Wets, 1991, Section 5). A sequence of functions fνinlsc-fcns([l,u])epi-converge to f if their epigraphs set-converge, i.e., in the sense of taking Painlevé-Kuratowski limits (Rockafellar and Wets, 1998, §7.B), which by (Rockafellar and Wets, 1998, Theorem 7.58) takes place if and only if dl(fν, f) → 0.When dealing with usc functions,usc-fcns([l,u]),now excluding the function≡−∞,after observing that hypograph of a function f,hypof:={(x,α)|f(x)≥α}is just a mirror image of the epigraph of−f,one can mimic the definitions and constructions described for lsc functions to set up the hypo-distancedlhypo(f,g):=dl(−f,−g),between any two functions f and g and generate the hypo-topology which again makes(usc-fcns([l,u]),dlhypo)a Polish space. A sequence of functions fν hypo-converge to f if−fνepi-converge to−f. The relationship between epi- and hypo-convergence and other modes are convergence in the present context is examined below; see also (Rockafellar and Wets, 1998, Chapters 4 & 7) and Royset and Wets (2014) for broader treatments.Since the supremum of an usc function on a compact set is attained, the consideration of usc densities naturally arises in applications where the subsequent use of the densities involves maximization, such as for the purpose of finding their modes. Similarly, the lsc densities is the natural class to consider in the context of subsequent minimization. We next state an approximation results for exponential epi-splines.Theorem 2.4(lsc and usc dense approximationsRoyset and Wets (2014)). For any p ∈ IN0and {mν}ν ∈ IN, an infinite refinement of [l, u], under the hypo-topology,(⋃ν∈INx−splp(mν))⋂usc-fcns([l,u])isdensein{e−s|s∈lsc-fcns([l,u])}and under the epi-topology,(⋃ν∈INx−splp(mν))⋂lsc-fcns([l,u])isdensein{e−s|s∈usc-fcns([l,u])}.Consequently, for a sufficiently fine mesh and regardless of the order, exponential epi-splines provide arbitrarily accurate approximations ofexp(−f(·)),f∈lsc-fcns([l,u])andf∈usc-fcns([l,u]). In the remainder of the paper, we therefore mainly focus on(Pp,mn)for fixed p and m.We now turn to a convenient representation of epi-splines, also given in Royset and Wets (2014), which plays an essential role in computations and analysis. This leads to a finite-dimensional optimization problem for computing the estimator hn, which we then analyze.Every s ∈ e-splp(m), withm={mk}k=0N,is uniquely represented by an epi-spline parameterr:=(s0,…,sN,a1,…,aN),sk∈IR,k=0,…,N,ak∈IRp+1,k=1,…,N,such that for any x ∈ [m0, mN],s(x):=〈cp,m(x),r〉,where〈z,z′〉:=∑izizi′andcp,m:[m0,mN]→IR(p+2)N+1is defined bycp,m(x):={(0→N+1+(p+1)(k−1),1,xk,xk2,…,xkp,0→(p+1)(N−k)),ifx∈(mk−1,mk),k=1,…,N(0→k,1,0→N−k+(p+1)N),ifx=mk,k=0,…,N,withxk=x−mk−1and0→kdenoting the k-dimensional zero vector, k ∈ IN, and0→0being a term that is omitted. This representation of an epi-spline s lets the firstN+1components in the vector r be the values of s on m. The remaining(p+1)Ncomponents are divided into N blocks of(p+1)-tuples, each of which gives the coefficients of the polynomial defining s on intervals of the form(mk−1,mk). Specifically,ak=(ak,0,ak,1,…,ak,p)is such thats(x)=∑i=0pak,i(x−mk−1)i,forx∈(mk−1,mk),k=1,2,…,N.Since the firstN+1components of r determine the value of an epi-spline only on m, which consists of a finite number of points, we refer to the remaining(p+1)Ncomponents of r as the essential epi-spline parameter and writer=(rmesh,ress),withrmesh∈IRN+1andress∈IR(p+1)N,to indicate this partition of r. Correspondingly, we letcp,m=(cmesh,cess).Since the value of a density at a finite number of points is immaterial for the characterization of the corresponding probability distribution, it may at first appear unnecessary to specify the value of an exponential epi-splinee−〈cp,m(·),r〉on m. Instead of determiningr=(rmesh,ress),one could simply focus on ress and this is certainly the case for continuous exponential epi-splines. However, in the discontinuous case the situation is more subtle. Since we consider functions inlsc-fcns([l,u]),which are defined on the whole [l, u], their approximations should also be defined on the whole [l, u]. In addition, we would like to handle soft information such as bounds on the values of a density estimate at particular points, including at m. Hence, we find it most convenient to consider the value of epi-splines at the mesh independently and proceed with the more general framework involving rmesh.We note that convergence in the epi-spline parameter is equivalent to uniform convergence of the corresponding exponential epi-splines and, under a restriction to usc functions, also to convergence in the hypo-distance. Specifically, if hν, h0 ∈ x-splp(m), withm={mk}k=0N,hν=e−sν=e−〈cp,m(·),rν〉,andh0=e−s0=e−〈cp,m(·),r0〉,then the following hold Royset and Wets (2014):rν→r0⟺hν→h0uniformlyon[m0,mN]⟹dl(−hν,−h0)→0⟺dl(sν,s0)→0.Moreover, if hν, h0 are usc, then alsohν→h0uniformlyon[m0,mN]⟸dl(−hν,−h0)→0.We observe that since the hypo-distance does not distinguish between a function and its usc regularization (see Proposition 7.4 in Rockafellar and Wets (1998)), uniform convergence cannot generally be implied from hypo-convergence, even for exponential epi-splines.We next deal with existence and uniqueness of the estimatorhn=exp(−sn(·))and consider a computational convenient equivalent form of(Pp,mn)using the representations=〈cp,m(·),r〉. We consider a realization of(Pp,mn)with X1, …, Xnreplaced by observed valuesx1,⋯,xn,and Snis now a realization of the random constraint set. Since the meaning is clear from the context, we denote realizations also by(Pp,mn). We letRn⊂IR(p+2)N+1be the set of epi-spline parameters corresponding to the set of epi-splines Sn, i.e.,Rn:={r∈IR(p+2)N+1|〈cp,m(·),r〉∈Sn};for example, ifSn=e−splp(m),thenRn=IR(p+2)N+1. When incorporating soft information, Rnand Snbecome more restrictive as we see in Section 3. We let both the random set and its realizations be denoted by Rn. We also letRIn:={r∈Rn|∫m0mNe−〈cp,m(x),r〉dx=1}.As stated next,(Pp,mn)is equivalent to the finite-dimensional problem:(P˜p,mn):minr∈RIn1n∑i=1n〈cp,m(xi),r〉.Clearly, a realizationx1,⋯xnand Sngenerates a realization(Pp,mn)as well as a corresponding realization(P˜p,mn).Theorem 2.5(computing estimates). Given p andm={mk}k=0N,for every corresponding realizations(Pp,mn)and(P˜p,mn),one has(i)If sn∈ e-splp(m) is optimal for(Pp,mn),then there exists anrn∈IR(p+2)N+1optimal for(P˜p,mn)withsn=〈cp,m(·),rn〉.Ifrn∈IR(p+2)N+1is optimal for(P˜p,mn),thensn=〈cp,m(·),rn〉is optimal for(Pp,mn)and the exponential epi-spline estimatorhn(x)={e−〈cp,m(x),rn〉,x∈[m0,mN]0,otherwise.IfRInis nonempty and Rn is compact, then(P˜p,mn)has an optimal solution.The equivalence of(P˜p,mn)and(Pp,mn)follows directly from the representations=〈cp,m(·),r〉. The existence of an optimal solution of(P˜p,mn)follows trivially from the continuity of the involved functions and the compactness of Rn.□While the objective function in(P˜p,mn)is linear,RInmay be nonconvex. Hence,(P˜p,mn)could possess local minimizers that are not globally optimal, increasing the complexity of solving the problem numerically. We see in Section 3 that Rnis often a polyhedron or at least convex. Hence, the main difficulty in(P˜p,mn)is associated with the integral constraint. However, under broad conditions stated next, that constraint can be relaxed as utilized in other contexts earlier (see for example Groenenboom et al. (2001)). These conditions essentially imply that r can be improved whenever the corresponding function integrates to a number less than one.Definition 2.6A realization(P˜p,mn)is said to be loosely constrained if for every r ∈ Rnwith∫m0mNe−〈cp,m(x),r〉dx<1,there existsr′∈RInwith∑i=1n〈cp,m(xi),r′−r〉<0.The following Proposition 2.8 and Section 3 provide examples of loosely constrained realizations. We give an immediate consequence next.Proposition 2.7Suppose that a realization(P˜p,mn)is loosely constrained. Then, that realization and the corresponding relaxed problem(rlxPp,mn):minr∈Rn1n∑i=1n〈cp,m(xi),r〉suchthat∫m0mNe−〈cp,m(x),r〉dx≤1have identical sets of optimal solutions. Moreover, if Rn is convex, then(rlxPp,mn)is a convex problem.In Theorem 4.7 below we show that even beyond loosely constrained realizations, the consideration of(rlxPp,mn)is justified. In view of the preceding discussion and results, it is clear that the exponential epi-spline estimator is computationally tractable by means of well-developed convex optimization algorithms in many practical situations and by means of nonlinear programming algorithms in even more situations. In some cases, for example when Rnis polyhedral, some further computational benefits may arise from utilizing the following reformulation, which is valid under additional assumptions; see Section 3 for examples. The next result also gives a sufficient condition for a realization(P˜p,mn)to be loosely constrained. We use the notation1→p,Nto indicate the((p+2)N+1)-dimensional vector consisting of zeros, except at entries 1 throughN+1as well as entriesN+2+(k−1)(p+1),k=1,2,…,N,where it is unity.Proposition 2.8A realization(P˜p,mn)for which every r ∈ Rn and β ∈ IR satisfyr+β1→p,N∈Rn,is loosely constrained and its set of optimal solutions is identical to that of the corresponding penalized problem(pnlPp,mn):minr∈Rn1n∑i=1n〈cp,m(xi),r〉+∫m0mNe−〈cp,m(x),r〉dx.Moreover, if Rn is convex, then(pnlPp,mn)is a convex problem.We consider corresponding realizations(P˜p,mn)and(pnlPp,mn)and let r ∈ Rnsatisfy∫m0mNe−〈cp,m(x),r〉dx=γ<1. Forr′=r+(logγ)1→p,N,(1)∫m0mNe−〈cp,m(x),r′〉dx=1γ∫m0mNe−〈cp,m(x),r〉dx=1.Moreover,∑i=1n〈cp,m(xi),r′−r〉=∑i=1n〈cp,m(xi),(logγ)1→p,N〉=nlogγ<0.Since r′ ∈ Rnby assumption,(P˜p,mn)is loosely constrained by Definition 2.6.We next consider the penalized problem. For any r ∈ Rn, letfn(r)=1n∑i=1n〈cp,m(xi),r〉+∫m0mNe−〈cp,m(x),r〉dxand letr^∈Rnbe arbitrary. Since every epi-spline is piecewise polynomial and therefore integrates on [m0, mN] to a finite number, there exists a γ ∈ (0, ∞) such that∫m0mNe−〈cp,m(x),r^〉dx=γ. By assumption,r^+(logγ)1→p,N∈Rnand, following the same argument as in (1),∫m0mNe−〈cp,m(x),r^+(logγ)1→p,N〉dx=1.Consequently,r^+(logγ)1→p,Nis feasible in(P˜p,mn). Suppose that rnis optimal for(P˜p,mn). It follows that rnalso minimizes fnonRInbecause this problem deviates from(P˜p,mn)only by the constant one in the objective function. Using an argument similar to that of Lemma 2.3 in Groenenboom et al. (2001), we find thatfn(r^)−fn(rn)=1n∑i=1n〈cp,m(xi),r^+(logγ)1→p,N〉−logγ+∫m0mNe−〈cp,m(x),r^〉dx−fn(rn)=fn(r^+(logγ)1→p,N)−logγ−1+γ−fn(rn)≥−logγ−1+γ,where the inequality follows from the fact that rnis optimal andr^+(logγ)1→p,Nis feasible in(P˜p,mn). Since−logγ−1+γ>0for γ ∈ (0, ∞), γ ≠ 1, we find that every r ∈ Rnwith∫m0mNe−〈cp,m(x),r〉dx≠1has fn(r) > fn(rn) and consequently cannot minimize fnon Rn. The first conclusion then follows. Convexity of(pnlPp,mn)follows directly from the convexity of the integral term.□In general, one cannot expect a unique optimal solution of a realization(P˜p,mn),and consequently a unique exponential epi-spline estimate, due to the flexibility in the choice of values of the epi-spline on a mesh that is not a subset of the sample realization x1, x2,…, xn. In fact, if the firstN+1components of the epi-spline parameter r are not constrained by Rn, then there is an infinite number of optimal solutions whenever one exists. The next result shows that when these values are uniquely determined by the essential epi-spline parameter, uniqueness may still be achieved. Such a dependence on the essential epi-spline parameter is manifest, for example, in the case of continuous epi-splines used when dealing with densities known to be continuous.Proposition 2.9Suppose that corresponding realizations(P˜p,mn)and(rlxPp,mn)have Rn convex,{x1,…,xn}∩m=∅,and satisfy the condition(rmesh,ress),(rmesh′,ress′)∈Rn,withress=ress′,impliesrmesh=rmesh′.Then, the following hold:(i)If an optimal solution r of the realization(rlxPp,mn)is inRIn,then there are no other optimal solutions.The realization(pnlPp,mn)has at most one optimal solution.We start by showing strictly convexity of the integral term as a function of the essential epi-spline parameters. Givenm={mk}k=0N,we defineψ:IR(p+1)N→IRandφ:[m0,mN]×IR(p+1)N→IRbyψ(ress):=∫m0mNφ(x,ress)dx,withφ(x,ress):=e−〈cess(x),ress〉.For all x ∈ [m0, mN] andress,ress′∈IR(p+1)N,twice differentiation with respect to the second argument in φ gives that〈ress′,∇2φ(x,ress)ress′〉=〈cess(x),ress′〉2e−〈cess(x),ress〉≥0.Suppose thatress′≠0. Then, there exists ak^∈{1,2,…,N}such that〈cess(x),ress′〉is a polynomial in x forx∈(mk^−1,mk^)with not all coefficients zero. Hence, there exists a subset of(mk^−1,mk^)with positive Lebesgue measure on which〈cess(x),ress′〉≠0and(2)∫m0mN〈cess(x),ress′〉2e−〈cess(x),ress〉dx>0.Since the dominated convergence theorem implies that the left-hand side of (2) equals〈ress′,∇2ψ(ress)ress′〉,we find that ψ is strictly convex by the second-order condition for convexity.We letψ˜=(1/n)∑i=1n〈cess(xi),·〉+ψ(·),which is therefore also strictly convex.We first consider (ii). Suppose for the sake of a contradiction that there existr=(rmesh,ress)≠r′=(rmesh′,ress′)that both are optimal for the realization(pnlPp,mn),with optimal value v*. Since{x1,…,xn}∩m=∅,the objective function in this problem depends only on the essential epi-spline parameter and, in fact,ψ˜(ress)=ψ˜(ress′)=v*. We consider two cases.(a)Suppose thatress=ress′,but thenrmesh=rmesh′by assumption and we contradict the hypothesis that r ≠ r′.Suppose thatress≠ress′. Sinceψ˜is strictly convex, there exists a unique minimizerress′′ofψ˜over the convex hull of ress andress′. Moreover, there exists an α ∈ (0, 1) such thatress′′=αress+(1−α)ress′andψ˜(ress′′)<v*. By the convexity of Rn,r′′=(αrmesh+(1−α)rmesh′,ress′′)∈Rnand its objective function value in(pnlPp,mn)isψ˜(ress′′)<v*,which contradicts the optimality of v*.Second, we focus on (i). Suppose thatr=(rmesh,ress)∈RInis optimal for the realization(rlxPp,mn). We consider two cases.(a)Suppose that∫m0mNe−〈cp,m(x),r′〉dx≥1for all r′ ∈ Rn. Then by strict convexity of ψ, there exists a unique minimizerress′′of ψ on{ress′′′∈IR(p+1)N|(rmesh′′′,ress′′′)∈Rnforsomermesh′′′∈IRN+1}. However,ress′′=ressbecauseψ(ress)=1. Another optimal solution for the realization(rlxPp,mn)would thus have essential epi-spline parameter identical to ress. However, by assumption, such a solution would then also be identical to r in the remaining components, which implies it coincides with r.Suppose that there exists r′ ∈ Rnsuch that∫m0mNe−〈cp,m(x),r′〉dx<1. Then, the Slater constraint qualification is satisfied and there exists a multiplier λ ≥ 0 such that the realization(rlxPp,mn)has the same set of optimal solutions as the problem(3)minr∈Rn1n∑i=1n〈cp,m(xi),r〉+λ∫m0mNe−〈cp,m(x),r〉dx.Repeating the arguments that lead to (ii), with (3) in place of the realization(pnlPp,mn),shows that there are no other optimal solutions of the realization(rlxPp,mn)than r.□We implement soft information about the density under consideration in the estimation problem(P˜p,mn)through the set Rn, which can be any, possibly random, subset ofIR(p+2)N+1. It is observed empirically and also illustrated in Section 5 that soft information tends to improve density estimates. In this section, we give a soft consistency theorem that, in part, explains these observations. We also give examples of constraints for specific instances of soft information. We start, however, with a convenient result regarding the Kullback–Leibler divergence.Let dKL(h||g) denote the Kullback–Leibler divergence from a density h to a density g defined on IR, i.e.,dKL(h∥g):=∫−∞∞h(x)logh(x)g(x)dx. Here and below we make the standard interpretation thatβ1log(β1/β2)=0whenβ1=0regardless of the value of β2 ∈ IR andβ1logβ1/β2=∞when β1 > 0 andβ2=0. An immediate consequence of the definition of the divergence is the following result, which facilitates formulation of certain soft information as well as theoretical results below.Proposition 3.1Suppose h ande−sare densities withs=〈cp,m(·),r〉∈e−splp(m),m={mk}k=0N. Then,dKL(h∥e−s)=〈∫m0mNcp,m(x)h(x)dx,r〉+∫−∞∞(logh(x))h(x)dx.If in additionh=e−s′withs′=〈cp,m(·),r′〉∈e−splp(m),thendKL(h∥e−s)=〈∫m0mNcp,m(x)h(x)dx,r−r′〉.The next theorem is a direct consequence of Proposition 3.1.Theorem 3.2(soft consistency). If the true densityh0=e−〈cp,m(·),r0〉,with r0 ∈ Rn and there exists a ρ > 0 such that∥r−r′∥≤ρfor all r, r′ ∈ Rn, then the estimate hn, ρobtained from solving a realization(P˜p,mn)satisfiesdKL(h0∥hn,ρ)≤∥∫m0mNcp,m(x)h0(x)dx∥ρ.Moreover, for any fixed n, if ρ → 0, then hn, ρ→ h0uniformly on [m0, mN].An effective strategy for improving exponential epi-spline estimates is therefore to reduce the size of Rn, of course, without eliminating the true epi-spline parameter.We next illustrate the wide range of soft information that is easily included within the exponential epi-spline framework22Naturally, with the possibility of including incorrect soft information, there is a need for validation. Although important, we limit the discussion of this topic to Theorems 4.2, 4.4, and 4.7 as well as Section 5.4; see for example Silvapulle and Sen (2005) and Carroll, Delaigle, and Hall (2011) for tests in related contexts..Support bounds and mesh. The choice of meshm={mk}k=0Naccounts for support bounds and m0 and mNshould, ideally, correspond to the lower and upper bounds of the support of the true density, respectively. If these are unknown, conservative values could be used as our ability to approximate extended real-valued functions does not rule out the possibility of vanishing densities on [m0, mN], even for the exponentially transformed kind. In practice, m0 and mNcan be selected such that the observed sample is well within [m0, mN]. The mesh is often selected to be uniform, but the methodology offers much flexibility and soft information about possible locations of discontinuities, for example, could lead to other choices. Consequently, the mesh is selected essentially independently of the sample and one should simply focus on having a mesh that is sufficiently fine to allow epi-splines to approximate the underlying functions with a sufficient accuracy. Of course, some restrain on mesh refinement might be imposed by computational considerations. The number of decision variables in the resulting optimization problem grows linearly in N.Semi-continuity, continuity and smoothness. It is straightforward to ensure usc, lsc, continuity, and various degrees of differentiability through linear constraints; see Royset et al. (2013) for details. The inclusion of such constraint will keep a problem loosely constrained as the sufficient condition for being loosely constrained in Proposition 2.8 is satisfied.We recall that the epi-spline parameter is of the formsr=(s0,s1,…,sN,a1,0,a1,1,…,a1,p,a2,0,a2,1,…,a2,p,…,aN,0,aN,1,…,aN,p),where the firstN+1components specify the value of the epi-spline at the mesh points m0, m1,…, mNand the remaining N blocks ofp+1components give the polynomial of order p in each interval(mk−1,mk),k=1,2,…,N.Slope information. The quantity∫−∞∞h′(x)2/h(x)dxis a “measure of smoothness” that is easily expressed in terms of the epi-spline parameter, but upper and lower bounds on this expression result in undesirable nonconvex constraints. However, an alternative “normalization,” which also squares the denominator, results in a convex constraint. Specifically, ifh=e−〈cp,m(·),r〉,then∫−∞∞(h′(x)/h(x))2dx=∑k=1N∫mk−1mk(∑i=1piak,i(x−mk−1)i−1)2dx.An upper bound on this quantity results is a convex constraint. In some application, one may also seek bounds atx∈(mk−1,mk)by restrictingh′(x)/h(x)=−〈cp,m′(x),r〉=−∑i=1piak,i(x−mk−1)i−1and/orh′′(x)/h(x)=−∑i=2pi(i−1)ak,i(x−mk−1)i−2+(∑i=1piak,i(x−mk−1)i−1)2.Upper and lower bounds on the first quantity result in linear constraints and upper bounds on the second quantity gives a quadratic convex constraint. The constraints could be imposed at any number of values of x, but we note that ifp=2and the density is log-concave, as describe below, and continuously differentiable, then lower bounds on h′(x)/h(x) at m1, m2,…, mNsuffices to ensure that the constraints are satisfied for all x ∈ [m0, mN]. Similarly, an upper bound on h′(x)/h(x) needs only be imposed at m0, m1,…,mN−1. The inclusion of the pointwise constraints keep a problem loosely constrained as the sufficient condition for being loosely constrained in Proposition 2.8 is satisfied. We observe that constraints on h′(x)/h(x) is an effective way of controlling the “tails” near m0 and mN.Monotonicity. We achieve a nondecreasing (nonincreasing) density by imposing nonnegativity (nonpositivity) on h′(x)/h(x) for allx∈(mk−1,mk),k=1,2,…,Nas well assk−1≥(≤)ak,0,sk≤(≥)∑i=0pak,i(mk−mk−1)i,k=1,2,…,N.Again, simplifications arise, for example, ifp=2and the density is log-concave. Then, it suffices to impose thatak,1+2ak,2(mk−mk−1)≤0(ak, 1 ≥ 0),k=1,2,…,N. Again, a problem remains loosely constrained after the inclusion of these constraints.Log-concavity. We recall thath=e−〈cp,m(·),r〉is log-concave if and only if ⟨cp, m( · ), r⟩ is convex. This condition is ensured if ⟨cp, m( · ), r⟩ is (i) continuous, (ii) fork=1,2,…,N−1,its left derivatives at mkis no larger than its right derivative, i.e.,∑i=1piak,i(mk−mk−1)i−1≤ak+1,1,k=1,2,…,N−1,and (iii) on each(mk−1,mk),k=1,2,…,N,⟨cp, m( · ), r⟩ is convex, i.e.,∑i=2pi(i−1)ak,i(x−mk−1)i−2≥0,k=1,2,…,N,x∈(mk−1,mk).Here, the obvious interpretations are required whenp=0,1. The latter condition simplifies to ak, 2 ≥ 0,k=1,2,…,N,whenp=2. Hence, in that case, the condition of log-concavity requires only a finite number of linear constraints. Again, the problem remains loosely constrained.Unimodality and locations of modes. We implement soft information about unimodality of a continuous density by designating one mesh pointmk′as the mode, and then constraining the density to be increasing and decreasing on(mk′−1,mk′)and(mk′,mk′+1),respectively, and nondecreasing on[m0,mk′)and nonincreasing on(mk′,mN]. Solving the resulting estimation problem gives a candidate density. The process is repeated for alternative mode locations mk,k=0,1,…,N,k ≠ k′, and the density with the largest likelihood is retained as the estimate. The same result is obtained by solving a single augmented problem involvingN+1binary variables. K-modality is achieved similarly by partitioning [m0, mN] into K intervals, with each having a unimodal constraint. The process must be repeated for each partition of interest. To specify that certain mkare modes is achieved by ensuring that the density is increasing and decreasing on(mk−1,mk)and(mk,mk+1),respectively.Symmetry. We ensure symmetry by designating a point of symmetry mkand then solving only for the upper half of the density on [mk, mN], with trivial changes to the likelihood function and integral constraint. The process is repeated for each possible symmetry point. Again, auxiliary binary variables would obtain the same effect within one augmented formulation.Bounds on density values. It is straightforward to impose pointwise upper and lower bounds hmax (x) and hmin (x) on the value ofh(x)=e−〈cp,m(x),r),with 0 < hmin (x) ≤ hmax (x) < ∞. It suffices to set−loghmin(x)≥∑i=0pak,i(x−mk−1)i≥−loghmax(x)forx∈(mk−1,mk)and−loghmin(x)≥sk≥−loghmax(x)forx=mk,k=0,1,…,N.While these constraints are linear, they do not satisfy the assumption of Proposition 2.8. However, if only the lower bound h(x) ≥ hmin (x) is imposed, the resulting problem remains loosely constrained.Kullback–Leibler divergence and the Bayesian paradigm. Proposition 3.1 provides a convenient form of implementing soft information about a reference density href. In a Bayesian-like paradigm, suppose that we seek a density that is “near” href, which for example could correspond to the posterior mean obtained through Bayes theorem. Then, a constraint(4)dKL(href∥e−〈cp,m(·),r〉)≤φ(n),indeed ensures that the estimate hnis within φ(n) of href as measured by the Kullback–Leibler divergence. If href resulted from Bayes theorem, then these constraints allow for some flexibility to explore densities near the one prescribed by a classical Bayesian approach. In view of Proposition 3.1, this constraint is linear in r and thus easily implementable. Here, φ: IN0 → [0, ∞) is the cognitive content of the reference density href and should satisfyφ(0)=0,limn→∞φ(n)=∞,and be increasing since an increasing sample size should place gradually less emphasis on href. Of course, ifφ(n)=0,then(P˜m,pn)simply returns href, or a density that deviates at most on m. Ifφ(n)=∞,then no information about the reference density is included. While technically not correct in the sense of classical Bayesian statistics, one can also view href as a “prior” density and the resulting density hnobtained from(P˜m,pn)as the “posterior” density. Of course, a constraintdKL(href∥e−〈cp,m(·),r〉)≥κ,for some κ > 0 is also easily implementable, and could be relevant in contexts where a “diversity” of densities is sought. For example, one may be concerned with the validity of the soft information imposed in an initial estimate of a density and seek a set of alternative densities that are some distance away from the original estimate; see Section 5.2 for an example.Bounds on moments. Soft information may result in constraints on the jth moment of the formμjmin≤∫m0mNxje−〈cp,m(x),r〉dx≤μjmax,whereμjmin,μjmax∈IR,μjmin≤μjmaxare given constants. The right-most inequality results in a convex constraint in r, while the left-most in a nonconvex constraint.Bounds on cumulative distribution functions. Suppose that the cumulative distribution function ofh=e−〈cp,m(·),r〉at γ ∈ [m0, mN] must lie between the lower bound pmin  and the upper bound pmax . This results in the two convex constraints∫m0γe−〈cp,m(x),r〉dx≤pmaxand∫γmNe−〈cp,m(x),r〉dx≤1−pmin.Being concerned, from now on, with asymptotics, we again view(Pp,mn)to be a random optimization problem, i.e.,(Pp,mn):mins∈Sn1n∑i=1ns(Xi)suchthat∫m0mNe−s(x)dx=1;whose random elements are the variablesX1,⋯,Xnand the random set Sn; we still designate a solution by snwhich is now, itself, a random epi-spline. To achieve consistency, derive asymptotics and other results, we view{(Pp,mn)}n=1∞,for given m and p, as a sequence of random optimization problems that under quite general assumptions converges in some sense to a limiting optimization problem, whose optimal solution recovers a true density h0 ∈ x-splp(m), as the sample size n → ∞. We note that the restriction to x-splp(m) for given m and p is justified by Theorem 2.4, but we also discuss the consideration of densities beyond this broad class; see Theorem 4.4 below.We define the “approximation” of a density h by an exponential epi-spline as follows:Definition 4.1(Kullback–Leibler projection). For any density h on IR and family e-splp(m),m={mk}k=0N,the Kullback–Leibler projection of h on e-splp(m) is the set(5)Sp,m(h):=argmins∈e−splp(m)dKL(h∥e−s)suchthat∫m0mNe−s(x)dx=1.If the minimization is further constrained by s ∈ S ⊂ e-splp(m), then we denote the set of optimal solutions bySp,mS(h)and refer to it as the Kullback–Leibler projection relative to S.We see thatSp,m(h)is the set of epi-splines that gives the “closest” exponential epi-spline densities to h in the sense of the Kullback–Leibler divergence. It is well known that dKL(h||g) ≥ 0 for all densities h and g, and thatdKL(h∥g)=0if and only ifh=g,except possibly on a set of Lebesgue measure zero. Hence, if a densityh=e−s∈x−splp(m),m={mk}k=0N,thens∈Sp,m(h)and alls˜∈Sp,m(h)are identical to s (Lebesgue) almost everywhere on [m0, mN]. Since s ands˜are polynomials of order p on each open interval(mk−1,mk),k=1,2,…,N,they must be identical possibly except on m.Suppose thath0=e−s0∈x−splp(m),m={mk}k=0N,is the density of a random variable X0, which we aim to estimate. Then, for any s ∈ e-splp(m),dKL(h0∥e−s)=E{logh0(X0)}+E{s(X0)}. Hence, there is a constant term (with respect to s) in the objective function of (5) that can be dropped and we reach the fact that every optimal solution of(6)(Pp,m0):mins∈e−splp(m)E{s(X0)}suchthat∫m0mNe−s(x)dx=1is identical to s0, except possibly on m. Consequently, if the family x-splp(m) under consideration contains the true density h0, then(Pp,m0)recovers h0 or a member in its “equivalence class.”In contrast to(Pp,mn),we refer to(Pp,m0)as the true problem. Intuitively, if s0 ∈ Snand n is large, the problem(Pp,mn)approximates the true problem in some sense and one would hope that the corresponding optimal solutions are close. We next formalize this observation, which implies strong consistency of the estimatorhn=e−snobtained from solving(Pp,mn).Theorem 4.2(consistency). Suppose that the true densityh0=e−s0,withs0=〈cp,m(·),r0〉∈e-splp(m)andm={mk}k=0N,(Pp,mn)is derived by independent sampling from h0, and{sn}n=1∞is a sequence of optimal solutions of(Pp,mn),with epi-spline parameters{rn}n=1∞.If lim Rn exists a.s.33Limits of sets are here taken in the sense of Painlevé-Kuratowski (Rockafellar and Wets, 1998, Section 7.B) and the probability space is that induced by{(Pp,mn)}n=1∞.and is deterministic, then every accumulation point r∞of{rn}n=1∞satisfies〈cp,m(·),r∞〉∈Sp,mS∞(h0)a.s.,whereS∞={s∈e-splp(m)|s=〈cp,m(·),r〉,r∈limRn}.Moreover, regardless of whether Rn has a limit, if there exists a sequence{r^n}n=1∞,withr^n∈Rnfor all n, such thatr^n→r0a.s., then the following hold a.s.(i)The accumulation point r∞also satisfies〈cp,m(·),r∞〉∈Sp,m(h0).The essential epi-spline parameter subvector of r∞is identical to the essential epi-spline parameter subvector of r0.If rn→Kr∞along a subsequence K, then ⟨cp, m( · ), rn⟩ →Ks0ande−〈cp,m(·),rn〉→Kh0uniformly on [m0, mN], possibly except on m.Since X0 ∈ [m0, mN] a.s., cp, m(X0) is a random vector with finite moments. By the law of large number(1/n)∑i=1ncp,m(Xi)→E{cp,m(X0)}a.s. Letr^0∈IR(p+2)N+1be arbitrary. Then, for any sequencer^n→r^0,〈1n∑i=1ncp,m(Xi),r^n〉→〈E{cp,m(X0)},r^0〉a.s..For anyR⊂IR(p+2)N+1,we define δR(r) := 0 if r ∈ R and δR(r) := ∞ otherwise. Moreover, letRI∞:={r∈limRn|∫m0mNe−cp,m(x),r〉dx=1}. Ifr^0∈RI∞,thenlim inf〈1n∑i=1ncp,m(Xi),r^n〉+δRIn(r^n)≥〈E{cp,m(X0)},r^0〉+δRI∞(r^0)a.s.SinceRI∞=limRIn,it is closed. Consequently, ifr^0∉RI∞,then the previous inequality holds with infinity on both sides. Next, suppose thatr^0∈IR(p+2)N+1is arbitrary. Ifr^0∉RI∞,thenlim sup〈1n∑i=1ncp,m(Xi),r^n〉+δRIn(r^n)≤〈E{cp,m(X0)},r^0〉+δRI∞(r^0)=∞a.s.Ifr^0∈RI∞,then, sinceRI∞=limRIn,there exists a sequencer^n→r^0withr^n∈RInfor all n. Consequently,〈1n∑i=1ncp,m(Xi),r^n〉+δRIn(r^n)→〈E{cp,m(X0)},r^0〉+δRI∞(r^0)a.s.Epi-convergence of〈(1/n)∑i=1ncp,m(Xi),·〉+δRInto〈E{cp,m(X0)},·〉+δRI∞a.s. then follows by Proposition 7.2 in Rockafellar and Wets (1998) and the first conclusions by Theorem 7.31 of Rockafellar and Wets (1998) and the fact thatr^∈argminr〈E{cp,m(X0)},r〉+δRI∞if and only if〈cp,m(·),r^〉∈Sp,mS∞(h0).We next turn to the second part of the theorem. Since the additional assumption implies that Rnbecomes arbitrary close to r0 a.s., item (i) follows by a similar argument as above. Items (ii) and (iii) are conclusions from the discussion following Definition 4.1.□The first part of Theorem 4.2 shows that regardless of the soft information, which may even exclude the true density, the resulting exponential epi-splines tend to one that is as “close” as possible to the true density under the given constraints as the sample size increases. Specifically, the epi-splines computed from{(Pp,mn)}n=1∞tend to a point in the Kullback–Leibler projection, relative to the soft information constraint set, of the true density on the class of epi-splines under consideration. We refer to Cule and Samworth (2010); Dumbgen, Samworth, and Schuhmacher (2011); Lim and Glynn (2012) for related results on model misspecfication. The second part shows that if the true density is not excluded by the soft information, then{(Pp,mn)}n=1∞eventually yields the true density, or possibly a closely related one that deviates at most on m.The preceding results deal with the case when the true density can be exactly represented by an exponential epi-spline. If the true density is outside the class under consideration, one cannot expect to tend to the true density even if the sample size goes to infinity. However, as we see next, if two densities are close in the hypo-distance, then their Kullback–Leibler projections on e-splp(m) must also be close in some sense. We will see that this has a direct consequence on the quality of density estimates when the true density is outside the class of exponential epi-splines. Before the main theorem, we give an intermediate result.Proposition 4.3Suppose that fn: IR → [0, ∞], f0: IR → [0, ∞] are Lebesgue integrable on every compact subset of IR anddl(−fn,−f0)→0. Then, for every compact set X⊂IR, ∫Xfn(x)dx → ∫Xf0(x)dx.The restrictions of fnand f0 to X, denoted byfXnandfX0,satisfydl(−fXn,−fX0)→0. Consequently,AXn:={(x,x0)∈X×[0,∞)|fXn(x)≥x0}→AX0:={(x,x0)∈X×[0,∞)|fX0(x)≥x0}in the Painlevé-Kuratowski sense. Since the Lebesgue measures ofAXnandAX0are identical to ∫Xfn(x)dx and ∫Xf0(x)dx, respectively, the conclusion follows:□(stability of Kullback–Leibler projection). Suppose that densities hn, h0on [l, u] satisfydl(−hn,−h0)→0. If rn is such that ⟨cp, m( · ), rn⟩∈Sp,m(hn)form={mk}k=0Nwithm0=l,mN=u,then every accumulation point of{rn}n=1∞is the epi-spline parameter of somes0∈Sp,m(h0).Following a similar argument as in Proposition 2.8, we see that the equality constraints in the problems definingSp,m(hn)andSp,m(h0)can be replaced by inequality. Consequently, everysn∈Sp,m(hn)is of the formsn=〈cp,m(·),rn〉,withrn∈argminrψn(r)+δI(r),whereψn(r):=〈∫m0mNcp,m(x)hn(x)dx,r〉and δI(r) := 0 if∫m0mNe−〈cp,m(x),r〉dx≤1and δI(r) := ∞ otherwise. Similarly, everys0∈Sp,m(h0)is of the forms0=〈cp,m(·),r0〉,where r0 is a minimizer of ψ0 defined similar to ψn, but with hnreplaced by h0. Clearly,ψn+δIandψ0+δIare convex.By Proposition 4.3, ∫Xhn(x)dx → ∫Xh0(x)dx for any compact set X⊂[m0, mN]. But since cp, mis piecewise polynomial and [m0, mN] is a bounded interval, we also have that for anyk=1,2,…,N,∫mk−1mkcp,m(x)hn(x)dx→∫mk−1mkcp,m(x)h0(x)dx.Hence, it follows by Proposition 7.2 and Theorem 7.53 in Rockafellar and Wets (1998) thatψn+δItotally epi-converges toψ0+δI. The result then is a consequence of Corollary 7.55 in Rockafellar and Wets (1998).□If we take the densities hnin Theorem 4.4 to be exponential epi-splines, possibly defined on increasingly fine meshes, Theorem 2.4 shows that these densities indeed can be made to approximate with arbitrary accuracy any lsc or usc density h0 with appropriate choice of the mesh. Consequently, the assumption ofdl(−hn,−h0)→0in Theorem 4.4 holds and, combined with Theorem 4.2, we find that for a fine mesh and a large sample size the resulting exponential epi-spline estimator is “close” to the true density, even if that density is outside the class of exponential epi-splines.“Convergence” in the Kullback–Leibler divergence is closely related to other modes of convergence as stated next.Proposition 4.5Suppose that densities hn, h0 ∈ x-splp(m), withhn=e−〈cp,m(·),rn〉,h0=e−〈cp,m(·),r0〉,rn=(rmeshn,ressn),andr0=(rmesh0,ress0). Then,rn→r0⟹dKL(h0∥hn)→0⟺dKL(hn∥h0)→0⟹ressn→ress0.We letrn=(rmeshn,ressn)andr0=(rmesh0,ress0). The implication rn→ r0 ⟹ dKL(h0||hn) → 0 follows directly from Proposition 3.1.To show thatdKL(h0∥hn)→0⟹ressn→ress0we observe that dKL( · || · ) ≥ 0 and for any two densities f, g on [m0, mN],dKL(f∥g)=0if and only iff(x)=g(x)for Lebesgue almost every x ∈ [m0, mN]. We therefore consider the problemminr∈RdKL(h0∥e−〈cp,m(x),r〉),withR={r∈IR(p+2)N+1|∫m0mNe−〈cp,m(x),r〉dx=1},where r0 is a minimizer and in fact every minimizer must coincide withress0in its last(p+1)Ncomponents. In view of Proposition 3.1, the objective function in this problem is linear and the single constraint is continuously differentiable. The first-order optimality condition for this problem and the fact that{r∈IR(p+2)N+1|∫m0mNe−〈cp,m(x),r〉dx≤1}is convex imply that the hyperplaneW={r∈IR(p+2)N+1|dKL(h0∥e−〈cp,m(x),r〉)=0}is a supporting hyperplane of R withress0being the only(p+1)N-dimensional vector ress that can be augmented by aβ∈IRN+1such that{(β,ress)}=R∩W. Since rn∈ R and for sufficiently large n is arbitrarily close to W, we reach the desired conclusion.We realize that dKL(hn||h0) → 0 ⟹ dKL(h0||hn) → 0 by establishing thatressn→ress0whenever dKL(hn||h0) → 0 using a similar argument as above and then use Proposition 3.1.We find that dKL(h0||hn) → 0 ⟹ dKL(hn||h0) → 0 by invoking thatdKL(h0∥hn)→0⟹ressn→ress0and Proposition 3.1.□Asymptotic normality of the distribution of the exponential epi-spline estimator and corresponding moments may also hold when we limit the scope to the essential epi-spline parameters. As we see from the discussion before Proposition 2.9, one cannot expect a unique estimator — a prerequisite for asymptotic normality — unless the scope is limited in this manner44One could appeal to more sophisticated central limit theorems, such as those in King and Rockafellar (1990), but additional conditions and machinery is required and would require us to stray too far from our main theme.. This focus on the essential epi-spline parameter requires additional notation that we introduce next.For anyress∈IR(p+1)N,let55We use ⟩y, y⟨ to denote the outer product yy⊤ for a column vector y.H(ress):=∫m0mN〉cess(x),cess〈e−〈cess(x),ress〉dxbe the Hessian of the function∫m0mNe−〈cess(x),·〉dxat ress. We also let Σess be the variance–covariance matrix of cess(X0), with X0 distributed by the true density h0, andΣ(ress):=H(ress)−1ΣessH(ress)−1,where we note that H(ress) is nonsingular by the argument in the proof of Proposition 2.9. For notational convenience, we also let Σk(ress) be the(p+1)×(p+1)submatrix of Σ(ress) consisting of elements in columns(k−1)(p+1)+1,(k−1)(p+1)+2,…,(k−1)(p+1)+(p+1)and the corresponding rows in the latter matrix. These are the coefficients corresponding to interval(mk−1,mk). Moreover, let ress, kbe the subvector of componentsN+1+(k−1)(p+1)+1,…,N+1+(k−1)(p+1)+(p+1)of ress, i.e., the parameters that define the epi-spline in(mk−1,mk),and the corresponding subvectors of cess are denoted by cess, k. Finally, we letμj0:=∫−∞∞xjh0(x)dxbe the jth moment of the true density h0,N(0,Σ)denote a zero-mean normal vector with variance–covariance matrix Σ, and →dconvergence in distribution. We are now ready to state an asymptotic result for an exponential epi-spline estimator, where we make the assumption that the soft information is “clearly” correct, i.e., the true density corresponds to a point in the interior of the sets Rna.s. for sufficiently large n. Moreover, we assume that the true density is an exponential epi-spline. Although this might at first appear restrictive, Theorem 2.4 shows that such densities can approximate to an arbitrary level of accuracy essentially any density.Theorem 4.6(asymptotics). Suppose that the true densityh0=e−s0∈x-splp(m),withm={mk}k=0N,s0=〈cp,m(·),r0〉,andr0=(rmesh0,ress0)is in the interior of the (set) inner limit of the Rn a.s. If(Pp,mn)is obtained by independent sampling from h0and{sn}n=1∞is a sequence of optimal solutions of(Pp,mn),with epi-spline parameters{rn=(rmeshn,ressn)}n=1∞,andhn=e−〈cp,m(·),rn〉for all n, then the following hold:(i)n1/2(ressn−ress0)→dN(0,Σ(ress0))Forx∈(mk−1,mk),k=1,2,…,N,n1/2(hn(x)−h0(x))→dN(0,e−2〈cess,k(x),ress,k〉〈cess,k(x),Σk(ress0)cess,k(x)〉).For j ∈ IN, and settingw=∫m0mNxjcess(x)e−〈cp,m(x),r0〉dx,the moment estimatorμjn=∫m0mNxje−〈cp,m(x),rn〉dxsatisfiesn1/2(μjn−μj0)→dN(0,〈w,Σ(ress0)w〉).The law of large number gives that the objective function in(Pp,mn)converges uniformly on compact sets to that of(Pp,m0)a.s. We recall that ⟨cp, m( · ), r0⟩ is an optimal solution of(Pp,m0)and, by assumption, r0 is also in the interior of the (set) inner limit of the Rna.s. Consequently, since(Pp,m0)does not involve a restriction Sn, the set of optimal solutions of(Pp,mn)coincides with those of the relaxation of(Pp,mn)with Snreplaced by e-splp(m) for sufficiently large n. Let(Pessn):minress∈IR(p+1)N1n∑i=1n〈cess(Xi),ress〉+∫m0mNe−〈cess(x),ress〉dx,where X1, X2,…, Xnis the sample from h0. We deduce from Propositions 2.8 and 2.9 that(Pessn)and the relaxation of(Pp,mn)have unique optimal solutions a.s. and that they are equivalent in the sense that they generate the same essential epi-spline parameter. Consequently, for sufficiently large n, the optimal solution of(Pessn)isressna.s.Let X0 be a random variable with density h0 and(Pess0):minress∈IR(p+1)NE{〈cess(X0),ress〉}+∫m0mNe−〈cess(x),ress〉dx.We deduce from Propositions 2.8 and 2.9 that an optimal solution of this problem is unique and coincides with the essential epi-spline parameterress0of h0.Since(Pess0)and(Pessn)are strictly convex and unconstrained a.s., their unique optimal solutions are equivalently characterized as the zeros of the objective function gradients. Since these gradients converge uniformly onIR(p+1)Na.s. by the law of large numbers, and the corresponding Hessians are identical and positive definite, item (i) follows directly from Theorem 4 of Pasupathy (2010). The next items follow by a direct application of a Delta Theorem; see, for example, Section 7.2.7 in Shapiro, Dentcheva, and Ruszczynski (2009).□Although Theorem 4.6 provides rates of convergence, it excludes the possibility of soft information in Rninfluencing the estimates for large samples and, in addition, deals only with the essential epi-spline parameter. We end the section by examining errors for a finite sample size under relaxed assumptions, which leads to another rate of convergence result. However, the treatment requires us to focus on ε-optimal solutions of(rlxPp,mn),now viewed as a random optimization problem, which for any ε ≥ 0 are defined asRɛn:={r∈Rn|1n∑i=1n〈cp,m(Xi),r〉≤Vn+ɛ,∫m0mNe−〈cp,m(x),r〉dx≤1},where the optimal value of(rlxPp,mn)isVn:=infr∈Rn1n∑i=1n〈cp,m(Xi),r〉suchthat∫m0mNe−〈cp,m(x),r〉dx≤1.The statements below deal with the difference between the true density h0 andhɛn:=e−〈cp,m(·),rɛn〉,withrɛn∈Rɛnfor ε > 0. In fact, a numerical method for solving a realization of(rlxPp,mn)generates an element ofRɛn,and consequently also an estimatehɛn,as such methods utilize finite precision and various tolerances. Also let ρIB ≔ {y | ‖y‖ ≤ ρ} in any Euclidean space,Δp,m:=maxl=0,1,…,p|m|l,andd(x,S):=infy∈S∥x−y∥for x ∈ IRk, S⊂IRk.Theorem 4.7(finite sample error). Suppose that the true density h0 ∈ x-splp(m),m={mk}k=0N,with epi-spline parameter r0, and(rlxPp,mn)is derived by independent sampling from h0, has a nonempty feasible set a.s., and Rn is closed and convex a.s. For any α > 0, ε > 0,ρ>max{−Vn,d(r0,R0n)},and somehɛn=e−〈cp,m(·),rɛn〉,rɛn∈Rɛn,d(r0,Rɛn)>KanddKL(h0∥hɛn)>∥∫m0mNcp,m(x)h0(x)dx∥K,with probability at most2(p+1)Ne−2n(α/Δp,m)2,whereK:=(1+4ρɛ)[α(ρ+∥r0∥)(p+1)N+(1+Δp,m(p+2)N+1)d(r0,Rn)].Let X0 be a random variable with density h0 and X1, X2,…, Xnbe the sample that generates(Pp,mn). We denote bycp,mj(X0)the components of cp, m(X0),j=1,2,…,(p+2)N+1. Forj=1,2,…,N+1,cp,mj(X0)=1ifX0=mj−1andcp,mj(X0)=0otherwise. Consequently,E{cp,mj(X0)}=0and, likewise,(1/n)∑i=1ncp,mj(Xi)=0a.s. Forj=N+1+(p+1)(k−1)+l+1,l=0,1,…,p,k=1,2,…,N,cp,mj(X0)=(X0−mk−1)lifX0∈(mk−1,mk)andcp,mj(X0)=0otherwise. Consequently, forj=N+2,N+3,…,(p+2)N+1,cp,mj(X0)∈[0,Δp,m]a.s. and by Hoeffding’s inequality,P(|1n∑i=1ncp,mj(Xi)−E{cp,mj(X0)}|≥α)≤2e−2n(α/Δp,N)2for every α ≥ 0. Moreover, Boole’s inequality gives, when taking advantage of the zero error forj=1,…,N+1,thatP(⋃j=1(p+2)N+1{|1n∑i=1ncp,mj(Xi)−E{cp,mj(X0)}|≥α})≤2(p+1)Ne−2n(α/Δp,m)2.Letφn:IR(p+2)N+1→IR¯be defined byφn:=(1/n)∑i=1n〈cp,m(Xi),·〉+δn(·)where δn(r) := 0 if r ∈ Rnand∫m0mNe−〈cp,m(x),r〉dx≤1,and δn(r) := ∞ otherwise. Letφ0,n:IR(p+2)N+1→IR¯be defined byφ0,n:=E{〈cp,m(X0),·〉}+δ0,n(·)where δ0, n(r) := 0 if∫m0mNe−〈cp,m(x),r〉dx≤1and r is in the convex hull of Rnand r0, and δ0, n(r) := ∞ otherwise.In view of the preceding results and definitions, forr−r0∈ρIB,with ρ ∈ (0, ∞),|(1/n)∑i=1n〈cp,m(Xi),r〉−E{〈cp,m(X0),r〉}|≤α(ρ+∥r0∥)(p+1)Nwith at least probability1−2(p+1)Ne−2n(α/Δp,m)2. Using this fact, Example 7.62 of Rockafellar and Wets (1998) gives that with the same probability,dlρ+(φn,φ0,n)≤α(ρ+∥r0∥)(p+1)N+(1+Δp,m(p+2)N+1)d(r0,Rn),wheredlρ+is closely related to dlρ; see Section 7.I in Rockafellar and Wets (1998). Then, from Theorem 7.69 in Rockafellar and Wets (1998), we deduce the first result after realizing that r0 is an ε-optimal solution of min φ0, n, where the additional factor1+4ρ/ɛarises from that theorem. Proposition 3.1 yields the second conclusion.□Theorem 4.7 shows that there are two sources of error in the estimation process corresponding to the two parts of K. The first source is sampling error, represented by the term involving α, which can be made small by selecting a small α and this error is only exceeded with a small probability if nα2 is large. The second source is caused by d(r0, Rn), the distance between the true epi-spline parameter and the constraint set Rn. Of course, if only appropriate soft information is included, then r0 ∈ Rnandd(r0,Rn)=0. Otherwise, incorrect specification of soft information induces a “bias” in the density estimator. We note that Theorem 4.7 provides additional support for considering(rlxPp,mn)also for instances which are not loosely constrained. Even in such cases,(rlxPp,mn)is guaranteed to generate a density near the true density.We recall the notion of “bounded in probability.” For a sequence of random variables{Yn}n=1∞,we writeYn=Op(1)when for any ζ > 0, there exists a β ≥ 0 such that Prob(|Yn| > β) ≤ ζ for all n.Corollary 4.8For sufficiently large n, suppose that the assumptions of Theorem4.7hold and d(r0, Rn)=0 a.s. Then,n1/2dKL(h0∥hɛn)=Op(1)forsomehɛn=e−〈cp,m(·),rɛn〉,rɛn∈Rɛn.Theorem 4.7 and the fact thatd(r0,Rn)=0imply that for sufficiently large nProb(n1/2dKL(h0∥hɛn)>K′αn1/2)≤2(p+1)Ne−2n(α/Δp,m)2,whereK′=∥∫m0mNcp,m(x)h0(x)dx∥(1+4ρ/ɛ)(ρ+∥r0∥)(p+1)N. We let ζ > 0 and couple α and n such thatζ=2(p+1)Ne−2n(α/Δp,m)2,i.e.,n=−Δm,p2log(ζ/2(p+1)N)/(2α2). Conse-quently,Prob(n1/2dKL(h0∥hɛn)>β)≤ζ,whereβ:=K′(−Δm,p2log(ζ/2(p+1)N)/2)1/2and the conclusion follows:□In view of the preceding result, we see that the canonical rate ofn−1/2is obtained for the exponential epi-spline estimator even if soft information is “active.”We illustrate the exponential epi-spline estimator through a series of examples using a freely available Matlab toolbox Royset and Wets (2013) that relies on the fmincon solver (Matlab 7.10.0); see also Buttrey, Royset, and Wets (2014) for a corresponding R toolbox. The focus is on showing the effect of including various sources of soft information in the context of small sample sizes. Section 5.1 shows estimates of an exponential density using 10 observation and an increasing collection of soft information. Section 5.2 provides an alternative to the Bayesian paradigm and demonstrates how a diverse family of densities can be generated. Section 5.3 examines the probability density of customer time-in-service for a modified M/M/1 queue. Section 5.4 shows the effect of incorrect soft information. The section ends with Section 5.5, where soft information about moments is examined for increasing sample sizes. It is beyond the scope of the paper to include a comprehensive comparison with alternative density estimators, which in any case have difficulties with incorporating an arbitrary set of soft information. Occasionally, we simply contrast with kernel estimates using “ksdensity” in Matlab, a Gaussian kernel, and default bandwidths, which are optimized in some sense for the normal densities. These estimates can possibly be improved with better bandwidth and kernel choices. In all cases, we use epi-splines of order two and if there is no soft information about support bounds, we set m0 (mN) to two sample-estimated standard errors below (above) the smallest (largest) sample point, and use uniform meshes. The set Rnalways includes the loose constraints−1000≤r≤1000. The Gauss–Legendre quadrature rule with 20 points evaluates the integrals over each segment(mk−1,mk)with high accuracy. We often assess the quality of an estimate hnof a density h0 by the mean-square error (MSE)∫−∞∞(hn(x)−h0(x))2h0(x)dx. For additional numerical results we refer to Royset et al. (2013); Royset and Wets (2014); Singham et al. (2013); Sood and Wets (2011).We illustrate the effect of soft information in a simple example. For a true exponential density with parameterλ=1(dotted black curves in Fig. 2) and a sample of size 10 (see green stems), Fig. 2 shows our exponential epi-spline estimates (solid red curves) under two classes of soft information: (a) continuously differentiable, nonnegatively supported, and log-concave density and (b) also nonincreasing density and a relative bound on the slope. The soft information about relative slope amounts to letting the quantity hn′(x)/hn(x) be in the interval[−1,0]. We observe that the exponential density h0 with parameterλ=1hash0′(x)/h0(x)=−1for all x ≥ 0.For comparison, a kernel estimate, incorporating information about a nonnegative support, is displayed by dashed black curves. The exponential epi-spline estimates are obtained using a mesh withN=10. In Fig. 2(a), MSE is 0.1144 and 0.3273 for exponential epi-spline and kernel estimates, respectively. The kernel estimate reaches well above 4.5 near zero, though the plots are truncated for the sake of clarity. Fig. 2(b) shows the visually improved exponential epi-spline estimate with a reduced MSE of 0.0416. The exponential epi-spline estimates miss the density peak at zero, but the present sample provides few indications about such a peak and its capture will naturally be difficult. Still, the exponential epi-spline estimate is both qualitatively and quantitatively close to the true density elsewhere. The ability to incorporate various kinds of soft information along the lines illustrated here offers the analyst a valuable tool for exploring assumptions and their consequences. One can attempt to improve the kernel estimate using various bandwidth as well as truncation (see for example (Tsybakov, 2009, p.19)). The effect on bandwidth in the kernel estimate is illustrated in Fig. 3(a), where the case with default bandwidth (given in Fig. 2(a)) is supplemented by estimates using bandwidth 0.15, 0.2625, 0.375, 0.4875, and 0.6. The combination of a nonnegative support and few data points make it nontrivial to select an appropriate bandwidth and the estimates remain mostly qualitatively similar. In Fig. 3(b) we remove the requirement of a nonnegative support. Again, the choice of bandwidth appears challenging. However, truncation and renormalization of the portion of the density estimates to the left of the origin improves the situation; see the dashed blue lines kernel (trunc) in Figs. 2(a), (b), and 3(a) that give the resulting estimate when truncating the (default) density estimate in Fig. 3(b) (black line).Our framework provides an alternative to traditional Bayesian updating. In addition to the inclusion of numerous types of soft information—which can be viewed as “prior” information—we may also directly restrict(P˜m,pn)to a neighborhood of a reference density href using (4). To illustrate the framework, consider a reference (prior) density that is standard normal and a sample consisting of 10 points from the same density; see Fig. 4. We setN=10and restrict the search to continuously differentiable densities. If no emphasis is placed on the reference density, i.e.,φ(10)=∞in (4), then we obtain the exponential epi-spline estimate marked with the red dotted line in Fig. 4. As proximity to the reference density is enforced more vigorously by settingφ(10)=1,0.1, and 0.01, we obtain the dashdot, dashed, and solid lines, respectively, in Fig. 4. The Kullback–Leibler divergence constraints dampen the oscillations caused by the sample by a degree determined by φ(10), which in practice should be selected based on the confidence in the correctness of the reference density.A related situation arises when an analyst would like to generate multiple densities that span a range of possibilities, for example to account in some manner for questionable soft information. For example, when the estimated density is to be used as input in further simulation and optimization, it may be prudent to consider a set of densities and possibly let planning be based on the worst density in some sense. We illustrate this situation by returning to the exponential example of Section 5.1. Suppose that the second density generated there (see Fig. 2b) is considered plausible, but we would like to also generate relevant alternatives. Retaining a restriction to continuously differentiable, nonincreasing, and nonnegatively supported densities, we construct three alternatives by imposing (4) with ≤ replaced by ≥ and right-hand side 0.1, 0.01, and 0.001, and href being the original estimate in Fig. 2b. Consequently, we determine densities that are at least certain “distances” away from the original estimate in the sense of Kullback–Leibler divergence, while still maximizing the likelihood function of the sample. Fig. 5shows the results with the solid red line and dotted black line showing the original estimate and true density as in Fig. 2b. The alternative densities are depicted with dashed, dot-dashed, and dotted red lines for right-hand sides of 0.001, 0.01, and 0.1, respectively. We observe that even though based on only 10 sample points, the original together with the alternative densities provide a “diversified” set of densities near the true density well suited as input for further studies.Significant challenges arise when the density to be estimated is discontinuous. We illustrate this situation here by an example taken from Singham et al. (2013); see Royset and Wets (2014) for additional examples. Suppose that the random variable of interest is the customer time-in-service of a modified M/M/1 queue with arrival rateλ=1and service rateμ=1.5,but where 50% of customers who enter the system are held at a separate station for two time units. Obviously, the true density is an equal mixture of the probability density of the customer time-in-service without a separate station (an exponential density) and the same density shifted to the right by two time units, yielding a discontinuous density. Using a sample of sizen=100,we aim to recover this density using a lower semicontinuous exponential epi-spline estimate withN=10.Table 1shows aggregated results across 100 meta-replications for a variety of soft information. The first row of results shows MSE under no additional information beyond lower semicontinuity and bounds on the second-order derivatives. The second row corresponds to a restriction of the slope to be in the interval[−4,0]and the third row assumes a nonnegative support. The last row incorporates bounds on the slope, nonnegative support, and log-concavity of the upper tail. We show that the average MSE (second column) decreases with increasing soft information and mostly also the standard deviation of the MSE (third column).Fig. 6shows an instance corresponding to the last row in Table 1. The MSE of the exponential epi-spline (red line) estimate is 0.0016. The exponential epi-spline estimate captures the essence of the true density (dotted line) rather well.As given by Theorem 4.2, optimal solutions of(Pp,mn)tend to a point in the Kullback–Leibler projection of the true density h0 relative to the set constructed by the soft information as the sample size grows. Consequently, in the presence of incorrect soft information that excludes h0, we achieve the density “nearest” to h0 within the set of densities satisfying the (incorrect) soft information. We illustrate this situation by considering a standard normal density and its exponential epi-splines estimates based onN=10. We adopt soft information about continuous differentiability and log-concavity. In addition, we impose the incorrect constraint that the expected value must be no larger than−0.5. Fig.7a shows the resulting exponential epi-spline estimate (solid red line) and the kernel estimate (dashed black line) forn=100. Fig.7b displays the corresponding results forn=1000. We observe that while the kernel estimator benefits from the larger sample size and obtains a nearly perfect estimate forn=1000,the unfortunate expectation constraint on the exponential epi-spline prevents it from approaching the true density. However, we obtain a “normal-looking” density with a shifted mean of−0.5.We end the section by presenting a summary of results over a range of sample sizes for a normal density with zero mean and standard deviation of two. We carry out 104 meta-replications and compute average and standard deviation of the resulting MSE for both an exponential epi-spline estimate and a kernel estimate. We useN=20and soft information that amounts to continuous differentiability, log-concavity, and bounds on first and second moments that ensure estimates with moments within 20% of their correct values.Fig. 8shows the corresponding average and standard deviation of the MSE for a range of sample sizes. We see that the exponential epi-splines estimates result in smaller MSE, on average. However, the advantage decreases as the sample size grows as expected.

@&#CONCLUSIONS@&#
We have developed a constrained maximum likelihood estimator that incorporates any soft information that might be available and therefore offers substantial flexibility for practitioners. In particular in situations with few (hard) observations, soft information can be brought in and reasonable estimates can be achieved with as little as 10 sample points. In simple but illustrative examples of estimating exponential, normal, and mixture of exponential distributions, we construct new estimates under a variety of soft information not commonly considered. The estimator requires the solution of an infinite-dimensional optimization problem, which we carry out approximately utilizing exponential epi-splines. The justification stems from the fact that exponential epi-splines can approximate to an arbitrary level of accuracy practically any density. We show that optimization over exponential epi-splines often reduces to convex programming. Our theoretical development establishes consistency, asymptotic normality, and finite sample error of orderO(n−1/2)under the assumption that the true density is an exponential epi-spline.