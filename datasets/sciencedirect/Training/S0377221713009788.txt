@&#MAIN-TITLE@&#
A heuristic for scheduling in a two-stage hybrid flowshop with renewable resources shared among the stages

@&#HIGHLIGHTS@&#
The flowshop with unrelated machines and resources shared among stages is considered.A heuristic using priority rules and a column generation algorithm is proposed.Priority rules depend on resource requirements.A column generation algorithm solves the problem with varying resource availability.The results of the experiment show that the performance of the heuristic is good.

@&#KEYPHRASES@&#
Scheduling,Heuristic,Column generation,Tabu search,Flowshop,Resource constraints,

@&#ABSTRACT@&#
In this paper we propose a heuristic for solving the problem of resource constrained preemptive scheduling in the two-stage flowshop with one machine at the first stage and parallel unrelated machines at the second stage, where renewable resources are shared among the stages, so some quantities of the same resource can be used at different stages at the same time. Availability of every resource at any moment is limited and resource requirements of jobs are arbitrary. The objective is minimization of makespan. The problem is NP-hard. The heuristic first sequences jobs on the machine at stage 1 and then solves the preemptive scheduling problem at stage 2. Priority rules which depend on processing times and resource requirements of jobs are proposed for sequencing jobs at stage 1. A column generation algorithm which involves linear programming, a tabu search algorithm and a greedy procedure is proposed to minimize the makespan at stage 2. A lower bound on the optimal makespan in which sharing of the resources between the stages is taken into account is also derived. The performance of the heuristic evaluated experimentally by comparing the solutions to the lower bound is satisfactory.

@&#INTRODUCTION@&#
A hybrid flowshop is a system which consists of a set of two or more processing centers (processing stages) with at least one center having two or more parallel machines. A job in such a system consists of a sequence of operations processed at successive stages, and all jobs pass through processing stages in the same order. At a stage with parallel machines a job can be processed on any of the parallel machines. The flowshops with parallel machines have received considerable attention from researchers during last years (e.g. Choong, Phon-Amnuaisuk, & Alias, 2011; Gupta & Stafford, 2006; Kis & Pesch, 2005; Linn & Zhang, 1999; Oguz, Ercan, Cheng, & Fung, 2003; Rashidi, Jahandar, & Zandieh, 2010; Ribas, Leisten, & Framinñan, 2010; Ruiz & Vazquez-Rodriguez, 2010; Yaurima, Burtseva, & Tchernykh, 2009), but scheduling problems with additional renewable resource constraints have been widely investigated only for the one stage systems (e.g. Błażewicz, Cellary, Słowiński, & Węglarz, 1987; De Werra, 1988; Edis & Ozkarahan, 2012; Różycki & Węglarz, 2012; Słowiński, 1980).In Figielska (2008, 2009, 2010, 2011) the research on the flowshop has been extended by including resource constraints into the stages. In Figielska (2008), a new algorithm based on the linear programming was proposed for minimizing makespan in the two-stage flowshop with parallel unrelated machines and renewable resources at the first stage and a single machine at the second stage. The resource requirements were assumed to be of 0–1 type. The paper of Figielska (2010) also deals with the problem with 0–1 resource requirements, but extended to the case of parallel machines and renewable resource constraints at both the stages, where to each of the stages its own resources are allotted. In Figielska (2009) the problem similar to that in Figielska (2008) but with arbitrary resource requirements was considered. In that paper, a column generation (CG) algorithm was used for solving the resource constrained parallel machine scheduling problem with job ready times equal to zero and resource availability, which was constant throughout the scheduling period. The obtained first stage schedule was composed of partial schedules. Then a metaheuristic was applied to sequencing these partial schedules so as to minimize the makespan in the flowshop. In (Figielska, 2011), three metaheuristics: tabu search, simulated annealing and a genetic algorithm were proposed to improve the solutions provided by the LP-based algorithms developed in Figielska (2008) for scheduling in the flowshop with 0–1 resource requirements.The present paper further extends hybrid flowshop scheduling research by considering the problem with renewable resources shared among the stages. Each resource is available at any moment in limited quantity and can be used at the same time at different stages, so, if a resource is used at one stage, its quantity available at the same time at the other stages lessens. Resource requirements of jobs are assumed to be arbitrary integers. We consider the flowshop with a single machine at the first stage and parallel unrelated machines at the second stage. The objective is to minimize the makespan.In this paper, we propose a CG algorithm which solves the second stage problem of resource constrained preemptive scheduling on parallel unrelated machines with non-zero ready times of jobs and with resource availability at any moment depending on the usage of the resources at the first stage. Both the resource availability and ready times at stage 2 depend on the order of jobs at stage 1. For ordering the jobs, 5 priority rules based on the Johnson’s rule (Johnson, 1954) are proposed. These rules (except one) depend not only on processing times of jobs but also on their resource requirements.The proposed CG algorithm in successive iterations minimizes the makespan by using linear programming (LP), generates n (n being the number of jobs) new columns (a column determines the assignment of jobs to machines for simultaneous processing) by means of a tabu search (TS) algorithm so as to improve the solution (makespan), and uses a greedy procedure to provide an initial solution to the TS algorithm. The CG algorithm provides a schedule which is composed of n sets of partial schedules (in a partial schedule jobs are assigned to machines for simultaneous processing during some period of time so that resource constraints are satisfied at every moment) belonging to successive time intervals which correspond to processing times of the consecutive jobs executed at stage 1. The availability of the resources and the number of jobs which are ready for processing at stage 2 are different in the partial schedules from the different sets.The performance of the heuristic is experimentally evaluated by comparing the solutions with a lower bound on the optimal makespan. Two lower bounds are considered. A lower bound is proposed in which sharing of the resources between the stages is taken into account. The influence of the choice of the TS initial solution on the convergence speed of the CG algorithm is also discussed.The problem under consideration arises in real-life systems that are encountered in a variety of industries, e.g. in chemical, food, cosmetics and textile industries. These systems usually consist of a number of production centers which can have parallel machines. They are often subjected to some resource constraints associated with limited availability of the resources such as skilled labor, tools, power. The resources may be capable of migrating to processing centers as needed, for example workers which are cross-trained to develop the skills required to perform different tasks associated with multiple processing centers (Daniels & Mazzola, 1994). Another example of shared resources can be equipment, personnel and information technology which are used in both of the two stages of hospital operations: to generate medical record to track treatments, tests, drug dosages and costs in the first stage and to generate the second stage patient services (Chen, Du, Sherman, & Zhu, 2010).The problem being considered can be formally described as follows. There are n jobs to be processed at two stages in the same technological order, first at stage 1 then at stage 2. Stage 1 has one machine, at stage 2 there are m parallel unrelated machines. A job upon finishing its processing at stage 1 is ready to be processed at stage 2; it may be processed at stage 2 when a machine is available there, or it may reside in a buffer space of unlimited capacity following stage 1 until one of the machines at stage 2 becomes available. At stage 2, a job can be processed on any of the parallel machines, and its processing times may be different on different machines. The processing time of job j (j=1,…,n) is equal to sjtime units if it is executed at stage 1, and pijtime units if it is executed on machine i (i=1,…,m) at stage 2. At stage 1, each job is processed without interruptions. Preemptions of jobs are allowed at stage 2, where the processing of a job on a machine may be interrupted at any moment and resumed later on the same or another machine without incurring any additional cost or loss of time. No setups or repetitions of parts of jobs are considered. Jobs for their processing at both the stages require, besides machines, additional resources. There are l types of additional resources. The resources are shared among the stages, i.e. at the same time some quantities of a resource of a given type can be used by both the stages. A resource of type r (r=1,…,l) is available in a quantity limited to Wrunits at a time. Job j uses αjrunits of resource r when it is processed at stage 1 and βijrunits of resource r while it is processed on machine i at stage 2. The total usage of resource r at any moment by jobs simultaneously executed at both the stages cannot exceed the availability of this resource. All required resources are granted to a job before its processing begins or resumes and they are returned by the job after finishing its processing at a stage or in the case of its preemption. The objective is to find a feasible schedule which minimizes makespan, Cmax, which is equal to the maximum job completion time at stage 2 of the flowshop.The considered problem is NP-hard in the strong sense since the problem of preemptive scheduling in the two-stage flowshop with two identical parallel machines at one stage and one machine at another is NP-hard in the strong sense (Hoogeveen, Lenstra, & Veltman, 1996).The heuristic first sequences jobs on the machine at stage 1 according to a priority rule. Given the order of jobs at stage 1, a CG algorithm solves the minimum makespan problem at stage 2.For solving the problem at stage 2, the scheduling period is divided into n time intervals with lengths (apart from the last one) equal to the processing times of consecutive jobs executed at stage 1 (starting from the second job), so interval k begins at the time of the completion of the kth job at stage 1 and has the length equal to the processing time of (k+1)th job at stage 1. The schedule at stage 2 begins when the first job completes its processing at stage 1. The last time interval (of index n) begins when the last job leaves stage 1, and its length is minimized.The CG algorithm minimizes the length of the last time interval by creating partial schedules in all the time intervals (let us recall that in a partial schedule jobs are assigned to machines for simultaneous processing during some period of time so that resource constraints are satisfied at every moment). The CG algorithm is an iterative procedure which starts from a number of randomly generated columns (a column determines an assignment of jobs to machines in a partial schedule), and, in every iteration, in turn minimizes the makespan by solving an LP problem using columns obtained in previous iterations, and generates n (n is the number of jobs) new columns (one column for each time interval) improving the solution. These new columns are created by means of a TS algorithm which uses a dual improve condition as an objective function and starts from an initial solution generated by a greedy procedure. Two greedy rules are considered, both take into account the values of dual variables, but one additionally includes resource requirements of jobs.Six priority rules used in the heuristic for sequencing jobs at stage 1 are presented in Table 1.Rule P1 creates a random sequence of jobs at stage 1 and is used to show the usefulness of the next rules. Priority rule P2 adapts the Johnson’s rule to the problem with parallel unrelated machines at the second stage. Rules P3–P6 extend rule P2 by introducing the dependence on the resource requirements of jobs. The values ofα¯jandβ¯j(0<α¯j,β¯j⩽1)used in these rules are defined as the average (over all the resources) ratio of resource requirements of job j, respectively at stage 1 and at stage 2, to the resource availability.The algorithms implementing priority rules P1–P6 will be, respectively, referred to as A1–A6.Detailed description of the heuristic is presented in the next subsections, and the pseudocode of the heuristic is given in Appendix A.To illustrate the problem and the solution method we present the following example. Consider the case of the two-stage flowshop with a single machine at stage 1 and m=2 machines at stage 2. The number of jobs n=5. There is one renewable resource shared among the stages. Its availability at any moment W=12. Job processing times and resource requirements are shown in Table 2.Fig. 1presents two flowshop schedules for this instance, one (in Fig. 1a) with a random order of jobs at stage 1, and another (in Fig. 1b) with jobs ordered at stage 1 according to rule P5 (see Table 1). The second stage schedules were obtained by using the CG algorithm proposed in this paper. These schedules begin after completing the first job at stage 1 and they are composed of 5 sets of partial schedules, 4 of which belong to time intervals 1–4 with lengths equal respectively to the processing times of consecutive jobs executed at stage 1, and one belongs to the last time interval whose length is minimized. For example, in Fig. 1b, in interval 4 there is the set of 3 partial schedules. The jobs in each of these partial schedules are executed simultaneously with job 3 processed at stage 1. As job 3 requires 4 resource units and the total resource availability is equal to 12, the availability of the resource in interval 4 at stage 2 equals 8. In the first partial schedule in this interval, jobs 2 and 5 are simultaneously executed and use 4 resource units each, in the second partial schedule, simultaneously processed jobs 2 and 1 also require 4 resource units each, in the third partial schedule, only job 4 is executed and its resource requirements are equal to 8. So in all these partial schedules the jobs use 8 resource units at every moment, hence resource constraints are satisfied. In time intervals 1, 2, 3 and 5 the availability of the resource at stage 2 is, respectively, equal to 8, 6, 10 and 12 and the partial schedules also satisfy resource constraints at every moment.We can see that the flowshop schedule in Fig. 1b obtained by the algorithm using priority rule P5 for sequencing jobs at stage 1 (Table 1) is considerable shorter than that in Fig. 1a obtained for a random order of jobs at stage 1. This is due to shorter idle time of the machines at stage 2.Without loss of generality, in what follows, we assume that jobs are indexed in such a way that they are executed at stage 1 in order 1,2,…,n. The intervals corresponding to job processing times at stage 1 are indexed from 0 to n, so that in kth interval, jobs of indices from 1 to k can be processed at stage 2 (in interval 0 no job is executed at stage 2). The length of interval k (k=1,…,n−1) is equal to sk+1 (skis the processing time of kth job at stage 1). The length of the nth time interval (the last one) is minimized. The availability of resource r (r=1,…,l) at stage 2 is equal to Wr−αk+1,rin intervals k=1,…,n−1, and it is equal to Wrin interval n.The solution to the scheduling problem of stage 2 is represented as a set of partial schedules. Each partial schedule b executed in time interval k is determined by its durationΔbkand by the assignment of jobs to machinesvijkb(i=1,…,m, j=1,…,k), wherevijkb=1if job j is processed on machine i in partial schedule b of interval k, andvijkb=0, otherwise. The length of the last interval is equal to the sum of the durations of the partial schedules executed in this interval.The problem is formally defined as follows:(1)min∑b∈BnΔbn,(2)∑k=jn∑b∈Bk∑i=1mvijkbΔbkpij=1,j=1,…,n,(3)∑b∈BkΔbk⩽sk+1,k=1,…,n-1,(4)∑j=1kvijkb⩽1,i=1,…,m,k=1,…,n,b∈Bk,(5)∑i=1mvijkb⩽1,j=1,…,n,k=j,…,n,b∈Bk,(6)∑j=1k∑i=1mβijrvijkb⩽Wr-αk+1,r,r=1,…,l,k=1,…,n-1,b∈Bk,(7)∑j=1n∑i=1mβijrvijnb⩽Wr,r=1,…,l,b∈Bn,(8)vijkb∈{0,1},i=1,…m,j=1,…,n,k=j,…,n,b∈Bk,(9)Δbk⩾0,k=1,…,n,b∈Bk,whereΔbkandvijkbare decision variables, Bkis the set of indices of all possible partial schedules in interval k. Constraints (2) ensure that each job finishes its processing. Constraints (3) restrict the sum of the partial schedule durations in each interval except the last one. Constraints (4) and (5) ensure that in each partial schedule, respectively each machine works on at most one job at a time and each job is processed on no more than one machine at a time. Due to constraints (6) and (7), in each partial schedule, the usage of each resource at every moment does not exceed its availability.The theoretical basis for the column generation technique has been provided by Dantzig and Wolfe (1960). Using a CG algorithm, we avoid the difficulty of explicitly generating all columns of the problem (the number of which is usually huge), by working with only a subset of the columns and adding new columns as needed (when they improve the solution). Such an approach was first suggested by Gilmore and Gomory (1961) for solving cutting stock problems.We propose the CG algorithm for solving problem (1)–(9). In this problem, constraints (2) are non-linear. The CG algorithm deals with this non-linearity in the following way. It divides the original problem (1)–(9) into two interlaced problems: a master linear problem and a sub-problem. These problems are solved alternately in consecutive iterations of the algorithm.The master LP problem has the following form.(10)min∑b∈QnΔbn,(11)∑k=jn∑b∈Qk∑i=1mvijkbΔbkpij=1,j=1,…,n,(12)∑b∈QkΔbk⩽sk+1,k=1,…,n-1,(13)Δbk⩾0,k=1,…,n,b∈Qk,where Qkis a subset of set Bk, which contains the indices of the already generated (in previous iterations) columns in interval k (column b determines the assignment of jobs to machines in partial schedule b),Δbk(k=1,…,n, b∊Qk) are decision variables.The CG algorithm starts from the initial set of n columns belonging to the last time interval. Each of these columns is assumed to contain exactly one job (different in different columns) assigned to the machine on which its processing time has the smallest value.In the first iteration, the master problem is solved with the set of n columns determined by the initial solution. Then, the sub-problem uses the dual information drawn from the solution of the master problem and generates a set of new columns. Adding these columns to the master problem improves its solution in the next iteration. Then, the improved solution of the master problem again provides the dual information for the sub-problem, and so on, until no column improving the solution of the master problem can be found.New columns are generated in successive iterations in the following way.Let πj(j=1,…,n) and φk(k=1,…,n−1) be the dual variables corresponding to constraints (11) and (12), respectively, andπj∗andφk∗denote the optimal solution to the dual problem to problem (10)–(13). The new column generated in kth interval is determined by the values ofvijk, wherevijk=1if job j is assigned to machine i in kth interval, and 0 otherwise. To improve the value of the objective function (10), this column has to satisfy one of the following inequalities:(14)∑j=1kπj∗∑i=1mvijkpij+φk∗>0,ifk=1,…,n-1,or(15)∑j=1kπj∗∑i=1mvijkpij-1>0,ifk=n.In each iteration of our CG algorithm, we look for a set of n new columns (one column for each time interval), by solving, for each k (k=1,…,n), the maximization problem (the sub-problem) with the following objective function(16)zk=∑j=1kπj∗∑i=1mvijkpij,and with the constraints (4)–(8) (without index b). If the column just generated for interval k satisfies inequality (14) or (15), a new index b (b∈Bk⧹Qk) is given to that column and set Qkis extended by this new index b. If at least one column satisfying conditions (14) or (15) has been found, a new iteration of the CG algorithm begins, otherwise the CG algorithm stops.For solving the above maximization problem, we propose the TS algorithm described in the following section.A TS algorithm is an iterative approach proposed by Glover (1989) which starts from an initial solution, and in each iteration moves to the best solution in the neighborhood of the best solution found previously. To avoid cycling some moves are forbidden (tabu) for a certain number of iterations.In this paper, a TS algorithm is used in each iteration of the CG algorithm. It consists of n search processes, kth (k=1,…,n) search process (denoted by ts(k)) aims at finding a column for kth interval, which improves the current solution (makespan) obtained by the CG algorithm.In our implementation, a solution in a single search process ts(k) is represented by sequence Akof m indices of jobs taken from set {0,1,…,k}, where 0 is the index of a dummy job (with zero resource requirements). The position of a job index in Akdetermines the machine on which the job is processed. None of the job indices taken from {1,…,k} can be placed in more than one position in Ak. Index 0 may appear in many positions in Ak. Akdetermines the assignment of jobs to machines in a partial schedule in interval k.To evaluate a current solution Akwe calculate the value of zk, (Eq. (16)) by settingvijk=1, if Ak(i)=j (job j is on the ith position in sequence Ak), andvijk=0, otherwise. The solution Akobtained by ts(k) improves the CG algorithm solution, if for k≠nzk+ϕk∗>0, and for k=n zk−1>0 (see Eqs. (14)–(16)).A neighborhood of solution Akis defined as a subset, P(Ak), of the set of all the neighbors of this solution. The size of the neighborhood was chosen to be equal to⌊k/10+1⌋. A neighborAk′of Akis created in the following way. Akis copied toAk′, the position i is randomly chosen and the index of a job is erased from this position inAk′. The new job index j is randomly selected from {1,…,k} and inserted in position i inAk′, if the following conditions are satisfied: job j is not in any position in Ak, a move leading from AktoAk′is not tabu, andAk′with job j in position i represents a feasible assignment of jobs to machines in interval k. If job j cannot be placed in position i, another job is considered, and so on until all jobs (available in interval k) have been considered or a suitable job has been found. If no suitable job exists, the dummy job 0 is inserted in position i.A move leading from solution AktoAk′(i.e. placing job j in position i) becomes forbidden and is added to the tabu list Tk(of size equal to 7) ifAk′is the best solution in P(Ak). A move inserting a dummy job in a selected position is not remembered in the tabu list.The search process ts(k) (k=1,…,n) terminates after a predetermined number, Nk, of solutions visited without any improvement of the best solution found so far, Nk=ηk,where η is the parameter whose value was experimentally chosen to be equal to 10.Let: solvisitedkbe the number of solutions visited by ts(k) without improvement of its best solution, finishedkbe equal to 1 if ts(k) is finished and 0 otherwise, cgsimprkbe equal to 1 if ts(k) process improves the CG algorithm solution and 0 otherwise.The TS algorithm proceeds as follows.1:For k=1,…,n do2:Initialize an empty tabu list Tk;3:Set finishedk=0, cgsimprk=0 and solvisitedk=0;4:Get the initial solution Akand setAkbest=Ak;5:Repeat6:Generate and evaluate P(Ak), update the value ofsolvisitedk;7:Choose from P(Ak) the best solutionAk,best′;8:IfAk,best′is better thanAkbestthen setAkbest=Ak,best′and solvisitedk=0;9:Replace AkbyAk,best′and update the tabu list Tk;10:If solvisitedk⩾Nkthen11:IfAkbestimproves the CG algorithm solution then set cgsimprk=1;12:Set finishedk=1;13:End if;14:Until finishedk=1;15:End for;16:Return setB={Akbest:cgsimprk=1,k=1,…,n}, containing the solutions, which become new columns of the CG algorithm.The initial solutions for TS processes are created by a greedy procedure which proceeds as follows. For each pair (i,j), i.e. (machine, job), its weight, denoted by Gij, is calculated. We consider two different ways of calculating weights Gij. The first one is given by the following expression derived from Eq. (16) directly:(17)Gij=πj∗pij.The second definition includes resource requirements at stage 2 and is given as follows:(18)Gij=lπj∗pij∑r=1lβijrWr.After determining weights Gij(i=1,…,m, j=1,…,n) according to one of the above expressions, pairs (i,j) are placed in a list in a non-increasing order of Gij. Then, in every TS process ts(k), they are considered one by one, and job j is inserted into position i of sequence Akif the following conditions are satisfied: j⩽k, position i in sequence Akis free, job j is not assigned to any machine, and resource constraints after inserting job j into position i are not violated.The greedy procedure using weights given by (18) favors jobs with small resource requirements and due to this, usually provides solutions with more jobs assigned to machines than the procedure using weights defined by (17). In Section 5, we will show how the choice of the initial solution for the TS algorithm influences the performance of the heuristic.We use two lower bounds, LB1 and LB2, on the optimal makespan to evaluate the performance of the proposed heuristic algorithms.LB1, defined below, is straightforward.(19)LB1=∑j=1nsj+mini=1,…,mj=1,…,n{pij}.LB2, in which sharing of resources between the stages is taken into account, is defined as follows.(20)LB2=∑j=1nsj+T∗,where T∗ is a lower bound on the length of the last time interval (which begins after finishing all jobs at stage 1). T∗ is obtained by solving an LP problem at stage 2 in which resource constraints at every moment are replaced by resource constraints over the whole time intervals, and ready times of jobs are equal to 0. It is assumed that at stage 1 job j is processed in interval j, and so, job j cannot be processed at stage 2 in this interval. This problem can be formulated as follows.(21)minT,(22)∑k=1,…,n+1:k≠j∑i=1mtijkpij=1,j=1,…,n,(23)∑j=1,…,n:j≠ktijk⩽sk,i=1,…,m,k=1,…,n,(24)∑i=1mtijk⩽sk,j=1,…,n,k=1,…,n,j≠k,(25)∑j=1,…,n:j≠k∑i=1mβijrtijk⩽sk(Wr-αkr),r=1,…,l,k=1,…,n,(26)∑j=1ntijn+1⩽T,i=1,…,m,(27)∑i=1mtijn+1⩽T,j=1,…,n,(28)∑j=1n∑i=1mβijrtijn+1⩽TWr,r=1,…,l,(29)tijk⩾0,i=1,…,m,j=1,…,n,k=1,…,n+1,j≠k,(30)T⩾0,where T andtijkare decision variables. T is the length of the last time interval (of index n+1),tijkis the processing time of job j on machine i in interval k, k=1,…,n+1. Constraints (22) ensure that each job is processed on all the machines for time sufficient for its completion. Constraints (23) and (24) say, respectively, that in each interval k, k=1,…,n, the working time of each machine does not exceed the length of this interval and the time of processing a job is not longer than the interval length. Constraints (25) ensure that the usage of each resource in interval k, k=1,…,n, does not exceed its availability over this interval, sk(Wr−αkr). Constraints (26)–(28) are similar to (23)–(25), respectively, but relate to the last interval.A lower bound on the optimal makespan in the considered two-stage flowshop is given by:(31)LB=max{LB1,LB2}.In this section, the results of a computational experiment conducted to evaluate the performance of the proposed heuristic are presented.As an effectiveness measure we used the percentage deviation of a heuristic solution from the lower bound on the optimal makespan (in other words the maximum percentage deviation from the optimal makespan) defined as(32)δ=Cmax-LBLB×100%,where Cmax is the makespan found by the heuristic algorithm, and LB is the lower bound on the optimal makespan defined by (31).The number of jobs was considered to be n=20, 40, 60 and 80, the number of machines, m, at stage 2 was set at 2, 4 and 6.The resource of one type was considered with three levels of its availability, W, equal to 10, 12 and 14. Resource requirements of jobs were generated from U[1,10] (U[a,b] denotes the discrete uniform distribution in the range of [a,b]).We considered two following schemes of generating the processing times of jobs.I.Job processing times are taken from uniform distributions U[1,100] and U[1,100m] at stage 1 and 2, respectively.Job processing times are drawn from Gaussian distributions with mean, μ, equal to 100 and 100m at stage 1 and 2, respectively, and standard deviation equal to 0.5μ. If the generated value was not greater than 0, it was set at 1.All programs for the algorithms presented in the paper were written in C++ and run on a PC with Celeron 1.3GHz processor. The LP problems were solved using lp_solve optimizer v.5.5 available from http://groups.yahoo.com/group/lp_solve.The results of the computational experiment are presented in Tables 3–8and in Fig. 2. All entries in Tables 3–5 and 8 and in Fig. 2 are average values over 10 problem instances.In Table 3, the results of the comparison of lower bounds are presented. Tables 4 and 5 present the average deviation, δ, for algorithms A1–A6, implementing, respectively, priority rules P1–P6 (presented in Table 1). Tables 6 and 7 present the results of statistical tests performed in order to analyze the observed differences among the algorithms. Table 8 contains computation times. Fig. 2 shows the convergence curves of the algorithm A5 with the use of tabu search starting from different initial solutions.In Table 3, we can see that LB2 is superior to LB1 for problems with strong resource constraints (W=10 and W=12). This is because LB2, in contrast to LB1, takes into account the fact that the resource availability at stage 2 is different in different time intervals.From Tables 4 and 5 we can make the following observations. For most of the examined problems, algorithm A5 provides better solutions than the other algorithms (the bold figures in the tables indicate the best results). The average (over all the examined instances) value of δ produced by A5 is equal to 0.63%. For the other algorithms A2, A6, A4 and A3 the average δ is equal to 0.74%, 0.78%, 1.53% and 1.77%, respectively. All the algorithms A2–A6 are significantly better than algorithm A1 with a random order of jobs at stage 1 which provides the average δ=7.47%.The superiority of A5 over the other algorithms is especially visible for the case of strong resource constraints (W=10). This can be explained as follows. Priority rule P5 used in A5 puts at stage 1 in the first time intervals jobs with great resource requirements at stage 1 and at stage 2. Thus, at stage 2, in the next time intervals, when a great number of jobs are ready for processing, the resource availability remains considerable and resource requirements of jobs are small, which allows jobs to be simultaneously processed on the available machines. In this way, most of the jobs are completed before the last interval begins, which consequently leads to short makespan in the flowshop.Algorithm A2, which uses for sequencing jobs at stage 1 the priority rule independent of job resource requirements, gives results similar to those of A5 for problems with weaker resource constraints (W=12, 14).Priority rule P6 used in algorithm A6 similarly to P5 favors jobs with great resource requirements, but it considers first resource requirements at stage 2 and then at stage 1. This approach usually performs worse than A5.The worst algorithms A3 and A4 use priority rules which place at stage 1 as first jobs with small resource requirements.In the tables, we observe that, with weakening the resource constraints (when W=12, 14) deviation δ decreases. This is because more jobs can be processed simultaneously in partial schedules at stage 2 and a small number of jobs are left to be processed at stage 2 in the intervals at the end of the schedule. On the contrary, if resource constraints are strong, there is a small chance of executing all available jobs in the partial schedules in the intervals preceding the last one.Regarding the job processing time generation schemes, we observe that the performance of the algorithms is a little worse for scheme II than for scheme I. This is because the values of processing times in scheme II are concentrated round the average value.An interesting fact noticeable for problems with strong resource constraints is that deviation δ of algorithms A2–A6 is much smaller for problems with m=4 and 6 than for problems with m=2. This follows from the fact that when the number of machines is greater, there is a greater chance of finding machines on which available jobs have sufficiently small resource requirements to be processed simultaneously, and thus most of them can be completed before the last time interval.Deviation δ decreases with increasing the number of jobs because, when the number of jobs is great, it is quite easy to choose jobs for simultaneous processing in the time intervals preceding the last one.In order to examine if there are any statistically significant differences between the developed algorithms, some additional tests were carried out using the Holm–Bonferroni procedure. The null hypothesis, which states that there is no difference between the performance of the reference algorithm Ajand the performance of the other algorithm Ai(j≠i) for a given set of problem instances, was tested against the alternative one, which says that algorithm Ajoutperforms Ai. Three sets of problem instances with different strengths of resource constraints were considered: PW10, PW12 and PW14 containing the problem instances with resource availability, W, set at 10, 12 and 14, respectively. The comparison of the algorithms proceeds as follows (Derrac, García, Molina, & Herrera, 2011; García, Fernández, & Luengo, 2009). First, for each P∊{PW10,PW12,PW14} the average ranks of the algorithms are determined. The average rank of algorithm Ajis given byRj=1NPI∑i=1NPIrij, where NPIis the number of problem instances in a set of problem instances (in our tests, NPI=240 for each set P (10 problem instances for each combination of n, m and the job processing time generation scheme)), andrijis the rank of algorithm Ajon the ith problem instance;rij=1, if Ajis the best performing algorithm,rij=2, if Ajis the second best algorithm, and so on. The average ranks of the algorithms are presented in Table 6. We can see that, for PW10 and PW12 algorithm A5 has the smallest value of the average rank, for PW14 the smallest average rank value is obtained for algorithm A2. Consequently, algorithms A5 and A2 are taken as the reference algorithms in the following calculations.For each algorithm Ai(except the reference one denoted by Aj), z-value is calculated from the following expression:zi=(Ri-Rj)NA(NA+1)6NPI,where NAis the number of algorithms, NA=6. The z-values are used to find the corresponding probabilities (p-values) from the table of normal distributions N(0,1). The algorithms are sorted in non-decreasing order of their p-values. The z-values along with p-values are presented in columns 4–5 of Table 7. Column 6 shows the values of ∝/(NA−h), where ∝ is the level of significance, ∝=0.05, and h is the position of an algorithm in the sequence determined by p-values (the values of h are indicated in column 3). The null hypothesis Hhsays that there is no difference in performance between the reference algorithm and the algorithm in position h in Table 7. According to the Holm–Bonferroni procedure (Holm, 1979), the null hypothesis H1 is rejected if p1<∝/(NA−1). If H1 is rejected, we are allowed to compare p2 with ∝/(NA−2). If H2 is rejected the test proceeds with H3, and so on. As soon as a certain null hypothesis cannot be rejected, all the remaining hypotheses are retained as well. The last column in Table 7 indicates whether the null hypothesis is rejected or not.From Table 7 we can see that in the set of problem instances with strong resource constraints, A5 statistically significantly outperforms all the other algorithms. In the set of instances with weaker resource constraints, algorithms A2, A5 and A6 have indistinguishable performances and perform better than A1, A3 and A4.Fig. 2 shows the values of δ provided by algorithm A5 in terms of the number of solutions visited by TS in all its search processes (let us recall that, in each iteration of the CG algorithm, the TS process is run n times for finding new columns in n time intervals). The results are presented for A5 using the TS algorithm with a randomly generated initial solution, and the TS algorithms with initial solutions provided by the greedy procedures with and without inclusion of job resource requirements (see Section 3.4.1).For the problems with more than 2 machines (Fig. 2) the algorithm with TS starting from a greedy solution with resource requirements inclusion is superior to the other algorithms in terms of both convergence speed and accuracy. The difference between the algorithms is more significant for problems with strong resource constraints (Fig. 2a) than for problems with weak resource constraints (Fig. 2b). The behavior of the algorithms presented in Fig. 2 is typical for the problems with m>2. For problems with 2 machines there is no difference in the performance between the three algorithms.Table 8 presents CPU times (in seconds) for the heuristic (using algorithm A5 as a representative because computation times of all the algorithms are very similar to each other). As expected, the computation times increase with the number of jobs and machines. They also increase with increasing the strength of resource constraints as problems become more difficult to solve in this case. The problems with job processing times generated according to scheme II need more CPU time than problems with job processing times from scheme I.In this paper, we proposed a heuristic which solves the problem of preemptive scheduling in the two-stage flowshop with one machine at stage 1 and parallel unrelated machines at stage 2, where availability of resources at any moment is limited and resources are shared among the stages, i.e. some quantities of the same resource can be used at both the stages at the same time. The objective was to minimize the makespan. The heuristic first sequences jobs at stage 1 and then solves the problem at stage 2.Several priority rules depending not only on processing times but also on resource requirements of jobs were proposed to order jobs at stage 1. The most effective of them (especially for difficult problems with strong resource constraints) appeared to be the one including resource requirements in the way that favors jobs with great resource requirements at both the stages.We proposed a column generation algorithm to solve the second stage problem in which jobs have non-zero ready times and resources are available in different quantities in different time intervals. Both ready times and availability of resources at any moment depend on the order of jobs at stage 1. The column generation algorithm uses, in successive iterations, linear programming to minimize the makespan, tabu search to create n (n is the number of jobs) new columns (a column determines an assignment of jobs to machines for simultaneous processing) improving the current solution, and a greedy procedure which provides an initial solution for the tabu search algorithm. Generation of n columns in each iteration is justified by the fact that there are n time intervals with different resource availability, and it allows us to obtain the final solution faster than by creating one new column. As it was shown experimentally the greedy procedure which favors jobs with small resource requirements improves the performance of the whole algorithm.We also derived a new lower bound taking into account the fact that the resources are shared among the stages and their availability is limited. This lower bound is the best for the problems with strong resource constraints.The extensive computational experiment has shown that the effectiveness of the heuristic proposed in this paper, measured by the percentage deviation of the makespan from the lower bound on the optimal makespan, is satisfactory.This paper extends the research on scheduling in the flowshop with parallel machines by including additional renewable resources which are shared among the stages. The further research should include developing methods for accelerating the column generation algorithm applied by the heuristic.

@&#CONCLUSIONS@&#
