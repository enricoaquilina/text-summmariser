@&#MAIN-TITLE@&#
A distributed storage framework of FlowTable in software defined network

@&#HIGHLIGHTS@&#
SDN-enabled network functions are limited by the switch TCAM capacity.We propose a distributed storage framework of FlowTable.Our framework can achieve high availability and failure resilience.There is a tradeoff between cost and high availability/failure resilience.We achieve the storage capacity without inviting large communication overhead.

@&#KEYPHRASES@&#
Openflow,SDN,FlowTable,TCAMs,Distributed storage framework,

@&#ABSTRACT@&#
Openflow, a novel Software Defined Network (SDN) technology, is developing rapidly and has already been utilized in many fields. It facilitates decoupling between the control and forwarding plane, enabling users to code the network functions easily and replace the traditional high-cost network functions devices. The FlowTable of Openflow, its base of operating the network packets, consists of many flow entries and is stored in the Ternary Content Addressable Memories (TCAMs) of the Openflow switch. When the FlowTable occupies the entire storage space of the Openflow switch and more flow entries are added, the delete operation on the TCAMs increases, and the latency and loss of packets deteriorates – this is the primary issue. In order to solve this problem, this paper proposes a distributed storage framework which stores the FlowTable in multiple Openflow switches, equipped with small TCAMs. To conclude, this paper simulates the algorithms used in the framework and builds a testbed. The experimental results prove the framework’s feasibility and successful performance.

@&#INTRODUCTION@&#
Openflow [1], a type of Software Defined Network (SDN) technology, is developing rapidly and is already quite widely-used. The campus network [1], data center network (DCN) [2] and inter-DCN Wide Area Network [3] are all built over an Openflow-enabled SDN. The control plane and forwarding plane [1] of the traditional network are decoupled in the SDN. All logical control is converged in the SDN controller, where the SDN switch simply forwards the packets as per the controller’s instruction. The whole overview of the SDN-enabled network makes the network easy to operate. In addition, the use of SDN technology can make network functions such as the Network Address Translation (NAT), firewall [4], gateway and load balancer [5], smarter, more secure and more manageable. In a traditional network, all the network functions require particular devices, some of which can be quite expensive. More pressing is the fact that failure of these devices severely affects the network’s ability to function. In some cases, the devices occasionally maintain hundreds of thousands of flows, but storage capacity limits the devices’ ability to maintain all of the flows.Before SDN-enabled network functions can really ground, certain problems must be solved [6], including availability and failure resistance. Openflow, one of the most common SDN technologies faces another problem: the storage limit of its Ternary Content Addressable Memory (TCAM). According to the specifications of Openflow [7], the Openflow controller adds FlowTable in the Openflow switch (OF switch). FlowTable is the base of operations for the OF switch to control the network packets. Additionally, the FlowTable consists of different flow entries which must be stored in the TCAMs. TCAM is an energy-consuming memory, however, and the storage capacity of an OF switch is limited. The most common OF switch can store thousands of flow entries. In Fig. 1, when the number of flows increases, the FlowTable will also become larger. Eventually, the OF switch will run out of storage space and begin deleting the entries in TCAMs. Network latency and packet loss will deteriorate at the same time. The network quality of services (QoS) also drops. To these effects, increasing storage capacity of TCAMs is the primary improvement necessary for the performance of Openflow-enabled network functions.Using big data storage architectures as a reference, plus an understanding of the flexibility of Openflow-enabled networks, this paper proposes a distributed storage framework of FlowTable. The framework not only solves the storage problem, but also the problems of flow entry deploy, storage load balance and network packet route. Under the user’s control, the framework can easily realize network functions and achieve a high availability and failure resilience, even though the overall cost of multiple OF switches may be higher than a single OF switch with big TCAMs. This tradeoff should be considered when users consider the proposed framework.In this paper, Section 2, firstly, describes the related work. In Section 3, we elaborate on the distributed storage framework of FlowTable in detail, including the method by which it ameliorates the design problem. In Section 4, we simulate the algorithms used in the framework. In Section 5, we build a testbed and form an evaluation which demonstrates the framework’s ability to provide large storage capacity while guaranteeing the performance. Conclusions, lastly, are drawn and explained in Section 6.Openflow’s specification recommends that FlowTable should be stored in the TCAM, because it has a high search speed and simple operation. Notably, though, it is highly energy-consuming. Many researchers have observed the importance of the TCAM to the OF switch, and proposed methods that improve the utilization of TCAM or reduce the FlowTable size.Paul Congdon et al. [8] realized that the heavy use of TCAM weakens the efficiency of the OF switch. To remedy this, they utilized packet prediction to speed flow classification and TCAM searching. They also made efforts to reduce FlowTable search latency and power consumption. Bedhiaf et al. [9] noted the high power consumption of TCAM, as well. They used a genetic algorithm and Tabu search to allocate the slice tables over the TCAM resources, in effort to save energy. Veeramani et al. [10] proposed a virtual compression approach to minimize the FlowTable for TCAM. All of this research, however, focused on one single OF switch, which does not resolve the FlowTable storage problem.As shown in Table 1, a flow entry contains several tuples, including match fields and actions, where the match fields can be considered match rules. In a flow entry, every match field ranges from 0 to the max value of the field. If every match field is considered a single dimension, all the match fields can form a hypercube, and each point of the hypercube can represent a match rule. Because the flow entries are similar to the match rules, it is possible to utilize certain compression methods to reduce the hyperspace size that the FlowTable occupies. Qi et al. [11], for example, used the HyperSplit algorithm to improve the memory resource utilization and packet-classify speed. Ganegedara et al. [12] proposed a novel modular Bit-Vector based architecture which eliminates the rule set expansion caused by range-to-prefix conversion. Additionally, Xu et al. [13] designed a hierarchical space-mapping algorithm, and Wang et al. [14] proposed a Hypercube Flow Table based on multi-dimensional hash tables. Fong et al. [15] also proposed a rule set partitioning algorithm based on range-point conversion. All this research divides the hypercube into several sub-hypercubes or builds decision trees [16] in every sub-hypercube. They use heuristic algorithms to delete the duplicate and merge any similar match rules, creating a single network device that can store more rules. In addition, because the actions are repetitive, Chiba et al. [17] replaces the action field of flow entries with a small number of bits to reduce the FlowTable size. However, regardless of the compressing ratio of the methods mentioned above, the storage space of an OF switch is always fixed, and the TCAM cannot store a large FlowTable. The limit of the storage capacity is still the primary problem.When the TCAM is exhausted, adding more TCAMs in a OF switch is another potential solution for the storage problem. More TCAMs mean more power consumption and heat dissipation, though. This stresses the cooling device and leads to a high probability of device failure. Furthermore, a single OF switch equipped with a large amount of TCAMs can lead to single point failure, affecting the network QoS severely and creating lengthy recovery time.When a single OF switch cannot store all FlowTable, a distribution method can be introduced. Yu et al. [18] defines two kinds of flow rules (flow entries), an authorized flow rule and normal flow rule; and two kinds of OF switches, an authorized switch and normal switch. The flow entries are prepared in advance and stored in one of the two kinds of OF switches. The normal switch is an ingress or egress of the network. When the normal switch cannot match a network packet, it demands the appropriate authorized switch for the valid flow entry and caches the flow entry. It is possible to increase the storage space by adding authorized switches. However, when a large number of network packets reach the normal switch, a high cache goes missing and the normal switch has to make frequent demands of the authorized switch. This results in an increase in network latency. Shimonishi et al. [19] utilized the storage capacity as a constraint to deploy flow entries. The storage capacity of OF switches can be used evenly, but when network packets converge to a single OF switch, a large number of flow entries overwhelms the storage capacity of the switch. At the same time, any other OF switches still have storage space available. As a result, the OF switch has to delete and add flow entries frequently, increasing network latency.If the flow entries are considered as file data with OF switches as storage nodes, traditional big data storage architectures such as Google File System (GFS) [20], Hadoop Distribute File System (HDFS) [21] and Dynamo [22] can be used as references to build a distributed storage framework of FlowTable.As we know, there are two main types of big data storage architectures: master–slave architecture, such as GFS and HDFS, and decentralized architecture, such as Dynamo. Master–slave architecture possesses two types of nodes, manage nodes and data nodes. The manage node has complicated functions, and manages all the data nodes. The data nodes’ functions are simple compared to the manage node. In decentralized architecture, all nodes are equal, aggregated through P2P technologies. And they use a hashing algorithm to complete major functions.The two types of architecture solve the problems of data item locate, storage load balance and data request route in different ways. In master–slave architecture, the deploy location of data items, the storage load balance and the data requests are all manipulated by the master. The slave nodes simply store the data and response to the data requests. In decentralized architecture, all the functions are completed in a collaborative way. It builds a distributed hashing table (DHT), stored in every node. According to the DHT, the storage nodes can calculate any data item’s location and route any data request to the appropriate node. Additionally, the storage load balance can be started by any node.The architecture of Openflow is similar to master–slave architecture. In a straightforward process, it splits the larger FlowTable and stores the flow entries in multiple OF switches equipped with small TCAMs. Failure of one OF switch thus affects the network function just slightly. Compared to a single OF switch with large TCAMs, multiple OF switches with small TCAMs reduce the probability of failure. Most importantly, the storage space limit problem is overcome by the addition of new switches. The overall cost of multiple OF switches may be greater than a single OF switch with big TCAMs, but the distributed method achieves high availability and failure resilience largely impossible for a single device. A single device is commonly used in research, but not in practice. The tradeoff between cost and high availability/failure resilience should be considered carefully. This paper proposes a distributed storage framework of FlowTable (DSFoF), and addresses details in the next section.When the FlowTable is stored in multiple OF switches, there are three main problems which must be addressed:(1)Flow entry deploy, which requires that any flow entry has an easily-identified deploy location. The controller can decide each flow entry’s deploy location using several methods, including round robin, weight factor and others. Additionally, the deploy method is the basis for load balance and network packet route. A well-designed deploy method can increase the efficiency of the storage load balance and speed up the packet route. When the storage spaces of OF switches are unbalanced, it should be easy for the controller to identify and balance the storage. When an OF switch cannot match a network packet, it should be easy to send the packet to the appropriate OF switch.A well-designed deploy method does not guarantee that the flow entries can be deployed in a balanced manner. When the storage load is imbalanced, the storage load balance must begin based on the deploy method. The redeploy of flow entries, however, affects the network packet route.Because the FlowTable is divided into multiple OF switches, some network packets are not matched at the ingress OF switch, causing the ingress OF switch to send the unmatched network packets to the controller. The controller takes the place of the router. However, there is a limit to the packet number that the controller can process per second. When the packet number increases, this creates a bottleneck at the controller. This necessitates the design of a feasible packet route method.In this paper, we propose the DSFoF, as shown in Fig. 2, which solves the three problems mentioned above. With the help of the controller, the cluster of OF switches is virtualized as a big switch which realizes network functions. Any OF switch can act as the ingress or egress of the cluster. The OF switches, except the ingress and egress, assist in storing the FlowTable and operate the packets. We also notice that the network topology of the DSFoF has a direct influence on the communication overhead and the number of routing flow entries. In order to improve the forwarding efficiency and reduce the routing flow entries, every ingress or egress OF switch must connect to the other OF switches directly.According to Openflow’s specifications, the OF switches are able to execute packet header modification, packet forwarding and other operations based on FlowTable. In addition, the controller can obtain the network view and the status of OF switches. To this effect, the feature can be used to solve problems in the DSFoF. In an Openflow-enabled network, the controller can manipulate all of the OF switches and deploy the flow entries. The OF switches can also modify and forward the network packets. The relationship between the controller and the OF switch is similar to roles found in master–slave architecture, and the design of DSFoF can reflect this.Flow entry deploy is the basis of both the storage load balance and the network packet route. Before storage load balance and network packet route, the flow entries must be stored in OF switches. The storage load balance also affects the network packet route, because after the load balances, the previous route rules become meaningless and require re-computing. When the DSFoF solves the three main problems, this relationship should be taken into consideration. The relationship between the three problems is shown in Fig. 3.Flow entry deploy requires that every flow entry has a deploy location and can be easily located. The controller determines the deploy location of flow entries by a variety of methods, such as round robin, weight factor and others. When the storage spaces of OF switches are unbalanced, one must redeploy the flow entries in a balanced way. Because the deploy method is the basis of the load balance, each flow entry should be easily locatable. The OF switch sends all unmatched network packets to the controller, and the controller routes the packet to the appropriate OF switch. If the number of unmatched packets is large and the flow entry deploy location is random, the controller’s bandwidth and capability limit the routing performance and increase network latency. To remedy this, the controller must use a feasible method to deploy the flow entry. The hashing algorithm is widely used in data storage architecture [20–22] and is an effective choice for DSFoF. When the hashing algorithm is introduced into DSFoF, it becomes possible to divide the fixed hashing value space into several subspaces, with each OF switch taking charge of a subspace, as show in Fig. 4. When a flow entry’s hashing value is located in one subspace, the flow entry is stored in the corresponding OF switch.While the deployment is conducted by the controller, the hashing algorithm affects the storage load balance and the packet route, requiring that a suitable hashing algorithm be chosen.All network packets can be classified into different flows, based on their matching rules (or flow entries). The match fields of a flow entry consist of the ingress port, source MAC address, destination MAC address, source IP address, destination IP address, source port number, destination port number and other relevant fields. For matching rules, accurate values and wildcards can be used. When more accurate values are used, finer grain for flow control is obtained. If a wildcard flow entry is replaced with an exact value, the grain of the flow entry becomes finer. A coarse-grain flow can be divided into several fine-grain flows, yet a coarse-grain flow covers more network packets than a fine-grain flow. When a certain number of packets pass through an OF switch, different grains of flow control can be adopted. If a coarse grain and less match fields are selected, the FlowTable occupies a small storage space, yet fine grain flow control becomes impractical. However, if a fine grain and more match fields are chosen, the FlowTable will occupy a large storage space, but more precise control of the network is possible.In an Openflow network, it should be possible to achieve traditional network functions using the OF switches, yet the functions of a traditional network are always fixed. Therefore, it is possible to find an appropriate control grain for every network’s function. When OF switches are used to take over certain functions, such as NAT, gateway and firewall, the extra matching fields’ bits of flow entry can be fixed. When the grain of flow control is fixed, it is possible to combine the extra match fields’ bits of a packet and obtain a value. A combination of the possible values form a value space. Every flow entry is assigned a value this way, without conflict. This method, then, can be used as a hashing algorithm. When the controller needs to deploy a flow entry, the flow entry can be hashed in this way and stored in a specific OF switch.For example, as shown in Table 2, the exact matching fields are the destination IP and the destination port. Through the proposed hashing method, we obtain the hashing value of the two flow entries.The specifications of Openflow insist that the OF switches must follow Openflow protocol. It does not require that the OF switch completes any complicated computation, such as hash and number comparison. In the above section, we used master–slave architecture as a reference to realize the deployment of flow entries. The controller can then refresh the storage information while deploying new flow entries, and easily monitor the storage load’s condition. When the storage load appears imbalanced, the controller can divide the flow entries and deploy them evenly across all the OF switches, as shown in Fig. 5.Due to frequent balancing’s effects on DSFoF performance whenever there are numerous flow entries and a large amount of network flows, the load condition is a crucial consideration. During the load balance process, the flow entries may have been deleted from the OF switch while the routing information is not refreshed. Consequently, all unmatched packets are sent to the controller. It is crucial that an appropriate time point is chosen to execute the load balance.There are two types of algorithm that calculate the condition of storage loads, the static threshold algorithm and the dynamic threshold algorithm. We assume the number of OF switches as N. Every OF switch stores up to S flow entries. At a specific time point, the nth OF switch storesSn(1⩽n⩽N)number of flow entries. When storage status changes, it is possible to compute the load condition and decide whether to begin load balancing.In the static threshold algorithm, value T is the threshold and the load condition L is calculated as follows:(1)L=Max(S-Sn)-Min(S-Sn)T(1⩽n⩽N)When L is greater than 1, storage load balancing begins. Eq. (1) demonstrates that a bigger T leads to increased storage load balance. In order to decrease the frequency, a feasible T according to the actual demand must be adopted.In the dynamic threshold algorithm, the threshold value changes alongside the load condition. When the load condition is light, the threshold will be large and the frequency of storage load balance is low. Conversely, the threshold will be small and the frequency high. Knowing this, the frequency of the load balance can be reduced while the storage load remains in an acceptable condition. In the DSFoF, we use the dynamic threshold algorithm. The threshold isMin(S-Sn)and L is calculated as follows:(2)L=Max(S-Sn)-Min(S-Sn)Min(S-Sn)(1⩽n⩽N)The value ofMin(S-Sn)is large when the storage space is large enough, yet it is small when the storage space nears its limit. When the value ofMin(S-Sn)is small, any new flow entry may trigger the storage load balance. In this case, one must add OF switches to the cluster.In the DSFoF, the controller stores all backups of flow entries. Before the load balance starts, the controller sequences the flow entries according to their hashing value, then the flow entries are divided into N parts on average and each OF switch stores a part of the flow entries. At the same time, this partition information is used as the DHT and the routing information is refreshed and stored in the OF switch. Because the OF switch is limited, one must represent the partition information using a feasible method. We address this method in next section.If network packets cannot be matched at the ingress, they will be sent to the controller. The controller acts as a router, sending the packets to their appropriate OF switch. There is a limit, however, to the number of packets the controller can process per second. If each OF switch is aware of the FlowTable’s partition information, they easily forward the unmatched packets without the help of the controller. This solves the route problem.In order to deploy the flow entry, the hashing algorithm is used. We built a DHT for routing information, similar to Dynamo. Every OF switch uses the DHT to route the packets, but the OF switch cannot store anything besides flow entries. Therefore, the DHT is calculated by the controller and represented by flow entries. The OF switch is not only capable of exact matching but also fuzzy matching, yet a fuzzy flow entry can be replaced by a set of exact flow entries. We used fuzzy flow entries to represent the hashing value ranges.If the hashing value of a fuzzy flow entry is 1∗∗∗∗∗∗∗ (8bits), the flow entry covers the value range [128, 255]. Supposing the exact match bit is added from left to right, when the fuzzy flow entry adds an exact match bit, it can be divided into two fuzzy flow entries. The two fuzzy flow entries are then 10∗∗∗∗∗∗ (8bits) and 11∗∗∗∗∗∗ (8bits). The covered ranges are [129, 191] and [192, 255]. Three of these flow entries are shown in Table 3. By this method, the hashing value space can be represented by fuzzy flow entries. While the fuzzy flow entries can be stored in the OF switch, we can use the fuzzy flow entries to compute the routing information.In order to translate the partition information to route flow entries, a translation method must be designed. This assumes there are N OF switches and the number of flow entries that each OF switch can store is S. At a given point in time, the nth OF switch storesSn(1⩽n⩽N)flow entries and the architecture starts balancing the load. After the load balancing, the number of flow entries stored in every OF switch is:(3)Savg=∑n=1NSnNIn the translation method, the controller first sorts all flow entries by their hashing value in a descending order. Then the controller computes the partition information as the following pseudo-code in Table 4.Using this pseudo code, two factors emerge that affect the running time of the partition algorithm, the number of OF switches and the number of flow entries. Thus, the time complexity is:(4)O1(N)∗O2(N)=O(N2)The process of this algorithm resembles the growth of a binary tree. The hashing value space is the root node, and all the hashing value ranges are the leaf nodes. Additionally, each node is able to represent a fuzzy flow entry and cover a hashing value range. Every node contains a certain amount of flow entries. In this binary tree, a left node’s range is always less than the right node’s range of the same root node, as shown in Fig. 6. The algorithm uses a list to save all the leaf nodes in order, and counts the numbered flow entries covered by the leftmost node. The process is as follows:1.When the number is less than the target value (the target value is a multiple ofSavg), one must count the number of flow entries less than the right node’s range and start again.When the number is equal to the target value, record the position of the node in the node list and continue to search for the next target value’s node position.When the number is greater than the target value, divide the node in half and begin again. Repeat the process from 1. until the hashing value space is divided into N parts.In order to improve the temporal complexity of the partition algorithm, one can reverse the algorithm’s computing order. As shown in Fig. 6, one can first divide every binary tree node in the same level simultaneously. In addition, the initial leaf nodes’ number can be2X(X is the partitions’ number,) which speeds up the second step. If one uses a multi-thread to implement this step, the computing time can be further reduced. The temporal complexity of this step still isO(N). In the second step, the number of flow entries is counted for every leaf the node covers, and the flow entries are partitioned as an average. If Binary-Search is used, the temporal complexity isO(log2N). Further, if the first partition cannot be obtained, the second step is unnecessary. This speeds the second step considerably. One then obtains the temporal complexity as:(5)O1(N)∗O2(log2N)=O(N∗log2N)The change of the algorithm will lead an increase in the number of routing flow entries, because every leaf node is divided. So when the improved algorithm ends, some routing flow entries must be aggregated in the same partition. The temporal complexity is thenO(N). This increases the algorithm’s temporal complexity.(6)O1(N)∗O2(log2N)+O3(N)=O(N∗log2N+N)In the DSFoF, we must consider another problem: the network topology. In order to achieve the high availability/failure resilience, the DSFoF will incur inevitable communication overhead and extra hardware costs, and the topology has a direct influence on them. Before the network packet arrives the destination switch, it will pass through some other switches, and this will incur the overhead and costs. The communication overhead and extra hardware costs will be different with the changes of topologies. The overhead and costs will affect the throughput of the DFSoF, thus we must choose a feasible topology.There are three kinds of traditional topologies: (1) star; (2) ring; (3) mesh. If we have eight nodes, we can get some topologies as shown in Fig. 7. The two blue nodes are ingress and egress of DFSoF. Ring’s variant K means every node on the ring connects2K(K=1, 2, …) adjacent switches.If the numbers of flow entries processed by every node are even, we can get the excepted number of the nodes that a network packet passes through. The result is shown in Fig. 8.The more connections an OF switch has, the less communication overhead and extra hardware costs the DFSoF incurs. We can see the full mesh or approximate full mesh topology is the best choice. It can reduce the overhead and cost mostly. Although the full mesh topology will limit the storage capacity of the DFSoF, we can still ensure the whole storage capacity is bigger than a single OF switch. For example, we have many 24-ports OF switches and use full mesh topology, and every OF switch can store 1000 flow entries, thus we can achieve the maximum storage capacity, 24,000 flow entries, by using 24 OF switches. When the hardware resource is sufficient, we can reduce the connections among the OF switches, and achieve more scalability of storage capacity.In order to test the way differing load balance trigger conditions affect balance frequency, both dynamic and static algorithms were simulated. Assuming that the number of OF switches is 10, each of the OF switches can store 100 flow entries. When one OF switch runs out of storage space, the load condition is at its largest, positive infinity. In order to strengthen the figure view, the largest load condition is 5.The dynamic threshold algorithm and static threshold algorithm with thresholds of 5 and 10 were simulated. The results are shown in Figs. 9 and 10.The results show that the static threshold algorithm with a bigger threshold has a lower balance frequency before the architecture uses 90% of the storage space. Conversely, the dynamic threshold algorithm has the lowest balance frequency. As seen in the load condition results shown in Fig. 9, the balance trigger time results are obtained, shown in Fig. 10. The first storage balance of the dynamic threshold algorithm begins later than the static algorithm. When the storage usage reaches 94%, the storage balance at static threshold 10 stops. When the storage usage reaches 96%, the storage balance at static threshold 5 stops. The storage balance of the dynamic threshold algorithm still functions, however, until the storage usage reaches 98%. This demonstrates the favorable performance of the dynamic threshold algorithm over the static threshold algorithm.In order to compute the routing of the flow entries, we designed a partition algorithm with a temporal complexity ofO(N2)(4) and an improved algorithm with a temporal complexity ofO(N∗log2N+N)(6). The number of OF switches and the number of total flow entries both affect the running time of the proposed partition algorithm. In this section, we prove our inference through simulation. At the same time, we evaluate the degree to which the number of OF switches and the number of total flow entries affect the number of routing flow entries.In the simulation, it is assumed that the OF switch needs to match the IPv4 destination address (32bits) and TCP destination port number (16bits). The hashing value space is thus fixed as [0, 248−1].First, we assume the number of total flow entries is 10,000,000 and the number of OF switches changes from 1 to 20. These results are as shown in Figs. 11 and 12. TheR2is the coefficient of determination of the observed data [23]. WhenR2is close to 1, it indicates the correlation of the data is high.There is a high correlation between the number of OF switches and the original algorithm running time, as theR2reaches 0.9879. In addition, there is a high correlation between the number of OF switches and the number of routing flow entries, as theR2reaches 0.9968. Because the distribution of the flow entries is not strictly even, the results are not smooth and theR2cannot reach 1. We also found the improved algorithm significantly refines the numbers of routing flow entries, where the two algorithms are almost the same.Second, we assume the number of OF switches is 10 and the number of total flow entries changes from 500,000 to 12,500,000. We obtained the results shown in Figs. 13 and 14.There is also a high correlation between the number of flow entries and the original algorithm running time, as theR2of the observed data reaches 0.9894. The number of flow entries has a slight effect on the number of routing flow entries, however. When the number of flow entries ranges from 500,000 to 1,250,000, the number of routing flow entries fluctuates between 155 and 205. Because the distribution of the flow entries is not strictly even, there are some errors in the results. Notably, the novel algorithm demonstrates a significant improvement where the number of routing flow entries of the two algorithms are almost the same.The number of OF switches and the number of flow entries both affect the original algorithm’s running time. This proves that the temporal complexity of the original partition algorithm isO(N2)(4). Because the improved algorithm uses certain heuristic methods to improve the algorithm performance, the test result is more favorable than the temporal complexityO(N∗log2N+N)(6).In order to test the relationship between the exact matching length and the original algorithm running time, we fix the number of flow entries at 10,000,000 and the number of OF switches at 5. We increase the exact match length from 1 to 32. We obtained the results as shown in Fig. 15.When the hashing space covers all of the flow entries (the exact matching field bits are above 24), the original algorithm running time is about 1010ms and the improved algorithm is about 190ms. The results in Fig. 14 demonstrate that when the number of OF switches is 5 and the number of flow entries is 10,000,000, the original algorithm running time is about 1000ms and the improved algorithm running time is about 200ms. The results of Figs. 14 and 15 are quite close. When the hashing value space fails to cover all of the flow entries, the original algorithm can predict this, so the running time is short. When the hashing value space covers all of the flow entries, the running time is stable and more exact matching fields have a slight effect on it. When the hashing value space does not cover all of the flow entries, it takes some time before the improved algorithm recognizes this situation. When the exact matching field bits are less than 21, the performance of the improved algorithm is less favorable than the original algorithm. This becomes insignificant, however, when the exact matching field bits are less than 24, because neither algorithm produces a correct result.Based on the simulation results, we can ensure the temporal complexity of the original partition algorithm asO(N2)when the hashing value space covers the number of flow entries. The number of OF switches and the number of flow entries both affect the running time. The more OF switches, the more routing flow entries. The number of flow entries has a slight effect on the number of routing flow entries. The improved algorithm performed better to this effect than the original algorithm.In order to evaluate the proposed architecture in a real environment, we built a testbed to perform a series of experiments. In the testbed, the match fields of flow entries contains the source IPv4 address. Then the hashing value space of the flow entries is [0, 232−1]. A Linux server with OpenSwitch is used as the OF switch, where every OF switch can store a maximum of 500 flow entries. The number of the OF switches is fixed to 4. These 4 OF switches are connected as shown in Fig. 16. The dark OF switches are the ingress and egress.PWhen the packets are forwarded from ingress to egress, we do not use the destination IPv4 address as the normal IP routing. Instead, we use the source IPv4 address, so we can distinguish the different flows from ingress, making it easier to control the OF network.First, we start a webserver at the egress and test the webserver response time. A computer is used to periodically send requests with various source IPv4 addresses to the webserver at the ingress, while the webserver has a fix destination IPv4 address, 10.0.0.1. The ingress computer changes the source IPv4 addresses increasingly from 1.0.0.2 to 1.0.7.200. We will send all the requests twice time. This produces two types of round-trip time (RTT), the RTT of the first time, and the RTT of the second time. The results are shown in Fig. 17. We also ascertain the relationship between the number of routing flow entries and the number of flow entries, as shown in Fig. 18, and the relationship between the load balance and the number of flow entries as shown in Fig. 19.We reform our testbed and use a 6% storage space as the cache. Thus, the testbed can work as DIFANE [18] under the controller. When the storage space depletes and the cache fails, the OF switches fail to match some of the flows and ask for appropriate flow entries. We obtained the RTTs as shown in Fig. 20. Comparing the results to Fig. 17, it is clear that the use of DIFANE will affect the forwarding efficiency, which creates a high probability of cache failure.We also use one single OF switch to test the RTTs, and we get the results as shown in Fig. 21. Because the DFSoF will deploy more flow entries and the controller will operate on more OF switches’ TCAMs, the first RTTs of DFSoF is slight bigger than one single OF switch. Furthermore, all flows are routed in the data plane and the speed of date plane is faster than the controller plane. When the number of OF switches that a flow passes through is small, the latency will be small too. As a result, when comparing the results to Fig. 17, the first RTTs of DFSoF does not increase obviously and both of the second RTTs are about 0.01s. If we can keep flows in data plane as much as possible, we can achieve the large storage capacity with slightly communication overhead.As shown in Fig. 17, the average RTT is about 1s due to the deploy time of the flow entries. While the flow entries have been already stored in the OF switches, the average RTT is about 10ms. Eventually, the number of the routing flow entries reaches 63. The number of flows that the testbed supports is 1748. According to Fig. 17, the OF switches load balance does not heavily influence on the RTTs, because calculating the routing flow entries and deploy flow entries takes only a short time. And the number of flow entries deployed at one time has slight influence on the deploy time.We compare the number of flows that the DSFoF supports to DIFANE, the architecture in [19], and a single switch. We change the testbed to a mesh topology. We then use different numbers of ingress switches, and obtain the result as shown in Fig.22. Regardless of ingress switch number, the number of flows in the DSFoF supports is about 1750, the DIFANE supports is about 1800, and the single switch supports is about 500. With an increase in the number of ingress switches, the number of flows that the architecture in [19] supports increases. When the number of ingress switches is 4, the number of flows is about 1623. Because the routing flow entries required by DIFANE is lower than DSFoF, the number of flows supported by DIFANE is larger than DSFoF. When storage space is depleting, the RTT of DIFANE is longer than DSFoF. Because the architecture in [19] does not aggregate the routing flow entries, the ingress switch number becomes the upper limit of the number of flows that it supports, requiring more routing flow entries to route the flow to the egress. Our architecture fully utilizes the entire storage space while maintaining a reasonable RTT.In this paper, we proposed a distributed storage framework of FlowTable, DSFoF, which solves the storage problems of OF switches. The framework also provides favorable availability and failure resilience. The overall cost of the DSFoF may be higher than a single OF switch with big TCAMs, but a single OF switch does not achieve availability or failure resilience when failure of the single OF switch occurs. In the DSFoF, three problems are solved: flow entry deployment, storage load balance, and network packet routing. We use a modified hashing algorithm to determine the deployment location of a flow entry, where the controller collects the storage status of the OF switches and decides whether to begin the load balance or not. After the load balance, we use a partition algorithm to compute fuzzy flow entries to refresh the ranges and routing information. In order to reduce the inevitable communication overhead, we recommend to use the full mesh topology. This architecture does possess notable disadvantages, however. When the numbers of OF switches and flow entries are large, the computing time of the routing information increases, yet the temporal complexity of the original partition algorithm remainsO(N2). This affects the network latency. To this effect, we improved the algorithm and achieved more favorable temporal complexity-O(N∗log2N+N)and performance. A method for building the real networking functions on the DSFoF is forthcoming.

@&#CONCLUSIONS@&#
