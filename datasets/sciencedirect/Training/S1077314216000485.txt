@&#MAIN-TITLE@&#
Robust object tracking by online Fisher discrimination boosting feature selection

@&#HIGHLIGHTS@&#
A novel online Fisher discrimination boosting feature selection method is proposed for tracking.A particle filtering framework with the context information around the particles is exploited to enhance the robustness of tracking.It outperforms 29 representative algorithms in the CVPR2013 tracking benchmark.

@&#KEYPHRASES@&#
Visual tracking,Particle filter,Boosting,Fisher discrimination,

@&#ABSTRACT@&#
Large appearance changes in visual tracking affect the tracking performance severely. To address this challenge, in this paper we develop an effective appearance model with the highly discriminative features. We propose an online Fisher discrimination boosting feature selection mechanism, which selects features that reduce the with-in scatter while enlarging the between-class scatter, thereby enhancing the discriminative capability between the target and background. Moreover, we utilize a particle filtering framework for visual tracking, in which the weights of candidate particles take into account the context information around the particles, thereby enhancing the robustness of tracking. In order to increase efficiency, a coarse-to-fine search strategy is exploited to efficiently and accurately locate the target. Extensive experiments on the CVPR2013 tracking benchmark demonstrate the competitive performance of our algorithm over other representative algorithms in terms of accuracy and robustness.

@&#INTRODUCTION@&#
Object tracking is a fundamental problem in the field of computer vision with a wide range of applications, such as surveillance, human computer interaction, and medical assistance [1]. Although much progress has been made in recent years, it remains a challenging task due to large variations in object appearance caused by illumination changes, partial occlusion, deformation and so on. To well cope with these factors, it is of great importance to design an effective and efficient appearance model.In general, the traditional tracking algorithms can be categorized into two categories based on their appearance models: generative trackers and discriminative ones. Generative trackers learn an appearance model with the target appearance information, and search for the most similar candidate in each frame with the minimum reconstruction error. In [2], Ross et al. present a tracking method that incrementally learns a low-dimensional subspace representation, which efficiently updates online to adapt the target appearance changes. In [3], tracking is modeled as a sparse approximation problem [4–7], in which a set of trivial templates are introduced to address the challenging issues like particle occlusion, pose variation, etc. Kwon and Lee [8] combine several observation and motion models in a particle filtering framework to handle large scale appearance and motion variations. Zhang et al. [9] model the tracking as a multi-task learning problem. Wang et al. [10] propose a generative tracking method based on a novel robust linear regression algorithm, which models the error term with the Gaussian–Laplacian distribution. Discriminative trackers localize the object by learning a decision boundary between the target and the background. Grabner et al [11]. propose an online boosting feature selection approach to select features for visual tracking. In [12], Grabner et al. further introduce an online semi-supervised boosting method into visual tracking, which alleviates the drifting problem in visual tracking. Hare et al. [13] employ an online kernelized structured output support vector machine (SVM) for visual tracking, which achieves favorable performance on the CVPR2013 tracking benchmark [14]. In [15], Babenko et al. formulate object tracking as a multiple instance learning (MIL) problem and proposes an online MIL boosting algorithm, which selects features to design an appearance model. Zhang and Song [16] further propose a weighted MIL tracker by considering instance importance. In [17,18] the random Haar-like features that satisfy the compressive sensing theory is exploited as the object representation, which can achieve real-time tracking performance. In [19], Henriques et al. propose a fast tracking algorithm which exploits the well-established theory of circulant matrices, and hence provides a link to Fast Fourier Transform, which achieves competitive performance with fast speed. In [20] Henriques et al. formulate kernelized correlation filters (KCF) with circulant matrices, and efficiently incorporated multi-channel features in a Fourier domain.The Haar-like features [21] are widely explored in the discriminative trackers [11–13,15,17] due to its simplicity and efficiency. However, most of these trackers cannot achieve competitive performance in the recently proposed CVPR2013 tracking benchmark [14] except for the Struck algorithm [13]. We argue that the potential power of the Haar-like features has not been fully exploited. In this paper, to enhance the discriminative capability of Haar-like features for visual tracking, we propose an online Fisher discrimination boosting feature selection method, which selects features that have small within-class scatter but big between-class scatter. Then we utilize a coarse-to-fine search strategy in a particle filtering framework to locate the target. Extensive evaluations on the CVPR2013 tracking benchmark [14] demonstrate its favorable performance compared with the representative trackers. In summary, the main contributions are summarized as follows:•We propose an online Fisher discrimination boosting feature selection method, which selects the Haar-like features that can well discriminate target from background.Our tracker achieves competitive results in the CVPR2013 tracking benchmark [14] with 50.2% in success plots and 72.0% in precision plots, showing the effectiveness of the proposed feature selection mechanism.Fig. 1illustrates the basic flow of our tracking algorithm and the main steps are summarized in Algorithm 1. Firstly, we construct a set of positive and negative feature template bags{Bi+,Bi−}i=1c,in which each bagBi={zij}j=1ncontains n rectangle feature templates, of which each templatezijrepresents a vectorized image patch inside the blue rectangle, and then we select k templates via an online Fisher discrimination boosting feature selection strategy, which constructs the feature template bagsTi⊂Bi. Secondly, a coarse-to-fine search mechanism is adopted to find the maximum classifier response location via the particle filtering method, in which the background context information is taken into account to reweight particles. Finally, the feature templates and classifier are updated according to object appearance variances. When the confidence of the classifier is lower than a threshold Θ, which means that the estimation is not credible, both the feature templates and classifier stop updating to avoid drift problem.We employ the simple and effective Haar-like features to describe the appearance model in our tracking system. Although the Haar-like features have been widely applied to visual tracking [11,13,15,17,22], most of them cannot achieve favorable performance in the CVPR2013 tracking benchmark [14] because the fixed Haar-like feature templates are used, thereby limiting their discriminative capabilities. To address this problem, we propose an effective appearance model that is constructed by the adaptive Haar-like feature templates that are selected by the Fisher discrimination boosting criterion.As illustrated by Fig. 2, to take into account the proper granularities of features, we first constrain the width and height of the feature templates by2<twi<round(w/2),2<thi<round(h/2). Then, we take into account the mutliscale appearance information by setting the templates in the same bag to the same size while the templates in different bags own varying sizes. Furthermore, to make the features more diverse, we constrain the locations of the every line rectangles by a Flock rule [23]:(1)dmin<|l(zij)−l(zim)|,∀j,m∈{1,2,…,n},∀i∈{1,2,…c},dmax>|l(zij)−l(zij)¯|,∀j∈{1,2,…,n}.wherel(zij)¯=1n∑j=1nl(zij),and|l(zij)−l(zim)|represents the Euclidean distance betweenl(zij)andl(zim). dminand dmaxare parameters that constrain distributions of rectangles so that they cover a certain area without being too converged or too scattered, thereby increasing the diversities of templates.Next, we introduce the proposed Fisher discrimination boosting criterion. As illustrated by Fig. 2, providing the positive and negative feature template bagsBi+,Bi−,i=1,…,c,we define an objective function that is the sum of the Fisher discrimination criterion for each bag(2)F=∑i=1cFi,whereFiis defined as(3)Fi=|zi+¯−zi−¯|22−λ1∑j=1n|zij+¯−zi+¯|22−λ2∑j=1n|zij−¯−zi−¯|22,which is the Fisher discrimination criterion [24], in which the left term denotes between-class difference while the right two terms denote the within-class variances.zij+¯andzij−¯in (3) denote the jth normalized feature templates in the ith bag of the positive and negative samples, respectively, andzil¯=1n∑j=1nzijl¯,l=+,−.We expand each item in (3) as follows:(4)|zi+¯−zi−¯|22=zi+¯⊤zi+¯−zi+¯⊤zi−¯−zi−¯⊤zi+¯+zi−¯⊤zi−¯=2n−2∑j=1nzij+¯⊤zij−¯(5)λ1∑j=1n|zij+¯−zi+¯|22=λ1∑j=1n(zij+¯⊤zij+¯−zij+¯⊤zi+¯−zi+¯⊤zij+¯+zi+¯⊤zi+¯)=λ1(∑j=1nzij+¯⊤zij+¯−2∑j=1nzij+¯⊤zi+¯+nzi+¯⊤zi+¯)=λ1n−λ1n∑j=1nzij+¯⊤∑j=1nzij+¯(6)λ2∑j=1n|zij−¯−zi−¯|22=λ2n−λ2n∑j=1nzij−¯⊤∑j=1nzij−¯It is easy to verify thatzij+¯⊤zij−¯≤zij+¯⊤∑j=1nzij−¯,so by putting (4)–(6) into (3), we have(7)Fi=2n−2∑j=1nzij+¯⊤zij−¯−λ1n−λ1n∑j=1nzij+¯⊤∑j=1nzij+¯−λ2n−λ2n∑j=1nzij−¯⊤∑j=1nzij−¯≥2n−2∑j=1nzij+¯⊤∑j=1nzij−¯−λ1n−λ1n∑j=1nzij+¯⊤∑j=1nzij+¯−λ2n−λ2n∑j=1nzij−¯⊤∑j=1nzij−¯=J(zi1¯+,⋯,+zin¯)whereJis the lower bound of the functionFi. For each sample p, its image representation in the ith bag isBi(p)={zij(p)}j=1n,and we utilize the template center bag to robustly represent each class asBi¯={zij¯}j=1n(see Fig. 2). Our objective is to select a subsect of feature templates{zij¯}j=1kfrom bagBi¯that maximizes the Fisher discrimination criterion functionFi,which can be readily achieved by maximizing its lower boundJ.(8){zi1¯,…,zik¯}=argmax{zi1¯,…,zik¯}⊂Bi¯J(zi1¯+,⋯,+zik¯)The vector boosting algorithm in [25] relies on the special case of the exponential loss function of AdaBoost, and thus cannot be readily adapted to solve the above problem. Now, we present the proposed novel online vector boosting algorithm that can readily address the above problem. Our method is motivated by the algorithm in [26] that takes the statistical view of boosting, which tries to optimize a specific objective functionLby sequentially optimizing the following criterion:(9)(hj,αj)=argmaxhj∈H,αL(Hj−1+αhj),whereHj−1(p):Ω→Ris a strong classifier that is the sum of the firstj−1weak classifiershi,i=1,…,j−1andHis the set of all possible weak classifiers.The proposed algorithm is an extension of the optimization problem in (9) in which both the outputs of its weak classifiers and final output are vectors rather than scalars. At all time we maintain n candidate weak classifiers in which the jth weak classifier is defined as(10)hij(p)=zij¯(p).To update the classifier, we first update a subset of weak classifiers in parallel via an online feature template update strategy (refer to Algorithm 1), and then we choose k weak classifiers hijfrom the candidate pool sequentially by maximizing the lower boundJin (8)(11)hij=argmaxhij∈{zi1¯,…,zin¯}J(Hi(j−1)+hij),whereHi(j−1)=∑l=1j−1hil. Algorithm 2summarizes the main steps of the proposed online Fisher discrimination boosting method.At last, we concatenate all the selected feature templates in all bags to yield a high-dimensional multiscale image representationx=(h11⊤,…,h1k⊤,…,hc1⊤,…,hck⊤)⊤∈Rk∑i=1ctwithi×1. We then utilize an orthogonal matrixS∈Rc×k∑i=1ctwithito project x onto a c-dimensional feature space(12)v=aSx,where the entry of S is denoted as(13)sij=1ktwithi×{0j<(i−1)ktwithi,j>iktwithi,±1withequalprobability,and the ith entry of v is represented as(14)vi=∑j=(i−1)×c+1(i−1)×c+k+1sijsum(hij),where sum(hij) can be efficiently computed by the integral images [21].To efficiently search for the optimal state l, a coarse-to-fine search strategy based on sequential Monte Carlo (SMC) [27] is exploited which gradually approaches the maximum location response. The main steps of the search mechanism are summarized in Algorithm 3, wherelrepresents object location (x, y), which also represents object state. The first step roughly approximates the object location, and therefore a large radius γ and search step Δ is used to generate several candidate windows. The number of windows is much smaller than the exhaustive search method, but the search result is close to the groundtruth object location. The weight of each candidate window is proportional to the classifier response (15), and the top N high score windows are delivered to further fine search. Then, based on the coarse location, we iterate R times to refine the location based on a random walk motion model. Each parameter inltis modeled independently by a Gaussian distribution around its counterpart inlt^,i.e.,lt∼q(lt|lt^)=N(lt^,Σxy),whereΣxy=diag(σx,σy)is a diagonal covariance matrix whose elements are the corresponding variances of translational parameters. Finally, the tracked object location is the state of the particle with the maximum confidence response.The background context information is integrated into particle weight via the Naive Bayes classifier(15)w∝H(v)=∑i=1clog(p(vi|y=+)p(vi|y=−)),and the conditional distributions are assumed to be Gaussian distributed as(16)p(vi|y=+)∼N(μi+,σi+),p(vi|y=−)∼N(μi−,σi−),whereμi+andσi+are the mean and standard deviation of the ith positive feature, respectively and similar toμi−andσi−. To take into account the target appearance variance, the parametersμi+andσi+are incrementally update by [16](17)μi+←λμi++(1−λ)μ+,σi+←λ(σi+)2+(1−λ)(σ+)2+λ(1−λ)(μi+−μ+)2,where 0 < λ < 1 is a learning parameter,σ+=1n+∑k=0|y=+n+−1(vi(k)−μ+)2andμ+=1n+∑k=0|y=+n+−1vi(k),n+is the number of positive samples. However, (17) takes into account the target appearance information from the first frame to the current frame, which is not suitable to update the background information due to the complex background changes over time. Therefore, we only utilize the last two frame negative samples to update the background information as(18)μi−=1∑j=02ni−j−(∑j=02∑k=0|y=−ni−j−1vi−j(k)),σi−=1∑j=02ni−j−∑j=02∑k=0|y=−ni−j−1(vi−j(k)−μi−)2.

@&#CONCLUSIONS@&#
