@&#MAIN-TITLE@&#
Progressive hedging applied as a metaheuristic to schedule production in open-pit mines accounting for reserve uncertainty

@&#HIGHLIGHTS@&#
A stochastic version of the mine production scheduling problem is considered.A two-phase solution approach is developed.It is based on the progressive hedging strategy and a fix-and-optimize heuristic.Numerical results indicating the efficiency of the proposed approach are provided.

@&#KEYPHRASES@&#
Open-pit mine production scheduling,Progressive hedging method,Lagrangian relaxation,Sliding time window heuristic,Metaheuristics,

@&#ABSTRACT@&#
Scheduling production in open-pit mines is characterized by uncertainty about the metal content of the orebody (the reserve) and leads to a complex large-scale mixed-integer stochastic optimization problem. In this paper, a two-phase solution approach based on Rockafellar and Wets’ progressive hedging algorithm (PH) is proposed. PH is used in phase I where the problem is first decomposed by partitioning the set of scenarios modeling metal uncertainty into groups, and then the sub-problems associated with each group are solved iteratively to drive their solutions to a common solution. In phase II, a strategy exploiting information obtained during the PH iterations and the structure of the problem under study is used to reduce the size of the original problem, and the resulting smaller problem is solved using a sliding time window heuristic based on a fix-and-optimize scheme. Numerical results show that this approach is efficient in finding near-optimal solutions and that it outperforms existing heuristics for the problem under study.

@&#INTRODUCTION@&#
Scheduling production for open-pit mines is an important and critical issue in surface mine planning as it determines the raw materials to be produced yearly over the life of the mine and can have a huge impact on the economic value of a mining operation. The expense of setting up mining operations, the volatility of the markets, and the in-situ spatial variability of the deposit grades are all factors that make profit margins tight and investments risky. There is thus a clear interest in careful planning that can reduce risk and establish the most economically efficient mine production schedule possible that enables the mine operator to meet production targets and make the best possible return on investment. Even a 1 percent increase in the efficiency of the ore removal scheme can significantly increase the profitability of an operation that is worth hundreds of millions of dollars, as mining companies recognize and as the body of literature on the topic attests (see for example Dimitrakopoulos (2011); Newman, Rubio, Caro, Weintraub, and Eurek (2010); Whittle (2015), and the references therein).In formulating the mine production scheduling problem (MPSP), the mineral deposit is typically represented as a three-dimensional array of blocks, each of which represents a volume of material that can be mined and then sent either to a processing facility to recover the metal it contains, to a stockpile for possible future processing, or to a waste dump. Each block has a weight, a metal content, and an economic value. Blocks with a metal content above a certain cut-off grade are referred to as ore blocks and are to be processed, while those whose metal content is below the cut-off grade are waste. The metal content is estimated using information obtained from drilling, while the economic value represents the value of the metal recovered less the mining, the processing, and the selling costs. MPSP seeks to determine the mining sequence of the blocks; i.e., deciding which blocks to mine in each period of the life of the mine, so that the highest possible net present value of the mine is achieved. There are constraints specifying physical and operational limitations that must be taken into account. In basic terms, a block can be mined at most once (reserve constraints) and only after the blocks overlying it have been mined (slope constraints). Both extraction equipment and processing facilities have capacities that cannot be exceeded in any period (mining and processing constraints, respectively).Metal uncertainty (also referred to as reserve uncertainty) is an inherent part of mine production scheduling, as the metal content required for the decision-making is known only from limited drilling information. Ignoring metal uncertainty; that is, solving a deterministic model using a single estimate for the blocks’ metal content, can lead to incorrect assessments of the the production forecasts, the final pit limits, and the net present value (Albor & Dimitrakopoulos, 2010; Asad & Dimitrakopoulos, 2013; Dimitrakopoulos, Farrelly, & Godoy, 2002; Dowd, 1994; Marcotte & Caron, 2013; Menabde, Froyland, Stone, & Yeates, 2007; Ravenscroft, 1992). One should therefore consider metal uncertainty in the scheduling process, which gives rise to the stochastic MPSP considered in this paper.Most of the literature on the stochastic MPSP has been devoted to designing heuristic-based solution methods, as exact methods are only effective for small problem instances with a few thousand blocks (Boland, Dumitrescu, & Froyland, 2008; Ramazan & Dimitrakopoulos, 2013), while realistic instances involves typically tens to hundreds of thousands blocks. Early work proposed a simulated annealing algorithm (Godoy, 2002). A three-phase constructive approach that involves generating nested pits, grouping them in mining phases, and generating a schedule based on these phases has been developed by Albor and Dimitrakopoulos (2010). The method used in Lamghari and Dimitrakopoulos (2012) is based on Tabu search, and hybrid approaches embedding variable neighborhood descent and a very large-scale neighborhood search mechanism have been developed in Lamghari, Dimitrakopoulos, and Ferland (2013) and Lamghari and Dimitrakopoulos (2013), respectively. Behrang, Hooman, and Clayton (2014) used a clustering approach to reduce the number of binary variables and thus make larger instances computationally tractable by exact methods, while Chatterjee (2014) proposed a sequential heuristic where the sub-problems are solved using lagrangian relaxation techniques. Lagos, Espinoza, Moreno, and Amaya (2011) compared three approaches for optimization under uncertainty, namely Value-at-Risk, Conditional Value-at-Risk and a robust optimization approach. Marcotte and Caron (2013) studied the effect of metal uncertainty on the pit limits, and Fricke, Velletri, and Wood (2014) developed an analytic framework to quantify the impact of uncertainty and support strategic mine planning decisions.In this paper, a novel heuristic-based method is proposed to efficiently address the stochastic MPSP. It is based on the progressive hedging algorithm (PH) introduced by Rockafellar and Wets (1991) for optimization problems under uncertainty. In PH, uncertainty is modelled by a limited number of scenarios that reflect the information available about the uncertain parameters of the problem under study (i.e., metal uncertainty in the case of this paper). Each scenario has a known probability of occurrence and provides possible values of the uncertain parameters. PH decomposes the problem according to the scenarios and solves the sub-problem for each scenario separately. This step provides multiple solutions that are tailored to the scenarios and thus might be different from each other. Each solution indicates what should be done if a specific scenario is realized. But, what is needed is an implementable solution; that is, a non scenario-specific solution that will be feasible and can be implemented regardless of which scenario is realized and that is also well-hedged against uncertainty. The essence of the PH algorithm is to introduce implementability constraints to drive the solutions of the sub-problems towards a single implementable solution. Such constraints are relaxed and dualized in the objective function; that is, a penalty term measuring their violation is added to each sub-problems objective function to minimize the differences between the scenario-solution (the sub-problem solution) and an estimation of the implementable solution. PH iterates between updating the estimation of the implementable solution, adjusting the penalties, and solving the sub-problems until an implementable solution is obtained. At this point, the method is said to have converged, and for continuous stochastic problems, it converges to a global optimum.A limitation of PH is that convergence is guaranteed only in the convex case. When applied to mixed-integer stochastic problems, such as the one at hand, convergence might be difficult to obtain and, if PH converges, it converges to a local optimum. The approach proposed in Løkketangen and Woodruff (1996) to alleviate the convergence problem proceeds in 2 phases. Phase I applies PH, but with a different stopping criterion. Rather than waiting for full convergence, PH terminates when a fixed number of iterations or a fixed amount of CPU time is reached. The variables that have converged to that point (i.e., those that have the same values in all sub-problems solutions) are fixed in the original problem. One thus obtains a smaller mixed-integer stochastic problem, referred to as the restricted problem, which is solved in phase II to generate an implementable solution. Because the restricted problem is of reduced size, it will not take as much time to solve as the original problem would.This approach was successfully used to solve stochastic lot-sizing problems (Haugen, Løkketangen, & Woodruff, 2001) and stochastic network design problems (Crainic, Fu, Gendreau, Rei, & S.W., 2011). However, applying it in the context of the stochastic MPSP addressed in this paper is computationally expensive because the sub-problems associated with the scenarios involve a large number of binary variables and are themselves difficult to solve. Moreover, preliminary tests showed that although the restricted problem is of smaller size compared to the original problem, it remains a large-scale problem and requires long computational times if solved with an exact method, as is typically done in the literature. To overcome these difficulties, we made the following refinements to improve the performance of the approach proposed in Løkketangen and Woodruff (1996):•At phase I, instead of having each sub-problem associated with a single scenario, the sub-problems are comprised of multiple scenarios. Grouping scenarios and solving multi-scenario sub-problems not only will reduce the number of sub-problems to be solved at each iteration of phase I (PH), and hence the time required to complete phase I, but it should also result in faster convergence. Similar ideas were recently used by Crainic, Hewitt, and Rei (2014) in the context of stochastic network design. While Crainic et al. (2014) use different strategies to group the scenarios, we use here a random strategy.We take advantage of the structure of the problem to better exploit the information that becomes available during the iterations of PH and fix additional variables (in addition to those that have converged). This substantially reduces the size of the restricted problem to be solved during phase II, and consequently the computational time required to solve it.Rather than solving the restricted problem using an exact method, we use an efficient heuristic; namely, a sliding time window heuristic (STWH) based on a fix-and-optimize scheme.Numerical results are provided to evaluate the efficiency of the proposed solution approach as well as an analysis that shows the advantages and disadvantages of increasing the size of the groups of the scenarios during the first phase of the algorithm. The proposed solution approach is then compared to the sequential heuristic proposed in Lamghari et al. (2013), which is based on a time-decomposition strategy rather than a scenario-decomposition strategy, to a solution approach where STWH is used alone, and to a branch-and-cut approach. The results of these tests indicate the superiority of the proposed solution approach over the other approaches and show that combining PH and STWH leads to an efficient algorithmic framework that offers an excellent trade-off between solution quality and solution time (near-optimal solutions, within less than 1 percent of optimality on average, are obtained in a few minutes up to a few hours). Finally, we demonstrate on benchmark instances the benefits of solving a stochastic model rather than its deterministic counterpart. In this comparison, it is found that the objective function value is 1–27 percent higher when the stochastic approach is used.Throughout the paper, we consider the “classical” and basic variant of the MPSP, where the cut-off grade determining whether an extracted block is to be processed or sent to the waste dump is fixed, and only reserve, slope, mining, and processing requirements are taken into account. Additional requirements may be added via constraints or as penalties in the objective function to reflect other concerns such as variable cut-off grade, grade blending, or stockpiling. This leads to different variants of the stochastic MPSP; however, the algorithm proposed in this paper is a general-purpose algorithm and should be applicable to any of these variants.In the next section, a mathematical formulation of the stochastic MPSP considered in this paper is given. The solution procedure based on the progressive hedging strategy is described in Section 3. Numerical results are reported in Section 4. The paper concludes with Section 5, where directions for future research are summarized.The stochastic MPSP described in the previous section is formulated as a two-stage stochastic programming model (Birge & Louveaux, 2011). This model is described in detail in Lamghari and Dimitrakopoulos (2012), and we only briefly recall it here. The following notation is used:T: Set of time periods over which blocks are being scheduled, indexed byt=1,⋯,h.N: Set of blocks considered for scheduling, indexed byi=1,⋯,n.Pi: Set of predecessors of block i; i.e., blocks that have to be mined to have access to block i, indexed by p.Ω: Set of scenarios used to model metal uncertainty, indexed bys=1,⋯,S.xit={1ifiisminedduringperiodt,0otherwise.dst: Surplus in ore production during period t under scenario s.Wt: Maximum amount of rock (ore and waste) that can be mined during period t (mining capacity in tonnes).Θt: Maximum amount of ore that can be processed during period t (processing capacity in tonnes).wi: Weight of block i in tonnes (tonnage).θis={1ifiisoreunderscenarios,0otherwise;i.e.,ifiiswasteunderscenarios.vist=vis(1+d1)t: Discounted economic value of block i if mined and processed during period t, and if scenario s occurs (visbeing the undiscounted economic value and d1 the discount rate per period).ct=c(1+d2)t: Unit surplus cost incurred if the total ore mined during period t exceeds the processing capacity Θt(c is the undiscounted surplus cost and d2 represents the risk discount rate).The two-stage stochastic programming model SMPSP can be summarized as follows:(1)max1S{∑s∈Ω∑t∈T∑i∈Nvistxit−∑s∈Ω∑t∈Tctdst}(2)s.t.∑t∈Txit≤1∀i∈N(3)xit−∑τ=1txpτ≤0∀i∈N,p∈Pi,t∈T(4)∑i∈Nwixit≤Wt∀t∈T(5)∑i∈Nθiswixit−dst≤Θt∀t∈T,s∈Ω(6)xit∈{0,1}∀i∈N,t∈T(7)dst≥0∀t∈T,s∈Ω.The objective function includes two terms to maximize the expected net present value of the mining operation and to minimize the expected recourse cost incurred whenever the processing constraints are violated due to metal uncertainty. In this paper, the scenarios used are realizations of a spatial random field with an equal probability of occurrence and hence the coefficient1Srepresents the probability that scenario s occurs (Goovaerts, 1997).Constraints (2) guarantee that each block can be mined at most once during the horizon (reserve constraints). The mining precedence (slope constraints) is enforced by constraints (3). Constraints (4) impose an upper bound Wton the amount of rock (waste and ore) mined during each period t (mining constraints). Constraints (5) are related to the requirements on the processing levels (processing constraints). The target is to have the total amount of ore mined during any period t and under any scenario s be less than or equal to Θt; otherwise, the surplus penalty cost is equal toctdst. Constraints (6)–(7) enforce integrality and non-negativity conditions on the variables. Note that the variablesxitspecifying the mining sequence are the first-stage decision variables, and the variablesdstmeasuring the surplus in ore production are the second-stage decision variables.The two-stage stochastic model (1)–(7) is NP-hard since it contains the constrained maximum closure problem as a special case (Bienstock & Zuckerberg, 2010; Hochbaum & Chen, 2000). If the instance size is not large, it can be solved exactly, but this is not typically the case in real-world applications, justifying the use of heuristic-based methods. The next section presents a heuristic solution method based on the progressive hedging strategy.The proposed solution method consists of two main phases. In the first phase, a modified version of the baseline PH algorithm is used. It iteratively generates and solves an optimization sub-problem for each group of scenarios until a stopping criterion is met. In phase II, information obtained during the PH iterations is used to identify the earliest time and the latest time in which each block can be extracted. This allows us to fix many variables in the original problem SMPSP. The resulting restricted problem is solved to obtain an implementable solution.In what follows, a step-by-step description of our adaptation of PH is first provided. The method used to solve the restricted problem is then described.The scenarios modeling metal uncertainty are partitioned into groups, which are then used to define the sub-problems. The scenarios within each group are chosen randomly.Let G be the set of groups, indexed by g. Denote the partition byΩ=(Ω1,⋯,Ω|G|),where Ωg⊆ Ω ∀g ∈ G,∪g∈GΩg=Ω,andΩg∩Ωg′=∅∀g, g′ ∈ G and g ≠ g′. The first stage variablesxitare subscripted with a group index. This can be seen as creating a copyxigt∈{0,1}of eachxitfor each group g in order to allow the mining decisions to depend on the group, and yields the following model:(8)max1S{∑g∈G∑s∈Ωg∑t∈T∑i∈Nvistxigt−∑g∈G∑s∈Ωg∑t∈Tctdst}(9)s.t.∑t∈Txigt≤1∀i∈N,g∈G(10)xigt−∑τ=1txpgτ≤0∀i∈N,p∈Pi,t∈T,g∈G(11)∑i∈Nwixigt≤Wt∀t∈T,g∈G(12)∑i∈Nθiswixigt−dst≤Θt∀t∈T,g∈G,s∈Ωg(13)xigt=xig′t∀i∈N,t∈T,g,g′∈G,g≠g′(14)xigt∈{0,1}∀i∈N,t∈T,g∈G(15)dst≥0∀t∈T,g∈G,s∈Ωg.Whereas the objective function (8) as well as constraints (9)–(12) and (14)–(15) are self-explanatory given the previous discussion in Section 2, constraints (13) deserve some explanation. They are the so-called implementability constraints, and are used to guarantee an implementable solution; that is, a solution that will be feasible and can be implemented regardless of which scenario is realized. Denote this solution byx¯=(x¯it). Constraints (13) can then be rewritten as follows:(16)xigt=x¯it∀i∈N,t∈T,g∈G(17)x¯it∈{0,1}∀i∈N,t∈T.Following the decomposition scheme proposed in Rockafellar and Wets (1991), constraints (16) are relaxed using an augmented Lagrangian strategy (Bertsekas, 1982), which yields the following objective function:(18)max∑g∈G|Ωg|S{1|Ωg|∑s∈Ωg∑t∈T∑i∈Nvistxigt−1|Ωg|∑s∈Ωg∑t∈Tctdst−∑t∈T∑i∈Nλigt(xigt−x¯it)−12ρ∑t∈T∑i∈N(xigt−x¯it)2}.The Lagrangian multipliersλigtare associated with the relaxed constraints (16), ρ is a penalty ratio, and|Ωg|Sis the probability associated with group g (recall that we consider that the scenarios are equiprobable, and group g is composed of |Ωg| scenarios). Given that constraints (17) require that variablesx¯itare binary, the objective function (18) becomes, after rearranging the terms:(19)max∑g∈G|Ωg|S{∑t∈T∑i∈N(1|Ωg|∑s∈Ωgvist−λigt−12ρ+ρx¯it)xigt−1|Ωg|∑s∈Ωg∑t∈Tctdst+∑t∈T∑i∈Nλigtx¯it−12ρ∑t∈T∑i∈Nx¯it}.Ifx¯=(x¯it)is fixed to a given value, then the model is decomposable according to the groups. Denotevigt˜=1|Ωg|∑s∈Ωgvist−λigt−12ρ+ρx¯it. The sub-problem SPgassociated with group g can be expressed as follows:(20)max∑t∈T∑i∈Nvigt˜xigt−1|Ωg|∑s∈Ωg∑t∈Tctdsts.t.(9)−(12)and(14)−(15).In this paper,x¯=(x¯it),henceforth referred to as the inclusive schedule, is fixed as in Rockafellar and Wets (1991); that is, the average function given the group probabilities is used. Because the scenarios are equiprobable, and the probability associated with group g is|Ωg|S,this means that:(21)x¯it=|Ωg|S∑g∈Gxigt∀i∈N,t∈T.The penalties,λigtand ρ, are also adjusted as in Rockafellar and Wets (1991). The strategy used is inspired by the augmented Lagrangian method (Bertsekas, 1982) and is as follows:(22)λigt:=λigt+ρ(xigt−x¯it)∀i∈N,t∈T,s∈Ω(23)ρ:=αρwhere α is a constant greater than or equal to 1 (α ≥ 1) to guarantee a slow increase in the penalty term. Sensitivity of the algorithm to parameter α is explored in Section 4.Each iteration of PH requires solving G sub-problems, each associated with a group of scenarios (problems SPgdescribed in the previous section). This is done using the sequential heuristic proposed in Lamghari et al. (2013). In brief, the heuristic separates the problem into smaller sub-problems, each associated with a period t ∈ T. The sub-problems are solved sequentially in increasing order of t, and their solutions are combined to generate a solution for the original problem. Moreover, logical implications of the reserve, slope and mining constraints are used to reduce the number of decision variables in the formulation of the sub-problems to make them easier to solve. The need to resort to a sequential approach is a consequence of the large number of binary variables in the original formulation.As indicated in the introduction, PH might not converge, and thus a second phase is required to generate a solution for the original problem SMPSP ((1)–(7)) described in Section 2. This second phase consists of solving a restricted problem obtained from SMPSP by considering only a subset of variables; the other variables being fixed using a strategy that exploits the information obtained during the PH iterations and the structure of the problem. In what follows, we explain which variables are fixed and to which values they are fixed.Recall that at each iteration of PH, the inclusive schedulex¯=(x¯it)is computed using the following formula, where(xigt)represents the solution of the sub-problem SPgassociated with group g obtained at the current iteration:x¯it=∑g∈G|Ωg|Sxigt∀i∈N,t∈T.It is clear that anyx¯itcan only take values in the interval [0, 1] sincexigtare binaries and∑g∈G|Ωg|S=1.Furthermore, the larger the number of fractional componentsx¯itis, the less consensus there is among the scenarios.Once PH terminates, letbestx¯be the best inclusive schedule obtained so far. By best it is meant the inclusive schedule with the most consensus among the scenarios (or in other words, with the fewest components having fractional values).Partition the set of blocks into two disjoint subsets:•N1={i∈N:bestx¯it∈{0,1}for all t ∈ T}: the set of blocks for which a consensus has been obtained among the scenarios, or in other words, all scenarios agree that these blocks have to be mined in a specific period.N2={i∈N:there exists a period t ∈ T such that0<bestx¯it<1}: the set of the remaining blocks or those for which a consensus has not been obtained.All variables associated with blocks in N1 are fixed to their values inbestx¯.Now consider a block i ∈ N2. Even if the scenarios do not agree on which period block i should be mined in, they might agree on which periods block i should not be mined in. This observation is used to specify a “time window” for each block i ∈ N2. This is done as follows.LetEi=min{t∈T:bestx¯it>0}andLi=max{t∈T:bestx¯it>0}. Variablesxitsuch that t ∈ [1, Ei[ ∪ ]Li, h] are fixed to the value 0, while those such that t ∈ [Ei, Li] are not fixed. This means that block i can be scheduled no earlier than Einor later than Li. Preliminary experiments indicate that this strategy gives better results than the alternative that consists of fixing only variables associated with blocks in N1. In addition, usingbestx¯itinstead of the inclusive schedule obtained during the last iteration of PH improves the results.Although the restricted problem is somewhat smaller than the original problem, it can be very large when the size of the set N2 is large (i.e., when, after the iterations of PH, consensus has been obtained for only a few blocks). Consequently, solving it using an exact method might be time consuming (c.f. results in Section 4.3). Instead, we propose using a sliding time window heuristic (STWH) that divides the set of time periods into three disjoint but consecutive subsets (T1, T2 and T3). When solving the restricted problem, all variables associated with periods in the first subset, T1, are fixed to feasible values; those associated with periods in the second subset, T2, are restricted to be binary; and finally, those associated with periods in the last subset, T3, are relaxed to be continuous. The algorithm proceeds in a sequential manner, where the subsets T1, T2, and T3 are updated as follows:1.Set τ ≔ 1, T1 ≔ ∅, T2 ≔ {τ}, andT3:={τ+1,⋯,h}Solve the resulting problemFix all variables associated with period τ to the optimal values just foundSet T1 ≔ T1 ∪ {τ}IfT1=T,stop. Otherwise, setτ:=τ+1,T2 ≔ {τ},T3:=T−{T1∪T2}and go to step 2.The approach used to solve the restricted problem is based on integer linear programming techniques. Approaches based on such techniques have been used in the past to solve the deterministic version of the open-pit mine production scheduling problem and are also used in mine planning software (see Caccetta and Hill (2003) and Bley, Boland, Fricke, and Froyland (2010), for instance). The closest approach to ours is the one in Cullenbine, Wood, and Newman (2011). The authors also use a STWH, but in combination with Lagrangian relaxation. For periods in T3, not only do they relax the integrality constraints as it is done in this paper, but they also relax all the other constraints (i.e., slope, mining and processing constraints) to reduce computational effort. Such a strategy is not used here because the restricted problem is of small size and can be solved in a reasonable amount of time without relaxing the other constraints of the problem, as can be seen from the results in Section 4.A template for the algorithm based on the progressive hedging strategy and integrating the elements that have been presented in the previous sections is given below (Algorithm 1). In our implementation, phase I terminates when full convergence is obtained or when a fixed number of iterations (nIter) have been performed, but one can consider any other stopping criterion; for example, a fixed amount of CPU time or when the convergence reaches a pre-specified threshold value.The solution method proposed in this paper is tested on the instances introduced in Lamghari and Dimitrakopoulos (2012). There are two datasets, D1 and D2, each consisting of 5 different instances from actual copper and gold deposits, respectively. Table 1provides details about the instances. While the number of blocks and the number of periods differ from one instance to another, the number of scenarios used to model metal uncertainty is similar in all instances. More specifically, 20 equiprobable scenarios representing the mineral deposits were generated from a limited amount of drilling information using the geostatistical techniques of conditional simulation, which can be seen as a complex Monte Carlo simulation framework able to reproduce all available data and information as well as spatial statistics of the data. Further details about these techniques can be found in Boucher and Dimitrakopoulos (2009); Chiles and Delfiner (2012); Goovaerts (1997); Horta and Soares (2010); Maleki and Emery (2015); Rossi and Deutsch (2014). Twenty scenarios are sufficient to capture metal uncertainty, as previous work, such as that of Albor and Dimitrakopoulos (2009), indicates that after about 15 simulated representations of an orebody, stochastic schedules converge to both a stable final physical schedule and stable production forecasts. The reason for this is that while simulated scenarios represent a mineral deposit at the support-scale of mining blocks with a volume of a few cubic meters, the production schedule of a mine groups several thousand of these blocks in a single time period, subject to certain constraints. Therefore, since the support-scale of a mine’s schedule is orders of magnitude larger than that of the simulated representations of the mineral deposit being scheduled, the stochastic schedule becomes insensitive to additional scenarios after a relatively small number of scenarios. Note that some papers addressing the same problem considered in this paper use only 5–10 scenarios (Boland et al., 2008; Menabde et al., 2007).Table 2reports the economic parameters used to compute the blocks’ economic values and the recourse costs, which are based on real-life data provided by our industrial partners.The algorithm outlined in Section 3.4 (Algorithm 1) is implemented in C++ and run on an Intel(R) Xeon(R) CPU E7-8837 computer (2.67 GHz) with 1 TB of RAM running under Linux. To speed up the algorithm, an OpenMP parallel implementation of the initialization phase and the first phase is used. It is based on a simple master-worker strategy. The master operates as a central memory, which manages the search. Each worker processor deals with one sub-problem. It updates the associated penalties (Lagrangian multipliers), computes the modified economic values, solves the sub-problem, and communicates the solution to the master. When all sub-problems are solved, the master computes the inclusive schedule and sends it to the worker processors to update the penalties.Version 12.2 of CPLEX is used to solve the sub-problems associated with the periods within the sequential heuristic described in Section 3.2, and to solve the restricted problem of the second phase introduced in Section 3.3. CPLEX is also used to solve the linear relaxation of the two-stage stochastic problem (1)–(7) presented in Section 2 to obtain an upper bound on the optimal value, which is used to assess the quality of the solutions produced by the proposed algorithm. In all numerical experiments, the predual parameter of CPLEX is set to 1; that is, the dual linear programming problem is passed to the optimizer. This is a useful technique for problems with more constraints than variables, such as the one considered in this paper. To reduce the time required to complete phase I, the optimality tolerance parameter of CPLEX is set to 1 percent when solving the sub-problems associated with the periods. This parameter is set to its default value when solving the restricted problem of the second phase. Unless otherwise specified, all other CPLEX parameters are set to their default values since preliminary experiments indicated that these settings yield better results.In what follows, the results of the experiments conducted to determine appropriate parameter values for the first phase of the algorithm are first discussed. This is followed by a comparison of the performance of the algorithm considering the different alternatives for each of the two phases (i.e., in phase I, varying the size of the groups, and in phase II, using the sliding time window heuristic versus using an exact method, namely the Branch-and-Cut algorithm implemented in CPLEX). Finally, results showing the expected gain from solving the proposed stochastic model rather than its deterministic counterpart are presented. Note that when calibrating parameters, only the 5 instances in the dataset D1 are used. All 10 instances summarized in Table 1 are used in the remaining tests.As indicated earlier, at each iteration of PH, the value of the penalty ratio ρ used to compute the modified economic values is adjusted by multiplying it by a parameter α ≥ 1. As in Crainic et al. (2011), the initial value of ρ is set to1+log(1+D),D being the inconsistency level; i.e., the number of variables that have not converged after the initialization phase. To identify an appropriate value for the parameter α, we considered the case where the groups are comprised of 1 scenario (the case where convergence is the most difficult to obtain), we fixed the number of iterations of PH to 30, and we ran tests using 4 different values for α: 1, 1.1, 1.2, and 1.3. The sliding time window heuristic (STWH) was used to solve the restricted problem of phase II. The results of these tests are reported in Table 3. The column headings are defined as follows:•Instance, the name of the instance.α, the value used for the parameter α.%Convergence=|{(i,t):bestx¯it∈{0,1}|nh×100,the percentage of binary variables that have converged after the first phase of the algorithm.Z*, the value of the solution found by the proposed algorithm in dollars.%Gap=ZLR−Z*ZLR×100,the gap between zLR, the linear relaxation optimal value of the two-stage stochastic problem (1)–(7) presented in Section 2, and Z* as defined above. The smaller the value of %Gap is, the better the solution is.Time, the solution time in minutes. Note that the time reported for phase I includes the time needed to initialize PH with the procedure described in Section 3.4.As one can expect, the value of the parameter α has almost no effect on the computational time required for the iterations of PH (phase I), but it does have a significant effect on the convergence rate and the time required to solve the restricted problem (phase II). By using a large value for α, a higher penalty is associated with violation of the implementability constraints (16), which accelerates convergence. The advantage of having a large rate of convergence is that a large number of variables are fixed in phase II. The restricted problem is thus considerably smaller, which results in a significant reduction of the time needed to solve it. It is worth mentioning, however, that when variables converge, they do not necessarily converge to optimal values, since we are dealing with a mixed-integer stochastic problem. This explains why, in terms of solution quality, smaller values of α yield in general better results. The results indicate that the value 1.2 is the most appropriate value if one is looking for a trade-off between solution time and solution quality. Thus this value is used in all further experiments.The number of iterations of PH performed before solving the restricted problem (i.e., before phase II) is another parameter of phase I. We considered 4 values for this parameter (nIter): 0, 10, 20, 30. The value 0 means that the algorithm skips phase I and moves directly to phase II once the initialization phase is completed (i.e., once the sub-problems are solved using the original economic values). Results obtained when considering the four aforementioned values for nIter are summarized in Table 4, which has the same structure as Table 3, except that the second column indicates the number of iterations of PH performed. Again, the restricted problem is solved using STWH and the time reported for phase I includes the time needed for the initialization phase.From these results, one can see that the total solution time decreases as nIter increases. This is explained by the fact that the second phase requires less time because more variables are fixed in the restricted problem (c.f. column “%Convergence”), and this decrease outweighs the increase in the time needed to complete the first phase (increase due to performing more iterations of PH). Note also that there is no guarantee that a better solution will be obtained by waiting for a higher rate of convergence (i.e., by increasing the number of PH iterations). Actually, quite the opposite is more likely to happen, as can be observed from the values of %Gap. Thus, in all further experiments, nIter is fixed to 10. Note that with this value, the total solution time is larger as compared tonIter=30,but it is acceptable, considering that the solution is better. On average, the gap is 0.1392 percent whennIter=10versus 0.1401 percent, 0.1420 percent, and 0.1949 percent whennIter=0,nIter=20andnIter=30,respectively. Note that the difference in gap does not appear significant, but given the scale of the problem it represents a difference in value of hundreds of thousands of dollars, as can be seen from the values of Z* in the fourth column of the table.In this section, the effects on convergence, solution time, and solution quality of grouping scenarios are analyzed. Five sizes for the groups are considered: 3, 5, 7, 10, and 20. For each size, the scenarios within the groups are chosen randomly. Recall that the instances considered in this paper contain 20 scenarios. Thus, size 3 signifies that 7 sub-problems are solved at each iteration of the initialization phase and of the first phase. The number of sub-problems reduces to 4, 3 and 2 for sizes 5, 7 and 10, respectively. Finally, with size 20, there is only one sub-problem, which is equivalent to the original two-stage stochastic problem in Section 2. Consequently, for this size, neither phase I nor phase II will be necessary since the algorithm will provide an implementable solution at the initialization phase. The interest of considering size 20 is that this will allow us to compare the algorithm proposed in this paper to the sequential heuristic proposed in Lamghari et al. (2013) and briefly described in Section 3.2.For each group size, we ran tests usingα=1.2andnIter=10. The restricted problem of phase II was solved using STWH. The results for the copper instances (dataset D1) and for the gold instances (dataset D2) are summarized in Tables 5and 6, respectively. As in Tables 3 and 4, we report the values of %Convergence, of Z*, and of %Gap, as defined previously. The time required to complete phase I, the time spent solving the restricted problem (phase II), and the total solution time are given in minutes in the last three columns.As expected, increasing the size of the groups results in a faster convergence and is computationally more efficient. Considering the 20 scenarios at once is the fastest (only 18 minutes on average, considering the 10 instances in the two datasets). This good performance is, as explained earlier, due to the fact that full convergence (an implementable solution) is obtained at the initialization phase and thus neither phase I nor phase II are necessary (they are not completed). When considering groups of 10 scenarios each, full convergence is not achieved after 10 iterations of PH, but on average, a rate of convergence of 91.40 percent is obtained, requiring a total of 166 minutes. Then in phase II, most variables are fixed, and consequently, solving the restricted problem is straightforward and required on average less than 8 minutes. It can be seen that decreasing the size of the groups leads to a smaller rate of convergence, and consequently longer total solution times. It is worth noting that the size of the groups does not have a significant impact on the time required to solve the sub-problems (Phase I). This is in part explained by the fact that the size of the sub-problems is not really affected by the introduction of additional scenarios. In fact, the number of variables and constraints increases very slightly when the number of scenarios is increased (T continuous variablesdstand the associated constraints (12) are added for every additional scenario s). The results in Tables 4 and 5 also reinforce the observations made in Sections 4.1 and 4.2: when the rate of convergence is small, solving the restricted problem requires a bit of extra time because it is of larger size, but this extra time allows an increase in solution quality.Now, if we compare the sequential heuristic in Lamghari et al. (2013) (size 20) to the algorithm proposed in this paper (sizes 3, 5, 7, and 10), we can see that the latter outperforms the sequential heuristic in terms of solution quality (on average, the gap is 1.547 percent when the sequential heuristic is used versus 0.538 percent, 0.566 percent, 0.656 percent, and 0.812 percent when the algorithm proposed in this paper is used with group sizes 3, 5, 7, and 10, respectively). It should be noted that, although the differences in the gap appear small, for the context of the problem studied in this paper, they are meaningful because they represent millions of dollars, as can be seen from the values of Z* in Tables 5 and 6. Considerable economic gains ranging between 2 and 6 million dollars are achieved if the proposed algorithm is used instead of the sequential heuristic. In terms of solution time, although the solution times of the proposed algorithm are somewhat long compared to those of the sequential heuristic (on average, the proposed algorithm runs 9–18 times longer than does the sequential heuristic), they are reasonable and still considerably smaller than those required by the commercial solver CPLEX to solve the stochastic integer program, as can be seen from the numerical results presented next.Instead of solving the two-stage stochastic formulation (1)–(7), we solved a slightly different but equivalent formulation where the binary decision variables are defined as follows:xit={1ifiisminedbyperiodt,0otherwisethat is, it is(xit−xit−1)that specifies whether block i is mined in period t or not. This way of defining the variables has been proposed by Caccetta and Hill (2003) and has been shown to be computationally more efficient if one has to solve the problem using branch-and-cut methods, which is the case in the next experiments. With this definition of the variables, the two-stage stochastic model becomes:(24)max1S{∑s∈Ω∑t∈T∑i∈Nvist(xit−xit−1)−∑s∈Ω∑t∈Tctdst}(25)s.t.xit−1≤xit∀i∈N,t∈T(26)xit≤xpt∀i∈N,p∈Pi,t∈T(27)∑i∈Nwi(xit−xit−1)≤Wt∀t∈T(28)∑i∈Nθiswi(xit−xit−1)−dst≤Θt∀t∈T,s∈Ω(29)xi0=0∀i∈N(30)xit∈{0,1}∀i∈N,t∈T(31)dst≥0∀t∈T,s∈Ω.CPLEX 12.2 was used to solve the formulation (24)–(31) with a time limit of 26 hours, which is larger than the largest computational time required by the proposed algorithm considering all tested instances and group sizes. The results of these tests are summarized in Table 7. For each instance, we recall its size (columns N and T) and then we give the value of the gap between ZLR, the linear relaxation optimal value, and the value of the solution found by CPLEX within the time limit, as well as the time required to find this solution. A dash (“-”) indicates that no feasible solution was obtained within the 26-hour time limit, while the symbol “*” indicates that the solution found by CPLEX is optimal. We also give in this table the results obtained by the proposed algorithm when group sizes 3, 5, 7, and 10 are considered, which we refer to as PA-G3, PA-G5, PA-G7, and PA-G10, respectively. Table 8provides a comparison of solution times between the proposed algorithm and the linear relaxation.Results presented in Table 7 clearly show the limitations of exact methods and the need for heuristic approaches, such as the one proposed in this paper, to deal with instances of realistic size. For the smallest instance, C1, the solution times of the proposed algorithm and CPLEX are comparable. For the other 9 instances, the proposed algorithm is clearly faster and its running time has a significantly smaller growth rate. CPLEX was able to solve the small instances C1 and C2 to optimality, while it could not find the optimal solution for the larger instances C3 and G1 within the 26-hour time limit. CPLEX could not even find a feasible solution for instances with more than 20,000 blocks (the largest instances C4, C5, G2, G3, G4, and G5). A near-optimal solution was obtained by the proposed algorithm for all tested instances, within significantly less time than the time required by CPLEX to solve the linear relaxation of the problems (cf. Table 8).In this section, we first evaluate whether using the sliding time window heuristic (STWH) instead of an exact method in phase II improves the performance of the proposed algorithm. We then examine the value-added of PH; that is, whether STWH is efficient if it is not combined with PH.STWH is compared to the Branch-and-Cut algorithm implemented in CPLEX, henceforth identified as BCA, and the results are given in Table 9. For each instance, the 4 alternatives described in the previous section for grouping scenarios in the first phase are considered (i.e., sizes 3, 5, 7, and 10). The values of %Convergence obtained for each group are reported in the second column, while the next four columns give the objective function values obtained by each method (Z*) and the %Gap with respect to the upper bound provided by CPLEX. The last two columns report CPU times, in minutes, spent by STWH and BCA solving the restricted problem (i.e., to complete phase II). The times required to complete phase I are not reported because they are similar whether STWH or BCA is used, and have already been reported in Tables 5 and 6. The maximum run time for BCA and STWH is set to 10 hours (600 minutes), except for the instance G5 when the group size 3 is used in phase I, which is allowed 20 hours (1200 minutes) as it seems to be the hardest to solve. The best results obtained for each instance and each group size are indicated in bold.Within the time limit, the results in Table 9 indicate that using STWH improves the performance of the algorithm considerably. More specifically, there are three cases that depend on the size of the restricted problem to be solved. When the size of the restricted problem is small (c.f. results for C1 and C2), BCA and STWH are comparable. However, when the size is large (c.f. results for G4 group size 3 and G5 group sizes 3 and 5), STWH dominates BCA both in terms of solution time and solution quality. In particular, for the largest instance G5, when group size 3 is used, the gap is reduced to 1.262 percent when STWH is used compared to 25.516 percent when BCA is used. For the remaining cases, STWH is 6 to 47 times faster, and any slight improvements in solution quality derived from using BCA are outweighed by the increase in solution time.Finally, to examine the value-added of PH, the 10 instances considered in this paper were solved with STWH without combining it with PH. In what follows, we refer to this solution approach as PSTWH (Pure STWH). The time limit for PSTWH was set to 26 hours, and the results obtained are summarized in Table 10, which has the same structure as Table 7. As in Table 7, a dash (“-”) indicates that no feasible solution was obtained within the time limit.As one would expect, PSTWH is computationally expensive. For instances with more than 20,000 blocks (C4, C5, G2, G3, G4, and G5), its performance is quite similar to the performance of CPLEX when solving the two-stage stochastic formulation (24)–(31), in the sense that both methods were not able to find a feasible solution within the 26-hour time limit. PSTWH also failed to solve the instance G1 within the time limit. For the smaller instances (C1, C2, and C3), PSTWH requires in general more time than the proposed algorithm. Although PSTWH obtains slightly better solutions, the difference in gap is not significant, especially if the proposed algorithm is used with group size 3 (on average, % Gap is 0.033 compared to 0.051). These results clearly highlight the significant benefit of the proposed algorithm combining PH with STWH.The last part of the numerical tests addresses the question of the value of including metal uncertainty in the optimisation process. To this end, we use the so-called Value of the Stochastic Solution (VSS), which measures the expected gain from solving a stochastic model rather than its deterministic counterpart (Birge & Louveaux, 2011). More specifically, the problem is first solved by replacing the random parameters (the metal content) by their means to get a first-stage solution (a mining sequence, defined by thexit). Then, the first-stage solution is fixed at that value, and the problem is solved for each scenario s to get the value of the objective function under each scenario (Zs=∑t∈T∑i∈Nvistxit−∑t∈Tctdst). Finally, the expected value of the so-obtained objective values Zsis computed (E[Z]=1S∑s∈ΩZs). The Value of the Stochastic Solution is defined as the difference between the value of the two-stage stochastic problem solution, denoted by Z* in the previous sections, and this expected value, E[Z] (VSS=Z*−E[Z]). It is well-known that VSS ≥ 0 (see Birge and Louveaux (2011), for example). The objective of the numerical tests presented below is to confirm, as shown in all related studies mentioned in the introduction, that it is worthwhile to account for metal uncertainty when scheduling production of open-pit mines despite the additional complexity this approach might entail.Table 11shows results obtained for the 10 instances considered in this paper when the two-stage stochastic model is solved using PA-G3, PA-G5, PA-G7, and PA-G10; that is, the proposed algorithm combining PH and STWH with group sizes 3, 5, 7, and 10, respectively. In addition to the VSS values, Table 11 also provides the values of %Gain, indicating the relative percentage increase in the objective function value resulting from solving the stochastic model rather than its deterministic counterpart. %Gain is calculated as follows:%Gain=Z*−E[Z]E[Z]×100.Clearly, it pays off to use the stochastic solution rather than the mean value solution. The objective function value is 1–27 percent higher, and the average %Gain, over all instances, is around 13 percent no matter which group size is used when solving the stochastic model. The increase of 1 percent may not be seen as very substantial, but it should be noted that it represents significant benefits in the order of millions of dollars (c.f. columns VSS). The largest increases are found for the gold instances, G1-G5, and this is partly explained by the fact that these instances have a higher variability compared to the copper instances, C1-C5.

@&#CONCLUSIONS@&#
This paper explores the development of an efficient optimization approach to address the large and complex problems faced by the mining industry when scheduling production in open-pit mines under metal uncertainty. We have proposed a two-phase solution approach based on the progressive hedging strategy (PH). PH is used in phase I where the problem is first decomposed by partitioning the set of scenarios modeling metal uncertainty into groups, and then the sub-problems associated with each group are solved iteratively to drive their solutions to a common solution. In phase II, a strategy exploiting information obtained during the PH iterations and the structure of the problem under study is used to reduce the size of the original problem, and the resulting smaller problem is solved to generate an implementable solution.Through computational experiments, we have shown that the proposed algorithm performs very well in terms of solution quality. We have provided an analysis that shows the advantages and disadvantages of increasing the size of the groups of the scenarios during the first phase of the algorithm. This analysis indicates that increasing the size of the groups has a positive impact on the solution time, but it negatively impacts the solution quality. For the second phase, we have compared two alternate solution methods: a sliding time window heuristic (STWH) and the branch-and-cut algorithm implemented in the commercial solver CPLEX (BCA). The results indicate that, with respect to solution quality, the two methods are comparable with a slightly better performance for BCA, but STWH has a significant superior performance to that of BCA in terms of solution time. The results also indicate that STWH is efficient only if combined with the progressive hedging algorithm (PH); i.e., only if used within the algorithm proposed in this paper. With respect to the sequential heuristic previously proposed by the authors, the algorithm proposed here dominates in terms of solution quality. Its weakness is that it requires longer solution times; however, these solution times are considerably smaller compared to those required by the commercial solver CPLEX to solve the problem directly; i.e., to solve the two-stage stochastic model in Section 2. The potential benefits of solving the stochastic model over solving the deterministic model in which the random parameters are replaced by their expected values have also been highlighted. The results of the tests indicate that it would pay off by millions of dollars (1–27 percent increase in the value of the objective function) to use the stochastic solution rather the mean value solution.As mentioned earlier, this paper represents ongoing efforts to efficiently address the stochastic MPSP. Future work may consider investigating whether the algorithm would be as successful or not in solving variants of the MPSP that include more operational constraints, such as variable cut-off grade, grade blending, and stockpiling, as it is in solving the “classical” variant considered in this paper. Indeed, it is a general-purpose algorithm and should be applicable to any of these variants. Other research avenues include considering other strategies for updating the penalties within PH and other methods for solving the sub-problems. Finally, another important research direction is the development of other efficient solution approaches. Since it has been observed empirically that the problem formulation often achieves small integrality gaps, one approach could be to solve the linear relaxation of the problem using an efficient algorithm and then to use an LP-rounding procedure to get an integer solution.