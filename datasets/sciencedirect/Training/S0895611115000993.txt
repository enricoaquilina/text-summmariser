@&#MAIN-TITLE@&#
Efficient TpV minimization for circular, cone-beam computed tomography reconstruction via non-convex optimization

@&#HIGHLIGHTS@&#
Better reconstruction model: constrained total p-variation minimization.Stable and efficient iteration scheme: alternating direction minimization.Explicit and efficient formulas: iterative p-shrinkage and linearized technique.Efficiently implemented by graphics processing unit.Robust and accurate for very sparse and noisy dataset.

@&#KEYPHRASES@&#
Alternating direction method,Few-view reconstruction,Generalized p-shrinkage,Total p-variation,

@&#ABSTRACT@&#
An efficient iterative algorithm, based on recent work in non-convex optimization and generalized p-shrinkage mappings, is proposed for volume image reconstruction from circular cone-beam scans. Conventional total variation regularization makes use of L1 norm of gradient magnitude images (GMI). However, this paper utilizes a generalized penalty function, induced by p-shrinkage, of GMI which is proven to be a better measurement of its sparsity. The reconstruction model is formed using generalized total p-variation (TpV) minimization, which differs with the state of the art methods, with the constraint that the estimated projection data is within a specified tolerance of the available data and that the values of the volume image are non-negative. Theoretically, the proximal mapping for penalty functions induced by p-shrinkage has an exact and closed-form expression; thus, the constrained optimization can be stably and efficiently solved by the alternating direction minimization (ADM) scheme. Each sub-problem decoupled by variable splitting is minimized by explicit and easy-to-implement formulas developed by ADM. The proposed algorithm is efficiently implemented using a graphics processing unit and is referred to as “TpV-ADM.” This method is robust and accurate even for very few view reconstruction datasets. Verifications and comparisons performed using various datasets (including ideal, noisy, and real projections) illustrate that the proposed method is effective and promising.

@&#INTRODUCTION@&#
Cone-beam computed tomography (CT) configured with circular scanning is widely used in many imaging applications [1]. Accurate image reconstruction with sparse or few views is greatly important in areas needing fast data acquisition [2] or low radiation levels [3]. A lot of work [2–13] regarding iterative image reconstruction (IIR) algorithms has focused on total variation (TV) minimization, which exploits the sparsity of the gradient magnitude image (GMI). TV is the L1 norm of the GMI and reconstruction done by minimizing TV can reduce data acquisition when the GMI is sparse in non-zero entities.Many efficient algorithms—such as the adaptive-steepest-descent-projection-onto-convex-sets (ASD-POCS) framework [5], Chambolle–Pock (CP) framework [14,15], split-Bregman (SB) method [16], alternating direction minimization (ADM) method [17,18], and so on—have been developed for TV minimization. Optimization models in the CP, SB, and ADM methods are always decoupled into two (or more) sub-problems involving the L1 and L2 norms. Every framework mentioned provides efficient and explicit proximal mappings to minimize every sub-problem. L1 relaxation often yields easy computation using shrinkage mapping (also known as “soft thresholding”), which makes it easy to use in practical CT imaging applications.However, the sparsity of a digital signal x is, in fact, directly measured by totaling the non-zero entities in it, i.e.,x0(the l0 quasi-norm of x other than TV). Thus, TV is not the most direct method to take advantage of the GMI's sparsity prior. TV's form is actually a convex relaxation of the minimization of the constrained L0 quasi-norm of GMI. The best way to survey the GMI's sparsity is to use the GMI's L0 quasi-norm. Unfortunately, the optimization for this problem lacks efficient solvers and no algorithms are practically designed for CT image reconstruction.It is logical to assume that a relaxation closer to the L0 quasi-norm can measure GMI sparsity better than can TV, which implies that the pth power of the Lp normzpp≜∑izip(0<p<1) is a feasible alternative. Researchers in both theoretical and applied fields have made many efforts to develop practical solvers for Lp minimization [17–20]. Fortunately, recent theoretical progress [20] and applications [21,22] show that, although the Lp norm causes non-convex optimization problems, it may be practical for use in CT imaging with reduced datasets and that efficient solvers can help design reconstruction algorithms [23] with greater capabilities than those designed with the conventional TV model. Recent work has derived a general class of shrinkages and their induced penalty functions [18–20] related to Lp minimization. Penalties related to Lp can empirically recover the original sparse signal from fewer measurements. Furthermore, theoretical proof [20] has guaranteed that one can obtain exact recovery when using these non-convex shrinkages given data and an observation matrix from a broad class of matrices. It should be pointed out that the penalty function utilized in [20] differs with that in [23] for the former does not have an explicit expression.Reference [23] put forward the concept of total p-variation (TpV) minimization; and by the inspiration of a general class of shrinkages and their induced penalty functions in [20], this paper proposes a class of generalized total p-variation (GTpV) which can serve as a better reconstruction model. This paper intends to develop a new reconstruction algorithm based on the constrained GTpV minimization. On the basis of not cause misunderstanding, GTpV is referred to as TpV in the following text. For efficient algorithm design, ADM is applied and splits the augmented Lagrangian function of the optimization into two sub-problems. The ADM framework makes use of the algorithm iterative p-shrinkage (IPS) [20], which has been proven capable of solving the sub-problems containing generalized penalty functions. Another sub-problem is a quadratic function and is solved by a local linearization and proximity technique that avoids pseudo-inverse computation of big matrices. The algorithm is tested by simulation data and real CT data and shows outstanding capabilities in sampling reduction.The remainder of this study is organized as follows. Section 2 introduces some necessary theoretical background. Section 3 presents the proposed reconstruction model and the design of the proposed algorithm. In Section 4, numerical simulations and real data experiments are used to validate the performance and capability of the proposed algorithm. In Section 5, a brief discussion is given of the results obtained from Section 4, and our conclusions are given in Section 6.As shown in Fig. 1(a), the methods and experiments presented in this paper are based on the cone-beam CT, which mainly consists of X-ray source, flat detector and the corresponding mechanical gantry system. The simplified geometry of the scanning system is described in Fig. 1(b) which characterizes some essential geometry terms, e.g., distance of source to the rotation axis (SOD), distance of source to the detector (SDD), source orbit (red solid circle) and field of view (FOV, depicted by greed area). Modeling the system serves as the basis of image reconstruction. In the discrete-to-discrete view, cone-beam CT data acquisition can be written as(1)Wf=g,wheref∈ℝNis the image vector comprised of the voxel coefficients,W∈ℝM×Nis the system matrix generated by projection of the voxels, andg∈ℝMis the data vector containing the measured projection data. Actually, there are many specific forms of this linear expression, depending on sampling, voxel expansion form, and approximation of continuous cone-beam projection. In this paper, f is composed of independent cubic voxels and the entities in W are measured by the intersection length between the rays and the voxels. We apply the data acquisition formula to the sparse- or few-view sampling, which means that the equation is severely under-determined and the unique solution is difficult to obtain.Under the GMI's sparsity prior, an accurate solution can be obtained by the following optimization:(2)f˜=argminf∇fpp,subject toWf=g,where 0≤p≤1 and the argument of the Lp norm is the component-wise magnitude of the image gradient and∇∈ℝ3N×Nstands for the discrete gradient operator on a three-dimensional object. In the constrained optimization (2), the most direct method to measure GMI sparsity is the case of p=0. However, the problem for this situation is equivalent to NP-hard (non-deterministic polynomial-time hard). The solving of the problem would be computationally intractable if there were no further assumptions on W and f.When dealing with the Lp minimization in the case of 0<p≤1, a hybrid form minimization comprising the Lp and L2 norm always has to be taken into consideration, that is,(3)z=argminzpp+β2z−∇f−r/β22,0<p≤1,wherez≜∇f∈ℝ3Nis an introduced variable andr∈ℝ3Nis an auxiliary variable of multipliers. The solution for (3) or, equivalently, the proximal mapping for penalty functionzpp, can be defined by(4)proxβ⋅pp∇f,r=argminzzpp+β2z−∇f−r/β22,where the optimized z is related to ▿f and r.For the case of p=1, the optimization turns to the conventional TV minimization and is a convex problem; thus, a lot of the efficient solvers mentioned above exist. It is known that when p=1, the proximal mapping ofzpphas a closed form expression and can be written as(5)proxβ⋅1(∇f,r)=S(∇f+r/β)=max∇f+r/β1−1β,0⋅sign(∇f+r/β),whereS:ℝ3N→ℝ3NandS(x)i=sxisign(xi)are shrinkage operators and s=sβis defined by(6)st=maxt−1β,0,which provides an efficient method for optimizing (3).Conventional TV minimization is useful and practical in computational aspects but sacrifices reconstruction capability for extremely under-sampled data. Non-convex penalty functionszpp(0<p<1), which are better approximations of the original L0 than of L1, can provide better sparse recovery results but lead to non-convex optimization problems. Moreover, there is no closed-form expression of solvers for general p and it is not practical for several of the efficient algorithms mentioned above to be generalized from L1 to Lp minimization.Reconstruction models and algorithms for enhanced exploitation of gradient sparsity should focus on both better GMI sparsity measurement and better algorithm efficiency. Thus, a shrinkage-like proximal mapping with a closed-form and its induced penalty function (which is closer to L0 minimization than to L1) should be studied.In the development of a practical CT reconstruction algorithm, it is urgent to derive proximal mappings with continuous and explicit forms. A generalized shrinkage operator, named “p-shrinkage,” is introduced for compressed sensing recovery in [18,19], and these p-shrinkage mappings have been proven to have some qualitative resemblance to the Lp proximal mappings.In order for this paper to be self-contained, we include a brief overview and basic conclusion of the general p-shrinkage and its induced penalty function from [20]. The p-shrinkage mapping Sp(x)=Sβ,p(x) for 0<p<1 that is considered in this paper is defined bySp(x)i=spxisign(xi), where the general p-shrinkage function sp=sp,βis defined by(7)sp(t)=maxt−βp−2tp−1,0.The p-shrinkage and the conventional form in (6) coincide when p=1.The motivation for using alternative shrinkage mappings in (7) is to get a closed-form formula in the reconstruction algorithm, which means that the alternative shrinkage mappings are required to be proximal mappings of the penalty functions and that their induced penalty functions should be guaranteed to be better measurements of sparsity than L1. The following theorem, indeed, guarantees these properties and was first proposed and proven in [19].TheoremSupposes:[0,∞)→ℝis continuous, satisfies x≤β⇒s(x)=0 for some β≥0, is strictly increasing on [β, +∞), and s(x)≤x. DefineSp(x)i=spxisign(xi)for each i. Then, Spis the proximal mapping of a penalty functionG(w)=∑ig(wi), where g is even, strictly increasing and continuous on [0, +∞), differentiable on (0, +∞), and non-differentiable at 0 iff β>0 (in which case ∂g(0)=[−1, 1]). If also x−s(x) is non-increasing on [β, +∞), then g is concave on [0, +∞) and G satisfies the triangle inequality.The proposed p-shrinkage in [20] satisfies all hypotheses of the above theorem for all parameter values. The proof of the theorem, conducted in [19] in detail, constructs g using the Legendre–Fenchel transform [24] of an anti-derivative of S. Although g cannot be guaranteed to have a closed-form expression for the penalty function G, this is considered an acceptable price to pay for having an explicit proximal mapping, which is much more suitable for implementing a practical, cone-beam CT algorithm than is having a closed-form penalty function and optimization model. In order to have an intuitive impression of the p-shrinkage and its induced penalty functions, we can compute Spand the corresponding g numerically; some examples are plotted in Fig. 2. The plots indicate that the smaller the value of p, the less the p-shrinkage shrinks large inputs and the slower the penalty function increases on large inputs. For both intuitive observation and theoretical analysis, the penalty function induced by p-shrinkage can serve as a better approximation to L0 minimization than to L1.GMI sparsity motivates the design of the CT reconstruction algorithm by utilizing the constrained total variation minimization. Inspired by conventional TV algorithms, the generalized total p-variation minimization model can be analogously expressed as(8)argminzGp(z),s.t.Wf−g=e,e2≤ε∇f=zf≥0,where Gp(·) is one of the penalty functions from Theorem with explicit proximal mappings defined by (7),e∈ℝM(controlled by parameter ɛ) stands for inconsistency in the projection data acquisition, and f≥0 indicates that every entity in f should be non-negative. Carefully compared with the models applied in [23], it can be found that although the two models shares a similar motivations and purposes, yet one of the distinguishing feature which makes the proposed model suitable is that (8) is driven by an easy and efficient algorithm implementation. This consideration will probably be useful for engineer and technicians as well as professional mathematicians. Consequently, one should try best to establish a model which can results an easy and robust algorithm. On these considerations and under recent theoretical developments, the proposed optimization model is chosen in this paper.The framework to design the algorithm is based on ADM which is originated to solve a class of problems with two variables which can be split. ADM framework is very suitable for solving the proposed model in that the proposed model has two separate variables. Moreover, the convergence analysis and conclusions [25] guarantee that this kind of methodology is of efficient performance. Note that (8) becomes conventional constrained TV minimization when p=1. Actually, there are two variables (f,z) in (8) that should be optimized. An unconstrained form of the above optimization, using the augmented Lagrangian function, can be expressed as follows:(9)LA(f,z)=Gp(z)−λ1T(z−∇f)+β12z−∇f22−λ2T(Wf−g−e)+β22Wf−g−e22+δpos(f),whereλ1∈ℝ3Nandλ2∈ℝMare Lagrangian multipliers attached to ∇f=z and Wf−g=e, respectively. Constants β1 and β2 are penalty parameters. Function δpos(f) is a non-negative indicator defined as(10)δpos(f)=0,f∈ℝ+N∞,f∉ℝ+N,whereℝ+N=x1,x2,…,xNT|xi≥0,i=1,2,…,N. Introducing z makes it convenient to apply the ADM method, which guarantees stable convergence.The augmented Lagrangian function can be split into two sub-problems with respect to z and f. First, for fixed fk,λ1k, andλ2k, the minimization of (9) can be formulated as(11)minzLA(z;fk,λ1k)=minGp(z)−λ1kTz−∇fk+β12z−∇fk22=minGp(z)+β12z−∇fk+λ1kβ122.Evidently, the minimization of (11) can be expressed by p-shrinkage mappings given by(12)zk+1←Sp∇fk+λ1kβ1.Second, for fixed zk+1, fk, ek,λ1k, andλ2k, assuming thatfk+1∈ℝ+N(i.e.,δposfk+1=0), the minimization of (9) can be expressed as(13)minfLAf;zk,λ1k,λ2k=minλ1kT∇f+β12zk+1−∇f22−λ2kTWf−g−ek+β22Wf−g−ek22,where it is a quadratic function with respect to f. In order to avoid computing the inverse of the matrix containing W,Wf−g−ek22can actually be linearized at current point fk:(14)Wf−g−ek22≈Wfk−g−ek22+2dkTf−fk+1τf−fk22,wheredk=WTWfk−g−ekis the gradient ofWf−g−ek22at fkand τ>0. By plugging (14) into (13) and setting the derivative to 0, we get(15)β1∇T∇+β2τ⋅If=ck,whereck=∇Tβ1zk+1−λ1k+WTλ2k+β2/τ⋅fk−β2dk. Note that, under the periodic boundary conditions for ▿, the coefficient (β1∇T∇+β2/τ·I) is a constant and block-circulant matrix that can easily be diagonalized by fast Fourier transform (FFT). Let J=diag[F−1(β1∇T∇+β2/τ·I)F], where F stands for the Fourier transform matrix (implemented by FFT) and diag[M] returns a vector constructed by the principal diagonal entities of M. Then, the solution of (13) can be expressed byfk+1=FF−1(ck)/J,where the division is a component-wise operator and J is a constant vector that can be computed beforehand to save time.Iffk+1∉ℝ+N(i.e.,δposfk+1=∞), fk+1 should be forced tofk+1∈ℝ+N. Therefore, fk+1 should be updated by(16)fk+1←posFF−1(ck)/J,where pos(x) forces all negative entities in x to be 0 while keeping positive entities unchanged. The noise term ek+1 can be updated by(17)ek+1←min1,ε/Wfk+1−g2⋅Wfk+1−g.Consequently, updated formulas for the Lagrange multipliersλ1kandλ2kare expressed by(18)λ1k+1←λ1k−ηβ1zk+1−∇fk+1,λ2k+1←λ2k−ηβ2Wfk+1−g−ek+1,where η∈(0, 1) is a relaxation factor.In brief, ADM is utilized to solve (9) and the two sub-problems are iterated in an alternating scheme. The relationship between the proposed generalized TpV minimization and the conventional TV one is clear: the TpV optimization expands the definition of the TV model and its numerical algorithm. In particular, TpV turns out to be conventional TV reconstruction by ADM when p=1 (termed the “TV-ADM algorithm”), which can be described as follows (List Algorithm 1). In this paper, p-shrinkage mappings are integrated into the ADM framework; it should be noted that, for different values of p, the reconstruction algorithm may have different recovery capabilities for sparse sampling. The general algorithm (termed the “TpV-ADM algorithm”) scheme can be expressed as follows (List Algorithm 2).Algorithm 1: Conventional TV minimization in the ADM (TV-ADM) frameworkInput W, g, β1, β2, ɛ, η, initialize f0, z0, and k=0.While “not converged,” Do(1) Update zk+1 by:zk+1←max∇fk+λ1k/β11−1β1,0⋅sign∇fk+λ1k/β1,(2) Update fk+1 by:fk+1←posFF−1(ck)/J,(3) Update ek+1 by:ek+1←min1,ε/Wfk+1−g2⋅Wfk+1−g,(4) Update multipliers by:λ1k+1←λ1k−ηβ1zk+1−∇fk+1,λ2k+1←λ2k−ηβ2Wfk+1−g−ek+1,(5) k←k+1.End DoGet reconstruction result: f.Algorithm 2: Generalized TpV minimization in the ADM (TpV-ADM) frameworkInput W, g, β1, β2, ɛ, η, initialize f0, z0, and k=0. Given p∈(0, 1):While “not converged,” Do(1) Update zk+1 by:zk+1←Sp∇fk+λ1k/β1,(2) Update fk+1 by:fk+1←posFF−1(ck)/J,(3) Update ek+1 by:ek+1←min1,ε/Wfk+1−g2⋅Wfk+1−g,(4) Update multipliers by:λ1k+1←λ1k−ηβ1zk+1−∇fk+1,λ2k+1←λ2k−ηβ2Wfk+1−g−ek+1,(5) k←k+1.End DoGet reconstruction result: f.Although there are only one forward- and one backward-projection operation in both algorithms at each iteration loop, yet these operations can occupy most of the running time. For more efficient implementation, a fast and parallel algorithm proposed by Gao [26] is utilized in this paper. Gao's method is one of the most efficient algorithms for projection calculation at the present state of the art. Although iterative algorithm is fundamentally sequential, the TV-ADM and TpV-ADM for CBCT can be implemented efficiently with the utilization of GPU card under the Nvidia framework of compute unified device architecture (CUDA). The updating formulas consist of a series of matrix-vector multiplications and vector additions. The computations of ▿f and Wf are well suit for parallel computation on GPU card, with each thread computing the difference of a voxel. The flow chart in Fig. 3demonstrates the GPU accelerated implementation of the algorithms in this paper.In order to test the algorithms developed in Section 3.2, it is necessary to put them into simulation, which can reflect their practical applications as closely as possible. However, in real-life applications, there are so many factors that need to be considered during algorithm design, testing, and optimization that it is non-trivial to incorporate and investigate every single one. Thus, a brief discussion of how to determine algorithm parameters is given here.Since the reconstruction algorithm is designed for real-life applications, it is necessary to simulate its practical applications when conducting numerical experiments. In our implementation, the scanning configuration (or data acquisition parameter) is set according to (or similarly to) the prototype cone-beam CT system built for industrial or medical usage.Even though a graphics processing unit (GPU) is utilized to accelerate the projection operation, it is still too computation costly for 3D iterative reconstruction algorithms. Thus, the projection data and reconstruction volume are down-sampled by a reasonable ratio.Parameters β1, β2, ɛ, η (for both TV and TpV algorithms) and p (for the TpV algorithms only) control the algorithms’ behavior. Actually, for ideal observation and noiseless data, the sufficient condition for convergence for both algorithms is β1>0, β2>0, ɛ=0, η∈(0, 1), which is a very loose restriction and can easily be attained. The upper bounds for β1 and β2 are undefined theoretically, but (according to our tests on algorithm performance) it is better to set them to less than 104. Generally, for better convergence speed, proper values for β1 and β2 should be chosen for maintaining the balance between the quadratic term and penalty function.For noisy data, the value for ɛ can no longer be set to 0; instead, it should be determined according to noise level. If the noise priore2is knowable, the value for ɛ should be equal to or less than that. Unfortunately, if the noise is complicated and not easily knowable, it is always better (as in the case of CT data acquisition) to set ɛ with a small value close to 0 in the beginning, observe the reconstruction results, and adjust ɛ with careful tuning.For the TpV algorithms, the parameter p is in (0,1) and further investigation should be conducted to determine its proper value in practical applications. The generalized penalty functions are somewhat approximations and not strictly identical to Lp minimization. Thus, the effects of the value of p on reconstruction cannot be arbitrarily determined. Based on our experiments, p∈[0.5, 0.9] is suitable for noiseless datasets, while p∈[0.7, 0.9] is suitable for noisy datasets.A series of simulation studies are first carried out using a perfect dataset. For these simulations, a commonly used digital test model (the Shepp–Logan phantom) is used to generate the projection data by multiplying the system matrix. Both the simulation studies and the followed real data experiments are carried out on the platform of a Lenovo workstation D30 with CPU (Intel Xeon E5-2620 v2) and GPU (Nvidia Tesla K20c). In this study, the entities in the system matrix are generated by calculating the intersection length of the rays and cubic voxels. The generated projection is then perfectly matched with the system matrix and phantom. These noiseless data are used to test the reconstruction capabilities of the algorithms.While the projection is generated, the distance from the source to the iso-center is 300mm and the distance from the source to the center of the flat panel detector is 600mm. The flat panel is composed of 256×256 detector bins, and each detector bin is 1.24mm×1.24mm. The object is composed of 128×128×128 voxels, and each cubic voxel is 1.24mm×1.24mm×1.24mm. The linear system described by (1) has 128×128×128 unknown voxels, while the GMI from the 3D Shepp-Logan phantom has about 81,747 non-zero entities; therefore, the phantom's GMI has a sparsity of about 4%.Ideally, for exact reconstruction, there should be twice as many observable entities for TV minimization reconstruction as there are non-zero GMI entities. However, this factor of two is only a theoretical lower bound for necessary observation quantity. Practically, even in simulations with noiseless data, sufficient data measurements should be much more than twice that of the non-zero entities in GMI. In our tests, a factor larger than four is proper for simulation experiments, which means that at least six projections should be observed for accurate TV-ADM reconstruction. Accordingly, two groups of simulation experiments are conducted using six and five projections evenly in π angular coverage, respectively. Naturally, measurements with six projections may be sufficient for both algorithms and can be used to compare the convergence speeds of TV-ADM and TpV-ADM. However, five projections may be sufficient for detailing the differences in recovery capabilities. In order to ensure convergence, the iteration numbers are all set to 1000.With the aid of GPU accelerations of forward and backward projections [25], the time required for five and six projections are 1.1 and 1.2s per iteration, respectively. In the first group of noiseless data simulations, six projections are used to perform the reconstruction. Some reconstruction results are shown in Fig. 4. Specifically, images at 500 iterations (400 for TpV-ADM when p=0.5) are utilized for comparison with each other. Every tested algorithm can produce highly accurate reconstructions at 1000 iterations (see left plot of Fig. 6), so only the intermediate results reflect the differences in the algorithms’ capabilities. Obviously, TpV-ADM algorithms provide relatively better quality than does the TV-ADM algorithm, which indicates that TpV-ADM possesses faster convergence.For the second group of noiseless simulations, only five projections are used for the reconstruction. It is worth noting that, for TV-ADM, five projections cannot provide an exact reconstruction, as shown in Fig. 5. For TpV-ADM (p=0.9, 0.7, 0.5), five projections can reconstruct much clearer images than can TV-ADM at the same iterations. The reconstruction errors are computed using the root mean square error (RMSE), and RMSE behaviors, along with associated iteration numbers, are plotted in Fig. 6. For six projections, TpV-ADM RMSEs decrease faster than do TV-ADM RMSEs. For reconstruction with five projections, TpV-ADM shows better recovery capability than does TV-ADM, since the former produces more accurate images.There are various inconsistencies in data acquisition, which causes the reconstruction to be contaminated by artifacts. Including more physical factors in the reconstruction may improve this situation; however, such an effort is beyond the scope of this paper. In this paper, simulations with the most important sources of data inconsistency, i.e., Poisson noise, are conducted to test algorithm performance.The initial number of photons is 66,000, and the Poisson noise algorithm used is the one proposed in [27]. Because of the existence of noise, admissible reconstruction needs more projections than does reconstruction in a noiseless situation; thus, simulations with noisy data are conducted using six and seven projections. The value for ɛ is set to 10−5. The rest of the scanning parameters are the same as with noiseless simulations. The time required for six and seven projections are about 1.2 and 1.4s per iteration, respectively. Results from using six and seven projections are shown in Figs. 7 and 8, respectively; RMSE curves and associated iteration numbers are plotted in Fig. 9. The reconstruction images in Figs. 7 and 8 indicate that TpV-ADM produces relatively better results with projections contaminated by random noise than does TV-ADM. Fig. 9 suggests that the new algorithms have better convergence properties.After simulations are completed, we compare reconstructions with real cone-beam CT projections. The object being scanned is a phantom human head for medical use. The cone-beam CT system mainly consists of the X-ray source (Hawkeye 130, Thales, France), the orthogonally rotary stage with object holder, and the flat-panel detector (4030E, Varian, USA) with pixel size 0.127mm. The working voltage and current of the X-ray source are 175kV and 200μA. In order to acquire projections with a proper signal to noise ratio, each pixel value of all projections is set according to the average of three observations.The scanning trajectory is a single circular line. The distance from source to iso-center is 1087.73mm, and the distance from the source to the center of the detector panel is 1434.72mm. A frame of the projection data is shown in Fig. 10. The flat panel detector has very high definition (3200×2304), and iterative reconstruction requires a lot of time. Therefore, down-sampling of four factors in each projection is done for the algorithms in this paper. Consequently, the equivalent bin size is 0.5080mm. The volume for iterative reconstruction is 384×384×384, and each voxel is 0.8024mm×0.8024mm×0.8024mm. Iterative reconstructions using 30 and 20 projections with an angular coverage of π are conducted for comparison. Each frame of the projection has 800×576 rays.The reference image in Fig. 10 is produced using the Feldkamp–Davis–Kress (FDK) algorithm with full-view data (360 views within 2π). This reference is the central transverse image of the volume since FDK algorithm only provides exact reconstruction in the central slice. Results from using 30 and 20 views with FDK algorithm are also presented in Fig. 10 and utilized as comparisons. Because there are many real-life factors that may affect image quality (even when full views are provided for FDK reconstruction), note that the so-called “reference” in Fig. 10 only provides an intuitive impression of what the object's attenuation is. Consequently, there is no numerical error adopted for comparison.For real-data iterative reconstructions, it is not necessary to set the iteration number to 1000, as in simulations, because an admissible reconstruction can be easily obtained early in the iteration stage and the practical application is time sensitive. In our experiment with real data, an iteration number of 100 is sufficient for producing satisfactory images. The time required for 30 and 20 views is about 12 and 18s per iteration, respectively, for both TV-ADM and TpV-ADM reconstructions.FDK reconstruction using 30 or 20 views suffers seriously from streak artifacts, while the results shown in Fig. 11for TV-ADM and TpV-ADM show few artifacts. It seems that the iterative algorithms (for TV-ADM and TpV-ADM) show better noise-suppression properties than does the FDK algorithm. Moreover, the TpV-ADM (p=0.8) algorithm produces relatively slightly higher qualities in reconstructions with both 30 and 20 projections.

@&#CONCLUSIONS@&#
This paper proposes a generalized TV reconstruction algorithm, i.e., the TpV-ADM algorithm, which is based on non-convex optimization and developed using the ADM scheme; it has been theoretically proven to be efficient and accurate. The proposed method can potentially help the specialists in medical imaging apply better algorithms to obtain satisfactory image quality when assisting in diagnosing. They can potentially help to reduce the radiation dose of the patients which makes the CT diagnose safer and easier.Simulation studies with both ideal and noisy data show that the proposed method can provide relatively better image quality than can conventional TV algorithms. In addition, real cone-beam CT data experiments indicate that the TpV-ADM algorithm is of significant practical value in real-life applications where needs fast and low dose iterative imaging. Further theoretical and technical optimization, as well as application-oriented research, should be carefully studied.