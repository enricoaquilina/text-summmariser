@&#MAIN-TITLE@&#
A review of recent advances in visual speech decoding

@&#HIGHLIGHTS@&#
A detailed review of the recent advances in the area of visual speech decoding.Visual features tackling speaker dependency, head poses and temporal information.Dynamic audio-visual speech information fusion.Recent techniques of facial landmark localization.Summary of audio-visual speech databases and ASR performance on them.

@&#KEYPHRASES@&#
Visual speech decoding,Automatic speech recognition,Lip-reading,Audio-visual speech recognition,Review,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
It is well known that speech perception is a bi-modal process that takes into account both the acoustic and visual speech information [70]. There has been clear evidence that visual information plays a key role in automatic speech recognition when audio is corrupted by, for example, background noise, or even inaccessible [97]. During the last two decades, there have been significant advances in the research of audio-based ASR [119], resulting in various commercial systems. Many believed that visual speech decoding would be relatively easily done following the success of audio-based ASR. However, early attempts did not achieve the anticipated results. As reported by Potamianos et al. [95], visual features provided very weak speech information in a large vocabulary continuous speech recognition (LVCSR) task. Despite the poor performance, the visual features still helped boost the ASR performance on some low-quality audio data through audio-visual (AV) speech information fusion.Since then, we have seen research focused on two major tasks: extracting better visual features and developing a better AV fusion scheme. For feature extraction, three particular questions have been asked and tackled recently:How to deal with speaker dependency in visual data? Visual data are typically stored as video sequences which contain a lot of information that is irrelevant to the uttered speech. Such information includes the variability of the visual appearances among different speakers. The first question addresses this important issue and is mainly concerned with the methods that attempt to suppress the speaker-dependent information in visual features.How to cope with head-pose variation? This question arises naturally when building a practical ASR system that uses visual information. In a real-world situation, it is unreasonable to assume that users would face the video camera all the time during speaking. Therefore, there could be various head poses in the captured visual data. Since the visual appearance of a talking mouth could vary significantly in images due to the view change, pose variation poses a serious challenge to any practical ASR system.How to encode temporal information in visual features? At first glance, this question seems less important. The temporal information of a talking mouth might well be characterized by statistical models (e.g., HMMs) built upon visual features. However, a number of studies [95,120,123] show that the use of statistical models alone may not be sufficient to capture the video dynamics. A solution is to encode temporal information to improve informativeness and stability of the extracted visual features.There have been various models proposed for fusing AV speech information in the past [95,97]. The recent research has been focused on answering the practical question: How to automatically adapt the fusion rule when the quality of the two individual (audio and visual) modalities varies? It is a critical question since there are many factors that could affect the quality. For instance, some abrupt background noise could significantly worsen the quality of the recorded audio, making the acoustic information unreliable temporally. In addition, a sudden loss of tracking of the speaker's facial landmarks could result in the failure of localizing the talking mouth, making the extracted visual features less informative. Therefore, it is important to develop a dynamic AV fusion scheme that is capable of handling the varying quality.In this paper, we review the recent studies concerning visual speech decoding. They are mostly organized and described with respect to the particular questions raised above. In addition, attentions are given to the state-of-the-art approaches to facial landmark localization. They can be used to accurately extract the region of the talking mouth, which is utterly important for any ASR system that uses visual speech information. However, we feel that they have been largely ignored in the recent work and would like to promote the use of these advanced methods in the future. Note that it is NOT our intention to provide a comprehensive review that covers every aspect of visual-based ASR and replaces previous surveys [95,97].Instead, this paper is aimed to serve as a supplement to them and focus on the recent advances in the area of visual speech decoding.This paper is organized as follows: Section 2 introduces some recently developed methods for facial landmark localization. Section 3 provides a detailed review of the recent studies that attempt to design better visual features. Section 4 describes the development of dynamic AV speech fusion schemes. AV speech databases are described in Section 5. In Section 6, we discuss the challenges and provide our vision for future research. Finally, Section 7 concludes this paper.Given a video of a talking face, to extract useful visual speech information, the first step is to locate the mouth region that contains the motion relevant to speech, or in other words, the region-of-interest (ROI). It is important since the quality of ROIs could significantly affect the speech recognition performance [97]. In general, we need to localize certain facial landmarks, such as eye corners, nostrils and lip corners, to correctly box the talking mouth in images. The ROI can then be cropped off and its size normalized for further visual feature extraction. Tracking facial landmarks has been an active research topic in recent years due to its wide applications to face-related vision problems. The state-of-the-art systems have been shown to be capable of accurately tracking facial points under various image qualities, head poses, facial expressions and partial occlusions.Unfortunately, the recent advances in facial landmark localization have largely been ignored by researchers in the speech recognition community especially when building up ASR systems using visual speech information. To extract ROIs, most of the systems relied on the active appearance model (AAM)11See [12,13,67] for details.[98,15,86,50,49,78], skin color thresholding [61,24], Haar-like feature based boosted classification framework22First introduced by Viola and Jones [114] and extended by Leinhart and Maydt [53].[59,60,62,26,120,124,125,123] or other lip-region classifiers based on linear discriminant analysis (LDA) [31] or support vector machines (SVMs) [103]. These methods are often heuristic and lack either tracking accuracy or capabilities of generalizing to new faces and handling large pose and illumination changes which are often encountered in a real-world environment. In this section, we briefly introduce some of the techniques recently developed for robust facial landmark localization. Note that it is NOT our intention to provide a comprehensive survey on this important and active topic that has a large literature. We expect that future research concerning visual speech decoding would benefit from the described techniques in terms of better ROI detection and extraction.The methods included in this section can be broadly grouped into two categories: the point-distribution-model (PDM) based and non-PDM based. Here we first describe the PDM-based methods. Let xidenote the location of the ith facial landmark and they are very often modeled by the point distribution model introduced by Coote and Taylor [14]:(1)xi=sRx¯i+Φiq+twhere the shape formed by the facial landmarks is modeled by a rigid transformation defined by scale s, rotation R and translation t, and a non-rigid transformation by q. Herex¯idenotes the mean location of the ith landmark in the training set and Φi the corresponding sub-matrix of the basis. Given a test imageI, from a probabilistic point of view, the objective is to maximize the posterior of the PDM parameter p={s,R,t,q}. Saragih et al. [106] defined the posterior as(2)pp|li=1i=1n,I∝pp∏i=1npli=1|xi,Iwhere liis a binary variable denoting whether the ith landmark is correctly located. The problem can then be formulated as(3)p∗=argminQpp=argminp−∑i=1npli=1|xi,I−logpp.The shape prior p(p) is often defined as a Gaussian with a diagonal covariance whose non-zero entries are set as the eigenvalues of the modes of the non-rigid deformation. Parameters p are often optimized in an iterative manner. In each iteration, parameter update Δp that leads to a local minimum of the lost functionQpstarting from the current values of p is calculated. After that, p←p+Δp. This process ends when p converges or the maximum number of iterations is reached. We will describe some recent systems built within such a framework below.In [17], Cristinacce and Cootes built a linear appearance model for the patches sampled at landmarks, similar to AAM. During each iteration of parameter updating, the appearance model generated a set of templates that best described the patches sampled at xi. The likelihood took the form p(li=1|xi,I)∝exp{−αRi} where Riwas the normalized correlation response of the ith template.Gu and Kanade [33] modeled p(li=1|xi,I) using the Gaussian mixture model (GMM) to account for the possible multiple modes. The modes were chosen as the K-largest responses in the response map calculated around xi. Saragih et al. [106] represented p(li=1|xi,I) as a kernel density estimate upon a set of candidate locations around xi. Instead of modeling the posterior, Asthana et al. [4] took a discriminative regression based approach. They proposed to learn a set of functions to predict Δp from the response maps calculated around {xi}. These functions were trained one by one such that the current function targeted the more difficult samples that previous ones failed to predict the correct Δp. During each iteration, Δp was estimated by the function that maximized the sum of the responses measured at the updated {xi}.The non-PDM based methods tend to estimate X={xi} directly fromI. Belhumeur et al. [5] adopted a Bayesian approach to tackle the problem. An SVM was trained as a local detector for each landmark and the problem was formulated as(4)X∗=argmaxxpX|Dwhere D denoted the detector responses. They assumed that X could be generated from the global model Xk,twhich was the kth labeled sample transformed by similarity transformation t. The posterior was then computed as(5)pX|D=∑k∫t∈TpX|Xk,t,DpXk,t|Ddt.During optimization, a RANSAC-like procedure was introduced to select a suitable subset of {Xk,t} to lessen the computational burden.Ong and Bowden [83] proposed a person-specific tracking system based on linear predictors (LPs). An LP is defined by a reference point c surrounded by a set of support positions, a linear mapping H, the base support pixel values v and bias b. Given image ℐ, the displacement of c is predicted by Hδv+b where δv is the difference between v and the pixel values at the support positions in ℐ. A number of LPs (a flock of LPs) were used to predict one point simultaneously and the output of the flock defined as the displacement averaged over all the LPs. For each facial point, two flocks were used to predict the horizontal and vertical displacements, respectively. Instead of randomly placing LPs, an iterative training step was introduced for selecting LPs within a flock based on their displacement prediction mean errors from training ground-truth data.Zhu and Ramanan [126] proposed to use a mixture of trees to capture the appearance and shape variations of facial landmarks. In particular, landmarks were connected such that they formed a tree. Each tree in the model encoded the landmark topology from a particular view. For mixture m, they defined the cost function(6)QmXI=AmXI+SmX+αm.whereAmXIwas the appearance evidence,SmXthe spatial configuration score and αma bias term.FunctionQmwas minimized over m and X to locate landmarks. Zhao et al. [122] also used a tree structure for landmark representation and a similar cost function. A cascaded strategy was proposed to prune the shape space efficiently. They first trained some AdBoost classifiers to reject the false positions for individual landmarks. They then further constrained the search space such that the true shape was assumed to be near one of the shapes in training samples.Martinez et al. [65] used a regression-based approach to localize facial landmarks. Two support vector regressors were learned to predict the horizontal and vertical displacements from a test location to the true one for every landmark. Landmarks were updated iteratively. Within the kth iteration, one test location was sampled from a sampling region for each xiand its predictiont^iktogether with previous predictionst^ijj=1k−1were used to approximate the true distribution of xias∑jNxi|t^ij,Σ. The mode of the distribution was used as the estimate of the true location so far.A Markov random field was trained to impose global shape constraints on all the centers of the sampling regions.Xiong and De la Torre [116] formulated facial landmark localization as a non-linear least square problem. The positions of facial landmarks X are updated iteratively. In the kth iteration, Xkwas computed as:(7)Xk=Xk−1+Rkϕk−1+bk−1where ϕk−1 was the SIFT features extracted at Xk−1 and Rkand bk−1 parameters learned from training data.The above described methods share the same principle that the non-rigid shape of facial landmarks is recovered based on evidence gathered from different local image patches. Such an evidence-gathering mechanism allows us to incorporate robust patch experts (e.g., the HOG [18] and SIFT [57] descriptor) and to train discriminative models that generalize well to unseen data. On the contrary, the widely used AAM and its variants [67,87,55,105] rely on a linear statistical model to generate the whole face texture. Such a model is often insufficient to represent the variations due to changes in identity, facial expression, pose and illumination [4].Finally, we list in Table 1the datasets used to evaluate the above-mentioned methods. It indicates, to some extent, their capabilities of handling factors such as image qualities, illumination changes, pose variations and facial expressions, as presented in the datasets.Despite of the many years of research, we have not seen any visual feature set universally accepted for representing visual speech, in contrast to the well-established features (e.g., MFCC [117]) for acoustic speech. Ideally, the extracted visual features should be relatively compact and sufficiently informative regarding the uttered speech, meanwhile showing a certain level of invariance against irrelevant information or noise in videos. It is a challenging problem largely due to the facts that there are uncertainties (e.g., speaker identity and head pose) that could significantly affect the visual appearance of a talking mouth in images and that visual features are extracted to describe a dynamic process (uttering) rather than static images.Traditionally, most of the feature extraction methods fall under one or some of the following categories as summarized by Dupont and Luettin [22]:1.Image-based[8,93,31,103,39] — raw pixel values are either used directly or undergone some image transformation as visual features.Motion-based[66,118] — features are designed to describe the motion observed during uttering.Geometric-feature-based[9,3,77,10] — geometric information of the talking mouth (e.g., the width and height of the mouth opening) is extracted as features.Model-based[68,25,48,50] — a model of the visible articulators is built and the compact model parameters are used as visual features.In this work, however, we categorize the recent development of visual feature extraction from a problem-oriented perspective which may provide more insight into the current progress in visual speech decoding. As mentioned in Section 1, there are three particular problems related to visual features. This section is therefore comprised of three subsections below. We describe those efforts that attempt to tackle speaker dependency and pose variation in Sections 3.1 and 3.2, respectively. Section 3.3 reviews the methods aimed to extract useful temporal information from video sequences.As illustrated in Fig. 1, speakers' mouths may look very different and so do their appearances in images. Such speaker dependency causes the major variation that troubles any attempt to extract useful speech related information from the cropped mouth images as pointed out by Cox et al. [15]. In acoustic speech recognition, techniques such as the vocal-tract normalization [52] and the maximum likelihood linear transformations [28] have been developed to effectively counter the variability in the acoustic signal among different speakers. In the visual domain, however, there has not been any universally accepted approach to tackling such speaker dependency.In the rest of this section, we review those methods that take into account the visual variability among speakers when extracting visual features. We first describe the efforts that attempt to search for a linear transformation that results in a low-dimensional subspace where speaker dependency is suppressed. We then introduce the articulatory feature based methods. Finally, we describe the latest work that uses the generative latent variable model to explicitly model the inter-speaker variations.The linear discriminant analysis has been widely used to deal with speaker dependency since it tries to pull the class means away from each other and push data points of the same class together at the same time. Here a class often corresponds to a speech unit (e.g., a viseme). Potamianos et al. [96] applied some image transformation (e.g., the DCT, DWT, or PCA) on ROI images and removed the mean from feature vectors' output by the transformation over each utterance. They then used LDA to further reduce the dimensionality. Later, they extended their method through applying the ‘inter-frame’ LDA on the concatenation of consecutive feature vectors output by the previous LDA which they referred to as the ‘intra-frame’ [95]. The extracted feature was named ‘HiLDA’. In this way, temporal information was encoded (we will discuss this issue in detail in Section 3.3). Lan et al. [50] adopted a similar strategy. Instead of the intra-frame LDA, they used AAMs to calculate features from images and applied the z-score normalization on a per-speaker basis. Note that their method required to acquire data of every test speaker to calculate the means and STDs for the z-score normalization before testing. Finally, they performed the inter-frame LDA on the normalized features.As argued by Yan et al. [117], the LDA projection can be considered as the linearized solution to a graph embedding (GE) problem. Within a graph, data points X=[x1,…,xN] are represented by vertices and connected by edges with assigned weights that quantify their similarities. A graph can be described by a matrix W whose element Wi,jrecords the similarity between the ith and jth data points. The GE defines W and Wp to encode the desired and undesired geometrical relationships of data points, respectively. The linearized GE (LGE) searches for linear projections w⁎ that preserve the desired and penalize the undesired geometrical information. Mathematically,(8)w∗=argminwTXLpXTw=cwTXLXTwwhere L and Lp are the Laplacian matrices of W and Wp and c a constant. The effectiveness of LGE has been demonstrated by its use in face recognition [37,117].Fu et al. [27,26] adopted the LGE framework for extracting visual speech features. For every data points xi, they searched for its K neighborsxjkik=1Kand weightswjkik=1Ksuch that xiwas best reconstructed by∑kwjkixjkiunder the constraints that xiandxjkibelonged to the same class,wjki≥0and∑kwjki=1. Let M be the matrix such thatMi,jki=wjki. They defined L=(I−M)T (I−M). Matrix Lp was defined in the same way except that the number of neighbors was set as Kp and xiand each of its neighborsxjkiwere from different classes. In this way, the learned subspace preserved the desired and suppressed the undesired local neighboring relationships. Note that W and Wp were not explicitly defined. However, Yan et al. [117] proved that L and Lp were indeed Laplacian matrices and W and Wp existed. For feature extraction, they concatenated DCT coefficients computed from every four consecutive frames and used PCA to reduce the dimensionality. The final visual features were projected from the DCT-PCA features using the learned linear mapping.The articulatory features (AFs) have been studied as an alternative to the traditional phonemic subword units for modeling speech. Kirchhoff [45] described them as the abstract classes which characterize the most essential aspects of articulation in a highly quantized, canonical form, leading to a representational level intermediate between the signal and the level of lexical units. Regarding visual speech, AFs could, for example, be the lip opening, lip rounding or labio-dental articulation as defined by Livescu et al. [56]. Instead of being described by a particular phone, a speech observation is characterized by multiple AFs simultaneously, resulting in a multivariate representation. Such a mechanism allows us to use just a few AFs to model speech that may otherwise requires dozens of phonemic units if they are context independent or hundreds (even thousands) if context dependent.Since we only need to train a small number of classifiers (or observation probability distributions) for AFs, we may gather a large number of training samples for each AF from training data, which potentially makes the trained classifiers more robust against the variability in the signal among speakers. Moreover, as shown by Papcun et al. [88], AFs themselves are to a certain extent speaker independent. Although it has been an active topic in acoustic speech recognition eLivEtAl07, there are few studies of AFs in the visual domain. Saenko et al. [101,103,102] used AFs for visual-only ASR, sometimes also referred to as automatic lip-reading. In their work, SVMs were constructed to classify AFs from ROI images. The classification scores were converted to probabilities by a fitted sigmoid function. They then fed the probabilities into a multi-stream dynamic Bayesian network (which allowed asynchrony between AFs) for speech recognition.The visual appearance of a talking mouth can be considered as the output of the combination of multiple sources of information. The desirable visual features need to preserve the relevant speech-related variation while suppressing the others. One way to do that is to use low-dimensional latent variables [75] to explicitly (in contrast to the above-mentioned methods) represent these sources of information and model the process that generates the observed images.Zhou et al. [123] identified two sources of major variations, caused either by the appearance variability among speakers or by speaking utterances, in frontal view mouth images. They considered the former as irrelevant, modeled by the latent speaker variable h and the latter as relevant, modeled by the latent utterance variable wt. The process of generating a video sequence was described as:(9)xt=μ+Fh+Gwt+ϵt.Here xtstands for the observed image at time t, μ a global mean image, F and W the factor matrices and ϵtthe normally distributed noise term. They placed p(wt) along a low-dimensional curve to preserve the temporal relationships among video frames (see Section 3.3 for details). To extract visual features from a video sequence {xt}, they fitted modelMthat was trained for a particular utterance to the sequence through measuring the posterior p(h, {wt}|{xt},M). The MAP estimatesw^twere directly used as visual features.Comparative studies among some features described above were conducted in their experiments. They were carried out through the task of recognizing ten short daily-use utterances among twenty speakers in a speaker-independent setting. Fig. 2shows the recognition rates averaged over the speakers on the OuluVS database [120]. Here the dark gray bars display the recognition rates obtained on the manually localized ROI images, while the light gray ones on those normalized automatically. The error bars represent one standard deviation. It can be seen that the quality of ROI images could significantly affect the ASR performance.Furthermore, they showed that the use of intermediate features such as the LBP [80] and LBP-TOP [121] (marked by ‘LBP-TOP’ in the figure) instead of raw pixel values (marked by ‘Raw’) could improve the recognition performance.Note that the proposed latent variable models were tested on recognizing isolated words or phrases, rather than continuous visual speech. They showed that their methods outperformed the HiLDA features on classifying smaller speech units such as visemes, indicating the possibility of extending the system to do continuous speech.It is impractical to assume that speakers would face the video camera all the time. Therefore, the talking face could be filmed not only from the frontal view (FV) but from other angles resulting in various head poses in images. Since the camera view can significantly affect the appearance of a talking mouth, pose variation challenges any system that tries to use visual speech information. Most of the methods concerning pose variation•either extracted some pose-dependent features (PDFs) from non-frontal view (NFV) images and directly used them for speech recognitionor transformed the PDFs into some pose-independent features (PIFs) before classification.For those extracting PDFs, the advantage is that there would be no information loss or added noise that may occur during the feature transformation. However, the disadvantage is that for every camera view, we will have to train a particular system using the corresponding PDFs. These methods may suffer from the lack of representative training data for a particular view. Hence, we may have to collect extra NFV data for training. On the contrary, the latter methods attempt to transform PDFs into a common PIF space such that they are comparable. Therefore, only one system needs to be trained based on the available training data. However, they often suffer some substantial performance drop due to the information loss or added noise caused by the feature transformation.Here we first describe those methods that extracted visual features directly from NFV images for speech recognition. Yoshinaga et al. [118] used the side-view videos recorded by a small camera installed in a headset, near the microphone. Optical flow was computed for each frame and the horizontal and vertical variances of the flow vector components as PDFs.In [58], Lucey and Potamianos used the cascade method described in [95] to extract PDFs from profile-view (PV) speech videos.In [60], Lucey et al. divided PV ROI images into some overlapping patches and from each path, calculated a set of PDFs using the same feature extraction method. They reported that features extracted from individual patches did not outperform those from the whole image. The fusion of them slightly improved the recognition performance.Kumar et al. [47] defined some geometric PDFs for speech recognition. To extract features from PV images, they first segmented foreground pixels using color thresholding. They then calculated the facial contour and located the nose tip and centers of the lip and chin as feature points. They measured four PDFs based on the points. Same geometrical features were measured from FV images. Experiments were conducted to recognize isolated words in a speaker-dependent setting and the PV features were reported to outperform the FV features. Saitoh and Konishi [104] adopted a similar approach. They defined an extra feature point, the lip corner and extracted eight geometrical PDFs. The features were used to recognize vowels and words in a speaker-dependent setting. Their system was reported to outperform human viewers.Having described the methods based on PDFs, we now focus on those aiming to design PIFs. Inspired by the pose-invariant face recognition work done by Blanz et al. [7], Lucey et al. [59,62] used linear regression to transform PDFs from an unwanted camera view to the wanted one. Let X=[x1,…,xN] and T=[[t1,1]T,…,[tN,1]T] where xnis the nth training sample of the unwanted view and tnthe synchronized counterpart of the wanted view. Linear transformation W was learned by(10)W=TXTXXT+λI−1.Given a sample x of the unwanted view, its transformed vector was computed ast^=Wx. In their work, linear transformations were learned to project features between PV and FV. From ROI images, PDFs were extracted using a cascade method similar to one in [95]. They showed that the regression-based strategy significantly improved the recognition performance when training and test datasets were of different poses. However, the performance was still substantially worse than the one obtained when training and test datasets were of the same pose.Esteller and Thiran [24] also used linear regression to normalize PDFs (extracted from views at 30°, 60° and 90°) to FV for pose-invariant speech recognition. In addition to the way used in Lucey et al. [59], linear regression was performed locally on individual patches of ROI images or partial PDFs. In their work, the former was named the global linear regression (GLR) and the latter the local linear regression (LLR). Features such as raw pixel values, DCT and LDA [95] were tested. They found that the GLR-projected LDA features achieved the best performance. LLR performed better than GLR on raw pixel values, but worse on DCT and LDA features.Lan et al. [49] computed AAM-based PDFs from various views based on their previous work [50]. The extracted features were appended with their second order derivatives for enhancement. They first trained a lip-reading system for each view and tested it on data recorded from that view. It was found out that for the proposed PDFs, 30° was the optimal view instead of FV. After that, they learned linear transformations (Eq. (10)) to project PDFs to the optimal view. They then trained a system for 30° and tested it on data from other views. The results confirmed that those feature transformations helped to increase system performance when training and test datasets were of different poses.Pass et al. [89] attempted to select DCT coefficients with the minimum cross-pose variances to form PIFs. Given two synchronized training datasets of different poses, they first applied 2D-DCT to obtain two feature streams F1 and F2. They then calculated F1–2=F1−F2 and matrix C1–2 that contained variances of elements of F1–2.Finally, they picked up the DCT coefficients corresponding to the smallest values in C1–2 as PIFs. Note that since the variances were calculated for a specific pair of wanted/unwanted views, the corresponding transformation was actually view-dependent. In case the selected coefficients were different, features transformed from different NFVs would not be able to share one ASR system built upon FV data.The visual speech signal contains not only the spatial information of the visual appearance of a talking mouth, but the important temporal information that characterizes the dynamic process of uttering. Potamianos et al. [95] proposed to concatenate consecutive feature vectors and employ LDA to obtain the final compact features that encoded temporal information. Since then the method has been used in a number of systems [59,60,62,50,49,24]. Despite of its popularity, such a simple linear approach may not be sufficient to capture the dynamic speech information. Other intuitive methods include the use of optical flow to capture the motion information [66] and the use of B-splines to model the temporal trajectories of the extracted individual visual features [115]. The former relies on the accuracy of the computed flow information and the latter on the quality of the extracted features, and therefore, may both be sensitive to noise. Below we review the more sophisticated methods that have been recently developed to tackle the problem.As illustrated in Fig. 3, a video can be viewed as a 3D volume from which we may obtain the temporal patterns (TPs) which are defined as the images formed by stacking a particular row/column of each frame along the temporal axis. Zhao and Pietikäinen [121] exploited the texture information within TPs to characterize video dynamics. They applied the LBP descriptors both on TPs to capture temporal and on video frames to extract spatial information. The concatenated spatio-temporal LBP histograms, named ‘LBP-TOP’, were successfully used for dynamic texture classification. Zhao et al. [120] used LBP-TOP features to handle the lip-reading task of classifying a limited number of short utterances. The features were computed from the whole utterance and classified by SVMs to recover utterance identities. Later, the above method was extended in [125] through adding a video normalization phase. Videos of the same utterance across speakers were linearly interpolated to have a pre-defined length before the calculation of LBP-TOP features. They found out that such a simple phase significantly improved the lip-reading performance (more than 20% better than the one reported in [120]).Ong and Bowden [82] proposed to use the temporal signatures (TS) to capture the temporal information. In their work, a static image was represented as a binary feature vector. A TS was defined as an arbitrary set of ‘1’ locations within a TP which is a binary image formed by stacking feature vectors extracted from a fixed number of consecutive video frames in this case. Weak classifiers were formed to detect TSs within an input TP. For utterances to be recognized, strong classifiers were constructed from the weak classifiers within the AdaBoost framework. Due to the huge number of TSs that could be defined within a TP, they proposed a gradient-descent based method to search for suitable TSs for learning strong classifiers.Pachoud et al. [85] extracted visual features directly from 3D image volumes. To do that, they divided a video segment into several ‘macro-cuboids’. Each macro-cuboid was then divided into cuboids and the SIFT descriptor adapted for cuboids [21] was used to calculate features from them. They collected a database of model templates (video segments of the utterances to be classified) and matched them with test video segments to recognize utterances.Zhou et al. [124] adopted the LGE approach (see Eq. (8)). Instead of measuring the actual distances between visual observations which did not take their temporal arrangement into account, they defined a pseudo-distance measure based on the frame alignment results between sequences of the same utterance.Interestingly, they found out that the training sequences were mapped onto temporal trajectories similar to sine waves. The waves were then Fourier transformed and the peak frequencies recorded for classifying utterances. Their method was used to tackle the same lip-reading problem as in [120] and found to perform well in a speaker-dependent setting.Inspired by the above finding, the same authors proposed to use a path graph to represent the temporal structure of a video sequence [125]. It was found out that the vertices of a path graph could be embedded onto a low-dimensional curve through graph embedding and more importantly, each dimension of the curve was exactly a sine wave, consistent with the results in [124]. Fig. 4illustrates the path-graph representation and the embedded curve. For a reference video sequence, they learned a linear map that projected the features extracted from images (e.g., LBP) onto the embedded curve.To match the reference, a test sequence was mapped using the learned projection and the resulting trajectory was compared with the embedded curve.Once again, the method was used to solve the above speaker-dependent lip-reading problem.An obvious drawback of the above model is that it is trained on a single video sequence and therefore, may generalize poorly to sequences of a different speaker. Zhou et al. [123] generalized the above representation. In particular, the representation was relaxed to allow graph vertices to represent data points that were either observed or unobserved. In such a way, the path graph could be used to model multiple video sequences of the same utterance. For feature extraction, they constructed a generative latent variable model (see Eq. (9)) that represented the speech related variations by latent variables wt. To preserve temporal information, they placed prior distributions p(wt) along the embedded curve, as illustrated by Fig. 5, to penalize values of wtthat contradicted the temporal relationships among image frames.Pei et al. [91] used AAM to track local feature points on the lip in video frames. Around each point, they extracted a small patch and calculated LBP and HOG features as the texture features. The point's in-image displacement from the previous frame was measured as the shape features. They then defined a patch trajectory which is the trajectory of the combined texture and shape features over time for the point. The random forest was constructed to measure similarities between patch trajectories and the multidimensional scaling algorithm performed to learn a low-dimensional manifold within which a patch trajectory was represented by a single data point. Video sequences were projected into sets of feature points in the manifold and their distances measured based on the points.It can be seen that the problem of temporal information extraction has been tackled by the above methods at two different levels. In [85,120,82], they considered the local pixel-level spatio-temporal structures while in [124,125,91,123] the structure at the frame level was modeled and enforced in the extracted visual features. Note that none of the above sophisticated methods were tested on continuous visual speech. In [120,124,82,91,123], systems were trained and tested for classifying isolated words/phrases, while in [85], the problem of detecting isolated digits in continuous speech was considered. Only Zhou et al. [123] provided results on classifying smaller visual speech units. For other methods, it is unknown whether they are suitable for classifying short video segments, for example, at the viseme level.Audio-visual speech information fusion is a non-trivial task in speech recognition. As summarized in [95,97], fusion can be performed at either the feature or decision level. The former calculates a new set of features, out of the ones extracted from the audio and visual streams, which are more discriminative for speech recognition. For such a purpose, Ngiam et al. [79] employed multi-modal deep learning for feature-level fusion recently. However, its major disadvantage is that it cannot cope with speech data with varying qualities, which would be expected for a practical system. In contrast, decision-level fusion combines the decisions made, or in other words, the probability output by the HMMs that are trained for the individual modalities. It allows us to encode the reliability of each modality such that AV-ASR systems can adapt to a new environment efficiently.Within the decision-level scheme, dynamic AV fusion (DAVF) has emerged as the focus of recent studies. One important reason is that it provides a system the capability of constantly adjusting itself to the volatility in the quality of the observed audio or visual streams. For instance, when there is a burst of noise in the background, the system should quickly adapt itself so as to temporarily reply more on the visual stream. Similarly, when there is a sudden illumination change, it should weigh more on the acoustic stream. In this section, we review the techniques recently developed for DAVF.Before going into any detail, we first introduce the most widely used model for combining the likelihoods output by HMMs. In the model, the extracted audio and visual feature vectors at time t, xtA and xtV, are assumed to be conditionally independent and the emission probabilities combined as:(11)pxtA,xtV|qt=pxtA|qtAλtApxtV|qtVλtV.Here q=[qtA, qtV]T stands for the state numbers and λtA and λtV the non-negative stream weights that control the contribution of each modality to the (unnormalized) joint likelihood. Note that when asynchrony is permitted between the audio and video HMM states, qtA and qtV may be different. Due to the dynamic nature of DAVF, the weights need to be set in an online manner and their values directly related to the modality reliabilities measured from the current observed data.To simplify the problem, the sum of the weights is often fixed, e.g., λtA+λtV=1 such that only one weight is required to be estimated. Next, we will first describe how the modality reliability is quantified and then the way it is converted into the stream weight.The signal-to-noise ratio (SNR) is perhaps the most intuitive and straightforward reliability measure [2,72,16,112,109,23]. For the audio signal, SNR can be calculated as the ratio between the power of the speech signal and that of the noise. The latter is often measured from some silence interval. Estellers et al. [23] found out that SNR obtained during the silence intervals within utterances could significantly degrade the fusion performance if treated the same as those measured at the speech time. Therefore, they constructed a detector based on the trained HMM classifier [110] to detect the non-speech moment and learned different functions for converting the corresponding speech/non-speech SNRs into stream weights. Shao and Barker [109] considered the way of measuring SNR using the noise power calculated from silence intervals as unreliable since there could be voice from other speakers at the background, which should also be considered as noise. They argued that the true SNR be measured using both the acoustic and visual information and proposed to use the three-layer multi-layer perceptrons (MLPs) to measure it. They first trained two HMM classifiers for the audio and visual streams, respectively. At time t, to estimate SNR, the likelihoods (of individual Gaussian mixtures) output by all the audio and visual states were passed to the bottom layer of the MLPs. SNR was then output by the top layer.Besides SNR, the modality reliability can also be quantified by the dispersion [2,100,94,38,95,63,23] and entropy measure [94,38,23]. The former is defined on the N-best state likelihoods output by the trained HMMs and mathematically, can be written as:(12)Dt=2NN−1∑n=1N∑m=n+1Nlogpxt|qn,tpxt|qm,twhere xtis the observed feature vector and qn,tthe number of the state with the nth best likelihood. The latter is defined on the posteriors of all the HMM states q and can be expressed as:(13)Ht=−∑qpq|xtlogpq|xt.Other reliability measures include the voicing index [6,30] and the N-best log-likelihood difference [94,95,63]. Recently, Estellers et al. [23] proposed to accumulate the transition probability between the previous and current most likely (ML) state to indicate the signal reliability. Mathematically, the measureCtwas defined as:(14)Ct=Ct−1+pqtML|qt−1ML.Here qtML was the number of the state with the best likelihood.The motivation behind the measure was that if the observed signal was somehow corrupted, the transition between the ML states would be likely unnatural and therefore,Ctwould be lowered by the corresponding small transition probability.Given the reliability estimate(s), there have been various functions designed to map the estimate(s) to one stream weight. Meiel et al. [72] proposed a piece-wise linear function. However, the weight in their work was fixed globally. Glotin et al. [30] adopted the same function to convert the estimated voicing index into λtA. Garg et al. [29] used a sigmoid function to evaluate λtA, i.e., λtA=1/(1+exp(a+wTy)) where y was the vector formed by multiple AV reliability estimates.Marcheret et al. [63] quantized λtA such that it had a fixed number of values that were evenly distributed between 0 and 1. They set λtA either as the expectation or as the MAP estimate given the posterior distribution p(λj|y) where λjwas the jth quantized weight value. They modeled each distribution using a full-covariance GMM and learned the model parameters from the instances that maximized the word error rate (WER) on the evaluation dataset.Gurbuz et al. [35] used a lookup table to convert the measured SNR. The table had two dimensions standing for the ratio and noise type. To learn its entry values, they first added various types and levels of noise to the audio evaluation data. They then filled in the table with the value that minimized WER on the modified data based on the HMMs learned from the original clean data. Shao and Barker [109] adopted the same approach. However, the table's dimensions represented SNR and a parameter used for normalizing the video stream likelihoods. Estellers et al. [23] defined the function asλtA=a1eb1y+a2eb2ywhere y was the reliability measure (they tried various measures in their work) and α1, α2, b1, and b2 were the parameters. They learned the global λtA values for various SNR levels and tuned the function parameters such that difference between the global values and converted weights were minimized.Note that the methods described so far all follow the same strategy that the stream reliability is first estimated from the observed data and then converted into λtA. However, there are other methods that did not use the strategy. In the rest of this section, we will review these methods. Instead of dynamically choosing the stream weights in Eq. (11), Kolossa et al. [46] focused on selecting the robust features at the running time. For visual features, they trained two Gaussian distributions Pmand Pnon the features extracted from the correctly and incorrectly detected mouth images, respectively. At time t, if pm(xtV)<pn(xtthrmV), xtV would not be considered by the system, i.e., λtV=0. The audio features were compared with a background noise estimate. An audio feature was deemed unreliable and therefore, discarded if the value of the corresponding feature extracted from the background noise exceeded 90% of its value. In their experiments, λtA was set globally according to different levels of noise in audio data.Stewart et al. [111] proposed the maximum weighted stream posterior model. They defined the posterior of the synchronized HMM state qtgiven a weight wtas:(15)pqt|wt,xtA,xtV=pxtA|qtwtpxtV|qt1−wt∑qt′pxtA|qt′wtpxtV|qt′1−wtand the optimal posterior as(16)pqt|xtA,xtV=maxwtpqt|wt,xtA,xtV.In their work, wtwas quantized to have a finite set of values within [0,1].Note that none of the fusion methods we have described so far explicitly modeled the actual noise, or in other words, the uncertainties in the observed stream features. As an exception, Papandreou et al. [86] considered the observed noisy features xtMwhere M={A,V}, as the sum of the underlying clean features ztMand some Gaussian noise ϵtMwith meanμntMand covarianceΣϵtM. The likelihood p(xtM|qtM) could then be expressed as:(17)pxtM|qtM=∫pxtM|ztMpztM|qtMdztM.Given that p(ztM|qtM) was modeled by a GMM, they obtained a close-form expression for p(xtM|qtM) which was also a GMM.They introduced uncertainty compensation fusion model (UCFM) as:(18)pqt|xtA,xtV=pqtpxtA|qtpxtV|qt.They also introduced a hybrid model that gave separate weights for the two modalities, i.e.,(19)pqt|xtA,xtV=pqtpxtA|qtλtApxtV|qtλtV.AAM was used to extract visual features. They setμϵtV=0andΣϵtVas the covariance matrix obtained as the by-product of the least-squares AAM fitting process [87]. On the audio side, they adopted the method described in [19] to measure the noise mean and covariance. In the experiments, the proposed models were compared with the stream weighting fusion model (SWFM) in Eq. (11). It turned out that the hybrid model achieved the best performance and SWFM significantly outperformed UCFM especially in the low SNR area. They also found out that the noise adapted likelihood in Eq. (17) could substantially boost the audio-only ASR performance.One major obstacle to the current research on visual speech decoding is the lack of suitable databases. In contrast to the richness of audio speech corpora, only few databases are publicly available for visual-only or audio-visual ASR [97]. Most of them include a limited number of speakers and a small vocabulary (e.g., digits, phrases or short sentences), and therefore, are not suitable for training LVCSR systems. Those eligible for LVCSR are often publicly unavailable, making it difficult for researchers to develop a large-scale visual-based ASR system. Furthermore, very few databases contain the multi-view visual speech data which are important for constructing systems that take head-pose variation into account. In this section, we provide details of the English databases that have been recently or can be potentially used for research in visual speech decoding. Fig. 6shows some sample images from those databases.Lee et al. [51] introduced the Audio-Visual speech In a Car (AVICAR) database that was recorded in a moving car. They employed four cameras in a lateral array on the dashboard for video recording, resulting in four synchronized video streams with different views. Due to the limited space in the car, the angles between the views relative to the speaker were modest and the actual degrees unknown. There are 100 speakers (50 male and 50 female) involved in the recording and data of 86 of them are available for downloading. There were 5 noise conditions set up during recording. Under each condition, each speaker was asked to first speak isolated digits and letters twice. It was followed by 20 phone numbers with 10 digits each and 20 sentences randomly chosen out of 450 TIMIT sentences [127]. Video was recorded at 30fps with a resolution of 720×480pixels and audio sampled at 16kHz, 16-bit resolution.The AVLetters database [68] consists of 10 speakers (5 male and 5 female) uttering isolated letters A-Z. Each letter was repeated three times by each speaker during recording. Video was recorded at 25fps with a resolution of 376×288pixels and audio at 22.5kHz with a 16-bit resolution. Image data were processed such that a 80×60 full-face region was cropped based on the manually located center of the mouth in the middle frame of each utterance. Moreover, they temporally segmented each utterance such that it began and ended with the speaker's mouth in the closed position.Cox et al. [15] collected a higher definition version of the AVLetter database, named ‘AVLetter2’. The corpus includes 5 speakers uttering 26 isolated letters seven times. Video was recorded at 50fps with a resolution of 1920×1080pixels and audio as 16-bit 48kHz mono.Hazen et al. [36] produced the AV-TIMIT database for their studies of speaker-independent AV-ASR. The corpus contains 4h of AV data collected from 233 speakers (117 male and 106 female). The spoken utterances were chosen from the phonetically balanced TIMIT sentences [127]. Each speaker was asked to read 20 sentences and each sentence read by at least 9 different speakers. They chose one sentence that was uttered by all the speakers. Video was recorded at 30fps with a resolution of 720×480pixels and audio sampled at 16kHz. Unfortunately, the corpus was not made accessible for public research.Patterson et al. [90] recorded the Clemson University Audio-Visual Experiments (CUAVE) database that included speaker movement and simultaneous speech from multiple speakers. It consists of two major sections. In the first section, 36 speakers (17 male and 19 female) were involved in the recording. Each speaker was asked to utter 50 isolated digits while standing naturally and another 30 isolated digits while moving side-to-side, back-and-forth, or tilting the head. After that, the speaker was framed from both profile views while uttering 20 isolated digits. The individual then uttered 60 connected digits while facing the camera again. The second section of the database includes 20 pairs of speakers. For each pair, one speaker was asked to utter a connected-digit sequence, followed by the other speaker and vice versa a second time. For the third time, both speakers uttered their own digit sequences simultaneously. Video was recorded at 30fps with a resolution of 720×480pixels and audio at 16-bit, mono rate of 16kHz. The data was fully labeled manually at the millisecond level.The Grid AV corpus was collected by Cooke et al. [11]. There are 34 speakers (18 male and 16 female) involved in its recording. Due to some technical oversight, video data for speaker 21 are not available according to the information provided on the downloading webpage. The utterances are sentences with the form <verb>+<color>+<preposition>+<digit>+<letter>+<adverb>. There are multiple words that could be chosen at each position, resulting 1000 sentences per speaker in total. Video was recorded at 25fps with a resolution of 720×576pixels (a lower quality version (360×288) also available) and audio down-sampled to 25kHz with the peak SNR varying across speakers from 44 to 58dB. During recording, subjects were asked to speak sufficiently quickly to fit each sentence into a 3-second time window.The IBM Infrared Headset (IBMIH) Database [42] contains the infrared videos instead of those recorded under idea visual conditions. The speaker was asked to wear a headset containing an infrared video camera in front of the mouth and a microphone. Only the mouth-chin area was framed in the videos. The corpus consists of 79 speakers uttering continuous digit strings and another 113 speakers reading ViaVoice [95] dictation scripts (sentences). There are a total of 4011 utterances of digit strings and 12186 utterances of sentences. Video was recorded at 30fps with a resolution of 720×480pixels and audio at 22kHz.The IBM Smart-Room (IBMSR) Database [60] was collected as part of the European project, CHIL [60]. The corpus consists of 38 speakers uttering continuous digit strings. There were two microphones and three cameras used for AV data collection. The cameras were set to frame the speaker from the frontal and both profile views. Video was recorded at 30fps with a resolution of 368×240pixels and audio at 22kHz. There are in total 1661 utterances included in the corpus.The Language Independent Lip-Reading (LILiR) database [1] collected at the University of Surrey consists of 20 speakers uttering 200 sentences from the Resource Management Corpus [99]. The speaker was framed by two HD cameras from the front and profile views and by three SD cameras placed at 30°, 45° and 60°. It is unknown about the video and audio quality.The MOBIO database [69] was designed for evaluating face and speaker authentication algorithms on mobile phones. Videos were recorded from a mobile phone held by speakers. Consequently, the microphone and video camera were no longer fixed and were used in an interactive and uncontrolled manner. There are in total 152 speakers each of whom had multiple sessions of video recording. They were asked short-response questions, free-speech questions and to read predefined texts. Video was recorded at 16fps with a resolution of 640×480pixels.Zhao et al. [120] recorded the OuluVS database for visual-only ASR. It consists of 10 daily-use English phrases uttered by 20 speakers (17 male and 3 female). Each utterance was repeated by a speaker up to nine times. Video was recorded at 25fps with a resolution of 720×576pixels.The XM2VTSDB database [73] was collected at the University of Surrey for personal identification. There were 295 subjects involved and the recording consisted of four sessions. In each section, each subject was asked to speak two continuous digit strings and one phonetically balanced sentence. The utterances remained the same in all the four sections.Table 2summarizes the AV databases used in the recent work on visual speech decoding. It lists the speaker number, utterance type, head-pose information, recent work conducted on the databases and their accessibility for public research. Table 3summarizes the visual-only ASR performance recently obtained on those publicly available databases included in Table 2.

@&#CONCLUSIONS@&#
