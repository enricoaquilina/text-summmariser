@&#MAIN-TITLE@&#
Automatic melody composition based on a probabilistic model of music style and harmonic rules

@&#HIGHLIGHTS@&#
A music compositor that generates new melodies of a selected music style is presented.Tempo and time signature are estimated.Reusable patterns are used for style-based music composition.A probabilistic model of rhythmic and pitch patterns is used for composition.Rules based on music theory concepts support the music composition process.

@&#KEYPHRASES@&#
Automatic melody composition,Music style,Music analysis,Probabilistic model,Machine learning,

@&#ABSTRACT@&#
The aim of the present work is to perform a step towards the design of specific algorithms and methods for automatic music generation. A novel probabilistic model for the characterization of music learned from music samples is designed. This model makes use of automatically extracted music parameters, namely tempo, time signature, rhythmic patterns and pitch contours, to characterize music. Specifically, learned rhythmic patterns and pitch contours are employed to characterize music styles. Then, a novel autonomous music compositor that generates new melodies using the model developed will be presented. The methods proposed in this paper take into consideration different aspects related to the traditional way in which music is composed by humans such as harmony evolution and structure repetitions and apply them together with the probabilistic reutilization of rhythm patterns and pitch contours learned beforehand to compose music pieces.

@&#INTRODUCTION@&#
The main objective in the field of music information retrieval is to provide algorithms and methods for the analysis and description of musical pieces to computational systems [1]. The advances achieved allow computers to perform different tasks like automatic music transcription [2], or unattended genre classification, among others [3]. Furthermore, the computational model of the human experience in the musical field and the way in which humans process this information are topics of great interest for psychology and musicology [4].In this context, the automatic generation of musical content is the topic considered in this paper. Often, music is defined as ‘organized sound’ [5], this organization requires order, structure and a logical composition style learned by training [6]. Thus, a novel algorithm designed for learning composition rules and patterns will be shown in this paper. Specifically, three different musical aspects will be considered: time (tempo and time signature estimation [1]), rhythm (rhythmic pattern learning) and intonation (musical contour detection).Different methods for music style classification have already been described [7,8]. However, unlike cited works, in our case, the classification is not performed to train a classifier but to build a data model upon the selected descriptors of a particular music style. It is not the characterization of music styles what is ultimately pursued in this work but the utilization of the descriptions selected for the unattended generation of new melodies according to those styles.In other words, in this work, style-based composition parameters will be learned from the low level features for style classification. Thus, a main novelty that will be presented will be based on the analysis and post-processing of low level musical features to determine the style of the music that will be generated. The procedure developed improves the performance and adds a different level of complexity with respect to other approaches. An ad hoc database will be built from the melodic and rhythmic patterns found for different musical styles. Using this information, the proposed composer system will be able to generate new melodies according to the characterized style [9] in a way that will be similar to the one followed by a human composer.As mentioned above, a set of descriptors must be analysed in order to model the style of the melodies. Concerning the temporal descriptors, namely tempo and time signature, previous related works can be found. The work presented by Uhle and Herre in [1] is focused on onset estimation by means of spectral analysis and a subsequent study to estimate the length of the bar. On the other hand, in [10], histograms are employed to find the mostly repeated inter onset value in order to estimate the tempo without performing a spectral analysis. Also, there are recent methods for structural analysis, as the one described in [11], based on a time-span tree for the detection of structural similarity; this task can be addressed by making use of the auto-similarity matrix [12]. This method can be employed for both the extraction of music patterns and tempo estimation.The low level temporal estimation process can be related to certain stages of previous MIDI-to-score works like the one described in [13], which is focused on the complex study of the musical object in the MIDI representation through the analysis of the accent and beat induction. Unlike this work, the goal of the temporal estimation scheme that will be presented in this manuscript is the characterization of simple rhythmic patterns (melodies without rhythmic ornaments).The scheme designed in this work proposes an innovative approach for tempo estimation based on the inter onset interval (IOI) histogram. Our approach is inspired by [1,10]. However, some improvements are proposed in order to attain better accuracy and lower computational cost. Specifically, the proposed system uses a multi-resolution method that reduces the number of cost function evaluations and achieves better accuracy.Regarding musical composition based on pattern reallocation and variations, which is the key point of the present work, other methods are described in the bibliography. In [14–16], Markov models are used for the modeling and composition process. The use of genetic algorithms, such as in Biles’ GenJam system [17], has also been considered in the automatic music composition context. Biles’ system and ours are based on learning musical sequences (rhythm and pitch) plus a mutation. However the system we propose does not require a fitness evaluation stage since every learned pattern can be adapted to fit the composition. Furthermore, GenJam implements pattern mutation capability trough genetic algorithms which increases the flexibility and variability of the content. However, this feature cannot be applied in our approach since our scheme pursues style-replication.Methods based on probabilistic approaches such as Cope’s Experiments in Musical Intelligence (EMI) [18,19] also focus on the creation of an automatic music composition framework. Both EMI framework and the proposed system are based on probabilistic models (for pattern selection) and the training data stored in the database comes from musical features extracted from MIDI files (rhythm, pitch, dynamics,…). However EMI performs beat-to-beat pattern matching while our approach makes use of complete measures. Although both schemes use actual motives, the use of longer patterns, as in our approach, provides a clearer style replication. Moreover, unlike Cope’s approach, that generates non-tonal compositions, the proposed system defines a harmonic and rhythmic structure to compose tonal music.Another music generation method based on probabilistic structures is Inmamusys [20]. This method follows a global scheme similar to the proposed system. Both schemes use previously learned patterns for generating new compositions by replicating and reallocating them. However, the learning stages and features are different, the composition rules are distinct and, also, our system performs a post-processing process to allow all motives in the database fit in a composition. The solution to this problem adopted in Inmamusys is to restrict the combinations to a subset of motives that had been previously tagged as compatible.Fractals, fuzzy logic and experts systems have also been used for the composition of melodies [21]. However, unlike the proposed system, the melodies generated by the scheme described in [21] are not related to any learned style and the composition scheme is radically different.Summing up, the main contribution of the composition stage of the proposed system is the reduction of the importance of fitness when reallocating musical motives. Biles solves this issue by using genetic algorithms that force the mutation of the patterns to fit [17]. In our proposal, the reallocation of randomly selected patterns plus a global melody contour arrangement stage allow all the elements to fit together. The global music arrangement stage, which is crucial in our work, is based on music theory rules.Additionally, a set of automatic processes for the extraction of musical features from MIDI files has been designed to learn the musical patterns required by the composition scheme. Thus, we have designed a key-independent, unattended and style-based pattern learning system plus a composition process that replicate the style of the training data assuming that the musical style is related to rhythmic patterns and melodic contours. Although harmony and chord progressions are also closely related to music style definition, the chord analysis was postponed for further development and it is placed out of the scope of this manuscript.This paper is organized in five sections. After the introduction, in Section 2, a description of the analysis module will be presented. In Section 3, the automatic composer will be explained and a complete specification of all its subsystems will be given. In Section 4, the tests done to evaluate the system and the results obtained will be drawn. Finally, the last section will present the conclusions coming out from this work.The selected approach for the design of the generation scheme is based on the music theory method called obstinato[9], as described in Sections 1 and 3. This method considers the composition of music on the basis of the repetition of patterns so that the repetitions themselves constitute the melodic structure. In order to learn the patterns that will be used to compose new melodies, it is necessary to analyse the available samples to identify and extract the required elements for the composition, namely: rhythmic patterns, pitch contours, harmonic progressions and tempo information (needed for the proper analysis of the data). Similar developments are proposed in [22].Thus, a database of musical parameters is designed to model musical styles (as in [23]) and organize and store the data that will be used for the composition of new melodies. Since the main objective is to develop a valid music model for the automatic creation of composition with style replication, it is necessary to discover which parameters can be used for the proper modeling of musical styles. We decided to base our approach on the probabilistic analysis of musical elements such as rhythm, pitch and harmony. Fig. 1presents a scheme of the analysis system.Tempo can be extracted easily by analysing MIDI meta-data messages [24]. However, this piece of information can be missing (undefined) or incorrect. In order to develop a generally usable robust scheme, an algorithm to estimate the tempo and time signature has been designed. Note that tempo information is critical in order to correctly identify rhythmic figures.Rhythm, pitch motives (melody) and harmony (which is considered at the generation stage) are main pieces of information necessary for music generation [25]. This information will be coded and stored in the database.The database will be divided into three levels hierarchically organized containing (1) time signatures, (2) rhythm patterns, and (3) pitch contours (see Fig. 2). The symbolic notation used in this database to store the duration of the rhythmic figures corresponds to the ratio between the lengths of the notes and the length of a whole note. In this way, figure identification is tempo independent and the complexity is reduced. So, the whole notes will be denoted as 1, the half notes as 0.5, and so on. Recall that the specific duration (in s) of the figures is important only when the waveform is created. Melodic contours will be converted to the equivalent MIDI note number [24].As an example, the rhythm instance associated with a 2/4 measure with C–D–E notes being a quarter note and two eighth notes will generate an instance[606264]under the element[0.250.1250.125]that belongs to the element[24]of level 1 (Fig. 2).The hierarchy of the levels in the database has been established from the most static to the most dynamic feature in a melody. First, note that although the time signature can change in the same song, this possibility is not taken into account in the present analysis. A time signature is extracted for each song, it will be the responsibility of the user to feed the training system with correct data. If this condition is not fullfilled, rhythmic patterns will be wrongly learned and the style characterization scheme will be mistaken.The rhythmic patterns define the song structure. They are expected to be repeated during the song, so the analysis process will discover and count the number of instances of each different rhythmic pattern. Note that each measure can have a different pitch contour. The probabilistic model defined is obtained by counting the number of instances of each rhythmic motive in the song analysed. There are some very common style-independent rhythmic patterns, conversely there are other rhythmic patterns well related to a particular music style. Observe that it would be possible to identify music styles by using this probabilistic model [26]. However, music style classification is not in the scope of this work, but the models are. Thus, our system will estimate a probabilistic distribution of the appearance of rhythmic patterns for modeling each style. As it will be explained later, these probabilities found for each rhythmic pattern will be used by the composition scheme independently for each of the measures that compose the generated musical piece.In Fig. 3, the distribution of the appearance rate of the rhythmic patterns1The 36 patterns presented in Fig. 3 were obtained by using the rhythm extraction module shown in Fig. 1. In further sections, this system will be described. The samples used were MIDI files created manually and extracted from [27]. These excerpts can be classified as Academic music, i.e. classical music used for teaching and learning in Western Conservatories.1of two different music styles can be observed. Each of the patterns correspond to a complete measure. As it will be described later, the segmentation into measures (controlled by the time signature estimation) defines the rhythmic patterns that will be stored and used for music modeling and composition. The comparison shows the results of the analysis of two classical academic music datasets. The upper figure shows a higher level of complexity (corresponding to advanced music learning courses), while the lower one is simpler since it was obtained after the analysis of basic music learning lessons. These figures illustrate the presence of some common patterns, also, the different patterns appear with different probability or they do not appear at all. A main hypothesis in the present work is that the absence or presence, with specific probabilities, of certain rhythmic patterns can characterize a musical style [26].The composition scheme will be based on the probabilistic representation of the rhythmic patterns, so that the melody generation system will select rhythmic patterns to replicate a style: a new musical composition will be created in which the distribution of the rhythmic patterns will be similar to the ones in the training data set of the target style.Finally, the lowest level in the database corresponds to the melodic contour. This is the most variable feature in a song and it is related to the style through the analysis of the tones and the intervals in a song. However, the tone and interval histograms are more related to chords, harmony and tonality.Recall that the probabilistic distribution of rhythmic patterns is considered to be the basis of the music style model. However, the pitch contours will be needed for music generation, so they are also analysed and stored in the database. The probability of appearance of each pitch contour will not be stored. Thus, all the pitch contours detected in a given music style for a certain rhythmic pattern will be equally considered. Details will be drawn in Section 3.2.Now, we describe in detail the analysis stages that extract the information used to create the music characterization database described.First of all, tempo and time signature have to be estimated from the MIDI file in order to correctly perform a bar separation process to properly split rhythmic patterns (since the measure length is considered the minimum element for the composition of music in this work, as in [20]). The information that will be used to this end will be the inter onset intervals (IOI) of the notes. Note that the estimated tempo will be used only in the analysis stage. The tempo is neither learned nor stored since the same musical composition can be played with different tempos.The estimation stages are described next.The proposed algorithm for tempo estimation is based on the work presented in [10]. However, in our scenario, there is no guarantee that a percussive signal will be available to help the tempo estimation process (observe that this type of signal is commonly periodic, so that the pulse can be easily estimated). In our case, the analysed IOI’s are directly extracted from the melody, which does not necessarily follow a stable periodic beat. However, the relation between IOI’s will be a divisor of the fundamental beat, or tactus [28,29].Initially, the IOI with the lowest value in the histogram, which represents the appearance of IOI values of a certain melody, will be used to approximate the beat (it will be the initial value of the fine search that will be defined later, see Fig. 4). However, some considerations must be done:•Rests are not explicitly extracted. MIDI files contain information about the notes solely (Note On and note Off events [24]). However, resting periods are indirectly extracted and taken into consideration to properly estimate the tempo.The tactus estimated can be too short or too long. Nevertheless, the value estimated will be a multiple or a divisor of the actual tactus. Considering a certain logical range for the tempo, this value can be adjusted.Because of the discrete nature of the histogram [10], the tactus obtained will differ from the real one, so a novel post processing or a fine adjustment of the tactus based on the minimization of the onset deviations is performed. The method proposed in [10,1] is based on the evaluation of the cost function for each of the tactus candidates defined in a temporal window around the initial estimation. The tactus that attains the lowest error is selected. In order to achieve the desired accuracy the time step between candidates should be small, and the time window around the initial candidate should be wide (related to the histogram resolution). Thus, this scheme is computationally expensive. This fact is a main drawback when a large number of long melodies are used for training.The method proposed in this paper reduces the computational load by means of a multi-resolution analysis scheme of the candidate temporal window. The algorithm proposed evaluates only a subset of the candidates in the selected temporal window.2The time window is the same as in [1]: a 66% of the initial tactus.2The algorithm divides the window into four intervals. For each interval, 5 candidates (equally separated) are evaluated. The interval with the tactus candidate that attains the lowest error candidate is divided again into 4 zones, and like in the previous step, 5 candidates are evaluated. This process is repeated 6 times. These parameters were heuristically selected after performing a large number of tests.The cost function employed computes for each onset the time deviation between the real onset (obtained from the MIDI) and the nearest onset when a certain beat is assumed. Thus, the normalized cumulative temporal deviation of the j-th candidate (or, simply, error,ej) is defined as follows:(1)ej=1Tj∑i=1nmink(|tj(k)-IOIi|)where n is the number of notes in the MIDI stream,Tjis the j-th tactus candidate,tj(k)is the multiple of the j-th beat candidate that is the nearest one toIOIi, andIOIiis the i-th inter onset interval. The objective is to find the tactus that minimizesejto define the final tempo.Note that since smaller tactus generate smaller deviations, the error function is normalized by the candidate value. Fig. 5shows a comparison between normalized and non-normalized errors.The proposed algorithm is both less computationally expensive and more accurate than the approach in [1]. In Table 1, a comparison between the algorithm proposed and the one in [1] is shown.Our system reduces the number of cost function evaluations significantly (specially for large tactus values), and improves the resolution (separation between consecutive candidate tactus).In Fig. 6, the cost function for the tactus candidates is presented together with the initial tactus estimation and the optimized one.The method described is useful for general MIDI files: MIDI files synthetically generated (quantized tempo) and actual performances, in which the note durations have slight variations due to the human performer.Once the tactus is found, it has to be associated with a certain rhythmic element in order to obtain the duration of the quarter note (by convenience) so as to express the tempo in quarters per minute. Due to the wide range of tempos in music: from Largo (40–60quarters per minute) to Presto (180–200quarters per minute) [30], the tempo range needs to be restricted in order to establish an accurate relation between durations and musical elements. This requisite is illustrated by the following observation: a tactus of 0.25s can correspond to a sixteenth note at 60quarters per minute, and to an eighth note at 120quarters per minute.We will assume that the tempo of the training data is Moderato (76–108quarters per minute). Nevertheless, the system will generate a mapping of figures by means of the calculation of the duration of each rhythmic figure using the set range. If the number of quarter notes per minute is between 76 and 108, then the mapping will be as shown in Table 2.If the tactus obtained is labeled as too short or too long (Table 2), then the system assumes that the tempo was incorrectly estimated and a new range will be considered. If the tactus is too short, the tempo range will be raised to Allegro values (105–132quarters per minute). On the contrary, the range will be decreased to Adagio (40–80quarters per minute). As stated in [31], the estimation algorithm often estimates a doubled or halved tempo. This behavior is caused by the perceptual tempo concept [32]. This means that the estimation of the real tempo is subjective: some editors may use a shorter rhythmic figure and reduce the original tempo, while others can do the contrary for the same composition (with the same performance velocity). So, this type of ‘error’ in the tempo estimation scheme is not really important.In order to evaluate the algorithm proposed, two tests have been performed. The first experiment was designed to evaluate the accuracy of the tempo estimator leaving aside the influence of the human performance. In other words, the input dataset was a group of 30 s MIDI files (extracted from [27]), with known tempo, synthetically created (using Steibergs’s Cubase 5.0) so that the duration of each of the rhythmic elements was exactly quantified according to the tempo (e.g. if the tempo is 100bpm, then the quarter note duration is exactly 0.6s). The results of the first set of experiments are shown in Table 3.The second experiment uses MIDI transcriptions of actual performances. In this case, the durations of the rhythmic figures are not quantized. Actually they are affected by normal deviations derived from the human interpretation. This means that the onset positions are not perfectly aligned with the beat and the durations are not exact multiples or divisors of the tactus.This second experiment is carried out using MIDI files corresponding to the same melodies as in the previous experiment but, in this case, played by a human performer and recorded using a MIDI controller (Kawai CA91). The results of this experiment are shown in Table 4. It can be observed that the system accurately estimates the tempo without a percussive reference and non quantized IOI’s.After the tempo has been obtained, the duration of the metric figures is well known and it is possible to decode a MIDI file into a sequence of musical figures. Specifically, some musical rhythmic features can be extracted in order to estimate the most probable time signature. Using that information, the MIDI file can be split into measures, which are the basic ingredients required by our scheme to compose music.The estimation of the time signature is based on the detection of repeated structures in a melody. The repetition of melodic and rhythmic patterns is a common practice when composing music. In our scheme, bar repetitions will be observed by using a multi resolution analysis [33,34] to find the bar length that best suits the input melody among a set of candidates. The main idea is to split the melody into bars of different tentative lengths.After performing the different splits, some descriptors related to a statistical analysis of the split bars are extracted. Our scheme uses the Rhythm Self Similarity Matrix, RSSM, as described in [35]. The rhythmic elements will be split into the number of tactus corresponding to their durations in order to build the RSSM using the tactus as measurement unit. Thus, the inter onset interval (IOI) of each note is divided into tactus (beats), as illustrated in Fig. 7.Each figure subdivision will be labeled according to the duration of the figure (number of beats). Rhythmic elements shorter than the pulse will be labeled 0. The rests are included in the inter onset intervals (IOI). In the following equation, a numeric representation of this labeling procedure, using the example shown in Fig. 7, is presented:(2)IOI‾=[0.5,0.375,0.125]⇒IOI′‾=[4,4,4,4,3,3,3,1]whereIOI‾is the original inter onset interval vector andIOI′‾is the labeled beat vector. 0.5 is divided into four beats and labeled as 4 (assuming that the pulse corresponds to an eighth note, a half note is subdivided into 4), 0.375 as 3, and 0.125 as 1. So,IOI′‾can be defined as:(3)IOI′‾=[IOI′(1),…,IOI′(j)]forj=1,…,MwhereIOI′(j)is the j-th tactus from the transcription and M is the number of tactus in the input MIDI file.The inter onset interval vector obtained is split according to the length of the different beat candidates. These lengths will correspond to a number of beats ranging fromk=2tok=12(note that the candidate index is the same as the measure length associated with the tactus candidate:λk=k, whereλkis the measure length associated with the k-th tactus candidate). These values were defined according to the most common time signature numerators [36].This process will create a set of vectors called Bar Split Vectors that will be denotedBSVik‾and defined as:(4)BSVik‾=[IOI′((i-1)∗λk+1),…,IOI′(i∗λk)]fori=1,…,Nwhere k indexes the tactus candidate, i identifies the measure vector obtained from the splitting ofIOI′‾in i-th place related to the bar length associated with the k-th tactus candidate,λk, and N is the number of measures found.After splittingIOI′‾into bars for all the candidates, the self similarity matrix,RSSMk, corresponding to the extracted bars will be computed. The matrix defined in this work is boolean: the element in the i-th row and j-column,RSSMk(i,j)(i,j∈[0…N], where N is the number of split bars), will store the similarity between the i-th and the j-th bars, which is defined as:(5)RSSMk(i,j)=1BSVik‾=BSVjk‾0BSVik‾≠BSVjk‾RSSM examples of a certain content are shown in Fig. 8.Using the different RSSM’s computed, the following descriptors are extracted:•Number of repeated bars: cardinal that counts the bars repeated in the split performed for a certain length candidate. The most repeated bar should perform a proper separation with higher probability.Number of repeated bar instances: average number of instances per repeated bar. The larger the number of instances of a particular bar, the more probable the separation will be considered.Number of ties between bars: number of notes divided between two bars. The lower the number of ties between bars, the more probable the separation should be. However, note that it is common to find ties between bars in some scores. Thus, this descriptor is not critical, nevertheless it can provide a useful piece of information: a large number of ties between bars is a sign of wrong separation.Number of detected bars: The number of bars detected is not a strict constrain, however, the number of bars is a power of two in most cases.Observe that the longer the bar candidate, the lower the number of detected bars for the sameIOI′‾. In order to take into account this fact, the number of repeated bars, bar instances and ties is normalized by the total number of bars detected.In order to classify the time signature using these descriptors, we make use of the classifier based on the Sequential Minimal Optimization (SMO) algorithm [37] of Weka [38]. The classifier was trained with 10-fold cross validation with a dataset comprising 100 instances. These instances were extracted after the analysis of 100 MIDI files coming from the human performance of 100 different scores using a MIDI keyboard (Kawai CA91). The scores were obtained from music textbooks studied during official music studies at Spanish conservatories [27].The time signature of these instances was manually labeled according to the number of tactus per measure. In this process, two important considerations must be taken into account: first, since the goal of the time signature module is to find the optimal measure length rather than to identify the actual time signature, signatures like 2/2 and 4/4 or 3/4 and 6/8 will be considered to be equivalent since their measure length is the same. Second, the tactus can vary from score to score, depending on the music score edition. A score with tactus an eighth note, can be rewritten using a quarter note as tactus. So, there will be cases where the tactus estimated is doubled of halved, and the number of tactus will not strictly coincide with the ground-truth. These cases could be considered a mistake, however, a more relaxed criterion can be used since the main objective of this stage is to define a bar length to divide a composition into measures with musical meaning. Thus, the estimation of linear combinations (specially power of two) of the length of the actual measure is acceptable because the patterns discovered will be meaningful from the musical point of view as they will belong to the same rhythmic structure [36]. The distribution of the ground-truth time signatures used for training is shown in Fig. 9.The cross-validation process attained a success rate of 69% in the classification of nine bar lengths. In Table 5, the confusion matrix is presented. It must be noticed that most of the mistakes appear between doubles and halves3In the case of the Three, Twelve is four times Three. So it can also be considered a correct combination of measures.3(circled numbers). If these results are considered as correct, then the success rate becomes 93%.Rhythm is probably the musical aspect more closely related to the definition of the structure of the composition. Also, the particular use of special rhythmic sequences like syncopations and ornaments can be related to the definition of music styles.The rhythm estimation system designed works with any time structure. Note that if the tempo or time signature were not correctly obtained, the rhythmic patterns that will be found will not be the correct ones, however, they will be perfectly useful for the composition stage.The parameters obtained by the tempo estimator module (see previous section) are used to quantize the duration of theIOI′‾of the MIDI file. In order to coherently store different patterns obtained with distinct tempos (different MIDI files with different speed), the patterns are scaled to a common convenient tempo: 240bpm (any other value could have been selected). At this speed, a quarter note lasts 0.25s and a whole note 1s. Thus, the scaling transformation can be expressed as:(6)T′=T14bpm60whereT′is the new duration of the figures, T is the original duration in seconds in the MIDI file, and bpm is the actual tempo in quarters per minute.At this point only the IOI information is obtained, rests are missed. In order to identify all the rhythmic elements in the melody, including the rests, some processes should be done: first, the difference between the Note Off and Note On events is used to determine the duration of the note, and the difference between the Note On event and the previous Note Off event is used to define the separation between two notes. Second, the duration information vector is created by the concatenation of the duration samples together with the separation information. If the separation is 0, no extra element is added; otherwise, the corresponding duration is added and treated as a common note (in the duration vector). At the same time, a pitch vector is generated. Both vectors have the same length and the rests are identified with null pitch in the pitch vector.Now, splitting into measures is easily accomplished by applying thresholds to the cumulative sum of the values in the duration vector and using the outcome of the Tempo Estimation Module [30]. The measure splitter can face two different cases: (a) the accumulation of durations equals the threshold and (b) the accumulation overpasses the threshold. In the first case, the measure splitter immediately divides the bar since the notes detected make a complete measure. The second case implies the existence of a tie between bars. However, as the composition method proposed in this work is based in the relocation of complete measures, the note durations must be fixed by removing the tie through the bar line. The note tied across the bar line will be split into two notes: one with the proper duration to complete the previous bar and another one with the remaining duration, which will be part of the following bar.The rhythmic patterns obtained by the splitting scheme are stored. Every time a rhythmic motive is repeated, the new pitch contour is linked to the rhythmic instance. Patterns with more contour versions have appeared more times, and they will be selected with higher probability than others by the composer system, replicating the probabilistic model of the rhythmic patterns in the composed melodies. In Section 3, the probabilistic model established at this point will be used for the pattern selection routine, since the random behavior of the rhythmic content should be replicated in the output musical compositions.The pitch information is easily obtained from the MIDI messages. However, the pitch contour [30] is more important for the generation scheme than the notes themselves. As a matter of fact, the exact notes are not really necessary because a harmony corrector has been implemented in the melody generator to adapt the melody to the chord progression.In order to maintain the personality and the style of the reused motives, the pitch contour must be preserved [39]. However, the use of variations instead of the unmodified patterns provides further flexibility. The patterns can be adapted to new harmonies and the output melody can be set up to any desired key signature.In our system, each discovered pitch contour defines a usable pitch contour for a certain rhythmic pattern of a given style. Recall that the appearance rate of the rhythmic patterns is learned since it is crucial for the definition of the music style [26]. Conversely, the measured probability of appearance of the usable pitch contour, is not considered. Thus, all the possible pitch contours for a certain rhythmic structure and style are considered equally likely.After the analysis stages, a database with rhythmic and pitch information and predefined chord progressions will be available. The Melody Generator will use all this data to create new melodies replicating the style of the songs analysed. The Melody Generator designed is based on the concatenation of patterns in the database.The user can set the size of the score, the initial tonality (the proposed system is able to introduce tone modulation [36] by changing the tonality along the score), the time signature and the database of parameters corresponding to a certain style. Furthermore, rhythmic repetitions and harmony progression can be defined by the user or selected from a list of predefined sequences. The specific parameters that can be selected by the user are presented in Table 6. Note that the user can select the tempo and the instrument that will be used to play the generated melody. However, this choice is not actually considered part of the melody generation scheme since the same melody can be played with different instruments and tempos.The system makes use of some rules to guide the pattern selection process, ensure harmony adaptation and guarantee the continuity of the pitch contour.Concerning the pattern selection, our main assumption for style replication is based on the probabilistic model of the rhythmic patterns and the selected pitch contours. On the one hand, we assume that there are some basic rhythmic patterns that appear independently of the style of music, but there are also some particular rhythmic structures that commonly appear in a particular style. On the other hand, we assume that all the pitch contours found during the analysis stage for a certain style and rhythmic pattern are equally probable [26]. So, the histogram of the rhythmic patterns is obtained (see Fig. 12). Then, the selection of rhythmic patterns will be performed randomly, according to the probability distribution derived from the histogram found for a certain music style. Afterwards, a suitable pitch contour will be selected.A main problem found when concatenating independent patterns is the melodic gap between them. This gap is perceived annoying by both trained and untrained musicians. According to Narmour’s Realization–Expectation model [40], there is a number of facts that objectively make the listener like or dislike the music played. This is related to the expectancy. If the sequence of tones is unexpected, it will be more likely that the listener will dislike the melody, and vice versa.In order to fix melodic gaps between patterns, the system makes use of an automatic method for the evaluation of the music expectancy and the adaptation of the melody based on Schellberg’s simplification of Realization–Expectation model [41]. This method will be described in depth in further sections.Finally, an automatic method for harmonic mutation is implemented. The usage of a chord-based database would restrict the utilization of certain patterns to the cases in which the target chord is found. This approach would require a massive training with melodic patterns in order to have enough elements in the database to guarantee a sufficient level of variability in the output melodies. Furthermore, the Narmour’s expectancy adjustment would not be as accurate as required. So, a harmonic adapter has been implemented. The designed approach is based on the harmonic adaptation method based on level changing [42] defined in the classic music theory literature.Thus, the process to create a melody using the learned database starts with the definition of the score parameters (time signature, tempo, tonality, length) (step 1 in Fig. 10) together with the rhythmic and harmonic structure. The selection of time signature implies the selection of a subset of the database for the subsequent steps (see Fig. 2) according to the information extracted from the samples analysed. Then, the required rhythmic patterns are selected to fill the score slots (one pattern per each measure in the score). For each rhythmic pattern a contour is selected from the database. Next, the melodic line gaps are fixed using the expectancy method. Later, the harmony is adapted. A graphical representation of this process is depicted in Fig. 10, a block diagram is presented in Fig. 11and details on each of the steps will be given in the next sections.The user can design a particular chord progression from scratch. This choice would probably, restrict the usability of the composition scheme to musicians and people with strong musical background (the chord progression is a very important parameter for the musical success, there are combinations of chords that do not sound well together while others do [36]). So, a library of harmonic progressions that sound well together has also been defined [43]. The selection of harmonic progressions is performed at step 2 in Fig. 10.The predefined progressions used in our system follow the Western music theory and have been taken from [43]. These are presented in Table 7.The fact that some harmonic progressions sound well while others do not depends on the listener’s expectation [40], which is related to the cultural environment and the listener’s preferences. According to some harmonic progression rules in Western classical music, tonic chords (I and vi) can evolve towards any chord, dominant chords (V, vii, and sometimes iii) must go to tonic and pre dominant chords (ii and IV) must go to a dominant chord. The predefined progressions used in our system (Table 7) follow Western music theory rules and have been taken from [43]. Note that these rules cannot satisfy the expectation of a non-Western listener, and they could perceive that this progression does not sound well.Additionally, a set of rhythmic structures at phrase level is also provided. These structures can be selected by the user and, then, the composition configuration will be set. In Table 8, the set of rhythmic structures provided is presented. Note that the configuration can be changed by the user. However, the structures provided are suggested to the user because they are commonly used.Finally, in order to adapt the patterns selected, an innovative algorithm for melody transposition based on music theory rules has been designed (step 4 in Fig. 10). This algorithm will be described in Section 3.3.Since the database contains several rhythmic patterns organized per time signatures, a subset of usable patterns that fullfil the time signature requirement will be selected. Among these patterns, several ones will be chosen according to the established rhythmic structure (step 2 in Fig. 10). For example if the rhythmic structure is A–B–B–A, only two patterns will be searched in the database, if the structure is A–B–A–C, like in Fig. 10, three different patterns will be searched.The previous establishment of the rhythmic structures means that the rhythms of the measures labeled with the same letter will be based on the same rhythmic pattern.In the next sections, the pattern selection process for each measure will be explained.The selection of rhythmic patterns is random using the estimated probabilistic pattern distribution (see Fig. 12as an example of a pattern distribution). The rhythmic patterns will be obtained according to the conditional rhythmic probability density function (pdf) learned from the analysis phase:(7)f(R|S)=∑i=1RsNRiNRTδ(R-Ri)where S represents the music style,Rsis the number of different rhythmic patterns discovered for the music styleS,NRiis the number of times the patternRiappeared in the training set andNRTis the total number of rhythmic patterns in the training set of style S.δ(·)is the Dirac delta function.Samples of a random variable defined by the pdf in Eq. (7) are drawn to select the rhythmic patterns that will be used by the melody generator. In order to obtain more natural results, the control module will not select the same patterns for measures labeled differently (A,B,C,…). Thus, the number of different rhythmic patterns to select will be defined according to the initial score configuration (step 3.1 in Fig. 10).After each measure in the structure has been assigned a rhythmic pattern, a pitch contour must be selected for each measure (step 3.2 in Fig. 10) among the ones available (level three of the database structure shown in Fig. 2).As already mentioned in Section 2, the relative frequency of the pitch contours for each rhythmic pattern and style are not stored. Thus, all the pitch contours discovered for each rhythmic pattern and music style are considered to be equally probable for the composition. Then, the following conditional probability density function of the pitch contour C given the rhythmic pattern R and the music style S is defined:(8)f(C|R,S)=1NCT∑i=1NCTδ(C-Ci)whereNCTis the number of different pitch contour patterns discovered in the training set for the rhythmic pattern R and the music style S.Cirepresents each of the usable pitch contour patterns for each rhythmic pattern in each style. Observe that the set of usable pitch contours is different (with different size and different elements) for each rhythmic pattern in each style.A sample of a random variable that behaves according to the conditional pdf in Eq. (8) is drawn for each measure to select the pitch contour to use for the composition.At this stage, the pitch progression selected may not (very likely will not) be in accordance with the proper harmony set up. At a later stage, the chord transposition system will adapt the selected contour to the harmony progression and it will ensure the continuity of the melodic line (step 4 in Fig. 10).The chord transposition system performs the proper changes to the notes to ensure that the harmony is the one selected and to guarantee the continuity of the melodic line according to the expectation model [40,41]. Our approach implements a level changing harmony adaptation method [42] based on music theory, plus additional constrains derived from the expectation model (stage 4 in Fig. 10).As stated before, the simple concatenation of independent patterns, like the ones selected according to the method described in Section 3.2.1, causes the appearance of transitions that do not sound natural to both trained and lay listeners. A simplification of the adaptation of the Narmour’s Implication–Realization model of expectation [40], done by Schellenberg [41], will be used to handle this issue. The idea is to generate a Narmour candidate that properly follows the melodic line in the posterior measure, after the harmonic adaptation. Thus, after a measure is completed, the first note of the following measure (under the hypothesis that the first note in the measure is a harmonic note) will be the nearest chord note to the candidate.In order to generate each new note, the system carries out perceptual analyses of the last two notes of each measure. In this context, perceptual means that they are relevant for the expectation of the continuation of the melody. These two notes (implication) are used to evaluate a third note (realization), which will be the candidate note [41]. Some perceptual descriptors used for the generation of the candidate note are the following [41]:•Interval: A small interval (less than a tritone) [44] implies that the next note should follow the direction of the pitch progression. A change in the direction would not achieve the expectation.Pitch jump: The pitch jump after a small interval should be similar to the previous one. For example, an interval of 1tone should be followed by another interval of 1 or 2tones in the same direction, according to the previous rule.Progression of the intervals:–If the implication interval is less than 2 semitones, then, the third note should be nearer the first note of the implication. This choice implies a change of direction of the pitch progression.After a change in the direction or a large interval, the realization interval should be smaller than a tritone.As a result, the first note of the pattern will be replaced by the candidate. If the first measure in the melody is being analysed, the first note will be replaced by the tonic. Then, the nearest progression that achieves the correct harmony will be created. The nearest progression is defined as the one that differs from the original progression in the smallest possible pitch distances and with the minimum number of note changes.Recall that the position of the notes is key for the chord transposition stage. So, the chord transposition block first identifies the chord notes (these are considered responsible of the harmony definition) and the non-chord notes (which do not necessarily belong to the chord, commonly called passing notes) [36]. This process is done by observing the position of each note in the measure. The notes placed in downbeats will be considered chord notes, the notes placed in upbeats will be considered non-chord ones.Note that, in some particular cases, the real chord notes are not placed in the measure downbeats [36], however, since the most important feature is the pitch contour, the model is simplified to consider the most common case, in which the chord notes are placed in measure downbeats.The chord transposition subsystem applies two different procedures to the two different types of notes:•Accented notes must belong to the established chord.–First chord note (or Narmour candidate [40]): This note is assigned to the nearest pitch that belongs to the chord.Secondary chord notes: In order to keep the original pitch contour, secondary accented notes are moved to the nearest pitch in the contour direction.Unaccented notes do not necessarily belong to the chord. In this case, the original interval between the previous note and the current note is replicated.The score shown after stage 4 in Fig. 10 draws the result of the application of the chord transposition process to the pitch contours shown in the same figure immediately after stage 3.2.When the pitch and harmony adaptation process is finished for every measure, the process of creation of a new melody is completed.

@&#CONCLUSIONS@&#
