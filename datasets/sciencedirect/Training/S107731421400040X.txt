@&#MAIN-TITLE@&#
Robust obstacle detection based on a novel disparity calculation method and G-disparity

@&#HIGHLIGHTS@&#
We propose a disparity calculation algorithm based on multi-pass aggregation and local optimisation.Disparity calculation is fast and accurate in real-world scenarios.We propose the G-disparity image which can be used with U–V-disparity for obstacle detection.Obstacle detection is more efficient and accurate.Free-space calculation is simplified after obstacle detection.

@&#KEYPHRASES@&#
Obstacle detection,U–V disparity,Free space calculation,Stereo vision,

@&#ABSTRACT@&#
This paper presents a disparity calculation algorithm based on stereo-vision for obstacle detection and free space calculation. This algorithm incorporates line segmentation, multi-pass aggregation and efficient local optimisation in order to produce accurate disparity values. It is specifically designed for traffic scenes where most of the objects can be represented by planes in the disparity domain. The accurate horizontal disparity gradient for the side planes are also extracted during the disparity optimisation stage. Then, an obstacle detection algorithm based on the U–V-disparity is introduced. Instead of using the Hough transform for line detection which is extremely sensitive to the parameter settings, the G-disparity image is proposed for the detection of side planes. Then, the vertical planes are detected separately after removing all the side planes. Faster detection speed, lower parameter sensitivity and improved performance are achieved comparing with the Hough transform based detection. After the obstacles are located and removed from the disparity map, most of the remaining pixels are projections from the road surface. Using a spline as the road model, the vertical profile of the road surface is estimated. Finally, the free-space is calculated based on the vertical road profile which is not restricted by the planar road surface assumption.

@&#INTRODUCTION@&#
Obstacle detection has been an active research area for two decades. It is widely used in many Advanced Driver Assistance Systems (ADAS) and Intelligent Transportation Systems (ITS). The most important goal is to accurately locate obstacles in front and measure the distances to the obstacles. Existing solutions are based on either active sensors or passive sensors. Active sensor based systems provide simple and accurate distance measurements. However, they cannot produce results with adequate spatial resolution in real-time. Active sensors also face the problem of mutual interference. Although this problem has been solved by the pseudo-random laser modulation scheme [1], the LiDAR system requires an unreasonably high power to detect objects in the far-field if adequate spatial resolution is required. Therefore, our research is focused on passive sensor based obstacle detection.The passive sensor based systems can be further divided into three main types: segmentation-based [2–7], motion-based [8–10] and depth-based [11–16] obstacle detection. The segmentation-based solutions separate the obstacles from the background by identifying and analysing potential obstacle features in the scene. Various features have been used, such as colour [4], edge [6], edge symmetry [3], texture [7] and so on. The distances to the detected obstacles are normally estimated using fixed camera parameters and a flat-road assumption. Most of the motion-based methods are based on perspective transformation [9,17] and global-motion estimation algorithms [8,10]. Differentiating the motion of local image areas from the global motion introduced by the vehicle ego-motion allows the detection of objects. If the ego-motion of the vehicle is directly available, perspective transformation can be applied to warp the current frame,f(i), to produce an ‘expected’ next frame,f^(i+1), assuming a flat road. Then, for obstacle detection, the actual frame,f(i+1), can be compared with frame,f^(i+1). When the global motion is unavailable, algorithms such as optical flow can be applied. The global motion needs to be first identified by analysing the flow field. The positions where the motion flows do not agree with the global motion field indicate possible dynamic objects which are very likely to be obstacles such as vehicles or pedestrians [18–20]. Generally, both shape-based and motion-based algorithms are able to provide accurate detection. However, the distance measurements can only be achieved with a flat-road assumption (or accurate prior road vertical profile detection), due to the lack of direct distance measurement. In reality, only near-field roads can be treated as flat planes. Therefore, the accuracy of the far-field distance measurements become unreliable with vertical road gradient variations.Stereo vision-based solutions hold an advantage over shape-based and motion-based approaches. Direct distance calculations can be achieved by evaluating the disparities between the two images without any assumption of the road structure. By doing so, obstacles are detected based on the disparity maps instead of intensity images and so the process can be significantly simplified. However, calculating a dense disparity map requires a significant amount of computation which is burdensome for the overall system. Block-based methods evaluate the similarities of local image areas. The position where the highest similarity is achieved can be selected as the correspondence. Global optimisation-based algorithms include more constraints, such as a smoothness term. Combining the constraints with the cost function, a global energy function representing the goodness of the current match can be formed. Many optimisation techniques [21–23] have been applied to find the maximum/minimum of the global energy function with varying degrees of success, depending on the test images. However, the global optimisation process is normally computationally demanding and difficult to implement in real-time. Dynamic programming (DP)-based approaches [24,25] optimise disparity values based on a Disparity Space Image (DSI), generated using a scanline. It is difficult for this method to incorporate inter-scanline information and to relax the monotonicity or ordering constraint (described in [26,27]).When designing a disparity calculation algorithm, one of the most important steps is to decide the cost function. Traditional cost functions are generally based on intensity differences such as the SSD, MD and the SAD. Intensity difference based cost functions are the simplest as they do not include divisions and are suitable for implementation on fixed-point embedded systems. However, these functions are sensitive to the differences of the intensity gains between the two cameras. The NCC is also a popular cost function. Although the NCC demands higher computational complexity, it is definitely preferred when intensity differences are non-negligible. Other methods are also proposed, focusing on reducing the sensitivity towards intensity differences, including image gradient and non-parametric transforms (rank and census transforms) [28–30] based cost functions.One of the general problems during stereo matching is that errors are likely to be introduced in the homogeneous areas (pixels in a large image area that share very similar intensities). In automotive applications, large homogeneous regions exist on the road surface. This leads to the matching cost being unpredictable in various locations. The position corresponding to the minimum cost might not be the correct correspondence. Grouping the pixels together within the homogeneous region during the cost aggregation solves the problem but it is difficult to allow smooth and gradual disparity changes within the group. Another factor that induces error is the occlusion, which is introduced due to the position difference between the left and right cameras. Existing solutions are mainly based on the left and right consistency check or global optimisation [22,21], which are computationally complicated.In this paper, we propose a disparity calculation algorithm which readily solves the homogeneous area problem. It also has partial occlusion handling capability. The algorithm consists of four main steps: cost calculation, image line segmentation, cost aggregation and optimisation. Line segmentation is applied to the reference image horizontally and vertically. The calculated costs are aggregated in both the horizontal and vertical directions. The horizontal aggregation enables a correct correspondence to be found in the homogeneous and occluded areas. Vertical aggregation allows gradual disparity variation within each horizontal segment. Segment based optimisation is then performed to identify the final disparity for each pixel. The proposed technique is especially suitable for this application since a gradual change of disparity in the road area and side planes must be encouraged (many global optimisation-based algorithms involve a smoothness term which adds a small penalty even to gradual disparity changes). During the optimisation, the proposed technique also generates an accurate horizontal disparity gradient which simplifies the side plane recognition during obstacle detection.Once the disparity map is available, the potential obstacles in the scene can be extracted. Many existing systems assume the road surface is planar [11]. This assumption allows the reference image to be perspective projected and matched with the other image. The final intensity difference map reveals the potential obstacles. The biggest advantage of such systems is that the disparity matching step can be omitted. However, if the flat road assumption is violated, the system performance will be heavily affected. In order to relax the flat road assumption while saving computation, disparities are only calculated at certain feature points in [31–33]. However, sophisticated feature extractors are computationally complicated. Most importantly, a large amount of valuable distance information is not included and cannot be shared with the other components in the autonomous vehicle system. With fast growing computing systems, dense disparity matching can be achieved in real-time. One of the most significant contributions, based on a dense disparity map, is called the U–V-disparity map [13]. By calculating the line disparity histograms horizontally and vertically, the V-disparity and U-disparity images can be generated respectively. Each plane in the world coordinate system will be shown as a line on the U–V-disparity map. Categorising road surfaces and obstacles into different types of planes simplifies the obstacle detection problem into a line detection one. The Hough transform has been the most popular algorithm to extract lines (mostly corresponding to horizontal or oblique planes) in the U–V-disparity space [13,34]. However, the Hough transform requires pre-set parameters whose settings may cause serious over-detection or the neglect of small obstacles. It is also extremely difficult to find one set of parameters that works in the fast varying traffic environment. In [13], the detection of the road plane is carried out prior to the obstacle detection. The obstacles are found by evaluating the disparity difference in the road disparity profile. This type of algorithm assumes that a very large percentage of the scene is occupied by the road surface, so that the longest line in the V-disparity map corresponds to the road surface. However, in the cases when obstacles are in the near-field, the road surface can be blocked and this results in large errors.In our system, the detection is separated into side and vertical plane detection. Side planes are located first on the U-disparity and the proposed G-disparity, which is calculated based on the disparity gradient. The detected obstacles are removed so that minimum outliers are involved during vertical plane detection. This approach does not include sensitive parameter settings and it can be implemented very efficiently. Furthermore, the proposed obstacle detection does not rely on the prior road surface exclusion and allows the presence of obstacles in the foreground. Another advantage is that, once all the obstacles are removed from the scene, the road surface can be easily detected and modelled. As proposed in [35], the vertical profile of the road surface is modelled using a spline. In our system, the model parameters are then optimised using the least squares method. An extra refinement step ensures the boundary between the obstacles and the road surface is accurately defined.The rest of this paper is organised as follows: Section 2 introduces the proposed stereo matching algorithm. Section 3 focuses on the obstacle detection. Section 4 describes the road modelling and free space computation. Section 5 presents experimental results and corresponding discussions. Section 6 concludes the paper.First of all, we define the world coordinate system as shown in Fig. 1. The relationship between disparity and depth is a result of perspective projection. As shown in Fig. 2, defining a point in the 3D space as P and projecting this point onto the stereo vision system with a horizontal baseline B, the resultant points on the left and right image plane can be represented aspLandpRrespectively. The relationship between these two points and the depth Z can be found as shown in Eq. (1).(1)d=BfZwhere d is the disparity which can be calculated by the horizontal position difference betweenpLandpRon the image space and f is the focal length.In order to locate the correct correspondence for each pixel in the reference image, we need to utilise the intensity information surrounding the point of interest. Even calibrated cameras could include a small amount of intensity difference which could lead to incorrect correspondence. In the case where the pixel disparities within a homogeneous region are identical, this problem can be solved by image segmentation and restricting the disparities of pixels within the same segment. However, in obstacle detection applications, the disparity of the road surface changes gradually so that assigning a single disparity is not an option. Furthermore, changes in distance also introduce changes in object appearance from one image to the other which then causes a correspondent segment not being found. Another factor that induces errors is occlusion, which occurs due to the position difference between the left and right cameras. This causes background pixels to be visible in the reference image, but not in the other one (as shown in Fig. 3). In theory, the disparity of the occluded areas cannot be measured and should be excluded from the final result. Existing ways of excluding these pixels are mainly based on the left and right consistency check and global optimisation [21–23] which are computationally complex. However, in most situations, the disparities of occluded areas can be estimated with the disparity of the neighbouring background object.In this paper, a multi-pass cost aggregation approach based on line segmentation is presented in order to solve the problems introduced by homogeneous and occluded regions. The system block diagram of the proposed disparity calculation algorithm is shown in Fig. 4. As discussed in [34], planes within a normal road scene can be separated into horizontal, oblique, vertical and side planes. Although shapes of the road surfaces on the two images are different, the widths of the road areas on both images are identical. Therefore, line segmentation instead of region segmentation is implemented in our system to calculate the disparities of the oblique and vertical planes. Aggregating the costs horizontally allows vertical disparity changes while restricting the segment disparities using object widths. In order to cope with the occluded areas, a weighted aggregation is conducted to suppress the contributions of occluded pixels. When the object is a side plane, the disparity values change linearly in the horizontal direction. In this case, vertical line segmentation is also employed. By aggregating the cost within each vertical segment, the disparity of side surfaces can be calculated accurately. The final segment disparities are then optimised based on the multi-pass aggregation results. The horizontal aggregation results are first optimised using the inter-scanline information. After that, an energy function is evaluated based on the horizontal segment DSI s [36] to determine the final disparity for each pixel.Cost calculation provides the most basic information needed by all high-level algorithms such as the popular BP and the SGM in order to produce the final disparity map. While choosing the cost function, the most important factors are accuracy and speed. Some of the simple functions such as the SAD and the SSD can produce accurate results if the input images are well calibrated (including brightness) and contain rich texture details (many colour/intensity changes). As many of our test image sets from EISATS1Enpeda Image Sequence Analysis Test Site.1contain significant brightness differences between the left and right views, NCC is implemented due to its robustness against intensity changes. The NCC coefficient or costδn,m(d)of pixelI(n,m)can be calculated as given in Eq. (2).(2)δn,m(d)=1(2h+1)2-1∑x=n-hn+h∑y=m-hm+h(Il(x,y)-Il‾)(Ir(x+d,y)-Ir‾)σlσrwhereδm,n(d)represents the normalised cross-correlation coefficient at disparity d.Il‾andIr‾denote the mean of the intensities within the left and right blocks respectively andσrepresents the standard deviation. The normalising term ensuresδm,n(d)is insensitive to any intensity variations.Unlike the SAD and SSD, larger NCC coefficients correspond to the likely matches. Therefore, in this paper, disparities corresponding to large costs are regarded as desirable matches.If the Winner-Take-All (WTA) optimisation method is used to locate the maximum cost location, the disparity map can be produced as shown in Fig. 5.This is a typical road scene with vehicles and boundary fences. A large amount of error is produced in the homogeneous regions and occluded areas. By inspecting the cost of a pixel inside the homogeneous region (shown in Fig. 6), we can see that the local cost calculation does not reflect the correct match, and the maximum in this case corresponds to an incorrect disparity value. In the occluded regions, correct correspondences do not exist. Therefore, errors cannot be avoided at this stage.In order to illustrate the influence of a homogeneous region during cost calculation, the NCC coefficients of one pixel inside a homogeneous regions are shown in Fig. 6. As all intensities inside the region are very similar, the maximum corresponds to an incorrect position. Therefore, the costs of pixels inside an homogeneous area need to be aggregated to find the correct disparity values.The proposed cost aggregation method is based on the horizontal and vertical line segmentations. First, the gradient of the reference (left) image is calculated horizontally by using a Sobel mask. Then, a small threshold is applied to identify the intensity changes. This threshold is easy to determine since only the areas with very little texture need to be grouped. Once the vertical edges are found, any pixels between the two edge points are grouped into a segment. The costδn,m(d)can therefore be represented asδs(j,d), where s is the segment index and j is the pixel index within segment s.The distribution of disparity values within a homogeneous line segment can be separated into three cases:•The segment of interest corresponds to either the horizontal (parallel to the X–Z plane), oblique (the X-axis is parallel to this type of planes) or the vertical planes (parallel to the X–Y plane) in the 3D space, with no occlusions. These planes are all parallel to the X-axis. While projecting points on these planes to the image plane, only the ones lying on the same line parallel to the image plane will be projected onto the same image row. Therefore, resultant disparities within the line segment are identical.The horizontal segment also corresponds to either the horizontal, oblique or vertical planes (parallel to the X–Y plane) in the 3D space with occlusions.Finally, if the line segment corresponds to a side plane (planes that are perpendicular to the road plane), the true disparities gradually change horizontally. Assigning a single disparity value to all pixels in the segment is thus inappropriate. A multi-pass aggregation method is developed in our system to accommodate each case differently.For the first case, the DSI of a image line segment within a homogeneous region is shown in Fig. 7. Each pixel in Fig. 7 corresponds to a cost value (normalised correlation coefficient). The x-axis represents the number of pixels inside this line segment. The disparity range increases along the y-axis. Defining the disparity d and the pixel index j within a segment s as two random variables, the normalised costδˆsA(j,d)can be treated as a probability density of correct correspondence and is shown below as:(3)δˆsA(j,d)=δs(j,d)∬δs(j,d)djddThe denominator is a normalising term to ensure the probability density integrates to one. The objective is to find the cost of the whole segment depending on d and to eliminate the term j. This is achieved by calculating the marginal density function as follows:(4)CsA(d)=∫δˆsA(j,d)djwhereCsA(d)represents the probability of d being the correct disparity for the whole segment. If d results in a large number of high values inδˆsA(j,d), the integrated result will provide a large value at d and vice versa. The aggregated cost should contain a unique peak which corresponds to the correct disparity. Fig. 8illustrates the cost aggregation result of the segment containing the pixel whose cost is shown in Fig. 6. As the figure shows, the maximum position is now correct atd=22. This indicates that cost aggregation within a homogeneous region can result in a distinct peak which is more suitable when used to estimate the disparity map.For the second case, an example of the DSI of a segment including an occluded region is shown in Fig. 9. The costs in the occluded area are completely different from the rest. If the cost is aggregated using Eq. (4), there is a possibility that the final result will be influenced by the occluded region and cause errors. Therefore, it is important to ignore the occluded areas during cost aggregation. However, the determination of the occluded area from the DSI is not an easy task. In [36], the authors detect the occluded areas using Dynamic Programming (DP). Since occlusion detection needs to be performed on every image row, the overall computational complexity will be dramatically increased. As the left image is selected as the reference, occlusions can only occur on the right side of an object. In order to restrict the confidence of the costs of right pixels during the aggregation, a weight function is applied to give higher support to the pixels close to the left end of a segment while suppressing the cost contribution of the pixels close to the right end. In the proposed approach, a Gaussian weighting functionw1is employed. It provides more flexibility while tuning since the weight distribution is controlled by its mean and standard deviation values. Simpler weights function such as the ramp funtion can also be taken into consideration. In our case, linearly distributed weighting coefficients do not suppress the occlusion problem in cases of large (wide) segments. Therefore, a Gaussian function is employed for good searching range coverage on the left end of the segment and sufficient restriction of occlusion on the right side. When the Gaussian weighting functionw1is applied toδs(j,d), the normalisation process becomes:(5)δˆsL(j,d)=w1(j)δs(j,d)∬w1(j)δs(j,d)djddHence, another aggregation function is created as shown below:(6)CsL(d)=∫δˆsL(j,d)djIf j is normalised within the range[-11], the mean of the Gaussian function is set at-1and the standard deviation is chosen to be0.4. This parameter is determined by manually tuning the value and evaluating the resultant disparity map using selected image sets from the Enpeda Image Sequence Analysis Test Site. The small standard deviation indicates the weights at the right side of the segment are very low. This choice is based on the fact that some of the occlusion effect is severe, which can occupy a big proportion in a segment or even cover the whole segment. If the complete segment is within an occluded area, the aggregated result of Eq. (6) cannot indicate the correct disparity. However, the possibility of this occurring is generally low in a road scene.With the above two aggregation functions, the costs of pixels within a segment are aggregated twice. The aggregation results of Eq. (4) could be affected by the occluded areas but it produces excellent results in occlusion-free areas. The results of Eq. (6) are less sensitive to the negative influence of the occluded areas but may produce errors due to the exclusion of a large portion of pixels on the right side of the segment. In order to make sure that at least one of the aggregation results corresponds to the correct disparity, another aggregation is performed using Eq. (6) but with a different weighting function. This time, the mean of the Gaussian function is still at-0.7but the standard deviation is chosen to be0.8(decided by tuning the value manually and evaluating the experimental results). This function assigns higher weights to more pixels while preventing errors introduced by small occluded areas. The resultant cost normalisation function is shown as follows:(7)δˆsM(j,d)=w2(j)δs(j,d)∬w2(j)δs(j,d)djddThe marginal density is therefore:(8)CsM(d)=∫δˆsM(j,d)djThe final decision about which aggregation result corresponds to the correct disparity will be discussed in detail in Section 2.4.For the third case, in order to allow gradual changes within the group while generating reliable aggregated results, segmentation is applied again vertically with the horizontal edges. The heights of a projected line on a side surface should be identical on the left and right images. Therefore, if d corresponds to the true disparity, a vertical segment in the reference image should find the one on the right image with the same height to be the correspondence. If v and l are used to represent the vertical segment and pixel indexes respectively, the normalisation process can be expressed by:(9)δˆvA(l,d)=δv(l,d)∬δv(l,d)dlddThe aggregation of pixels inside a vertical segment is therefore:(10)CvA(d)=∫δˆvA(l,d)dlThe occluded areas are not considered during vertical aggregation since they only exist on the right side of an object and it is difficult to identify them in the vertical direction.The proposed multi-pass approach aggregates the cost four times in total. Each aggregation utilises different information, aiming to solve a specific problem. These aggregation results need to be further processed in order to generate an optimised disparity map. The four available aggregation results areCsA(d),CsL(d),CsM(d)andCvA(d)(Eqs. (4), (6), (8) and (10)).With the above four sets of cost functions, the WTA optimisation is applied to each set and produces four disparity mapsDWi, wherei=1,2,3,4as shown in Fig. 10. Each pixel should select its disparity among the four results. As Fig. 9 shows, each of the four disparity maps contains errors. However, most of the errors are not shared by all. For example,DW4produces accurate disparities for the side planes but a large number of errors is introduced on the oblique planes. The objective of the optimisation is to find the optimum value among the four for each pixel or segment. Many global optimisation methods (such as SA [37], SGM [21] and BP) can be applied to solve the problem. As the disparity levels are restricted to only four values, the required computation to achieve the final result is significantly reduced. In our system, a local optimisation method is conducted based on segments instead of pixels in order to further decrease the required computational power.Although the computational complexity of a 2D optimisation is greatly reduced after the cost aggregation, it is still difficult to achieve real-time performance. In our system, the horizontal segmented disparity maps (DWi=1,2,3) are optimised first. The vertical smoothness of local image areas are evaluated. These local areas are determined by the horizontal and vertical segmentation. In practice, it is reasonable to assume that the disparity within a vertical segment should be identical (such as in the obstacle areas) or slowly changing (such as in the road surface area). Any sudden changes are irregular and should be removed. On each of the three maps based on horizontal segmentation, vertical disparity differences are calculated. The locations where the differences are greater than 1 are labelled with 1s on a positive difference mapσ+iwherei=1,2,3. The locations where the differences are smaller than-1are labelled with 1s on a negative difference mapσ-i. The final uncertainty mapσican be calculated using information contained in both difference maps as follows:(11)σi(v,l)=min∑lσ+i(v,l),∑lσ-i(v,l)σiprovides an accurate measure of the number of distinctive disparity changes within a vertical segment. At pixel(v,l), the disparityDHcan be represented as:(12)DH(s,j)=DWip(s,j)whereσip(s)=mini∑jσi=1,2,3(s,j)whereDHis the optimised result based on horizontally segmented disparity maps. This optimisation process enforces the inter-scan relationship on each horizontal segment so that errors can be identified while a smooth vertical disparity change is not interrupted.AlthoughDHis now optimised, it is only based on the horizontal segment results. The disparity values representing the side planes are still inaccurate. In order to get the final disparity map, these disparity values inDHmust be replaced by the corresponding information stored inDW4(a disparity map calculated based on the vertical segments). Unlike the inter-scanline optimisation discussed above, the disparities for the side planes are selected based on the DSIs of horizontal segments.It is important to note that unlikeDH,DW4has not yet been analysed and optimised. It could contain errors introduced by insufficient pixels in a segment or occlusions. IfDHis directly compared withDW4without preprocessing, it is very likely to cause inaccurate optimisation. The objects most likely to appear on the sides of the vehicle could be other vehicles, buildings and fences. The side surfaces of these objects are approximately planar. Even in some extreme cases where the objects have curved side surfaces, it is very unlikely for the complete curvature surface to be a homogeneous area which cannot be separated during line segmentation. If the curved surface is segmented into a few segments, disparity changes within each segment can be treated as approximately linear. For the above reasons, the disparity contained in a horizontal segment ofDW4is restricted to be on a straight line on the DSI. The parameters of the line corresponding to horizontal segment s are calculated with least squares fitting based on smoothed and sub-sampledDW4(s)whereDW4(s)containsDW4(s,j)for all j. Since sub-pixel resolution is not required, the lines are then sampled, digitised and stored inDV(s,j). It is important to note that the value ofDV(s,j)changes according to the fitted line, whereasDH(s,j)contains the same value for different j. ForDH, each segment disparity is unique and so it shows a horizontal line on the DSI according to the disparity value. The confidence for the segment disparities being correct is evaluated by averaging the cost values crossed by the line as follows:(13)F(s,Dx)=∑j(δs(j,Dx(s,j)))JwhereDxis a disparity map which can be eitherDVorDH. Since the negative values of normalised correlation values are set to 0, the cost values vary from 0 to 1. J is the number of pixels within segment s.F(s,Dx)varies from 0 to 1 for each group. IfF(s,DV)>F(s,DH), the final disparities of the segmentD(s,j)are set toDV(s,j). Similarly, IfF(s,DV)<=F(s,DH), the final disparities of the segmentD(s,j)are set toDH(s,j).This line fitting process not only correctly optimises the disparity map, but also calculates the disparity gradients of segments. The detection of the side surfaces can be significantly simplified by utilising this information during the obstacle detection stage (discussed in detail in Section 3).Finally, the disparity values of the edge positions are filled with the WTA optimisation based on the NCC cost. High frequency components on the disparity map D are removed and interpolated. An example of the final disparity map is shown in Fig. 11and more experimental results and comparisons can be found in Section 5.With the disparity maps extracted earlier, we now focus on obstacle detection. Our work is based on the U–V-disparity representation [13] and it is extended for faster and more accurate detection. The projection of a disparity map in U-disparity and V-disparity domains can be treated as two histograms of disparity values. From this point, ‘the projection of the disparity map in V/U-disparity domain/space’ is generally stated as ‘U/V-disparity’. For the V-disparity, the histogram of each horizontal line is calculated. If the number of pixels sharing the same disparity on each line is represented by brightness, an image in which vertical and road planes are represented by lines is formed. An example of V-disparity is shown in Fig. 12. The longest diagonal line in the V-disparity represents the road plane. The vertical lines represent the obstacles with vertical planes. The length of each vertical line indicates the height of the corresponding obstacle. The U-disparity is similar to the V-disparity but it is the histogram of columns of the disparity map. Likewise, the U-disparity (as shown in Fig. 12) encodes the horizontal information of obstacles. The road plane is no longer visible but the side surfaces are included and are represented with non-horizontal lines. As carefully analysed in [34], the U–V-disparity includes essential information for most planes that would appear in a road scene. It reduces the time consuming plane detection problem to a much simpler line detection one. However, all objects are included in the histogram and it is difficult to extract an obstacle without the interference of other outliers. Many algorithms detect the road surface first with a flat road assumption. The relationship between the road plane and vertical planes can then be defined to restrict the pixels of interest. The Hough transform has been one of the most commonly used algorithms for U–V-disparity based line detection. However, it has been found in our previous work [38] that the parameters of the Hough transform are very difficult to determine in changing environments. It requires a threshold to binarise the U–V-disparity, another threshold for the Hough space accumulator, a parameter to determine the acceptable point spacing, a line orientation restriction and more parameters involved in the 2D peak detection. Even though the parameters are carefully selected based on a single set of test data, errors are unavoidable if shorter lines (far range obstacles) need to be detected. Including a post processing step is almost essential if the Hough transform is used. However, implementing this step indicates the inclusion of even more parameters. In summary, the Hough transform takes a large amount of computational power and memory space but cannot consistently produce promising results.In our system, obstacle detection is based on the U-disparity and the proposed G-disparity space (encloses disparity gradient information). The V-disparity is not used to extract the vertical profiles of obstacles. The most important reason to exclude the V-disparity is that the heights of the side planes are difficult to extract. Furthermore, errors will be introduced in a special case. If two obstacles with different heights have an identical disparity, the V-disparity will result in single line with changing intensities. Although the two obstacles can be separated horizontally by analysing the U-disparity, the difference in their height is very difficult to distinguish. A similar situation will happen to the U-disparity when two obstacles with similar disparity exist vertically. Fortunately, this happens very rarely in traffic scenes. In the proposed approach, the disparity gradient information extracted earlier is used, and the obstacle detection problem has been separated into two stages: side plane recognition and vertical plane detection. By doing so, the detection of obstacles can be faster and more accurate.The detection of side planes is a more complicated problem than the vertical plane detection since the orientations of the lines are not priorly available. When the Hough transform is used for line detection, it is almost essential to include the local edge orientation to achieve accurate and fast performance. However, the orientations of the edge points in the U-disparity are not accurate due to the discrete nature of the disparity map (low resolution in the disparity axis). In most cases, disparity changes slowly in the horizontal direction on a side plane. The disparity level only changes every few pixels. In this case, the orientations of the edges corresponding to the side planes are inaccurate and mostly vertical as shown in Fig. 13. Thus, a range of orientations needs to be allowed which increases the system complexity. Another problem is that once detection of small objects is allowed (represented by very short lines on the U-disparity), the horizontal segments of the side lines can be easily confused as small vertical planes.The proposed obstacle detection system extracts lines representing the side planes before the detection of vertical planes. Once the detected side planes are removed from the disparity map, vertical planes can be very easily extracted. During the disparity calculation stage (discussed in Section 2), the disparities of some of the horizontal segments are selected fromDV. These segment disparity values are fitted with a straight line with a non-zero gradient. They are very likely to be included in the side planes and the gradient of the fitted lines can be useful. However, it is possible that not all pixels from the side planes are correctly chosen fromDV. Therefore, using these pixels to represent the side planes will result in incorrect sizes and positions. In this paper, we propose a similar approach to the generation of the U-disparity. The gradients of the segments corresponding to the same side plane can be aggregated which results in the proposed G-disparity and allows convenient side line detection.Defining the disparity gradient map asDG, the corresponding gradient for each pixel can be represented asDG(n,m)orDG(s,j)where n and m represent the column and row indices. Like before, s and j represents the jth pixel within a horizontal segment s. A histogram of the disparity gradient can then be calculated for each column ofDGlike the calculation of the U-disparity. The result is called the G-disparity. In most situations, many pixels onDGare zero, which corresponds to oblique or vertical planes. These pixels are excluded during the G-disparity calculation since the objective is to locate the side planes. An example G-disparity is illustrated in Fig. 14. As the figure shows, similar gradients on the same column are accumulated and form horizontal lines on the G-disparity. The width and the horizontal positions of these lines are identical to the side lines in the U-disparity (as shown in Fig. 14 and 15). This indicates that the gradient and horizontal profiles of side lines in the U-disparity can be extracted by locating the horizontal lines in the G-disparity. In our system, the detection of these horizontal lines is achieved by analysing the pixel connectivities but other methods can also be applied. Once the horizontal lines are found, the corresponding horizontal positions, widths and gradients of the side lines on the U-disparity space are available. The only unknown parameter of each line on the U-disparity is the line offset. For every horizontal line, a one dimensional Hough transform with fixed line orientation is applied to extract the offsets. The number of Hough transform executions is equivalent to the number of horizontal lines on the G-disparity space. Each time, only a vertical band on the U-disparity is used. This vertical band is determined by the position and width of the corresponding horizontal lines on the G-disparity. Unlike applying the Hough transform directly on the U-disparity, the proposed algorithm finds the horizontal profile and line orientation prior to the use of the Hough transform. Another important fact is that only the offset corresponding to the maximum in the Hough space needs to be selected. During the whole process, no manually selected parameter is needed except a small thresholdTUon the U-disparity for binarisation. The determination of this threshold is based on the performance of the disparity calculation algorithm and the size of the smallest detectable obstacle. In our system, it has been chosen to be 10 so that most of the pixels are preserved (this indicates a small object with height of only 10 pixels will be included). A side line detection result is shown in Fig. 15. The resultant line detection is fast, accurate and requires only one parameter which can be easily determined.Vertical profiles of the side planes are detected based on the original disparity map. For each side line, the indicated disparities are used for side plane detection. The height of a plane is detected by locating vertically the locally connected pixels with approximately identical disparity values. The result of a side plane detection is shown in Fig. 17. The side planes are highlighted in yellow. As the figure illustrates, the side planes are detected accurately.Once the side planes are detected, all the pixels belonging to the sides are removed from the disparity map. The U-disparity is recalculated and contains almost only the horizontal lines. This step is very important since significantly less outliers are included during vertical plane detection. The same thresholdTUis applied to the U-disparity for binarisation.For each row d, the adjacent pixels are grouped and form a line. If the distance between two valid pixels is smaller thanTL, they are treated as being on the same line. This parameter is related toTU, since if more pixels are preserved, a lowerTLis needed to connect the points together. Thus, this relationship is assumed to be linear as shown below:(14)TL=γ∗TUIt has been found by experiments that settingγ=0.4is a suitable choice for our test data.Finally, the lines on row d that are shorter than a object size thresholdTS(d)(shown in Eq. (15)) are removed. This is a linear equation so thatTSchanges from small to large according to the disparity. This is because objects appear smaller in the far-field and larger in the near-field. The size threshold should also change with depth.(15)TS(d)=TS(0)+d·TS(dmax)-TS(0)dmaxwhereTS(0)=5andTS(dmax)=20defines the size threshold at the furthest and nearest distance respectively. This size limitation is very small which allows the detection of small objects and it is extremely rare that an obstacle in the road scene is smaller than these limits. All the above choice of thresholds are based on640×300input image with 30 disparity levels. If the resolution of the input data changes significantly, these thresholds need to be tuned accordingly. This is reasonable since all these parameters are in pixel units. Identical objects will be represented by more pixels in high resolution images. The result of this line selection process is shown in Fig. 16. As we can see, the detection is accurate due to the exclusion of outliers.The height and vertical position of a vertical plane is extracted based on the vertical image band defined with the horizontal line obtained earlier. Furthermore, the corresponding d of the line indicates that only pixels with the same d belong to the obstacle of interest. Therefore, a one dimensional accumulator can be used to store the number of pixels having the same d within the vertical region. A similar pixel connecting process is carried out as described for the detection of horizontal lines. If more than one line is detected, extra potential obstacles are created to allow the detection of multiple vertically positioned obstacles having an identical disparity value.The final obstacle detection result is shown in Fig. 17. The proposed algorithm uses the minimum amount of parameters, which are easy to determine, to detect both small and large obstacles and identifies the side and vertical planes quickly and accurately. It does not rely on the prior road plane detection and successfully avoids the disadvantages of applying the Hough transform.In this section, we include a free-space calculation algorithm to illustrate the benefit of the proposed obstacle detection approach. It is important to note that our aim is to detect all the image areas that fit with the road surface and are free of any obstacles. Therefore, some off-road areas and road surfaces beneath obstacles could be included in the results. With the obstacles removed from the disparity map, the free-space calculation becomes a much easier task. The remaining areas are mostly occupied by road, sky and some other far-field objects. The most significant advantage of the system is that the road surface is not assumed to be a very large area. Also, the amount of outliers involved during model fitting is reduced to the minimum which enables easier and faster computation. The vertical profile of the road surface is modelled using a spline which is proposed in [35]. This flexible model has very few restrictions, and therefore, allows most of the road vertical profiles to be accurately represented.The first step is to pick the road region from the disparity map. In order to remove the area of sky, the furthest possible distance of the road to be modelled is limited to100m. This is the case where no obstacle is ahead of the vehicle. If an obstacle blocks the view of the far-distance road surface, the distance of that obstacle is used as the furthest modelling distance.An example of a disparity map without obstacles and extreme far-field pixels is shown in Fig. 18. Since the nearest road surface pixels always lie on the lowest row of the image, the search for the road surface starts expanding from the centre (can vary depending on the mounting position of the camera) of the lowest line. The width of the area is set to be limited by any discontinuities of valid pixels. From the lowest line upwards, the search stops once no valid pixels can be found in the upper row or the number of the pixels on the row is smaller than a threshold. This threshold is set to confirm that enough pixels are available for the following steps. In our experiments, the threshold is chosen to be 60 while the image width is 640.Then the V-disparity is constructed based on the selected road area. The resultant image contains a clear line corresponding to the road as shown in Fig. 19. On each row of the V-disparity, the d corresponding to the maximum intensity is selected to represent the road disparity. This process reduces the number of pixels involved during line fitting to the minimum and removes any potential outliers.A least squares line fit is then performed based on the data points extracted earlier. The B-spline equation is defined as:(16)X(t)=∑Bio(t)QiwhereX(t)represents the curve defined by the B-spline basis function B and control points Q. The degree of the spline is represented by o.t∈[to,tK-o-1]denotes sampling points on the curve where K is the number of knots. The control point index is represented by i. In our system, a cubic B-spline is used (o=3).The least squares algorithm finds the minimum of the summed squared error between the spline to each data point by setting the differentiation of the error equation to 0. The resultant function is shown in Eq. (17).(17)BTBQ-BTP=0B contains all components of B. Q is a matrix containing allQiand P represents the data points. The control points (Q) can then be calculated. The advantage of the least squares method is that a global minimum can be guaranteed. However, the algorithm can be sensitive to outliers since the difference of all points is involved in the calculation. This problem will not cause any issue in our case since all data points are carefully sampled from the road surface. The result of curve fitting based on Fig. 19 is shown in Fig. 20. The chosen disparity d at each image row m is illustrated as small black circles. The blue line is the spline fitted to these points. As this figure shows, although the resolution of d is low, the fitted line is accurate and represents a flat road. More spline fitting results can be found in supplementary data.The original disparity map is compared to the disparities obtained from the vertical road profile. The initial selection of the road area is expanded if the horizontally adjacent non-road area contains the same disparity level as the road profile. In far distances where the road disparity is not modelled, the spline is linearly extrapolated based on the gradient at the minimum of the modelled d. This step enables the road area to be extended to very far distances. Finally, morphological processing (erosion and dilation) is applied to the road area to remove expansion errors. The final free-space calculation result is shown in Fig. 21. As the figure shows, the free-space is detected correctly up to a very far distance.

@&#CONCLUSIONS@&#
