@&#MAIN-TITLE@&#
Twenty years of linear programming based portfolio optimization

@&#HIGHLIGHTS@&#
Survey of LP computable mean-risk and mean-safety models for portfolio optimization.Portfolio selection with real features.Transaction costs, minimum lots, investment thresholds and cardinality constraint.Exact and heuristic solution approaches.

@&#KEYPHRASES@&#
Survey,LP computable mean-risk and mean-safety models,Real features,Transaction costs,Exact and heuristic algorithms,

@&#ABSTRACT@&#
Markowitz formulated the portfolio optimization problem through two criteria: the expected return and the risk, as a measure of the variability of the return. The classical Markowitz model uses the variance as the risk measure and is a quadratic programming problem. Many attempts have been made to linearize the portfolio optimization problem. Several different risk measures have been proposed which are computationally attractive as (for discrete random variables) they give rise to linear programming (LP) problems. About twenty years ago, the mean absolute deviation (MAD) model drew a lot of attention resulting in much research and speeding up development of other LP models. Further, the LP models based on the conditional value at risk (CVaR) have a great impact on new developments in portfolio optimization during the first decade of the 21st century. The LP solvability may become relevant for real-life decisions when portfolios have to meet side constraints and take into account transaction costs or when large size instances have to be solved. In this paper we review the variety of LP solvable portfolio optimization models presented in the literature, the real features that have been modeled and the solution approaches to the resulting models, in most of the cases mixed integer linear programming (MILP) models. We also discuss the impact of the inclusion of the real features.

@&#INTRODUCTION@&#
The portfolio optimization problem considered in this paper follows the original Markowitz’ formulation and is based on a single period model of investment. At the beginning of a period, an investor allocates the capital among various securities, assigning a share of the capital to each. During the investment period, the portfolio generates a random rate of return. This results in a new value of the capital (observed at the end of the period), increased or decreased with respect to the invested capital by the average portfolio return. This model has played a crucial role in stock investment and has served as basis for the development of the modern portfolio financial theory.In the original Markowitz model (Markowitz, 1952) the risk is measured by the standard deviation or variance. Several other risk measures have been later considered, creating a family of mean-risk models. Whereas the original Markowitz model is a quadratic programming problem, following Sharpe (1971a), many attempts have been made to linearize the portfolio optimization problem (c.f., Speranza (1993) and references therein).Nowadays, solution methods available for quadratic programming models are quite competitive also with respect to linear models. Nevertheless, the introduction of real features involving the use of integer variables may increase problem complexity significantly and makes LP solvable models more competitive with respect to quadratic models for which satisfactory solution methods are not available. Moreover, the recent advance in computers capability has opened up new solution opportunities and led to an extraordinary progress in statistics (see Efron (2000)) as well as in optimization (see Mulvey (2004) and Cornuejols & Tütüncü (2007)) with enormous effects in different application contexts including finance.Obviously, in order to guarantee that the portfolio takes advantage of diversification, no risk measure can be a linear function of the portfolio shares. Nevertheless, a risk measure can be LP computable in the case of discrete random variables, when returns are defined by their realizations under the specified scenarios. This applies, in particular, to the mean absolute deviation from the mean. The mean absolute deviation was very early considered in the portfolio analysis (Sharpe (1971b) and references therein) while Konno and Yamazaki (1991) presented and analyzed the complete portfolio optimization model based on this risk measure – the so-called MAD model. The MAD model presented in 1991 was not the first LP portfolio optimization model as earlier Yitzhaki (1982) introduced the mean-risk model using Gini’s mean (absolute) difference as risk measure. Nevertheless, the MAD model as much simpler computationally has drawn a lot of attention resulting in much research and speeding up development of other LP models. Young (1998) analyzed the LP solvable portfolio optimization model based on risk defined by the worst case scenario (minmax approach), while Ogryczak (2000) introduced the multiple criteria LP model covering all the above as special aggregation techniques. Following Rockafellar and Uryasev (2000, 2002), the CVaR models had a great impact on new developments of risk measures in finance during the first decade of 21st century. While several LP computable measures are dispersion type risk measures, some are safety measures which, when embedded in an optimization model, are maximized instead of being minimized. A first survey on risk and safety basic LP solvable portfolio optimization models can be found in Mansini, Ogryczak, and Speranza (2003a).In practical financial applications the portfolio optimization problem has to take into account real features such as transaction costs, minimum transaction lots, cardinality constraints, thresholds on maximum or minimum investments. The impact of the introduction of real features in a portfolio optimization model on the resulting portfolio has been discussed in Kellerer, Mansini, and Speranza (2000), where it is shown on real data that the introduction of fixed transaction costs reduces the number of securities selected, and that considering transaction lots substantially changes the structure of the resulting portfolio, both in terms of securities selected and capital invested in the securities.In most cases the inclusion of real features in a basic model requires the introduction of integer and binary variables. We refer to these models as models with real features. In some cases the modeling of real features is possible by using as decision variables the security shares (percentages). We call the models based on shares relative models and the investment variables relative. In several cases the introduction of real features implies the need of variables that represent the absolute values of the capital invested in each security. We call this second type of models absolute models and the investment variables absolute.In this paper we review the basic LP solvable portfolio models and the models with real features that were presented in the literature, together with the solution approaches proposed for the latter class of models. Though optimization models are a consolidated approach to solve complex real problems, the relevance of heuristics has been also well recognized since the eighties (see Zanakis & Evans (1981)). The use of a heuristic, a threshold-accepting algorithm, for portfolio optimization is discussed in Dueck and Winker (1992). Since then, and in particular in the last decade thanks to the enormous growth in computing power, practitioners and financial firms have made a massive recourse to efficient and easy to implement heuristic techniques when performing strategic what-if analysis studies (see Gilli & Schumann (2012)). Besides, a new relevance in practical applications is obtained by approaches taking into account the multiple criteria nature of the portfolio problem (see the recent work by Xidonas, Mavrotas, Zopounidis, & Psarras (2011) for an integrated methodological framework for portfolio optimization based on multiple criteria decision making (MCDM), Zopounidis & Doumpos (2002) and Steuer & Na (2003), for literature reviews on multicriteria decision in financial decision making).The paper is organized as follows. Section 2 is devoted to an introduction to risk and safety measures and reviews the basic LP solvable portfolio optimization models. In Section 3 we recall short fall risk measures as the basic LP computable risk measures. We also analyze mixed criteria obtained combining basic measures in weighted sum (enhanced measures). In Section 4 we introduce the relative and absolute models, then we review the literature on portfolio optimization problems with real features and classify them according to the type of variables used (relative or absolute models). Section 5 is devoted to solution approaches and computational issues. We survey the main algorithms proposed in the literature for portfolio problems with real features classifying them according to their nature in heuristic and exact solution approaches. Even though the main focus is on mixed integer linear programming (MILP) models, we briefly survey also main solution methods for the mean–variance model with real features. A part of this section will also deal with the important computational issue concerning the solution of large size LP problems including a high number of securities and scenarios. We will discuss recent results from the literature showing how computational efficiency in solving huge LP portfolio problems can be addressed taking advantages from LP duality.The portfolio optimization problem considered in this paper follows the original Markowitz formulation and is based on a single period model of investment. At the beginning of a period, an investor allocates the capital among various securities, thus assigning a nonnegative weight (share of the capital) to each security. During the investment period, a security generates a random rate of return. This results in a change of capital invested (observed at the end of the period) which is measured by the weighted average of the individual rates of return.Let J={1,2,…,n} denote a set of securities considered for an investment. For each security j∈J, its rate of return is represented by a random variable Rjwith a given mean μj=E {Rj}. Further, let x=(xj)j=1,…,ndenote a vector of decision variables xjexpressing the weights defining a portfolio. To represent a portfolio, the weights must satisfy a set of constraints. The basic set of constraints is defined by a requirement that the weights must sum to one, i.e.∑j=1nxj=1and xj⩾0 for j=1,…,n. An investor usually needs to consider some other requirements expressed as a set of additional side constraints. Most of them can be expressed as linear equations and inequalities. We will assume that the basic set of portfolios Q is a general LP feasible set given in a canonical form as a system of linear equations with nonnegative variables. Although, in farther sections we show that taking into account real features such as transaction costs, minimum transaction lots, cardinality constraints, thresholds on maximum or minimum investments in most cases requires the introduction of integer and binary variables into the LP structure.Each portfolio x defines a corresponding random variableRx=∑j=1nRjxjthat represents a portfolio rate of return. The mean rate of return for portfolio x is given as:μ(x)=E{Rx}=∑j=1nμjxj. Following Markowitz (1952), the portfolio optimization problem is modeled as a mean-risk bicriteria optimization problem(1)max{[μ(x),-ϱ(x)]:x∈Q}where the mean μ(x) is maximized and the risk measure ϱ(x) is minimized. A feasible portfolio x0∈Q is called the efficient solution of problem (1) or the μ/ϱ-efficient portfolio if there is no x∈Q such that μ(x)⩾μ(x0) and ϱ(x)⩽ϱ(x0) with at least one inequality strict.In the original Markowitz model (Markowitz, 1952) the risk is measured by the standard deviation or variance:σ2(x)=E{(μ(x)-Rx)2}. Several other risk measures have been later considered thus creating the entire family of mean-risk (Markowitz type) models (cf. Mansini, Ogryczak, & Speranza (2003b)). We focus our analysis on the class of Markowitz-type mean-risk models where risk measures, similar to the standard deviation, are shift independent dispersion parameters. Thus, they are equal to 0 in the case of a risk free portfolio and take positive values for any risky portfolio. Moreover, in order to model possible advantages of a portfolio diversification, risk measure ϱ(x) must be a convex function of x.While the original Markowitz model forms a quadratic programming problem, following Sharpe (1971a), many attempts have been made to linearize the portfolio optimization procedure (c.f., Speranza (1993) and references therein). Certainly, to model advantages of a diversification, risk measures cannot be linear function of x. Nevertheless, the risk measure can be LP computable in the case of discrete random variables, i.e., in the case of returns defined by their realizations under the specified scenarios (actually, in practice, a variable can still be considered continuous and then approximated by scenarios using a (typically large) sample). We will consider T scenarios with probabilities pt(where t=1,…,T). Assume that for each random variable Rjits realization rjtunder the scenario t are known. Typically, the realizations are derived from historical data though they should represent the distribution of future returns. Frequently, a straightforward approach treating T historical periods as equally probable scenarios (pt=1/T) is considered. However, we consider any arbitrary probability distributions represented by various probabilities pt. Similar to the mean μ(x), the realizations of the portfolio returns Rxare given byyt=∑j=1nrjtxj. Therefore, several risk measures referring to the realizations can be LP computable. In particular, Konno and Yamazaki (1991) presented and analyzed the complete portfolio optimization model (MAD model) based on the risk measure defined as the mean absolute deviation from the mean:(2)δ(x)=E{|μ(x)-Rx|.}For a discrete random variable represented by its realizations the mean absolute deviation (2) is LP computable as:(3)δ(x)=min∑t=1Tdt++dt-pt:dt--dt+=∑j=1n(μj-rjt)xj,dt+,dt-⩾0fort=1,…,T.The MAD model proposed increased interest in LP portfolio optimization approaches resulting in many new developments at the beginning of the 21st century. However, historically earlier Yitzhaki (1982) introduced the mean-risk model using Gini’s mean (absolute) difference as the risk measure (hereafter referred to as GMD model). For a discrete random variable represented by its realizations yt, the Gini’s mean difference is defined asΓ(x)=12∑t′=1T∑t″=1T:|yt′-yt″|pt′pt″. Thus, obviously, it is LP computable as(4)Γ(x)=min12∑t′=1T∑t″=1Tpt′pt″dt′t″:dt′t″⩾∑j=1nrjt′-rjt″xj,dt′t″⩾0fort′,t″=1,…,T.Actually, several risk measures can be expressed as the optimal value of an LP problem of the following form:(5)ϱ(x)=min{cTv:Av=Bx,v⩾0},x∈Q,where v is a vector of auxiliary variables while the portfolio vector is defined by variables x. One may notice that, except from x∈Q, all the LP constraints are homogeneous. It is related to the fact that the risk measures ϱ(x) we consider are positively homogeneous functions of x. This property allows us to demonstrate easily that all the LP computable risk measures (5) are actually convex functions of x. Indeed, the optimal value of the minimization LP problem min{cTv: A v=b, v⩾0} is subadditive with respect to the vectors b. Hence, for any 0 ⩽α⩽1, one gets:ϱ(αx′+(1-α)x″)=min{cTv:Av=αBx′+(1-α)Bx″,v⩾0}⩽min{cTv:Av=αBx′,v⩾0}+min{cTv:Av=(1-α)Bx″,v⩾0}=αϱ(x′)+(1-α)ϱ(x″)which proves the convexity of ϱ(x).The Markowitz model is frequently criticized as not consistent with axiomatic models of preferences for choice under risk (Rothschild & Stiglitz (1969)). The Markowitz model is not consistent with the Second Degree Stochastic Dominance (SSD) since its efficient set may contain portfolios characterized by a small risk but also very low return (see Ogryczak & Ruszczyński (1999) and references therein). Unfortunately, it is a common flaw of all Markowitz-type mean-risk models where risk is measured with some dispersion measures. This can be illustrated by two portfolios x′ and x″ (with rate of return given in percents):P{Rx′=ξ}=1,ξ=1.00,otherwisePRx″=ξ=1/2,ξ=3.01/2,ξ=5.00,otherwisewhere the risk free portfolio x′ with the guaranteed result 1.0 is obviously worse than the risky portfolio x″ giving 3.0 or 5.0. In all preference models based on the risk aversion axioms (Artzner, Delbaen, Eber, & Heath, 1999; Levy, 1992) portfolio x′ is dominated by x″, in particularRx″≻SSDRx′. On the other hand, when a dispersion type risk measure ϱ(x) is used, then both the portfolios are efficient in the corresponding mean-risk model since for each such a measure ϱ(x″)>0 while ϱ(x′)=0.In order to overcome this weakness of the Markowitz model already Yitzhaki (1982) while introducing the Gini’s mean difference (GMD) model considered maximization of the safety measure μ(x)−Γ(x) and demonstrated its SSD consistency. In the literature some of the LP computable measures are dispersion type risk measures and some are safety measures, which, when embedded in an optimization model, are maximized instead of being minimized (or defined on losses instead of returns and then minimized like in Rockafellar & Uryasev (2000)). We have shown (Mansini et al., 2003a) that each risk measure ϱ(x) has a well defined corresponding safety measure μ(x)−ϱ(x) and vice versa. Although the risk measures are more “natural”, due to the consolidated familiarity with Markowitz model, the safety measures, contrary to the dispersion type risk measures, are SSD consistent, in the sense that(6)Rx′⪰SSDRx″⇒μ(x′)-ϱ(x′)⩾μ(x″)-ϱ(x″).Moreover, the LP computable safety measures we consider satisfy axioms of the so-called coherent risk measurement of Artzner et al. (1999) (with the sign change as shown in Mansini et al. (2003a)). If the risk measure ϱ(x) is SSD safety consistent (6), then except for portfolios with identical values of μ(x) and ϱ(x), every efficient solution of the bicriteria problem(7)max{[μ(x),μ(x)-ϱ(x)]:x∈Q.}is an SSD efficient portfolio.Note that a portfolio dominated in the mean-risk model (1) is also dominated in the corresponding mean-safety model (7). Hence, the efficient portfolios of problem (7) form a subset of the entire μ/ϱ-efficient set. We illustrate this in the μ/ϱ image space in Fig. 1. Due to the convexity of ϱ(x) and linearity of μ(x), the portfolios x∈Q form in the μ/ϱ image space a set with the convex boundary from the side of μ-axis (i.e., the set {(μ,ϱ): μ=μ(x), ϱ⩾ϱ(x), x∈Q} is convex). This boundary represents a curve of the relative minimum risk portfolios spanning from the best expectation portfolio (BEP) to the worst expectation portfolio (WEP). The minimum risk portfolio (MRP), defined as the solution of minx∈Qϱ(x), limits the curve to the mean-risk efficient frontier from BEP to MRP. Similar, the maximum safety portfolio (MSP), defined as the solution of maxx∈Q[μ(x)−ϱ(x)], distinguishes a part of the mean-risk efficient frontier, from BEP to MSP, which is also mean-safety efficient. In the case of a SSD safety consistent risk measure, this part of the efficient frontier represents portfolios which are SSD efficient.There are two ways of modeling risk averse preferences and corresponding approaches to handle bicriteria mean-risk problem (1): the bounding analysis and the trade-off analysis. The former is a common approach based on the use of a specified lower bound μ0 on expected returns which results in the following problem:(8)min{ϱ(x):μ(x)⩾μ0,x∈Q.}This bounding approach provides a clear understanding of investor preferences. One may use models with bounded risk instead of bounded return:(9)max{μ(x):ϱ(x)⩽ϱ0,x∈Q.}Due to convexity of risk measures ϱ(x) with respect to x, by solving the parametric problem (8) with changing μ0∈[minj=1,…,nμj, maxj=1,…,nμj] one gets various efficient portfolios. Actually, for μ0 smaller than the expected return of the MRP, problem (8) generates always the MRP as the solution. Larger values of μ0 provide the parameterization of the μ/ϱ-efficient frontier by generating efficient portfolios with μ(x)=μ0. Portfolios corresponding to larger values of bound μ0 exceeding the expected return of the MSP are also efficient solutions to the corresponding mean-safety problem (7). However, having a specified value of parameter μ0 one cannot know if the optimal solution of (8) is also an efficient portfolio with respect to the corresponding mean-safety model (7) or not. Therefore, when using the bounding approach one should rather consider explicitly a separate problem(10)max{μ(x)-ϱ(x):μ(x)⩾μ0,x∈Q}for the corresponding mean-safety model (7).Another approach to implementation of the Markowitz-type mean-risk model takes advantage of the efficient frontier convexity to perform the trade-off analysis. Having assumed a trade-off coefficient λ between the risk and the mean, the so-called risk aversion coefficient, one may directly compare real values μ(x)−λϱ(x) and find the best portfolio by solving the optimization problem:(11)max{μ(x)-λϱ(x):x∈Q.}Various positive values of parameter λ allow the generation of various efficient portfolios. By solving problem (11) with changing λ>0 with a special parametric optimization procedures one can determine the whole frontier, without bothering to invoke an optimization solver for many times. In the context of mean–variance model the technique was introduced by Markowitz (1959) as the so-called critical line approach. However, the increased computational power makes such parametric optimization techniques not very attractive.Due to convexity of risk measures ϱ(x) with respect to x, λ>0 provide the parameterization of the entire set of the μ/ϱ-efficient portfolios (except of its two ends BEP and MRP which are the limiting cases). Note that (1−λ)μ(x)+λ(μ(x)−ϱ(x))=μ(x)−λϱ(x). Hence, bounded trade-off 0<λ<1 in the Markowitz-type mean-risk model (1) corresponds to the complete weighting parameterization of the model (7). Opposite to the bounding approach, having a specified value of parameter λ one can immediately know if the optimal solution of (11) is also an efficient portfolio with respect to the mean-safety model (7) or not. Thus, the trade-off model (11) offers a universal tool covering both the standard mean-risk and the corresponding mean-safety approaches. It provides easy modeling of the risk aversion and control of the SSD efficiency. Therefore, in our analysis we will focus on this specification of the Markowitz-type mean-risk models.An alternative specific approach looks for a risky portfolio offering the maximum increase of the mean return while comparing to the risk-free investment opportunities. Namely, having given the risk-free rate of return r0 one seeks a risky portfolio x that maximizes the ratio (μ(x)−r0)/ϱ(x). This leads us to the following ratio optimization problem:(12)maxμ(x)-r0ϱ(x):x∈Q.The optimal solution of problem (12) is usually called the tangency portfolio or the market portfolio. Note that clear identification of dispersion type risk measures ϱ(x) for all the LP computable performance measures allows us to define tangency portfolio optimization for all the models. For LP computable risk measures (5) the ratio model (12) can be converted into an LP form (see Mansini et al. (2003a)).The notion of risk is related to a possible failure of achieving some targets. It was formalized by Roy (1952) as the so-called safety-first strategies and later led to the concept of below-target risk measures (Fishburn (1977)) or shortfall criteria. The simplest shortfall criterion for a specific target value τ is the mean below-target deviation (first Lower Partial Moment, LPM)(13)δ¯τ(x)=E{max{τ-Rx,0}}.The mean below-target deviation is LP computable for returns represented by their realizations as:(14)δ¯τ(x)=min∑t=1Tdt-pt:dt-⩾τ-∑j=1nrjtxj,dt-⩾0fort=1,…,T.Actually, as shown in Ogryczak and Ruszczyński (1999), the SSD relation is defined by pointwise comparison of functions:Fx(2)(τ)=∫-∞τFx(ξ)dξ=P{Rx⩽τ}E{τ-Rx|Rx⩽τ}=δ¯τ(x). Hence, the SSD relation is the Pareto dominance for mean below-target deviations from infinite number (continuum) of targets.The below-target deviations are very useful in investment situations with clearly defined minimum acceptable returns (e.g. bankruptcy level, Fishburn (1977)). Otherwise, appropriate selection of the target value might be a difficult task while the model is very sensitive to the target value changes as shown by Grootveld and Hallerbach (1999). A combination of mean below-target deviations from a few targets was used in the Russel–Yasuda–Kasai financial planning model (Carino, Myers, & Ziemba, 1998) to define the corresponding risk measure. However, for portfolio optimization they are rather rarely applied. Recently, the so-called Omega Ratio measure defined for a given target as the ratio of the mean over-target deviation by the mean below-target deviation has been introduced by Shadwick and Keating (2002):Ωτ(x)=E{max{Rx-τ,0}}E{max{τ-Rx,0}}=∫τ∞(1-Fx(ξ))dξ∫-∞τFx(ξ)dξFollowing Ogryczak and Ruszczyński (1999), one getsΩτ(x)=Fx(2)(τ)-(τ-μ(x))Fx(2)(τ)=1+μ(x)-τδ¯τ(x)Thus, Omega ratio maximization is equivalent to the standard ratio (tangent portfolio) model (12) for theδ¯τ(x)measure with τ representing the risk-free rate of return.The below-target deviations do not represent any shift independent dispersion type risk measure to be considered in the Markowitz-type mean-risk model. In particular, the below-target deviation may be equal to 0 for various risky portfolios, thus violating the risk relevance requirement. When the mean expected return is used as a performance measure, then one should consider the concept of shortfall applied to the mean as a target. This results in the risk measure known as the downside mean semideviation from the mean(15)δ¯(x)=E{max{μ(x)-Rx,0}}=Fx(2)(μ(x)).The downside mean semideviation is always equal to the upside one (cf. Speranza (1993) and Ogryczak & Ruszczyński (1999)) and, therefore, we refer it hereafter as to the mean semideviation. The mean semideviation is a half of the mean absolute deviation from the mean, i.e.δ(x)=2δ¯(x). Hence, the corresponding mean-risk model is equivalent to the MAD model. For a discrete random variable represented by its realizations, the mean semideviation (15) is LP computable as:(16)δ¯(x)=min∑t=1Tdt-pt:dt-⩾∑j=1n(μj-rjt)xj,dt-⩾0fort=1,…,TAs shown in Ogryczak and Ruszczyński (1999), the mean semideviation is SSD safety consistent and the corresponding safety measure can be expressed as(17)μ(x)-δ¯(x)=E{μ(x)-max{μ(x)-Rx,0}}=E{min{Rx,μ(x)}},thus representing the mean downside underachievement.The MAD model introduced by Konno and Yamazaki (1991) with a directly defined mean absolute deviation was not the first LP portfolio optimization model. Nevertheless, it has drawn a lot of attention resulting in much research and speeding up development of other LP models. The MAD model was quite extensively tested on various stock markets (Konno & Yamazaki, 1991; Mansini et al., 2003a; Xidonas, Mavrotas, & Psarras, 2010) including its application to portfolios of mortgage-backed securities by Zenios and Kang (1993) where distribution of rate of return is known to be not symmetric. The MAD model usually, similar to the Markowitz one, generated the portfolios with the largest returns but also entailing the largest risk of underachievements. Certainly, the MAD measure can be applied to multi-period problems of portfolio management as demonstrated in Carino et al. (1998), Pflug and Świetanowski (1999), and Sodhi (2005).For a discrete random variable represented by its realizations yt, the worst realization mint=1,…,Tytis a well appealing safety measure, LP computable as(18)M(x)=maxv:v⩽∑j=1nrjtxjfort=1,…,T.The corresponding (dispersion) risk measure Δ(x)=μ(x)−M(x), the maximum (downside) semideviation, is LP computable as(19)Δ(x)=minv:v⩾∑j=1n(μj-rjt)xjfort=1,…,T.The measure M(x) was applied to portfolio optimization by Young (1998) while the maximum semideviation was introduced in Ogryczak (2000) and analyzed Kamil, Mustafa, and Ibrahim (2010).A natural generalization of the measure M(x) is a measure defined as the mean of the specified size (quantile) of worst realizations. This leads to the quantile shortfall risk measures related to the so-called Absolute Lorenz Curves (ALC) (c.f., Shorrocks (1983), Shalit & Yitzhaki (1994), Ogryczak (1999), & Ogryczak & Ruszczyński (2002a)) which represent the second quantile functions defined as(20)Fx(-2)(p)=∫0pFx(-1)(α)dαfor0<p⩽1andFx(-2)(0)=0,whereFx(-1)(p)=inf{η:Fx(η)⩾p}is the left-continuous inverse of the cumulative distribution function Fx. Actually, the pointwise comparison of ALCs provides an alternative characterization of the SSD relation Ogryczak and Ruszczyński (2002a) in the sense thatRx′⪰SSDRx″if and only ifFx′(-2)(β)⩾Fx″(-2)(β)for all 0<β⩽1. The duality (conjugency) relation between F(−2) and F(2)Ogryczak and Ruszczyński (2002a) leads to the following formula:(21)Fx(-2)(β)=maxη∈Rβη-Fx(2)(η)=maxη∈R[βη-δ¯η(x)]where η is a real variable taking the value of β-quantile Qβ(x) at the optimum.For any real tolerance level 0<β⩽1, the normalized value of the ALC defined asMβ(x)=Fx(-2)(β)/βis the Worst Conditional Expectation which is now commonly called the Conditional Value-at-Risk (CVaR). This name was introduced by Rockafellar and Uryasev (2000) who considered (similar to the Expected Shortfall by Embrechts, Klüppelberg, & Mikosch (1997)) the measure CVaR defined asE{Rx|Rx⩽Fx(-1)(β)}for continuous distributions showing that it could then be expressed by a formula analogous to (21) and thus be potentially LP computable. The approach has been further expanded to general distributions in Rockafellar and Uryasev (2002). For additional discussion of relations between various definitions of the measures we refer to Ogryczak and Ruszczyński (2002b).The CVaR measure is a safety measure according to our classification (Mansini et al., 2003b). The corresponding risk measure Δβ(x)=μ(x)−Mβ(x) is called the (worst) conditional semideviation (Ogryczak & Ruszczyński, 2002b) or drawdown measure (Chekhlov, Uryasev, & Zabarankin, 2005). Note that, for any 0<β<1, the CVaR measures defined by F(−2)(β), opposite to below-target mean deviations F(2)(η), are risk relevant. They are also coherent as shown by Pflug (2000) and SSD consistent as demonstrated by Ogryczak and Ruszczyński (2002a). For a discrete random variable represented by its realizations, due to (14), problem (21) becomes an LP. Thus(22)Mβ(x)=maxη-1β∑t=1Tdt-pt:dt-⩾η-∑j=1nrjtxj,dt-⩾0fort=1,…,T,whereas the conditional semideviations may be computed as the corresponding differences from the mean(23)Δβ(x)=min∑j=1nμjxj-η+1β∑t=1Tdt-pt:dt-⩾η-∑j=1nrjtxj,dt-⩾0fort=1,…,T.Following Rockafellar and Uryasev (2000), the CVaR models had a great impact on new developments of risk measures in finance during the first decade of 21st century. The measure was studied in many applications Andersson, Mausser, Rosen, and Uryasev (2001), Krokhmal, Palmquist, and Uryasev (2002), Roman, Darby-Dowman, and Mitra (2007), Topaloglou, Vladimirou, and Zenios (2002), Mansini et al. (2003a), and Consiglio and Staino (2012) and expanded in various formats Acerbi (2002), Krzemienowski (2009), Mansini, Ogryczak, and Speranza (2007). It is important to notice that, although the quantile risk measures (VaR and CVaR) were introduced in banking as extreme risk measures for small tolerance levels (like β=0.05), for the portfolio optimization good results are usually shown by rather larger tolerance levels (Mansini et al., 2007).Actually, all the classical LP computable risk measures are well defined characteristics of the Lorenz function (Ogryczak, 2000). However, both the mean semideviation and the maximum semideviation are rather rough measure when comparing to the Gini’s mean difference (Shalit & Yitzhaki, 1989; Ringuest, Graves, & Case, 2004). Note that the corresponding safety measureμ(x)-Γ(x)=E{Rx∧Rx}expresses the expectation of the minimum of two i.i.d.r.v. RxYitzhaki (1982), thus representing the mean worse return. This leads to the following LP formula for the Gini’s mean difference(24)Γ(x)=min∑j=1nμjxj-∑t=1T∑j=1nrjtxjpt2-2∑t′=1T-1∑t″=t′+1Tut′t″pt′pt″:ut′t″⩽∑j=1nrjt′xj,ut′t″⩽∑j=1nrjt″xj∀t′=1,…,T-1;t″=t′+1,…,T.The most popular LP computable risk measures may be derived from the shortfall criteria of SSD. They may be further extended to enhance the risk aversion modeling capabilities. First of all, the measures may be combined by the weighted sum which allows the generation of various mixed measures.In particular, one may build a multiple CVaR measure by considering, say m, tolerance levels 0<β1<β2<⋯<βm⩽1 and using weighted sum of the conditional semideviationsΔβk(x)as a new risk measure(25)Δw(m)(x)=∑k=1mwkΔβk(x),∑k=1mwk=1,wk>0fork=1,…,m,with the corresponding safety measure(26)Mw(m)(x)=μ(x)-Δw(m)(x)=∑k=1mwkMβk(x).The resulting Weighted CVaR (WCVaR) models Mansini et al. (2007) use multiple CVaR measures thus allowing for more detailed risk aversion modeling. The WCVaR risk measure is obviously LP computable as(27)Δw(m)(x)=min∑j=1nμjxj-∑k=1mwkηk-1βk∑t=1Tdkt-pt:dkt-⩾0,dkt-⩾ηk-∑j=1nrjtxjfort=1,…,T;k=1,…,m}.For appropriately defined weights the WCVaR measures may be considered some approximations to the Gini’s mean difference with the advantage of being computationally much simpler than the GMD model itself. We analyzed the WCVaR measures defined as simple combinations of a very few CVaR measures (Mansini et al., 2007). There were introduced two specific types of weight-settings which related the WCVaR measure to the Gini’s mean difference (the Wide WCVaR) and its tail version (the Tail WCVaR). This allowed us to use a few tolerance levels as only parameters specifying the entire WCVaR measures while the corresponding weights are automatically predefined by the requirements of the corresponding Gini’s measures. Our experimental analysis of the models performance on the real-life data from the Milan Stock Exchange confirmed their attractiveness (Mansini et al., 2007), as the WCVaR models usually performed better than the GMD, the Minimax or the extremal CVaR models.The risk measures introduced in the previous section, although all derived from the SSD shortfall criteria, are quite different in modeling the downside risk aversion. Definitely, the strongest in this respect is the maximum semideviation Δ(x) while the conditional semideviation Δβ(x) (CVaR model) allows us to extend the approach to a specified β quantile of the worst returns which results in a continuum of models evolving from the strongest downside risk aversion (β close to 0) to the complete risk neutrality (β=1). The mean (downside) semideviation from the mean, used in the MAD model, is formally a downside risk measure. However, due to the symmetry of mean semideviations from the mean (Speranza, 1993; Ogryczak & Ruszczyński, 1999), it is equally appropriate to interpret it as a measure of the upside risk. Similar, the Gini’s mean difference, although related to all the conditional maximum semideviations, is a symmetric risk measure (in the sense that for Rxand −Rxit has exactly the same value). For better modeling of the risk averse preferences one may enhance the below-mean downside risk aversion in various measures. The below-mean risk downside aversion is a concept of risk aversion assuming that the variability of returns above the mean should not be penalized since the investors are concerned about an underperformance rather than the overperformance of a portfolio (Markowitz, 1959). This can be implemented by focusing on the distribution of downside underachievements min{Rx,μ(x)} instead of the original distribution of returns Rx.Applying the mean semideviation (15) to the distribution of downside underachievements min{Rx,μ(x)} one getsδ¯2(x)=E{max{E{min{Rx,μ(x)}}-Rx,0}}=E{max{μ(x)-δ¯(x)-Rx,0}}.This allows us to define the enhanced risk measure for the original distribution of returns Rxasδ¯(2)(x)=δ¯(x)+δ¯2(x)with the corresponding safety measureμ(x)-δ¯(2)(x)=μ(x)-δ¯(x)-δ¯2(x). As shown in Michalowski and Ogryczak (2001) the above approach can be repeated recursively resulting in m (defined recursively) distribution dependent targets μ1(x)=μ(x),μk(x)=E{min{Rx,μk(x)}}for k=1,…,m and the corresponding mean semideviationsδ¯1(x)=δ¯(x),δ¯k(x)=E{max{μk(x)-Rx,0}}for k=1,…,m.(28)δ¯w(m)(x)=∑k=1mwkδ¯k(x)1=w1⩾w2⩾…⩾wm⩾0is SSD consistent measure of the m-MAD model (Michalowski & Ogryczak, 2001). Actually, the measure can be interpreted as a single mean semideviation (from the mean) applied with a penalty function:δ¯w(m)(x)=E{u(max{μ(x)-Rx,0})}where u is increasing and convex piece-wise linear penalty function with breakpoints bk=μ(x)−μk(x) and slopes sk=w1+⋯+wk, k=1,…,m. Therefore, the measureδ¯w(m)(x)is referred to as the mean penalized semideviation(29)δ¯w(m)(x)=min∑k=1mwkzk:zk-∑t=1Tptdkt-=0,dkt-⩾0,dkt-⩾∑j=1n(μj-rjt)xj-∑i=1k-1zifort=1,…,T;k=1,…,m.The Gini’s mean difference is a symmetric measure, thus equally treating both under and overachievements. The enhancement technique allows us to define the downside Gini’s mean difference by applying the Gini’s mean difference to the distribution of downside underachievements min {Rx,μ(x)} (Krzemienowski & Ogryczak, 2005). The downside Gini’s safety measure takes the form:(30)μ(x)-Γd(x)=E{min{Rx∧Rx,μ(x)}}which is SSD safety consistent (Krzemienowski & Ogryczak, 2005) and obviously LP computable.The LP computable risk measures are based on exactly known distribution of returns in terms of realizations and probabilities for several scenarios. Robust variants of the measures have been recently considered where the underlying distribution is only known to belong to a certain set P. Zhu and Fukushima (2009) defined such a robust version of CVaR: the Worst-Case CVaR (WCCVaR). They showed that its maximization remains LP tractable under box uncertainty. Generally, such robust versions can be built for various risk criteria (see Thiele (2008)) leading to LP models while applied to LP computable measures. Actually, as shown by Ogryczak for box uncertainty the robust model of the mean is essentially a CVaR, and also the robust model of the CVaR itself is a CVaR with appropriately redefined probabilities while robust MAD model is a nested CVaR measure (Ogryczak, in press). Till now there are no reported portfolio optimization applications of LP computable robust risk measures.As shown in the previous sections several LP computable risk measures have been considered for portfolio optimization. Some of them were originally introduced rather as the safety measure in our classification (e.g., CVaR measures). Nevertheless, all of them can be represented with positively homogeneous and shift independent risk measures ϱ of classical Markowitz type models. Simple as well as more complicated LP computable risk measures ϱ(x) can be defined by (5), i.e. as(31)ϱ(x)=min{cTv:Av=Bx,v⩾0},x∈Q,where v is a vector of auxiliary variables while the portfolio vector x apart from original portfolio constraints x∈Q only defines a parametric vector b=Bx. Obviously, the corresponding safety measures are given by a similar LP formula(32)μ(x)-ϱ(x)=max∑j=1nμjxj-cTv:Av=Bx,v⩾0,x∈Q.Table 1summarizes the major measures with the sizes of the corresponding LP problems (31) in terms of number of auxiliary variables and constraints (matrix A dimensionality).One may notice the number of auxiliary variables and constraints used in the MAD model is equal to the number of scenario. Similar size (with one more variable) has the LP model for the CVaR measures while the Minimax model requires only one auxiliary variable. The GMD model is much more complex with number of auxiliary variables and constraints proportional to T2. The multiple level MAD and CVaR models (m-MAD and WCVaR, respectively) multiply the number of auxiliary variables and constraints by the number of levels. Thus, replacing the GMD with its WCVaR approximation based on a few levels may dramatically reduce the LP problem complexity (see Table 1).For each type of model, the mean-risk bounding approach (8) leads to the LP problem(33)minx,vcTv:Av=Bx,v⩾0,∑j=1nμjxj⩾μ0,x∈Q,while the mean-safety bounding approach (10) results in(34)maxx,v∑j=1nμjxj-cTv:Av=Bx,v⩾0,∑j=1nμjxj⩾μ0,x∈Q.Thus, both the LP models extend the basic LP risk model (31) only with one inequality and the explicit portfolio variables and constraints of x∈Q. Similarly, the trade-off analysis approach (11) results in LP model(35)maxx,v∑j=1nμjxj-λcTv:Av=Bx,v⩾0,x∈Q,extending the basic one with only the explicit portfolio variables and constraints of x∈Q.As mentioned, an alternative approach to bicriteria mean-risk problem of portfolio selection depends on search for the tangency portfolio which maximizes the ratio μ(x)−r0/ϱ(x). The corresponding ratio optimization problem (12) can be converted into an LP form by the following transformation (Mansini et al., 2003a): introduce an auxiliary variable z=1/ϱ(x), then replace the original variables x and v withx̃=zxandṽ=zv, respectively, getting the linear criterion and an LP feasible set. For risk measure ϱ defined by (31) one gets the following LP formulation of the corresponding ratio model(36)maxx̃,ṽ,z∑j=1nμjx̃j-r0z:cTṽ=z,Aṽ=Bx̃,ṽ⩾0,∑j=1nx̃j=z,x̃j⩾0forj=1,…,n,where the second line constraints correspond to the simplest definition of setQ={x:∑j=1nxj=1,xj⩾0∀j=1,…,n}and can be accordingly formulated for any other LP set. Once the transformed problem is solved, the values of the portfolio variables xjcan be found by dividingx̃jby the optimal value of z.We call real features all the additional characteristics an investor may wish to consider when selecting a portfolio of securities or is obliged to include as practical restrictions reflecting common financial market conditions. Real features may include, for example, transaction lots, transaction costs, buy-in threshold on investments or number of securities.The objective of this section is twofold. We first introduce the concepts of relative and absolute models. In fact, the modeling of some real features is possible by using as decision variables the security shares (percentages). We call the models based on shares relative models and the investment variables relative. In several cases, however, the introduction of real features implies the need of variables that represent the absolute values of the capital invested in each security. We call this second type of models absolute models and the investment variables absolute. Then, we show how the introduction of real features modifies the portfolio optimization model and review main contributions in the literature dealing with portfolio real features.We define available capital the total amount of money that is available to the investor, both for the investment in securities and possible additional costs. In general, part of this money may also be left uninvested. The invested capital is the capital strictly used for the investment and that yields a return.More frequently in portfolio models, the invested capital coincides with the available capital. In this case the capital is treated as a constant parameterC‾, and possibly normalized to 1 in the case of relative models. This leads to relative models (RM), as presented in earlier section, with general structure as follows:(37)RM:maxz-λϱ(x)(38)z=∑j=1nμjxj(39)∑j=1nxj=1(40)0⩽xj⩽1j=1,…,n,with decision variables xjexpressing the shares of invested capital.While taking into account real feature it may be in some cases necessary to define absolute investments in securities (the invested amounts), e.g. to calculate transaction costs or meet lots size requirements. In order to clearly distinguish the absolute variables from the relative variables we denote the former with capital letters X. In the case of constant invested capitalC‾, absolute values can easily be defined by a linear transformationXj=C‾xj. Actually, when real features are considered, while the available capital is always a constant, the invested capital depends on investment opportunities (restrictions), transaction costs, etc., and it must be rather treated as a problem variable (we denote it as C). The same applies to any dynamic portfolio optimization models (portfolio rebalancing problems) where the amount of capital depends also on earlier returns. The introduction of the invested capital as a variable causes the use of the quadratic expression Cxjto represent the absolute investment in security j, j∈J.There are two ways to avoid the quadratic expressions Cxjin an optimization model. The simplest approach depends on directly dealing with absolute values instead of shares thus leading to the so-called absolute model (AM). For the sake of simplicity, we do not constrain the X variables to a set of linear constraints corresponding to the set Q that we have introduced for the relative models. Instead, we explicitly write the main linear constraints needed for the definition of a feasible portfolio. The trade-off model (11) formulated as an absolute model takes the following form:(41)AM:maxZ-λϱ(X)(42)Z=∑j=1nμjXj(43)∑j=1nXj=C(44)Xj⩾0forj=1,…,nwhere Z is the expected amount of the portfolio return and ϱ(X) is the risk of the portfolio X computed on returns as absolute values. The capital C must be obviously in some manner related with the available capital and/or some functions of decision variables. In the literature, the capital availability is frequently constrained through an upper and possibly a lower bound on the variable C:(45)C‾L⩽C⩽C‾U.Upper bound inequality (45) can be reformulated to take into account an explicit amount X0 of uninvested capital, thus eliminating the variable C and leading to the following balance equation:(46)X0+∑j=1nXj=C‾U.In practical implementations X0 may cover both the transaction costs and the possible money left uninvested. However, in the literature such a reformulation of the absolute model is not used.Alternatively, to avoid the quadratic expressions Cxj, one may consider shares as the amounts invested relatively to the capital availableC‾Urather than to the capital invested C, leading to the following relative to constant model (RCM):(47)RCM:maxz-λϱ(x)(48)z=∑j=1nμjxj(49)∑j=1nxj⩽1(50)C=C‾U∑j=1nxj(51)0⩽xj⩽1j=1,…,n.The invested amounts corresponding to the shares xjare now available as quantitiesC‾Uxjand the model has linear constraints. Again, the model can be reformulated to take into account an explicit share x0 of uninvested capital, thus standardizing the balance constraint (49) to the following:(52)x0+∑j=1nxj=1.Similarly to the absolute model, x0 may cover both the transaction costs and the possible money left uninvested in practical implementations.Both AM and RCM models are consistent with the corresponding bicriteria mean/risk (Markowitz-type) dominance. They are equivalent in the sense that they apply the same returns to both risk (safety) measure and mean criterion. The are linearly transformable when balanced with constraints (46) and (52), respectively. However, in the literature they are considered separately and, in some cases, they may lead to different results. The selected trade-off coefficient λ may ’push’ the value of the invested capital towards the lower or the upper bound. Actually, the trade-off optimization supports increase of the invested capital (if profitable). The other classical mean-risk bounding approach may lead to unjustified limitation of the invested capital. We illustrate this with the following example.Example Let us considerC‾L=awhile the capital to invest is equal to 2a(C‾U=2a). The set of alternatives consists of two securities. Security 1 has a minimum lot value of c1=a and return equal to r11=b, r12=b+2 with μ1=b+1. Security 2 is risk free with c2=a, r21=b+3, r22=b+3 and μ2=b+3. A current portfolio consists of one lot for security 1 and zero lot for security 2. Hence, it is defined byX1″=aandX2″=0in absolute model AM, byx1″=1andx1″=0in RM and byx1″=0.5andx1″=0in RCM. Suppose that this initial portfolio can be expanded (still keeping security 1) by including one lot of security 2. Expanded portfolio will be equal toX1″=aandX2″=ain AM,x1″=0.5andx1″=0.5in RM and RCM. Table 2provides the outcomes mean returns and risk values measured by the mean semideviation. In terms of preferences under risk the expanded portfolio is obviously much better than the original one. One may easily notice that all the models recognize improvement (higher expected return) while expanding the initial portfolio. However, only RM model shows also an improvement of the risk value (lower relative risk) while in AM and RCM models risk values do not change.In this section we discuss the main portfolio real features and their introduction in the portfolio optimization models specifying when the use of absolute or relative variables is required. Main contributions on portfolio problems with real features are also provided and classified.Some real features can be incorporated in both absolute and relative portfolio optimization models, whereas some others require the use of absolute variables. To discuss how some of these features can be incorporated in a portfolio optimization model we need additional binary variables zj, one for each security j, j=1,…,n. Variable zjwill be equal to 1 when security j is selected in the portfolio, and to 0 otherwise. In some optimization models, zjbehavior is enforced by means of the linear constraint zj⩾xjif the model is relative and byC‾zj⩾Xjif absolute. Note, however, that these conditions let variable zjfree to take value 1 when xj(Xj) is zero. While it is possible to prove that if xj=0 (Xj=0), then an optimal solution with zj=0 always exists, in practice the introduction of these binary variables is usually associated with investment threshold constraints as ljzj⩽xj⩽ujzjif the model is relative, and Ljzj⩽Xj⩽Ujzjif absolute, where uj=1 andUj=C‾.Main real features for portfolio selection problems can be classified as follows:1.Transaction costs In real financial markets transaction costs are entailed by purchases and sales of securities and are paid both in case of portfolio revision and in case of buy and hold investments. Transaction costs have a direct impact on portfolio performance so that ignoring them may result in inefficient portfolios (see Arnott & Wagner (1990)). Transaction costs may be fixed or variable.While variable transaction costs render individual securities less attractive but do not inhibit portfolio diversification, fixed transaction costs provide an explanation for reduced portfolio size. This is especially true for individual investors who, thanks to on-line trading services (see Baumann & Trautmann, in press), access the stock market and typically seek for the number of securities that optimally trades-off diversification against (fixed) transaction costs. On the contrary, for large institutional investors the amount of transaction costs may be practically meaningless with respect to the huge capital invested. Before the pioneering work by Patel and Subrahmanyam (1982) where fixed costs have been explicitly modeled in a mean variance portfolio problem with absolute variables, the fixed transaction costs were analyzed only indirectly by placing restrictions on the number of securities in the optimal portfolio (see, for instance, Levy (1978)).1.1Variable costs These transaction costs depend on the amount or on the share invested in each security.If cost is proportional, the models (AM) and (RM) can easily be adapted to incorporate such a cost by subtracting it from μjin the return constraint (42) and (38), respectively.In some cases, variable costs might be incurred only if capital invested overcomes a given amount. More precisely, non overlapping intervals are specified and a different cost percentage is applied depending on the interval in which the capital invested lies. This is the case of the entering commissions for mutual funds where the applied rates typically decrease when the capital invested increases (see Chiodi, Mansini, & Speranza (2003)). A structure with step increasing transaction costs can be found in Le Thi, Moeini, and Pham Dinh (2009). To model this feature we need to introduce a binary variables zijfor each security j and each interval of investment (and rate) i and to add a number of constraints depending on the number of securities and of intervals, i.e. Mi−1,jzij⩽Xij⩽Mijzijand∑i∈Izij⩽1, where Xijis the amount invested for security j in interval i, Mi−1,j,Mijare capital lower and upper bound for interval i, and I is the set of intervals. This feature can be similarly incorporated in a relative model (RM) provided that transaction costs are inserted only in return constraint.In fact, if costs are charged independently for each security and thus the total cost is the sum, over all securities, of a cost that depends on the amount of investment in each security, then the total capital invested (actually invested in securities and used to pay costs by an individual investor) cannot be assumed as known a priori and depend on the portfolio.Recalling the discussion of Section 4.1 about the cases where the capital invested is variable, we need to adapt to this case constraint (45), where C is a variable of the model. Let Kj(·) be the transaction cost function for security j. Then, the constraintin absolute models(53)C‾L⩽C+∑j=1nKj(Xj)⩽C‾U,in relative models(54)C‾L⩽C+∑j=1nKj(Cxj)⩽C‾Uneeds to be added. Notice that (54) introduces a quadratic expression into the relative model. Moreover, in general, Kj(·) might be a non linear function of the investment (see, for instance, Konno & Wijayanayake (2001) where the authors analyze a concave transaction cost for each security).Fixed costs Fixed costs are odd-lot commissions and/or lump taxes. A fixed cost fjis applied to each security j if selected in the portfolio (variable zj=1) or may be incurred if the security investment exceeds a given threshold (see, for instance, Kellerer et al. (2000)). In the latter case variable zjis forced to 1 if amount invested in security j is larger than a given amount Mj, i.e.zj⩾(Xj-Mj)/C‾, and 0 otherwise.In the literature, fixed and variables costs have been mainly dealt with in absolute models (see Table 3). In all these contributions but few exceptions, transaction costs are assumed to be incurred at the end of the period and therefore globally deducted from the portfolio return. In the past only Young in Young (1998) when dealing with transactions costs as possible real extension of his linear minimax model, inserts them also in the capital constraint. More recently, Woodside-Oriakhi, Lucas, and Beasley (2013) bounded transaction costs in a separated constraint.Transaction lots (rounds) A transaction lot, also called round, is a minimum transaction unit required to invest in a security. These constraints are common trading requirements implying that the investment in a security has to be expressed as a multiple of a transaction lot. Angelelli, Mansini, and Speranza (2008) show that ignoring transaction lots may result in selecting infeasible portfolios. More precisely, they prove that, in general, the set of securities selected by the optimal solution of an LP model considering transaction lots may not be included in the set of securities selected by its continuous relaxation.Let cjrepresent the monetary value of the transaction lot for security j and let wjbe the variable representing the number of lots of security j in the portfoliowj⩾0integer.Then, the transaction lots can be modeled as follows:•in absolute models(55)Xj=cjwj,in relative models(56)Cxj=cjwj.Applying transaction lot constraints, it may not be possible to exactly satisfy the budget requirement, thus budget is a variable C ranging in the interval[C‾L,C‾U]. Note that constraint (56) introduces a nonlinear relation into relative models.Many contributions are available in the literature on portfolio selection problems including transaction lots either in absolute and in relative models. For instance, absolute models that include transaction lots are presented in Mansini and Speranza (1999) and in Kellerer et al. (2000), whereas Streichert, Ulmer, and Zell (2004) introduce transaction lots into a relative mean–variance model. In Chang, Meade, Beasley, and Sharaia (2000) transaction lots are only mentioned. In Jobst, Horniman, Lucas, and Mitra (2001) the cash value of each transaction lot is expressed as a fraction of the portfolio wealth so that the portfolio weights are defined in terms of such fractions and the integer number of lots. Budget constraint is made ‘elastic’ using undershoot and overshoot variables, ∊− and ∊+, respectively, which are penalized in the objective function with a high cost, γ. In an optimum solution ∊− and ∊+ are made as small as possible so that the fractional holdings xisum to a value as close as possible to 1.Cardinality constraint One basic implication of modern portfolio theory is that investors hold well diversified portfolios. However, there is empirical evidence that individual investors typically hold only a small number of securities. Market imperfections such as fixed transaction costs provide one of the possible explanations for the selection of undiversified portfolios (see Wilding (2003)), but frequently the need to avoid costs of monitoring and of portfolio re-weighting leads investors to the common practice of limiting the number of securities (portfolio cardinality) that can be selected in a portfolio.Cardinality constraint can be expressed either as a strict equality or as an inequality imposing that the number of selected securities cannot be larger than a predefined number k(57)∑j=1nzj⩽k,and is usually associated with threshold constraints to correctly enforce the value of binary variables.Many works both on mean variance approach and on linear risk/safety measures have been presented in the literature dealing with cardinality constraint portfolio optimization. In Chang et al. (2000) the authors extend the relative mean–variance model to include the cardinality constraint. The same model was previously studied by Bienstock (1996). Mean–variance models with the cardinality constraint are presented in Jobst et al. (2001) and Liu and Stefek (1995), in Lee and Mitchell (1997), in Li, Sun, and Wang (2006), in Fieldsend, Matatko, and Peng (2004) and many others (see Table 3). All these models are relative. Linear models including cardinality constraint are proposed by Speranza (1996) and by Angelelli et al. (2008), Angelelli, Mansini, and Speranza (2012) and are all absolute models. In Sankaran and Patil (1999) Sankaran and Patil introduce the cardinality constraint into an absolute model. Finally, in Anagnostopoulos and Mamanis (2010) cardinality constraint is directly minimized as an objective function.Investment threshold constraints These constraints define lower and upper limits on the proportion/amount of each asset held in the portfolio. They may model institutional restrictions on the composition of the portfolio and usually are used to rule out negligible holdings of asset in the portfolio, thus making its control easier.If the constraint is on a single security it is commonly formulated as:(58)lj⩽xj⩽ujin relative models, and as(59)Lj⩽Xj⩽Ujin absolute models, where lj(uj) and Lj(Uj) are the lower (upper) bounds on the investment in security j, the former expressed in percentage, the latter in amount of capital. When such constraints are generalized to all the securities they are modeled using binary variables.In general, it may happen that a single security or a little diversified portfolio is SSD dominating over all other (more diversified) portfolios, and the SSD consistent Markowitz-type models will select such an undiversified solution. Especially, the SSD consistent models based on the LP computable risk measures may fail to generate sufficiently diversified portfolios. Therefore, additional restrictions may be set on the feasible portfolios to guarantee the required diversification. The simplest way to enforce portfolio diversification is to limit the maximum share as in (58) and (59). This, however, may result in a portfolio with a few equal shares depending on the value set to the maximum share. A better modeling alternative would be to allow for a relatively large maximum share provided that the other shares are smaller. Such a rich diversification scheme may be introduced with the CVaR constructs applied to the right tail of the distribution of shares (see Mansini et al. (2007) for a detail description). In particular, any model under consideration can easily be extended with direct diversification constraints specified as follows:(60)ksk+∑j=1ndkjs⩽γkanddkjs⩽xj-sk,dkjs⩾0j=1,…,n,where skis an unbounded variable (representing the kth largest share at the optimum),dkjsare additional nonnegative (deviational) variables, and γkis the upper bound on the total of the k largest shares.Finally, a lower and an upper bound on the investment may also refer to a set of securities instead than to a single one (class constraints). These are typical sector/industry constraints (see Chang et al. (2000) where they are only modeled and Anagnostopoulos & Mamanis (2010)). Let Gsbe a set of securities of the same sector s. A class constraint is, in relative models, formulated as follows:(61)ls⩽∑j∈Gsxj⩽us,where lsand usare lower and upper bounds expressed as percentage on the total unitary investment available for securities belonging to sector s. Similarly, in the case of absolute models, with the X’s instead of the x’s and the constants that represent amounts.It is worth noticing that all references reported in Table 3 for cardinality constraints and fixed costs also include threshold constraints typically used to enforce binary variables zjvalue. Thus, threshold bounds only refer to contributions where lower and upper bounds are the only real feature introduced or where investment bounds (especially upper bounds) are modeled without the use of binary variables.Decision dependency constraints Decision dependency requirements are common in financial dealings. To be correctly modeled, they need the binary variables zjalready described. Usually they take one of the following forms:Both securities i and j have to belong to the portfolio if security k is selected (joint investment):(62)zi+zj⩾2zk.Stock i cannot be selected if security j is in the portfolio (mutually exclusive investment):(63)zi+zj⩽1.Security i can be selected only if security j is in the portfolio (contingent investment):(64)zi⩽zj.Combinations of these conditions are also possible, resulting in more complex relationships. These investment restrictions can be an essential part of a diversification strategy for investing in, for instance, a mutual fund.An early study that incorporated some of these conditions is by Weingartner (1963). Examples of such kind of constraints can also be found in Syam (1998) and Young (1998), where the author mentions them without any experimental application.In Table 3 we summarize the main contributions available in the literature on portfolio selection problems with real features. We classify them according to the real features considered and for the type of model (relative or absolute) in which real features have been inserted. References are sorted by year of publication.In the last years developments in portfolio optimization have been especially stimulated by efficiency issues, i.e. by the capability to handle in an efficient manner portfolios with a large number of securities and scenarios and possibly including real features.Without real features, also a quadratic mean–variance model can be readily solvable using a standard quadratic programming solver, and methods available are quite competitive also with respect to linear models. Indeed computational issues may still arise, but only for problems of very large size and when solutions are needed quickly. On the contrary the introduction of real features when requiring integer and/or binary variables may increase problem complexity significantly, and the gap between linear and quadratic models solution efficiency and effectiveness may become relevant. We will devote a part of this section to the analysis of solution algorithms for portfolio optimization with real features dividing them in exact and heuristic approaches. The focus will be on methods proposed for solving mixed integer linear programming portfolio problems, but main references on solution algorithms for the mean–variance model with real features will also be surveyed.Another important computational issue on portfolio optimization is related to the solution of very large size problems including a high number of securities/stocks and scenarios. LP models have a number of constraints proportional to the number of scenarios, whereas the number of variables is proportional to the total of the number of scenarios and of instruments (see Table 1). They can be solved effectively with general purpose LP solvers provided that the number of scenarios is limited. In real-life contexts, financial decisions are usually based on simulation models employed for scenario generation where one may have several thousands of scenarios. This may lead to the solution of LP models with huge number of variables and constraints, thus decreasing their computational efficiency and making them hardly solvable by general LP tools. A part of this section will discuss recent results from the literature showing how computational efficiency in solving huge LP portfolio problems can be addressed.Nowadays, computationally effective algorithms for the exact solution of nonconvex quadratic programming in which the feasible region is a mixed-integer set do not exist, and until recently there has been relatively little work presented in the literature on this subject. Thus, while most of the solution methodologies that tackle discrete features in portfolio selection with mean–variance formulation are heuristic in nature, the computational challenge of solving large real portfolio problems has justified an increasing interest for mixed integer LP portfolio models and for both their exact and heuristic solutions. We recall that finding a feasible solution for the portfolio selection problem with minimum transaction lots and for the portfolio selection problem with fixed costs have been proved to be NP-complete problems (see Mansini & Speranza (1999) and Kellerer et al. (2000), respectively).In the following we will analyze the main solution algorithms proposed in the literature for LP models classifying them according to their nature in heuristic and exact solution approaches. Even if the main focus is on mixed integer linear programming models, we briefly survey also main solution methods for the mean–variance model with real features. Table 4report references of exact algorithms and Table 5of heuristic methods. References are sorted by year of publication.A major advantage of modeling a problem as a mixed integer linear programming problem is that, if the problem is of small size, it can be solved by a standard (general purpose) MILP solver. However, if the problem is of medium or large size (as for portfolio problems) the continuous relaxation of the MILP problem may convey useful information for its solution. Indeed, almost all the heuristics proposed in the literature for MILP portfolio problems use as starting point the optimal solution of the continuous relaxation either to get a feasible solution through some rounding procedure or to “measure” the likelihood of a variable to be in the optimal solution (i.e., to take a positive value in the optimal solution of the MILP problem).Speranza (1996) analyzes a portfolio problem based on mean absolute semideviation including minimum transaction lots, fixed and proportional transaction costs. An intuitive rounding procedure of the continuous relaxation optimal solution to satisfy model constraints is proposed.Angelelli et al. (2008) provide a financial and computational comparison of MAD and CVaR models with real features analyzing their performance on real size instances. At this aim they use simple and effective heuristics to be used when integer optimal solutions cannot be found in a reasonable amount of time. Since the optimal solution of the continuous relaxation of the proposed models can be efficiently computed by means of a standard commercial software as CPLEX and the time required is very small, even on problems of realistic size, the basic idea of such heuristics is that securities selected in the optimal solution of the continuous relaxation or with the smallest reduced costs are the most interesting. Then, non-interesting securities are discarded and the set of interesting securities is taken as the only set on which model with real features is solved. The size of the models becomes in this way much smaller and the optimal solution can be obtained by means of a software in a reasonable time.This idea of identifying a subset of more significant securities was firstly proposed in Mansini and Speranza (1999) to solve a portfolio problem with transaction lots optimizing the mean semideviation risk measure, and further developed in other papers where different portfolio real features were considered (see Kellerer et al. (2000), Chiodi et al. (2003)) up to a more general heuristic framework called Kernel Search (see Angelelli et al., 2012) including and extending all previous approaches. The main idea of Kernel Search is to obtain a solution, of hopefully high quality, from a small set of promising securities, called the kernel. The kernel is initially built using information provided by the solution of the linear relaxation of the original problem. Then, new promising securities are identified by solving a sequence of small/moderate size MILP problems. The first MILP problem is restricted to the initial kernel. Any other MILP problem in the sequence is restricted to the previous kernel plus a set of other securities that were initially excluded. The solution of the current MILP problem may identify some new securities not yet included in the kernel. If this is the case, such new securities are added to the kernel. The possibly updated kernel will be forwarded to the next MILP problem of the sequence. The kernel increases in a monotonic way, i.e. no security will be discharged at any time, and the solution of any MILP problem in the sequence provides a bound on the optimal solution for all the successive ones. One of the main issues the authors address concerns the size of these MILP problems. This value should be small enough to limit the computational time required to solve each MILP problem and large enough to be likely to contain most of the difficult to select securities (i.e. those that can be selected only if all securities were considered altogether). Different heuristics can be designed as implementations of the proposed Kernel Search framework. Such heuristics have two major characteristics relevant from a practical point of view. The first one is that they require little implementation effort because the most cumbersome part of the search is carried out by a software for the solution of linear and mixed integer linear programming problems. The second characteristic is that the same heuristic can be easily applicable also to other problems. The authors apply several of such heuristics and test them on a complex portfolio optimization problem taking different real features such as minimum transaction lots and cardinality constraint into account. The model maximizes a performance measure represented by the CVaR. Indeed, since the Kernel Search framework exploits a major characteristic of the portfolio selection problem which is the fact that the number of securities selected by an optimization model is usually quite small, independently of the initial size of the problem, any other mixed integer linear programming formulation using a different performance measure could have been used. Kernel Search can also be easily applied also to other combinatorial optimization problems (see, for instance, Angelelli, Mansini, & Speranza (2010)). Computational results show that this general method is extremely effective finding the optimal solution in almost all tested instances involving up to 600 securities and 104 scenarios (2years weekly returns).In Mansini and Speranza (2005) a local search heuristic is proposed to solve a mixed integer linear programming portfolio problem with transaction costs and minimum lots. The method is based on the optimal solution of the continuous relaxation of subproblems formulated considering a subset M of securities and by adding a cardinality constraint∑j∈Mzj=l. Value of parameter l is iteratively changed by means of a local search procedure. At each iteration an integer solution is constructed by using the optimal solution of the current relaxed subproblem. More precisely, local search is guided by the parameter l and a parameter w, which represents the maximum number of iterations allowed without any improvement of the objective function value. First, the value of the parameter l is decreased (downside search phase). At each reduction of one unit of l, the procedure used to construct an integer solution is repeated and a new subproblem is solved. If the current objective function value is not improved, the value of the parameter w is decreased by one unit. The downside search phase ends when l=1 or w=0. Then the procedure searches for higher cardinality portfolios (upside search phase). The value of l is increased from the initial value to ∣M∣. The upside search phase ends when l=∣M∣ or w=0. The procedure is used as initial solution of an exact algorithm. Computational results show that this procedure is extremely efficient and quite effective.In Lin and Liu (2008) the authors present three possible relative models for portfolio selection problems with minimum transaction lots. One of these models is based on MAD measure of risk and is thus a mixed integer linear programming problem. The authors devise genetic algorithms (GA) to solve all proposed models using Taiwanese mutual fund data from the year 1997 to 2000. The results of the empirical study show that the portfolios obtained using the proposed algorithms are very close to the efficient frontier, indicating that the proposed method can obtain near optimal and also practically feasible solutions to the portfolio selection problem in an acceptable short time (no more than few minutes). This paper shows that a general metaheuristic approach as GA can be easily adapted to solve different problems optimizing MAD as well as variance.In Gilli and Këllezi (2002), the authors point out that financial optimization problems using measures of risks as Value at Risk (VaR), expected shortfall, mean semi-absolute deviation, semi-variance may become quite complex exhibiting multiple local optima and discontinuities, in particular when the trading variables are restricted to integers, constraints are added on the holding size of assets or on the maximum number of assets in the portfolio. In these cases classical optimization methods may fail to work efficiently and heuristic optimization techniques may be the best alternative. They show how the particular optimization heuristic, called Threshold Accepting (TA) proposed by Dueck and Winker (1992), can be successfully used to solve complex portfolio choice problems. TA is a meta-heuristic from the class of local search algorithms. The method is similar to simulated annealing, but using deterministic rule to escape local optima by accepting solutions which are not worse by more than a given threshold.Indeed, several contributions can be found in the literature on metaheuristic approaches for solving the mean–variance model with different real features (we refer to Di Tollo & Roli (2008) for a comprehensive survey). See, for instance, among the others, the metaheuristics proposed for portfolio selection with cardinality constraint: Maringer and Kellerer (2003) introduce an iterative hybrid algorithm combining local search strategies with principles of simulated annealing and evolutionary strategies; Anagnostopoulos and Mamanis (2010, 2011) apply multi objective evolutionary algorithms (MOEA) and state of the art evolutionary multi objective optimization techniques, namely the Non-dominated Sorting Genetic Algorithm II (NSGA-II), Pareto Envelope-based Selection Algorithm (PESA) and Strength Pareto Evolutionary Algorithm 2 (SPEA2), providing their performance comparison; Fieldsend et al. (2004) provide a modified MOEA to optimize constrained portfolio frontiers in parallel; Fernández and Gómez (2007) present a neural networks method; Crama and Schyns (2003) use a simulated annealing method, whereas Chang et al. (2000) apply three heuristics based upon genetic algorithms, tabu search and simulated annealing.Metaheuristics provide flexible and powerful solving strategies that can effectively and efficiently tackle various instantiations of the portfolio problem also considering different objective functions other than variance. Since basic building blocks of metaheuristics such as the search space and the neighborhood structures usually do not depend on the problem objective function, we believe that all metaheuristic methods proposed for mean–variance model with real features could be easily extended to the corresponding problems based on LP risk measures with a large saving in terms of computational time for evaluating solutions. This is still an open issue of high interest.In the past the lack of computers performance (in terms of power and memory capability) made even small size LP portfolio problems with real feature difficult to solve by standard MILP tools (see, for instance, Mansini & Speranza (1999)). Nowadays, the size of problems solved has increased (see Angelelli et al. (2008)), while the development of new specialized exact solution algorithms has made its appearance in the literature. In this section we will survey these specialized exact approaches as methods going beyond a pure model solution through a standard software.To the best of our knowledge, Mansini and Speranza (2005) propose the unique exact approach for a MILP portfolio problem. Other contributions can be found in the literature where the portfolio problems optimize some LP risk measures, but the resulting model is nonlinear due to the introduction of concave transaction costs (see Table 4). In the following we will briefly survey all of them.Mansini and Speranza (2005) study the problem of portfolio selection in which the mean downside underachievement (see (17)) is maximized while taking into account fixed transaction costs and integer transaction units (rounds). A capital–gain tax is also considered as a percentage of the portfolio return. They propose an exact algorithm able to significantly reduce the memory and time resources required by CPLEX to find the problem optimal solution. The algorithm structure is quite general and is based on the idea of partitioning the feasible solution set of the initial problem P into subsets and then solving the problem over each of the subsets. More precisely, the method generates subproblems by introducing inequalities (cuts) to the initial problem P. Subproblems are solved in sequence so that the solution value of problem P(i) can be used as a cutoff bound for problem P(i+1). In particular, the instantiation for this problem uses information provided by the optimal solution of the continuous relaxation to partition problem P into two subproblems. Assets with a reduced cost lower than a given threshold belong to the first subproblem. The second subset is obtained by adding to problem P an inequality imposing to select at least one asset from those not belonging to the first set. The selection of an appropriate subset of assets entering the first problem is a critical step in the algorithm. Such a subset should be small enough to make the first subproblem easy to solve but, at the same time, should be large enough to contain, with a high probability, the subset of securities which are selected in the optimal solution. This algorithm is very simple to implement, when a software for the solution of linear and mixed-integer linear programs is available. Moreover, its structure is quite general and can be easily extended to any other mixed-integer programming model for portfolio selection. Authors solved instances with up to 1000 securities and 300 scenarios (almost 6years weakly returns).Konno and Wijayanayake (2001) analyze a portfolio construction/rebalancing problem under concave transaction costs and minimal transaction unit constraints while employing mean absolute deviation as risk measure. Since the transaction cost function C(x) is separable, i.e.,∑j=1nCj(xj), the authors propose a branch and bound algorithm exploiting this structure. In particular, they solve linear programming subproblems by introducing (piecewise) linear underestimating function for the concave transaction cost functions. As claimed by the authors, due to the recent progress in global optimization, one can solve a fairly large scale linearly constrained concave minimization problem using the special structure of the problem. Nevertheless, the success of their branch and bound algorithm critically depends upon the employment of the absolute deviation as risk measure. The proposed method allows the solution of problems with up to 200 stocks and 60 scenarios (monthly returns).Konno and Yamamoto (2005) consider a portfolio optimization problem based on absolute deviation as risk measure where transaction cost functions are piecewise linear concave and piecewise constant with several jumps. The standard approach for handling a concave or piecewise constant cost function is to introduce a number of 0–1 variables and solve the resulting 0–1 integer programming problem by branch and bound or branch and cut algorithms. When, however, the number of linear pieces (or number of jumps) is large, then the problem becomes more difficult requiring the introduction of many integer variables. Their work aims at comparing the branch and bound approach proposed in Konno and Wijayanayake (2001) with state-of-the-art integer programming approach, proving the former method being much faster.In Konno, Akishino, and Yamamoto (2005), the authors considered a long-short portfolio optimization problem in the mean–absolute deviation framework where one can sell assets short if this leads to a better risk-return structure of the portfolio. The purpose of their paper is to propose a branch and bound algorithm for solving a class of long-short portfolio optimization problem with concave transaction costs (when purchasing) and difference of two convex functions (d.c.) transaction costs (when selling). The first step for solving the problem is to replace nonconvex cost function by their maximal linear underestimating functions and then use a branch and bound approach. Their experiments consider up to 84 scenarios (monthly return) and 225 stocks.In the recent work by Le Thi et al. (2009), the authors address a portfolio optimization problem under step increasing transaction costs using mean absolute deviation as risk measure. The step increasing functions are approximated, as closely as desired by a difference of polyhedral convex functions. Then they apply the difference of convex functions algorithm (DCA) available from the literature (see Pham Dinh & Le Thi, 1998) to the resulting program. For testing the efficiency of their method they compare it with CPLEX and the branch and bound algorithm proposed by Konno and Yamamoto (2005) on instances with 457 stocks and 289 scenarios (weekly returns).Finally, Table 4 reports different exact approaches also for mean–variance model with real features. In particular, Syam (1998) analyzes a problem with dependency constraints and round lots. He assumes independence among risky securities, which leads to a diagonal covariance matrix, and then adopts dual ascent and branch and bound solution methods. Bienstock (1996) consider a cardinality constrained portfolio optimization problem and discuss a number of valid inequalities (cuts) for the problem to be used in a branch and cut algorithm. Computational results were presented for both sequential and parallel implementations of his algorithm involving up to 3897 assets. Lee and Mitchell (1997) study a cardinality constrained portfolio optimization problem and describe an interior-point algorithm within a parallel branch-and-bound framework for solving nonlinear mixed integer programs. Best and Hlouskova (2005) analyze mean–variance problem with transaction costs and develop an exact algorithm for its solution in terms of a sequence of subproblems with corresponding savings in computer time and storage. The key idea was to treat the transaction costs implicitly rather than explicitly. Li et al. (2006) analyze a round-lots and cardinality constrained portfolio selection under concave transaction costs. The resulting model is a nonseparable, nonconvex, nonlinear integer programming problem. The authors exploit the special features of the mean–variance formulation to develop a convergent Lagrangian and contour-domain cut method as an exact solution algorithm and test it on instances with 30 stocks and three years monthly returns. Xue, Xu, and Feng (2006) modify mean–variance portfolio to introduce concave transaction costs and thresholds on investment. They propose an exact approach based on a branch and bound method using underestimation functions for the concave transactions costs. They solve instances with only 9 securities.In portfolio models stock returns are represented by their realizations under T scenarios. In LP models, the number of structural constraints (matrix rows) is proportional to the number of scenarios T, while the number of variables (matrix columns) is proportional to the total of the number of scenarios and the number of instruments T+n. The fact that the model dimensionality is proportional to the number of scenarios T, does not cause any computational difficulties if a few hundreds of scenarios are taken into account. This is the case of the common computational analysis based on historical data. However, real-life financial analysis must be usually based on more advanced simulation models employed for scenario generation (Carino et al. (1998)) using several thousands of scenarios (see Pflug (2001), Guastaroba, Mansini, & Speranza (2009b)). This leads to LP models with a huge number of auxiliary variables and constraints and thereby hardly solvable by general LP tools. Actually, in the case of fifty thousand scenarios and one hundred instruments the model may require more than one hour of computational time with the state-of-art LP solver (CPLEX) or even remain unsolved. To overcome this difficulty some alternative solution approaches were developed trying to reformulate the optimization problems as two-stage recourse problems (Künzi-Bay & Mayer (2006)), to employ nondifferential optimization techniques (Lim, Sherali, & Uryasev (2010)), cutting planes (Fabian, Mitra, & Roman (2011)) or to approximate the returns with a factor representation (Konno, Waki, & Yuuki, 2002).More recently, in Ogryczak and Sliwinski (2011) Ogryczak and Śliwiński show that the computational efficiency can be simply achieved with an alternative model formulation taking advantage of the LP duality. In the introduced model the number of structural constraints is proportional to the number of instruments n while only the number of variables is proportional to the number of scenarios T, thus not affecting so seriously the simplex method efficiency. The new model can effectively be solved with general LP solvers even for very large numbers of scenarios. In this case, the computational time for the case of fifty thousand scenarios and one hundred instruments becomes lower than one minute. The authors test such a reformulation for all the classical LP portfolio optimization models using medium scale instances with 5000, 7000 and 10,000 scenarios and 76 securities, and large scale tests instances with 50 or 100 securities and 50,000 scenarios. Computational advantages are particularly evident for the model based on the Weighted CVaR measures defined as combinations of CVaR measures for m tolerance levels and for model based on Gini’s mean difference (Yitzhaki, 1982) where standard formulation require T2 auxiliary constraints which makes them hard already for medium numbers of scenarios, like a few hundred scenarios given by historical data.Certainly, for large scale problems potential use of parallel optimization algorithms might be crucial for the solution process efficiency. Parallel algorithms for large scale (stochastic) linear programming financial models have been successfully developed as presented by Censor and Zenios (1997) or Vladimirou and Zenios (1999). LP computable risk measures enable application of parallel optimization methods as shown by Pflug, Świetanowski, Dockner, and Moritsch (2000) for the financial model employing the MAD measure.

@&#CONCLUSIONS@&#
Since the milestone work by Markowitz on mean–variance portfolio selection problem, many alternative risk and safety measures have been proposed that are computationally attractive as LP computable in the case of discrete random variables. The LP solvability is very important for applications to real-life problems where the portfolios have to meet numerous side constraints as transaction lots, minimum or maximum investment thresholds, and cardinality constraints or account for transaction costs. The inclusion of real features in a model has in most cases relevant consequences in terms of modeling. The first is that it may be necessary to express the decision variables in terms of absolute value of the investment. The second is that the real features usually imply the need of integer and binary variables that make the model computationally hard to solve.In this paper we have introduced and surveyed the LP solvable portfolio optimization models presented in the literature. We have also discussed the relative (variables as percentages of the capital) and absolute (variables as absolute values of the capital) form of the models. The various real features of portfolio selection problems are discussed and the related literature surveyed, including the computational approaches adopted for the solution of the resulting optimization models.