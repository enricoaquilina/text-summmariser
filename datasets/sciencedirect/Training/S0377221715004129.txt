@&#MAIN-TITLE@&#
Stochastic inflow modeling for hydropower scheduling problems

@&#HIGHLIGHTS@&#
We model inflows for hydro-scheduling problems to be solved by the SDDP algorithm.Problems can be solved exactly as posed; no scenario sampling required.The model is fitted to univariate inflow time series by quantile regression.

@&#KEYPHRASES@&#
OR in energy,Hydro-thermal scheduling,Stochastic dual dynamic programming,Time series,Quantile regression,

@&#ABSTRACT@&#
We introduce a new stochastic model for inflow time series that is designed with the requirements of hydropower scheduling problems in mind. The model is an “iterated function system’’: it models inflow as continuous, but the random innovation at each time step has a discrete distribution. With this inflow model, hydro-scheduling problems can be solved by the stochastic dual dynamic programming (SDDP) algorithm exactly as posed, without the additional sampling error introduced by sample average approximations. The model is fitted to univariate inflow time series by quantile regression. We consider various goodness-of-fit metrics for the new model and some alternatives to it, including performance in an actual hydro-scheduling problem. The numerical data used are for inflows to New Zealand hydropower reservoirs.

@&#INTRODUCTION@&#
In a hydro- (or hydro-thermal) scheduling problem, one aims to optimally manage the operation of water reservoirs over a period of time. The objective may be to minimize the shortage that ensues when reservoirs run dry, or (more generally) the economic cost of meeting a given demand for electricity from hydro and other sources. Alternatively, it may be to maximize the revenues from selling water or hydro-electricity in external markets.In all such problems, the central element is a stochastic model for natural inflows. This will typically be an autoregressive or Markov process which has been obtained by fitting to observed data.The leading technique for the solution of hydro-scheduling problems is stochastic dual dynamic programming (SDDP). SDDP is a sampling-based version of the nested decomposition algorithm which dates back to Pereira and Pinto (1991). Originally, it was designed to solve risk-neutral multistage stochastic linear programs. Recently, two extensions of the method to deal with risk-averse multistage stochastic programs have been proposed in Guigues and Römisch (2012a), Guigues and Römisch (2012b), Shapiro (2011) and have been applied to hydropower scheduling problems in Guigues (2014a), Philpott and de Matos (2012), Shapiro, Tekaya, Costa, and Soares (2013). The convergence of the method is proved in Philpott and Guan (2008) for risk-neutral linear programs, in Girardeau, Leclere, and Philpott (2015) for risk-neutral convex programs, and in Guigues (2014b) for risk-averse convex programs.In problems amenable to SDDP, the flows and other quantities at each time step are described by a linear programming problem; thus they are continuous variables. But the SDDP algorithm also involves the computation of expectations, which (as a practical matter) requires a discrete probability space. For problems formulated with continuously-distributed random variables, the usual tactic is to replace each continuous distribution with a finite sample from it, giving a “sample average approximation” of the original problem.Note that when we resort to sampling in this way, we incur both a statistical sampling error (when the inflow model is fitted to the original data) and a probabilistic, or simulation sampling error (when the fitted continuous inflow model is replaced by a discrete approximation obtained through sampling).The motivating idea of the present paper is that errors of the second type should be avoidable by designing an inflow model in which the inflow variables themselves are continuous, but the random innovation at each time step is discrete. Such an inflow model renders the associated hydro-scheduling problem solvable by SDDP exactly as posed. The process of fitting the inflow model obtains, in one step, a good representation of the original data by a model of the final form required.Another advantage of such a model is that it may actually be more general. By ignoring standard continuous model types (autoregressive, ARMA, etc.) and instead considering classes of discrete-innovation models that play well with SDDP, it may be possible to make better use of the limited flexibility that SDDP ultimately allows.The present paper will not attempt to give a complete description of the SDDP method as applied to hydro-scheduling problems. Instead, we refer the reader to Girardeau et al. (2015), Guigues (2014a), Guigues and Römisch (2012a), Guigues and Römisch (2012b), Philpott and Guan (2008), Pereira and Pinto (1985), Pereira and Pinto (1991), Shapiro (2011) for the full details, and focus here on those considerations relevant to stochastic modeling.SDDP is an algorithm for solving multi-stage stochastic convex programming problems. In a practical implementation, the convex functions will be piecewise linear, which gives the problem the following general form. We wish to compute, or at least estimate, the functions(1)gt−1(x)=E[minu{ct·u+gt(u):Atu=bt+Btx,u≥0}]for t = 1, … , T. Here ct, btare random vectors and At, Btare random matrices. The vector u may be thought of as representing the decisions taken during the tth time period, or stage; we aim to make these decisions so as to minimize the sum of their immediate cost ct· u and the expected cost gt(u) of their consequences in stages t + 1, … , T. The possible decisions are constrained by the corresponding decisions x taken at the previous stage (Atu = bt+ Btx).As a simple example, consider the classic hydro-thermal scheduling problem for a single reservoir, in which we must meet a demand δtfor energy in stage t with some combination of thermal generation (which costs γ per unit) and hydropower (which costs nothing, though the water supply is limited). There is a random inflow Itin stage t. The decision vector is u = (uθ, uH, uS), with the components representing thermal generation, hydro generation, and reservoir storage at the end of the stage. Then the problem is(2)gt−1(x)=E[minu{γuθ+gt(u):uθ+uH=δt,uH+uS=It+xS,u≥0}[minu{γuθ+gt(u):uθ+uH]Note that the formulation (1) implies that the random elements at each stage (ct, bt, At, Bt) are independent of those at all previous stages (because gt(u) is non-random). For problems involving serially dependent processes, this limitation can be overcome by adding “decision” variables whose purpose is to transmit stochastic information to future stages. For example, suppose the inflows (It) above behave as an AR(1) process:It=αIt−1+Et,where the Etare independent. We can incorporate this behaviour into the problem by giving u an additional component uIto represent the current stage inflow, and writing the constraint set in (2) asuθ+uH=δt,uH+uS−uI=xS,uI=αxI+Et,u≥0.Now suppose that a problem of the form (1) is to be solved by stochastic dual dynamic programming (SDDP). In this method, the most demanding step (the “backward pass”) requires the computation of the expectation in (1). In problems where the random elements ct, bt, At, Bthave a discrete joint distribution, this can be done by considering all their possible values (and solving the optimization problem for each one). Otherwise, it is usually necessary to resort to sampling. Replacing the true probability distribution by a finite sample at each stage gives a “sample average approximation” of the problem which we can solve.Note that there is no requirement in SDDP for the problem’s stages to resemble each other closely; it is enough that they all fit the general form (1). The implication for modeling is that random processes can be non-stationary – indeed, it is hardly necessary to think of them as processes at all. An inflow process, for example, could be modeled not only with different parameter values for winter and summer, but by entirely different statistical techniques for different times of year – a point exploited by Purdie and Bardsley (2010). The only requirement is that, whatever methods are used, the model must eventually be expressible in the form (1).Consider the problem of developing a stochastic model of a (univariate or multivariate) inflow process, from raw data consisting of a time series of flow observations. The model should be convenient for use within a hydro-scheduling problem of the general form (1).It is well-known that stagewise-independent inflow processes are inadequate for this purpose. Inflows to real hydrological catchments usually have positive serial correlation; the effect of this is to increase the risk that reservoirs will run dry or overflow. To introduce appropriate serial dependence, our model must be expressible in the general explicit form(3)Xt=Ft(Wt−1).That is, the inflow Xtto the catchment at stage t is expressible as some random function Ftof a state variable Wt−1. The latter is a vector whose entries comprise some of the previous stage subproblem’s variables; it can be thought of as representing the “climatic state” at the previous stage. In the univariate (single catchment) case, a natural choice would be Wt−1 = Xt−1 (or possibly Wt−1 = (Xt−1, Xt−2, … , Xt−r), for a higher-order model).The random functions(Ft)t=1Tmust be independent, and concave with probability 1; in an actual implementation they will be expressed as piecewise linear functions:(4)Xt≤minjFtj(Wt−1),where theFtjare random affine functions. Although the formulation requires an inequality (≤), the intent is that equality will prevail at optimality. This will generally be so in situations where water has positive marginal value (i.e. more is better). It may fail in situations where water has zero marginal value (e.g. spilling from a reservoir) or negative marginal value (e.g. a binding maximum flow constraint on a canal). To maintain the equality in these situations, it may be necessary to place a “big M” penalty on the amount of any inequality.The present paper will confine itself to first-order models in which Ftitself is affine with probability 1. This turns out to be a quite adequate model in many instances, and avoids any restrictions on the sign of the marginal water value. That is, we have(5)Xt=Dt+MtXt−1,where Dtis a random vector and Mta random matrix. In the univariate case, Dtand Mtare random variables.A commonly-used model is the autoregressive (AR(1)) process:Xt=αt+βtXt−1+Et,where αt, βtare deterministic and the innovation process (Et) is a sequence of independent random variables. This gives a special case of (5) in which only the intercept Dt= αt+ Etis random, while the slope Mt= βtis deterministic.Alternatively, the process may be applied at the logarithmic level:log(Xt)=αt+βtlog(Xt−1)+Et.(It is common to transform flow data by taking logarithms, because they often have right-skewed distributions.) This givesXt=eαtXt−1βteEt.The functionx↦xβtis concave (provided 0 < βt< 1, which we may expect), so we have a model of the required form. A piecewise linear approximation of the sort in (4) will be required; if this is simply an affine piece we obtain a model(6)Xt=(θt+ηtXt−1)Rt,where θt, ηtare deterministic and Rt= exp (Et) is random. See Shapiro et al. (2013) for examples. This approach gives another special case of (5): both the intercept Dt= θtRtand the slope Mt= ηtRtare random, but their randomness derives from the factor Rt, which they have in common. The only difference between “wet” and “dry” realizations of the random linear function Ftis a constant multiplicative factor.Thus, neither variety of autoregressive model takes full advantage of the flexibility offered by the form (5).The approach championed in the present paper will be to give Ftin (3) a finite discrete distribution. That is, there will be a finite collection of (affine) functionsft1,…,ftmtgiving the possible values of Ft, with associated probabilities. The main motivation for this idea is the thought that we will eventually need a model of this form anyway, for (SDDP) computational purposes. Rather than approximating the data with a continuous model, and then approximating that model with a discrete one (a sample average approximation) by sampling, it might be more efficient to seek from the outset the best possible representation of the original data by a discrete model of the final form required.Note that (3) with discretely distributed Ftimplies that each Xtis also discretely distributed (provided the initial X0 is discretely distributed). However, the number of possible values of Xtmay be combinatorially large (as many as∏s=1tms). It is thus useful in practice to continue to think of Xtas a continuous variable, even when it is not.Stochastic processes of the general form (5) with discretely distributed random affine mappings have been studied for several decades under the name “iterated function systems”. See (Barnsley & Demko, 1985; Diaconis & Freedman, 1999) for early papers; also (Barnsley, 1989; Forte & Vrscay, 1995; Torre & Vrscay, 2012; Yang, 2013). The main problem in the field is the inverse problem: given a probability distribution μ on Rd, find an iterated function system that reproduces μ. That is, we seek a finite collection f1, … , fmof affine maps on Rd, with associated probabilities p1, … , pm, such that when the Ftare drawn independently from this discrete distribution, the Markov process defined by Xt= Ft(Xt−1) has limiting distribution (approximately) μ. The principal application to date has been to graphic image compression with d = 2.In applying the iterated-function-system technique to inflow modeling, we face a couple of complications. One is that the targeted probability distribution for inflow varies seasonally. The Markov process (Xt) must therefore be time-inhomogeneous, with the distribution of Ftvarying in a periodic (seasonal) fashion. Another is that we aim to reproduce not only the correct distribution for Xt, but also the serial dependence structure of the inflow process.In the univariate case, both complications can be addressed by a technique of fitting the distribution of Ftvia a quantile regression of Xtvs. Xt−1 and seasonal regressors. The method will be described in detail in the next section.This section considers the inflow modeling approaches described in Section 2 in more detail, using a typical example univariate flow data set to illustrate (Fig. 1).The example data are weekly natural inflows to the Waitaki River system in New Zealand. This system includes eight hydroelectric generation sites, of which only the three furthest downstream have access to the whole flow. The water flows between dams are not heavily constrained, meaning that a hydropower scheduling model could reasonably treat the entire catchment as a single equivalent-energy reservoir. Accordingly, all inflow values have been converted to their electrical energy equivalent, giving a univariate time series for total inflow expressed in power units (megawatts).The sources of the inflow include both summer melting of snowpack and glaciers in high mountain areas and year-round rainfall in lower-lying parts of the catchment. This combination gives the raw inflow series a fairly strong seasonal dependence, with average summer peak flow around three times the average winter minimum.As with many other hydrological series, the distribution of weekly inflow is strongly right-skewed, with prompt runoff from storm events producing observations exceeding three times the seasonal average at the rate of about one per year. However, there is no obvious concentration of these extremes at any particular time of year.There is also a clear serial correlation, with a correlation length on the order of a few weeks. Closer examination reveals that this correlation structure also varies seasonally, with the strongest serial correlations in the late winter, and the weakest in late summer. This presumably reflects the different physical processes dominating at different times of year.We begin by applying an initial scaling to the inflow series (Xt) :Qt=Xt/g(t),where g(t) is a seasonal periodic function. We work thereafter with (Qt). This scaling is not meant to imply any assumed stationarity of (Qt); it merely simplifies the problem a little for later modeling efforts, by obviating the need to deal with large seasonal changes of magnitude.The function g(t) is constructed by Fourier regression at the logarithmic level:(7)log(Xt)=f(t)+Zt,where f(t) is a fitted trigonometric polynomial and (Zt) are mean-zero errors. (The trigonometric polynomial form is used because the data are weekly, so observations do not fall on the same dates in each year. For monthly data, it would be simpler to just estimate a seasonal adjustment factor for each of the 12 months.) We then takeQt=eZtand g(t) = ef(t), making g(t) an estimate of the conditional geometric mean inflow for the time of year of t.Fig. 2shows the fitted scale function g(t) for the example data set. The fitting, and all other statistical analysis in the present paper, were conducted using the statistical language R (R Development Core Team, 2009). Code and data are available from the author on request.Following the approach outlined in Section 2.1, suppose we now postulate an AR(1) model for Zt(= log Qt):(8)Zt=γ(t)Zt−1+ϵt.Here γ(t) is a seasonally varying coefficient and (εt) a white noise process. There is no need for a constant term because (7) has removed it. This givesQt=Qt−1γ(t)Rt,where Rt= exp (εt). Since Qt≈ 1 is a typical value at any time of year, we approximate the function q ↦ qγ(t) by its linearization about q = 1, i.e.qγ(t)≈1−γ(t)+γ(t)q,giving the model(9)Qt=(1−γ(t)+γ(t)Qt−1)Rt.The estimation of γ(t) requires some care. Perhaps the most obvious method is to fit the linear autoregression (8): this will be good as far as (8) goes, but it makes no allowance for the effect of the subsequent linear approximation step. Alternatively, one could reason (as in Shapiro et al. (2013)) that (9) with Rt= 1 suggests the autoregression(10)Qt−1=γ(t)(Qt−1−1)+error.This approach is also questionable because it assumes the wrong error structure (additive, rather than multiplicative). For the purposes of the present paper, we fit (9) for the multiplicative error structure by maximum likelihood. Assuming normal errors εt, this is equivalent to minimizing the sum of squares∑i(log(qi1−γ(t)+γ(t)qi−1))2,where the qiare the observations of the process (Qt). Differences among the fitting methods have a noticeable effect on the estimate obtained for γ(t), as illustrated by Fig. 3.To completely characterize the stochastic process, it is also necessary to describe the distribution of the errors εt, or equivalently of Rt. The estimation of γ(t) assumed a white noise process for (εt); the observed distribution of the regression residuals (Fig. 4), together with their low serial correlations (− 0.04 at lag 1, and no larger than 0.06 at any lag), reassure us that this is nearly enough true for accuracy in estimation. Nonetheless, there is appreciable non-normality in Fig. 4, which we may wish to reflect in the stochastic process we construct.We will take the errors εtto be independent random variables, with one of three distributions:(i)(AR-N) Normal, with mean zero and standard deviation (0.368) chosen to match that of the regression residuals.(AR-R) Re-sampled regression residuals, with all values depicted in Fig. 4 being equally likely to be chosen. This gives better treatment of the non-normality.(AR-S) Regression residuals, re-sampled from those occurring at the same time of year as t. This allows for any non-normality to have a seasonally varying structure.Other parametric families of distributions (e.g. the lognormal or hyperbolic) might also be considered, with parameters either constant or periodically varying to reflect seasonality. Also, the re-sampling methods could potentially benefit from the use of scenario selection to control the number of scenarios. As the goal of the present paper is to compare different inflow models, we do not attempt to use model selection procedures to determine which of these possibilities might be considered the best fit.With the choice of distribution for εt, we have completed the description of a stochastic process modeling inflows. However, the resulting model is unlikely to be suitable as-is for use in optimization with SDDP. With distribution (i), we have a continuous model, which must be made discrete by some form of sampling. With distribution (ii), the model is already discrete, but the large number of scenarios (as many as the observations in the original data set: about 4000 for our example data) mean that some form of scenario selection will be needed to make the problem tractable. This may also be true with distribution (iii), although here there are fewer scenarios to begin with (as many as the years in the original data set: 79 for our example data).The approach outlined in Section 2.2 suggests fitting models(11)Qt=Dt+MtQt−1directly. The pair (Dt, Mt) should be drawn from a discrete probability distribution. That is, we need a collection of scenarios(dt1,mt1),…,(dtk,mtk),with associated probabilitiespt1,…,ptk,creating a discrete approximation to the conditional distribution of Qtgiven Qt−1. Thedti,mti,andptiare all functions of time, giving a time-varying (e.g. seasonal) conditional dependence structure.This can be achieved via quantile regression (Koenker, 2005). Fitting (11) as a quantile regression, for a collection of k different quantiles r1, … , rk, gives us k different models(dt1,mt1),…,(dtk,mtk). In these regressions, we take thedtiandmtito have trigonometric polynomial form (in t), so that they (and thus the IFS model constructed from them) will have an annual seasonal variation. The regressors will thus be the lagged data (Qt−1), a collection of trigonometric functions (sines and cosines) of time, and interaction terms (products) between the lagged data and the periodic functions.Since scenario i represents outcomes falling near quantile riof the conditional distribution, a reasonable choice for the probabilitiesptiispti=|{r∈[0,1]:|r−ri|≤|r−rj|∀j}|.That is,pti=si−si−1,where s0 = 0, sk= 1, and si= (qi+ qi+1)/2 for i = 1, … , k − 1.An appealing feature of this approach is that the stochastic model can be designed with as many or as few scenarios as desired, without resorting to any additional post-selection or sampling steps.Figs. 5 and 6 show 12 scenarios obtained by this approach, corresponding to the 2nd, 6th, 15th, 30th, 45th, 60th, 70th, 80th, 88th, 94th, 97th, and 99th percentiles of the conditional distribution of Qtgiven Qt−1. These quantiles were chosen to give good coverage of the distribution, with particular emphasis on the extremes. The upper tail is especially well-represented, as high-inflow events can be of great importance in reservoir optimization problems. Third-order trigonometric polynomials are used for the seasonal variation in the first nine scenarios. In the other three, a first-order model (i.e. a simple sinusoid) seems better attuned to the relative paucity of data at this upper end of the distribution.The primary measure ofmerit in a stochastic model for inflow is whether it produces outputs which are statistically similar to the data on which it is based. In this section, we evaluate the performance of our models by simulating them and comparing the results to the original data.There are many possible measures of statistical similarity, but the ones of most interest to us are those directly relevant to reservoir-management and hydro-scheduling optimization problems. We will consider two measures: the distribution of the total inflow over a time comparable to the characteristic time scale of the storage reservoir, and the distribution of extreme high inflows.Consider the distribution of the total (or, equivalently, average) inflow over a time comparable to the characteristic time scale of the main storage reservoir involved. For the New Zealand hydrological system of our example data, this time scale is on the order of 1–3 months. Note that the distribution of inflow over such a period depends on both the distributions of the stage-wise (weekly) inflows and the degree of serial dependence in stage-wise inflows.Consider, in particular, the inflow in a given calendar month or quarter. From the raw data, we have as many observations of this quantity as there are years in the data set (79 in the example), giving us a direct empirical estimate of the shape of its distribution. From a stochastic model for inflow, we can obtain an equivalent set of observations by Monte Carlo simulation, giving us an empirical estimate of the analogous distribution for the model. (This distribution would be difficult to derive in any other way). As the length of the simulation is limited only by our patience, this synthetic sample can be much larger than is available for the raw data. We can then compare the two empirical distributions via a quantile–quantile plot, with the direct observations (sorted into increasing order) plotted vs. the corresponding quantile in the synthetic sample.Figs. 7 and 8 show these results for our autoregressive and IFS models over a 1-month period (December) and a 3-month period (the fourth calendar quarter). In each case, the model inflows are taken from a simulation over 8000 years, or about 100 times as long as the original data series. All the models seem to do a reasonable job of reproducing the distributions of inflows over these time scales. In particular, the IFS model is not obviously handicapped by having only 12 scenarios at each stage, relative to the other models with 79, 4000, and infinitely many scenarios.The 3-month results are particularly reassuring: having the right inflow distribution over this longer period suggests that the serial dependence of inflows has been well modeled.The greatest differences between the various models lie in the upper tail, i.e. in the way they extrapolate from the data to produce extreme high inflows. A common method for this kind of extrapolation is to fit a log-Pearson III distribution to the annual maximum flows (Griffis and Stedinger (2007a, b); Phien and Ajirajah (1984)). This has long been a standard technique in statistical hydrology, following its adoption by the US Water Resources Council in the 1960s (Council (1967, 1982)).Fig. 9compares the annual maximum flows of our example data to the corresponding quantiles of a log-Pearson III distribution fitted by the mixed-moment method of Rao (Rao, 1983). The fit is quite good. The same log-Pearson III distribution is then used for each panel in Fig. 10, where its quantiles are compared to the annual maxima of each of our 8000-year model simulations. As these simulated series are much longer than the original data, the highest inflows within them are more extreme than anything observed historically. It is apparent that the IFS model extrapolates to extreme high inflows in a way similar to the log-Pearson III method. The autoregressive model with normal errors produces an inflow distribution with a thinner upper tail – as we might have expected from observing the non-normality in Fig. 4. Re-sampling the regression residuals, by contrast, produces relatively fat tails, suggesting perhaps that the low serial correlation among those residuals belies the dependence at the extreme upper end of the distribution. The effect can be reduced, but not eliminated, by requiring the re-sampled residuals to come from the correct time of year.In this section we assess the behaviour of the inflow models in their intended application: a hydro-scheduling problem. We use a simple one-reservoir problem similar to that outlined in Section 1.1. The problem data are given in Table 1. The system described does not correspond to the real power generation and consumption infrastructure in the Waitaki Valley and New Zealand; all elements other than the inflow process are fictional.The problem begins in the autumn when inflows are falling, and continues through the low-inflow winter period. Allowing the reservoir to run dry incurs a high penalty for the unserved demand, and there is a credit for stored water remaining after the last stage. The first-stage inflow is set to 500 megawatt (which is below average for that time of year, giving Q1 ≈ 0.54) and inflows evolve thereafter according to the chosen inflow model.As noted in Section 3.1, the three versions of our autoregressive inflow model are unsuitable for immediate use in this context, as they have a large number (or in the case of normal errors, a continuous distribution) of scenarios at each stage. To put them on the same footing as our IFS model, some scenario selection is required. This was done by replacing the error distribution in each model by a set of 12 quantiles of that distribution: the same quantiles (and associated probabilities) as used for the IFS model in Section 3.2.With the models in this common form, it is straight-forward to compute analytically the exact mean inflow delivered by each model over the 30 stages of the problem. The models can then be adjusted slightly so that this mean is the same for all of them. This is a small adjustment, but an important one to make when comparing inflow models of disparate kinds. Typical hydro-scheduling problems are most sensitive to the overall amount of water received, and we do not want any differences in the results to be due simply to one model running slightly wetter or drier than another. In this exercise, the inflows of the autoregressive models with normal, resampled, and seasonally-resampled errors were reduced by (respectively) 4.9, 1.5, and 0.5 percent (after the first stage) to bring them into line with the IFS model.The problem was solved for each inflow model using Doasa 2.0, an SDDP code developed at the University of Auckland (Philpott & Guan, 2008). In each case, 2500 forward and backward passes were computed. This is rather more than required to get a good solution; the primal variables have converged to within 5 percent after only a few hundred iterations. Running times were on the order of 10 minutes on a 32-bit x86 CPU. Each solution was then simulated over 40,000 Monte Carlo sample paths to assess its performance. Common random numbers were used across inflow models, i.e. the initial random seed was the same for each model. Since all the models have the same number of inflow scenarios (12) at each stage, and the same probabilities for those scenarios, this means that corresponding scenarios were sampled.A “wait-and-see” version of the problem, in which the inflows are visible in advance, was also solved; this is much less computationally demanding, as it is merely a linear program. The same common inflow scenarios as in the SDDP problem were used. The expected optimal objective value for this problem serves as a lower bound for the objective value of the corresponding SDDP problem.Table 2shows the results. It is evident that in certain key respects the hydro-scheduling problem is very sensitive to small differences among inflow models; this is not atypical for such problems, and highlights the importance of detailed inflow modeling. The inflow models with resampling are particularly prone to spilling (reservoir overflow); this is consistent with the results on high inflow depicted in Fig. 10. The risk of shortage is reflected in the energy prices, and these too vary substantially among the inflow models, even though there is little apparent difference among them at the low-inflow end of the distribution.

@&#CONCLUSIONS@&#
