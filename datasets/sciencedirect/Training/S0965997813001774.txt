@&#MAIN-TITLE@&#
A generic framework for data acquisition and transmission

@&#HIGHLIGHTS@&#
We propose a configurable Data Acquisition and Transmission Framework (DATF).The DATF has 4 parts: a controller, a set of extractors, an assembler and a loader.We design the hierarchy of the four parts.Modular design and shared library technique make the DATF extensible and flexible.

@&#KEYPHRASES@&#
Data acquisition and transmission framework,Reusable framework,Modular design,Shared library,Configuration driven,High extensibility and flexibility,

@&#ABSTRACT@&#
Online applications such as decision support systems depend on data collected from multiple sources. We develop a generic data acquisition and transmission framework by modularizing the repetitive functions. Other than data acquisition and distribution with necessary transformation, the framework can act as a middle storage when the sources cannot connect to the destination directly. We designed three types of data extractors to accommodate the data acquisition from the file system, the net protocol and the database. After being collected by the extractors, the data is processed by an assembler module, to fit the target’s data structure. The data is inserted into the database by a loader module, which gets data from the assembler module. The assembler module and the loader module are controlled by a monitor and controller module. These modules are highly configurable and they form a 3-Level hierarchy. Taking advantage of modular design and shared library technique, the framework is extensible and flexible.

@&#INTRODUCTION@&#
Online applications such as decision support systems demand a large amount of data, which is obtained from multiple sources and inserted into the target’s database or data warehouse [1,2]. The process is often referred to Extraction, Transformation and Load (ETL) [3]. For the target applications, the ETL generally loads data into the database, especially the data warehouse [4]. Some functionalities in ETL are reusable in other applications. We are working on a highly configurable and extensible Data Acquisition and Transmission Framework (DATF) for general purpose.Data acquisition and transmission frameworks have been established in many fields [5–7]. There are also mature products and relational database and flat files are common sources [3]. However, our application for a wind power base requires information from wind farms, wind turbines and etc. The source information carriers are database, file media and messages transferred via network.We aim at reducing the workload and simplify the transplantation of the framework between different applications. Furthermore, it should be unnecessary to restart the whole program when adding or removing a function module. To achieve this, the functionalities are modularized, and each module can be configured easily by editing a text file.Depending on the system configuration, the working models can be classified into two categories: direct connection and indirect connection.In many cases, which can be classified as direct connection, there is no isolation between the target application and the data sources (Fig. 1). Being considered as a part of the target application, the DATF connect the data sources with the application by extracting data from the external sources and sending it to the target database after transformation if necessary.The connection between the target application and the sources may be restricted in some cases, which can be classified as indirect connection. For the electricity systems in China, the DATF is separated from the management system. The DATF cannot connect to the data source and the target simultaneously, and the data cannot be transmitted within the management system and the productivity environment directly (Fig. 2).Indirect connection is suitable for security critical application in order to prevent the source and the application from being hacked. When extracting the data, the DATF connects the sources and disconnects the target. The sources are disconnected when the DATF is sending data to the target. The DATF uses its own database for data storage.In some other cases, the combination of the two models is required.The structure of the DATF consists of four major parts as shown in Fig. 3: the Monitor and Controller (MAC), the extractors, the assembler and the loader. The DATF integrates the pieces of data retrieved from the sources into a whole one which can be inserted into a data table of the target database. And then it inserts the final records into the table. We call the above two sections a task. An instance, usually a process, of the assembler cooperates with a loader instance to serve a task. We call these two instances a pair. So the number of the pairs equals to the number of the tasks.The MAC is on the top of the hierarchy chart as shown in Fig. 4. There is only one MAC instance during the execution. It can start and terminate a pair according to the task configuration. It checks the status for the pairs periodically to capture the errors.An extractor handles one kind of data source. It is responsible for extracting information from the external source.The assembler manages at least one extractor as shown in Fig. 4. It receives all the streams from the extractors (Fig. 3) and makes them a whole one. It appends the integral data to the temporary storage.The loader retrieves the data from the temporary storage and inserts them into the target database. Upon successful insertion, the data in the temporary storage is deleted.The temporary storage is a place which holds data instead of a module. There may be multiple types of temporary storage, such as memory blocks, files and a table in a database.The above framework is reusable. The MAC and parts of the assembler provide supportive functions instead of actual services. So their designs do not need modification when being transplanted among different environments. The assembler does the transformation as well as the control job of the extractors. The implementation of the entire extraction process varies in practice. The design which can reduce the workload will be discussed later.To make the DATF extensible for different applications, we separate the control function from the business logic. The user only needs to provide a set of configuration files and several modules to complete a task.For readability and simplicity, we focus on the data acquisition and transmission in this paper. Issues such as error handling and security policy are beyond the scope of this paper.As shown in Fig. 5, the MAC mainly does two things: task management and cleanup. The management is the kernel and it includes: (1) it shuts down tasks as the users require; (2) it creates new tasks; (3) it checks all tasks and handles errors if any. The MAC does cleanup in the following steps when it goes out of scope: (1) it terminates all running tasks and waits until all of them exit; (2) it releases all allocated memory; (3) it closes all files opened.The MAC uses a configuration file and a task table in memory to manage tasks. The configuration file contains initialization parameters and task sections which describe tasks. A task section includes a task ID and several relevant moulde descriptions. A task has a unique ID during its lifetime. There are some modules deriving from the MAC which serve the task, such as the assembler and the loader in this paper. The task section tells the MAC how to start such modules.Fig. 6shows the structure of a row in the task table. The flag indicates whether the row is in use. The TASKID equals to the task ID in the configuration file. A handle represents a running instance of a module. The number of handles identifies the number of modules which serve the task. The MAC controls the instances via the values in the handle list.The MAC performs task management periodically in the following steps.(1)It checks the task table against the configuration file. If a task appears in the configuration files only, then the MAC starts the task and pollutes an unused row in the task table. The MAC tries to allocate more memory if the table is full. The MAC terminates a running task and sets the corresponding row unpollutted if it appears in the table only.The DATF uses heartbeat to represent if a running instance works normally. The MAC handles error in a predefined manner on a task if its heartbeat is abnormal.Given the above mechanism, users can start or shut down the entire framework by operating the MAC. The users can also add or remove a task by editing the configuration file of the MAC. Besides, the MAC is not only a moulde but also a kind of activity in this paper. And the activity works in a reverse order of the hierarchy as shown in Fig. 4. An extractor tries to fix the problem if an error occurs. On failure, the extractor tells its parent (the assembler) that it is abnormal. The assembler does similar things.The assembler is the most complex module in the DATF. As shown in Fig. 7, the assembler mainly does two jobs: data processing and extractor controlling. The assembler does not stop working until a stop signal arrives or it encounters an unrecoverable error. It does the following job in the main loop.(1)Like the MAC, the assembler starts, monitors and shuts down the data extractors according to the configuration and user operations.The assembler picks middle data from some place where the extractors store data.The assembler transforms the middle data into the proper result which meets the need of the target application.It puts the result to the temporary storage with specified form.The last three operations depend on the application so the design of the assembler is not pre-determined. Fortunately, all the four operations can form a framework which reduces the repetitive work when developing other applications. So what we should do is to provide such a framework.As mentioned before, an instance of a module is a process. Inter-Process Communication (IPC) [8] is necessary when two processes exchange information. Comparing with the data flow inside a single module, IPC is complicated and expensive. Instead of distributing such functionalities into different modules, we adopt another approach, i.e. the shared library [9].Dynamic Link Library (DLL) is the Microsoft implementation of the shared library [10]. We use the word “DLL” to emphasize the dynamic loading property of the shared library. The DLL is an effective way which extends to the existing applications [11]. Our strategy is to design uniform invoke interfaces for the DLLs and let the module call a DLL according to the configurationOne or more data extractors provide the assembler with data. So the assembler should make sure that its extractors are working properly. This part is similar to the MAC except the organization of the extractor table as shown in Fig. 8. And this extractor table seldom changes in contrast to the table in the MAC.There should be a place to hold the middle data when data flow from one process to another one. Although the shared memory technique [12] is a good IPC approach, we can use other means such as file, pipe and database instead. These strategies are called temporary storage. The reading and writing operations can be encapsulated in one DLL for one storage type. The processes invoke proper DLLs according to the temporary storage type.The extractors can write data to any one of the three kinds of temporary storage. But it is a good idea that all the extractors that belong to one assembler adopt the same storage type. An assembler must know information about each extractor such as storage type and data format. And such information is identical to that of the extractor. Fig. 9is an example of that information in the assembler configuration file. These sections describe the properties of the extractor, the temporary storage and the data fields.From Fig. 9 we can see the extractor has a unique ID named exdb1. The extractor gets data from a database and puts the data in a file (exdb1.dat) with a specified temporary storage handler (fileio.dll). The exdb1 extractor fetches four fields from the database: AA, AB, AC and AD. Their types are integer, date, integer and string with maximum length 20. The extractor uses its ID and the field name to identify its data. For example, “exdb1_AA” means this data is AA which is returned by extractor exdb1.There are two situations when the assembler gathers data from more than one source. In case one, all items come from one source and each one is a single record of the target application. Or each source contributes some fields of one item to the target record. In this case, the assembler needs to know which source fields can form a record of the target. For example, we collect data from two relations: A (AA, AB, AC, AD) and B (BA, BB, BC, BD). And the target relation is D (DA, DB, DC, DD, DE, DF). Each of A and B provides one tuple to form a row of D. Fig. 10shows a simple conversion of the three relations. Note the DA is an auto-increment (AI) field.Let us assume that a, b and d are the tuples of A, B and D respectively. We can see each of a and b must provide at least one field to identify that they belong to the same d. Neither the numbers of the fields for matching nor the matching rules keep unchanged for different applications. But the matching result is always true or false. It avoids redesigning the assembler by implementing the matching process in the DLL form. To invoke the functions in a DLL, we design a uniform interface with four parameters as shown in (1).(1)f(SrcNum,ParamNumPerEachSrc,ParamTypes,Params)Here the SrcNum is the number of the data sources. The ParamNumPerEachSrc is the number of the parameters, i.e. fields, in each source for matching. The ParamTypes is the type of each parameter. And the Params is the list of all the parameters. Here we propose an example of our implementation using C programming language.(2)f(intsrcNum,int∗arrParamNum,int∗arrParamTypes,void∗∗arrAddrs)In (2), the arrParamNum is an array holding srcNum integers which are the numbers of parameters of each source. The total number of all the parameters (NT) is the sum of the elements in arrParamNum. The arrParamTypes is an array with NTelements which represent the parameters types. And the arrAddrs is an array with NTelements and each one is the address of the corresponding parameter.We use the AA, AB, BA and BB to complete the matching in our example. And we use variables with the same names as the four fields to identify them in C programming language. Table 1shows the input parameters with the values and a simple matching rule using C notation. The return value of f() indicates whether the source tuples can be joined together.Data matching just tells the assembler whether the two or more items from different sources can produce a target record. The essential section of the assembler is generating the target data. Fig. 10 shows very simple transformations as the following descriptions.(1) DA is an auto-increment field;(2) DB equals to A.AB;(3) DC equals to A.AC;(4) DD equals to A.AD;(3) DE equals to B.BC;(4) DF equals to B.BD.Although the above process is simple, a DLL can encapsulate even more complicated transformation algorithms. The assembler just provides the DLL with the target fields and their corresponding source data. The relation between the target and the sources is described in the configuration file.The assembler puts the transformed data to the temporary storage where the loader fetches data. This process is rather simple. What the assembler needs is a handler of the temporary storage. The configuration file contains sections which describe the names, types, sizes and order of the output data. The assembler uses the information to organize output.There are essential validations such as type and integrity checking during this process. The types of the source field and the target field are often the same or compatible. But it is breakable. For example, the source is string “Jan.” and the target is number “1” when performing date conversion. The acquisition should meet one of the two following conditions to avoid breaking the integrity constraint.Condition 1: The DATF or the application can produce all data that generate all properties of a target tuple.Condition 2: The Database Management System (DBMS) sets the values of the properties that are unavailable in the sources.To obey the integrity constraint, the extractor must not miss the column where the NULL value is not allowed. It should be careful of the columns which the DBMS provides default values. For example, the “auto increment” is a useful column property provided by the DBMS, and the framework must not pollute such columns. Our DA is an example of that situation.Assuming the temporary values are correct, the design of the loader is quite simple. It collects the data from the temporary storage and inserts them into the target database. It removes the used data from the temporary storage upon success. Certainly, it has a set of error handling mechanics upon failure.Fig. 11is the main loop of the loader working process. It consists of data picking and insertion two major parts. The loader keeps on checking the temporary. It picks data and then remove the fetched data from the storage if it is not empty. After that, the loader inserts the fetched data into the target database.Note: (1) the logic of data picking is identical to that of the assembler; (2) the target can be any other means for carrying data.Data extraction is the front end of the DATF. It is usually a difficult task for large amount of data [13]. A common and convenient approach is fetching data from the database or data warehouse [14–17]. The predefined formatted file is another media to carry data [18–20]. The transmission protocol is unavoidable when the data is extracted from the net message [21,22]. In fact, both the DBMS and the file extractors realize some protocol parsing facilities during the transmission.The DATF has to solve two essential problems when performing data acquisition: (1) it needs to know to which object the extracted record belongs; (2) it needs to know which records to extract. The extractor may return a large amount of useless data when the assembler or the loader solves the problems. And this may waste time and space. The extractor needs information from the target database if it does the job alone. However, the loader should be the only moulde communication with the target database in our design.To solve the problem, we invoke a “sync table” for synchronization as shown in Fig. 12. The extraction works in the following steps:(1)The extractor checks current data against the table. It sends data which “Object ID” is not in the table to the back ends.When a record’s “Object ID” is in the table, the extractor checks if it meets the “Sync condition”. It will ignore records which do not meet the condition.The loader updates the synchronize table after data insertion.The synchronization table prevents the DATF from refetching data. It is harmful when data refetching never occurs and should be abandoned. The implementation of an extractor is application dependent and is out of our scope.The DATF was first introduced when we were developing the Monitoring System for Chinese Jiuquan Wind Power Base in Gansu province in 2011. The DATF loads running data of wind farms, step-up substations, wind turbines and anemometer towers. External databases store the data of the wind farms, step-up substations and anemometer towers. The data of the wind turbines are recorded in “E-text” files located on an FTP server.The organization of data in the monitoring system contains two parts as shown in Fig. 13. The “SrcID” and “ObjID” build the relation between the source and the target. There are 4 groups of tables corresponding to the 4 object categories. The record tables of the anemometer tower and the wind turbine copy the structures of the sources. However, the source structure of the wind farm or station (Fig. 14a) is quite different from that of the target. The source structure is really incomprehensible and the explanation is:(1)The ID field contains two meanings. For example, “DLF” identifies a wind farm and “sp” means the sum of active power.The real record time is defined by three fields: Date, Min and vxx. For example, the “12.5” in the second line (Fig. 14a) means the sum of the active power of DLF which is recorded at 1:05 on Jan. 1, 2010.Fig. 14b is a simple schematic of the transformation from the source to the target. The numbers at the left of each side are the line numbers. “P” and “Q” identifies the different types of the values in the two lines. The conversion shows: (1) one line from the source becomes at most 24 lines in the target; (2) fields at the same position of different source lines make up one target line.The DATF needs 3 different extractors according to the data structures. There is no data matching but the DATF needs 2 transform DLLs. One is for anemometer and turbine records and the other one is for wind farm and station records. Given the framework, we just need to write the 3 extractors and 2 DLLs (Fig. 15).We design an experiment to compare the DATF with the single process method (non-DATF). The comparison includes two parts: coding workload and performance under different data count. Table 2shows a trend though the result highly depends on the programmer skills, tools, software and hardware configuration.From Table 2 we can see the non-DATF solution is faster than the DATF solution. The reason is that it lacks of many sections such as process synchronization and resource management. However, the DATF’s coding work is much less than that of the non-DATF. The performance difference will become acceptable if the record count is large enough. And the DATF has an important advantage that it reduces potential error risks and debug work.

@&#CONCLUSIONS@&#
