@&#MAIN-TITLE@&#
Disparity-based just-noticeable-difference model for perceptual stereoscopic video coding using depth of focus blur effect

@&#HIGHLIGHTS@&#
Disparity-based just-noticeable-difference model.Depth-of-focus (DOF) blur effect in the real-world.Perceptual stereoscopic video coding based on human depth perception.Successful removal of redundancy in stereoscopic 3D (S3D) videos.

@&#KEYPHRASES@&#
Binocular fusion limit,Depth-of-focus blur,Just-noticeable-difference model,Perceptual stereoscopic video coding,Human visual system,Visual comfort,

@&#ABSTRACT@&#
Human 3D perception provides an important clue to the removal of redundancy in stereoscopic 3D (S3D) videos. Because objects outside the binocular fusion limit cannot be fused on retina, the human visual system (HVS) makes them blur according to the depth-of-focus (DOF) effect to increase the binocular fusion limit and suppress diplopia, i.e. double vision. Based on human depth perception, we propose a disparity-based just-noticeable-difference model (DJND) to save bit-rate and improve visual comfort in S3D videos. We combine the DOF blur effect with conventional JND models in the pixel domain into DJND. Firstly, we use disparity information to get the average disparity value of each block. Then, we integrate the DOF blur effect into luminance JND (LJND) by a selective low pass Gaussian filter to minimize the visual stimulus in S3D videos. Finally, we incorporate disparity information into the filtered JND models to obtain DJND. Experimental results demonstrate that the proposed method successfully improves both image quality and visual comfort in viewing S3D videos without increasing the bit-rate.

@&#INTRODUCTION@&#
With the rapid advances in the stereoscopic 3D (S3D) media technology, the demands on realistic visual experiences are also growing exponentially. S3D videos which are captured by stereo cameras provide users more vivid experiences on S3D displays. Unfortunately, compared with the real world, S3D displays often yield some distortions in 3D effects [1] which cause binocular stimulus, discomfort and fatigue for viewers. They are also usually caused by unsuitable screen disparity, large disparity change, and unnatural blur/sharpness [1–3]. Thus, for stereoscopic video coding, it is required to effectively enhance visual comfort and quality while maintaining storage space and transmission bandwidth. To achieve this, perceptual stereoscopic video coding based on human depth perception is a promising solution.Disparity information which provides users with depth effects and vivid experiences plays an important role in many applications such as stereo vision, 3D reconstruction, and depth-image-based-rendering (DIBR). Recently, many researchers have paid much attention to reduce visual discomfort with accurate disparity, stereoscopic distortion correction and synthetic depth of focus (DOF) blur [4,5]. Jung et al. [4] presented a selective DOF blur technique which was applied to the local regions where induced high visual discomfort but were less visually important. This technique could effectively reduce the visual discomfort based on depth maps and saliency maps. Um et al. [6] proposed a novel disparity-based asymmetrical filtering technique which used the disparity information in a stereo image to control severe blurring artifacts. It maintained image quality, but didn’t consider visual comfort improvement. Leroy et al. [7] presented a visual comfort enhancement method to remove irritating high frequencies in high disparity zones. It conducted adaptive blur according to the elements such as disparity, content, and focus. Shao et al. [8] proposed a visual discomfort relaxation method to automatically adjust disparity ranges of the crossed and uncrossed disparities according to the zero disparity plane (ZDP). Jiang et al. [9] proposed a three-dimensional visual comfort assessment method using preference learning. They estimated the visual comfort score by a simple linear mapping strategy based on robust VCA and preference classification models, which achieved good performance in visual comfort enhancement. However, most methods improved the visual comfort in S3D videos without considering the bit-rates.In this work, we adopt just noticeable difference (JND) models in stereoscopic video coding to reduce perceptual visual discomfort in S3D videos while maintaining the coding bit-rate. This is because the JND model can simulate the perceptual redundancy to determine the maximum distortion less than HVS. In [10], Chou and Li proposed a spatial JND model, i.e. minimally noticeable distortion (MND) profile, which considers the masking effects of background luminance and texture to quantify the perceptual redundancy. In [11,12], the foveation-based HVS JND model obtained by foveation masking effects was utilized to the video coding structure. It was useful to simulate the visual masking effect. Shao et al. [13] proposed an asymmetric stereoscopic video coding method based on the binocular vision suppression theory which the JND threshold exists between left and right views and chrominance degradation in right view would not affect the stereoscopic 3D effects. It saved bit rate significantly. Shao et al. [14] presented a depth map coding method which formulated the maximum tolerable depth distortion (MTDD) and the depth dis-occlusion mask to save bit rate for view synthesis. They provided a warped-SKIP mode to reduce inter-view redundancy. Thus, they achieved high view synthesis performance while improving the coding efficiency. The JND models were suitable for single-view videos, but not effective in removing the stereoscopic perceptual redundancy in stereo videos. It has been reported that HVS is more sensitive to the nearby regions and the background regions are often blurred on retina as shown in Fig. 1[1]. In the real world (left figure in Fig. 1), focal and vergence distances are the same no matter where the viewer looks. The focused regions have higher resolution on retina while the others are blurred by HVS, called DOF blur. However, in S3D displays (the right figure in Fig. 1), focal distance is fixed at the display while vergence distance varies depending on the part of the simulated scene the viewer fixates [1]. The whole scene has higher resolution, but doesn’t conform to the human depth perception. It has low visual comfort, which causes visual fatigue in S3D displays. Thus, if we apply traditional JND models in single-view videos to stereo videos, it is hard to remove the stereoscopic perceptual redundancy as well as likely to overestimate the visibility threshold of the frontal regions and underestimate that of the background regions. As shown in Fig. 2(a), viewers generally pay more attention to the frontal flowers than the background window. That is, the JND thresholds of foreground regions should be lower than those of the background ones. However, the traditional JND map in Fig. 2(b) produces exactly the contrary effect. To address this problem, we newly propose a disparity-based JND model (DJND) to consider disparity information which can distinguish foreground regions from the background. We combine the DOF blur effect with JND model in the pixel domain based on disparity to significantly improve the image quality and visual comfort in S3D displays. The proposed method is different from [14] because the proposed method assigns fewer bits to the non-focused regions, but assigns more bits to the focused ones. Thus, the proposed method successfully improves both image quality and visual comfort in viewing S3D videos without increasing the bit-rate.Fig. 3illustrates the entire framework of the proposed method. It can adjust the visibility threshold of different regions into a suitable scope. First, we get the traditional JND models and use a Gaussian low pass filter on the luminance JND model to reduce the background visual stimulus and discomfort. Then, we integrate the average disparity value of each 5×5 block to update JND models and get the proposed DJND model. Finally, we use the JND thresholds to adjust quantization parameter (QP) in rate-distortion optimization of the stereo video coding structure. It can utilize the bit-rate of the perceptually unimportant regions in coding of important regions and minimize the visual stimulus of the background regions on retina. Experimental results demonstrate that the proposed method effectively improves both image quality and visual comfort in viewing S3D videos while maintaining its bit-rate.The rest of this paper is organized as follow. In Section 2, we explain the disparity-based luminance JND model. In Section 3, we describe the disparity-based texture JND model and the proposed DJND model in detail. In Section 4, we address the use of JND thresholds on JMVC. In Section 5, we provide experimental results and draw conclusions in Section 6.The luminance JND model has been estimated based on the background luminance masking effect which is a primary factor of the human visual system [15]. The masking effect means that the human attracts higher attention in medium gray background regions than dark and light regions. Thus, according to [16], the luminance JND threshold is obtained as follows:(1)LJND(x,y)=171-P(x,y)‾127+3,ifP(x,y)‾⩽1273128(P(x,y)‾-127)+3,otherwise(2)P(x,y)‾=132∑i=15∑j=15P(x-3+i,y-3+j)·B(i,j)whereP(x,y)‾is average background luminance; andB(i,j)is a weighted low-pass filter [10].To adjust the visibility threshold on different regions and reduce redundant visual stimulus and discomfort in the background, we integrate DOF blur effect into the luminance JND model by a Gaussian low pass filter. Thus, the filter strength is adaptively selected by the average disparity value of each block as follows:(3)Dep(x,y)‾=125∑i=-22∑j=-22Dx+i,y+j(4)ε(x,y)=ψ+e[-α·Dep(x,y)‾-β]2whereDep(x,y)‾is the average disparity value of each 5×5 block,ε(x,y)is filter strength, α and β are constants. Then, the luminance JND model is filtered as follows:(5)FLJND(x,y)=1G·∑n∈Be-‖l-n‖22·ε(x,y)2·LJND(x,y)(6)G=∑n∈Be-‖l-n‖22·ε(x,y)2where B is a 5×5 block centered at the pixel l; and‖·‖is Euclidean distance.The JND threshold should be lower in the regions with larger disparity while the threshold should be larger in the regions with smaller disparity. Thus, we incorporate the disparity information into the filtered JND models by a negative exponential function to obtain the disparity-based luminance JND model (DLJND). DLJND is computed from disparity as follows:(7)DLJND=e-2·Dep(x,y)‾·FLJND(x,y)+∂where ∂ is a constant.Fig. 4shows conventional and proposed disparity-based luminance JND maps in Puppy. In the figure, the bluer the color is, the lower the JND threshold is. As shown in Fig. 4(a), the puppy, box, and flowers have the same values by the conventional luminance JND. It underestimates the values in the background regions because the farther the object is, the higher the masking ability is. Thus, the larger JND value is needed and the puppy should be bluer than the box and flowers. However, DLJND effectively deals with this problem as shown in Fig. 4(b).The texture JND model is estimated based on the texture masking effect, which means that the textural regions can mask more errors than smooth regions [17]. It is obtained using a canny operator [18] as follows:(8)TJND=ψ·Gr(x,y)·We(x,y)(9)Gr(x,y)=maxm=1,2,3,4{|gradm(x,y)|}(10)gradm(x,y)=116∑i=15∑j=15P(x-3+i,y-3+j)·gm(i,j)where ψ stands for a control parameter; andWe(x,y)is an edge-related weight based on a canny edge detector; and gm(i, j) are a four directional high-pass filter for texture detection. However, this JND model often overestimates the visibility threshold of the frontal regions and underestimates that of the background regions.Thus, we use the disparity information to distinguish the foreground and background as follow:(11)DTJND=e-2·Dep(x,y)‾·TJND(x,y)whereDep(x,y)‾is the same as (3).Finally, we obtain DJND from DLJND in (7) and DTJND in (11) by a nonlinear additive model [18] as follows:(12)DJND=DLJND+DTJND-ϕ·min{DLJND,DTJND}where ϕ is a constant. Fig. 5shows conventional and proposed texture JND maps in Puppy. As shown in Fig. 5, edges along the frontal objects have lower masking ability than those along the background regions. Edges of words and puppy should have lower JND values than those of the box and flowers. Fig. 5(a) shows that the JND thresholds of the edge regions in puppy, box, and flowers have almost same values by the conventional texture JND model. The conventional texture JND model overestimates the values in the foreground edges of words and puppy. However, Fig. 5(b) successfully weakens the threshold in these edges, and thus their colors become bluer than the background box edges.Moreover, we provide conventional and proposed luminance and texture JND maps in Fig. 6. Some regions which have 3D effects and are more attractive have lower masking ability and their JND values should be lower, i.e. become black. In contrast, background regions should have higher JND values, i.e. become white. As shown in Fig. 6(a), conventional JND map does not reflect those characteristics well in S3D videos. However, Fig. 6(b) shows that the proposed method effectively enhances JND thresholds in background regions and weakens those in the frontal objects.To achieve better perception image quality and visual comfort in S3D videos, the quality in the foreground regions is improved by lower quantitative parameter (QP) values which are adjusted by the proposed JND threshold for video encoding. Moreover, to achieve bit-rate control, in background regions, it is not desirable to support high-quality services because the human eye cannot easily detect the quality degradation in the background regions while viewing S3D videos. Thus, we increase the QP value in foreground regions based on the proposed DJND model to share the bit-rates of background regions with foreground regions. Furthermore, the reduction of the bit-rates in background regions can reduce annoying visual stimuli, thus improving visual comfort in S3D videos. According to the encoding scheme of joint multi-view video coding (JMVC), the distribution of bit-rate is based on the rate distortion optimization (RDO) obtained by minimizing the rate distortion cost as follows:(13)min(J)=min(D+λ·R)where λ is obtained as follows:(14)∂J∂R=∂D∂R+λ=0Thus, we get:(15)λ=0.85·2(QPi-12)/3Then, we use the proposed DJND model to adjust QP of each macro-block as follows:(16)QPi=ν·QPowhereQPois the original quantitative parameter; and ν is a regulation parameter which obtained by [19]:(17)ν=α+β·1+e-μ·DJNDi-DJND‾DJND‾-1where JNDiis averageDJNDofithmacro-block; andDJND‾is the averageDJNDof the frame.Therefore, as DJND of a macro-block is larger, ν is smaller. It indicates that this macro-block is in the background region and has higher masking ability; hence, its QP becomes larger.On the contrary, as DJND of a macro-block is smaller, QP becomes smaller. Fig. 7shows two examples of the regulation parameter. The bluer the color is, the lower the masking ability is, and the lower the parameter is, i.e. QP is lower. The color becomes redder in the opposite case.

@&#CONCLUSIONS@&#
