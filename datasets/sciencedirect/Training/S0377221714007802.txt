@&#MAIN-TITLE@&#
The linear ordering problem revisited

@&#HIGHLIGHTS@&#
We study the linear ordering problem and we propose a method to improve the state-of-the-art algorithms.We introduce an algorithm that identifies positions at which indexes within a solution cannot generate local optimum solutions.We propose a modified version of the insert neighbourhood which discards the operations that lead indexes to these restricted positions.We propose a modified version of the insert neighbourhood which discards the operations that lead indexes to these restricted positions.Experimental study clearly confirms the advantage of using our neighbourhood in the state-of-the-art algorithms, instead of the classical neighbourhood.

@&#KEYPHRASES@&#
Combinatorial optimisation,Linear ordering problem,Local optima,Insert neighbourhood,

@&#ABSTRACT@&#
The Linear Ordering Problem is a popular combinatorial optimisation problem which has been extensively addressed in the literature. However, in spite of its popularity, little is known about the characteristics of this problem. This paper studies a procedure to extract static information from an instance of the problem, and proposes a method to incorporate the obtained knowledge in order to improve the performance of local search-based algorithms. The procedure introduced identifies the positions where the indexes cannot generate local optima for the insert neighbourhood, and thus global optima solutions. This information is then used to propose a restricted insert neighbourhood that discards the insert operations which move indexes to positions where optimal solutions are not generated. In order to measure the efficiency of the proposed restricted insert neighbourhood system, two state-of-the-art algorithms for the LOP that include local search procedures have been modified. Conducted experiments confirm that the restricted versions of the algorithms outperform the classical designs systematically when a maximum number of function evaluations is considered as the stopping criterion. The statistical test included in the experimentation reports significant differences in all the cases, which validates the efficiency of our proposal. Moreover, additional experiments comparing the execution times reveal that the restricted approaches are faster than their counterparts for most of the instances.

@&#INTRODUCTION@&#
The Linear Ordering Problem (LOP) is a classical combinatorial optimisation problem which has received the attention of the research community since it was studied for the first time by Chenery and Watanabe (1958). Garey and Johnson (1979) demonstrated that the LOP is an NP-hard problem, thereby evidencing the difficulty of solving the LOP instances up to the optimality. However, due to its numerous applications in diverse fields such as archeology (Glover et al., 1972), economics (Leontief, 2008), graph theory (Charon and Hudry, 2007), machine translation (Tromble and Eisner, 2009) or mathematical psychology (Kemeny, 1959), we can find a wide variety of papers that have dealt with the LOP by means of exact, heuristic and metaheuristic strategies.Among the exact methods, the most meaningful include Branch and Bound (Kaas, 1981; Charon and Hudry, 2006), Branch and Cut (Grötschel et al., 1984) and Cutting Plane algorithms (Mitchell and Borchers, 1996; 2000). These methods, as Schiavinotto and Stützle (2004) highlighted, behave competitively for instances from specific benchmarks with up to a few hundred columns and rows. However their computation time increases strongly with the size of the instances, and thus, it is not possible to solve large instances in a reasonable time span. Beyond the exact proposals, pioneering works proposed constructive heuristics (Chenery and Watanabe, 1958; Aujac, 1960; Becker, 1967). Such approaches were later outperformed by the advances produced in metaheuristic optimisation. Proof of this are the solutions based on Local Search (Kernighan and Lin, 1970; Chanas and Kobylanski, 1996), Genetic Algorithms (Charon and Hudry, 1998), Genetic Programming (Pop and Matei, 2012), Tabu Search (Laguna et al., 1999), Scatter Search (Campos et al., 2001), Variable Neighborhood Search (Garcia et al., 2006), Ant Colony Optimisation (Chira et al., 2009; Pintea et al., 2009), and recently Estimation of Distribution Algorithms (Ceberio et al., 2013).According to a recent review of Martí et al. (2012), the Memetic Algorithm (MA) and the Iterated Local Search (ILS) proposed by Schiavinotto and Stützle (2004), are the algorithms that currently shape the state-of-the-art of the LOP. The MA is a hybrid algorithm which combines the canonical structure of a Genetic Algorithm with a high presence of local search procedures, either in the initialisation of the population or in the evolutionary process itself. On the other hand, the ILS is a strategy that iteratively applies a local search algorithm to a single solution. When the process gets trapped in a local optimum solution, the ILS applies a perturbation to the current solution, and continues with the optimisation process until a termination criterion is satisfied. Both algorithms include an efficient implementation of a greedy local search algorithm with the insert neighbourhood designed specifically to solve the LOP.As seen for most of the combinatorial optimisation problems, the hardness of solving a specific instance is not only limited to the size of this, but also to other additional parameters, unknown in most cases. In this regard, the community has tried to better understand the characteristics of the LOP that determine the difficulty of the instances, and, similarly, has tried to identify the features that could be useful to guide the algorithms throughout the optimisation process. In this sense, Schiavinotto and Stützle (2004) sketched out the properties that could somehow characterise the hardness of the LOP instances. The authors defined the sparsity, variation coefficient (VC), skewness and fitness distance correlation as measures of instance hardness, and showed that real-life instances, which are apparently more difficult than artificial ones, present significant differences in sparsity, VC and skewness with respect to random benchmark instances. Nevertheless, the relation between the mentioned properties, and the suitability of the MA and the ILS to solve the LOP is not straightforward.In the same research line, Betzler et al. (2011) published a detailed work on the parameterised complexity for intractable median problems, and particularly on the Kemeny ranking problem, which can be seen as a subclass of LOP. Although the parameterised complexity studied in the cited work is of great relevance, the analysis of Betzler et al. (2011) stands on a specific property of the Kemeny, which does not hold for the general LOP, and thus, the extension of the parameterised complexity to the LOP is not straightforward.The aforementioned works and the absence of a detailed work that performs an in-depth analysis of the LOP motivated this paper. In this work we study the properties that the optimal solutions of the LOP hold in the framework of local search algorithms, placing special emphasis on the position where the indexes are placed, and identifying the role of the associated matrix entries of the instance in the generation of local optima.The paper is divided into two parts: first, we provide a detailed description of the LOP, introducing definitions and theorems that study the structure of the problem with respect to the optimality of the solutions in the context of local search algorithms. Particularly, we emphasise the influence that the positions of the indexes that compound a solution have when generating local optima solutions. As a result of the theoretical study, a restricted version of the insert neighbourhood is proposed. This neighbourhood discards specific insert operations that involve moving indexes to positions at which they cannot generate local optima solutions. The theoretical analysis demonstrates that these insert operations will never be the operations that most improve the solution in the neighbourhood.The second part of the paper is devoted to demonstrating the validity of the restricted insert neighbourhood. In this sense, we develop a restricted version of the two best performing algorithms for the LOP: the MA and the ILS. Experimental results show that the restricted versions of the algorithms outperform the classical designs in 90 percent and 93.3 percent of the executions respectively, obtaining the same results for the rest of the cases. Moreover, additional experiments devoted to measure the execution time needed to perform a given number of iterations show that the restricted approaches are faster than the classical versions for most of the instances.The remainder of the paper is organised as follows: in the next section, the definition of the LOP is described. In Section 3, the structural analysis of the LOP is introduced placing special emphasis on the contribution of the indexes to the objective function. Next, in Section 4 the optimality of the LOP solutions is described in the context of local search algorithms, and in particular for the insert neighbourhood system. Section 5 is devoted to investigating the basis for the restricted insert neighbourhood system. In order to demonstrate the validity of the introduced analysis, a complete experimental study is introduced in Section 6. Finally, some conclusions and ideas for future work are drawn in Section 7.Given a matrix B = [bkl]n × nof numerical entries, the linear ordering problem consists of finding a simultaneous permutation σ of the rows and columns of B, such that the sum of the entries above the main diagonal is maximized (or equivalently, the sum of the entries below the main diagonal is minimized). The equation below formalizes the LOP function:(1)f(σ)=∑i=1n−1∑j=i+1nbσiσjwhere σidenotes the index of the row (and column) ranked at position i in the solution σ.11From now on, σ will denote any permutation inSn,and e will stand for the identity permutation (1, 2, …, n) of size n. In addition k and l will denote the indexes within a permutation σ, and i, j and z will be used to identify the positions of σ.This representation of the LOP is also known as the triangulation problem of input-output matrices. Although alternative representations of the problem can be found in Martí and Reinelt (2011) and Charon and Hudry (2007), due to the theoretical simplicity and readability of the exposed approach, in the remainder of the paper the triangulation representation will be considered.Example 2.1Let us introduce an example for a n = 5 LOP instance which will be used throughout the paper.22This example was extracted from Martí and Reinelt (2011).In Fig. 1, three different solutions, e, σ and σ* are described. The initial matrix is represented by the identity permutation e = (1, 2, 3, 4, 5) (see Fig. 1a), and its fitness, f(e), is 138. The solution σ = (2, 3, 1, 4, 5) introduces a different ordering of the indexes that provides a solution better than e (see Fig. 1b), f(σ) is 158. The optimal solution for this example is given by σ* = (5, 3, 4, 2, 1) (see Fig. 1c), with fitness f(σ*) = 247.In this section, we analyse the LOP by explaining the association between the indexes in σ and the arrangement of the bklentries of the matrix B. In addition, we describe the fitness variation that provokes changing the position of an index within σ, and the role of the bklentries in this regard. As necessary background to understand the latter content of the paper, in the following list we outline some meaningful properties of the LOP that define the association between the indexes in σ, and the bklentries in the B matrix.For any permutation of indexes σ of size n and a matrix B of size n × n:•Every index σi= k, i = 1, …, n, has associated 2(n − 1) entries of B: n − 1 from row k and n − 1 from column k.The set of associated entries of every index σi= k, i = 1, …, n, can be organised in pairs, i.e. every entry in row k,bkσj(where j = 1, …, n), has a pair in column k,bσjk,symmetrically located with respect to the main diagonal.All the pairs of entries associated to index σi= k, {bk1, b1k}, …, {bkn, bnk}, remain associated to this index regardless of its position and the position of the rest of the n − 1 indexes.Every entrybσiσjis associated to two indexes, σiand σj.For every pair{bσiσj,bσjσi}of entries, one entry is always located above the main diagonal, and the other entry is located below, thereby bounding the best fitness contribution of this pair tomax{bσiσj,bσjσi}in the best case, and tomin{bσiσj,bσjσi}in the worst case.In the remainder of the section, these characteristics and some extra definitions will be detailed with the help of illustrative examples.Example 3.1In this example, two different solutions are introduced, e = (1, 2, 3, 4, 5) in Fig. 2a, and σ = (1, 3, 2, 4, 5), in Fig. 2b. In both figures, the entries associated to the index 2 are highlighted in bold. We see that, in spite of the different ordering, in both solutions the set of the entries associated to the index 2 are the same i.e. (21, 14, 15, 9, 16, 23, 22, 28). Moreover, even though the position of index 2 is different in e and σ (e2 = 2 and σ3 = 2), the pairwise relation of the associated entries remains unchanged (see the circled indexes in Fig. 2).Checking the location of the associated entries of index 2 in σ, we note that the pair (14, 23) has exchanged its positions in σ, being now 14 below the main diagonal and 23 above it. Prior to studying the implications that the movement of an index has in the fitness, in the following lines we first introduce a new term: the contribution of an index to the fitness function.When index k = 1, …, n is ranked at position i in σ, i.e. σi= k, the contribution of index k to the objective function is given by the sum of the entries of column k in the rows {σ1, …, σi − 1} and the sum of the entries of row k in the columns {σi + 1, …, σn}. That is to say, the previous i − 1 indexes {σ1, …, σi − 1} and the posterior n − i indexes {σi + 1, …, σn} determine the contribution of the index k to the objective function. Formally, it is expressed as(2)c(σ,i)=∑j=1i−1bσjσi+∑j=i+1nbσiσjBack to Example 3.1, due to the exchange of the pair (14, 23), the contribution of index 2 has varied from 54 (16 + 14 + 15 + 9) in e (Fig. 2a) to 63 (16 + 23 + 15 + 9) in σ (Fig. 2b). In the case of index 3, its contribution has also increased, since the pair of (14, 23) is associated to both indexes, 2 and 3. Inversely, in the cases of the indexes 1, 4 and 5, their contribution does not change from e to σ.If we look carefully at Eq. (2), we realise that the contribution of index σi= k is not actually determined by the specific ordering of the indexes in the previous and posterior positions of i, but it is determined by their grouping in those two sets of positions. As shown in Example 3.1, the contribution of the indexes 1, 4 and 5, does not change from e to σ since the grouping of the rest of the indexes into the previous and posterior sets of positions associated to the indexes 1, 4 and 5 was the same.Proposition 3.1Given a solution σ, the contribution of the index σi, i ∈ {1, …, n}, to the objective function c(σ, i), is independent of the ordering of the previous indexes {σ1, …, σi − 1} and of the ordering of the posterior indexes {σi + 1, …, σn}.This example illustrates how the contribution of index 3 is independent to the ordering of the previous and posterior sets of indexes. In Fig. 3a, the contribution of index 3, c(e, 3) is 63 as a result of the sum (11 + 14 + 26 + 12). If we check the contribution of the index 3 in σ (see Fig. 3b), we see that it also sums 63, even though the indexes {1, 2} and {4, 5} have swapped their positions.In Proposition 3.1, we saw that the contribution of index k to the objective function is independent of the ordering of the indexes in the previous and posterior sets. But, what happens if an index σj= l is moved from the previous set of indexes of σito the posterior set of indexes? Contrarily to the previous case, the contribution of the index σi, c(σ, i), does not hold Proposition 3.1. At this point, it is worth remembering that each pair of entries{bσiσj,bσjσi}in the matrix is associated to two indexes, σiand σj, and thus, any exchange of location of σiby definition affects the contribution to the fitness function of σiand σj. In fact, moving σito position j, affects the contribution of all the indexes located between positions i and j. The example below illustrates the fitness variations produced by the movement of an index.Example 3.3Fig. 4a and b show matrix B according to solutions e = (1, 2, 3, 4, 5) and σ = (1, 3, 4, 2, 5). In this example we analyse the implications of moving index e2 = 2 to position 4. Due to this modification, indexes 3 and 4, are shifted one position to the left, thus changing their contribution to the fitness function. Particularly, we observe that the pairs {14, 23} and {15, 22} associated to the indexes 2–3 and 2-4, have exchanged their positions. Therefore, the contribution of index 3, c(e, 3) changes from 63 (11 + 14 + 26 + 12) to 72 (11 + 26 + 23 + 12). Similarly, the contribution of index 4 changes from 69 (15 + 15 + 26 + 13) to 76 (15 + 26 + 22 + 13). And as regards index 2, its contribution also changes from 54 (16 + 14 + 15 + 9) to 70 (16 + 23 + 22 + 9). Note that the variation in the fitness contribution of index 2 is the sum of the variations of indexes 3 and 4.As previously mentioned in the Introduction, many of the most successful algorithms proposed for solving the LOP are partially or totally based on local search procedures. For that reason, we adopted the framework of local search algorithms in order to identify and extract meaningful information that could be used to improve their performance. It is well known that local search algorithms start from an initial solution and iteratively try to replace the current solution with a better one in a previously defined neighbourhood system (Blum and Roli, 2003). Among the different neighbourhood systems proposed in the literature for the LOP, most of the works (Schiavinotto and Stützle, 2004; Garcia et al., 2006) clearly point to the insert neighbourhood system as the best performing. For that reason, this is the system considered in this paper.In what follows, we start by introducing some basic definitions about the insert neighborhood system and the local optimality of solutions.Definition 4.1Two solutions σ and σ′ are neighbors under the insert neighborhood (NI) if σ′ is obtained by moving an index of σ to a different place. It is formally defined asσ′∈NI(σ)⇔∃i,j∈{1,…,n},i≠js.t.{(σz′=σz,z<i∧z>j)∧(σz′=σz+1,i≤z<j)∧wheni<jσj′=σi(σz′=σz,z<i∧z>j)∧(σz′=σz−1,i<z≤j)∧wheni>jσi′=σjWhen an insert operation is performed, that is to move index σito position j, some entries in the upper triangle are exchanged with their respective pairs in the lower triangle, as we showed in Section 3. In the following lines, we distinguish two different insert operation scenarios, and we highlight in each case the specific entries that are exchanged:•i > j (see Fig. 5a). The entries{bσjσi,bσj+1σi,…,bσi−1σi}inside the upper triangle (light line pattern cells) are moved to positions {( j + 1, j), ( j + 2, j), …, (i, j)} in the lower triangle, and the entries{bσiσj,bσiσj+1,…,bσiσi−1}outside the upper triangle (dark line pattern cells) are moved to positions {(j, j + 1), (j, j + 2), …, (j, i)} in the upper triangle. The objective value of σ′ (neighbour of σ), f(σ′), can be calculated by summing the objective value of f(σ) and the difference of the entries exchanged, that is:(3)f(σ′)=f(σ)+∑z=ji−1(bσiσz−bσzσi)i < j (see Fig. 5b). The entries{bσiσi+1,bσiσi+2,…,bσiσj}inside the upper triangle (light line pattern cells) are moved to positions {(j, i), (j, i + 1), …, (j, j − 1)} in the lower triangle. Alternatively, the entries{bσi+1σi,bσi+2σi,…,bσjσi}outside the upper triangle (dark line pattern cells) are moved to positions {(i, j), (i + 1, j), …, (j − 1, j)} in the upper triangle. Similarly to the previous case, the objective value of σ′, f(σ′) can be calculated as:(4)f(σ′)=f(σ)+∑z=i+1j(bσzσi−bσiσz)As Fig. 5 illustrates, the pairs of entries in line pattern cells (dark and light), are the only entries that are exchanged because of the insert operation. The rest of the entries remain on the same side of the main diagonal as they were before the insert operation. As described in Section 3, every σihas associated{bσi1,b1σi},…,{bσin,bnσi}pairs of entries, and so we introduce the term vector of differences(bσi1−b1σi,…,bσin−bnσi)where each value in the vector describes the fitness variation that a specific pair of entries produces when it is exchanged because of a movement in σi. In what follows, we will see how the vector of differences associated to each index will be essential in order to determine if an index can generate local optima solutions at a given position.Definition 4.2A solution σ* is a local optimum for the insert neighbourhood if all neighbouring solutions σ have a lower fitness value.∀σ∈NI(σ*)f(σ*)≥f(σ)Therefore, in the insert neighbourhood system, a solution is considered local optima if and only if among all the possible insert operations, there is no movement that outperforms the current solution. Taking into account what was exposed Definition 4.1, given a solution σ and the neighbouring solution σ′ obtained moving index k from position, f(σ′) can be computed by recalculating the fitness contribution of the index k in the new position, and summing the variation to f(σ). This way, we state that a solution is local optima in the insert neighbourhood if any insert operation performed over σi, being i = 1, …, n, does not increase the contribution of σi.Example 4.1Let us consider the entries associated to the index at position 2, σ2 = 2 with c(σ, 2) = 54. Fig. 6illustrates the vector of differences(bσ12−b2σ1,…,bσn2−b2σn)of index 2 when performing all the possible insert operations. Besides, the contribution of index 2 is also given for each case.As we can see, the insert operation that moves index 2 to position 5 is the one that maximises the contribution of this index, from 54 to 89. Note that in order to find the position at which the contribution of an index k is maximised, we need to find the arrangement of the vector of differences associated to index k such that the sum of the values in the positions {1, …, i − 1} is maximised, and the sum of the values in the positions {i + 1, …, n} is minimised.33Note that the vector of differences is calculated by subtracting the entries in the column from the entries in the rowThe property below studies the arrangement of the vector of differences of each index when σ is a local optimum solution.Property 4.1Given a local optimum solution σ* for the insert neighbourhood, then for every indexσi*,i = 1, …, n, all the partial sums of the differences between the associated entries located before i are positive:(5)∑j=i−1z(bσj*,σi*−bσi*,σj*)≥0,z=i−1,…,1and all the partial sums of the differences between the associated pairs located after i are negative:(6)∑j=i+1z(bσj*,σi*−bσi*,σj*)≤0,z=i+1,…,nBack in Example 4.1, it can be observed in Fig. 6 that only the insert operation that moves index 2 to position 5 organises the vector of differences in the way that complies with Eqs. (5) and (6): all the partial sums from position 4 to 1 are positive: 19 ≥ 0, 7 + 19 ≥ 0, 9 + 7 + 19 ≥ 0, − 5 + 9 + 7 + 19 ≥ 0.Due to the vector of differences induced by the ordering of the remaining indexes in σ, index 2 only complies with Eqs. (5) and (6) at position 5. However, changing the ordering of σ, the ordering of the vector of differences changes too, and thus, Eqs. (5) and (6) might not longer hold. For illustrative purposes, let us consider a solution σ′ = (5, 3, 4, 2, 1) that has been obtained exchanging the position of the indexes 1 and 5 in σ. The vector of differences associated to index 2, according to σ′, is (19, 9, 7, *, −5)44* denotes the position at which index 2 was placed, the position 4.which indeed complies with Eqs. (5) and (6). Moreover, in the case of σ′, index 2 complies with the previous equations at either position 4 or position 5.Nonetheless, there exist positions at which index 2 cannot generate a local optimum independently of the position of the remaining indexes, as is the case of the positions 1, 2 and 3. Looking at the vectors of differences associated to index 2 in Fig. 6 in positions 1, 2 and 3, it can be seen that no ordering of the values in the vector complies with Eqs. (5) and (6).In view of this property, in the next section, we propose a more efficient insert neighbourhood where we discard those insert operations that move indexes to positions at which they cannot generate local optima solutions independently of the remainder indexes.In the previous section, we described the properties that the indexes within a solution σ need to comply with, in order for σ to be a local optima solution (Property 4.1). The next obvious step would consist of identifying the specific positions where the indexes generate local optima solutions. An in-depth analysis in this sense, however, suggests that such an approach is NP-hard, since for each index at each position that complies with Property 4.1, we need to check (n-1)! permutations. Nevertheless, there are some positions at which indexes do not generate a local optima regardless of the ordering of the rest of the indexes, and contrary to the previous case, to detect those positions is straightforward.Based on the vector of differences associated to the indexes, in this section we analyse the basis for discarding the positions at which indexes cannot generate local optima solutions. As a result of the analysis, we propose the restricted insert neighbourhood, which discards some insert operations that move indexes to the positions where local optima solutions are not generated.In order to illustrate the process of identifying the positions where an index cannot generate a local optima, we start studying the trivial cases, i.e. the boundary cases at which an index k is located either first or last:•In order to discard the first position for index k, σ1 = k, we need to demonstrate that no arrangement of the vector of differences associated to index k complies with Eq. (6). In order to prove that index k does not comply with that condition, independently of the order of the rest of indexes, it is enough to see that Eq. (6) is false when σ1 = k and z = n. Note that the result of the sum operator in the previous equation is independent of any ordering of the {σ2, …, σn} indexes, since the full vector of differences(bσ1k−bkσ1,…,bσnk−bkσn)is considered in the sum as a result of z = n.Similarly to the previous case, in order to discard the last position for index k, σn= k, we need to demonstrate that no arrangement of the vector of differences associated to index k complies with Eq. (5). In order to prove that index k does not comply with that condition, independently of the order of the rest of indexes, it is enough to see that Eq. (5) is false when σn= k and z = 1.Beyond the boundary cases, in order to state whether an index k does not generate a local optima at a given position i, σi= k, we need to check that a partition of indexes does not exist which locates a group of values of the vector of differences in the positions {1, …, i − 1} and another group in {i + 1, …, n}, such that the arrangement of the vector complies with Eqs. (5) and (6) at the same time.In this regard, we propose a simple algorithm that starts sorting in descending order the vector of differences associated to the index k. Since our aim is to discard positions, the second step consists of checking whether the most favourable partitioning of the vector of differences complies with Eqs. (5) and (6) when σi= k. The allocation procedure consists of placing the largest value in the vector at position i − 1, second largest at position i − 2 and so on. On the other hand, the lowest value is placed at position i + 1, the next lowest at i + 2, following the same procedure as for the largest values in the vector of differences. Let us denote as σ′ the solution induced by the new ordering of the vector of differences. Then, index k does not generate a local optima solution at position i if the following equation:(7)∑z=i−11(bσz′k−bkσz′)<0or∑z=i+1n(bσz′k−bkσz′)>0is true.Extending this procedure to the whole set of indexes in σ for all the positions, we calculate a binary matrix, called restrictions matrix R, where the entries with 0 represent the positions at which indexes do not generate a local optima solution. The computational complexity of the algorithm used to calculate the restrictions matrix is, in the worst case, O(n3). Algorithm 1summarises the pseudocode of the proposed algorithm.Example 5.1Fig. 7illustrates the algorithm we propose to identify the positions where an index, in this example index 2, cannot generate a local optima solution. As can be observed in Fig. 7, a solution is local optima if and only if index 2 is ranked at positions 4 or 5, since in the rest of the positions, even with the most favourable arrangement of the vector of differences, Eq. (7) is not complied. Extending the application of the algorithm applied over index 2 to the rest of the indexes, we then calculate the restrictions matrix R (see Fig. 8).In the view of the restrictions matrix in Fig. 8, solving the toy LOP example used throughout the paper is trivial, since some indexes can generate local optima solutions only at one position, as is the case of indexes 1 and 5. In addition once these indexes are fixed, index 2 and 3 are left with only one position, 2 and 4. Finally index 4 must be placed in position 3. The global optimum solution for the example is σ* = (5, 3, 4, 2, 1).For non-toy instances, the number of restricted positions is lower, but still significant. For this reason we propose to improve the performance of local search based methods by introducing a restricted version of the insert neighbourhood, called the restricted insert neighbourhood. This neighbourhood discards the insert operations that move indexes to positions at which they do not generate local optima solutions.At first glance, it is obvious that using the restricted insert neighbourhood will not necessarily outperform the classical greedy local search method. It is well known that, sometimes, bad solutions can lead the algorithm to more promising areas of the search space than fitter solutions. However, in the case of the LOP, we discovered that the insert operation that is chosen in a greedy local search, the one under which the largest improvement is given, is never a restricted operation according to the matrix R. The theorem below formalises this result.Theorem 5.1Given a non local optima solution σ, for every index σi, i = 1, …, n, the insert movement that maximises its contribution to the fitness function is not given in a restricted position of σi.ProofIn order to demonstrate that the theorem is held, we will prove that the inverse scenario cannot be true, i.e., let us assume that there exists an insert operation that moves an index σito a restricted position j which improves the solution the most. This means that the maximum contribution of the index σiis given at position j. If the maximum contribution position of index σiis at a restricted position, then Property 4.1. should hold, implying that Eq. (7) is true for σiat position j. Therefore, position j cannot be restricted to index σiif it is the maximum fitness contribution position.□As a result of Theorem 5.1, the restricted insert neighbourhood, is a subsystem of the insert neighbourhood, which has some meaningful properties:•Given a solution σ and the restrictions matrix R, the size of the restricted insert based neighbourhood NR(σ) is reduced to|NR(σ)|=(n−1)2−∑i=1n∑j=1,j≠in1[R[i][j]==0]Due to the reduction of the neighbourhood, a greedy local search performed on this neighbourhood will check less insert operations than on the classical insert neighbourhood. Note that, as seen in Theorem 5.1, the restricted movements are useless, since they will never be the movements under which the largest improvement is made, and therefore, they will never be selected in a greedy search.When running a greedy local search on the classical and the restricted neighbourhoods, the solutions obtained at each step of the search are the same (both follow the same search path). However, the restricted approach will require checking fewer insert movements (function evaluations).In order to demonstrate the improvement of using the restricted insert neighbourhood, we have applied our neighbourhood proposal to the best performing algorithms proposed in the literature: Memetic Algorithm and Iterated Local Search (proposed by Schiavinotto and Stützle, 2004). Both algorithms include an optimised implementation of a greedy local search algorithm for the LOP in their procedure. The goal in this experimentation is to analyse the improvement obtained using the restricted insert neighbourhood instead of the classical insert neighbourhood.Due to the lack of challenging benchmarks in the literature, Schiavinotto and Stützle (2004) proposed a new benchmark, the extended LOLIB (xLOLIB), which was generated by randomly sampling the instances of the LOLIB benchmark. Particularly, they generated 39 instances of size 150 and 39 instances of size 250. In addition to these instances, we generated an extra benchmark to include in this experimentation, xLOLIB2, with 200 instances of sizes 300, 500, 750 and 1000 (50 instances of each size) following the same procedure used for generating the xLOLIB benchmark.Both source codes, MA and ILS, were obtained from the authors, and so the restricted insert neighbourhood was directly implemented on the original code (written in C), adding only the necessary code to calculate the restrictions matrix and implement the restricted insert neighbourhood. The experimentation was conducted on a cluster of 20 nodes, each of them equipped with two Intel Xeon X5650 CPUs and 48GB of memory.In order to analyse the contribution of the restricted insert neighbourhood as fairly as possible, we ran the original implementations of MA and ILS, and their restricted versions MArand ILSrfor three different maximum numbers of evaluations, 1000n2, 5000n2 and 10000n2 (n denotes the size of the instance). The evaluation numbers were set without performing any previous experimentation.Each algorithm-instance pair was run 20 times and the average fitness of the best solutions obtained was calculated. Due to the large size of the results-tables obtained from the conducted experiments, the results have been summarised in Table 1, divided in three groups according to the different stopping criterion and the size of the instances. Besides, the results have been presented as the number of times the restricted versions of the algorithms beat the classical versions. Next to the results, within parentheses, we show the number of instances for which the same results on both versions, restricted and non-restricted, were obtained.55Supplementary results, original source codes, instances, and extended material of the experiments can be obtained from http://www.sc.ehu.es/ccwbayes/members/jceberio/LOP.html.Remember that the restricted versions will at least equal the results of the classical proposal.In the view of the results, the restricted algorithms, MArand ILSr, outperform the classical implementations in almost all the evaluated cases. With respect to MAr, it outperforms MA in 81.2 percent, 94.6 percent and 95.3 percent of the instances for 1000n2, 5000n2 and 10000n2 maximum numbers of evaluations. Similarly, ILSroutperforms ILS in 97.4 percent, 94.2 percent and 88.4 percent of the instances.In order to assess whether there exist statistical differences among the results, we applied a nonparametric Wilcoxon test to the average results obtained by the pair MA − MAr, and the pair ILS − ILSrfor each size of the instances. A level of significance α = 0.05 was set. The statistical test reported significant differences between the algorithms for all the sets of instances and for the three maximum number of evaluations. The p-values obtained for the pair MA − MArwere in the worst cases 1.23 × 10−06, and 5.6 × 10−07 for ILS − ILSr.In addition to evaluating the performance of the algorithms in terms of the quality of the solutions obtained for a fixed number of evaluations, we have also carried out an evaluation in terms of computation time. Basically, we measure the computation time spent by both pairs of algorithms, MA − MArand ILS − ILSr, to perform k iterations66As regards MA, we consider as iteration a generation of the algorithm, while in the ILS, we count an iteration every time that the algorithm reaches a local optimum and applies the perturbation procedure.(recall that both approaches, the classical and the restricted, follow the same path, and therefore, after k iterations the same solution will be reached). Particularly, we compare the computation time of the basic approaches with respect to the computation time of the restricted approaches that include: (1) the computation of the restrictions matrix at the beginning of the algorithm, and (2) a greedy local search under the restricted insert neighbourhood which, for each index, reads in the restrictions matrix the range of the positions to which the index can be moved.It is clear that the execution time needed to perform a given number of iterations with the restricted approaches, ILSrand MAr, depends on the sparsity of the restrictions matrix associated to each particular instance. That is, the higher the number of zero-valued entries, the lower the checked neighbours, and therefore, the faster the greedy local search. In order to obtain a more general picture of the influence of the sparsity in the execution time, in this experiment, apart from the previously used benchmarks, xLOLIB and xLOLIB2, we have included additional instances with different sparsities. Particularly, we have considered the most common benchmarks for the LOP: LOLIB, LMC and MB. The number of iterations was set to k = 10, 000 without any previous experimentation. Results are displayed in Figs. 9 and 10.From these figures, we can observe that sparsity varies between (almost) zero and 0.37. When sparsity is about 0.15 or higher, our restricted approaches run faster than the original versions (including the calculation of the restrictions matrix). In the best cases (highest sparsities), ILSris 27 percent faster than ILS (22 percent in the case of MAr). In the worst case, ILSrruns 8 percent slower than ILS (7 percent for MAr). Therefore, in view of the results, in most of the instances, our restricted versions require shorter execution times than the original versions.

@&#CONCLUSIONS@&#
In this paper we introduced a detailed theoretical study of the LOP in the context of local search algorithms. Based on this study we presented a method that allows to extract static information about the problem and incorporate it to improve the performance of local search algorithms. Particularly, we developed a method to detect the positions where the indexes cannot appear in local optima solutions of the insert neighbourhood. As a result of this study, an improved version of the insert neighbourhood system, called restricted insert neighbourhood was proposed, in which the insert operations that lead indexes to restricted locations are discarded.In order to demonstrate the efficiency of the restricted insert neighbourhood, we applied this neighbourhood to the best performing state-of-the-art algorithms for LOP: Memetic Algorithm and Iterated Local Search. Average fitness values of the best solutions from 20 repetitions of MA, ILS, MAr, and ILSrwere calculated for three different maximum numbers of evaluations on a benchmark of 278 instances.Conducted experiments showed that the restricted version of the algorithms systematically outperforms the classical versions when a maximum number of evaluations is set as stopping criterion. From 278 instances tested (xLOLIB - 78 instances) and (xLOLIB2 - 200 instances), MAroutperformed MA in 90 percent of the cases, and ILSrimproved ILS in 93.3 percent. The Wilcoxon statistical test confirmed the behaviour reported by the experiments, indicating that MArand ILSrare significantly better than the classical version of the algorithms for all the studied sets of instances.In addition, the execution time of the restricted and the classical algorithms was measured. Reported results showed that, in most of the cases, the restricted versions run faster than their classical counterparts. In particular, in the best cases, ILSrand MArwere 27 percent and 22 percent faster than the classical approaches. Alternatively, results showed that for instances with almost no sparsity, ILSrand MArare in the worst cases 7–8 percent slower.Studying a problem and extracting information that can be used to guide the optimisation process of an algorithm is an interesting task that, in most of the cases, requires thorough research. In this work, we just scratched the surface of the linear ordering problem with the restricted insert neighbourhood, and therefore, there are many issues that deserve deeper analysis. An easy extension of the theoretical study presented in this paper is that of the relative ordering of the indexes in which some adjacent orderings of indexes cannot generate local optima solutions due to the associated entries that share the consecutive indexes.Finally, how to exploit the extracted knowledge in the most advantageous way is another challenging task that, in this case, has been addressed by discarding solutions within a neighbourhood, and proposing a restricted version of it. However, taking into account that the restrictions matrix describes partially the global optimum solution, such information could be used in exact, heuristic and metaheuristic algorithms in order to guide the search toward better solutions. Particularly, we find it interesting to include the restrictions matrix to Branch and Bound algorithms in order to discard the branches that do not comply with the restrictions. Alternatively, the implementation of constructive heuristics to initialise metaheuristic procedures, or the implementation of guided crossover and mutations within Evolutionary Algorithms are other interesting applications to explore.