@&#MAIN-TITLE@&#
Mean-risk analysis with enhanced behavioral content

@&#HIGHLIGHTS@&#
The Mean-Risk model presented originates from a behavioral theory of choice under risk.It is based on deviations of outcomes from one another, not a single target as in classic models.It satisfies strict first and second order stochastic orderings, and defines a Convex Risk Measure.It allows commonsensical risk taking behaviors that standard models do not permit.A stock portfolio selection application illustrates the use of the Mean-Risk model.

@&#KEYPHRASES@&#
Risk analysis,Uncertainty modeling,Utility theory,Stochastic dominance,Convex risk measures,

@&#ABSTRACT@&#
We study a mean-risk model derived from a behavioral theory of Disappointment with multiple reference points. One distinguishing feature of the risk measure is that it is based on mutual deviations of outcomes, not deviations from a specific target. We prove necessary and sufficient conditions for strict first and second order stochastic dominance, and show that the model is, in addition, a Convex Risk Measure. The model allows for richer, and behaviorally more plausible, risk preference patterns than competing models with equal degrees of freedom, including Expected Utility (EU), Mean–Variance (M-V), Mean-Gini (M-G), and models based on non-additive probability weighting, such as Dual Theory (DT). In asset allocation, the model allows a decision-maker to abstain from diversifying in a positive expected value risky asset if its performance does not meet a certain threshold, and gradually invest beyond this threshold, which appears more acceptable than the extreme solutions provided by either EU and M-V (always diversify) or DT and M-G (always plunge). In asset trading, the model provides no-trade intervals, like DT and M-G, in some, but not all, situations. An illustrative application to portfolio selection is presented. The model can provide an improved criterion for mean-risk analysis by injecting a new level of behavioral realism and flexibility, while maintaining key normative properties.

@&#INTRODUCTION@&#
Mean-risk analysis is an appealing approach to decision under risk that has sprung abundant literature and applications. This is because measuring the value of gambles as a function of their rewards and risks goes to the heart of decision makers’ concerns in a direct, transparent manner (Jia & Dyer, 1996). There seems to be general agreement—even compelling arguments (de Giorgi, 2005)—that the potential reward of a gamble should be captured by its expected value, i.e., its mean. There is less accord about what constitutes an acceptable measure of risk. The challenge is to balance desirable normative properties with intuitively or behaviorally appealing considerations. This tension ultimately lies at the heart of any prescriptive theory of choice under risk.Here, we propose a mean-risk model that results from a reformulation of Disappointment without Prior Expectation (Delquié & Cillo, 2006), a theory of Disappointment in which every outcome of a prospect can act as a reference point for any other outcome. We show that this mean-risk model presents advantages over the standard competing models because it is able to produce solutions to mean-risk optimization problems that are behaviorally more realistic, and at the same time it retains key normative properties required for use in a wide range of applications. Due to this flexibility, our model may provide an attractive criterion to capture decision makers’ risk-return preference patterns in mean-risk analysis.The paper proceeds as follows. Section 2 introduces the model. We show how it relates to other models of risk, and that it defines a class of risk measure distinct from the classic families widely considered throughout the literature. In Section 3, we provide necessary and sufficient conditions for monotonicity with respect to first and, more importantly, second order stochastic dominance, two essential normative criteria for ordering risky prospects. This generalizes previous results concerning the Mean-Gini model (Ogryczak & Ruszczyński, 2002; Yitzhaki, 1982). In Section 4, we show that the model yields a Convex risk measure, which is highly desirable for use in risk management because it favors diversification. Next, in Section 5, we examine the model’s implications for asset trading and optimal allocation. There, we show that the model allows for a richer pattern of risk taking behaviors than other standard models, and we specify the conditions under which qualitatively different types of behaviors occur. The risk taking behaviors produced by the model appear more realistic than those of other classic models with comparable degrees of freedom. These results are closely tied to the model’s ability to bridge first order and second order risk aversion. Section 6 addresses practical issues in calibrating and using the model for applications, and provides a numerical example in stock portfolio selection. By way of summary, Section 7 concludes that the model provides a tractable, sound analysis of choice under risk, expanding the range of available solutions in mean-risk analysis. All proofs appear in the Appendices.The literature on risk emanates from several intellectual traditions, notably Statistics (measures of dispersion and the moments approach), Economics (the EU approach, but also the inequality measurement approach), Finance (the portfolio efficient set approach), and Psychology (the behavioral/cognitive approach). Sarin and Weber (1993) present an overview of the Risk-Value models literature at the time of their writing; Pedersen and Satchell (1998) provide a fairly detailed review of risk measures.Expected Utility stands as the ultimately rational approach to choice under risk, however, there is no explicit construction of a risk index as a primitive in EU. For an individual with utility function u, the risk of a gamble X can be measured as its risk premium, defined as π(X)=E[X]−u−1(Eu[X]) (Pratt, 1964). However, the valuation of a gamble, i.e., its certainty equivalent CE(X)=u−1(Eu[X]), cannot in general be calculated directly from its expected value and its risk premium in a Risk-Value spirit, because the estimation of π(X) usually requires calculating the certainty equivalent, leading to a circularity. Under particular conditions on the u function and/or the distribution of X, EU can take a Risk-Value form. For example, if the utility function is exponential and gambles have a normal distribution, or if the utility function is quadratic, then EU is equivalent to a Mean–Variance model. Further ways to cast EU as a function of risk and return have been explored in some depth by Bell (1995) and Jia and Dyer (1996): the possibilities seem confined to a limited set. Because the notion of risk in EU is entirely driven by the concavity of the utility function, it is completely intertwined with the concept of diminishing marginal utility of wealth. To require that the valuation of each and every risk be entirely and only determined by the pattern of utility for wealth may be too rigid for some decision makers. That is, EU may leave out some aspects of risk that legitimately matter to the decision maker.The so-called Risk-Value framework may offer more flexibility in dealing with risk (Dyer & Jia, 1997) by allowing to define a risk measure “from scratch,” that is, unconstrained by whether it is consistent with the maximization of a particular EU function. Because risk is associated with the presence of uncertainty in the payoffs, that is, the extent to which their distribution departs from a sure outcome, risk measures are germane with measures of dispersion. Risk is traditionally measured as the propensity of a random outcome to deviate from some reference level. Stone (1973) proposes that three basic ingredients are relevant to devising a risk measure: (i) a reference level, from which deviations are measured; (ii) the range of deviations taken into account; and (iii) how deviations are weighed. He shows that this defines a general family that includes the standard risk measured used in Finance: variance, semi-variance, mean absolute deviation, and the probability of a loss worse than some specified level.Among the wide variety of risk measures that has been proposed, some have received special attention as regards their normative properties (e.g., compliance with stochastic orderings) and their computational performance in optimization. This is the case for the Mean Absolute Deviation, Semi-lower Deviation, Conditional Value-at-Risk, and the Gini Mean Difference, among others (see the work of Krzemienowski & Ogryczak, 2005; Mansini, Ogryczak, & Speranza, 2003, 2007; Ogryczak & Ruszczyński, 1999, 2002). The risk measure we propose here was motivated by a desire to account for risk preferences that deviate systematically from EU, such as the widely observed Allais’ (1953) paradox and certainty effect (Kahneman & Tversky, 1979), the common ratio effect, and reference-dependence effects.Delquié and Cillo (2006) developed the Disappointment without Prior Expectation model of choice under risk based on the postulate that individuals are liable to experience a mixture of disappointment and contentment from comparing the outcome received from a gamble to all the other possible outcomes, worse and better, rather than a single prior expectation. This extends the notion of reference dependence by allowing each and every outcome in the gamble to play the role of a reference point, that is, the value of an outcome is relative to the entire context in which it is embedded. In all previous formulations of Disappointment, including Bell (1985) and Loomes and Sugden (1986), the gamble is summarized into a single reference point. Kőszegi and Rabin (2007) proposed a model of reference-dependent risk taking behavior in which the reference level is stochastic, consisting of the expectations the decision maker held in the recent past.It was also shown in Delquié and Cillo (2006) that Disappointment without Prior Expectation could be reformulated as a Risk-Value model, taking the following form:V(X)=∑i=1npiv(xi)-∑i=1n∑j⩾ipipjH(v(xi)-v(xj)),where X is a gamble that yields payoff xiwith probability pi, i=1,…,n, ∑pi=1 and x1⩾x2⩾⋯⩾xn; v(·) is an increasing function that describes the subjective value of outcomes; and the function H describes how an individual values discrepancies between obtained and missed outcomes, that is, the loss associated with getting the lower of two outcomes. The immutable properties of H, that stem from its very definition, are: (i) H(0)=0, and (ii) H is defined on the non-negative domain, that is, it takes non-negative deviations as argument, i.e., differences between ordered outcomes.Here, for parsimony and for the sake of having a Risk-Value representation comparable to those that have appeared before, we focus on a special case of the above model: we will assume v linear throughout this paper. This assumption does not play a role in the essential results and claims developed in the paper, and it will enable us to concentrate on what can be accomplished with the simpler form. Thus, the model we are interested in here is:(1)V(X)=E[X]-Δ(X)withΔ(X)=∑i=1n∑j⩾ipipjH(xi-xj),where E[X] is the mean of X, a measure of its potential reward, and Δ(X) defines a risk-premium, that is, the amount by which the reward will be discounted to account for the presence of risk in X. For example, for a binary gamble X with outcomes x, y with probabilities p, 1−p respectively, and x⩾y, we have: V(X)=px + (1−p)y−p(1−p)H(x−y). If the outcomes are not ordered, we can just enter their absolute difference in the H function. If F denotes the cumulative distribution of X, the continuous form of Δ(X) is:(2)Δ(X)=∫-∞+∞∫-∞xH(x-y)dF(y)dF(x)=E∫-∞XH(X-y)dF(y).From now on, we will refer to the model expressed in (1), either its discrete or continuous form (2), as the M-Δ model. Note that the M-Δ model produces a valuation directly in the form of a certainty equivalent. That is, V(X) in (1) is in the same units as the gamble’s payoffs, and the gamble is worth exactly as much as a sure amount s=V(X).The function H weighs the relative impact of large and small deviations. Although H could be an increasing, decreasing, or even non-monotonic function, given the pervasiveness of risk averse behavior, it is sensible to focus on the case of H (strictly) increasing: this will imply H(y)>0 for all y>0 (since H(0)=0), leading to a positive risk premium Δ(X) for any gamble. Under this assumption, a sure payoff equal to the expected value of the gamble will always be preferred to the gamble itself, and zero mean gambles will always be rejected.Δ(X) measures the riskiness of a gamble with no regard to its location, that is, irrespective of how good or bad the outcomes are (this, of course, is captured by E[X]). This may be regarded as a desirable feature for a risk measure, because a risk judgment itself should be distinct from the overall desirability of a gamble. Eq. (1) specifies how risk should be traded off against the reward.M-Δ in (1) has constant risk aversion (CRA) for any form of H. That is, if a constant is added to a gamble, the valuation of the gamble increases by the same constant. The property of decreasing risk aversion is often regarded as behaviorally more compelling than CRA. However, CRA presents a great practical advantage for applications, because it allows analyzing problems in terms of gains and losses rather than in terms of total wealth, which is often impossible in practice. This convenience is the reason why exponential utility is used so routinely in EU applications. The essence of a Risk-Value representation is to accept risk as a primitive construct, not necessarily tied to the valuation of sure, final wealth: in this mind-set, the property that Δ(X) is independent of wealth is not shocking (Mitchell & Gelles, 2003). Also, if the random variables considered are returns, i.e. relative payoffs, then M-Δ implies constant relative risk aversion, that is, diminishing risk aversion in the absolute payoffs.Fishburn (1977) considers mean-risk analysis when risk is measured as a probability weighted function of the deviations below a specified target return, as follows:(3)ρt(F)=∫-∞tφ(t-x)dF(x),where F is the cumulative distribution function of the random payoff, t the target level, and φ measures how deviations below the target are weighed. Fishburn (1977) examines the special caseφ(t-x)=(t-x)α, for x⩽t, so-called the ‘α-t’ model. The α-t model belongs to a general family considered by Stone (1973).The measure Δ(X) is neither a member of the class of risk measures considered by Fishburn (1977), Eq. (3), nor part of the family proposed by Stone (1973). Indeed, one essential difference is that these traditional risk measures are sprung from the outcomes’ deviations from a fixed reference level, whereas Δ(X) is built on the mutual deviations of outcomes from one another. Nevertheless, Eq. (2) makes a relationship to Eq. (3) apparent: for X=x, the expression∫-∞xH(x-y)dF(y)within the expectation in (2) is nothing but the Fishburn (1977) measure of risk, ρx(X), representing the risk of failing to achieve at least outcome x in gamble X. Thus, Δ(X) can be thought of as the mathematical expectation of the collection of Fishburn’s risk measures generated by taking as target level each and every value of X in turn. In Δ(X), each outcome of X can be viewed as playing the role of a target and contributing its own ‘à la Fishburn’ risk to the gamble: the total risk Δ(X) of the gamble is just the average of the risks associated with individual outcomes. Thus (1) can be written as:V(X)=E[X]-E[ρX(X)]=E[X-ρX(X)]=E[uX(X)]withuX(x)=x-ρx(X),where uX(x) can be interpreted as the risk-adjusted utility of outcome x in gamble X. In other words, ρx(X) is the “risk premium” associated with just outcome x in gamble X, and E[ρX(X)]=Δ(X) is the risk premium of the whole gamble.Notice from (1) that every pairwise difference between two outcomes enters exactly once in the makeup of Δ(X). For H non-decreasing, Δ(X) constitutes a general measure of dispersion, which includes an important case. Indeed, for H linear, Δ(X) yields the Gini Mean Difference (up to a positive multiplicative constant), also known in Statistics as the ‘Absolute Mean Difference’ measure of dispersion. Gini’s Mean Difference is defined as the mean of the absolute difference between two observations of a random variable.1The Gini Index, also called Gini Coefficient of Concentration, is a normalized, unit-free measure obtained by dividing the Gini Mean Difference by twice the mean of the distribution.1The Gini measure is most prominently used as a measure of inequality of income or wealth among a population, but it has also been used as a risk measure (Ogryczak & Ruszczyński, 2002; Yitzhaki, 1982). Observing that Δ(X) in (1) can also be written as:Δ(X)=12∑i,j=1npipjH(|xi-xj|),we see that for H(y)=y, Δ(X)=1/2G(X), where G(X) is the Gini Mean Difference of X.Various generalizations of the Gini measure have been proposed before (e.g. Donaldson & Weymark, 1980; Yitzhaki, 1983). Some generalizations introduce parameters that, in effect, transform the decumulative distribution function of X. As another type of extension, Krzemienowski and Ogryczak (2005) consider the Gini measure computed over below-mean outcomes in order to capture downside risk only. Our extension of Gini differs from previous generalizations by introducing a weighting function over the deviations. However, our approach did not start with the Gini measure and seek to extend it; instead our purpose was to account for widely observed non-EU preference patterns, and this yielded a risk measure that happens to include Mean-Gini as a particular case.2Maccheroni et al. (2006) show that the Gini concentration index also arises in the so-called divergence preferences model they introduce to represent ambiguity preferences. The index of ambiguity aversion in Maccheroni et al.’s (2006) model includes the relative Gini concentration index as a particular case. They also show that their model with Gini as index of ambiguity aversion is equivalent to mean–variance preferences, when restricted to the domain of monotonicity of mean–variance.2To avoid the difficulties connected with knowing decision makers’ utility functions, several authors have examined the merits of ordering prospects in terms of dominance rules (Egozcue & Wong, 2010; Hadar & Russell, 1969; Wong, 2007). Let X and Y be two random variables, and F and G, respectively, their cumulative distribution functions. Let us recall the definitions of first and second order stochastic dominance (FSD and SSD):Definition 1X is said to be as large as Y in the sense of FSD, denoted asX⪰FSDYif and only ifF(x)-G(x)⩽0for all x.X is said to be as large as Y in the sense of SSD, denoted asX⪰SSDY, if and only if∫-∞x(F(t)-G(t))dt⩽0for allx.The above define weak ordering relations, the strict stochastic dominance (SD) relation, for both Definitions 1 and 2, is defined as:X≻SDYif and only ifX⪰SDYandY⪰SDX,i.e., the inequality is strict for at least one x in the above definitions; that is to say, F and G are not identical.Stochastic dominance establishes a partial ordering of probability distributions, and it can be shown that distribution F dominates distribution G in the sense of nth-order stochastic dominance if and only if all EU maximizing individuals with utility functions whose successive derivatives to order n alternate in sign (that is, u such that sign uj=(−1)j+1 for j=1,…,n) prefer F to G (Levy, 1992).By selecting a risk measure ad hoc, we expose ourselves to—and presumably tolerate—violating some normative principles, but we would like to maintain others. In particular, we do not want to give up monotonicity with respect to larger payoffs and decreasing risk, that is, respectively, FSD and SSD orders. SSD is critical because it lies at the heart of fundamental notions of risk and risk aversion, also it can rank more prospects than FSD. However, working directly with stochastic dominance orders, e.g., as constraints in portfolio optimization (Dentcheva & Ruszczyński, 2006), is computationally challenging and not always tractable. Thus, a key issue in using any Risk-Value model is: does it rank prospects consistently with FSD and SSD? Propositions 1 and 2 below address these questions for the M-Δ model.Proposition 1Assume that H is differentiable and all expectations exist. The M-Δ model satisfies strict first order stochastic dominance if and only if0⩽H′(y)⩽1for ally⩾0.The intuitive interpretation of the condition is that the sensitivity to outcomes differences should not exceed the sensitivity to the outcomes themselves. Indeed, if the weight placed on payoff deviations should ever exceed the weight put on the payoffs, it would be possible to have a situation in which a strict increase in a payoff (making the gamble strictly better) would increase the risk Δ(X) of the gamble more than its expected reward E[X]. Delquié and Cillo (2006) showed the result of Proposition 1 for weak FSD using Machina’s (1982) concept of “local utility function”. The new proof we provide here uses a different approach and shows the result for strict FSD.Proposition 2Assume that H is twice differentiable and all expectations exist. The M-Δ model satisfies strict second order stochastic dominance if and only if H is such that, for all y>0:0<H′(y)⩽1andH″(y)⩾0, that is, H is strictly increasing, convex, and never grows faster than the identity function.The convexity of H essentially guarantees that adding an independent, zero-mean risk to X will never cause Δ(X) to decrease. Behaviorally, it reflects increasing sensitivity to larger deviations. Yitzhaki (1982) showed that the Mean-Gini model satisfies FSD and SSD in the weak sense. Ogryczak and Ruszczyński (2002) showed that the Mean-Gini model meets strict SSD. Proposition 2 shows a more general result, since the M-Δ model includes Mean-Gini as a special case.Several axiomatic approaches to constructing risk measures have been proposed. Some approaches have a prescriptive orientation, that is, they attempt to outline general properties deemed desirable or necessary for adequate risk management (Ma & Wong, 2010). In this section, we examine how our risk model relates to two classes that have received a lot of attention: the so-called “Coherent” and “Convex” risk measures.Artzner, Delbaen, Eber, and Heath (1999) measure risk as the amount of cash that should be added to a risk position, i.e., a gamble, to make it acceptable (their formulation also incorporates the interest rate earned on the cash provisioned). They argue that the following axioms, P1–P4, are necessary for the proper management and regulation of risk, and they call measures satisfying them Coherent risk measures. Let ρ denote the risk measure as defined by Artzner et al. (1999), then for all X, Y:P1. Translation invariance: ρ(X+δ)=ρ(X)−δ, for all δ.P2. Subadditivity: ρ(X+Y)⩽ρ(X)+ρ(Y).P3. Positive homogeneity: ρ(λX)=λρ(X), for all λ⩾0.P4. Monotonicity: If X⩽Y, ρ(X)⩾ρ(Y).The way Artzner et al. (1999) define risk is different from just variability or dispersion: it is essentially the negative of a mean-risk measure (or 0 if the mean-risk index is positive). Therefore, it corresponds to the M-Δ model by taking: ρ(X)=max(−V(X),0)3Indeed, if position X is unacceptable under the M-Δ model, i.e., V(X)<0, then the amount by which the position needs to be augmented to make it, at the limit, acceptable is just −V(X).3; and the above axioms may be recast in terms of Δ(·) as defined in (1) and (2). The correspondence between deviation risk measures and the measures considered by Artzner et al. (1999) was studied by Mansini et al. (2003). Also see Rockafellar, Uryasev, and Zabarankin (2006) on the one-to-one correspondence between deviation measures and the type of risk measures defined by Artzner et al. (1999).Coherent risk measures are generally not consistent with SSD (de Giorgi, 2005), but some are. For example, Ogryczak and Ruszczyński (1999) show that certain coherent risk measures based on semi-deviations (standard or absolute) preserve SSD. Also, Mansini et al. (2003) showed that a SSD efficient measure that is LP decomposable is a coherent risk measure.The property of positive homogeneity (P3) may be considered rather restrictive. One could argue that doubling the position in a gamble will at least double the risk incurred, that is:P3′.ρ(λX)⩾λρ(X)for allλ⩾1(this implies thatρ(λX)⩽λρ(X)forλ⩽1).The property P3′ may be deemed more compelling, and more flexible, than P3. In this spirit, Föllmer and Schied (2002) propose to replace P2 and P3 by the weaker property:P2′.Convexity:ρ(λX+(1-λ)Y)⩽λρ(X)+(1-λ)ρ(Y),for all0⩽λ⩽1.Property P2′ just provides that diversification should not increase risk, a cornerstone principle of risk management. Föllmer and Schied (2002) define risk measures satisfying Translation Invariance (P1), Convexity (P2′) and Monotonicity (P4) as Convex risk measures. They show a representation theorem for Convex risk measures parallel to that obtained by Artzner et al. (1999) for Coherent risk measures. See de Giorgi (2005) and Brown and Sim (2009) for further characterization of Convex risk measures.It is not difficult to show that the M-Δ model will comply with the axioms of Convex risk measures under the following conditions:•Translation invariance: fulfilled for any H. Note that this, of course, is tantamount to constant risk aversion, which we discussed previously.Convexity: fulfilled whenever H is convex.Monotonicity: holds if and only ifH′⩽1, as seen in Proposition 1.Thus, it turns out that the conditions for SSD (Proposition 2) ensure that M-Δ is, in addition, a Convex risk measure.For those who would like to use a Coherent risk measure, the Subadditivity axiom (P2) will be fulfilled if H is subadditive, and Positive homogeneity will hold if and only if H is linear. Thus, to satisfy the four axioms of Coherent risk measures simultaneously, H has to be a seminorm, that is, we have to take H linear: H(y)=βy, with 0⩽β⩽1. In that case, we have the Gini measure: Δ(X)=β/2G(x), where the parameter β reflects the decision maker’s trade-off between risk and reward. This shows incidentally that the Gini measure is an example of a Coherent risk measure satisfying SSD.In sum, the M-Δ model can accommodate the general axioms reviewed above, either individually or collectively, for convexity or coherence, and convexity of the risk measure is guaranteed whenever H fulfills the conditions of Proposition 2.In this section, we examine two central problems in the economic analysis of investment behavior: the trading of an asset, and the optimal allocation of wealth to a risky asset. In each case, we show that the M-Δ model has the flexibility to encompass the qualitatively different predictions made by EU and by models based on non-additive probability weighting. Before analyzing these problems, we begin with a general result on the ability of M-Δ to produce first order or second order risk aversion, because this distinction has been shown to drive risk taking behavior in a general fashion.The notions of first and second order risk aversion were presented by Segal and Spivak (1990). Consider the gamble αX, corresponding to taking a fraction α of the random variable X. Under first order risk aversion, the risk premium for a small risk αX, i.e. for α sufficiently small, is proportional to α, that is, linear in the size of the risk taken. Under second order risk aversion, the risk premium is proportional to α2, and thus approaches 0 faster than α. Thus, an individual with second order risk aversion becomes nearly risk neutral for small risks.First and second order risk aversion imply qualitatively different behaviors when risks are scalable, such as when it is possible to buy variable quantities of an asset, or partially insure against a risk.Let X be a non-constant random variable, and denote by π(α) the risk premium of the gamble αX.Definition 3An individual’s risk aversion is of–first order if for every non-constant X such that E[X]=0, dπ/dα|α=0+>0;second order if for every non-constant X such that E[X]=0, dπ/dα|α=0=0 and d2π/dα2|α=0+>0.Note that the above definition, from Segal and Spivak (1990), is stated for the case of a risk-averse attitude, not risk-seeking. The inequalities in Definition 3 are reversed for a risk-seeking attitude. Also, we do not specify that the definition holds at a particular wealth level, because the individual’s wealth level is immaterial in the M-Δ model, due to constant risk aversion.All models based on rank-dependent probability weighting embody first order risk aversion, whereas EU and models with smooth Fréchet differentiability have second order risk aversion. The M-Δ model can embody either first or second order risk aversion, depending on a simple condition on the first derivative of H at 0.Proposition 3Consider an individual behaving according to the M-Δ model with a twice differentiable H function. The individual’s risk aversion is:(i)first order if and only if H′(0)>0;second order if and only if H′(0)=0 andH″(0)>0.The proof of Proposition 3 also shows that in the M-Δ model the risk premium for a small risk is proportional to the Gini Mean Difference. Under EU, the risk premium is a function of the variance (Pratt, 1964).The order of risk aversion determines behavior in taking a position on an asset, as we study next.Consider an asset whose present monetary value can be described by the random variable X. Assume that it is possible to trade the asset in any (small) quantity. Under EU, a risk-averse individual with differentiable utility function will invest a positive quantity of his/her money in the asset if and only if the expected value E[X] of the asset exceeds its price, and he/she will short sell (some of) the asset if and only if the asset price exceeds E[X]. This result, shown in Arrow (1974) and also discussed in Segal and Spivak (1990), is due to the fact that risk aversion is of the second order under EU: as mentioned in Section 5.1, this implies that for sufficiently small risks the individual is locally risk-neutral and decides according to expected value. Therefore, the individual will neither buy nor short sell the asset if, and only if, the price is exactly equal to the expected value of the asset. Dow and Werlang (1992) showed a contrasting result that, for an individual maximizing expected utility with non-additive subjective probabilities, an interval exists such that, for any price in this interval, the individual abstains from trading the asset. If the price is lower than the lower bound of the interval, the individual buys; if the price is higher than the upper bound, he/she short sells; for prices within the interval, he/she declines holding a position. This result, intuitively plausible and compatible with observed investment behavior, is further extended by Chateauneuf and Ventura (2010).4Under EU, no-trade intervals may be explained by the presence of trading costs, but it is unclear whether this accounts for empirical observations.4Corollary 4Consider an individual behaving according to the M-Δ model with a convex, differentiable H function and constant initial wealth. For any random asset X, the individual will:hold no position when the asset price is in the interval[E[X]-12G(X)H′(0),E[X]+12G(X)H′(0)]if he/she has first order risk aversion;take a position as an EU maximizer if he/she has second order risk aversion.The no-trade interval is always centered on E[X] and it has a strictly positive length if and only if the individual has first order risk aversion. In the case of first order risk aversion, the size of the no-trade interval depends on both the individual’s attitude toward infinitesimal risks (captured by H′(0)) and the riskiness of the asset as measured by the Gini measure: the riskier the asset, the wider the range of prices within which the individual is unwilling to take a position. For H′(0)=0, the individual is (nearly) risk-neutral for infinitesimal risks, just like an EU maximizer.RemarkWhen initial wealth W is random (includes background risks), if X and W are comonotonic, i.e., X does not provide a hedge against W, it can be shown that the individual will be willing to buy an amount of Xonly if the asset price π is<E[X]-12G(X)H′(0), that is, the no-trade interval includes (is wider than) the interval given in Corollary 4. Because in this case X tends to compound the risk in W, the individual will be more conservative in buying it than he/she would be in the absence of background risk, which conforms to intuition. If, on the other hand, X and W are countermonotonic (W and −X are comonotonic), X provides a hedge against the risk in W, then the individual (even with first order risk aversion) will want to buy a strictly positive amount of X whenever π<E[X]. Thus, for such X, we do not have a no-trade interval. Indeed, there is always an advantage to buying (some of) X whenever it is priced at anything less than its expected value, because it will dampen the background risk while providing an increase in expected total wealth. This again matches common sense.The class of models considered by Dow and Werlang (1992) always produce a no-trade interval of non-zero length, while EU never produces a no-trade interval. The M-Δ model bridges these two situations. Chateauneuf and Ventura (2010) show that Dow and Werlang’s (1992) result holds for non-positive assets. Note that, owing to translation invariance, no assumptions were necessary about the sign of X in the M-Δ analysis above. If X<0, the price at which the individual would be willing to buy the asset is, of course, negative, which is tantamount to selling insurance against X.A question of interest for any model of choice under risk is the kind of solutions it provides to the optimal asset allocation problem. This is especially relevant if M-Δ is to be used as a criterion for building optimal portfolios of risky assets.Under EU, a risk-averse individual with differentiable utility should always invest a strictly positive amount of money in a risky asset that has a positive expected value, no matter how risky the asset, or how risk-averse the individual. This is because a risk-averse EU maximizer behaves arbitrarily close to risk-neutral for risks sufficiently small. Other models, such as Yaari’s (1987) Dual Theory (DT), predict “plunging,” that is, for any risky asset, invest either nothing or the full capital available in the risky asset.5“Dual Theory” (Yaari, 1987) is an axiomatic model of Rank-Dependent Utility (RDU) (Quiggin, 1982) with linear utility. The Mean-½Gini model considered by Yitzhaki (1982) also predicts plunging, because it can be shown to be equivalent to RDU with linear utility, i.e., DT, and probability weighting function w(p)=p2.5Yaari (1987) argues that the two classes of solutions produced by EU (always an interior solution) and DT (always a corner solution) are extreme, and that an intermediate between these two situations would be more satisfactory. The M-Δ model is able to bridge these two extremes and, more interestingly, also produce a new type of diversification behavior intermediate between the two: it allows an investor to hold back or diversify depending on whether the performance of the risky asset meets a certain threshold. That is, the M-Δ model does not prescribe diversification in all cases (as EU and M-V do), but it does not have the problem of bang–bang solutions (as DT does).To show this, let us consider a simple asset allocation problem, involving a safe asset with 0 rate of return and a risky asset with a random rate of return θ. Assume that the return of the risky asset is distributed in the interval [−1,a] and has a positive expected value E[θ]>0. Let K be the total amount available to invest, and x the amount to be invested in the risky asset, 0⩽x⩽K. Thus, the net payoff is described by the random variable X=K+θx. For example, if the return achieved is θ=−1, then the amount invested x is entirely lost and the investment capital is reduced accordingly.To state the main results below, it will be helpful to define the following quantity:(4)S(θ)=E[θ]1/2G(θ)where G(·) is the Gini measure of risk. S(θ) defines a measure of performance of the risky asset: its return per “unit of risk,” or its reward-to-risk ratio, akin to the Sharpe ratio. It depends solely on the characteristics of the risky asset, not on the investor’s risk preferences, which are captured by H. S(·) so defined also happens to be the reciprocal of the Gini Index. Proposition 5 states that the optimal asset allocation can be either a corner solution (as in DT) or an interior solution (as in EU) all depending on S(θ) and the individual’s pattern of risk aversion over the range of returns.Proposition 5Consider an individual behaving according to the M-Δ model with a differentiable, convex H function. For this individual, the optimal allocation to a risky asset may be to invest none, some, or all of the capital available depending on the performance of the risky asset S(θ). Specifically, if x∗denotes the optimal allocation:(i)forS(θ)⩽H′(0), the optimal allocation is the corner solution x∗=0;forH′(0)<S(θ)<H′((a+1)K), there exists a unique solution 0<x∗⩽K;forS(θ)⩾H′((a+1)K), the optimal allocation is x∗=K.If the individual has second order risk aversion, then we will have S(θ)>H′(0)=0 whenever E[θ]>0. In such case, the solution necessarily involves a strictly positive investment, x∗>0, as under EU. An individual with first order risk aversion will hold back if the reward-to-risk performance of the risky asset remains below a certain threshold, and begin to diversify if the performance is beyond the threshold. The optimal diversification x∗ will gradually augment with S(θ). That is, M-Δ provides more flexibility not only by allowing different individuals to behave differently as we saw in asset trading, but also by allowing the same individual to adopt qualitatively different behaviors in different situations.RemarkSuppose the investor’s risk preferences comply with SSD. Then, according to Proposition 2, we haveH′⩽1. In this case, if the risk asset has performance S(θ)⩾1, we are automatically in case (iii) of Proposition 5. Thus for any security with S(θ)⩾1, the maximum amount should be invested in the risky asset.6For H linear: H′(y)=c, with0⩽c⩽1, we have plunging, i.e., corner solutions, for any risky asset. If the risky asset is such that S(θ)>c, the optimal solution is x∗=K; if S(θ)<c, the optimal solution is x∗=0; if S(θ)=c, the investor is indifferent toward any level of investment between 0 and K. This, of course, concords with Yaari (1987), since the case H linear yields a Mean-Gini model equivalent to Yaari’s Dual Theory with quadratic, convex probability weighting.6In sum, the value of the diversified portfolio can be monotone decreasing, monotone increasing, or non-monotone single peaked over the range of possible investment levels. Thus, the M-Δ model is able to produce a richer pattern of optimal solutions to the asset allocation problem, depending on the features of the risky asset relative to the investor’s pattern of risk aversion over the range of the portfolio’s outcome. To illustrate the point, Fig. 1provides an example of how the pattern of asset allocation under M-Δ is intermediate between those produced by EU and DT. The discontinuity in the DT pattern is the so-called plunging phenomenon.To wrap up this section, the M-Δ model can yield thresholds in diversification, and no-trade intervals without resorting to non-linear probability weighting. The decision maker’s sensitivity to small risks (i.e., the derivative of H at 0) and the reward-to-risk ratio of the asset, as defined by S(·) in (4), both play special roles in these results. Fig. 2illustrates three main patterns of weighting of deviations in the M-Δ model. An individual with a linear H function with slope less than 1, as shown in (a), is a DT (or Mean-Gini) maximizer. An individual with pattern (c) will behave qualitatively as an EU maximizer. An individual with pattern (b) will behave as an EU maximizer in some cases, although he/she is not acting according to any specific utility function, and as a RDU maximizer in other cases, although he/she is not acting according to a specific non-additive probability weighting, all depending on the reward-to-risk performance of the gambles faced relative to the obtuseness of the kink in H at 0. Patterns (a) and (b) have first order risk aversion, while (c) has second order risk aversion. The strength of first order risk aversion depends on the size of H′(0), thus M-Δ offers a continuum of some sort between first order and second order risk aversion.Using the M-Δ model requires obtaining an estimate of the deviations weighting function, H. This function can be assessed by eliciting the decision maker’s preferences for simple gambles, much like in utility assessment. For example, the well-known methods of Certainty Equivalence (CE) and Probability Equivalence (PE) (Hershey & Schoemaker, 1985) can be used to obtain a set of indifference statements, from which non-parametric estimates of values of H can be directly calculated. The CE and PE methods obtain indifference statements between a binary gamble X={x,p; 0,1−p} and a sure payoff s by varying s or p, respectively. This readily yields a point value estimate of H as follows:V(s)=V(X)s=px+(1-p)0-p(1-p)H(x-0)=px-p(1-p)H(x)Hence:H(x)=(px-s)/(p(1-p)).Other elicitation methods can be used, of course. The elicitation questions will produce a system of linear equations in the unknowns, and these can be designed to have as many unknowns as (independent) equations so as to yield an exact, unique solution. For example, three outcome lotteries with equally spaced outcomes will result in only two unknowns on H. Consider X={x+d, p1; x, p2; x−d, 1−p1−p2} and Y={y+d, q; y, 1−q}. A preference relation between X and Y involves up to 5 different outcomes, but only 2 unknowns, H(2d) and H(d). One other equation involving either or both of these unknowns would be sufficient to solve. Realizing this can provide great flexibility in designing easily solvable assessment questionnaires.An arbitrary set of indifference statements may result in equations involving a set of unknown H values that cannot be solved exactly (as may also arise in assessing utility functions). In this case, a numerical method may be used to find a set of H values providing a best fit to the preference data, such as minimizing least-square error, or other appropriate criterion.Selecting a parametric form for H can further simplify the assessment, for in that case, it reduces to the estimation of just the parameter(s) of the functional form. Examples of one-parameter H functions that satisfy the conditions of Propositions 1 and 2 are: H(y)=y2/(y+α), α⩾0; or H(y)=y+e−αy−1 with 0⩽α⩽1. The latter features first order risk aversion for 0⩽α<1, and second order risk aversion for α=1 (see Proposition 3). Another, somewhat crude possibility is to take a piecewise linear function: H(y)=0 for 0⩽y⩽δ, H(y)=y−δ for y>δ, that is, H(y)=max(0, y−δ). This function lets risk aversion kick in when the spread of gambles exceeds δ, that is, deviations up to a certain level are just ignored, while deviations beyond this range are weighed linearly. The value of the parameter δ could be readily determined by asking: “what is the largest range of deviations for which risk would not be a concern at all?” Of course, this simple function satisfies SSD in the weak sense only for small gambles (whose range does not exceed δ), because it is risk-neutral for such gambles.The above functions are proposed just as illustrative examples, not to suggest that they are more desirable than other possible forms. The choice of an appropriate H function should be based on how well it accounts for the decision maker’s risk preference patterns, and other considerations such as computational tractability.The computational tractability of a risk measure is an important consideration for use in large scale optimization problems. Risk measures that enable linear programming (LP) formulations are of special interest, due to the great computational efficiency of LP optimization (Krzemienowski & Ogryczak, 2005; Mansini et al., 2003). The M-Δ model with non-linear weighting of deviations will, of course, not allow an LP formulation of mean-risk optimization, and thus sacrifice computational power. The Gini measure, which weights deviations linearly, does give rise to an LP specification, although it produces larger size optimization models than linear risk measures based on deviations from a target, such as, e.g., Mean Absolute Deviation. Indeed, for an optimization problem in which the data set consists of random variables (e.g. stock returns) with n discrete realizations, linear single target deviation measures will require n additional decision variables and associated constraints, while the Gini measure will require n2 additional variables and associated constraints, specifically one for each deviation between any two realizations. The use of piecewise linear H functions in M-Δ would permit LP formulations, although this would produce LP problems larger than Mean-Gini, because each piecewise linear segment of the H function would necessitate its own set of decision variables in the LP formulation. Nonetheless, very large LP problems can routinely be solved efficiently nowadays. Therefore, piecewise linear H functions could offer a good compromise between solvability, by allowing LP formulations, and descriptive flexibility, by allowing a wide diversity of risk preference patterns.7Care should be taken to verify that the results shown in Sections 3 and 5 for differentiable functions hold for piecewise linear functions (which have unequal left and right derivatives at a finite number of points), which we believe to be the case.7See Mansini et al. (2007) for a study of this issue in the case of using Conditional Value-at-Risk (C-VaR) as a risk measure involving an LP formulation.We developed a set of compact computational spreadsheet formulas for calculating Δ(X) of any discrete distribution, for a number of parametric H functions. The functions accept data arrays as arguments, which can be either a set of observations of the random variable X, {x1,x2,…,xn} (a one-dimensional array), or a frequency distribution, {pi, xi; i=1,…,n} (a two-dimensional array). The parameter(s) of the H function can also be specified as arguments. These functions (available from the authors) can be loaded in the function library of the spreadsheet program, and used like other standard spreadsheet functions.For numerical illustration purposes, we built the M-Δ efficient frontier for a basket of 15 stocks of large companies, selected to cover a diversified range of industries and geographical origins (North America, Europe, and Asia). For each stock, monthly prices adjusted for dividends and stock splits were obtained for the period from January 1999 to January 2010, allowing calculation of monthly returns for 11years, that is, 132 observations. A number of points of the efficient frontier were computed by minimizing the portfolio risk, Δ, for different levels of expected return set as a constraint, assuming no short sales (i.e., non-negativity constraints on stock weights). The decision variables in the optimization model are the weights on the stocks, with the constraint that they sum to 1. The H function used in the Δ risk measure was the linear plus exponential form mentioned as an example in Section 6.1 with parameter α=0.2.Because Mean-Variance plays a central role in modern finance and, despite shortcomings, is still the most widely used criterion to select portfolios of securities, it is relevant to compare the portfolios generated by M-Δ to those of M-V. The optimization model formulation for M–V is identical to that described above, except that the objective function is to minimize the portfolio variance instead of the Δ measure of risk. The portfolios produced by M-Δ and M-V have generally similar profiles, but with differences that appear to be systematic.First, M-Δ appears to produce more diversified portfolios than M-V over the range of achievable returns, except at high returns. Everywhere except toward the northeast extremity of the efficient frontier, the M-Δ portfolio includes more stocks than the M-V portfolio, also the mean absolute deviation of portfolio weights from equal weights (the so-called “naïve diversification” portfolio) is lower for M-Δ than for M-V. When high returns are required, the M-Δ and M-V portfolios tend, as expected, to become more concentrated on a smaller number of stocks, those capable of producing high expected returns. In those cases, M-Δ selects the same number or one or two fewer stocks than M-V. This phenomenon was again observed by replicating the analysis on a different set of 10 stocks, selected arbitrarily by ticker symbol alphabetical order from the CRSP data base of Wharton Research Data Services, with monthly return history from 1999 to 2008. Throughout the range of the efficient frontier, the M-Δ portfolios include a greater or equal number of stocks than M-V. Also, the mean absolute deviation of weights from equal weights is lower for M-Δ than M-V with one exception, again, at high returns (the same holds if the standard deviation is used as a measure of how spread out the weights are).Second, M-Δ portfolios have return distributions with more pronounced skewness. This is reported in Table 1, showing a summary of comparative features of optimal portfolios obtained by M-Δ and M-V at different levels of target expected return covering the efficient frontier.As can be seen in Table 1, the M-Δ optimal portfolios have systematically and sizably higher positive skewness than the M-V optimal portfolios. Of course, positive skew in the portfolios is due to the presence of positive skew in the distributions of individual stock returns. As it turns out, the other data set of 10 stocks contained stocks with mostly negative skewness. For those stocks, M-Δ portfolios have stronger negative skewness than M-V portfolios. The point is that M-Δ seems to select efficient portfolios that retain more of the skewness of the component stocks than M-V. Empirical evidence indicates that investors often prefer positive skewness. To the extent that investors are able to screen stocks for positive skewness, M-Δ may help construct portfolios that preserve this desirable feature. Alternatively, because higher skewness is associated with higher risk-return combinations (as evidenced in Table 1), M-Δ may better allow investors to satisfy their desire for upside potential without having to sacrifice efficiency or take excessive risk exposure (see Mitton & Vorkink, 2007 for a study of this issue under M-V).Finally, the data set and optimization model at hand for this illustration gave us the opportunity to verify the predictions of Proposition 5. For this, we construct an efficient portfolio (either by M-Δ or M-V) and calculate the reward-to-risk ratio, S defined in (4), of this portfolio. By solving the optimal allocation between cash and the portfolio for investors with different risk aversion levels, we verify that investors with H′(0)⩽S (H′(0)=1−α for the H function we used) do not wish to invest and prefer to keep all cash, while investors with H′(0)>S allocate a positive proportion of their money to the portfolio, and the higher H′(0), the larger this proportion.

@&#CONCLUSIONS@&#
