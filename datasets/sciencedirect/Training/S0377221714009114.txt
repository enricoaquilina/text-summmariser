@&#MAIN-TITLE@&#
Multi-attribute sequential decision problem with optimizing and satisficing attributes

@&#HIGHLIGHTS@&#
Several decision alternatives with multiple attributes are presented sequentially over time.We assume that the multiple attributes can be divided into a major attribute and minor ones.The major attribute must be “optimized”, while the minor attributes must be “satisficed”.We propose the optimal strategy that maximizes the probability of selecting the best choice.We consider the continuous-time case as well as the discrete-time case.

@&#KEYPHRASES@&#
Sequential decision making,Secretary problem,Multi-attribute decision analysis,Optimal stopping rule,

@&#ABSTRACT@&#
We deal with the multi-attribute decision problem with sequentially presented decision alternatives. Our decision model is based on the assumption that the decision-maker has a major attribute that must be “optimized” and minor attributes that must be “satisficed”. In the vendor selection problem, for example, the product price could be the major factor that should be optimized, while the product quality and delivery time could be the minor factors that should satisfy certain aspiration levels. We first derive the optimal selection strategy for the discrete-time case in which one alternative is presented at each time period. The discrete-time model is then extended to the continuous-time case in which alternatives are presented sequentially at random times. A numerical example is used to analyze the effects of the satisficing condition and the uncertainty on the optimal selection strategy.

@&#INTRODUCTION@&#
In many decision problems, not all decision alternatives are available to the decision-maker (DM) simultaneously. Instead, the DM evaluates only one alternative at a time, and decides whether to make a final choice or continue searching for better alternatives (Korhonen, Moskowitz, Salminen, & Wallenius, 1993). Consider, for example, the problem of hiring an employee or buying a house in the open market. Several decision alternatives (or choices) are presented to the DM randomly and sequentially over time. After evaluating a choice currently available, the DM may either select it and terminate the search process, or reject it and continue the uncertain search process.In some situations, any choices that have been rejected cannot be recalled at a later stage. Therefore, if the search process is terminated too early, choices superior to the selected choice may not have been presented yet; if the selection is made too late, the superior choices may have been rejected earlier in the search process. In such a case, the DM's goal is to derive the optimal stopping rule that maximizes the probability of selecting the “best choice” (Chun, 1999).In many complex decision problems, each choice is evaluated in terms of multiple conflicting attributes. We may simply assume that only one of those attributes is a major factor and other attributes are minor ones that can be ignored. Then, the multi-attribute sequential decision problem is simply reduced to a single-attribute decision problem with only one major attribute. In the house selling problem, for example, the most important attribute is the offer from a potential buyer (Chun, Plante, & Schneider, 2002). From the seller's point of view, the objective is to find the highest offer in terms of dollar value within a limited time period.In the house buying problem, on the other hand, each house is compared in terms of multiple attributes (or dimensions), such as the asking price, size, age of the house, and so forth. From the buyer's point of view, buying a house is presented as a multi-attribute sequential decision problem. The job search problem is another example of the multi-attribute sequential problem. The job offers are usually compared based on the starting salaries, fringe benefits, locations, future growths, and so forth (Bearden, Murphy, & Rapoport, 2005). In the vendor selection problem in supply chain management, the most popular evaluating criteria are product price, quality, and delivery time. The marriage problem (or the bachelor's dilemma) also involves many conflicting objectives such as the appearance, intelligence, personality, financial security, and so on.The single attribute sequential decision problem is also known as the secretary problem, marriage problem, job search problem, parking spot problem, or asset-selling problem, with each of them using different assumptions. Since its introduction in the early 1960s, this particular field of study has experienced rapid growth, and its applications extend to a wide variety of managerial decision problems. Readers who are more interested in various types of single-attribute sequential decision problems are referred to many excellent review papers, including Bearden and Rapoport (2005), Ferguson (1989), and Freeman (1983).For the non-sequential version of multi-attribute decision problems, many decision models, such as analytic hierarchy process (AHP), genetic algorithms, simple multi-attribute rating techniques (SMART), fuzzy set theory, and data envelopment analysis (DEA), have been proposed (Ho, Xu, & Dey, 2010). The goal programming is one of the most popular methods in treating multi-attribute decision problems (Aouni & Kettani, 2001; Tamiz, Jones, & Romero, 1998). The goal programming and its variants have been successfully applied to a wide variety of multiple objective/criteria decision problems (Abdelaziz, 2007; Aouni, Colapinto, & La Torre, 2014; Köhn, 2011).Only recently has serious consideration been given to the sequential decision problem with multiple attributes, which has inherent difficulties in the formulation of models. The first difficulty is that there might not be a single choice that “globally” dominates all of the other choices in terms of all of the multiple attributes (Korhonen, Moskowitz, & Wallenius, 1986). The second difficulty is that it is also not realistic to assume the DM's multi-attribute utility function to be known explicitly (Korhonen & Wallenius, 1986). Finally, the multiple attributes are statistically inter-dependent with each other. Despite these difficulties, some authors have assumed that the DM has a given utility function of multiple attributes with known parameter values or that the DM can somehow rank-order the choices.Bearden and Connolly (2007) considered two types of search strategies for the multi-attribute sequential selection problem. They assume that the multi-attribute utility function and its parameter values are given, and the value of a choice is simply the sum of its attribute values. Recently, Smith, Lim, and Bearden (2007) and Lim, Bearden, and Smith (2006) considered a similar multi-attribute sequential selection problem with a search cost. They also assume that the multi-attribute value function is a separable function, and that the DM's objective is to maximize the expected payoff. In the rank-based sequential selection problem, Bearden and Rapoport (2005) assumed that attributes are statistically independent with each other, and that the value of a choice is the sum of its rankings with each attribute.In the paper, we overcome the inherent difficulty by dividing multiple attributes into two groups – one “major” and several “minor” attributes. Dividing or prioritizing multiple attributes is not uncommon in multi-objective decision analysis. In pre-emptive goal programming, for example, the decision maker prioritizes his or her goals into different priority levels (Jones & Jimenez, 2013). The idea behind the preemptive goal programming approach is that lower priority level goals should not be attained at the expense of higher priority goals. As a result, some of the goals satisfy their aspiration levels, but other goals may not achieve their aspiration levels perfectly.We then assume that a major attribute should be “optimized”, whereas minor attributes should be “satisficed”. (The term satisficing is a portmanteau of satisfy and suffice.) In the house buying problem, for example, a buyer's goal is to find the least expensive house among the ones that have at least 2000 square feet of living area and are less than 10 years old. In such a case, the asking price is the major attribute (or goal) that should be optimized, and the size and the age of a house are the minor attributes (or constraints) that should be satisfied.The assumption significantly simplifies the search process and the computations involved in executing it. Any choices that satisfy the minimum requirements of the minor attributes are called “acceptable”. Among the acceptable choices, the “best choice” is defined as the one that has the best value on its major attribute. In the paper, we propose a rank-based sequential selection strategy that maximizes the probability of selecting the best choice.The idea of satisficing and optimizing attributes is adopted from the two cognitive styles suggested by Simon (1955, 1959). A satisficing conception of rationality will permit the “suitable means” to be good enough; a maximizing conception will require the “suitable means” to be the best (Byron, 1998). Thus, “satisfiers” usually set an aspiration level and simply try to find any choice that reaches or exceeds that level. On the other hand, “maximizers” try to make an optimal decision among all feasible choices.These two distinct approaches to human decision-making have been widely studied in economics and behavioral science (Bearden & Connolly, 2008; Byron, 1998; Sen, 1997). The two approaches have also been applied in the field of operational research. Chance-constrained programming (Charnes & Cooper, 1963) and goal programming (Tamiz et al., 1998) in multi-criteria decision analysis are well-known examples of the satisficing approach. As far as I know, there has not been a research effort devoted to considering the “optimizing” and “satisficing” attributes in the context of the sequential decision analysis with multiple attributes.The paper is organized as follows: In Section 2, we formally define several terms and introduce notations that will be used throughout the paper. The classical “discrete-time” model with a known number of choices is formulated in Section 3, which is modified in Section 4 in order to consider the uncertain availability of a choice at each stage. In Section 5, we consider the “continuous-time” model in which choices are presented at irregular intervals. Section 6 is devoted to the sensitivity analysis of discrete-time and continuous-time models, followed by concluding remarks in Section 7.In a sequential decision problem, we assume that n choices will be presented to the DM sequentially in a random order. The total number of choices, n, is a known constant in the discrete-time case. In the continuous-time case, the inter-arrival time between two successive choices follows a continuous Markov process, and thus the total number of choices is unknown. In the continuous-time case, we assume that the DM must make a decision before a given due date.Each choice will be evaluated based on multiple attributes. One of them is identified as a “major” attribute, and the rest of them are regarded as “minor” attributes. Without loss of generality, we assume that the DM's utility is increasing in each attribute (i.e., the higher is the better) (Korhonen & Wallenius, 1986). A choice is said to be “acceptable” if all of its minor attributes satisfy pre-specified aspiration levels. Among the acceptable choices identified up to the current stage, the choice with the highest value on its major attribute is referred to as the “relatively best choice” or the “candidate”. The “absolutely best choice” (or simple the “best choice” in short) is the candidate with the highest value on its major attribute from all the candidates.In accordance to the dynamic programming formulation of a sequential decision problem, we define the stage, state, and decision as follows: When m more choices will be presented for further consideration, the DM is said to be at the mth “stage” in the discrete-time search process. In the continuous-time case, the “stage” is defined as the remaining time t until the due date. The “state” at each stage is simply represented as a zero-one binary variable, 1 representing that the choice currently under consideration is a candidate, and 0 representing that the current choice is not a candidate.If the state is 1 at the current stage, the DM's “decision” is either (1) to select the candidate currently available and stop the search process or (2) to reject the current candidate and wait for another choice. If the state is 0 at the current stage, the current choice is not a candidate and the DM cannot win with that choice. Thus, the DM must continue the search process at a stage if its state is 0. The “optimal decision” at a given stage is the one that maximizes the probability of winning the game or selecting the absolutely best candidate.Consider, for example, a two-attribute sequential decision problem with a major attribute X and a minor attribute Y. As shown in Fig. 1, suppose that 9 choices will be presented one at a time. Let s be the satisficing condition on the minor attribute Y; any choices with its value yi< s are not acceptable. Thus, the first, fourth, and eighth choices in Fig. 1 are unacceptable and should be eliminated from further considerations.In Fig. 1, only the second, fifth, and seventh choices are considered candidates. From the candidates, the seventh choice is considered the best choice. The DM is said to “win” the game if the DM successfully selects the seventh choice and stop the search process. (Note that the fourth choice is not acceptable, even though its X value is the best of all the available choices.)When the values of a candidate on X and Y are (v, w) as shown in Fig. 1, let ϕm(v, w) be the chance of winning if the DM stops the search process at stage m. Likewise, let πm(v, w) denote the probability of winning if the DM continues the search process at stage m. The optimal decision at stage m hinges on the possibility of having another candidate with X > v and Y > s.As shown in Fig. 1, the several satisficing conditions on minor attributes simply weed out unacceptable choices, truncating the multivariate probability distribution. No matter how many minor attributes we have in a multi-attribute case, there is only one major attribute that should be optimized. Therefore, the number of minor attributes is not relevant in the formulation of a model. For the rest of the paper, we will restrict our attention to the “bi-attribute case” without loss of generality: One is the major attribute and the other is the minor attribute.Consider a simple discrete-time model in which the search process progresses through a series of stages, and at each stage, exactly one choice is available for consideration. In a bi-attribute sequential decision problem, let Fx,ydenote the joint cumulative distribution function of the attributes X and Y. As in Bearden and Connolly (2007) and Smith et al. (2007), we assume that the function Fx,yand its parameter values are given a priori. Also let Fxand Fybe the marginal distributions of X and Y, respectively. We do not need to assume that the attributes X and Y are statistically independent.Suppose that the minimum threshold value on Y is s, and the values of the current candidate are (v, w) with w > s as shown in Fig. 1. If a new choice (X, Y) is available at the next stage, the new choice may or may not be acceptable. Depending on its value X, the new choice also may or may not be better than the current candidate. Thus, the two-dimensional plane in Fig. 1 is divided into four quadrants, and the joint and marginal probabilities can be found from Table 1.If all remaining m choices are either unacceptable or worse than the current candidate, then the DM wins the game at stage m by stopping the search process and selecting the current candidate. Therefore, the probability of winning if the DM stops at stage m and selects the current candidate with values (v, w) can be defined as:(1)ϕm(v,w)=[Fx(v)+Fy(s)−Fx,y(v,s)]m.Among the m remaining choices, the number of acceptable choices j that are better than the current candidate is a binomial random variable defined as:(2)Pm[j]=(mj)[1−Fx(v)−Fy(s)+Fx,y(v,s)]j×[Fx(v)−Fy(s)+Fx,y(v,s)]m−j.Note that ϕm(v, w) in (1) is a monotonically decreasing function of m. Thus, the sequential decision problem belongs to the “monotone case” (Chow, Robbins, & Siegmund, 1971, p. 54) where the “one-stage-look-ahead” policy is optimal (Ross, 1970, p. 138). Consequently, among the j acceptable choices that are better than the current candidate, the DM will choose the first available one. The probability that the first one is the best among j choices is simply 1/j. Therefore, the probability of winning if the DM rejects the current candidate at stage m and continues the search process is defined as:(3)πm(v,w)=∑j=1m1j(mj)[1−Fx(v)−Fy(s)+Fx,y(v,s)]j×[Fx(v)−Fy(s)+Fx,y(v,s)]m−j.The optimal decision at stage m is to select the current candidate with v and w and stop the search process if ϕm(v, w) ≥ πm(v, w). Otherwise, the DM must reject the current candidate and continue the search process.Since the value w on the minor attribute Y is irrelevant as long as w is larger than s, the optimal decision depends only on the value v of the major attribute X. Thus, the optimal decision value vmat stage m is the solution v to the following equation: ϕm(v, w) = πm(v, w) or, equivalently,(4)1=∑j=1m1j(mj)[1−Fx(v)−Fy(s)+Fx,y(v,s)Fx(v)−Fy(s)+Fx,y(v,s)]j.For any given probability distribution function and the minimum aspiration level s on the minor attribute Y, we can find the optimal decision value vmfor each stage m = 1, 2, …, n. The optimal decision at stage m is to stop the search process and select the current candidate if its value v of the major attribute X is higher than the optimal decision value vm. (Note that the optimal decision value is also known as the indifference value or threshold value at stage m.)Consider two special cases. First, if X and Y are statistically independent [i.e., Fx, y(v, s) = Fx(v)Fy(s)], then the optimality equation in (4) becomes(5)1=∑j=1m1j(mj)[[1−Fx(v)][1−Fy(s)]1−[1−Fx(v)][1−Fy(s)]]j.Second, if there is no restriction on Y (i.e., s = −∞), then the optimality equation for the bi-attribute case is simply reduced to(6)1=∑j=1m1j(mj)[1−Fx(v)Fx(v)]j.This equation is consistent with the single attribute case in Gilbert and Mostellar (1966).Consider a discrete-time model under uncertainty, in which the search process progresses through a series of stages, but the DM may or may not have a choice at each stage. Let p be the probability a choice will be presented at a given stage. We assume for simplicity that a choice is presented at each stage according to a Bernoulli distribution with probability p, and p is a known constant where 0< p ≤ 1. In the job search problem, the probability p can be interpreted as the probability of being offered a job after each interview. From the employer's point of view, it is the probability that an applicant will accept the job offer if an offer is extended to the applicant.For notational convenience, the probability that a new choice is both acceptable and better than the current candidate (v, w) is denoted as(7)Fx,y(v¯,s¯)=1−Fx(v)−Fy(s)+Fx,y(v,s).Suppose that the DM has m more stages to go, but the number of remaining choices available is i, where i = 0, 1, 2, …, m. We assume that i is a binomial random variable with parameters m and p. Thus, following from (1) and (7), the probability of winning if the DM stops the search process at stage m is represented as:(8)ϕm(v,w)=∑i=0m[1−Fx,y(v¯,s¯)]i(mi)pi(1−p)m−i=[1−pFx,y(v¯,s¯)]m∑i=0m(mi)[p−pFx,y(v¯,s¯)1−pFx,y(v¯,s¯)]i×[1−p1−pFx,y(v¯,s¯)]m−i=[1−pFx,y(v¯,s¯)]m.Likewise, it follows from (3) and (7) that the equation for the probability of winning if the DM continues the search process at stage m is(9)πm(v,w)=∑i=1m∑j=1i1j(ij)[Fx,y(v¯,s¯)]j[1−Fx,y(v¯,s¯)]i−j×(mi)pi(1−p)m−i=∑j=1m(mj)[pFx,y(v¯,s¯)]j∑i=jm1j(m−j)!(i−j)!(m−i)!×{p[1−Fx,y(v¯,s¯)]}i−j(1−p)m−i=∑j=1m1j(mj)[pFx,y(v¯,s¯)]j[1−pFx,y(v¯,s¯)]m−j.From (8) and (9), the optimal decision value vmat stage m is the solution v to the following equation:(10)1=∑j=1m1j(mj)[pFx,y(v¯,s¯)1−pFx,y(v¯,s¯)]j.There is an easier way to find the optimal decision value vm. For an arbitrary integer m, let cmbe the unique root to the following equation:(11)1=∑j=1m1j(mj)[1−cmcm]j.For example, the unique solutions cmto Eq. (11) are 0.5000, 0.6899, 0.7758, 0.8246 for m = 1, 2, 3, and 4. It can be shown from (6) that cmis indeed the optimal decision value at stage m for the single attribute sequential decision problem with standard uniform distribution Fx. (Table 7 of Gilbert and Mosteller (1966) shows the values cmfor m = 1, 2, …, 50.)From (10) and (11), we derive the following:(12)cm=1−pFx,y(v¯m,s¯)=1−p[1−Fy(s)−Fx,y(vm,s¯)].Thus, in order to find the optimal decision value vmfor the multi-attribute case with an arbitrary multivariate probability distribution, we need to first find cmfor the single attribute case, and then find vmthat satisfies the following equation:(13)Fx,y(vm,s¯)=1−Fy(s)−1−cmp.The optimal decision at stage m is to terminate the search process and select the current candidate if its value v is higher than the decision value vm.As in the previous section, let us consider two special cases. If X and Y are statistically independent, the decision value vmat stage m is given as follows:(14)vm=Fx−1(1−1−cmp[1−Fy(s)]).If there is no restriction on Y (i.e., s = −∞), then it simply reduces to(15)vm=Fx−1(1−1−cmp).Note that, as shown in (13), the optimal decision value vmdepends on the satisficing constraint s and the availability p of a choice at each stage. The availability p is also expected to decrease as the DM progresses the search process.In the continuous-time model, the inter-arrival time between two consecutive choices is a continuous random variable. As in most continuous-time models, we assume that several choices are presented like the events of a Poisson process with parameter λ. Consequently, the continuous-time model is simply treated as a limiting case of the discrete-time model that has already been explained in the previous section.Suppose that the DM has identified a candidate when the remaining time is length t. Also suppose that we break the time period t into m equal intervals, and consider the m intervals as independent, stationary trials from a Bernoulli process. If the probability of having a choice in a time interval t/m is a constant p, the number of choices available during the time period t is a binomial random variable with parameters m and p. It can be shown mathematically that if m is made larger and larger and p is made smaller and smaller in such a way that mp remains constant λt, then the binomial distribution approaches the Poisson distribution (Chun, 2000).Therefore, it follows from (8) that the probability of winning if the DM selects a candidate at stage t and stops the search process is defined as:(16)ϕt(v,w)=limm→∞p→0ϕm(v,w)=limm→∞p→0[1−pFx,y(v¯,s¯)]m=e−λtFx,y(v¯,s¯),where the parameter λ is called the arrival (or intensity) rate of the Poisson process.In this limiting case, the binomial distribution in (9) becomes a Poisson distribution. Thus, the probability of winning if the DM continues the search process at stage t is defined as:(17)πt(v,w)=∑j=1∞1jPoisson[j|λtFx,y(v¯,s¯)]=e−λtFx,y(v¯,s¯)∑j=1∞1j[λtFx,y(v¯,s¯)]jj!.It follows from (16) and (17) that the optimal decision value vtat stage t is the unique solution v to the following equation:(18)1=∑j=1∞1j[λtFx,y(v¯,s¯)]jj!.There is also an easier way to solve the equation for vt. Note that the root to the following equation is shown to be c = 0.80435226286 (Gilbert & Mosteller, 1966).(19)1=∑j=1∞1jcjj!.Thus, the optimal decision value vtat stage t is the solution to the following simple equation:(20)Fx,y(v¯t,s¯)=0.80435λt,or, alternatively,(21)Fx,y(vt,s¯)=1−Fy(s)−0.80435λt.Let us consider two special cases. First, if X and Y are statistically independent, the decision value vtat stage t is given as follows:(22)vt=Fx−1(1−0.80435λt[1−Fy(s)]).Second, if there is no restriction on minor attributes (i.e., s = −∞), it simply reduces to(23)vt=Fx−1(1−0.80435/λt).As expected, the optimal decision value vtdecreases as the remaining time t decreases. If the remaining time is less than a certain limit τ, the DM must select any candidate and terminate the search process immediately. The minimum time length τ can be found from (21) with the left-side being zero:(24)τ=0.80435λ[1−Fy(s)].As shown in the optimality equations in (10) and (18), the primary factor in determining the optimal decision value at each stage is the satisficing condition s on the minor attribute. Another important factor is the availability p of a choice in the discrete-time case, and the arrival rate λ of a choice in the continuous-time case. In the next section, we will analyze the effects of those factors on the optimal decision values.Suppose that the values for Xiand Yiof the ith choice presented in the search process are independent, and identically distributed bi-variate random variables from a continuous probability distribution Fx,y. For the sensitivity analysis, we consider two types of probability distributions: bi-variate uniform distribution and bi-variate normal distribution.First, suppose that the bivariate random variables (Xi, Yi) are uniformly distributed over the range (0, 100). Then, its cumulative distribution function is presented as:(25)Fx,y(v,s)=vs10,000,for0<v<100and0<s<100.Since X and Y are statistically independent in this case, it follows from (14) that the optimal decision value vmat stage m in the discrete-time case is(26)vm=100(1−1−cmp[1−s/100]).In the continuous-time case, the optimal decision value vtat stage t is defined from (22) as:(27)vt=100(1−0.80435λt[1−s/100]),and the minimum time length τ in (24) becomes(28)τ=0.80435λ[1−s/100].Second, suppose that the bi-variate normal distribution Fx,yhas the expected values μx= μy= 50 and the standard deviations σx= σy= 10. The correlation coefficient between X and Y is assumed to be ρxy= +0.5.In the discrete-time case, it follows from (13) that the optimal decision value vmat stage m is the unique solution to the following equation:(29)Φ(vm−5010)−Φx,y(vm−5010,s−5010|ρ=0.5)=1−Φ(s−5010)−1−cmp,where Φ is the cumulative distribution function of the standard normal distribution.In the continuous-time case, the optimal decision value vtat stage t can be found from (21) as follows:(30)Φ(vt−5010)−Φx,y(vt−5010,s−5010|ρ=0.5)=1−Φ(s−5010)−0.80435λt,The minimum time length τ in (24) becomes(31)τ=0.80435λ[1−Φ(s−5010)].We use the Microsoft Excel function, norm.s.inv( ), and the Microsoft Solver add-in to find the optimal decision values in (29) and (30).To analyze the effects of s and p on the optimal decision value vm, we systematically change the satisficing condition s from 0 to 80 by 40 and the availability p of a choice at each stage from 0.2 to 1.0 by 0.4. (Note that the default values are s = 40 and p = 0.8, respectively.) From (11), we first find cmfor each stage m = 1, 2,…,9, which is listed on the last row of Table 2. Then, based on cm, we calculate the optimal decision values vmfrom (26) for a given combination of s and p.When s = 40 and p = 0.6, for example, the optimal decision value at stage 9 is 76.679 as shown in Table 2. This implies that the DM must select the choice at stage 9 if (1) its Y value is satisfactory (i.e., larger than 40), (2) its X value is the best among all acceptable choices presented up to the current stage, and (3) its X value is also higher than 76.679. As shown in Table 2, the decision value at each stage decreases as the DM has fewer choices.Fig. 2 illustrates the effect of the satisficing condition s and the availability p on the optimal decision value vmin the discrete-time case. The solid lines in Fig. 2 clearly show the trade-off between the satisficing condition s and the optimal criterion vm. If the satisficing condition s is set higher, for example, the DM has a smaller number of acceptable choices during the search process. Therefore, the optimal decision value vmshould be set much lower.The dotted lines in Fig. 2 show the relationship between the availability p of a choice at each stage and the optimal decision value vm. As a new choice is less likely to be available at each stage, the DM must lower the decision value vm, because a smaller number of choices will be presented during the search process.As expected, the optimal decision value vmin Fig. 2 is decreasing as the DM is close to the end of the search process. Particularly when the satisficing condition s is set high and the availability p is relatively small, the optimal decision value vmreaches its lowest limit at an early stage of the search process. (In the marriage problem, for example, the bachelor must lower his aspiration level as he becomes older.)We also analyze the effects of satisficing condition s and availability p on the optimal decision value vmwhen Fx,yis a bi-variate normal distribution with μx= μy= 50, σx= σy= 10, and ρ = +0.5, We systematically change s from 20 to 60 by 10 and p from 0.2 to 1.0 by 0.2. The default values are s = 40 and p = 0.8 respectively. The optimal decision values vmfor each combination of s and p are given in Table 3, along with cmon the last row.The effect of the satisficing condition s on the optimal decision value vmis illustrated in Fig. 3. As in the previous case, the decision value vmon the major attribute must be set lower as the satisficing condition s on the minor attribute is set higher, and vice versa. Fig. 3 also shows the positive relationship between the availability p and the optimal decision value vm. If the possibility p of having a choice at each stage is high, then DM expects to have more acceptable choices during the search process and, thereby, sets the optimal decision value vmmuch higher.Table 4shows the optimal decision value vtat stage t in the continuous-time case when Fx,yis a bi-variate uniform distribution over 0 < X < 100 and 0 < Y < 100. The satisficing condition s has been changed from 0 to 80 by 40, and the arrival rate λ of a choice per unit of time is set equal to 0.1, 0.5, and 2. The default values of s and λ are 40 and 1, respectively.Note that, if the satisficing condition s is set too high when the arrival rate λ is relatively low in Table 4, the DM must select any available candidate with vt= 0. The maximum remaining time τ for vt= 0 is given in the third column on the table. When s = 40 and λ = 0.1, for example, the critical value is τ = 13.41 from (28). Thus, the DM must select the first available candidate and terminate the search process if the remaining time is less than 13.41.Fig. 4shows the same result as in the discrete-time case. There is a trade-off between the satisficing condition s and the optimal decision value vt. The positive effects of the arrival rate λ on the optimal decision value vtare also illustrated in the same figure.The optimal decision value vtin the continuous-time case can be found from Table 5when Fx,yis a bi-variate normal distribution with μx= μy= 50, σx= σy= 10, and ρ = 0.5. The satisficing condition s has been changed from 40 to 60 by 10. We also set the arrival rate λ of a choice per unit of time equal to 0.1, 0.5, and 2. The default values of s and λ are 40 and 1, respectively.Fig. 5illustrates the effects of the satisficing condition s and the arrival rate λ on the optimal decision value vt. As expected, the results are very similar to those from the previous cases. The results show the trade-off between the satisficing condition s and the optimal decision value vtand the positive relation between the arrival rate λ and the optimal decision value vt.For the multi-attribute sequential decision problem under uncertainty, we propose in the paper the optimal search strategy which takes the following form: Calculate the decision value vn(or vt) for each stage n (or t) with a parameter p (or λ) in the discrete-time (or continuous-time) case. Then, choose the first candidate whose value on the major attribute is higher than the decision value at that stage. As expected, the decision value is monotonically decreasing in time because fewer and fewer choices will be available for consideration.In the sensitivity analysis, we observed that there is a trade-off between the satisficing condition on the minor attributes and the decision value on the major attribute. We also find that the decision value on the major attribute has a positive relation with (1) the availability p of a choice at each stage in the discrete-time case, and (2) the arrival rate λ of a choice per unit of time in the continuous-time case.Note that the sequential decision problem considered in this paper belongs to the “full-information” case, where the probability distribution Fx,yof the values of choices are assumed to be fully known to the DM. In the “no-information case”, on the other hand, we assume that the distributional form and its parameter values are completely unknown to the DM. Between these two extreme cases, we have the “partial information” case in which the distributional form is known to the DM, but its parameter values must be updated in a Bayesian matter as more choices are available in the search process. The full-information selection strategy considered in this paper could be extended to the no-information or partial information cases with varying degrees of difficulty.Some researchers have tested whether the calculus for probabilities is the appropriate framework to build mathematical models of human decision processes in “single attribute” sequential search problems. For example, Rapoport and Tversky (1970) conducted an experimental test, and observed that their subjects tended to observe fewer choices than the number of choices predicted by optimal search strategy. Recently, Bearden, Rapoport, and Murphy (2006) and Bearden et al. (2005) also found that their subjects tended to terminate the search process earlier than expected. Therefore, one possible line of future inquiry is to conduct empirical research on subjects’ behavior in “multi-attribute” sequential decision problems. It would be interesting to compare the normative solution proposed in this paper with the observed solution derived from the empirical research study.

@&#CONCLUSIONS@&#
