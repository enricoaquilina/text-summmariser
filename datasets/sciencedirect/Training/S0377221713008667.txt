@&#MAIN-TITLE@&#
Automatically improving the anytime behaviour of optimisation algorithms

@&#HIGHLIGHTS@&#
A method to automatically improve the anytime behaviour of optimisation algorithms.Anytime behaviour is evaluated by the hypervolume measure.Decision-maker’s preferences may be incorporated into the automatic tuning procedure.Case-studies include configuring a heuristic algorithm and an MIP solver.

@&#KEYPHRASES@&#
Metaheuristics,Anytime algorithms,Automatic configuration,Offline tuning,

@&#ABSTRACT@&#
Optimisation algorithms with good anytime behaviour try to return as high-quality solutions as possible independently of the computation time allowed. Designing algorithms with good anytime behaviour is a difficult task, because performance is often evaluated subjectively, by plotting the trade-off curve between computation time and solution quality. Yet, the trade-off curve may be modelled also as a set of mutually nondominated, bi-objective points. Using this model, we propose to combine an automatic configuration tool and the hypervolume measure, which assigns a single quality measure to a nondominated set. This allows us to improve the anytime behaviour of optimisation algorithms by means of automatically finding algorithmic configurations that produce the best nondominated sets. Moreover, the recently proposed weighted hypervolume measure is used here to incorporate the decision-maker’s preferences into the automatic tuning procedure. We report on the improvements reached when applying the proposed method to two relevant scenarios: (i) the design of parameter variation strategies for MAX-MIN Ant System and (ii) the tuning of the anytime behaviour of SCIP, an open-source mixed integer programming solver with more than 200 parameters.

@&#INTRODUCTION@&#
Many optimisation algorithms are designed without a specific termination criterion, and generate a sequence of feasible solutions that are increasingly better approximations of the optimal solution. However, the performance of an algorithm is often crucially determined by the choice of the termination criterion and the parameters of the algorithm. If the parameter settings of an algorithm result in fast convergence to good solutions, this may prevent the algorithm from adequately exploring the search space to find better solutions if given ample time. On the other hand, parameter settings that give higher exploration capabilities may produce poor results if the termination criterion is too short. Hence, there is a trade-off between solution quality and the runtime of the algorithm that can be adjusted by appropriately setting the parameters of the algorithm.In many practical scenarios, an optimisation algorithm may be terminated at an arbitrary time, and, upon termination, the algorithm returns the best solution found since the start of the run. In such scenarios, the termination criterion is not known in advance, and, hence, the algorithm should produce as high quality solutions as possible at any moment of its run time. Algorithms that show a better trade-off between solution quality and runtime are said to have a better anytime behaviour (Zilberstein, 1996).There are two classical views when analysing the anytime behaviour (Hoos & Stützle, 2005). One view defines a number of termination criteria and analyses the quality achieved by the algorithm at each termination criterion. In this quality-over-time view, the anytime behaviour can be analysed as a series of plots of time-dependent solution quality distributions. A different view defines a number of target quality values and analyses the time required by the algorithm to reach each target. In this time-over-quality view, algorithms are often analysed in terms of a series of qualified runtime distributions.In this paper, we consider a third view that does not favour time over quality or vice versa. Instead, this third view models the performance profile of an algorithm as a nondominated set in a multi-objective space. An algorithm has better anytime behaviour when it produces better nondominated sets, where “better” means better in terms of Pareto optimality. Surprisingly, this third view has received little attention (Hoos & Stützle, 2005; Chiarandini, 2005; den Besten, 2004), despite the important advances in theory and practice achieved in performance assessment of multi-objective optimisers in the last decade. Essentially, this model allows us to apply the same unary quality measures used in multi-objective optimisation to assign a single numerical value to the anytime behaviour of an algorithm’s run. In this paper, we use the hypervolume measure as the unary quality measure for this purpose. The main reason is that the hypervolume is the quality measure with the highest discriminatory power among the known unary quality measures (Zitzler, Thiele, Laumanns, Fonseca, & Grunert da Fonseca, 2003). In addition, recent work has made possible to describe user preferences in terms of a weighted hypervolume measure (Auger, Bader, Brockhoff, & Zitzler, 2009), and, hence, our proposal allows incorporating user preferences when analysing the anytime behaviour of an algorithm. Moreover, as shown in this paper, evaluating the anytime behaviour of an algorithm in terms of the hypervolume allows applying automatic algorithm configuration methods to find parameter settings of an algorithm that optimise the trade-off between quality and time.Recent advances in automatic configuration of algorithms (also called offline parameter tuning) have shown that such methods can save a significant amount of human effort and improve the performance of optimisation algorithms, when designing and evaluating new algorithms and when tuning existing algorithms to specific problems (Birattari, 2009; Bartz-Beielstein, 2006; Hutter, Hoos, Leyton-Brown, & Stützle, 2009; Eiben & Smit, 2011; Hutter, Hoos, & Leyton-Brown, 2011; Hoos, 2012). Our proposal here is to combine automatic configuration with the use of the hypervolume as a surrogate measure of anytime behaviour in order to enable the automatic configuration of algorithms in terms of anytime behaviour.In the scenario described above, where the algorithm does not know its termination criterion in advance, techniques such as parameter adaptation are often applied to improve the anytime behaviour of the algorithm (Eiben, Michalewicz, Schoenauer, & Smith, 2007; Aine, Kumar, & Chakrabarti, 2009; Stützle et al., 2012). However, designing such parameter adaptation strategies is an arduous task, and they usually add new parameters to the algorithm that need to be tuned. The method proposed in this paper will help algorithm designers to compare and fine-tune such parameter adaptation strategies to find the settings that improve the anytime behaviour of the algorithm on the problem at hand. Indeed, the first case study reported here derives from our own efforts on designing parameter adaptation strategies for ant colony optimisation algorithms. This experience motivated us to develop the method proposed here, since the classical trial-and-error approach for designing such strategies proved extremely time-consuming.The second case study reported here deals with a different scenario, in particular, a general purpose black-box solver (SCIP (Achterberg, 2009)) with a large number of parameters. The default parameter settings of such solvers are tuned for solving problem instances to optimality as fast as possible. However, in some practical scenarios, users may not want to wait until a problem instance is solved to optimality, and may decide to stop the solver at an arbitrary time. Using our method for fine-tuning the parameters of the solver with respect to anytime behaviour, users can improve the quality of the solutions found when the solver is stopped before reaching optimality, without knowing in advance the particular termination criterion.The outline of the paper is as follows. Section 2 provides a background on automatic algorithm configuration, summarises the state of the art and describes the automatic configuration method (irace) used throughout this paper. Section 3 introduces the two classical views of the analysis of anytime algorithms and the less-explored multi-objective view. In Section 4, we describe our proposal in detail. We explain the benefits of using the hypervolume to evaluate the anytime behaviour of an algorithm in the context of an automatic configuration method. We discuss the choice of reference point and how to combine irace with the hypervolume measure. An additional section summarises related work and highlights the differences with our proposed approach. Section 5 describes our first case study, where we apply this proposal to the design of parameter adaptation strategies for MMAS. Section 6 discusses how our proposal enables a decision maker to incorporate preferences regarding the anytime behaviour of an algorithm to the automatic configuration procedure. A second case study is considered in Section 7, where we tune the anytime behaviour of SCIP. Finally, Section 8 provides a summary of our results and discusses possible extensions of the present work.This section is a brief introduction to automatic algorithm configuration. We define the algorithm configuration problem, give an overview on the state of the art of automatic configuration methods, and describe irace, the automatic configuration method used throughout this paper. A more detailed and formal introduction is available from the literature referenced here and in the extended version of the paper (López-Ibáñez & Stützle, 2012a).Most algorithms for computationally hard optimisation problems have a number of parameters that need to be set. As an example, ACO algorithms (Dorigo & Stützle, 2004) often require the user to specify not only numerical parameters like the evaporation factor and the number of ants, but also components like the type of heuristic information and update method. Another example is mixed-integer programming solvers, such as SCIP (Achterberg, 2009), which often have a large number of configurable parameters affecting the main algorithm used internally, e.g., selecting among different branching strategies. The process of designing complex algorithms from a framework of algorithm components can be seen as an algorithm configuration problem (KhudaBukhsh, Xu, Hoos, & Leyton-Brown, 2009; Montes de Oca, Stützle, Birattari, & Dorigo, 2009; López-Ibáñez & Stützle, 2012c).Given a parametrised algorithm, where each parameter may take different values (settings), a configuration of the algorithm is a unique assignment of values to parameters. When considering a problem to be solved by this parametrised algorithm, the goal of automatic configuration is to find the configuration that minimises a particular cost function over the set of possible instances of the problem. The cost function assigns a value to each configuration when applied to a single problem instance. In the case of stochastic algorithms, this cost measure is a random variable. Since most algorithms and problems of practical interest are sufficiently complex to preclude an analytical approach, the configuration of such algorithms follows an experimental approach (Birattari, 2009; Bartz-Beielstein, 2006).The traditional approach to algorithm configuration consists of ad hoc experiments testing relatively few configurations. The use of experimental design techniques (Coy, Golden, Runger, & Wasil, 2001; Adenso-Díaz & Laguna, 2006) began a trend in which the task of finding the most promising configurations to be tested is performed automatically. The natural evolution of this trend has been to tackle algorithm configuration as an optimisation problem (Nannen & Eiben, 2006; Ansótegui, Sellmann, & Tierney, 2009; Hutter et al., 2009; Bartz-Beielstein, 2006; Bartz-Beielstein, Lasarczyk, & Preuss, 2010; Hutter et al., 2011). It is becoming widely accepted that automatic configuration methods may save substantial human effort during the empirical analysis and design of optimisation algorithms, and, at the same time, lead to better algorithms (Hoos, 2012; Eiben & Smit, 2011; Bartz-Beielstein, 2006; Birattari, 2009).Iterated F-race (I/F-Race) (Balaprakash, Birattari, & Stützle, 2007; Birattari, Yuan, Balaprakash, & Stützle, 2010) is a method for automatic configuration that consists of three steps: (1) sampling new configurations according to a probability distribution, (2) selecting the best configurations from the newly sampled ones by means of F-Race, and (3) updating the probability distribution in order to bias the sampling towards the best configurations. These three steps are repeated until a termination criterion is met, usually a predefined budget of runs of the algorithm being tuned. F-Race (Birattari, Stützle, Paquete, & Varrentrapp, 2002) is a racing procedure (Maron & Moore, 1997) for the selection of the best among a given set of algorithm configurations, by means of the non-parametric Friedman’s two-way analysis of variance by ranks, and its associated post-hoc test (Conover, 1999).The irace software (López-Ibáñez, Dubois-Lacoste, & Stützle, 2011) implements a general iterated racing procedure, which includes I/F-Race as a special case. There are some notable differences between irace and the original description of I/F-Race, such as the use of truncated normal distribution for sampling numerical parameters, a restart mechanism for avoiding premature convergence, and other features described in the irace documentation (López-Ibáñez et al., 2011). For an outline of the iterated racing algorithm, we refer the reader to the extended version of the paper (López-Ibáñez & Stützle, 2012a).This section is an introduction to the analysis of stochastic optimisation algorithms and, in particular, anytime algorithms. We describe the two classical views, and the lesser studied multi-objective view. The concepts of bivariate runtime distributions (RTD), performance profiles, and the two classical views are described in textbooks (Hoos & Stützle, 2005). The extended version of the paper provides a more detailed introduction to these concepts (López-Ibáñez & Stützle, 2012a).Dean and Boddy (1988) describe an anytime algorithm as one that, first, may be interrupted at any moment and return a solution and, second, it keeps steadily improving its solution until interrupted, eventually finding the optimal. Most metaheuristics and other optimisation algorithms satisfy this condition, and, hence, they are anytime optimisation algorithms. A concept of anytime behaviour more useful in the context of metaheuristics was introduced by Zilberstein (1996), who highlights that algorithms with good anytime behaviour return as high-quality solutions as possible at any moment of their execution. A single run of an anytime algorithm generates a sequence of solutions that are increasingly better approximations of the optimal solution. Hence, in the context of anytime algorithms, an algorithm run is often described as a performance profile:Definition 1Performance profile (Zilberstein, 1996)Let us consider a single run r of an anytime optimisation algorithm A on a problem instanceπ, and record the computational effort (ti, measured, for example as CPU-time in seconds) and the solution quality (qi, measured, for example, as relative percentage deviation from the optimal solution quality), whenever a new best-so-far solution is found during the run of the algorithm. The setPr={(t1,q1),(t2,q2),…c}is called the performance profile of run r, where(ti,qi)are sampled with probabilityrtd(t,q), andti<tj∧qi>qj,∀i<j, wherertd(t,q)is called the bivariate runtime distribution (RTD) (Hoos & Stützle, 2005).Fig. 1gives an example of performance profiles for three runsA,B, and C. The above definition of performance profile does not favour quality over time, or viceversa. It would be equivalent to plot quality or time on either axis. Nonetheless, performance profile plots traditionally place time on the x-axis and quality on the y-axis, and we follow this custom here. A problem arises, however, when one wants to aggregate the information from several performance profiles in order to analyse the behaviour of an algorithm.There are two classical views on how to summarise performance profiles over multiple runs (Hoos & Stützle, 2005). The solution-quality-over-time (SQT) view examines solution quality distributions (SQDs) over fixed run-times. However, instead of plotting the SQDs, it is far more common to aggregate the performance profiles over fixed run-times and examine SQT curves. This SQT view is the most popular in combinatorial optimisation, where it is common to compare the anytime behaviour of optimisation algorithm by visually inspecting mean SQT curves (Hoos & Stützle, 2005; Wah & Chen, 2000; Loudni & Boizumault, 2008; Stützle et al., 2012).A second classical view is to aggregate performance profiles over fixed quality targets. Although plots of mean time over quality are possible (Hoos & Stützle, 2005), the most common approach is to examine the qualified run time distribution (QRTD), which is the probability of attaining fixed quality-targets over time. In an analysis based on QRTDs one can state that an algorithm is faster by some factor than another at reaching a particular quality target.A third alternative is to not favour either of the classical views, but instead use techniques from multi-objective optimisation (den Besten, 2004; Chiarandini, 2005). Thus, we do not aggregate the performance profiles over fixed run-times or fixed quality-targets. Instead, we describe the performance profiles generated by an optimisation algorithm as random nondominated sets. First, let us introduce some basic definitions borrowed from multi-objective optimisation, but adapted to the analysis of RTDs.Definition 2Weak dominanceGiven two vectors(t,q),(t′,q′)∈R0×R0, we say that(t,q)weakly dominates(t′,q′), and denote it by(t,q)⩽(t′,q′)ifft⩽t′∧q⩽q′.A setX={(t1,q1),(t2,q2),…c}is called nondominated iff∄(ti,qi),(tj,qj)∈X,i≠jsuch that(ti,qi)⩽(tj,qj).According to the definition of performance profile above (Definition 1), the elements of a performance profile, must also be mutually nondominated. Therefore, we can analyse performance profiles as nondominated sets, using the same techniques as in multi-objective optimisation.The multi-objective view considers the bivariate RTD without aggregation, which is a well-known approach to the analysis of optimisation algorithms (Hoos & Stützle, 2005), but much less studied due to the inherent difficulty of analysing a bivariate distribution. Chiarandini (Chiarandini, 2005) considered such a view for analysing the anytime behaviour of several metaheuristics for graph colouring, concretely using the attainment function (Grunert da Fonseca & Fonseca, 2010).1For brevity, we do not explain the attainment function approach here and its application to the analysis of anytime algorithms. A complete description is provided in the extended version of the paper (López-Ibáñez & Stützle, 2012a).1For the purposes of automatic configuration, the available Kolmogorov-Smirnov test based on the attainment function (Knowles, Thiele, & Zitzler, 2006) is not enough, and graphical exploration methods (Knowles et al., 2006; Knowles, 2005; López-Ibáñez, Paquete, & Stützle, 2010) require substantial human interaction.As mentioned above, directly using the attainment function as the basis of an automatic configuration tool for anytime algorithms seems difficult. Instead, we identify the hypervolume measure as the best available choice for this task. The main reasons are its high discriminatory power, being Pareto-compliant, and the possibility of incorporating user preferences into the automatic configuration process. There is also substantial and ongoing research on the theoretical properties of the hypervolume (Auger, Bader, Brockhoff, & Zitzler, 2012).As a first step, let us define the classical Pareto-dominance relation on performance profiles:Definition 4Better in terms of Pareto-optimality,◁Given two performance profilesPiandPj, which are the result of running two algorithmsAiandAjon the same problem instanceπ, we say thatPiis better, in terms of Pareto-optimality, thanPj(Pi◁Pj)iffPi≠Pj, and∀(tj,qj)∈Pj,∃(ti,qi)∈Pi, such that(ti,qi)⩽(tj,qj).It is often the case, however, that neither performance profile is better than the other, i.e., they are incomparable. These relations are independent of the classical views of fixed-quality (first view) or fixed-runtimes (second view) as described above, and only make sense in the third view that considers both quality and runtime in terms of Pareto-optimality.In the following, we assume that, without any a priori information about the actual termination criterion of the algorithm or the preferred trade-off between solution-quality and computation time, a performance profile that is better than another in terms of Pareto-optimality is also better in terms of anytime behaviour, in the sense that the former is always preferred to the latter.In order to apply an automatic configuration tool from the literature for improving the performance profiles produced by an algorithm, we would desire a unary quality measure that unequivocally indicates whether a performance profile is better than another. Unfortunately, a well-known result from multi-objective optimisation states that no unary quality measure (or finite combination thereof) can indicate whether a performance profile is better, as defined above, than another (Zitzler et al., 2003). The most powerful of the unary quality indicators can at most indicate that a performance profile is not worse than (better than or incomparable to) another. The hypervolume measure (Zitzler & Thiele, 1999) is the only unary quality measure known to have such discriminatory power (Zitzler et al., 2003).The hypervolume can be defined as the measure of the region that is simultaneously weakly dominated by any point in a nondominated set and bounded above by a reference point that is strictly dominated by all points in the set. In the context of performance profiles of single-objective optimisation algorithms, this region is the area contained within the orthogonal polygon defined by the elements of a performance profile P and an arbitrary reference point(tr,qr), such that(ti,qi)<(tr,qr),∀(ti,qi)∈P.In this paper, we only consider single-objective optimisation algorithms. Nonetheless, it is trivial to extend the above discussion to multi-objective algorithms, where there is more than one measure of solution quality. In fact, we have applied the method proposed here to automatically improve the anytime behaviour of multi-objective evolutionary algorithms (Radulescu, López-Ibáñez, & Stützle, 2013).In the context of anytime algorithms, the choice of reference point is application specific. The bivariate runtime distribution is defined without any limits on how bad the solution quality might be or how long it may take to generate any solution. In practice, however, large deviations from the optimal quality may be of little interest (no matter how fast they can be generated) and algorithms need to be stopped at some point (cut-off time). These limitations are not specific to any of the three views discussed above. A default approach is to consider a cut-off quality that corresponds to the worst solution quality found by any run of the algorithms under analysis, and a cut-off time that is slightly larger than the maximum time that would be reasonable for a single run of the algorithm. The reference point would then be defined as a factor larger than the cut-off quality and cut-off time.How much larger this factor should be is an open question in multi-objective optimisation. There are some theoretical results on how the choice of reference point affects the distribution of elements within a nondominated set that maximises the hypervolume (Auger et al., 2012). However, it is not clear how these results extend to the relations between nondominated sets. In any case, the only effect of the reference point is to bias the preference between incomparable performance profiles. In that sense, our suggestion (and the usual practice in multi-objective optimisation) is to define the reference point in a consistent manner and bias this preference by other means, such as the weighted hypervolume (Section 6).The use of the hypervolume to compare performance profiles in terms of Pareto-optimality has the additional benefit of providing a unary scalar measure to evaluate anytime behaviour. Integrating such a measure in most automatic configuration methods should be straightforward. Here, we discuss the practical aspects of the integration of the hypervolume in irace for improving the anytime behaviour of single-objective optimisation algorithms.Our procedure requires to specify a maximum cut-off time for the algorithm being tuned by irace. As discussed above, this cut-off time is necessary because anytime algorithms may in principle run forever. The cut-off time could be dynamic or different for each run, however, for the sake of simplicity, we do not explore these possibilities here. Each run of the algorithm must produce a performance profile as defined in Definition 1.Within a single race in irace, a set of algorithm configurations are evaluated on a sequence of training instances. After evaluating an instance, some configurations may be discarded. LetΘπdenote the configurations that have not been discarded before evaluating instanceπ. LetPθ,πdenote the performance profile generated by running configurationθon instanceπ. First, we normalise the performance profilesPθ,π, for eachθ∈Θπto the range [0.0,0.9]. Thus, after normalisation we obtain for eachθa new performance profile:Pθ,π′={ti′,qi′|∀(ti,qi)∈Pθ,π}whereti′=0.9·(ti-tmin)/(tmax-tmin)qi′=0.9·(qi-qmin)/(qmax-qmin)wheretminis usually zero,tmaxis the cut-off time,qmin=min{qi|∀(ti,qi)∈Pθ,π,∀θ∈Θπ}andqmaxis defined similarly for the maximum solution quality found after running all configurations inΘπon instanceπ.Finally, we compute the hypervolumehv(θ,π)of eachPθ,π′using (1.0,1.0) as the reference point. In our proposal, thishv(θ,π)value becomes the cost measure used by irace.Neither quality values, nor hypervolume values from different instances are directly compared because normalisation is done within each instanceπ, as defined above, and the application of the F-test within irace transforms the hypervolume values into ranks per instance. Hence, quality values or hypervolume values may have different ranges on each instance without introducing a bias. This approach also allows for instance-dependent cut-off times, although we do not explore this possibility in this paper.There is substantial work on automatic configuration for decision problems and single-objective optimisation problems. We refer to recent overviews (Hoos, 2012; Eiben & Smit, 2011) and books (Bartz-Beielstein, 2006; Birattari, 2009) for a complete bibliography. By comparison, there are relatively few works on tuning multi-objective optimisation algorithms. Wessing, Beume, Rudolph, and Naujoks (2010) automatically tuned the variation operator of a multi-objective evolutionary algorithm applied to a single problem instance. Simultaneously, López-Ibáñez and Stützle (2010, 2012c) automatically instantiated new designs of multi-objective ant colony optimisation (MOACO) algorithms for the bi-objective travelling salesman problem from a framework of MOACO algorithmic components. More recently, Dubois-Lacoste, López-Ibáñez, and Stützle (2011) applied this latter approach to outperform the state of the art in several bi-objective permutation flow-shop problems. These works share with our proposal the use of unary quality measures, such as the hypervolume, as the cost function used by the automatic configuration method.On the other hand, our proposal should not be confused with parameter tuning as a multi-objective problem (Dréo, 2009), where the aim is to produce a set of parameter configurations that are mutually nondominated with respect to multiple criteria. In this paper, our aim is to produce a single parameter configuration that generates an anytime behaviour that is as good as possible.There have been some recent attempts at tackling the problem of tuning anytime algorithms. As mentioned above, Chiarandini (2005) used the attainment function to analyse the anytime behaviour of several metaheuristics for graph colouring. However, it is far from obvious how to effectively use the attainment function in an automatic configuration method. The proposal closest to ours is by den Besten (2004), who combined racing and a performance measure based on the binary∊-indicator. The use of a binary measure involves computing a matrix of∊-measure values, comparing each alternative with the rest, and transforming it into ranks. More recently, Branke and Elomari (2011) combined a meta-level evolutionary algorithm and an ad hoc ranking procedure for tuning the mutation rate of a lower-level algorithm for multiple termination criteria in a single tuner run. Their ranking method is not based on any multi-objective quality measure. Instead, it ranks each configuration with respect to the number of discrete time steps in which the configuration was better than other configurations. In that sense, it is an example of the classical fixed-runtimes view (what we call first view above).Many anytime algorithms use parameter adaptation strategies (Eiben et al., 2007; Aine et al., 2009), that is, the variation of parameter settings while solving a problem instance, to adapt the parameters to different phases of the search, and to balance exploration of the search space and exploitation of the best solutions found.Designing and comparing parameter adaptation strategies is, however, an arduous and complex task. Traditionally, the analysis is performed in terms of one (or both) classical views, that is, either measuring solution quality over fixed-runtimes (Aine et al., 2009), or runtime (CPU-time or function evaluations) over fixed quality-targets (Auger & Hansen, 2005).In previous work (Stützle et al., 2012; Maur, López-Ibáñez, & Stützle, 2010), we studied parameter adaptation strategies for ant colony optimisation (ACO) algorithms using the classical solution quality over fixed-runtimes view. In particular, we studied the anytime behaviour of MAX-MIN Ant System (MMAS) on the travelling salesman problem (TSP) by experimenting with various static parameter settings and parameter variation strategies. As a result, we identified parameter configurations that are significantly better in terms of anytime behaviour than the default settings of MMAS (Maur et al., 2010; Stützle et al., 2012). Our analysis relied on visually comparing the mean SQT curves of various strategies that were deemed interesting. Needless to say, this was a human intensive task that required many iterations of experimentation and analysis. We roughly estimate that the overall effort for obtaining the best configurations was close to one person-year.This effort could have been significantly reduced by using an automatic algorithm configuration tool for improving the anytime behaviour. This case study was our main motivation for developing the method proposed in this paper. In this section, we describe the case study in detail, we examine the setup required for applying automatic algorithm configuration, and we compare the parameter adaptation strategies identified as the best by the automatic configuration procedure versus the ones identified in our previous work.MMAS is an ACO algorithm that incorporates an aggressive pheromone update procedure and mechanisms to avoid search stagnation. When applying MMAS to the TSP, each ant starts at a randomly chosen initial city, and constructs a tour by randomly choosing at each step the city to visit next according to a probability defined by pheromone trails and heuristic information. In particular, the probability that ant k chooses a successor city j when being at city i is given by(1)pij=[τij]α·[ηij]β∑h∈Nk[τih]α·[ηih]βifj∈Nk,(otherwise,pij=0)whereτijis the pheromone trail strength associated to edge(i,j),ηijis the corresponding heuristic information;αandβare two parameters that influence the weight given to pheromone and heuristic information, respectively;Nkis the feasible neighbourhood, that is, a candidate list of cities not yet visited in the partial tour of ant k.Following previous work (Stützle, 1997), we also incorporate the pseudo-random action choice rule of ACS (Dorigo & Gambardella, 1997), which allows for a greedier solution construction. With a probabilityq0, an ant chooses next a cityj∈Nksuch thatj=argmaxh∈Nk[τih]α·[ηih]β; otherwise, the ant performs the probabilistic selection based on Eq. (1). A value ofq0=0reverts back to the original MMAS.The pheromone update of MMAS updates all pheromone trails as(2)τij←maxτmin,minτmax,(1-ρ)·τij+Δτijbest,whereρ,0<ρ⩽1, is a parameter called evaporation rate andΔτijbest=1/f(sbest)if edge(i,j)∈sbest, (Δτijbest=0, otherwise), wheref(s)is the tour length of solution s, andsbestis either the iteration-best solution, the best-so-far solution or the best solution since a re-initialisation of the pheromone trails (restart-best). In MMAS, these solutions are chosen alternately (Stützle & Hoos, 2000).Finally, solutions constructed by the ants may be further improved by the application of a local search algorithm. In this paper, we will use MMAS with 2-opt local search, as was done in previous work (Maur et al., 2010; Stützle et al., 2012).Following our previous work (Stützle et al., 2012), we focus on two basic schemes for parameter variation in MMAS, which we call henceforth delta and switch strategies. During a single run of MMAS, the variation strategy called delta applied, for example, to parameterβincreases the value ofβat each iteration of the algorithm by a certain amountΔβ, starting from the valueβstartand stopping at the valueβend. Ifβstart>βend, then the value ofβis decreased at each iteration byΔβinstead of increased. Conversely, the variation strategy called switch changes, at iterationβswitch, the value of parameterβfrom the valueβstartto the valueβend. An additional parameterβvarcontrols the variation strategy, which is either delta, switch or none, where none means that the parameter value ofβstays constant throughout the run of the algorithm. As a result, we add to MMAS five additional parameters for varyingβ:βstart,βend,Δβ,βswitchandβvar.We apply the same parameter variation strategies to parametersβ,ρ, the number of ants (m), andq0. Hence, we add five additional static parameters for each parameter that is dynamically varied. Table 2 describes the domains of all parameters, and Table 1describes the default values (Stützle & Hoos, 2000).We consider random uniformly generated instances of the symmetric TSP with 3000 cities (Johnson, McGeoch, Rego, & Glover, 2001). Our instances are available in the supplementary material page (López-Ibáñez & Stützle, 2012b). We generate 50 training (tuning) instances and 50 test instances.We apply our proposed method for the automatic configuration of the anytime behaviour (Section 4.2). The automatic configuration tool is the implementation of I/F-Race provided by the irace software package (López-Ibáñez et al., 2011). As explained above, we incorporate the hypervolume measure to irace in order to evaluate the anytime behaviour of a single run of MMAS. We use a publicly available implementation of the hypervolume measure (Fonseca, Paquete, & López-Ibáñez, 2006), and the MMAS implementation is based on the ACOTSP software (Stützle, 2002). All experiments are run on Intel Xeon E5410 CPUs (2.33GHz, 2×6MB L2 cache) running Cluster Rocks Linux version 6/CentOS 6.3, 64 bits. Each individual run of the algorithms being tuned uses just one core.In a first step, we tune separately the variation strategy of one dynamic parameter at a time, while other parameters are fixed to their default values as given in Table 1. That is, we perform one run of irace for each parameter{m,β,q0,ρ}. For example, in the run of irace that tunes the variation strategy ofβ, the parameters tuned areβvar,β,Δβ,βswitch,βstart,βend, whereas the other parameters (m,q0,ρ) are fixed to their default values (Table 1) and their variation strategies (paramvar) are set to none, and, hence, their corresponding variation parameters (Δparam,paramswitch,paramstart,paramend) are not considered (see Table 2). We give each run of irace a budget of 1000 runs of MMAS. Each run of MMAS is stopped afterTimeCPUseconds (Table 1).In a second step, we automatically configure all parameter variation strategies at the same time, that is, we configure 24 parameters instead of six. Since the parameter space is much larger now, we assign a larger tuning budget to this run of irace, specifically 10000 runs of MMAS.After each run of irace finishes, we apply the resulting parameter configurations (Table 4) to the test instances. In addition, we also run, on the test instances, the default parameter configuration of MMAS (Table 1) without any variation strategy and several variation strategies previously found by manual ad hoc experimentation (Table 3) (Stützle et al., 2012). We present these results in the following sections.Our goal is to improve the anytime behaviour of MMAS over the whole set of test instances. Hence, we analyse the overall results by plotting the average solution quality over time (SQT) for all test instances at once. However, as explained above, our proposed approach does not rely on these SQT curves for improving the anytime behaviour, and it does not favour solution-quality over time or viceversa. We could also visualise our results in terms of qualified runtime distributions (Hoos & Stützle, 2005), or in terms of attainment surfaces (Grunert da Fonseca & Fonseca, 2010; Chiarandini, 2005). We chose SQT curves as one of the traditional means of visualising the anytime behaviour of an algorithm, and the most popular view in the literature on combinatorial optimisation algorithm (Hoos & Stützle, 2005).For each algorithm configuration, we have the best solution quality foundfriton run r on instance i at time t. We compute the relative percentage deviation (RPD) from the optimal solution for each instance asRPDrit=100·frit/fiopt, wherefioptis the optimal tour length of instance i. Then, we compute the mean RPD over all 50 instances and over all 15 independent runs of each algorithm asRPDt=150·15·∑i=150∑r=115RPDrit. Each line in the plots in Fig. 2corresponds to theRPDtof one algorithm configuration, that is, aggregating solution-quality over time.In the case of the hypervolume computation, we do not aggregate over time, but compute the hypervolume of the (nondominated) performance profile of each run on each instance by normalising both time and solution quality to the interval [0.0,0.9] and using (1.0,1.0) as the reference point. Then, for each algorithm configuration we compute its mean hypervolume over all its runs on all test instances, and we give this value in the legend of each plot.The first important observation is how a larger hypervolume value matches a better anytime behaviour. The four plots in Fig. 2 show a large improvement in the anytime behaviour of the manually tuned configurations with respect to the default configuration of MMAS. Nonetheless, the automatically found configurations are able to match, and in most cases surpass, the manually tuned configurations in terms of hypervolume, despite the fact that the manually tuned configurations were found by extensive experimentation under the guidance of human expertise.Fig. 3(a) compares the configuration obtained after automatically configuring all parameter variation strategies at once versus the best configurations obtained after automatically configuring the variation strategy of one parameter at a time. In our previous study, the manual tuning and analysis of all parameter strategies at once was ruled out as infeasible, given the extremely large number of potential configurations and interactions among different parameters. Here, we see that automatically configuring all parameters at once leads to an additional improvement in anytime behaviour.Figs. 2(a–d) and 3(a) show the mean hypervolume over all runs and all test instances. To assess whether the observed differences are statistically significant, we perform a statistical analysis of the results over the whole set of test instances. The analysis is based on the Friedman test for analysing non-parametric unreplicated complete block designs, and its associated post-test for multiple comparisons (Conover, 1999). First, we calculate the mean hypervolume of the 15 runs of each algorithm for each instance. Then, we perform a Friedman test using the instances as the blocking factor, and the different configurations of MMAS as the treatment factor. The null hypothesis is that the configurations have identical effect on the ranking according to the hypervolume within each instance. If the Friedman test rejects the null hypothesis given a significance level ofα=0.05, we proceed to calculate the minimum difference between the sum of ranks of two configurations that is statistically significant (ΔRα). In this manner, we identify which configurations are significantly different from the best ranked one, i.e., the one with the lowest sum of ranks.Table 5summarises the results of the statistical analysis. It shows the value ofΔRαforα=0.05, the different configurations of MMAS sorted by increasing sum of ranks, and the difference between the sum of ranks of each configuration and the best configuration (ΔR). For each parameter considered, the ranking shown in Table 5 always ranks higher the configurations found automatically than their counterparts found by ad hoc experimentation (auto vs. manual, respectively). More importantly, it shows that the best ranked configuration is the one that automatically configured all parameters at once, and that the difference in ranks between this configuration and the rest is statistically significant.Here, we show that the use of the hypervolume as the tuning criterion is the key factor for improving the anytime behaviour. Fig. 3(b) shows four configurations of MMAS: the default configuration (default); the one resulting from automatically tuning all variation parameters (auto var ALL); a configuration obtained by tuning all variation parameters with respect to final quality, that is, the solution quality obtained at 500 seconds (auto var final); and a configuration obtained by tuning the classical MMAS parameters, without any variation, with respect to final quality (auto fix final). The plot shows that, independently of whether MMAS uses parameter variation or not, the results not tuned with respect to the hypervolume have worse anytime behaviour.A possible concern of tuning for anytime behaviour is a significant loss of final quality. Hence, we examine the final quality achieved by these four variants of MMAS in Fig. 4. According to the boxplots, there is an important improvement in the final quality achieved in comparison with the default configuration of MMAS, even for the configuration tuned for anytime behaviour. The boxplot does not show a large difference between the three automatically configured variants. Nonetheless, the Friedman test indicates that the final quality obtained by the variant tuned for anytime behaviour is statistically worse than the variants tuned for final quality (Table 6). In order to assess the loss of final quality, we compute the 95% confidence interval on the mean difference in final quality between the configuration tuned for anytime behaviour and the best ranked configuration, which is [0.0244,0.0480], measured in RPD.2The confidence interval is computed using the Welch’s t statistic for two paired samples, which assumes that the samples follow a normal distribution. Nonetheless, for large sample sizes, as used here, the method is robust against deviations from normality.2Although the loss in final quality when tuning for anytime behaviour is small in this case, an anytime algorithm should aim to match the best possible final quality in the ideal case.The use of the hypervolume for automatic tuning of anytime algorithms has an additional advantage compared to other unary measures, that is, the possibility of specifying the decision-maker’s preferences. A recent proposal extends the hypervolume indicator by a weight function over the objective space (Zitzler, Brockhoff, & Thiele, 2007; Auger et al., 2009). A weight function that assigns a larger value to a certain region of the objective space will bias the hypervolume indicator to favour nondominated sets that dominate that region. We show here that this formulation can straightforwardly be used to introduce a bias in the anytime behaviour produced by automatic configuration.As an example, let us assume that the decision maker’s preference is to obtain as good final solution quality as possible, while still giving some minor importance to achieving a good anytime behaviour. In other words, the decision maker prefers configurations that generate solution-quality curves that are better towards minimising the solution quality (in our case, the second objective). Zitzler et al. (2007) suggest to model this preference by considering the following weight function (adapted here to minimisation):(3)wqual(z)=e20·(1-z2)/e20wherez=(z1,z2)∈Zis an objective vector, withz1representing time andz2representing solution quality, andZ=[0,1]×[0,1]represents the normalised bi-objective space of time×quality.The weighted hypervolume is computed as the integral of the weight function over the region dominated by a set of nondominated points and bounded above by a reference point. To give a rough idea of this integral when using the weighted functionwqual, Fig. 5(b) shows the value of the weighted hypervolume for each individual vector in the normalised objective space Z and with reference point (1,1). The plot shows that vectors with very small values ofz2are assigned a high hypervolume, but vectors with values ofz2larger than 0.2 are assigned a hypervolume close to zero. By comparison, Fig. 5(a) shows the value of the non-weighted hypervolume, which is symmetric around the diagonal, that is, without a preference for either objective.As shown in Fig. 5(b), when using the weighted functionwqual, the gradient of the hypervolume values is very steep and most of the objective space has a hypervolume close to zero. We can make the gradient gentler by weighting also thez1component (corresponding to time), but then we have to increase the exponent associated toz2in order to keep a strong preference for low solution quality. This is done with the following weight function:(4)wxqual=e10·z1/e10+e100·(1-z2)/e100The weighted hypervolume using this weight function for each individual vector in the objective space Z is shown in Fig. 5(c). In this case, there is a gentler gradient of the hypervolume value than in Fig. 5(b). Moreover, the value of the hypervolume increases exponentially in the direction of decreasingz2(solution quality), while it stays roughly constant alongz1(except for very high values ofz1).We illustrate the differences between the original hypervolume and the two weighted variants above with an example. Fig. 6shows five performance profiles (not aggregated over fixed run-time or over fixed quality-targets) in the normalised objective space Z. The plot shows the regionz2∈[0.0,0.15], where we can see that the performance profiles are ordered according to the final quality achieved, with profile a being the best and profile e being the worst. The legend provides three numbers for each profile, which correspond to evaluating the profile with the classical hypervolume, the hypervolume weighted bywqualand the hypervolume weighted bywxqual, respectively. Table 7gives the profiles in increasing order of preference according to each measure.In this example, the classical hypervolume ranks profile a, which is the profile with the best final quality, worse than other three profiles. The weighted hypervolume functions increase the preference for profile a, and our proposed variantwxqualgives it the highest rank.Next, we test the effect of these two weighted hypervolume functions on the automatic configuration procedure. In particular, we carry out additional runs of irace using the weighted hypervolume variants described above, i.e.,wqual(Eq. (3)) andwxqual(Eq. (4)). We run irace with the same setup as for tuning all parameter variations at once in Section 5.2, in particular, with a budget of 10000 runs of ACOTSP. These additional tuning runs produce two new configurations of MMAS, which we ran 25 times with different random seed on each test instance.Fig. 7(a) plots the mean RPD over all runs of the resulting four configurations of MMAS: the default configuration (default); the one resulting from automatically tuning all variation parameters at once using the classical hypervolume (auto var ALL); the configuration obtained with the same tuning setup but using the weighted hypervolume withwqual(whv qual); and the configuration obtained using the weighted hypervolume withwxqual(whv xqual). In addition, the legend provides three numbers for each profile, which correspond to evaluating the results with the classical hypervolume, the hypervolume weighted bywqualand the hypervolume weighted bywxqual, each of them averaged over all runs.Interestingly, the values reported in the legend of Fig. 7(a) indicate that the configuration tuned usingwxqualas the anytime criterion obtains a better hypervolume weighted bywqualthan the configuration tuned usingwqualas the anytime criterion. Taking into account Fig. 5(b and c), we can observe that both weight functions are strongly correlated and also thatwqualis “flatter” thanwxqual, that is, there are more plateau regions with almost the same value for different points. Our conjecture is that the strong correlation makes possible to tune for one weight function and maximise the other. At the same time, the relative flatness ofwqualmakes it a harder optimisation criterion for tuning thanwxqual. Additional experiments (López-Ibáñez & Stützle, 2012a) appear to confirm this conjecture.In terms of final quality, the two configurations tuned with the weight functions (wqualandwxqual) are slightly better than the one tuned with the classical hypervolume, as indicated by the boxplots given in Fig. 7(b). Moreover, the configurations tuned with the weight functions obtain the lowest final quality in most instances. In fact, according to the Friedman test, these configurations are significantly better than configurations obtained by tuning for the classical hypervolume and for final quality (Table 8). The main conclusion of these experiments is that the weighted hypervolume allows us to set preferences on the trade-off between quality and time. For example, the weighted functionwxqualimposes a strong preference for good final quality.In this second scenario, we apply our proposed approach to a very different problem with a large number of parameters. In particular, we tune 207 parameters of SCIP (Achterberg, 2009), a mixed integer programming (MIP) solver. The number of parameters is too large to be detailed here, but details can be found in the supplementary page (López-Ibáñez & Stützle, 2012b).The benchmark set is composed of 2000 MIP-encoded instances (200 goods, 1000 bids) of the NP-hard winner determination problem for combinatorial auctions (Leyton-Brown, Pearson, & Shoham, 2000; Hutter et al., 2009). The benchmark set is split in two disjoint sets of 1000 instances each, one is used for training and the other for testing. In a combinatorial auction, bids are placed for subsets of goods. The goal in the winner determination problem is to find an assignment of goods to bids that maximises the total value of the winning bids.For our experiments here, we use SCIP version 2.0.2 linked with the linear programming solver SoPlex 1.5.0. We set the maximum memory limit of SCIP to 350MB. During our experiments, we discovered that some parameter configurations produced an incorrect behaviour of SCIP, and we assign those configurations the worst possible hypervolume. We give SCIP a time limit of 300 seconds, and we allow 5000 runs of SCIP for each run of irace. We carry out the tuning as before, that is, we combine irace with the hypervolume measure in order to improve the anytime behaviour of SCIP. We seed the automatic configuration procedure with the default configuration of SCIP.For the purposes of comparison, we perform two additional tuning runs with two different objectives: (1) minimising the runtime to find the optimal solution and (2) maximising the final objective value obtained after 300 seconds. Thus, we obtain two additional configurations of SCIP, which we label as auto time and auto quality, respectively. We use these configurations to asses the potential loss of either run time or final solution quality, when tuning for improving the anytime behaviour. Finally, we run all configurations of SCIP obtained from the various tuning setups plus the default configuration one time on each test instance.As a first step, we graphically examine the solution quality over time. For each configuration, we compute the mean RPD over the 1000 test instances at each time step. Next, we plot the mean RPD over time (Fig. 8). The legend gives the mean hypervolume value corresponding to each configuration of SCIP. The plot uses a logarithmic scale for the x-axis (time), since the largest differences appear on the first half of the computation time limit.The plot shows that the configuration tuned with the hypervolume (auto anytime) obtains a better anytime behaviour (and a higher hypervolume) than the rest. Moreover, both the configuration tuned for final quality and the one tuned for solving time show worse anytime behaviour (and lower hypervolume) than the default configuration of SCIP. The differences observed in the hypervolume values (and, hence, in the anytime behaviour) of each SCIP configuration are more evident in Fig. 9(a), which shows that the hypervolume values corresponding to auto anytime are much larger than those corresponding to the other configurations of SCIP.Improving the anytime behaviour does not necessarily mean that instances are solved faster to optimality. Fig. 9(b) shows the time required by each configuration to solve each of the 1000 test instances. The best configurations of SCIP according to this criterion are the default configuration and the configuration tuned specifically for this criterion (auto time). This result is not surprising, since this is the most popular evaluation criterion in mixed-integer programming, and, hence, we presume that SCIP is by default tuned for it.We also examine the potential loss of final quality. Fig. 9(c) shows the RPD from the optimal at the cut-off time of 300 seconds. All configurations solve most of the instances to optimality (or very close to it). However, the configuration tuned for anytime (auto anytime) is the one that diverges most often from near-optimality. Hence, there is some loss of final quality when tuning using the hypervolume.If we look at the solution quality up to a different cut-off time, the situation is certainly different. For example, if we consider solution quality up to 10 seconds (Fig. 9(d)), there is a large difference between the configurations. While the auto anytime configuration obtains an RPD value much lower than 10% in most cases, the RPD values of the default configuration are frequently larger than 10%.The observations above are further confirmed by statistical analysis. We carry out four independent Friedman tests (as described in Section 5.4.1), one for each evaluation criterion shown in Fig. 9. The results of the four tests are reported in Table 9. As expected, the best configuration in terms of hypervolume is the one tuned for that criterion (auto anytime), which is significantly better than the rest by a large margin. The auto anytime configuration is also the clear winner in terms of the solution quality obtained if stopped after 10seconds. Moreover, in terms of final quality, the differences between the strategies are not statistically significant. The difference in the sum of ranks between auto anytime and default is only 44.5.Finally, by using the weighted hypervolume as explained in Section 6, we are able to find a configuration of SCIP with good anytime behaviour and that ranks better than default according to final quality. However, the differences in ranks are still not statistically significant. Hence, for conciseness, we do not discuss the results of using the weighted hypervolume for tuning SCIP here, but we provide the results as supplementary material (López-Ibáñez & Stützle, 2012b). The results provided here are sufficient to conclude that the proposed method was able to find a configuration of SCIP that has better anytime behaviour than the default, without a significant loss of final quality.

@&#CONCLUSIONS@&#
In this paper, we have shown that the combination of irace and the hypervolume quality measure is effective at improving the anytime behaviour of optimisation algorithms. We have presented two representative and challenging case studies. The first case study compared the results obtained automatically against those obtained by a human expert for the task of designing parameter variation strategies that show good anytime behaviour. Our results show that the automatic configuration method is able to match the anytime behaviour obtained by the parameter variation strategies designed by a human expert. Moreover, the automatic method allows exploring a much larger design space, potentially leading to configurations with better anytime behaviour. These are expected results when using automatic configuration tools for tuning with a fixed termination criterion. However, this is the first time that such results have been obtained when automatically designing anytime algorithms. In a follow-up work, we have applied the approach proposed here to improve the anytime behaviour of a state of the art optimiser for black-box continuous optimisation (López-Ibáñez, Liao, & Stützle, 2012). Our results there show that even for such state-of-the-art optimisers, the default parameter settings are not well-suited for scenarios where the termination criterion is unknown in advance. Although the results presented here focus on single-objective optimisers, our approach is applicable to multi-objective optimisers as well. In another follow-up work, we have applied it to automatically configure the parameters of multi-objective evolutionary algorithms in order to improve their anytime behaviour (Radulescu et al., 2013).In the second case study presented here, we apply our approach to an off-the-shelf optimisation solver, with a very large number of parameters. In this case, the optimisation solver is already tuned to solve problems to optimality as fast as possible. However, we show that if stopped before reaching optimality, the results may be very poor. Our proposed approach helps to tune such solvers in order to be more robust in case of earlier termination, without specifying in advance when the algorithm could be terminated. Our results show that important improvements can be obtained, specially for very early termination, without sacrificing much of the final quality. Moreover, comparing the configurations that produce better anytime behaviour versus those that produce better final quality (or shorter time to optimality) may lead to improvements in the solvers themselves.The choice of the hypervolume measure also allows incorporating preference information into the automatic configuration process by means of the weighted hypervolume. We propose a weighted formulation that emphasises a good final quality but still takes into account the overall anytime behaviour of the algorithms. We show that, by adding such preferences, it is possible to effectively bias the configurations selected by the automatic configuration tool. This allows customising algorithms to very specific anytime scenarios, where an exact termination criterion is not known, but there is some a priori knowledge of what is expected.An open question is how to extend the results to longer termination criteria than the ones that are feasible to test during automatic configuration. A problem that may arise is that configurations produce good results up until the tested termination criterion, but the performance becomes unsatisfactory for longer runs. Woodruff, Ritzinger, and Oppen (2011) have studied how to dynamically set a termination criterion. Survival analysis techniques may help to estimate the behaviour of the algorithms for longer runtime (Gagliolo & Legrand, 2010). These techniques could be incorporated into our approach in order to dynamically adjust the maximum cut-off time while tuning the anytime behaviour.Finally, we are convinced that our approach contributes towards the final goal of designing algorithms that are more robust to different termination criteria and, hence, applicable to a wider range of scenarios, without sacrificing solution quality.