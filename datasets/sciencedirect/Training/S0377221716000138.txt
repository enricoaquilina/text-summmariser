@&#MAIN-TITLE@&#
Nonlinear stochastic programming–With a case study in continuous switching

@&#HIGHLIGHTS@&#
The value function of nonlinear stochastic optimization problems is continuous with respect to changing the measure.Various acceleration methods for numerical solutions are proposed.AC transmission switching is considered the first time in a stochastic context.

@&#KEYPHRASES@&#
Stochastic optimization,Nonlinear programming,Risk measures,Robust optimization,Wasserstein metrics,

@&#ABSTRACT@&#
The optimal solution, as well as the objective of stochastic programming problems vary with the underlying probability measure. This paper addresses stability with respect to the underlying probability measure and stability of the objective.The techniques presented are employed to make problems numerically tractable, which are formulated by involving numerous scenarios, or even by involving a continuous probability measure. The results justify clustering techniques, which significantly reduce computation times while guaranteeing a desired approximation quality.The second part of the paper highlights Newton’s method to solve the reduced stochastic recourse problems. The techniques presented exploit the particular structure of the recourse function of the stochastic optimization problem. The tools are finally demonstrated on a benchmark problem, which is taken from electrical power flows.graph of the transmission network consisting of buses B and transmission lines Lbuses i,k,…the set of transmission lines linking buses: (i, k) ∈ L if a transmission line connects i and kgenerators (PV bus)demand, or load bus (PQ bus)net power injected at bus i. The superscripts g indicates power generated at bus i, while d relates to demandnet reactive power injected at bus icomplex power (measured in watt)impedance (measured in ohm). Rikis the resistance of transmission asset linking the buses i and k, Xikthe reactanceadmittance (measured in siemens). Gikis the conductance, Bikthe susceptancevoltage magnitude at bus i (measured in volt)difference of voltage angles at buses i and jvoltage angle (phase) at bus i (measured in radian)imaginary unit,j2=−1

@&#INTRODUCTION@&#
Stochastic programming problems are often formulated as linear programs. A main reason for that is perhaps that linear programming historically emerged from stochastic optimization: the initial problem considered by Dantzig (1955) to develop the linear theory is indeed a stochastic optimization problem. Moreover efficient solvers are available, which can be employed to solve even large scale linear problems. For these reasons genuinely nonlinear problems are often linearized in order to solve the approximating, linear problem by using well established and accepted linear solution methods and techniques. However, important problems are known for which the linear approximation is irrelevant to the understanding of the initial problem (a prominent example in physics, which is often referred to in this context, is the explanation and description of a ship’s bow wave).This paper addresses general two-stage stochastic optimization problems involving a general, nonlinear objective, with nonlinear constraints on a general probability space (cf. Shapiro, Dentcheva, & Ruszczyński, 2009 for stochastic optimization). It investigates continuity with respect to the underlying probability measure and outlines implications of the particular structure of the problem on numerical solution techniques. A case study, taken from electrical engineering, is presented, for which it was realized recently that the simple, linearized problem does not lead to useful results. The problem in the case study thus has to be solved by accepting its nonlinear complexity.The general two-stage stochastic optimization problem is(1)miny∈YR(minz∈Z(y)c(y,ξ,z)),where c is a (cost) function and z ∈ Z is the wait-and-seedecision. The inner problem, minz ∈ Z(y)c(y, ξ, z), depends on the here-and-now decision y ∈ Y and the random variable ξ and thus is random itself. The convex risk measureRsummarizes the different outcomes of the random inner problem in a single real number. In the simplest case,R(·)=E(·)is the expectation so that problem (1) reads(2)miny∈YEξ(minz∈Z(y)c(y,ξ,z)).Optimal solutions and the objective in (1) depend on the probability measure for ξ. It follows from this observation already that it may not make sense to solve problem (1) with ultimate precision, if the probability measure is not known precisely. A reasonable accuracy goal depends on the knowledge of the measure of ξ. Indeed, the underlying probability measure is not known precisely in many situations. This particularly holds in the following three cases:(i)Often, the measure in (1) is not available explicitly. Instead, the empirical measure is employed, which is built from historically observed samples. The empirical measure is just an approximation of the true baseline measure, and one may not expect that the solution of problem (1) subject to the empirical measure is the optimal solution for the true baseline model, even if the sample size is large.Many models of economic relevance artificially involve possible outcomes (often called scenarios) ξ to model potential future behavior of the economy. As the future behavior will differ from the artificially chosen scenarios (i.e., with probability one) it is necessary to have a framework which justifies the use of scenarios.Numerical solution techniques typically replace the original probability measure in problem (1) by a discrete approximation. As for the empirical measure, the solution of (1) subject to the approximating measure is not more than an approximation.This raises the general question if there are useful conditions to guarantee that the solutions, obtained by numerical schemes, are relevant for the initial problem (1) at all?Different solution techniques for nonlinear stochastic optimization problems include sample average approximation methods (cf. Shapiro et al., 2009) and stochastic approximation methods (cf. Nemirovski, Juditsky, Lan, & Shapiro, 2009), which employ samples, but not the probability measure directly. Solution techniques and heuristics from global optimization particularly apply to solve the general, nonlinear problem (1), and we refer to the extensive literature on global optimization.This paper provides bounds for the objective of the genuine problem (1). The results are presented in terms of the Wasserstein distance. The distance provides a quality control of solutions of approximations of (1), as the objective turns out to be continuous with respect to the distance. Approximations, found by applying discrete measures, are justified in this way.In addition we adapt Newton’s method to the particular structure of the nonlinear stochastic optimization problem (1). Newton’s procedure makes predictor-corrector methods available without additional effort, which we exploit to solve the inner problem for different scenarios faster. The results are finally demonstrated in an extensive case study. The case study addresses a stochastic extension of the optimal power flow problem taken from electrical engineering.The following Section 2 addresses the theoretical justifications by discussing the baseline probability measure and its impact to problem (1). This section contains the central result of the paper, Theorem 5. Section 3 addresses numerical solution techniques, which are useful in reducing the computational burden to solve the nonlinear problem. The second part of the paper outlines the stochastic optimal power flow problem in Section 4. We conclude with results, a what-if analysis and a discussion in Section 5.This section introduces the Wasserstein distance for probability measures. It is demonstrated that the objective of the stochastic optimization problem (1) is continuous with respect to the Wasserstein distance and in addition, the risk functionalRis Wasserstein continuous as well. This is the essential result in investigating the two-stage optimization problem, particularly for nonlinear problems.The results provided ensure that passing to different, simpler probability distributions does not destroy a solution, the quality of the solution is kept to a reasonable extent. This section thus ensures that the solution of the problem with respect to a simpler, discrete probability measure is a good solution for the real problem as well. Important is just the quality of the approximation of the measure in the Wasserstein distance.The Wasserstein distance provides a distance of probability measures. We prove first that the expectation and risk functionals are continuous with respect to the Wasserstein distance. This is the essential property with various consequences for stochastic optimization:(i)it is possible to derive bounds for (1) by comparing evaluations for different probability measures;continuous probability measures can be replaced by discrete measures, which are eligible for numeric computations;the computational burden for the numerical computation of the initial problem (1) is much lower for a smaller number of scenarios. The results justify clustering methods to reduce the computational burden, while error bounds are made available simultaneously.Several books are dedicated to the Wasserstein distance. Details and mathematical properties of this distance can be found in the books by Rachev and Rüschendorf (1998) or Villani (2003).Definition 1Wasserstein distanceThe Wasserstein distance of order r ≥ 1 of two probability measures P andP˜on the metric space (Ξ, d) is given bydr(P,P˜)=(inf∫∫Ξ×Ξd(ξ1,ξ2)rπ(dξ1,dξ2))1r,where the infimum is among all probability measures π with marginals P andP˜,that is, they satisfyπ(A×Ξ)=P(A)andπ(Ξ×B)=P˜(B)whenever A and B are measurable sets.The Wasserstein distance depends on the distance d on the initial space and is occasionally considered with a cost function instead of the distance d (cf. Villani, 2003). For the applications in mind of this paper it is enough to considerΞ=Rm,equipped with a distance induced by a (weighted) Euclidean norm.We have the following lemma for the pushforward measure (image measure) under Hölder, or Lipschitz continuous functions.Lemma 2LetQ:(Ξ˜,d˜)→(Ξ,d)be a function between metric spaces which is Hölder continuous,(3)d(Q(x),Q(y))≤C·d(x,y)βwith exponent β ≤ 1. For β fixed, denote the infimum of the constants satisfying(3)by ‖Q‖β. Thendr(PQ,P˜Q)≤∥Q∥β·dβr(P,P˜)β,where PQ(A) ≔ P(Q ∈ A) is the image measure (pushforward measure) andr≥1β.Let π have marginalsπ(A×Ξ)=P(A)andπ(Ξ×B)=P˜(B),thenπ˜(A×B):=π(Q−1(A)×Q−1(B))has marginals PQandP˜Q. Hence, by the change of variables formula,dr(PQ,P˜Q)r≤∫∫d(x,y)rπ˜(dx,dy)=∫∫d(Q(x),Q(y))rπ(dx,dy)≤∫∫∥Q∥βr·d(x,y)βrπ(dx,dy).Taking the respective infimum reveals thatdr(PQ,P˜Q)r≤∥Q∥βr·dβr(P,P˜)βr,from which the assertion follows.□There are some results characterizing the convergence of the empirical measure11δξis the Dirac measure (or point measure) at ξ, i.e.,δξ(A)={1ifξ∈A0else.Pn:=1n∑i=1nδξitowards its limit P in Wasserstein distance (cf. Graf & Luschgy, 2000 or Bolley, Guillin, & Villani, 2007). In rough words, convergence is of orderO(n−1m),wheneverΞ=Rm. Hence, the number of samples ξ has to be increased by a factor of 2m, whenever an improvement ofd(Pn,P)by a factor of12is desired. In many situations of practical relevance the factor 2mis much too high for numerical tractability.Lemma 2 provides an essential reduction. By involving a Lipschitz, or Hölder continuous functionQ:Rm1→Rm2with m2 ≪ m1 the measuresPnQconverge much faster towards PQ, the order isO(n−1m2).The risk functionals considered in problem (1) forR-valued random variables Q are of the form(4)R(Q)=supσ∈SRσ(Q),whereRσ(Q):=∫01FQ−1(u)σ(u)duis called a distortion risk functional (or spectral risk functional) andσ∈Sis a distortion function: a distortion function σ: [0, 1) → [0, ∞) is nonnegative, nondecreasing and satisfies∫01σ(u)du=1.FQ−1(α):=inf{q:P(Q≤q)≥α}is the quantile.The expectation is a simple version of a risk functional (4), asEQ=∫01FQ−1(u)du.A specific example, which is often employed in stochastic optimization, is the (upper) Average Value-at-Risk (or Conditional Value-at-Risk) at level α defined asAV@Rα(Q):=11−α∫α1FQ−1(u)du.These types of coherent risk functionals are discussed in many places, not only in stochastic optimization, but particularly in mathematical finance. For a broad discussion on risk functionals we refer to the book (Pflug & Römisch, 2007) by Pflug and Römisch, and for distortion risk functionals in particular to Pflug (2006). A comprehensive mathematical treatment can be found in Pichler (2013, 2015) as well.It is of essential importance for stochastic optimization that the risk functionals (4) are continuous with respect to the Wasserstein distance. This is the content of the following lemma.Lemma 4Distortion risk functionals are continuous with respect to changing the underlying probability measure, that is,|Rσ;P(Q)−Rσ;P˜(Q)|≤∥Q∥β·dβp(P,P˜)β·∥σ∥q.q ≥ 1 is the exponent conjugate to p,1p+1q=1.It follows from Hölder’s inequality thatRσ;P(Q)−Rσ;P˜(Q)=∫01(FQ;P−1(u)−FQ;P˜−1(u))σ(u)du≤(∫01|FQ;P−1(u)−FQ;P˜−1(u)|pdu)1p×(∫01σ(u)qdu)1q=(∫01|FQ;P−1(u)−FQ;P˜−1(u)|pdu)1p∥σ∥q,where the probability measure P is explicitly exposed as a subscript by writingFQ;P−1(α)=inf{q∈R:P(Q≤q)≥α}. Due to the identity∫01|FQ;P−1(u)−FQ;P˜−1(u)|pdu=dp(PQ,P˜Q)pin Ambrosio, Gigli, and Savaré (2005, Theorem 6.0.2) it follows from Lemma 2 thatRσ;P(Q)−Rσ;P˜(Q)≤∥Q∥β·∥σ∥q·dβp(P,P˜)β.The assertion is immediate by interchanging the roles of P andP˜.□The following theorem combines the ingredients collected, it is the central statement of the present text. The theorem states that the stochastic optimization problem (1) is continuous with respect to the Wasserstein distance.Theorem 5Continuity of the stochastic optimization problem (1)Let c be uniformly Hölder continuous in its random component ξ, that isc(y,ξ,z)−c(y,ξ˜,z)≤∥c∥β·d(ξ,ξ˜)βfor all y, z, ξ andξ˜. Then the stochastic optimization problem(1)is continuous in its probability measure, it satisfies(5)|infy∈YRP(infz∈Zc(y,ξ,z))−infy∈YRP˜(infz∈Zc(y,ξ,z))|≤∥c∥β·dβp(P,P˜)β·supσ∈S∥σ∥q.Inequality (5) provides an upper bound on the error induced when replacing a probability measure P byP˜. We employ this result later and approximate P by a simple probability measureP˜. The theorem provides a bound then in both directions when comparing the solution of the approximating problem (P˜) with the true problem (P).The infimum of uniformly Hölder continuous functions is Hölder continuous again (this is detailed in Lemma 21 in the Appendix A), henceQ(y,ξ):=infz∈Zc(y,ξ,z)is Hölder continuous with the same constant ‖c‖β. It follows from Lemma 4 thatRσ;P(Q)−Rσ;P˜(Q)≤∥c∥β·∥σ∥q·dβp(P,P˜)βfor everyσ∈S.Now chooseσɛ∈Ssuch thatsupσ∈SRσ;P(Q)≤Rσɛ;P(Q)+ɛ,and it becomes obvious thatRP(Q)−RP˜(Q)−ɛ≤Rσɛ;P(Q)−Rσɛ;P˜(Q)≤∥c∥β·supσ∈S∥σ∥q·dβp(P,P˜)β.By the same reasoning as above it follows thatinfy∈YRP(Q(y,ξ))−infy∈YRP˜(Q(y,ξ˜))≤∥c∥β·supσ∈S∥σ∥q·dβp(P,P˜)β.This is the assertion, as the roles of P andP˜can be interchanged.□Theorem 5 provides sufficient conditions for continuity of the general stochastic optimization problem (1) with respect to changing the probability measure. It is worth a remark that the infimum in Theorem 5 is among general sets Y in the first stage and general sets Z in the second stage, no special structure of these sets is required. This means in particular that the outer and inner minimization can be over integers or may include binary variables, and the conclusion on continuity is still valid.Every probability measure P onΞ=Rmcan be approximated arbitrarily close in the Wasserstein distance by a discrete measureP˜=∑i=1n˜p˜iδξiwithp˜i>0and ξi∈ Ξ. It follows from Theorem 5 that every stochastic optimization problem (1) can be approximated by replacing the (eventually continuous) measure P by a simple, discrete measureP˜. In this way, every stochastic optimization problem is eligible for computational, numerical treatment.Using clustering one intends to replace a probability measure by a simpler one, such that the computation of (1) can be done even more quickly and more efficiently. In what follows we demonstrate that clustering is continuous with respect to the Wasserstein distance, which makes clustering a useful method in reducing the computational burden.To this end consider a discrete probability measureP˜=∑i=1n˜p˜iδξ˜i,whereΞ˜:={ξ˜i:i=1,⋯,n˜}are the supporting points. One may compare this measure with P.The following Definition and Lemma (cf. Pflug & Pichler, 2011) provides the tool to determine the discrete measureP˜,which approximates the original measure P as well as possible in terms of the Wasserstein distance.Definition 8A tessellation of Ξ consists of measurable sets(Vi)i=1n˜such that⋃i=1n˜Vi=ΞandVi∩Vj=∅wheneveri≠j.A tessellation is a Voronoi tessellation with centers{ξ˜i:i=1,⋯,n˜},ifd(ξ,ξ˜i)≤d(ξ,ξ˜k)forallξ∈Viandk=1,⋯,n˜.The probability measureP˜=∑i=1n˜p˜iδξ˜i,which is located on{ξ˜i:i=1,⋯,n˜}and approximating P in best possible way in terms of the Wasserstein distance has the weights(6)p˜i:=P(Vi),where(Vi)i=1n˜is a Voronoi tessellation with centers{ξ˜i:i=1,⋯,n˜}. The best distance is given by the explicit formula(7)dr(P,∑i=1n˜p˜iδξ˜i)r=∫Ξminj=1,⋯n˜d(ξ,ξ˜j)rP(dξ)for all r ≥ 1.It is a consequence of Lemma 9 and (7) that the Voronoi tessellation does not have to be available explicitly in order to compute the Wasserstein distance if the weights are chosen as specified in (6). For algorithms to compute (7) or approximations of it we may refer to Pflug and Pichler (2014); 2015). These reference address the problem of finding optimal locationsξ˜1,...,ξ˜n˜as well by employing clustering methods and stochastic approximation.Let π have marginals P andP˜,then∫∫d(ξ,ξ˜)rπ(dξ,dξ˜)≥∫∫minj=1,⋯,n˜d(ξ,ξ˜j)rπ(dξ,dξ˜)=∫minj=1,⋯,n˜d(ξ,ξ˜j)rP(dξ),and hencedr(P,P˜)r≥∫minj=1,⋯,n˜d(ξ,ξ˜j)rP(dξ),a lower bound.Next, define the mapT(ξ):=ξ˜i,ifξ∈Viand the bivariate measureπ(A×B):=P(A∩T−1(B)). It holds thatπ(A×Ξ)=P(A)andπ(Ξ×B)=PT(B)=P˜(B),such that π has adjusted marginals. It thus holds that∫∫d(ξ,ξ˜)rπ(dξ,dξ˜)=∫∫minj=1,⋯,n˜d(ξ,ξ˜j)rπ(dξ,dξ˜)=∫minj=1,⋯,n˜d(ξ,ξ˜j)rP(dξ),such thatdr(P,∑i=1n˜p˜iδξ˜i)r=∫minj=1,⋯,n˜d(ξ,ξ˜j)rP(dξ),the assertion.□Theorem 5 in the previous section gives a constructive bound for the objective of the two-stage stochastic optimization problem, whenever the probability measure P in (1) is replaced by a simpler measureP˜. Lemma 9, in addition, gives an explicit formula for the Wasserstein distance of the probability measure, which is located on{ξ˜i:i=1,⋯,n}.These results have the following implications for numerical solutions:(i)A continuous probability measure is not eligible for numerical computations in (1). However, numerical algorithms easily apply for the problem with probability measure P replaced byP˜=∑i=1n˜piδξ˜i. Hence, even the continuous problem gets numerically tractable, and an explicit bound is given by (5) in Theorem 5(cf. also Remark 6).A further consequence is that complicated problems involving a measure of the form∑i=1npiδξican be replaced by a simpler measureP˜=∑i=1n˜piδξ˜i,wheren˜is much smaller than n,n˜≪n. This reduces numerical computation times significantly (cf. Lemma 9).For each realization ofξ˜i,i=1,⋯,n˜,the inner minimization of the stochastic optimization problem (1) has to be solved separately. The following section addresses and exploits this particular problem structure.We consider a discrete realization for the measureP=∑i=1npiξi(for example an empirical measure resulting from Monte Carlo simulations, or a measure reduced according Lemma 9). For every realizationξ∈Ξ:={ξi:i=1,⋯,n}the recourse problemminz∈Z(y)c(y,ξ,z),i.e., the inner problem in (1), has to be solved. Without loss of generality one may assume that the problem is given in the form(8)Q(ξ):=minimizeinzc(ξ,z)subjecttoh(ξ,z)=0,z∈B.Q is called the recourse function. The essential observation is that every scenario ξ can be considered as a parameter, and Q(ξ) then are the final, total costs after optimization for the particular scenario ξ. The optimal solution z notably differs for every fixing of ξ, such that the solution z of (8) is a function of ξ,z=z(ξ).To evaluate the expected value orR(cf. Eq. (1)), the recourse function Q(ξ) has to be evaluated for every assignment of ξ, that is, the inner minimization in z (Eq. (8)) has to be feasible for every outcome of ξ separately. Indeed, if Q were not feasible almost everywhere, thenP(Q=∞)>0and thusR(Q)≥EQ=∞.We list this following, important observation regarding the feasibility of the problem for different scenarios.Lemma 10Let Ξ and B be compact and the constraint function h be continuous. Then the set of feasible scenarios{ξ∈Ξ:∃z∈B:h(ξ,z)=0}is closed and compact.Consider the setF:={(ξ,z)∈Ξ×B:h(ξ,z)=0}. This set is closed, as h is continuous andF=h−1({0}). The set F is moreover compact, as F ⊂ Ξ × B and Ξ and B are compact. The projection i: Ξ × B → Ξ is continuous, such that{ξ∈Ξ:∃z∈B:h(ξ,z)=0}=i(F)is compact.□Especially in an economic situation it is typically expected that the recourse function Q(ξ) provides similar results for parameters ξ, which are close (i.e., that Q is continuous). However, the previous Lemma 10 explains that this cannot be expected in general. It is an important consequence of Lemma 10 that small aberrations from a feasible scenario ξ may not be feasible for the inner problem (8). As a consequence the result of the problem (8) is possibly not continuous with respect to the parameter ξ, such that further conditions are necessary to apply Theorem 5. Another consequence is that passing to a simpler measure, as outlined in Lemma 9, may not be possible.Reasonable conditions to insure the assertions of Theorem 5 and Lemma 9 often can be derived from the particular problem at hand, sometimes in connection with Remark 13 or 14 below.The nonlinear optimization problem (8) is typically (and efficiently) solved by applying Newton’s method.22To simplify the exposition and for convenience of notation we treat active inequality constraints and the box constraints z ∈ B as equations incorporated inh(ξ,z)=0(cf. Ruszczyński (2006, p. 326)). The active constraints notably vary with ξ and z.For this the LagrangianL(z,λ;ξ):=c(ξ,z)+λ⊤h(ξ,z)is considered, where we treat ξ as a parameter.The necessary conditions of optimality are(9)f(z,λ;ξ):=(Lz(z,λ;ξ)Lλ(z,λ;ξ))=(cz(ξ,z)+λ⊤hz(ξ,z)h(ξ,z))=(00),wherecz(ξ,z)=∇zc(ξ,z)=(∂c∂z1,⋯,∂c∂zn)is the partial derivative of c with respect to the vector z (hzis the partial derivative of h, resp.), etc.The symmetric Jacobian of the system of equations (9),(10)f′(z,λ;ξ)=(czz(ξ,z)+λ⊤hzz(ξ,z)hz(ξ,z)⊤hz(ξ,z)0),is often called KKT-system (czz=∇z2cis the symmetric Jacobian matrix with entries∂2c∂zi∂zj,etc.).Starting with some tentative solution(z0λ0)to solve (9), Newton’s method provides the iterates(zk+1λk+1):=(zkλk)+(ΔzkΔλk),where the linear system of equationsf′(zk,λk;ξ)·(ΔzkΔλk)=−f(zk,λk;ξ)has to be solved in successive iterations.Remark 12The Lagrange multiplier λ is uniquely determined whenever the matrix hz(ξ, z) has linearly independent rows. Further, regularity of matrix (10) can often be verified by employing Cauchy’s interlacing eigenvalue theorem. This theorem ensures that the eigenvalues of (10) are strictly positive or strictly negative, such that the matrix is regular (invertible).Details on Newton’s procedure are elaborated, e.g., in Ruszczyński (2006) and in Boyd and Vandenberghe (2004).The implicit function theorem provides sufficient conditions to ensure that h is invertible in a neighborhood. When employing Newton’s method to solve problem (8), then relevant information is automatically available due to the (inverted) Jacobian matrix. This provides numerical evidence for a feasible region in a neighborhood of a single, feasible scenario.Newton’s methods converges quickly whenever a good starting value is available. In order to obtain a good starting value a predictor corrector method can be employed.33The idea of predictor corrector methods is adopted from evolution equations (differential equations).The predictor provides a reasonable, tentative guess, and the corrector improves the initial guess to compute a solution. It turns out that a predictor, as well as a corrector, are provided by Newton’s method.To computez(ξ˜)one may considerz(ξ+Δξ),whereΔξ:=ξ˜−ξ. Provided that Δξ is small it is to be expected thatz(ξ)+z′(ξ)·Δξis a reasonable starting value for Newton’s method, whenever z(ξ) is known. This predictor can be specified by taking the derivative of (9) with respect to the parameter ξ, resulting in the following equation:(LzzLλz⊤Lλz0)·(zξλξ)+(LzξLλξ)=(00).This system rewrites as(czz(ξ,z)+λ⊤hzz(ξ,z)hz(ξ,z)⊤hz(ξ,z)0)·(zξλξ)+(czξ(ξ,z)+λ⊤hzξ(ξ,z)hξ(ξ,z))=(00).For an incremental change of Δξ we have thatΔz=z′·Δξ,and the latter equation thus is(11)(czz(ξ,z)+λ⊤hzz(ξ,z)hz(ξ,z)⊤hz(ξ,z)0)·(ΔzΔλ)=−(cz(ξ+Δξ,z)+λ⊤hz(ξ+Δξ,z)h(ξ+Δξ,z)),because (9) holds with equality at ξ.The linear equation (11) has to be solved to obtain the predictorz(ξ)+Δz. But this equation is notably the equation for the Newton step at z(ξ), except that the right hand side is disturbed. Hence, starting Newton’s method at ξ by employing the solution z(ξ) represents a predictor in direction of the Taylor approximation to computez(ξ˜). Importantly, an implementation of Newton’s method can be reused to numerically compute the predictor without additional effort.This feature exposes Newton’s method as a central tool to compute the recourse function Q of the stochastic optimization problem (1) for varying scenarios.Exact knowledge of z(ξ) is not necessary to compute the recourse function Q(ξ). Even more, in many situations a reliable bound for Q(ξ) might be enough for some selected scenarios ξ or for scenarios with small probability pξ. Given that we can estimate an upper bound L for the derivative it holds thatQ(ξ˜)≥Q(ξ)−L∥ξ−ξ˜∥andQ(ξ˜)≤Q(ξ)+L∥ξ−ξ˜∥. The envelope theorem, which we address in what follows, often provides such a reasonable bound for the derivative.Consider the optimization problem44Again, active box constraints are incorporated in h, cf. Footnote 2.(12)Q(ξ):=minimizeinzc(ξ,z)subjecttoh(ξ,z)=0,where we treat ξ as a parameter. An application of the envelope theorem provides the derivative Qξ(ξ) as a simple byproduct of the optimization (12). Combined with the Taylor series expansion we obtain the approximationQ(ξ˜)≃Q(ξ)+Qξ(ξ)·(ξ˜−ξ). An approximation ofQ(ξ˜)thus is available, without explicitly solving (12) for the optimal valuez(ξ˜). This approximation is sufficient for many scenariosξ˜,especially ifξ˜is close to ξ, or if its corresponding probabilitypξ˜is small.The optimal value z in (12) is a function of ξ, which satisfies the implicit conditionsQ(ξ)=c(ξ,z(ξ))andh(ξ,z(ξ))=0.By differentiating these equations it follows that(13)Qξ(ξ)=cξ(ξ,z(ξ))+cz(ξ,z(ξ))·z′(ξ)and0=hξ(ξ,z(ξ))+hz(ξ,z(ξ))·z′(ξ).Moreover, employing the LagrangianL(z,λ)=c(ξ,z)+λ⊤h(ξ,z)for (12) the first order conditions are(14)Lz(z,λ)=cz(ξ,z)+λ⊤hz(ξ,z)=0andLλ(z,λ)=h(ξ,z)=0.After multiplying (13) with λ⊤ and (14) with z′(ξ) it becomes evident thatλ⊤hξ(ξ,z(ξ))=cz(ξ,z(ξ))·z′(ξ). Hence,(15)Qξ(ξ)=cξ(ξ,z(ξ))+cz(ξ,z(ξ))·z′(ξ)=cξ(ξ,z(ξ))+λ⊤hξ(ξ,z),in accordance which the envelope theorem.Remark 14Lipschitz constant of the recourse functionIt follows from (15) (the assertion of the envelope theorem) that∥Qξ∥≤∥cξ∥+∥λ⊤hξ∥.The Lipschitz constant of the recourse function Q thus is bounded byLip(Q)≤∥cξ∥+∥hξ⊤∥·∥λ∥,where∥hξ⊤∥is the consistent matrix norm induced by the norm for ‖λ‖. The norm of cξis available by inspecting the cost function.Boundedness of∥hξ⊤∥often can be derived for the particular problem at hand, while boundedness of the dual variable ‖λ‖ is addressed in Remark 12 above. In addition the dual variable λ(ξ) can be monitored during the computation of Q(ξ) to track its norm, such that a global Lipschitz constant of the recourse function Q is available during the computations.It was elaborated that Newton’s method can be employed to evaluate the recourse function Q(ξ) for different parameters ξ by minimizing with respect to z. Further, Newton’s method automatically provides a predictor by starting at a previous solution.We recall here two variants of Newton’s method to exploit this method further and to accelerate numerical computations.Newton’s method requires computing the Newton step, that is, the matrix in (10) has to be inverted. As it is time consuming to invert these matrices in each iterative step it is tempting to reuse the (inverse) Jacobian matrices, or its LU or QR decomposition from previous iterations. Indeed, the Jacobian can be reused as long as ‖f (zk, ξ)‖ decreases during an iteration.The (inverse) Jacobian thus can be reused in both situations,(i)during the computation of Q(ξ) when solving the systemf(z,ξ)=0with respect to z, andas predictor to restart the Newton procedure to minimizeQ(ξ˜)for a new scenarioξ˜,which is different (but in ideal case close) to the previous ξ.It is the advantage of reusing the inverse Jacobian that it does not have to be constructed nor inverted again, which is typically the most expensive step. However, convergence is not further insured, or can be expected to be slower. Typically linear convergence is obtained, whereas quadratic convergence of the full Newton method is lost.A survey on the convergence of Newton-like methods can be found in Yamamoto (2000). The following theorem, dating back to Dennis (1968), justifies the method outlined.Theorem 15Convergence of Newton-like methodsLet f ∈ C2be twice continuously differentiable with∥f′(z)−f′(z˜)∥≤c·∥z−z˜∥and let M(z) satisfy∥1−f′(z)M(z)∥≤δ<1and ‖M(z)‖ ≤ B. If z0can be chosen such that∥f(z0)∥<21−δB2c,then the Newton-like sequencezk+1:=zk−M(zk)·f(zk)converges to a zero of f. Convergence is at least linearly.The convergence result on Newton-like methods in Theorem 15 provides a qualitative alternative to the implicit function theorem (cf. Remark 11): given that the conditions in Theorem 15 are satisfied for the function f( ·, ξ) for some ξ and z is available, such thatf(z,ξ)=0,then, by assuming enough smoothness,f(·,ξ˜)is invertible as well and Newton’s method converges toz˜,the solution off(z˜,ξ˜).Although convergence is already obtained by employing the matrix Mk, which is available from a previous iteration, the speed, and the rate of convergence can be improved by successively updating the matrix Mk(cf. Broyden & Vespucci, 2004). The following proposition provides a useful method to update the matrix Mkafter each iteration.Proposition 17Let Mk be a matrix andfk*an arbitrary functional such thatfk*(fk+1−fk)≠0. Then the updated matrixMk+1:=Mk+(zk+1−zk−Mk(fk+1−fk))⊗fk*fk*(fk+1−fk)satisfies(16)Mk+1(fk+1−fk)=zk+1−zk(⊗ is the outer product of two vectors).Notice, thatf(zk+1)≈f(zk)+f′(zk)·(zk+1−zk)by Taylor’s expansion, andzk+1−zk≈f′(zk)−1(fk+1−fk)in first Tayler approximation. Mkthus can be expected to be an approximation of the inverse Jacobian, that is,f′(zk)−1≈Mk. The new approximationMk+1is an improved approximation off′(zk)−1,at least it correctly recovers the new directionzk+1−zk.The assertion of Proposition 17 is immediate, asMk+1(fk+1−fk)=Mk(fk+1−fk)+(zk+1−zk−Mk(fk+1−fk))⊗fk*(fk+1−fk)fk*(fk+1−fk)=Mk(fk+1−fk)+(zk+1−zk−Mk(fk+1−fk))=zk+1−zk.□The updated iterateMk+1notably satisfies Eq. (16), which is the equation for the Newton–Raphson step. It is hence to be expected that the matrixMk+1is a better approximation ofF(zk)−1than Mk. This is indeed the case, iffk*is chosen appropriately. For the choicefk*(·):=〈fk+1−fk,·〉convergence is even superlinear (cf. Gay, 1979).Remark 19While f′ andf′−1are symmetric matrices (cf. (10)), the update proposed in Remark 18 is not necessarily symmetric again. Moreover the sparse structure of the matrix f′ may be lost by the updates. The matrix can be kept symmetric by choosing the linear functionalfk*(·):=〈zk+1−zk−Mk(fk+1−fk),·〉.We refer to Nocedal (1980) for further aspects on this topic.Algorithm 1 outlines the techniques collected to accelerate the computation of all second stages.Bienstock (2013) points out that power flow problems can surprise optimization experts by their difficulty. For this reason they are often considered as benchmark problems, in particular for the choice of algorithms to numerically solve optimization problems. From practical perspective, power flow problems are motivated by an increasing demand of electrical power and varying operational costs, as well as the need to manage system failures as blackouts (cf. Bienstock, 2013).A second motivation – with increasing importance – is the need to incorporate renewable energy in the power flow network: many countries are in a transition period and currently redesign their power network. Germany, for example, builds new transmission lines in order to transport electricity from the north, where electricity is generated in off-shore wind parks, to the south, where demand is high. A new situation is created in Denmark as well (cf. Villumsen, Brønmo, & Philpott, 2012), as the country has decided that nearly 50 percent of its demand should be provided by wind power within a period of ten years. These situations provide the opportunity to question and rethink the existing power transmission network, and to improve its efficiency by establishing an optimal power network topology.This case study addresses the optimal power flow problem in an economic, stochastic environment. The stochastic character is given by the fact that future demand and supply are random, they can only be assumed or estimated from today’s perspective. However, the power grid has to be designed today, although its future utilization and the capacities necessary are not completely known today.Villumsen and Philpott (2012) consider the particular problem to decide on investments in electricity networks with transmission switching. They formulate the problem as a stochastic optimization problem by introducing (economic) scenarios. Every scenario ξ describes a reasonable pattern of demand and supply in the network, for which the power has to be generated at the costs c( ·, ξ, ·). These costs to generate the electric power are aggregated in a single objective function by assigning probability weights to every scenario ξ. In this way Villumsen and Philpott propose the two-stage stochastic optimization problem in the form presented in (2),(19)miny∈YE(minz∈Z(y,ξ)c(y,ξ,z)).This model balances investment costs (y ∈ Y) against potential reductions in operational cost (z ∈ Z): the inner minimization minz ∈ Z(y, ξ)c(y, ξ, z) models the decision of the system operator to generate and supply the power, which corresponds to the actual demand ξ in the network. It is the optimal power flow problem and the decision z ∈ Z, the wait-and-see decision, may differ for different scenarios ξ. The expectationEsummarizes the costs corresponding to ξ according the respective probability weight.The here-and-now decision y ∈ Y identifies the optimal network design, which is an investment decision to be established today. The investment decision consists in finding reasonable places to install automated switching devices (for example FACTS, flexible AC transmission system devices), which operate on remote basis.It was observed that switching off existing lines may increase the overall efficiency and economic profitability of an electricity network (cf. Potluri & Hedman, 2012). This fact (this paradox) is perhaps counter-intuitive, but it is clearly a starting point to incorporate switching possibilities in the network in order to adjust the power flow and to ensure constant, high profitability. This holds true even more as electricity demand, as well as its supply, are random: weather (as wind, rain or insolation), water-level of rivers and reservoirs, as well as outside temperature, time of day and time of the year are influencing factors amongst various others (Fig. 1a exemplary displays the demand profile over a year for nordic countries.) The expansion of renewable energy sources currently increases the respective (stochastic) volatility of electricity supply. This situation creates a window of opportunity for power network design.It was realized recently that the error caused by linearizing the inner problem in (1) exceeds the effect which is obtained by choosing different investment decisions y ∈ Y (Fuller and Soroush (2013) point this out for switching in transmission systems). A solution of the simplified, linear DC approximation is possibly a misleading candidate for the genuinely nonlinear problem (the AC formulation). Solving the linearized problem, even with high accuracy, thus does not allow justified conclusions to the real world problem. For this reason problem (1) has to be considered in its nonlinear formulation.For the investment problem (19) the inner optimization is nonlinear, whereas the outer minimization is combinatorial. The problem thus can be classified to be of mixed integer, nonlinear, non-convex, combinatorial stochastic optimization type. However, the evaluation of related functions is not expensive and analytic expression for derivatives are available. The optimal power flow problem in a stochastic environment thus nicely exposes the difficulties related to the nonlinear characteristics of (1) (cf. Bukhsh, Grothey, McKinnon, & Trodden, 2013) – a main reason why transmission switching was chosen to outline general solution techniques for stochastic optimization problems as (19).Electric power is generated at generation buses i ∈ G (cf. nomenclature) and transferred to the load buses to satisfy the demand there. The demand is a known quantity at every load bus in a network. The AC power flow within the network then simultaneously satisfies at every bus i ∈ B the real power balance equations(20)∑k∈BViVk(Gikcosθik+Biksinθik)+Pid−Pig=0(i∈B)and the reactive power balance equations(21)∑k∈BViVk(Giksinθik−Bikcosθik)+Qid−Qig=0(i∈B∖G),which are derived from Kirchhoff’s laws.The power flow problem assumes that the real power generatedPig,and the voltage magnitude Viare given quantities at generator buses (for this reason, generator buses are known as PV buses, i ∈ G). The net and reactive power demand (PidandQid) are known at the load buses (PQ buses, i ∉ G). A solution of the balance equations (20) and (21) consists of voltage angles θifor all buses and the voltage magnitudes Viat the remaining buses, the load buses. Table 1collects the known quantities and the variables of the AC power flow equations.The balance equations (20) and (21) do not specify a power flow solution uniquely, as for example uniformly shifting all voltage angles θiof a solution by a constant angle (θ*, say) solves the power flow equations equally well. To select a unique solution from the manifold of solutions the voltage phase θ of a selected generator bus (the reference, or slack bus) is occasionally fixed.To abbreviate the notation the unknown variables are collected in a vector z, i.e., we setz:=((θi)i∈B,(Vi)i∈B∖G).We summarize the balance equations (20) and (21) as well in vectors and set(22)h(z):=(hP(z)hQ(z))=(hP(θ,V)hQ(θ,V))=(00)=0.hP(z) and hQ(z) represent Eqs. (20) and (21), respectively. Eq. (22) is simply referred to as power flow equation, or AC power flow equation.An efficient and well established method to numerically solve the power flow equations (22),h(z)=0,is by applying Newton–Raphson’s method. The method iteratively defines a sequence zkby(23)zk+1=zk+Δzkandh′(zk)·Δzk=−h(zk),where h′ is the Jacobian matrix (the derivative with respect to z). The sequence zkconverges quadratically to a solution under general conditions, provided that the starting value z0 is already close enough to a solution of the power flow equation.Various techniques have been considered and developed in the past to approximately, or efficiently solve the nonlinear power flow equations (22). An important approximation is based on linearization, which is natural if the phase angles in the network are almost parallel, that is, ifθik=θi−θk≈0for all directly connected buses (i, k) ∈ L.The linearized equations derive from the observations cos θik≈ 1 and sin θik≈ θik(the approximation are of second order for small θik), by neglecting the conductance Gikand assuming that the deviations fromV=1are very small. The approximating equations obtained are(24)Pi=∑kBik(θk−θi)andQi=∑k≠iBik(Vk−Vi)−∑kBikVi,often referred to as DC power flow equations. In contrast to the above AC power flow equation (22), the system (24) is linear in the variables θ and V, and thus comfortably easy to solve. As already addressed, the approximation quality of the solution of (24) is typically not satisfactory. The solution of the approximating problem, however, is an important starting point for iterative methods (cf. Hedman, O’Neill, Fisher, and Oren (2008a, 2008b); Hedman and Oren (2009)), for example for the nonlinear solvers addressed in what follows.The system of linear equation, which has to be solved in (23), can be computed efficiently as the derivative(25)h′=(∂hP∂θ∂hP∂V∂hQ∂θ∂hQ∂V)in typical applications is a sparse matrix with entries(26)∂hP,i∂θi=∑k≠iViVk(−Giksinθik+Bikcosθik),(27)∂hP,i∂θℓ=ViVℓ(Giℓsinθiℓ−Biℓcosθiℓ)(ℓ≠i),(28)∂hP,i∂Vi=2GiiVi+∑k≠iVk(Gikcosθik+Biksinθik)and(29)∂hP,i∂Vℓ=Vi(Giℓcosθiℓ+Biℓsinθiℓ)(ℓ≠i)in the upper row of the matrix (25) (the lower row for the derivatives of hQ, ibeing analogous). Note that if the buses i and ℓ are not connected by a transmission line, (i, ℓ) ∉ L, then the admittance isYiℓ=Giℓ+jBiℓ=0and the entries in the derivative vanish,∂hP,i∂Vℓ=∂hP,i∂θℓ=0. For a typical power flow network the derivative h′ is a sparse, connected matrix. Its entries reflect the adjacency matrix of the graph (B, L).The classical convergence theory for the Newton procedure requires that the derivatives are Lipschitz continuous, as outlined in Theorem 15 (cf. also Kantorovich’s theorem, two different proofs are given in Kantorovich, 1948; Kantorovich & Akilov, 1965). The following lemma uncovers that this is the case for the power flow problem, a uniform Lipschitz constant can be chosen on bounded domains.Lemma 20As an operator from ℓ∞to ℓ∞, the derivative h′ is uniformly bounded by(30)∥h′(y)∥≤2(1+Vmax)2·maxi∑k|Yik|=2(1+Vmax)2·∥Y∥,where Y is the bus admittance matrix and |Vi| ≤ Vmax for all buses i ∈ B. ‖Y‖ is the norm of the admittance matrix, as an operator Y: ℓ∞ → ℓ∞.Moreover it holds that h′ is continuous, i.e, there is a constant c such that∥h′(y)−h′(y˜)∥≤c∥y−y˜∥. As in(30)the constant c depends on(1+Vmax)2.The norm of a matrix J induced by ℓ∞ issup∥x∥∞≤1∥J·x∥∞=sup∥x∥∞≤1maxi|∑jJijxj|=maxisup∥x∥∞≤1|∑jJijxj|=maxi∑j|Jij|.Applied to the JacobianJ=h′(z)with entries (26) and (27) it holds thatsup∥x∥∞≤1∥h′(y)·x∥∞≤maxi{2|Yii||Vi|+∑k≠i|Vk||Yik|+|Vi|∑ℓ≠i|Yiℓ|+∑k≠i|Vi||Vk||Yik|+|Vi|∑ℓ≠i|Vℓ||Yiℓ|}=maxi{2Vmax∑k|Yik|+2Vmax2∑k|Yik|}≤2(Vmax+1)2∑k|Yik|,as|Gikcosθik+Biksinθik|≤Gik2+Bik2=|Yik|.Lipschitz continuity of the derivative h′ itself follows by the same reasoning as above, as h′′ can be provided explicitly, although collecting the terms is cumbersome.□There exist further methods to solve the AC power flow equations (22). The Gauss–Seidel method is based on the observations∂hP,i∂θi+∑ℓ≠i∂hP,i∂θℓ=0(Eqs. (26) and (27)) and∂hP,i∂Vi⪆∑ℓ≠i∂hP,i∂Vℓ(Eqs. (28) and (29)), such that the Jacobian matrix (25) is apparently a (column) diagonal dominant matrix (it is indeed a diagonal dominant matrix in the caseθik=θi−θk≈0). Diagonal dominance is a simple criterion to ensure convergence of the related Gauss–Seidel method. Although the Jacobian is not diagonally dominant in general, the Gauss–Seidel method often converges in practice and provides reasonable solutions.The method to solve the power flow equations (22) requires less memory than the Newton–Raphson method, although it is typically slower.The fast decoupled method takes advantage of the fact that practical power transmission lines have a highXR-ratio (i.e., G ≪ B and G can be neglected), and in this situation the Jacobian (25) can be approximated by(31)h′≅(∂hP∂θ00∂hQ∂V).Substituting the initial equations (20) and (21) in the simplified Jacobian (31) it turns out that h′ is a constant matrix. The matrix thus has to be inverted just once at the beginning of the iterations.We finally mention that the AC power flow equations (22) can be formulated in rectangular coordinates as well. The solution techniques change then accordingly. The before mentioned overview Bienstock (2013) covers this aspect as well.The optimal power flow problem is the economic problem of minimizing the total production costs in the transmission network, while the given demand has to be met and satisfied. In order to reduce costs, the system operator may adjust the power production of different generator units. The optimal power flow problem is often stated in the form(32)minimizeinV,θ,PandQ∑g∈Gcg,P·Pg+cg,Q·Qg(33)subjecttoh(V,θ;P,Q)=(00),(34)|Sm,n|≤Smax,|Sn,m|≤Smax(35)Vimin≤Vi≤Vimax(i∈B),(36)Δθmin≤θi−θk≤Δθmax(i∈B),(37)Pgmin≤Pg≤Pgmax(g∈G),(38)Qgmin≤Qg≤Qgmax(g∈G).The nonlinear power flow equations are discussed in detail in the previous section, cf. (22). The line flow constraints (34) on the complex power Sn, minvolve real and reactive power on the line (n, m), which are the individual components (summands) in the sums (20) and (21) (cf. Sahraei-Ardakani, Korad, Hedman, Lipka, & Oren, 2014).These equations enter the optimal power flow problem as nonlinear equality constraints in (33). The power flow constraints are denotedh(V,θ;P,Q)=0to express that Pgand Qg, the net and reactive power injected at the generators g ∈ G, are additional variables, which are free variables in the optimal power flow problem. They represent the real and reactive power, which can be regulated by the system operator in order to minimize the total production costs with the restriction to satisfy the demand.Constraints (35) and (36) insure that the voltage level is met in all buses within a limited bandwidth and difference of phase angles, while constraints (37) and (38) control the power, and net power injected at the generator buses. These constraints (35)–(38) represent box constraints.In order to write the problem concisely it is comfortable to aggregate the variables again as(39)z:=((Vi)i∈G,(θi)i∈B,(Pi+jQi)i∈G).The comprised form of the optimal power flow problem (32)–(38) thus isminimizec(z)subjecttoh(z)=0,z∈B,where c is a general cost function. For ease of presentation we include the line limits (34) in the box constraints (34)–(38) and rewrite them as z ∈ B (cf. Footnote 2).The two-stage stochastic optimization problem we address here as a test case is taken from transmission switching and formulated as an optimal investment problem. We consider a transmission network under different demand loads (scenarios) ξ. These load scenarios represent future demands of the network considered. Based on these loads it is to be decided if it is beneficial to install an automated transmission switch, and where this switch should be located. Villumsen and Philpott (2012), from which the following statement is cited, describe the problem in further detail. “Note, that even though the fixed cost of enabling a line to be switched instantaneously may be small (e.g., if the switch is already present and only communication equipment needs to be installed) it may not be worthwhile to enable switching on all lines (unless this cost is 0 for all lines), since some lines may never be switched.”Installing a switch at the line y ∈ L (this is the outer problem) alters the network. The switch can be leveraged (inner problem) after scenario ξ has materialized. Once the demand Si(ξ) is realized at all nodes i ∈ B, the inner problem assigns the power generators with the objective to produce the energy as cheaply as possible. The inner problem thus consists in solving the optimal power flow problem (32)–(38) for every scenario ξ.The line y ∈ L, where the two-stage stochastic optimization problem(40)miny∈LEξ(minz∈Z(y)c(y,ξ,z))attains its minimum, has the highest cost savings on average for all the scenarios considered. This is the line where the new switch should be installed and solving (40) identifies this line.The cost function of the inner problem, as stated in (32), collects injected real and reactive power in a linear way,c(y,ξ,z)=∑g∈Gcg,PPg+cg,QQg.Notice that all components of z (cf. (39)) are random, i.e.,z=z(ξ): the components Pi(ξ) and Qi(ξ) are explicit under scenario ξ, while Vi(ξ) and θi(ξ) result from the inner optimization.Optimization problem (40) is discrete, and it can be solved, in principle, by inspecting the objective for each y ∈ L separately. Because of long computation times this strategy can only be considered for small networks. The related time amount explodes even, if not just one line is subject to switching, but two or even more lines,(y1,⋯,yn)∈Ln. The combinatorial problem of selecting the best combination of transmission lines cannot be solved by inspection and different solution techniques thus have to be considered. For this we address a heuristic in Section 5.2 below.The transmission networks for the test cases we consider are contained in Matpower (cf. Zimmerman and Murillo-Sánchez, 0000; Zimmerman, Murillo-Sánchez, & Thomas, 2011) and include test cases from Poland with a number of buses varying from 2 383 to 3 375 (see Table 2for some characteristics). They represent winter and summer, as well as morning and evening peak loads between 1999 and 2008. As well we have run the IEEE 11855cf. Matpower or the webpage http://www.ee.washington.edu/research/pstca/.bus network, which is more frequently addressed in the literature. We have chosen to present this test case here, as the results show similar patterns for many other grids.The literature follows several approaches to determine future scenarios. These approaches often can be distinguished between applications having long term, or short term horizons in mind. The scenarios in long term horizons typically reflect expert opinions on future developments. Villumsen et al. (2012), for example, follow this approach by choosing different future scenarios of the economy, each describing likely developments of the society 20 years ahead in time. The problem formulation (40) applies for both.Our application is short term. It tries to identify specific transmission lines, where profits may be expected if a switch device is installed such that the line can be switched off during some hours during the day at this point.To this end we base our scenarios on real load patterns (real and reactive power Pi, Qiat the buses i ∈ B), which reflect the power at some instants of time during a day and during a year. By taking a snapshot on hourly basis during a year a sample of size 24*365 is obtained, the dimension of each snapshot is given by the number of buses in the network (the dimension is 118 for the IEEE 118 test case, for example). The loads observed at all buses follow a multivariate empirical distribution.In this case study we simulate these scenarios by employing multivariate log-normal distributions,66This modification of the normal distribution ensures nonnegative outcomes.which are based on the following parameters:1.the mean of the observations follows the average load displayed in Fig. 1b;a (time depending) variance is employed at every individual bus to reflect its varying consumption pattern over time at this bus. The parameter is extracted from data provided by the British National Grid (cf. Electricity Ten Year Statement), and finallya covariance is imposed between the buses via a covariance matrix. A high correlation coefficient accounts for the fact that different buses show a similar consumption behavior. This is indeed the case, as individual households switch on their light, TV, the heating or air conditioner, e.g., at about the same time during a day. Energy consumers at different buses act in a co-monotone way and their loads thus are highly correlated.Theorem 5 justifies employing different probability measures to evaluate the objective. Further, Section 2.3 outlines that clustering methods can be employed to reduce the number of scenarios to be considered. We combine these results to reduce the number of scenarios in total to a reasonably small amount, which can be treated in further computations (cf. also Pflug & Pichler, 2014; 2015).Table 2 collects the Wasserstein distance of the original sample, compared to a reduced (clustered) sample by employing the weighted ℓ2-norm (weighted Euclidean norm,∥x∥2=1m∑i=1mxi2). The table displays the results for 5, 7, 10 and 100 representative scenarios.It is apparent from Table 2 that approximations with more representative points are better, they are closer to the original distribution. However, to improve the approximation quality considerably, significantly more representative points have to be accepted in the approximating measure. Increasing the number of representative points from 5 to 100, for example, improves the precision by less than 50 percent in all examples outlined in Table 2 (cf. Remark 3, the curse of dimensionality).The test case IEEE 118 has 186 lines. In its genuine setting provided by Matpower, 24 of these lines can be switched off by obtaining cost savings. The best line (line 104) leads to savings of about 0.4‰. Table 3a relates the behavior of the network IEEE 118 in this genuine setting with real distributions (indicated are the standard deviation and the correlation of the different distributions used to run (40)). The table shows a rather small dependence on the correlation ρ. Half of the lines, which lead to savings in the genuine scenario, still lead to savings under much higher standard deviation.The situation does not differ significantly for the Polish network 2 383, winter peak (Table 3b). A smaller correlation and higher standard deviation reduce the number of lines, which lead to savings in the stochastic case as well.We conclude from these tables that being a savings line is a stable property. Once a savings line is identified, then with high probability the line will provide savings in a stochastic environment as well, even if the standard deviation of the future distribution is high.Switching off a transmission line (i, k) ∈ L corresponds to setting the corresponding admittance to zero,Yi,k=0. It is hence possible to switch off the transmission line (i, k) by continuously sliding the admittance from Yi, k(the line in full, 100 percent service) down to 0 (the line being completely switched off). In this way the combinatorial problem (40) of finding transmission lines to be switched off can be studied by considering the continuous relaxation instead.Fuller, Ramasra, and Cha (2012) elaborate a heuristic to identify transmission lines, which offer a potential for savings when being switched off. The heuristic they propose is related to continuous switching, as it is based on ranking the dual variables (shadow prices) associated with transmission lines in full service. Their procedure (in a nutshell) selects those lines, which have a positive shadow price in full service and ranks them accordingly. In Fig. 2a and b, a positive shadow price corresponds to a positive slope for the line in full (100 percent) service.Fig. 2a displays production costs for the IEEE 118 bus test case, where every single transmission line is continuously faded out. Every line in the plot corresponds to a transmission line:(i)Production costs are apparently unbounded for two transmission lines. These two lines cannot be switched off, as some demand buses would be cut off from energy supply.Decreasing the admittance to 0 corresponds to increasing the impedance or the resistance. It is not to be expected that the graph corresponding to a transmission line is defined for every admittance between 0 (out of service) and 1 (full service), as this impacts feasibility. This is, however, the case for all lines in the IEEE 118 test case (Fig. 2a).Fig. 2b collects production costs for varying admittance of 7 selected transmission lines. These are the lines which show savings for reduced admittance. The total savings potential, however, is small, it is within a range up to 0.4‰ for the IEEE 118 test case. We have observed a total savings potential of less than 1‰ in many other test cases as well.Economic savings of the transmission switching problem are marginal in usual situations, the cost saving we observe are often less than 1‰ of the total production costs. The savings we observe are higher in (highly) congested networks (indeed, congestion management has been the main reason for line switching in the previous literature).Figs. 3–5expose continuous switching in different configurations. We display the results in a what-if analysis for 7 individual, representative scenarios obtained by scenario reduction, as outlined in Section 2.3. It is a repeated pattern that the curves corresponding to continuous switching are almost parallel for the different scenarios. We interpret this behavior by saying that the heuristic is a reliable indicator to predict savings lines, irrespective of the individual load.Fig. 3 displays continuous switching for the baseline scenario (thick, this is the load taken from the Matpower file) and under 7 scenarios after reducing the distribution as described in Section 5.1. While savings can be expected on the line 32 (Fig. 3a), production costs increase for the line 105 (Fig. 3b).Fig. 4 addresses line 32 (the 2nd-best individual line to achieve savings), which is already depicted in Fig. 3a. Displayed are now the results for independentlychosen scenarios (the correlation coefficient isρ=0) for varying variance (σ=10percent in Fig. 4a, andσ=0.1‰ in Fig. 4b).Fig. 5a finally shows the impact of correlation. As mentioned, a high correlation among nodes in the electricity network is realistic, as electricity demand in private households is of course correlated (lights are switched on in the evening, heating units are switched on in case it is cold, and air conditions are activated if it is hot).All plots in Figs. 3–5 show a common pattern. Irrespective of the correlation or the variance, and irrespective of the particular line, switching off a transmission line exposes a pattern, which is a characteristic of the transmission line itself. The particular scenario or load pattern in the network is not important, the important driving factor is the particular line in the network topology. This is the case for individually drawn scenarios, as well as for representative scenarios obtained by clustering according the Wasserstein distance as outlined in Section 2.Algorithm 1 and the scenario reduction based on Wasserstein balance each other. It is desirable to increase the number of scenarios to obtain a higher precision. We have seen in Table 2, however, that considerable improvements may only be obtained by choosing significantly more representative scenarios, the relation of the corresponding Wasserstein distance and the number of scenarios is extremely disproportionate. The curse of dimensionality (cf. Remark 3) imposes very restrictive limitations on increasing the number of scenarios, as the dimension of the problem is huge (see Table 2). More scenarios explode computation times, but Algorithm 1 is designed to accelerate the computation for similar scenarios.The predictor in Algorithm 1 (see step (iii)) does not help much for a small number of scenarios, as these scenarios are not similar in this case. The predictor reduces total computation time by a factor of approximately 5 if many scenarios are similar. In other words, the algorithm can handle 5 times more recourse problems (scenarios) in about the same time, compared to a usual Newton procedure. Parallelization can be used as well to reduce the total computation time of the decomposed problem (8), but again by not more than a factor, the number of parallel processors.Regarding complexity of the problem we mention as well that the algorithm is (at least) of orderO(n3),as n × n-matrices have to be inverted. In the case study, n is the number of buses in the network. Larger networks thus can handle onlyn−1/3scenarios in about the same time, which makes clustering an essential tool, and the explicit bound in Theorem 5 a valuable error bound.

@&#CONCLUSIONS@&#
