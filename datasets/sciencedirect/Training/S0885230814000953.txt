@&#MAIN-TITLE@&#
Robust continuous digit recognition using Reservoir Computing

@&#HIGHLIGHTS@&#
Study of robustness of Reservoir Computing (RC) based continuous digit recognizers.Discovery of new relations between RC control parameters, input and output dynamics.Use of these relations to find heuristics to reduce the reservoir development time.Creation of an RC-based recognizer that is more noise robust than the AFE-GMM-HMM.

@&#KEYPHRASES@&#
Reservoir Computing,Recurrent Neural Networks,Acoustic modeling,Automatic Speech Recognition,Noise robust spoken digit recognition,

@&#ABSTRACT@&#
It is acknowledged that Hidden Markov Models (HMMs) with Gaussian Mixture Models (GMMs) as the observation density functions achieve excellent digit recognition performance at high signal to noise ratios (SNRs). Moreover, many years of research have led to good techniques to reduce the impact of noise, distortion and mismatch between training and test conditions on the recognition accuracy. Nevertheless, we still await systems that are truly robust against these confounding factors. The present paper extends recent work on acoustic modeling based on Reservoir Computing (RC), a concept that has its roots in Machine Learning. By introducing a novel analysis of reservoirs as non-linear dynamical systems, new insights are gained and translated into a new reservoir design recipe that is extremely simple and highly comprehensible in terms of the dynamics of the acoustic features and the modeled acoustic units. By tuning the reservoir to these dynamics, one can create RC-based systems that not only compete well with conventional systems in clean conditions, but also degrade more gracefully in noisy conditions. Control experiments show that noise-robustness follows from the random fixation of the reservoir neurons whereas, tuning the reservoir dynamics increases the accuracy without compromising the noise-robustness.

@&#INTRODUCTION@&#
Despite many years of research, devising an Automatic Speech Recognition (ASR) system that correctly interprets a naturally spoken utterance captured in realistic conditions, is still a big challenge. Significant improvements are still needed before solutions will be available for voice driven applications with strict specifications such as high accuracy and robustness against confounding factors. In this work, we are concerned with continuous digit recognition (CDR). Although somewhat less elaborate than large vocabulary continuous speech recognition, CDR is important because many voice driven applications require the user to provide numeric data (telephone numbers, PIN-codes, account numbers, coordinates, etc.), and providing such information in the form of a spoken digit sequence is quite natural. However, since the sequences are often long, a very high accuracy is needed at the individual digit level to attain a high accuracy at the sequence level. Furthermore, many applications involving CDR must accommodate native as well as accented non-native speakers (e.g., tourists) communicating through mobile devices. The latter means that the speech exhibits a lot of channel variability and that it is often captured in a noisy environment (e.g., on the street).The CDR problem is commonly formulated as a statistical pattern recognition problem. First, an acoustic front-end converts the raw speech signal into a compact feature representation. Then, a back-end employs stochastic models to retrieve the most likely digit sequence given this representation. The feature representation consists of consecutive acoustic feature vectors representing short fixed length speech slices, called frames. In most cases, the back-end contains one stochastic model per digit, usually a left-to-right multi-state Hidden Markov Model (HMM), with each state having its own model for estimating the likelihood of an observed feature vector being generated by that state. If standard short time spectral features are employed in combination with acoustic models trained on low-noise training utterances, high accuracies (less than 1% of the digits wrong) can be achieved on similarly low-noise test utterances. However, severe degradations occur when the test utterances are noisy: more than 30% errors at a signal-to-noise ratio (SNR) of 10dB (Hirsch and Pearce, 2000). Consequently, a tremendous effort went into the development of robust speech recognition methods, in particular, techniques for making CDR more resistant to the presence of various types of noises at arbitrary SNRs.The research has led to a multitude of techniques that can roughly be categorized into four classes. First of all, there is a class of front-end methods that aim at retrieving ‘clean’ speech feature vectors from the observed noisy vectors so as to employ these cleaned vectors in an otherwise conventional system. Examples of this approach are described in Moreno (1996), Droppo et al. (2002), Chen and Bilmes (2007), Tamura and Waibel (1988), Cooke et al. (2001), Bengio et al. (1991) and Raj and Stern (2005). An alternative approach is to hold on to the raw noisy features, but to take the hypothesized impact of the noise into account during the likelihood computations in the back-end. Examples of such model-based approaches are Gales (1995), Gales and Young (1996), Sagayama et al. (1997), Acero et al. (2000), Droppo et al. (2002), Liao and Gales (2008) and Xu et al. (2011). These approaches are in general more powerful than the feature-based approaches, at the expense of being far more computationally taxing. A third option is to adjust the parameters of pre-trained models during actual operation on the basis of recognized outputs and confidence measures computed for these outputs. Examples of such model adaptation techniques can be found in Li et al. (2007), Li et al. (2009), Xiao et al. (2012) and Seltzer and Acero (2011). Finally, one can also substitute conventional GMM-HMM systems – in which state likelihoods emerge from state-dependent Gaussian Mixture Models (GMMs) – by hybrid systems encompassing neural networks that are trained to compute posterior state probabilities (Trentin and Gori, 2001). Given that no single class of methods is optimal in all respects, most of the solutions presented in the literature incorporate elements of multiple methods.The present paper builds on our former work on acoustic modeling for continuous speech recognition (phones as well as digits) by means of Reservoir Computing (RC), a technique originally introduced in Jaeger (2001, 2002). Although RC was already applied to isolated digit recognition some time ago (Verstraeten et al., 2005; Skowronski and Harris, 2007), we were the first (Jalalvand et al., 2011, 2012) to demonstrate that it offers noise robust CDR on an internationally accepted benchmark.Based on a novel analysis of reservoirs as non-linear dynamical systems, we gained new insight in how to tune the reservoir dynamics to the observable input dynamics, defined as the dynamics of the acoustic features, and to the anticipated output dynamics, defined as the dynamics of the variables that are derived from the reservoir outputs to represent the different speech units. By conducting artificial digit state recognition experiments, we could turn these insights into powerful heuristics which significantly facilitate the design of an appropriate reservoir system. The RC-based systems created in this way compete well with conventional systems in clean conditions and degrade more gracefully in noisy conditions. Control experiments show that the noise robustness mainly follows from the random fixation of the reservoir neurons but that properly tuning the reservoir dynamics is indispensable for combining this with a high accuracy in matched conditions.In the reminder of this paper, we first review the Aurora-2 experimental framework and discuss formerly proposed solutions to noise robust CDR that were assessed using this framework (Section2). Next, we introduce the fundamental principles of Reservoir Computing (Section3) and explain how to apply RC in a speech recognizer (Section4). Then, we elaborate and validate the novel reservoir analysis method and the reservoir design method that is derived thereof (Sections5 and 6). In Sections7 and 8, we provide compelling evidence of the good CDR performance of the designed systems. The paper ends with some conclusions.The Aurora-2 experimental framework (Hirsch and Pearce, 2000) was designed to bolster research on noise robust continuous digit recognition. In this section, we briefly introduce the framework and review a number of methods that have been evaluated using this framework.The Aurora-2 corpus consists of clean and noise corrupted digit sequences counting 1 to 7 digits per utterance. Each utterance is passed through a G712 or a MIRS filter, and then sampled at 8kHz (Hirsch and Pearce, 2000). The G712 filter has a flat response in the range from 300 to 3400Hz and is characteristic of a fixed line telephone connection. The MIRS filter exhibits a rising characteristic which is more characteristic of a mobile link (Hirsch and Pearce, 2000; ETSI-SMG, 1994). Since there are two variants of ‘0’ in American English, namely zero and oh, the vocabulary consists of 11 digits.The data is divided into a training part and an evaluation part. The framework supports two types of experiments: clean training experiments in which systems are developed on 8440 clean training utterances from 110 adults, and multi-style training experiments in which systems are developed on 8440 noise corrupted versions of the same utterances. The corruptions cover four noise types and five SNRs (∞ (clean), 20, 15, 10 and 5dB). The evaluation utterances come from speakers that are not present in the training data. They are divided into three test sets; Tests A and B each contains 28,028 utterances covering 4004 different digit sequences, 4 noise types and 7 SNRs (∞ (clean), 20, 15, 10, 5, 0, and −5dB). The noise types occurring in Test B do not occur in the multi-style training data, while those of Test A do. Test C contains 14,014 utterances covering 2002 different digit sequences, 2 noise types (one matched and one mismatched) and 7 SNRs. Unlike all other utterances, they are passed through a MIRS instead of a G712 filter (see Table 1).The abundance of papers on noise robust digit recognition makes it impossible to present an exhaustive review. Our aim is, therefore, to review methods that were tested on Aurora-2 and that, in our opinion, provide a good image of what state-of-the-art CDR systems can achieve. The mostly used error measure is the average word error rate (aWER) – defined as an average over SNRs 0, 5, 10, 15 and 20dB – obtained in clean training experiments.At the time the Aurora-2 campaign was launched, a baseline system was made available as a reference. It consisted of the ETSI standard front-end for generating the Mel-Frequency Cepstral Coefficients (MFCCs) proposed in Davis and Mermelstein (1980) and of a conventional GMM-HMM recognizer with 16-state whole word models embedding a GMM in each state. The models were trained using Maximum Likelihood Estimation (MLE). In a clean speech training experiment, this baseline system yields a poor performance in the presence of noise (Hirsch and Pearce, 2000): the aWER is nearly 39% (see Table 2).By means of utterance-based normalization of the MFCCs to zero mean and unit variance variables, it is possible to reduce the aWER to 20% with a negligible extra computational load (Xiao et al., 2010).By putting two additional blocks in front of the MFCC front-end, namely an adaptive Wiener filter applied to the raw speech signal and a voice activity detector (VAD) block performing some SNR dependent processing of the filtered signal, a large gain in noise robustness with respect to the raw MFCCs can be attained (ETSI, 2002). With this so-called ETSI advanced front-end (AFE), the aWER can be reduced to 13%.While front-end methods have shown improved performance on several tasks, they all, by definition, make point-estimates of the clean speech features. Errors in these estimations can cause further mismatch between the features and the acoustic model, resulting in degraded performance. Model adaptation techniques avoid this problem by directly compensating the probability distributions employed by the recognizer.One example of such an approach is vector Taylor series (VTS) adaptation (Moreno et al., 1996; Kim et al., 1998; Acero et al., 2000; Li et al., 2007) which improves the recognition in clean as well as in noisy conditions. As it is not fair to compare noise adapted models with models that originate from clean speech training alone, the VTS result is mentioned between brackets in Table 2.In Cooke et al. (2001), a so-called Missing Data Technique (MDT) was proposed. It detects cells of a spectrogram-like time–frequency representation that have become unreliable (or missing) due to noise masking. The values in these cells are then substituted by marginal values (Van Segbroeck and Van hamme, 2011). This technique has been continuously improved since its introduction, and an aWER of 11.4% is reported in Van Segbroeck (2010).Exemplar-based systems rely on the assumption that an arbitrary fragment of e.g., 30 frames from the test utterance of a digit can be represented as a sparse linear combination of suitably selected speech and noise fragments stored in a speech and noise dictionary, respectively. By retaining just the speech components from that combination, one can create an enhanced MFCC stream. Since its introduction in Gemmeke and Cranen (2008), the technique has continuously been improved in order to make it better and faster, and a general review is presented in Gemmeke et al. (2011). From Gemmeke and Van hamme (2012), we derive that in combination with acoustic models trained on clean speech only, the aWER for Test A+B (no results for test C) can be reduced to about 6%. However, this result cannot be compared to the result of a clean speech training experiment, because the feature enhancement is obtained with the help of a dictionary of noise samples.Traditionally, the parameters of an HMM are trained using MLE, but discriminative training schemes like Maximum Mutual Information (MMI) as proposed in Bahl et al. (1986) and Woodland and Povey (2002) can significantly outperform MLE. However, in Du et al. (2006) it is demonstrated that discriminative training only helps in matched conditions. Using the standard ETSI features, the discriminatively trained HMMs (D-HMM) do very well for clean speech, but they achieve only a minor improvement over the baseline system for noisy speech: the aWER is equal to 36%. We found no figures about D-HMMs in combination with the AFE, but there is no reason to expect a major improvement in that setting either.An appealing approach to the robust recognition, called uncertainty decoding (UD), is proposed in Liao and Gales (2008) and Cressie et al. (2009). In UD, there is a model for estimating the amount of uncertainty about the features. During decoding, this model is used to replace the actual observation by a distribution and to compute state likelihoods by integrating over the feature space. Using a model-based joint uncertainty decoding technique (JUD) the aWER can be reduced to 10% (Xu et al., 2011).Many research groups have conceived systems embedding a discriminatively trained neural network. The best results were obtained with a so-called tandem which considers the neural network outputs as a new type of acoustical features replacing the MFCCs in an otherwise conventional GMM-HMM. With a tandem employing a Deep Neural Network (DNN) as the neural component, an aWER of 21% was obtained (Vinyals and Ravuri, 2011; Vinyals et al., 2012).Even though the results obtained with neural network-based and discriminatively trained models discussed in the previous section were not so promising, we started to investigate another neural approach which is based on Reservoir Computing (Verstraeten et al., 2007). We did so mainly because RC offers an attractive way of taking the speech dynamics into account.The basic principle of RC is that information can be retrieved from sequential inputs by means of a two-layer Recurrent Neural Network (RNN) with the following characteristics (see Fig. 1). The first layer is a hidden layer composed of non-linear neurons which, at time t are driven by actual inputs Utand delayed hidden layer outputs Rt−1. The hidden neurons, also, have randomly fixed coefficients. The second layer consists of linear neurons which are driven by actual hidden layer outputs Rtand which have trainable coefficients. The recurrently connected hidden neurons can be imagined as a pool of interconnected computational neurons, excited by inputs. Such a pool is called a reservoir. Together with the linear output neurons, it forms a reservoir computing network. The network outputs Ytare usually called readouts (Jaeger, 2001) to differentiate them unambiguously from the reservoir outputs Rt.The reservoir can perform a temporal analysis of the input stream. In order to be effective, it should have the so-called echo state property (Jaeger, 2001). The latter states that, with time, the reservoir should forget the initial state it was in. This corresponds to the requirement that a linear filter should have an out-fading impulse response to be suitable for performing a meaningful short-term analysis of a non-stationary signal such as speech. As each reservoir output is the output of a non-linear filter with multiple inputs and as the filter coefficients are chosen randomly, a big enough reservoir will give rise to a large variety of filters. By training the readouts on speech, they will focus on the outputs of those filters that resonate to frequencies which are typical for the dynamics of the speech signal. This leads to the hypothesis that an RC-network can filter-out noise-inflicted dynamics (modulations) whose frequencies are a-typical for speech.If there are Nininput features and Noutreadouts and if the reservoir consists of Nresneurons, the reservoir network is governed by the following equations:(1)Rt=fres(WinUt+WrecRt−1),(2)Yt=WoutRt.The Nres×Ninmatrix Winand the Nres×Nresmatrix Wreccomprise the weights of the input connections and the recurrent connections, respectively. fres(·) is an activation function performing component-wise non-linear transformations of the neuron activations (e.g., fres(·)=tanh(·)). In order to compute Yt, we actually extend Rtwith a bias of 1. It can be shown (Jaeger, 2001) that the echo state property holds if the spectral radius ρ, defined as the maximal absolute eigenvalue of Wrec, is smaller than 1.In order to deal better with the random inter-frame changes observed in the inputs (e.g., due to the spectral analysis), one can introduce leaky integration by replacing Eq. (1) with(3)Rt=(1−λ)Rt−1+λfres(WinUt+WrecRt−1),with 0<λ≤1. In this case, the reservoir neurons are called Leaky Integrator Neurons (Jaeger et al., 2007) with a leak rate λ. All reservoirs in this paper employ neurons of this type. The dynamical behavior of the reservoir can be expressed in terms of two time constants:(4)τρ≐−τfrln(ρ)andτλ≐−τfrln(1−λ),with τfrbeing the time-shift (in ms) between frames. It will become clear that τρdetermines the memory capacity of the reservoir, whereas τλdetermines how smooth the readout patterns (i.e., temporal evolutions) are.Without any loss of generality, we assume that the reservoir inputs all have zero means and identical variances. In that case, drawing all the input weights from the same zero-mean normal distribution is the logical thing to do to ascertain that every input has the same chance of affecting the reservoir output. Therefore, we draw the input weights from a single distribution with varianceαU2and we call αUthe input scaling factor as it controls how strongly the inputs excite the reservoir. In a similar vein, we draw all recurrent weights from a zero-mean normal distribution with varianceαR2. In that case, the reservoir outputs are also bound to have similar distributions and a similar chance to excite the reservoir neurons. Obviously, αUand αRcan be used to control (1) the relative importance of the recurrent and the input connections and (2) the level of excitation of the reservoir neurons.As we will see later, instead of working with full weight matrices, we randomly select Kinentries per row of Winand Krecentries per row of Wrecand we only initialize those entries, leaving the other ones zero. This restriction reduces the amount of computations per time step. Unless Krecis close to 1, the squared norm of every row of Wrecis approximately equal toKrecαR2. This means thatρ2≈KrecαR2, and thus, Krecand ρ can be considered as the independent reservoir parameters instead of Krecand αR.The aim of the training is to find the output weights that minimize the mean squared difference between the readouts Ytand their desired values Dtacross Ntravailable training examples. Introducing the matrices R and D with columns Rtand Dtrespectively, the output weights are the solution of a regularized Tikhonov regression problem (Bishop, 1994):(5)Wˆout=argminWout1Ntr||WoutR−D||2+ϵ||Wout||2,with ϵ being the regularization parameter. The latter is intended to prevent over-fitting to the training data. The solution is obtained in a closed-form (Marquardt and Snee, 1975) as(6)Wˆout=(RRT+ϵI)−1(DRT),with I representing the identity matrix and A−1 the Moore–Penrose pseudo inverse of A (Penrose, 1955).In our experiments, the desired output Dtis always a unit vector referring to the desired HMM-state at time t (see Section4). Furthermore, we observed that including a regularization term did not improve our results. Hence, we did not include such a term, meaning that we set ϵ=0.An RC-network can be considered as an extension of the Extreme Learning Machine (ELM) proposed in Huang et al. (2006, 2010). The ELM is defined as a feed-forward neural network (a Multi-Layer Perceptron or MLP) with a randomly fixed hidden layer and a linear output layer whose weights are fixed to minimize the mean squared difference between the computed and the desired outputs. In Huang et al. (2006), it is mathematically proven that the ELM is as powerful as a fully trained MLP: a system with N hidden neurons can learn exactly N distinct observations. Moreover, it is shown that the ELM generalizes best to unseen data because it employs the least square solution of the Tikhonov regression problem. The various experiments presented in Huang et al. (2006) lead to the following conclusions: (1) in terms of generalization ELM behaves as well as a Support Vector Machine (SVM) employing a linear kernel (Cortes and Vapnik, 1995), and much better than a fully trained MLP, (2) an ELM is much more compact than an SVM (it needs less hidden neurons than an SVM needs support vectors) and (3) the generalization performance of an ELM remains stable over a wide range of hidden units. We argue that good generalization to test data should transfer to good noise robustness. Moreover, we expect that introducing recurrent connections which are also randomly fixed, is bound to maintain the noise robustness while improving the model accuracy. Adding these arguments to the formerly mentioned noise filtering capacity of an RC-network, we have enough reasons to believe in the high potential of RC for noise robust CDR.An RC-network also resembles an SVM, but one with a hidden space whose size and identity do not follow from a long and delicate supervised training process. Likewise, it resembles a radial basis function (RBF) network using a fixed hidden layer. However, the RBFs often follow from a clustering procedure and they typically represent local functions in the input feature space, meaning that they only react to inputs that fall in a restricted area of the input space. A reservoir neuron, on the other hand, typically exhibits a non-local activation function. Finally, an RC-network resembles the Recurrent Neural Network (RNN) applied in Robinson (1994) for continuous speech recognition. However, the memory of that network originated from feeding the outputs back to the hidden layer and, importantly, all the network weights were trained by means of back-propagation through time, a method that is found to be very time consuming and likely to yield a sub-optimal solution when the size of the network is large. In an RC-network, the optimal weights are found in a straightforward manner, even for a reservoir with several thousands of neurons.The architecture of an RC-HMM hybrid for CDR is depicted in Fig. 2. The front-end generates acoustic inputs Utand the readouts Ytare converted to state likelihoods (see further) before they are supplied to a looped HMM that models the digits with multiple states and the silence with a single state. There is only one transition probability, Po, namely on the transition from the final state to one of the two possible initial states. It is used to control the balance between digit deletions and insertions.The joint likelihood of the HMM state sequence Q=q1, …, qTand the input stream U=U1, …, UTis computed as(7)P(Q,U)=PoNQ∏t=1TP(Ut|qt),with NQbeing the number of digits implied by Q. In analogy with Richard and Lippmann (1991), one can show that if the training corpus were infinitely large and the targets during training were either 0 or 1, readout Yt,qwould be equal to P(q|Ut) and Bayes’ law could be applied to convert it to a likelihood. However, in case of a finite training set, the readouts only approximate the posteriors and they can be outside the interval [0,1]. To fix that problem, we introduce a mapping function, fmap(·) and compute the likelihood as follows:(8)P(Ut|q)=fmap(Yt,q)P(Ut)P(q).One way to define the mapping is to create two histograms for each state q: one representing the global distribution of Yt,qand one representing the distribution over the times that state q is visited. From these histograms, one can then derive a lookup table for estimating P(q|Yt,q).A solution with fewer free parameters is a sigmoid function with a steepness gqand an offset bqderived from the estimated P(q|Yt,q):(9)fmap(Yt,q)=11+e−gq(Yt,q−bq).Two other alternatives are a state-independent sigmoid and a simple clip-and-scale approach. The latter is given by(10)fmap(Yt,q)=max(Yt,q,Yo)maxj(Yt,j),Yo≪1.In Section7, we experimentally test all four approaches as they represent different trade-offs between model detail and model generalization.Even though the optimal output weights of the reservoir network can be obtained in a closed-form, the embedded training of an RC-HMM hybrid is an iterative process. And since the Aurora-2 corpus is delivered without timing information we even adopt a two-stage training procedure.In the first stage, only isolated digit utterances are considered. They are first segmented into silence–digit–silence by means of an energy criterion and the digit intervals are uniformly divided into digit state segments. The matrix D derived thereof is employed to train an initial readout layer. Then, three iterations of embedded training are conducted. Per iteration, the outputs of the available RC network are used in a Viterbi-alignment of the training utterances with their silence–digit–silence models. From these alignments a new matrix D is computed and substituted in Eq. (6).In the second stage, all available training utterances are considered. They are modeled as sequences of digits interleaved with optional silences. Starting with the alignments emerging from the system of the first stage, the training is continued until the digit error rate on a validation set saturates.In terms of training effort, one can note that the reservoir is fixed throughout the training, meaning that the matrix(RRT+ϵI)−1appearing in Eq. (6) can be computed once, during the first iteration. The matrix DRThowever changes from one iteration to the other because D changes. Even though R does not change across iterations, it is impractical to store it for a big reservoir and/or a large dataset. Therefore, we compute and store(RRT+ϵI)−1in the first iteration, but recompute the reservoir outputs during each iteration.From Section3 it follows that a reservoir is governed by six control parameters: Nres, Kin, αU, Krec, ρ and λ. However, we argue that the reservoir size is mainly determined by the number of available training examples and that the other parameters should follow from properties of the reservoir inputs and the desired readouts, irrespective of the number of training examples.In this section, we analyze the RC-network as a dynamical system and we postulate some fairly general and comprehensible principles which lead us to the following conclusions: (1) Kinand Krecare non-critical and are easy to set and (2) it is possible to establish for any combination (ρ, λ) a closed-form expression that provides a near-optimal value for the input scaling factor αU. In the experimental section, we will use the insight gained in the current section to translate empirical findings into a very simple recipe for tuning the reservoir dynamics to the observable input dynamics and the desired output dynamics.In order to develop our strategy, we write the input–output equations for reservoir neuron i in the following form:(11)rt,i=(1−λ)rt−1,i+λfres(at,i),(12)at,i=bt,i+ct,i,(13)bt,i=∑j∈JiWijinut,j,(14)ct,i=∑k∈KiWikrecrt−1,k.The setsJiandKicollect the inputs and reservoir outputs effectively driving neuron i. The symbols ut,jand rt,irefer to components of Utand Rt, whereas at,i, bt,iand ct,irespectively refer to the total activation of the neuron and to its input and recurrent components. The equations are visualized in Fig. 3.As the readouts approximate posterior probabilities of the HMM states, and as each state is only visited during short time intervals, a good readout should only become high during short time intervals too. This means that the power spectrum of such a readout must be concentrated in a frequency band from 0 to some frequency F=1/T, where T is the average state duration. We call F the bandwidth of the desired readouts and consider it a basic characteristic of the output dynamics.Based on F, we define the in-band activation pattern of a reservoir neuron as the pattern originating from the activation components with frequencies inside (0, F), and we postulate that a reservoir neuron can maximally contribute to the solution of an envisioned recognition problem, if its in-band activation pattern has a preferred strength. In fact, if the activation strength is too small, the non-linearity of the neurons will not be exploited and if it is too large, the neurons will saturate too frequently and become insensitive to input changes in these cases. We, actually, expect that there exists a preferred in-band activation strength that works well for all reservoirs and for an arbitrary bandwidth of the readouts. We further contemplate that variance is a good measure of activation strength.As described in Section3, Winhas Kinnon-zero entries per row and the values of these entries are drawn independently from a zero-mean Gaussian distribution. Consequently, if Kinis sufficiently large, the mean of the input activation component of a neuron will approximately be zero, even if the inputs have non-zero means. Exactly the same holds for the recurrent activation component. Introducing the short notation Et,i[··] for a mean over t and i, the mean variances of the input and the recurrent activation components are given by(15)Vb≐Et,i[bt,i2]=αU2KinVU,(16)Vc≐Et,i[ct,i2]=αR2KrecVR=ρ2VR,where, VUand VRare the mean variances of the reservoir inputs and outputs, respectively. Since the strength of an in-band activation component is proportional to the strength of the full activation, the above formulas imply that one can freely change Kin(or Krec) provided the scale αU(or αR) is updated accordingly to maintain Vb(or Vc).Measurements show that there are only very weak correlations between bt,iand ct,ias well as between their in-band componentsbt,i*andct,i*. Therefore, we can assume that Va≈Vb+VcandVa*≈Vb*+Vc*.Given an optimal value VoptforVa*and given some combination (λ, ρ), we will now derive an analytical formula for finding the corresponding optimal αU.First of all, we relateVb*to Vb. To do so, we need the mean power spectrum |B(f)|2 of the input activation. By applying Parseval's theorem, we have(17)Vb*=ϕb(F)Vb,(18)ϕb(F)=∫−FF|B(f)|2df∫−0.50.5|B(f)|2df.As |B(f)|2 is independent of the reservoir parameters other than Kin, it can be measured using an arbitrary reservoir with the desired Kin.Now, we try to relateVc*to Vc. This is much more complicated due to the non-linearities involved. Therefore, in order to derive an approximate relation, we make two assumptions: (1) the activation function fres(x) is linear and equal to x, and (2) the recurrent activation component is equal to ρrt−1,i. Strictly speaking, the linearity assumption is in contradiction with the requirement that the in-band activation should be large enough to let the non-linearity play some role. However, since we also stated that this role has to remain modest, we expect that results emerging from the linearity assumption will remain valid for a soft and compressing activation function such tanh. The assumption that ct,i=ρrt−1,ishould be seen in the context of Eq. (16) which says that the expected variance of ct,iis equal to ρ2 times VR. The proposed ct,i, at least, has this variance and it makes the analysis tractable. The effect of just feeding the neuron output back into the neuron rather than a weighted sum of other neuron outputs is of course difficult to assess, but the experimental validation will confirm that our assumption leads to acceptable results.Applying Parseval's theorem again, we find thatVc*is proportional to Vcwith a factor that is a function of the power spectrum |Cλ,ρ(f)|2 of ct,i:(19)Vc*=ϕc(F,λ,ρ)Vc,(20)ϕc(F,λ,ρ)=∫−FF|Cλ,ρ(f)|2df∫−0.50.5|Cλ,ρ(f)|2df.Obviously, as |Cλ,ρ(f)|2 depends on λ and ρ, it cannot be measured using an arbitrary reservoir anymore. However, it can be derived from the closed-loop transfer function of the neuron (as a linear system). This function is the product of two first-order transfer functionsHρ(z)=11−ρz−1andHλ(z)=λ1−(1−λ)z−1.Consequently, we obtain that(21)ϕc(F,λ,ρ)=∫−FF|Hλ(f)|2|Hρ(f)|2|B(f)|2df∫−0.50.5|Hλ(f)|2|Hρ(f)|2|B(f)|2df.By sticking to the linearity assumption and to the equality Va=Vb+Vc, it is easy to verify that(22)VR=∫−0.5+0.5|Hλ(f)|2|B(f)|2df+∫−0.5+0.5|Hλ(f)|2|Cλ,ρ(f)|2df.Taking into account that |Cλ,ρ(f)|2 has a smaller bandwidth than |Hλ(f)|2 for any (λ, ρ), we argue that the second integral in Eq. (22) is approximately equal to the integral of |Cλ,ρ(f)|2 alone, and this latter integral is equal to Vc(Parseval). Consequently,(23)VR=ϕ(λ)Vb+Vc,(24)ϕ(λ)=∫−0.5+0.5|Hλ(f)|2|B(f)|2df∫−0.5+0.5|B(f)|2df.Combining this result with Eq. (16) leads to a relation between Vcand Vb:(25)Vc=ρ2VR=ρ2ϕ(λ)1−ρ2Vb.Putting everything together then leads to the following relation betweenVa*and Vb(or conversely αU):(26)Va*=(1−ρ2)ϕb(F)+ρ2ϕc(F,λ,ρ)ϕ(λ)1−ρ2VbIn the following section, we will discuss how to exploit this relation during the design of a good reservoir.We propose to fix Kinand Krec(almost free choice, as will be demonstrated) and to estimate the expected state duration T (based on knowledge about the task), first. Then, conduct two small-scale experiments to determine |B(f)|2 and Vopt:1.Estimate the power spectrum |B(f)|2 of the input activations by means of a memoryless reservoir (meaning that (λ, ρ)=(1, 0)) that is just big enough to ensure that the recurrent weight matrix is sparse for the chosen Krec.Test networks with reservoirs with the same size and reservoir dynamics but with different values of αU. Establish the αUoffering the highest classification accuracy and use Eq. (26) to derive Vopt. The size of the reservoir is not critical (just take it large enough to get at least a reasonable performance).After these experiments one can start to design the envisaged (large) reservoirs by trying different combinations (λ, ρ) and by deriving the optimal αUgoing with this combination from(27)αU2KinVU=(1−ρ2)Vopt(1−ρ2)ϕb(F)+ρ2ϕc(F,λ,ρ)ϕ(λ).(derived from Eqs. (15) and (26)). Obviously, the design time is then going to be proportional to the number of combinations (λ, ρ) to consider. In the experimental section, it will be demonstrated that we can unequivocally tune the reservoir dynamics to the recognition problem, which eliminates the necessity of considering more than one combination (λ, ρ).The aim of this section is to seek experimental validation of the proposed strategy and to establish the Voptto impute in Eq. (27).In this stage, we conduct different digit state recognition experiments within the Aurora-2 context. The aim is to define tasks implying different output dynamics and different numbers of readouts. Therefore, we consider left-to-right digit models of different lengths (different number of states S per model), we label the states and perform digit state recognition by replacing the digit loop in Fig. 2 by a digit state loop that can generate an arbitrary state sequence.By changing S, one changes the output dynamics (F) and the number of readouts (11×S digit states plus one silence state). From a histogram of the digit durations, we derive that most digits are longer than 250ms. We, therefore, choose F=S/250 as the readout bandwidth (in kHz). Note that if S is low, each state may encompass acoustically diverse speech frames which are hard to cluster in the acoustic feature space, whereas if S is high, the states represent acoustically more similar speech frames. Due to this, a larger S does not necessarily lead to a more difficult task.In each experiment, an RC-HMM hybrid is trained on two-thirds of the clean training utterances and validated on the remaining third of these utterances. The state error rate (SER) on the validation set is used as the evaluation measure. It is obtained by counting the minimum number of state deletions, insertions and substitutions that transform the recognized state sequence into one that is compatible with the spoken digit sequence, given the left-to-right and no-skip models of these digits. As before, a penalty Pois employed to control the deletion/insertion balance.In all experiments, the 39 acoustic features per frame are generated by a standard MFCC analysis (30ms Hamming-windowed frames, frame shift of 10ms) yielding 12 Mel-cepstral coefficients, a log-energy and the first and second order derivatives thereof. The feature extraction is followed by an utterance-wise normalization that creates zero-mean and unit-variance inputs (VU=1). The ultimate mean and variance normalized features are denoted by the acronym MVN.Unless stated otherwise, we employ a lookup-table (see Section4) to map the readouts to the interval [0,1] and we choose S=3 as a default.If variance were a good measure of activation strength then the theory would predict that the SER is independent of Kinand Krecas long as the variances Vband Vcare maintained.To verify this hypothesis for Kin, we test a reservoir with Krec=0 for three values of Vb(0.01, 0.02 and 0.05) and another one with (ρ, Krec, Vb)=(0.8, 1, 0.02). To verify the hypothesis for Krec, we test a reservoir with (Kin, Vb)=(10, 0.02), and three values of ρ (0.1, 0.5 and 0.8). Note that we verified experimentally that as long as ρ is not too close to 1, keeping ρ constant is equivalent to keeping Vcconstant. In all experiments, we set λ=1.Fig. 4largely supports the envisioned predictions. There is only one strong exception, namely the case that the neurons have no recurrent connections and are driven by only 1 or 2 inputs. In that case the rows of the input matrix no longer have the same norm as assumed in the theory. From now on, we always work with Kin=Krec=10.Let us now establish whether there exists a single value forVa*that is quasi-optimal for different combinations of (λ, ρ, F). We, first, keep F fixed and examine different combinations (λ, ρ). For each of them, we determine the optimal Vband compute the corresponding optimalVa*from Eq. (26). In Table 3, one finds these values for a reservoir with 750 neurons. The table suggests that the optimalVa*depends on λ and to some extent also on ρ. However, Fig. 5shows that for small values of λ, the interval of quasi-optimal Vb's becomes wider and thatVa*=0.035(the circles in Fig. 5) always leads to a quasi-optimal SER. This means that Eq. (26) is not significantly violated and that it offers an acceptable means of expressing the relation betweenVa*and Vbat one particular F.Let us now check if Eq. (26) remains suitable when F is changed. Therefore, we consider RC-HMM hybrids for state recognition tasks corresponding to S=1, 3 and 5. We employ reservoirs with (Nres, λ, ρ)=(750, 0.33, 0.8) and different values of Vb, namely those values for which Eq. (26) yields values ofVa*that are equal to 0.01, 0.025, 0.035 and 0.05. According to the results listed in Table 4, the value of 0.035 forVa*, that was established for S=3 remains a good value here too, even though it yields a slightly suboptimal result for the not so interesting case S=1.The conclusion of our experiments is that Eq. (27) leads to a good approximation of the optimal Vbin all cases, provided Vopt=0.035. This value corresponds to a standard deviation of 0.19. Presuming a Gaussian distribution for the in-band activation, this means that 5% of the in-band activation samples are larger than 0.375, ensuring that the non-linearity is effectively starting to play, as anticipated.In this section we develop RC-HMM hybrids for CDR. They comprise one basic RC-network, designed with Kin=Krec=10 and Vopt=0.035. The size of the reservoir, the number of states per digit and (ρ, λ) are the dependent parameters.During system development, hybrids are trained on two-thirds of the clean training utterances of Aurora-2 and validated on the remaining clean training utterances. The evaluation measure is the WER. We perform experiments to set the reservoir dynamics, to decide about the readout-to-posterior mapping and to investigate the effect of Nresand S on the WER.In order to derive practical heuristics for finding good dynamic parameters, we first consider a reservoir of 750 nodes with S=3 and we assess the validation WER as a function of τλand τρ(see Eq. (4)).Fig. 6supports the notion of τρand τλbeing two independent parameters to tune the reservoir to the task it is intended for: the optimal value of one of them is the same in an broad range of values for the other. Therefore, we propose to proceed as follows: (1) track the WER as a function of τλfor reservoirs with τρ=50ms (corresponding to ρ=0.82) and determine the optimal τλ, (2) track the WER as a function of τρfor reservoirs with τλ=100ms and determine the optimal τρ, (3) try to link the optimal values to the dynamical properties of the task in the hope to find a good and simple heuristic. Note that steps (1) and (2) can be run in parallel.Fig. 7shows the two tracks for three values of S. The left track reveals that leaky integration can significantly reduce the WER. Its positive effect is maximal if its time constant reaches a certain point that is clearly a function of S, and conversely, of the output dynamics. The data support the expectation that leaky integration can suppress fluctuations in the reservoir outputs that originate from random fluctuations in the acoustic features. In particular, they are consistent with the heuristic to fix τλto the average digit state duration T. The right track reveals that recurrence is a powerful tool for improving the performance, as well. The optimal point seems to be much less dependent on S. This agrees with our expectation that the memory of the reservoir should be adapted to the dynamical properties of the reservoir inputs, and consequently to the bandwidth FBof the power spectrum |B(f)|2. Since that spectrum is a low-pass spectrum, FBcan be defined as the frequency where |B(f)|2 drops below its half maximal value. The optimal τρ=50ms then follows from the heuristic that τρ(ms)=0.35/FB(kHz) where FB(kHz) is obtained by converting the normalized frequency FB(running from 0 to half the frame rate of 100Hz) to kHz. Note that the just formulated heuristic is exactly the relation between the bandwidth of a first order linear system and the time constant of its impulse response. From now on, we use this relation to fix τρ.Table 5lists the WERs obtained with a reservoir of 750 nodes for four readout-to-posterior mapping methods. The differences between methods are modest but nevertheless, the difference between the state-specific lookup-table method and the two global methods is almost statistically significant (Wilcoxon test, p<0.10 (Wilcoxon, 1945)). We, therefore continue to use the lookup-table method.Fig. 8shows the WER on the validation set as a function of the number of states per digit for different sizes of the reservoir. The WER decreases with the reservoir size, even though a reservoir of 16K neurons already encompasses 1.7M trainable parameters, whereas, there are only 0.8M training frames. This indicates that the risk of over-training the readouts is low, a conclusion that confirms our earlier findings (Triefenbach et al., 2013). Our data suggest that even bigger reservoirs could offer further improvements. However, we did not try that here because, as we will explain later on, the training of systems with very big reservoirs becomes impractical on standard computers and the anticipated improvements can be achieved more effectively by systems encompassing multiple reservoirs of manageable sizes.The WER decreases with the number of states until it saturates at S=7. Consequently, there is no need to use more than 7 states per digit.During system evaluation, the output weights of the reservoir network are trained on the full training set using the same reservoir parameters and number of iterations that were employed in the development stage. Furthermore, both clean and multi-style training are considered now. All systems are evaluated on all the Aurora-2 test sets (A–C).In Table 6, the WERs of eight RC-HMM hybrids are listed next to the WERs of two baseline GMM-HMM systems: one that employs the same acoustic features (MVN) and one that works on the advanced front-end features (AFE).The RC-HMM systems encompassing a reservoir of 16K or more neurons clearly outperform the baseline GMM-HMM systems in mismatched conditions, and are competitive with a GMM-HMM system working in matched conditions on the same acoustic features. However, the GMM-HMM working with AFE's still stands out in the clean test condition.The RC-HMM data in Table 6 confirm that the improvements due to more reservoir neurons and digit states are in line with those observed during system development. Although further enlarging the reservoir is bound to lead to further improvement, we did not pursue this path because the required computational resources increase steeply then (see Section7.4) and also because multi-layer RC-HMM systems will turn out more effective in this respect (see Section8).So far, we have established that RC-based systems offer more noise-robustness than GMM-based systems, but we did not yet establish why that is. Is it due to the ability of the reservoir to filter-out noise inflicted modulations whose frequencies fall outside the speech band (between 0 and F), or it is due to the random fixation of the reservoir weights? To answer these questions, we have performed control experiments with systems encompassing an 8K reservoir and 7 state digit models.In Fig. 9, we depicted the WER as a function of the SNR of the test sets (A–C) for the GMM-HMM (MVN) system and for four RC-HMM systems which are labeled by their τλand τρ(in ms). System RC-00-00 is a system incorporating a reservoir without recurrent connections and with memoryless neurons and system RC-35-50 is the one with the optimal dynamics. The figure clearly shows that the noise degradation curve of the GMM-HMM system is steeper than that of the RC-HMM hybrids and that the RC-HMM curves have a similar steepness. This means that the noise robustness (slope of the curve) follows from the random fixation of the reservoir weights. Good reservoir dynamics on the other hand significantly improve the accuracy. The same conclusions also hold for multi-style experiments.In order to give the reader a concrete idea about the time needed to train a reservoir with Nresnodes and Noutreadouts on Ntrtraining frames, we decompose the training into six actions and assess the expected number of operations for each action (see Table 7). As a reference, we also give actual times (in minutes) consumed by each action on a 3.4GHz single core CPU for the case of a reservoir with 16K neurons, 78 readouts (S=7) and training on the complete Aurora-2 training set (≈ 1.5M frames).From the figures one can derive that the computation of RRTin the first iteration is by far the most time consuming part. Fortunately, it is also the easiest part to run in parallel on multiple cores. Pruning in the Viterbi alignment can reduce the time consumed in the further iterations.Research on Deep Neural Networks has shown that the performance of a neural network can be increased by stacking multiple hidden layers on top of each other. However, since in the case of RC the hidden layer (the reservoir) is not trained, it does not make much sense to stack reservoirs. Instead, we stack complete reservoir networks (reservoir + readouts) as shown in Fig. 10. Each reservoir network, from now on called a layer, is driven by the readouts of the preceding layer or by the acoustic inputs (layer 1) and the layers are trained one by one. There is already empirical evidence from phone recognition (Triefenbach et al., 2013) that cascading reservoir networks can improve the recognition.Since we see no advantage in doing it otherwise, we consider the digit states as the training targets in each layer. Consequently, each layer has the same number of readouts. However, as will be illustrated in Section8.1.3, the readouts driving the higher layers are smoother than the acoustic features driving the first reservoir. This means that the higher layer reservoirs need other parameters than the first layer reservoir. Another difference is that the task of the higher layers is to adjust the hypotheses of the former layer, whereas the task of the first layer is to bridge the big gap between the acoustic features and the state hypotheses.Another point is that the readouts of the first layer no longer have zero means and unit variances. However, this is not so problematic, because the zero mean input weights will still cause the input activations to be zero mean. Furthermore, since each digit state has the same prior probability, one can assume the corresponding readouts to have an equal variance that can be substituted as VUin Eq. (15).Since the inputs of the higher layers are much closer to the target readouts than those of the first layer, we experimentally verified whether the formerly found preferred in-band activation strength of 0.035 still is an acceptable value. This fact can be interpreted as an indication that the preferred activation strength may be problem independent, as hypothesized.To verify whether our heuristics for finding the reservoir dynamics also hold for the higher layers, we repeated the experiment that gave rise to Fig. 7, but now we just did it for 7 states per digit. The results in Fig. 11support our claim about the purpose of leaky integration. As the readouts exhibit less random fluctuations than the acoustic features (as illustrated in Section8.1.3), the choice of τλis not that critical anymore. Any value not exceeding the average digit state duration is acceptable now. The heuristic for τρnow yields a value of 130ms which also seems to be a suitable value. Actually, this need for more memory agrees with the utilization of longer windows in the higher layers of other hierarchical systems, such as the one proposed in Pinto et al. (2011).Although this is not necessary, we stick to multi-layer systems with equally large layers for the time being. This is partly for convenience but also because equal layers yield the minimal training time for a given number of trainable parameters: the number of trainable parameters is proportional to the sum of the reservoir sizes whereas the training time is proportional to the sum of squares of the reservoir sizes. Note that in the recognition phase, the computational complexity is determined by the total number of trainable parameters, irrespective of the number of layers involved.To create our deep architectures, we use the same design choices for layers 2 and higher. This means: Vopt=0.035, τλ=T and τρ=130ms. The results depicted in Fig. 12support the following conclusions:1.Any single layer system can be improved by adding extra layers, but the attainable improvement decreases when the reservoir size increases. Nevertheless, even for a reservoir size of 16K, the attainable improvement is still as large as 30% relative.For a given number of trainable parameters, multi-layer systems outperform single-layer systems, but again, the improvement decreases when the reservoir size increases. In fact, the gain of a 4×4K system over a 1×16K system is about 20% relative, whereas the gain of a 3×8K system over a single-layer 24K system has already dropped below 8%.The larger the reservoirs become, the fewer layers are needed to reach the asymptotic performance of multi-layer systems using that reservoir size.Normally, the performance curve as a function of the number of layers is convex, but for the case of 16K reservoirs this is no longer the case. The latter probably indicates that some over-training has finally occurred. This is not so surprising since there are already 1.24 million trainable parameters per layer in that case.To provide a more qualitative impression of what the different layers achieve, we considered a clean example of digit zero and we plotted some of the normalized MFCCs as well as the readouts of the subsequent layers that correspond to the subsequent digit states (Fig. 13). It is clear that the readouts of all layers are smooth functions of time and that the competition between the desired readout and the other readouts diminishes when traversing the layers.Following the development experiments, we tested multi-layer systems with 8K and 16K reservoirs. As before, we retrained the systems on the full training set, but of course using the control parameters established during the development phase.The results for 8K (see Table 8) demonstrate that in matched conditions adding layers leads to significantly better results at the price of a negligible loss in performance in mismatched conditions. In the clean training experiment, only the clean condition can be considered matched, be it that Test C already introduces some mismatch (MIRS instead of G712 filter) in that condition as well. In the multi-condition training experiment, both the clean and the 0–20dB conditions can be considered matched as most of the SNRs involved were also present during the training. The findings for the matched conditions are in very good agreement with the results emerging from the development phase.The results for 16K (see Table 8) demonstrate more or less the same tendencies, but the differences are smaller. Note that just like in the development phase, the improvement (in matched conditions) caused by the second layer is much smaller than that caused by the third layer.Out of curiosity we also evaluated some additional systems composed of layers of different sizes. Two typical examples are listed in the last two rows of Table 8. They show that in case of a big first layer, the correction work of the higher layers can be accomplished by smaller layers (16K+4K+2K works as well as 3×16K) which does not come as a surprise. When the first layer is small, the corrections to make in the higher layers are obviously more difficult to make and the 2K+4K+16K system performs significantly worse. However, we did not investigate to what extend this is due to the fact that the outputs of the first layer now have a larger bandwidth, which calls for other reservoir parameters than the ones we derived from the outputs dynamics of a first layer comprising a big reservoir.Recalling the significant gain obtained by replacing MVN features by advanced front-end (AFE) features in a GMM-HMM recognizer, we were curious to examine the effect of doing the same in an RC-based recognizer. The results for a 3×8K system driven by AFEs are listed in Table 9.Apparently, removing some of the noise effects in the front-end also helps an RC-based system to some extend (in the clean training experiment for instance, the aWER is reduced from 11.5% to 9.9%). However, its effectiveness is much smaller than in GMM-HMM systems.

@&#CONCLUSIONS@&#
We have studied reservoir based acoustic modeling for noise robust continuous digit recognition. A reservoir based acoustic model computes the state likelihoods in an HMM by means of a two-layer recursive neural network. This network is peculiar in the sense that it consists of a hidden layer of recurrently connected non-linear neurons with fixed (i.e., non-trained) coefficients – called a reservoir – and an output layer of linear neurons with trained coefficients which ‘read out’ the outputs of the reservoir.A major contribution of the paper is the introduction of a novel analysis of the reservoir as a non-linear dynamical system, an analysis that also offers insight in the behavior of the system. Based on that insight, empirical findings are translated into a few very simple and comprehensible rules which permit anyone to design a reservoir system that is properly tuned to the dynamical properties of the reservoir inputs (can be measured) and the dynamical properties of the expected outputs (can be estimated from knowledge of the recognition problem). These simple rules made it feasible to investigate the performance of RC-based systems as a function of reservoir size and number of states per digit in a systematic way.The main objective of our work was of course to demonstrate that Reservoir Computing can lead to robust acoustic models. In a first step, we developed a single layer RC-based continuous digit recognition (CDR) system, driven by mean and variance normalized MFCC features. From experiments on Aurora-2 we established that this system is indeed significantly more robust against the presence of noise than a traditional GMM-HMM system working with the same acoustic parameters, whilst it is competitive in the clean condition. Under noisy conditions, the RC-HMM system even outperforms a GMM-system working with the advanced front-end features, but the latter system still stands out under clean conditions.The experimental data presented in the paper also reveal that noise-robustness mainly follows from the random fixation of the reservoir weights. Properly tuning the reservoir dynamics mainly raises the accuracy in matched conditions without compromising the noise-robustness.Motivated by the success of single-layer systems, we also considered multi-layer systems comprising multiple reservoirs. We demonstrated that such systems can further improve the results for the matched test conditions, again without compromising the noise robustness in the noisy conditions. Our best multi-layer systems are now competitive with the AFE-GMM system in clean conditions as well. Our present system outperforms all other neural-based approaches we know of that were recently evaluated for continuous digit recognition. A particular advantage of reservoir based models seems to be their robustness against over-training.We hope that our research will motivate others to develop new ideas that can further improve the performance of the RC-based speech recognition systems and that, in time, can contribute to the design of more noise robust systems for other tasks such as large vocabulary speech recognition.