@&#MAIN-TITLE@&#
Kinect microphone array-based speech and speaker recognition for the exhibition control of humanoid robots

@&#HIGHLIGHTS@&#
Kinect microphone array-based voice control for operating a robot is proposed.Speech and speaker recognition are effectively combined for fine robot control.Kinect fuzzy-DTW with an accurately designed fuzzy controller is proposed.

@&#KEYPHRASES@&#
Kinect microphone array,Speaker recognition,Speech recognition,Humanoid robot,Kinect-SSA,Kinect fuzzy–DTW,

@&#ABSTRACT@&#
Graphical abstractImage, graphical abstract

@&#INTRODUCTION@&#
Currently, the use of voice control–based human–computer interaction for device operations, including the exhibition control of humanoid robots, is widespread. Speech pattern recognition is a critical task in speech processing and is vital in the voice-based control of devices, including the control of robot exhibitions. Speech and speaker recognition are two primary tasks in speech pattern recognition [1–5]. Mature automatic speech recognition techniques are widely used in numerous commercial products, such as intelligent mobile devices and smart human–machine interactive games. With the maturing of speech recognition techniques [3–5], another class of speech pattern recognition—speaker recognition—has gradually emerged; it is more functional than speech recognition [1,2]. Speaker recognition is employed in surveillance and security systems, such as for access control and in community surveillance systems [1]. Although both speech and speaker recognition can be accurately achieved in specific systems, effective integration of speech and speaker recognition for controlling the action of a robot has rarely been reported. The main objective of this study was to develop a Kinect microphone array-based speech pattern recognition scheme involving integration of speech and speaker recognition for the exhibition control of humanoid robots.The Kinect device developed by Microsoft is a sensor that can be used as a signal receiver for collecting sensed video and audio data [6]. The Kinect sensor is known for its excellent performance in simplifying acquired image data. The Kinect-derived human skeleton, with each human joint position represented by a corresponding three-dimensional data point, is used for easy human gesture recognition [7–11]. In human gesture recognition, both RGB and depth-sensing Kinect cameras are employed to obtain three-dimensional data of human joint positions. The vast majority of Kinect applications are in the field of video-based gesture recognition. Studies have employed the Kinect microphone array for audio-based tasks. For example, the Kinect sensor was used for audio-based human behavior recognition in [12].The current study developed a Kinect microphone array–based voice control scheme for regulating the actions of a robot. Studies have employed Kinect sensors for controlling robots [8, 13–15]. In [13], a Kinect-based control system with a proportional-derivative control algorithm was proposed for a manipulator robot; a combined scheme was used for controlling both the robot arms and Kinect. In [14], Kinect was used to recognize body gestures and provide an interactive interface between the body gesture module and the humanoid robot Nao (manufactured by Aldebaran Robotics). Furthermore, in [15], Kinect, gesture recognition systems, and mobile devices were integrated for enabling interactive discussions with the users. In the author's previous study [8], Kinect-based gesture recognition based on an adaptive hidden Markov model was presented for the use in a humanoid robot that imitates human actions. The main objective of all studies that have used Kinect for robot control has been to achieve gesture recognition-based control; few studies have used speech pattern recognition-based methods for controlling robots. The Kinect microphone array-based voice control scheme for robot control presented herein is a speech pattern recognition-based method wherein speech and speaker recognition are effectively combined for realizing fine robot control; moreover, speaker recognition is divided into speaker identification and speaker verification for accurate robot operator authentication.Fig. 1depicts a practical application scenario for the operational exhibition control of a humanoid robot through an operator's voice commands captured by the microphone array of a Kinect sensor. The humanoid robot responds to valid voice commands. The robot performs three tasks: it authenticates the operator, identifies the operator, and executes the voice commands following successful operator authentication and identification; these tasks are performed using three recognition schemes: support vector machine (SVM) [16], Gaussian mixture model (GMM) [17], and Kinect fuzzy–dynamic time warping (DTW) schemes, respectively. For increasing speech recognition accuracy, the conventional DTW method [18] is improved using the Kinect microphone array and a fuzzy logic control (FLC) scheme [5, 19]; the improved method is termed the Kinect fuzzy–DTW method; the method is detailed later in this paper. Note that the author's previous work in [2] is completely different to the current study herein. The main research of [2] is to develop a method by integrating GMM, SVM and DTW for the application of speaker recognition. In this study, a Kinect microphone array-based scheme to properly combine these three classification calculation methods is presented for performing speaker verification, speaker identification, and speech recognition to further realize advanced voice-based control of humanoid robot exhibitions.The remainder of this paper is organized as follows. Section 2 introduces the implementation of voice-based control of humanoid robot exhibitions by using Kinect microphone array-based speech and speaker recognition. The fundamentals of SVM speaker verification and GMM speaker identification are provided, followed by a discussion of the proposed Kinect fuzzy–DTW speech recognition method. Subsequently, voice command-controlled robot exhibitions are discussed. Section 3 presents the experimental results that demonstrate the effectiveness and high performance of the proposed robot exhibition control. Finally, in Section 4, concluding remarks summarizing the overall study are provided.The framework of the developed Kinect microphone array–based speech and speaker recognition is presented in Fig. 2. The framework consists of three main calculation phases: speaker verification using the SVM, speaker identification using the GMM, and voice command recognition using the Kinect fuzzy–DTW speech recognition method. When a robot operator issues a voice command, the command is sensed and recorded by the Kinect microphone array. If the voice is from an invalid user, the robot control system ignores it and maintains the existing gesture; if the voice command is from a valid operator, the voice signal is forwarded to the speaker identification phase. After speaker identification, the robot control system ignores the command if it is from an unauthorized operator; otherwise, the command is processed for identification and implementation.Fig. 2 depicts the validation of a test command from a speaker; the result is either “imposter” or “valid speaker.” The widely used SVM classifier is appropriate for validating the command. The SVM is commonly used as a data classifier and is based on the structural risk minimization theory in statistics [16]. It classifies new input data by using a separating hyperplane. To determine whether the input speech data is from a valid set of speakers, the SVM model first attempts to locate the SVM model of the valid speaker set in the SVM database. The separating hyperplane of the SVM model of the valid speaker set determines whether the input speech data is valid.Consider the set of labeled training points (x1,y1), (x2,y2), . . ., (xn,yn). Each training point xibelongs to either of two classes (valid speaker and imposter) and is assigned a labelyi∈{−1,1}fori=1,2,…,n. Using these training data, the hyperplane is expressed as(1)w·x+b=0.It is defined by the pair (w,b) such that the point xican be separated according to the function(2)f(xi)=sign(w·xi+b)={1,ifyi=1−1,ifyi=−1.The set S is linearly separable if a pair (w,b) exists such that the inequalities(3){(w·xi+b)≥+1,ifyi=1,(w·xi+b)≤−1,ifyi=−1,i=1,2,…,nare valid for all elements of S. Note that in (3), the hyperplane,w·xi+b=1, and the hyperplane,w·xi+b=−1, are parallel and there exists no training points locating between them. If S is linearly separable, a unique optimal hyperplane exists for which the margin between the projections of the training points of the two classes is maximized. During speaker verification, (2) can be used to determine the class to which the signal xibelongs.The command provider is validated only when the voice command data from the user successfully clears the first verification phase. If the command is invalid, it is ignored immediately and no further calculation is performed; otherwise, the second phase (speaker identification) is executed. In this study, a GMM was employed for speaker identification. GMM calculations comprise training and test phases [17]. After training the GMMs, recognition is then performed using the trained GMM models. Consider the combination of the classifierX={xi|i=1,2,…,n}operating with a decision window (or equivalently, over a time interval) covering n acoustic feature vectors of D dimensions and m GMMs that are speaker models. m users are present in the valid group in the first phase of SVM-based speaker verification.During GMM-based recognition, the class of X is determined by maximizing the a posteriori probability P(λs|X):(4)s^=argmaxs={1,2,…,m}P(λs|X)=argmaxs={1,2,…,m}f(X|λs)f(X)·P(λs).Furthermore, we have(5)f(xi|λs)=∑j=1Mwj·bsj(xi)and(6)bsj(xi)=1(2π)D/2·|Σsj|1/2·exp{−12(xi−μsj)T(Σsj)−1(xi−μsj)}.At the end of the recognition phase, the signal X is classified as belonging to one of the m users in the aforementioned valid group indicated bys^.The identity of the robot operator can be determined reliably by using a double-check procedure for speaker verification and identification, and therefore, access can be denied to invalid/unidentifiable users. When the voice command provider clears both SVM-based speaker verification and GMM-based speaker identification, the user is authenticated and identified, and the voice command data is forwarded to the third phase for recognizing the voice command. In this study, DTW was employed for speech recognition. DTW belongs to the category of dynamic programming algorithms and is a type of optimal algorithm; it has been widely used to solve optimality problems.DTW is a nonlinear warping algorithm that combines time warping and appropriate template matching calculations [18]. In pattern recognition, the DTW algorithm is used to search for an optimal path between the test data and a reference template. The main purpose of DTW calculations is to compute the degree of similarity between the test data and a reference template. In the present study involving voice command recognition for humanoid robot operation, a series of voice commands were designed; therefore, a series of corresponding DTW reference templates were created. In the test phase, each DTW reference template is compared with the user's voice data through time warping: a lower distortion between the template and voice data implies a higher degree of similarity. The voice command issued by the robot operator is considered the test data T consisting of M frames. An arbitrary frame (a feature vector) is denoted as m, and the reference template database consists of P template commands, each denoted by R. For this aforementioned setting, the DTW distance corresponding to the smallest distortion between the test data and the reference template database is derived using the following equation:(7)D=mini={1,2…P}∑m=1Md(T(m),Ri(m)).After completing DTW-based template matching operations in the third phase of speech recognition, a decision on the voice command is finally made on the basis of the degree of similarity between the test data and the reference template, as shown in (7).Fig. 3illustrates the use of the Kinect microphone array for collecting acoustic data for speech recognition. The sound source angle (SSA) of the voice command can be determined using the Kinect microphone array; it is referred to as Kinect-SSA. The sound waveforms obtained from the M1 Kinect microphone for very small (VS), small (S), moderate (M), large (L), and very large (VL) Kinect-SSAs are plotted in Fig. 3.In DTW-based speech recognition performed using the Kinect microphone array, recognition decision through computational fusion of the four sets of DTW distances obtained from the four Kinect microphones is more effective and efficient than the recognition decision made from only one set of DTW distances from single compound sound data. The fusion method employed for DTW-based speech recognition by using Kinect is as follows:(8)D=mini={1,2,…P}W1·D1(i)+W2·D2(i)+W3·D3(i)+W4·D4(i),where D1(i), D2(i), D3(i), and D4(i) represent the DTW distortion distance between the test data and the ith reference template, which are obtained using the four Kinect microphones M1, M2, M3, and M4, respectively.Fig. 3 and (8) indicate that the estimate of the fusion weights W1, W2, W3, and W4 for each of the DTW distortions from the four Kinect microphones is crucial for reliable speech recognition. In the next section, a Kinect fuzzy–DTW method for accurately determining the aforementioned fusion weights is presented; the method entails a Kinect SSA-driven fuzzy logic controller.The proposed Kinect fuzzy–DTW method for enhancing both the aforementioned DTW with Kinect and conventional DTW is as follows:(9)D=mini={1,2,…P}C1·D1(i)+C2·(D2(i)+D3(i)+D4(i)),where(10)C1=W(Kinect−SSA)and(11)C2=1−W(Kinect−SSA)3.In (9), the index C1 denotes the fusion weight assigned to the DTW distortion from microphone M1. The index is an adjustable parameter and is regulated through Kinect SSA-driven FLC. The index C2 indicates the remaining fusion weight equally allocated among the Kinect microphones M2, M3, and M4.As seen clearly in Fig. 3, to determine the fusion weights to be allocated to the four Kinect microphones, the SSA obtained from Kinect microphones, referred to as the Kinect-SSA index, should be considered. The single microphone M1 and the set of three microphones forming a microphone array should be separately considered for estimating the fusion weights. The estimation of a very large fusion weight for the M1 microphone might result in very few references for calculating DTW distortion from each of the remaining microphones in the microphone array (M2, M3, and M4) whereas a very small fusion weight for M1 would result in insufficient processing of the acoustic data pertaining to the user command sensed from the vicinity of M1.The use of an appropriate fusion weight for M1 is the core of the proposed voice command recognition system for controlling robot operations. The fusion weight for M1 should be small when the voice command has an improper sound source; this reduces the importance of M1 data in the DTW-based speech recognition process, thereby increasing the reliability and accuracy of speech recognition. The use of situation-dependent weights is essential in DTW-based speech recognition applications where reliable recognition is crucial. An FLC mechanism has been proposed for this purpose [19]. In the current study, the fusion weight was controlled through FLC: the fusion weight for M1 is set according to the calculated SSA of the given voice command relative to the Kinect microphone. The Kinect-SSA index, which determines the fusion weight in DTW-based speech recognition involving Kinect, was used as the driving input to the fuzzy controller.As explained, the Kinect-SSA index can be used for controlling the fusion weight. Accordingly, an FLC involving the following five if–then fuzzy rules was designed:Rule1:IfKinect-SSAisverysmall(VS),thenC1isverysmall(VS).Rule2:IfKinect-SSAissmall(S),thenC1issmall(S).Rule3:IfKinect-SSAismoderate(M),thenC1ismoderate(M).Rule4:IfKinect-SSAislarge(L),thenC1islarge(L).Rule5:IfKinect-SSAisverylarge(VL),thenC1isverylarge(VL).Here, Kinect-SSA is the input to the FLC, and C1 is the output. Quantitatively, the aforementioned FLC rule set is transformed into the following form:(12)Rule1:IfKinect-SSAisM1(Kinect-SSA),thenCVS=f1(Kinect-SSA).Rule2:IfKinect-SSAisM2(Kinect-SSA),thenCS=f2(Kinect-SSA).Rule3:IfKinect-SSAisM3(Kinect-SSA),thenCM=f3(Kinect-SSA).Rule4:IfKinect-SSAisM4(Kinect-SSA),thenCL=f4(Kinect-SSA).Rule5:IfKinect-SSAisM5(Kinect-SSA),thenCVL=f5(Kinect-SSA).The output of the FLC is as follows [19]:(13)C1=W(Kinect−SSA)=∑i=15Mi(Kinect−SSA)·fi(Kinect−SSA)∑i=15Mi(Kinect−SSA).The membership functions of the Kinect SSA-driven FLC, which areM1(Kinect−SSA),M2(Kinect−SSA),M3(Kinect−SSA),M4(Kinect−SSA)andM5(Kinect−SSA)denoting the very small value, the small value, the moderate value, the large value, and the very large value of Kinect-SSA, respectively, are depicted in Fig. 4.After clearing SVM-based user verification in the first phase, GMM-based user identification in the second phase, and Kinect fuzzy–DTW speech recognition in the third phase, the recognized voice command is finally used to control the robot through wireless transmission between a Kinect-connected Windows platform and the robot. The Bioloid humanoid robot manufactured by the South Korean company Robotis was adopted in this study; this robot has three classes of mechanical design—Types A, B, and C—according to its functional complexity [20], and this study adopted the Type-A Bioloid humanoid robot. The components and modular servomechanisms (artificial joint motors) of the Bioloid robot can be arranged according to user requirement [20]. In this study, a series of voice commands were designed for robot operations; therefore, different configurations of the robot were used for eliciting the corresponding gestures.The Bioloid humanoid robot has 18 modular servomechanisms, each of which represents an artificial joint motor. The voice command issued by the test user is first recognized, and the three-dimensional positions of a series of joint sets, with each joint set containing 18 joints, are determined according to the active label indicated by the command. The setting configurations for the corresponding robot action are realized according to the position information derived for all joint sets. The Kinect fuzzy–DTW speech recognition method presented in Section 2.3 yields a reliable recognition decision for all voice commands, resulting in the robot accurately displaying the commanded action.

@&#CONCLUSIONS@&#
In this paper, a Kinect microphone array-based voice control method is proposed for operation control of the humanoid robot. The developed Kinect microphone array-based approach performs both speech recognition and speaker recognition, and speaker recognition in this study is divided into speaker identification and speaker verification for accurate user authentication. The presented method for effectively controlling the robot is composed of three main computation phases, which are SVM-based speaker verification, GMM-based speaker identification, and DTW-based speech recognition. The humanoid robot can be controlled remotely using the voice command through wireless transmission between a Kinect-connected platform and the robot only when the command made from the speaking operator is authenticated and valid, clearing SVM-based speaker verification in the first phase, GMM-based speaker identification in the second phase, and DTW-based speech recognition in the third phase. Furthermore, for the DTW-based speech recognition phase, a Kinect fuzzy–DTW method with an accurately designed fuzzy logic controller is proposed for appreciably enhancing the conventional DTW method. In the proposed Kinect fuzzy–DTW approach, a decision fusion method is presented first for DTW-based speech recognition by using Kinect where all DTW computational decisions from each of the microphones in the Kinect microphone array are taken into consideration. A Kinect SSA-driven fuzzy logic controller, using the Kinect microphone array-determined sound source angle of the voice command as the driving input to the fuzzy controller, is then developed for accurately determining the fusion weights of DTW decision fusions. Experimental results show that proposed Kinect fuzzy–DTW performs best, which has a high recognition accuracy of 85.0% and is superior to 79.2% of the conventional DTW with a Kinect microphone array and 57.6% of conventional DTW with a general microphone. Speaker recognition incorporated with Kinect fuzzy–DTW speech recognition enables excellent control of the robot.