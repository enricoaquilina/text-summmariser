@&#MAIN-TITLE@&#
The D-Day, V-Day, and bleak days of a disruptive technology: A new model for ex-ante evaluation of the timing of technology disruption

@&#HIGHLIGHTS@&#
Performance improvement process of a disruptive technology can be captured by ease and network factors.The process of technology disruption can be characterized by the D-Day, V-Day, and bleak days.A dynamic consumer model can be used to analyze the D-Day, V-Day, and bleak days of technology disruption.Empirical tests show that ease and network factors across technology generations are fairly consistent.A decision tree can be used to guide managers in evaluating a potential disruptive technology.

@&#KEYPHRASES@&#
Disruptive technologies,New product development,Bass diffusion model,Defender consumer model,Ex-ante technology evaluation,

@&#ABSTRACT@&#
The recent failure of major PC and smartphone makers in launching new generations of high-tech products in time shows that analyzing and capturing the timing of technology disruption is an important yet less explored research area. This paper conducts theoretical and empirical analyses for ex-ante quantitative evaluation of the timing of technology disruption. We conceptualize the ease and network factors as key determinants of performance improvement for a disruptive technology. A dynamic consumer model is developed to identify two critical times, termed D-Day and V-Day, of technology disruption. We also show that, if the network factor dominates the performance improvement process, there may exist some “bleak days” during which a firm would discontinue a “promising” technology that will eventually disrupt. Empirical tests are conducted with data of hard disk drives, semiconductor technologies, and CPU performance for mobile devices to verify key model assumptions and to show how to estimate the ease and network factors. We also perform a numerical experiment to demonstrate how to forecast the timing of technology disruption. A decision tree and a systematic framework are also developed to operationalize key model parameters and analytical results from a decision-support perspective. This paper contributes to the literature by presenting a novel analytical tool and new insights for high-tech companies to forecast and manage the timing of technology disruption.

@&#INTRODUCTION@&#
The tablet takes cutting-edge PC technology and makes it available wherever you want it…It's a PC that is virtually without limits–and within five years I predict it will be the most popular form of PC sold in America (Bill Gates at the COMDEX Fall 2001 Computer Show).The tablet PC is a flop, with sales in the hundreds of thousands, not millions as forecast by Gates. Today, the Tablet is stuck mostly in such vertical markets as insurance and health care (Business Week, September 27, 2004).Hewlett-Packard introduced a touchscreen tablet similar to Apple's iPad…Investment and technology analysts gave the HP strategy mixed initial reviews…They questioned the amount of developer interest and the timing of the release, noting that Apple's next-generation iPad is expected to be shipping by April (Financial Times, February 10, 2011).Dell intends to launch its first consumer tablet computer in late 2012, marking its entry into a hotly contested and increasingly crowded arena that has already claimed arch-foes Hewlett Packard and Apple (The Economic Times, January 11, 2012).Blackberry – Ten years ago, it was the toast of the business world; it was the must-have accessory…Sadly, RIM was not able to keep pace with its rivals – it was too wedded to old ideas… They had a dominant market position – as much as 46 percent of the business market in 2008 – which has now shrunk to a pathetic 2 percent…Blackberry never saw Apple coming, always assumed it would control the high-end business market. They saw the iPhone as no threat to the mighty RIM product, that a specialist phone company would always beat a vaguely artsy design-led computer firm (The Telegraph, January 30, 2013).High-tech companies today are no stranger to the concept of technology disruption. But why did many of them, such as Dell, HP, and Blackberry in the above anecdotes, continued to make the same mistake of missing the innovation opportunities for the next technology generations as that made by those incumbent firms in the famous “hard disk drive puzzle” more than two decades ago? Since the pioneering works by Bower and Christensen (1995) and Christensen (1997), extensive studies have also been done by researchers and practitioners to understand and characterize the market conditions and competition outcomes of technology disruption. However, the recent failure of major PC and cell phone makers to develop and launch disruptive technologies in time shows that analyzing and capturing the timing of technology disruption is as important as understanding the well explored issues of whether and under what conditions a particular technology would become disruptive in the existing literature. While both HP and Dell did develop the “HP Compaq TC1100” and “Dell Streak”, respectively, as the prototypes of consumer tablets in the early 2000s, their innovation efforts were hampered by the limited market success and technical problems experienced by tablet PCs in the early stage of technology innovation. By the time when the tablet market took off with Apple's iPad, it was already too late for both companies to ramp up their efforts in R&D and creating collateral assets. This failure, in large part, was not due to their inability to identify a potential disruptive technology or to understand the competition outcomes, two issues which have been extensively studied in the existing literature, but due to their inability to predict and capture the timing of technology disruption.Despite the significant amount of literature related to disruptive technologies, most of the studies focus on the ex-post characterization of the market conditions and competitive outcomes as opposed to the ex-ante evaluation of the timing of technology disruption. Our communication with managers from a number of leading high-tech companies, including IBM (Almaden Research Center) and Taiwan Semiconductor Manufacturing Company, also confirms that there is a lack of analytical tools for evaluating and predicting the timing of technology disruption. Even though most of today's companies recognize the importance of identifying and developing disruptive technologies, they still face two key questions: (i) When would be the exact time(s) for a disruptive technology to capture an emerging market and/or the mainstream market? (ii) What would be the time-based process for a disruptive technology to capture the emerging market and/or the mainstream markets so that a firm can deploy its physical, financial, and human resources in a timely fashion? Without the ability to conduct ex-ante evaluation of the timing of technology disruption, a firm would not be able to continuously allocate the right amount of resources at the right time as the strategic move to manage disruptive innovation.Research in technology disruption generally focuses on either the supply- or demand-side studies of technology development and competition. Most of the early work on the supply side focuses on the descriptive behaviors and characteristics of technology trajectories. Foster (1986) suggests that, although the inception of a new technology seems random, its development over time appears to follow a reasonably consistent pattern, which is termed the technology S-curve. This concept is extended to multiple technology generations along the so-called technology cycles in which disruptive technologies and dominant deigns appear in a cyclic fashion over time (Anderson & Tushman, 1990, Utterback, 1994). While these studies on the supply side have provided good explanations of the outcomes of disruptive innovation based on technology characteristics, the fundamental, endogenous determinants of the technology S-curve and technology cycles have been a less explored area in the existing literature. As noted by Foster (1986), if one can capture the underlying determinants of the technology S-curve, it will provide an important basis for technology forecasting. In fact, the endogenous determinants indeed hold the key to analyzing and capturing the timing of technology disruption, as will be demonstrated in the paper.The more recent work on disruptive innovation has focused on the demand-side analyses of the consumer and market conditions under which a disruptive technology may emerge as well as firms’ strategic behaviors (e.g., Dawida, Kopelb, & Kort, 2013). Based on the nominal profit functions, Nault and Vandenbosch (2000) identify the conditions under which an entrant will launch a next generation product for competitive considerations. Adner (2002) proposes a framework to study the demand conditions that enable disruptive dynamics to explore how the demand structure influences the emergence and extent of competition. From a demand-based perspective on technology life cycles, Adner (2004) characterizes demand maturity by introducing the idea of a demand S-curve as a complement to the traditional technology S-curve for disruptive innovation. Based on a game-theoretic model with horizontal and vertical differentiation, Adner and Zemsky (2005) show how the threat of disruption depends on the rates of technology advance, market sizes, and firms’ ability of price discrimination. Huang and Sošic (2010) analyze the competition between an incumbent firm and a new entrant under perfect and imperfect information about customer valuation. Chen and Turut (2013) analyze the context-dependent innovation strategy where a follower needs to choose to improve the new technology either on a key performance dimension or a new performance dimension. Using empirical data of 36 technologies, Sood and Tellis (2011) show some important market characteristics of technologies that adopt a lower attack. Sood, James, Tellis, and Zhu (2012) propose a model for predicting the path of technological innovation with the considerations of the timing and order of entry, number of competing technologies, and technology-specific characteristics.In this paper, we integrate studies on both the demand and supply sides to propose a new analytical model with empirical validation for ex-ante quantitative evaluation of a potential disruptive technology with a focus on the timing of disruption. Unlike those models in the existing literature which focus on the ex-post characterization of the conditions of disruption, we explicitly model the timing of technology disruption to identify the critical times when (i) a disruptive technology captures an emerging market (D-Day), (ii) a disruptive technology captures the mainstream market (V-Day), and (iii) a disruptive technology may experience some early setback in terms of closing the performance and utility gaps (bleak days). Specifically, we conceptualize the ease factor and network factor as the key determinants of performance improvement for a disruptive technology which lead to the technology S-curve, and use a dynamic model to identify the critical times of technology disruption. We show that, if the network factor dominates the performance improvement process, there may exist some “bleak days” at the introductory stage during which a firm would make an early exist and discontinue a “promising” technology that will eventually disrupt. In addition, we perform empirical tests based on data of hard disk drives, semiconductor technologies, and CPU performance for mobile devices to verify key model assumption and to show how to estimate the ease and network factors. Moreover, we conduct a numerical experiment to demonstrate the technical process for analyzing and capturing the timing of technology disruption. A decision tree and a systematic framework are also developed to operationalize key model parameters and analytical results from a decision-support perspective. The remainder of the paper is organized as follows. In Section 2, we present the supply-side model, which is integrated with a demand-side defender consumer model in Section 3 with key analytical results given in Section 4. Empirical tests are presented in Section 5. A numerical experiment and a systematic framework for implementation are presented in Section 6, and concluding remarks are in Section 7.We now conceptualize the determinants of disruptive innovation on both the supply and demand sides. It should first be emphasized that technology forecast/evaluation is for long-term decision-making. Therefore, some of short-term decisions such as pricing and production can only be captured in their macro-forms when modeling technology forecast/evaluation. For example, a model for technology forecast/evaluation is aimed to capture the long-term pricing pattern of a new technology as opposed to determining how prices should be charged and/or changed on the daily or weekly basis (Rogers, 2003, Stummer, Kiesling, Günther, & Vetschera, 2015).Since the influential work by Foster (1986), it has long been recognized that the performance trajectory of a new technology generally follows the so-called technology S-curve where the performance improvement (i) starts out slowly when it is first introduced; (ii) increases significantly as a company accumulates more experience in R&D and technology development over time; and (iii) eventually reaches the natural (physical) performance limit of the technology. The above pattern of performance improvement shares some intriguing similarities with the famous Bass diffusion model that is capable of generating the S-shaped curve to capture the cumulative sales of a new product Bass (1969). We thus propose a novel application of the Bass model to hypothesize that the process of performance improvement of a new technology is determined by some underlying “macro” factors which can be used to describe and capture its performance trajectory over time. We further hypothesize that these macro factors, like the Bass model, are quite consistent across multiple generations of a particular technology. As will be demonstrated with our empirical tests, the new application of the Bass model does capture the technology S-curves of disruptive technologies well for different generations of hard disk drives, semiconductors, and CPU technologies for mobile devices with rather consistent values of underlying factors. It is important to note that our conceptualization of the performance improvement process of a new technology described above differs from the traditional conceptualization of the diffusion process which is based on product sales (Mahajan, Muller, & Wind, 2000) or the adoptions and communication among the members of a social system (Rogers, 2003). In addition, while the theory of disruptive technologies is built upon the phenomenon of technology S-curve, the time-based performance improvement process of a disruptive technology along its technology S-curve has not been formally specified and tested in the existing literature to identify the underlying determinants of technology disruption.11For example, all the performance improvement functions used in Christensen and Raynor (2013) to conceptualize the competition among disruptive and sustaining technologies are illustrated as straight lines without statistical validation.Our conceptualization of the performance improvement process of a disruptive technology, however, requires a distinct set of interpretations of key model parameters. For a disruptive technology introduced at t = 0, let F(t) denote the cumulative level of performance improvement at time t. Also let M denote the natural limit of a technology S-curve (i.e., the physical limit to performance improvement of the disruptive technology) as discussed in Foster (1986). The functional form for the proposed use of the Bass model is(1)F′(t)=[p+q×F(t)M][M−F(t)],wherep = the “ease” factor of performance improvement, and q = the “network” factor of performance improvement.Given the boundary condition F(t) = 0 at t = 0, i.e., the initial performance improvement of the disruptive technology is zero, the explicit functional form of the Bass model used to capture the technology S-curve is as follows:(2)F(t)=M×1−e−(p+q)t1+(q/p)e−(p+q)t.The parameters p and q, analogous to the coefficients of innovation and imitation of the Bass model (Bass, 1969), are termed the “ease” and “network” factors for performance improvement with different interpretations in our model. Specifically, p represents the intrinsic characteristic of a technology in terms of performance improvement, and q represents the network effect in solving design and engineering problems in the process of performance improvement. Specifically, the ease factor is a parameter of “external influence” since the contribution of the ease factor to performance improvement does not depend on prior performance. On the other hand, the contribution of the network factor to performance improvement is proportional to the cumulative product performance. In other words, the network factor is a parameter of “internal influence” since the higher the product performance, the stronger the influence of the network factor due to the interactive effect among the existing design and engineering attributes within a product system. When the influence of the network factor (q) is significant, the performance improvement of a disruptive technology depends heavily on how many other relevant design and engineering problems are solved. For example, the performance improvement of electric vehicles depends greatly on solving a variety of engineering problems associated with battery life and advanced material substitution. On the other hand, when the influence of the ease factor (p) is significant, the performance of a disruptive technology can be independently improved in the absence of solving other design and engineering problems (due to such factors as a simpler design architecture, larger frame, etc.). For example, due to its high energy densities, zinc–air fuel cell has an intrinsic advantage in improving rechargeability for commercial applications than other fuel cell technologies. In the semiconductor industry, it has been reported that the performance improvement of RISC (Reduced Instruction Set Computing) chips can be easier and faster achieved than that of CISC (Complex Instruction Set Computing) chips due to the RISC architecture's relatively smaller numbers of machine instructions, instruction formats, and addressing modes.The above functional form suggests that the likelihood of performance improvement at time t is the sum of two components on the right-hand-size of (1). The first component, which is influenced by p, refers to a constant propensity of performance improvement that is independent of the current functional performance. The second component, which is influenced by q, is proportional to the current functional performance and thus represents the extent of favorable interaction between the design and engineering problems that have been already solved. The influences of both p and q, however, will both diminish as the performance approaches the physical limit M according to (1).22The lower rate of performance improvement with diminishing influences by p and q usually occurs at the maturity stage of a technology when industry standards of key components are set. Firms often start to utilize modular design as a form of process innovation for the purpose of cost saving and resource sharing (Utterback 1994, Baldwin and Clark 2004).The entire technology S-curve of a disruptive technology can thus be determined by the ease and network factors (p and q) as well as the physical limit to performance improvement (M) of the technology based on (2). As will be discussed in more details later, the values of p and q can be estimated from analogous technologies, such as different generations of hard disk drives and semiconductor technologies. Many high tech industries also have established rules, such as the Moore's Law for the computing hardware industries in general and the Kryder's Law for the hard disk drive industry in particular (Scientific American, 2005), for estimating the “macro” levels of performance changes over time, which can be used as the basis for estimating parameter M. (The proposed model can then be used to identify the specific path and timing to reach a particular micro level of performance changes.) Regression analyses based on historical data with initial patterns of performance improvements can also be applied to estimate p, q, and M (Bass, 1969, Lilien & Rangaswamy, 2003, Lilien, Kotler, & Moorthy, 1992). It should be noted that the curve of performance improvement defined in (2) will be S-shaped if and only if the ease factor is smaller than or equal to the network factor (i.e., p ≤ q). Otherwise, the function in (2) will lead to a reverted U-shaped curve of performance improvement. In the next section, we will integrate the above supply-based technology model with a demand-based consumer model to analyze the market dynamics of disruptive innovation.We now develop a dynamic, demand-based consumer model. Since there have been various consumer models with different specifications for analyzing disruptive technologies in the existing literature, to ensure the accuracy and applicability of our analysis, we will closely follow the original work by Bower and Christensen (1995) and Christensen (1997) to construct our model.We first define two different types of technologies. According to Bower and Christensen (1995): Sustaining technologies give customers something more or better in the attributes they already value…Disruptive technologies introduce a very different package of attributes from the one mainstream customers historically value. We will thus assume that there are two different types of technologies available in a market: the Disruptive Technology, labeled as Technology D, and Sustaining Technology, labeled as Technology S. Bower and Christensen (1995) also describe the specific characteristics of the disruptive and sustaining technologies as follows: Disruptive technologies have two important characteristics: First, they typically present a different package of performance attributes–one that, at least at the outset, are not valued by existing customers. At first, disruptive technologies tend to be used and valued only in new markets or new applications; in fact, they generally make possible the emergence of new markets. Second, the performance attributes that existing customers do value improve at such a rapid rate that the new technology can later invade those established markets. Only at this point will mainstream customers want the technology. We thus assume that there are two types of performance attributes: the Traditional Performance Attribute, labeled as Attribute1, and the New Performance Attribute, labeled as Attribute2. Let Fijdenote the level of performance Attribute i of Technology j, where i ∈ {1, 2} and j ∈ {D, S}. Typical examples of the traditional and new attributes include the storage capacity and portability of a hard disk drive as well as the processing capability and portability of a personal computer.From the market perspective, we assume that there are two distinct market segments: the mainstream Existing Market, labeled as Market E, and the emerging New Market, labeled as Market N, according to the above statement about market characteristics in Bower and Christensen (1995). As in most standard models of product design and positioning (Chen, 2001, Krishnan and Zhu 2006, Fang, Gavirnenib, & Rao, 2013), we adopt a conjoint framework to model customer valuations for the two technologies. Let vE1 and vE2denote the customers’ marginal valuations for Attribute 1 and Attribute 2 in the Existing Market. Also let vN1 and vN2denote the customers’ marginal valuations for Attribute 1 and Attribute 2 in the New Market. Then the overall valuations for customers in Market k ∈ {E, N} for Technology j ∈ {D, S} are given byUkj=vk1F1j+vk2F2j. We also assume that customers in the New Market value the New Attribute more than the Traditional Attribute compared to those in the Existing Market, i.e., vN2/vN1 > vE2/vE1. It is noted that a large number of disruptive technologies discussed in the literature, such as hard disk drives and microprocessors, are business-to-business (B2B) products. While the conjoint framework has traditionally been applied to business-to-consumer (B2C) settings, Auty (1995), Kivetz, Netzer, and Srinivasan (2004), and Lilien and Grewal (2012) have shown that the methodology is also suited to B2B market research. However, under B2B settings where decision making in a “buying center” tends to be a joint process, the population for assessing customer valuations with the conjoint framework should include all categories of role players, including buyers, users, influencers, and gatekeepers (Hutt & Speh, 1998).Based on the performance improvement process conceptualized in the previous section, we now model the performance trajectory of the disruptive technology. According to Bower and Christensen (1995): Performance trajectories are the rate at which the performance of a product has improved, and is expected to improve, over time. Almost every industry has a critical performance trajectory. In the mechanical excavators, the critical performance trajectory is the annual improvement in cubic yards of earth moved per minute. In photocopiers, an important performance trajectory is improvement in number of copies per minute. In disk drives, one crucial measure of performance is storage capacity. This statement indicates that the performance trajectory for an industry refers to the rate of performance improvement on the traditional attribute of a technology. For example, according to the famous “hard disk drive puzzle” (Bower & Christensen, 1995, Christensen, 1997), as the storage capacity (the traditional attribute) of a new generation of hard disk drive (with a fixed but smaller size as the new attribute) continuously improved, the previous generation of hard disk drive (with a fixed but bigger size) was eventually displaced in the marketplace. We thus assume that F1Dand F1Sof the disruptive and sustaining technologies are both functions of time (t) with performance improvements captured by the technology model defined in (2) along their respective technology S-curves.To analyze the market dynamics, we now integrate the supply-based technology model and demand-based consumer model by using an expanded framework of the defender consumer model (Hauser & Shugan, 1983) with the additional component of dynamic technology changes to analyze the relative time-based positions of the two technologies. Fig. 1illustrates the dynamic positions of the disruptive and sustaining technologies for customers in the new and existing markets with the two “ideal vectors,” analogous to the “value trajectories” in Adner (2002), showing how they make tradeoffs between the traditional and new performance attributes. Suppose that the original position of the disruptive technology at the time of introduction (t = 0) is at point D° with(F1D0,F2D0)as the combination of the traditional and new performance attributes, and that the position of the sustaining technology is at point S° with(F1S0,F2S0)at the same time.Now consider the dynamic situation in which both the disruptive and sustaining technologies progress along the dimension of traditional attribute F1 (e.g., storage capacity) to Dtand Stover time along their respectively technology S-curves, as shown in Fig. 1, while leaving their respective levels of new attribute F2 (sizes of a hard disk drives) unchanged. As the two technologies interact, if the traditional attribute of the disruptive technology improves with a faster rate than that of the sustainable technology, the gap between the sustaining and disruptive technologies on the traditional attribute will be decreasing over time. Under the “right” conditions (to be derived), there will exist a critical time when customers in the new market are indifferent between the disruptive and sustaining technologies, and we term the critical time the “D-Day” of the disruptive technology when the invasion starts. Similarly, there may exist another critical time when customers in the existing market are indifferent between the disruptive and sustaining technologies, and we term the critical time the “V-Day” of the disruptive technology when it starts to achieve victory in the mainstream market.33Similar to the various successive versions of the Bass diffusion model with additional considerations of R&D and marketing efforts (e.g., Lilien et al. 1992), the original function of the technology S-curve presented in (2) can also be modified to consider other internal and external factors, such as financial investments and R&D efforts, in the process of technology improvement.To take price changes into consideration, let rD(t) > 0 and rS(t) > 0 denotes the prices of the disruptive and sustaining technologies at any given time t. Assume that the prices are decreasing but with slower rates of reduction over time, i.e.,rj′(t)≤0andrj′′(t)≥0, where j ∈ {D, S}. Notice that the assumed properties of the price functions are consistent with the common pattern of “IT hardware price decline” due to shared inputs, which is empirically verified by Aizcorbe, Flamm, and Khurshid (2007) with extensive historical pricing data of different computer and communication devices and equipments. For example, the commonly adopted price decline function,r(t)=roe−at+b, where rois the initial price and a > 0 and b > 0 are regression coefficients, can be used to incorporate the price changes of an IT hardware product into our analysis. Another way to analyze the effect of price changes is to derive the values of parameters p and q based on the price-adjusted attribute levels, which will be demonstrated in our second empirical test presented later in the paper.In this section, we will analyze the timing of technology disruption by establishing the conditions for the D-Day and V-Day to exist. Assume that the improvements on the traditional performance attribute for the disruptive and sustaining technologies will both follow the time-dependent function defined in (2) with two different sets of parameters (p1D, q1D, M1D) and (p1S, q1S, M1S). One justification of this assumption is that most sustaining technologies, such as the BlackBerry push technology, cellular phones, and multiple generations of hard disk drives (Bower & Christensen, 1995, Christensen, 1997), were once disruptive technologies when they were first introduced into their respective markets according the concept of “technology cycles” (Anderson & Tushman, 1990) and the “aggregate view of technological maturity” (Christensen, 1992a). Let tDdenote the time when the disruptive technology is first introduced into the market with(F1Do,F2Do)as the combination of the initial levels of the traditional and new attributes, and let tSdenote the time when the sustaining technology is first introduced into the market with(F1So,F2So)as the initial levels of the traditional and new attributes, where tS< tD(i.e., the sustaining technology was introduced earlier than the disruptive technology). As assumed previously,F2DoandF2So, the levels of the new attribute (e.g., size of a particular generation of hard disk drive), do not change over time. (The situation with performance improvements on both attributes will be analyzed later.) Also assumeF1So>F1DoandF2So<F2Do, i.e., the disruptive technology outperforms the sustaining technology on the new attributes, but underperforms the sustaining technology on the traditional attribute at their respective times of introduction (Bower & Christensen, 1995). Define F1D(t)and F1S(t)as the functions of cumulative performance improvements on the traditional attribute at any given time t for the disruptive and sustaining technologies, i.e.,(3)F1D(t)=FD(t−tD)=M1D[1−e−(p1D+q1D)(t−tD)]1+(q1D/p1D)e−(p1D+q1D)(t−tD)andF1S(t)=FS(t−tS)=M1S[1−e−(p1S+q1S)(t−tS)]1+(q1S/p1S)e−(p1S+q1S)(t−tS),whereFD(t−tD)andFS(t−tS)are the curves of performance improvement defined in (2) with different starting times for the two technologies. Based on Tirole (1988), the utilities derived by customers from the disruptive and sustaining technologies over time, denoted by UkD(t) and UkS(t), can be defined as the difference between the value (functional benefit) received by customers in a segment and the price they pay, i.e.,(4)UkD(t)=vk1(F1Do+F1D(t))+vk2F2Do−rD(t),andUkS(t)=vk1(F1So+F1S(t))+vk2F2So−rS(t),where k ∈ {E, N}. Then the critical condition under which the D-Day or V-Day exists (i.e., the utility received by customers in the new or existing market from the disruptive technology is higher than that of the sustaining technology) is(5)UkD(t)=vk1[F1Do+F1D(t)]+vk2F2Do−rD(t)≥UkS(t)=vk1[F1So+F1S(t)]+vk2F2So−rS(t),which is termed the “condition of disruption” in the remainder of the paper. We first analyze the situation where both the disruptive and sustaining technologies will stay in the market until the very ends of their respective curves of performance improvement to fully realize the performance improvements within the physical limits. Note that F1D(t) → M1Dand F1S(t) → M1Sas t → ∞ according to (3). Sincerj′(t)≤0andrj′′(t)≥0, it is also reasonable to assume that rD(t) → 0 and rS(t) → 0 as t → ∞, i.e., the values of the disruptive and sustaining technologies will both approach zero at the end of their useful lives. Based on (5), we have the following proposition.Proposition 1(i) If(M1S−M1D)+(F1So−F1Do)F2Do−F2So>vN2vN1, then there exists no D-Day or V-Day; (ii) IfvN2vN1>(M1S−M1D)+(F1So−F1Do)F2Do−F2So≥vE2vE1, then only the D-Day exits, but there exists no V-Day; (iii) IfvE2vE1≥(M1S−M1D)+(F1So−F1Do)F2D0−F2S0, then both the D-Day and V-Day exist.The proposition shows that the D-Day and V-Day may or may not exist for a potential disruptive technology as the existence of the feasible (finite and nonnegative) D-Day or V-Day depends on the interactions among the physical performance limits of the disruptive and sustaining technologies, the initial attribute levels, and customer valuations in the two markets. That is, without the “right” conditions, a disruptive technology may never disrupt at all (without a feasible D-Day or V-Day) even with continuous performance improvement over time. Despite the complexity of the above conditions, a few observations can be made. In particular, the bigger (smaller) the initial performance gap between the disruptive and sustaining technologies on the new (traditional) attribute, or the smaller the difference between the physical limits of the disruptive and sustaining technologies on the traditional attribute, the higher the chance for the D-Day and/or the V-Day to exist. It should also be noted that the conditions in Proposition 1 are based on the mathematical properties of the model with the assumption that the performance improvements of both the disruptive and sustaining technologies will be fully realized. In practice, however, a firm (especially a new entrant) may not have sufficient financial and other resources to stay in the market until the end of the curve of performance improvement. Rather, a firm will closely watch the changes in the time-dependent performance gap and utility gap between the sustaining and disruptive technologies, i.e.,FS′(t−tS)−FS′(t−tD)andUks′(t)−Ukd′(t)based on (3) and (4), as indications of the prospect of whether the condition of disruption (5) will ever be satisfied. For example, as discussed in Bower and Christensen (1995) and Christensen (1997), for a disruptive technology with rapid performance improvement, the performance gap on the traditional attribute will be quickly shortened, i.e.,FS(t−tS)−FD(t−tD)is decreasing in t with a very fast rate, leading to a more favorable condition of disruption (i.e., it will become easier to satisfy condition (5)). That is, whether the performance gap and the resulting utility gaps are being shortened as time goes by would provide critical information for a firm to determine whether or not to stay in the market and continue the development efforts of the disruptive technology over time.We now derive the conditions to identify and analyze the D-Day and V-Day of a disruptive technology. As assumed previously, we will investigate the situation when the disruptive technology adopts low attack upon its introduction, i.e.,UkS(tD)−UkD(tD)>0. For ease of exposition, we also assume that, when deriving the D-Day and V-Day of a disruptive technology, the respective existence conditions given in Proposition 1 are satisfied. Based on the condition in (4), the D-Day and V-Day when customers in the new and existing markets are indifferent between the disruptive and sustaining technologies are characterized in the next proposition.Proposition 2If the disruptive technology is introduced at any given time tD, the corresponding D-Day and V-Day, denoted bytN*andtE*, satisfy the following condition:(6)M1S[1−e−(p1S+q1S)(tk*−tS)]1+(q1S/p1S)e−(p1S+q1S)(tk*−tS)−M1D[1−e−(p1D+q1D)(tk*−tD)]1+(q1D/p1D)e−(p1D+q1D)(tk*−tD)=vk2ΔF−vk1ΔF+Δr(tk*)vk1,Notice that (6) is a nonlinear equation oftk*which can be solved with standard numerical methods with a given set of parameter values. A comparative static analysis based on (6) reveals that the D-Day and V-Day are decreasing (with earlier D-Day and V-Day) in the customer valuation for the new attribute (vk2) and the physical limit of the disruptive technology (M1D), but increasing in the customer valuation for the traditional attribute (vk1) and the physical limit of the sustaining technology (M1S). The effects of the ease and network factors, (p1D, q1D) and (p1S, q1S), of the disruptive and sustaining technologies, however, are somewhat complicated. On the one hand, the D-Day and V-Day are decreasing in the total strength (p1D+q1D) of the ease and network factors for the disruptive technology and in the relative strength of the two factors of the sustaining technology (q1S/p1S). On the other hand, the D-Day and V-Day are increasing in the total strength (p1S+q1S) of the ease and network factors of the sustaining technology as well as in the relative strength (q1D/p1D) of the disruptive technology since a relatively high network factor will lead to a delay in performance improvement in the early stage of introduction when only few design and engineering problems are solved.The above results can be extended to the situation where performance improvements can be achieved on both the traditional and new performance attributes. In other words, the performances of the disruptive and sustaining technologies will progress on both dimensions of performance attributes along the respective technology S-curves. Although most examples provided in Bower and Christensen (1995) and Christensen (1997) are technologies with performance improvements on only the traditional performance attributes (e.g., improving the storage capacities of hard disk drives with fixed sizes), we consider the case of two-dimensional performance improvements for completeness of our analysis. Since the analytical results are fairly complicated, we will assume that the initial levels of the traditional and new attributes of both the sustaining and disruptive technologies are zero for ease of exposition, though the results can be easily extended to the situation with positive levels of initial attributes. Let (p2D, q2D, M2D) and (p2S, q2S, M2S) be the additional sets of parameters of the technology S-curves along the dimension of the new performance attribute for the disruptive and sustaining technologies, respectively. Based on the condition in (3), we have the following result.Proposition 3If the improvements of the traditional and new performance attributes of the disruptive and sustaining technologies both follow the respective technology S-curves, the D-Day and V-Day, denoted bytN*andtE*, satisfy the following condition:(7)M1S[1−e−(p1S+q1S)(tk*−tS)]rD(tk*)1+(q1S/p1S)e−(p1S+q1S)(tk*−tS)−M1D[1−e−(p1D+q1D)(tk*−tD)]rS(tk*)1+(q1D/p1D)e−(p1D+q1D)(tk*−tD)M2D[1−e−(p2D+q2D)(tk*−tD)]rS(tk*)1+(q2D/p2D)e−(p2D+q2D)(tk*−tD)−M2S[1−e−(p2S+q1S)(tk*−tS)]rD(tk*)1+(q2S/p2S)e−(p2S+q2S)(tk*−tS)=vk2vk1,k∈{N,E}.The above nonlinear equation, despite its complexity, can still be solved with standard numerical methods but with more computational efforts. The effects of the ease and network factors on the performance improvement process derived from comparative analysis based on Proposition 2 can also be extended to the two-dimensional case. It can also be shown that most mathematical properties for the single-dimensional case can be applied to the two-dimensional case as well.To develop a systematic framework for ex ante quantitative evaluation of the timing of technology disruption, we now formally introduce the concept of “planning horizon for technology disruption” (PHTD), which is defined as “the future time for which an organization plans to exert time-based efforts, supported by sufficient financial and human resources, in improving technology performance in the process of new product planning and innovation.” That is, the planning horizon of a disruptive technology can be viewed as the length of future time within which a company plans to continuously keep a disruptive technology “alive” with sufficient supporting resources beyond the initial efforts such as exploratory market studies and the development of prototypes. It should also be noted that simply setting the planning horizon equal to the D-Day or V-Day, if exists, is not practical since dealing with technology disruption also requires significant financial and managerial commitments from a firm in developing the disruptive technology itself as well as the supporting collateral assets. Our integrated model, however, does provide a basis for determining the planning horizon through an interactive process taking into account the market, technology, and consumer factors as well as other relevant information. For example, the cash flows generated by the disruptive technology over time can be calculated from the estimated technology performance, market shares, and consumer valuations, which in turn could determine whether a particular length of planning horizon is financially sustainable.Given the PHTD of a potential disruptive technology and the D-Day (tN*) and V-Day (tE*) derived from Propositions 2 or 3, we now present a framework for technology evaluation and forecasting in the following proposition.Proposition 4(i) If PHTD <tN*, the disruptive technology will not invade either the new or existing market; (ii) IftN*≤PHTD<tE*, the disruptive technology will invade the new market only; and (iii) IftE*≤PHTD, the disruptive technology will invade both the new and existing markets.The proposition, which is summarized with a decision tree in Fig. 2, can be used as the basis for a firm or an industry (i.e., a group of firms with similar demand and supply characteristics) to characterize the possible outcomes of technology evaluation as well as to predict the future “fate” of a disruptive technology. Starting from the top of the decision tree, if the development effort for a technology is merely focused on the (incremental) improvements in the old (traditional) performance attribute without any new value proposition (e.g., improving the storage capacity of a hard disk drive with the same size or the fuel economy of a traditional gasoline-powered engine), the technology can only be characterized as a sustaining technology–Outcome I in Fig. 2. If a technology does offer a new performance attribute, but the D-Day is longer than PHTD (assuming that the disruptive technology starts at time zero), we have a disruptive technology with “no strategic significance” (Bower & Christensen, 1995) as Outcome II. This implies that, despite the new value proposition of the disruptive technology, the performance improvement characterized by its technology S-curve as well as the market conditions would not be enough for the technology to achieve any significant market success (e.g., Apple Newton, electric vehicles in the 1990s and early 2000s). If the planning horizon is between the D-Day and V-Day, we have a disruptive technology with strategic significance which will capture (create) the new (emerging) market only as Outcome III (e.g., the early generations of cell phone in the 1990s, Satellite radio, Microsoft WebTV, etc.). Finally, if the planning horizon is longer than the V-Day, we then have a disruptive technology with strategic significance which will capture both the new and existing (mainstream) markets as Outcome IV (e.g., personal computer in the 1990s and smart phones in the 2000s). By using the decision tree, decision makers would be able to assess the potential of a disruptive technology based on the nature of the technology characterized by its technology S-curve as well the market conditions characterized by other model parameters.44The conditions presented in Fig. 2 should not be confused with the conditions for the D-Day and V-Day to exist in Proposition 1 that is under the assumption that a firm will keep the disruptive technology alive until the entire performance improvement (M) is realized. Also, an infinite value can be assumed for the D-Day or V-Day that does not exist based on Proposition 1 when using the decision tree.From a decision-support perspective, another application of the decision tree is to forecast the lengths of time a firm needs to keep a potential disruptive “alive” in order to achieve different degrees of market success.We now analyze the condition of disruption to characterize an interesting phenomenon–the bleaks days–in the process of technology disruption. As mentioned previously, if p ≤ q, the curve of performance improvement will be S-shaped. Otherwise, the curve will be reverted U-shaped. Assume that the performance improvement curves of both the disruptive technology and sustaining technology are either S-shaped (i.e., p1D≤ q1Dand p1S≤ q1S) or reverted U-shaped (i.e., p1D> q1Dand p1S> q1S). This assumption is reasonable since it is unlikely for a new technology to completely deviate from the basic functional shape of a technology class.Most existing studies suggest that, for a technology that would eventually disrupt, the performance gap and/or the utility gap between the sustaining and disruptive technologies are decreasing over time (e.g., Adner, 2004, Bower & Christensen, 1995, Chen & Turut, 2013, Christensen, 1997). We first establish a sufficient condition for such a result to occur as follows.Proposition 5For a technology that would eventually disrupt with a feasible D-Day or V-Day, if the ease factor is greater than the network factor (p > q), the performance and utility gaps between the sustaining and disruptive technologies will be decreasing over time.As shown in the proof given in the Technical Appendix, if p > q, then F′(t) > 0, F″(t) < 0,Ukj′(t)>0andUkj′′(t)<0for t > 0, which suggests that the functions of utilities derived by customers from the disruptive and sustaining technologies are both reverted U-shaped (concave increasing). As depicted in Case 1 of Fig. 3, for a technology that would eventually disrupt (i.e. ∃t* > 0 such that UkD(t*) > UkS(t*) whereUkS(tD)−UkD(tD)>0), the performance gap and utility gap are both decreasing in t, i.e.,FS′(t−tS)−FD′(t−tD)<0andUkD′(t)−UkS′(t)<0fromt=tDtot=t*, which makes it easier to satisfy condition of disruption (5). This proposition implies that, if the influence of the ease factor is stronger than that of the network factor for a technology class, i.e., it is relatively easy to improve the performance independent of solving other design and engineering problems, one may expect that the performance and utility gaps between the sustaining and disruptive technologies will be continuously shortened as an encouraging sign for a firm to stay in the market and keep the disruptive technology alive over time.The above common result assumed and/or analyzed in most studies in the existing literature, however, cannot explain why some firms would make an early exit from a “promising” market if both the performance and utility gaps are continuously shortened. To unlock this mystery, we will investigate the other situation with p ≤ q where the network factor dominates the process of performance improvement in the next proposition.Proposition 6For a technology that would eventually disrupt with a feasible D-Day or V-Day, if the ease factor is small than or equal to the network factor (p ≤ q), the performance and utility gaps between the sustaining and disruptive technologies may be increasing during a period of time at the introduction stage before they eventually becomes decreasing over time.The proof of the proposition is given the Technical Appendix, which shows that, if p ≤ q for a technology class, there may exist a period of time, termed the “bleak days” in the remainder of the paper, when the performance and utility gaps between the sustaining and disruptive technologies are both increasing. For example, if the sustaining technology just reaches the peak of performance improvement (the inflection point of its technology S-curve) when the disruptive technology is introduced, then we can solve fortD=tS+ln(q1s/p1s)/(p1s+q1s)based on (3). Sincelimt→tDFD′(t−tD)=M1Dp1D, as shown in the proof given in the Technical Appendix, if the values of M1Dand/or p1Dare sufficiently small, we can identify the combination of model parameters to make utility gap increasing at and after tD. In other words, given an actual technology S-curve with the ease factor smaller than the network factor (p ≤ q), the performance improvement depends more heavily on how many other relevant design and engineering problems have been solved. Even for a technology that would eventually disrupt, there may exist some bleak days within which the performance and utility gaps are both increasing, i.e., it is more difficult to meet the condition of disruption in (5), due to a slow process of performance improvement and/or a slow rate of price reduction at the early stage55According to the utility function defined in (4), it is also possible that the performance gap is increasing while the utility gap is decreasing given a sufficiently large rate of price decreasing by the disruptive technology.. Case 2 of Fig. 3 shows an example of the bleak days during which the prospect of the disruptive technology looks “hopeless” since the performance and utility gaps are becoming larger over time (as opposed to being shortened as suggested by most studies in the existing literature). Many firms would thus choose to exit the market or discontinue their efforts in R&D and creating collateral assets during the bleak days (even though the D-Day and/or V-Day would eventually arrive if they continue their efforts beyond the bleak days). This provides a possible explanation for why major PC makers decided to halt their efforts in developing tablets which experienced some initial delays and setbacks in market growth and performance improvement with their prototypes in the early 2000s. It should be noted that, the bleak days have never received attention in the existing literature since such a period with increasing performance/utility gaps cannot be identified or characterized under the commonly assumed reverted U-shaped curve (Adner, 2004) or linear function (Christensen & Raynor, 2013) of performance improvement.In this section, we perform empirical tests based on data of three technologies: hard disk drives, semiconductors, and CPU for mobile devices. The purpose of the empirical analysis is to verify key assumptions of using the supply-based model to capture the technology S-curve and to demonstrate how to empirically estimate key model parameters with industry data for technology forecasting.Hard (rigid) disk drives are used as the primary examples of disruptive technologies in a number of influential articles and books by Clayton Christensen (e.g., Christensen, 1997, Christensen & Raynor, 2013). Therefore, our first empirical test of using the Bass model to capture the technology S-curve is based on data of average areal densities for different generations of hard disk drives available in Christensen (1992b, 1993, 1997), and Disk/Trend Report (1985–1999). Different generations of hard disk drives are primarily characterized by different platter sizes, which include Disk Pack, Winchester drive, 14-inch drive, 8-inch drive, 5.25-inch drive, 3.5-inch drive, 2.5-inch drive, and 1.8-inch drive. The reduced sizes over time provided customers with a new performance attribute: portability or better space utilization in design. When a new generation of hard disk drive with a smaller size was introduced, however, its storage capacity tended to be relatively low compared to the previous generations (with bigger sizes). Therefore, the areal density, as the measure of storage capacity, of each generation of hard disk drives is treated as the traditional performance attribute valued by the mainstream customers which was improved at “such a rapid rate” that the disruptive technology would eventually invade the established markets of the previous generations of hard disk drives (Bower & Christensen, 1995).For each generation of hard disk drives, the data of average areal densities (MB/inch2) are used to run a nonlinear least squares (NLS) regression to fit the functional form of the technology S-curve in (2) with an additional error term to estimate the values of parameters p and q. The value of the physical limit (M) is estimated from historical data of the maximum areal density achieved by each generation of hard disk drives. Table 1 presents the results of the empirical test based on data of hard disk drives. As shown in the table, the Bass model provides rather good statistical fits for the eight generations of hard disk drives. Specifically, the values of R-squared for all the technology generations are higher than 0.92. In addition, the parameter values of p and q (the ease and network factors) for most technology generations are statistically significant at 1 percent level except for the value of parameter p for the disk pack (whose p-value is 0.0785).Our second empirical test is based on data of normalized percentages of yield improvements of four generations of semiconductor technologies provided by the Taiwan Semiconductor Manufacturing Company (TSMC), one of the world's largest independent semiconductor foundries. In semiconductor manufacturing, the term “yield” is defined as the percentage of chips in a finished wafer that pass all tests and function properly. Similar to hard disk drives, different generations of semiconductor technologies are primarily characterized by the sizes of circuits, and a smaller size usually means better space utilization in design and manufacturing as a new performance attribute. (The term “size” is defined as the width of the smallest circuit wires on the chip.) Our empirical test is based on four generations of semiconductor technologies: 0.22 micrometer, 0.18 micrometer, 0.15 micrometer, and 0.13 micrometer. When a new generation of semiconductor technology with smaller circuit size is first introduced, however, the yield (non-defective rate, which is closely related to the unit production cost) tends to be relatively low due to various issues in design and manufacturing. Therefore, the normalized percentage of yield improvement (i.e., the reduction in normalized defect density) of each generation of semiconductor technology is treated as the traditional attribute in our empirical test. To demonstrate the effect of price decline, we also use the price-adjusted attribute levels based on the price decline function of semiconductors derived from Aizcorbe et al. (2007) in this test. Technically, such a test can be done by modifying parameter M in (2) as in the case of changing market potential of the original Bass diffusion (Lilien et al., 1992, Lilien & Rangaswamy, 2003).For each generation of semiconductor technologies, the quarterly data of price adjusted normalized percentage of yield improvement are used to run a nonlinear regression to fit the functional form of the technology S-curve based on (2). The value of the physical limit (M) is estimated from setting the maximum yield at 100 percent, which is a reasonable assumption since the normalized yield rates of most commercialized semiconductor technologies would eventually reach a level close to 100 percent over time as most technical problems in design and manufacturing are solved. Table 2presents the results of the empirical test based on data of the four generations of semiconductor technologies. According to the table, the Bass model provides fairly good statistical fits for the four technology generations with the values of R-squared greater than 0.94. In addition, the parameter values of p and q are all statistically significant at 1 percent level.The third empirical test is based on the performance data of CPU technologies for mobile devices. The performance improvement of CPU for mobile devices is a key factor that leads to the recent popularity of tablet computers. In the late 1990s and early 2000s, tablet computers only achieved limited market success largely due to the relatively slow performance improvements of the traditional single-core CPU technologies based on the mobile chips produced by either Intel (x86 core) or ARM (classic core). Since the mid-2000s, Apple Inc. and a few other companies have developed a range of “System on Chip” (SoC) to power their mobile devices, which has significantly changed the competitive landscape of the markets of tablet and laptop computers (Wall StreetJournal, 2011). Our test is based on data of two single-core CPU technologies, Intel x86 core and ARM classic core, as well as the tablet CPU performance data of Apple's SoC technology. Due to the different design architectures used in the three technologies, the measures of CPU performance in available data are also slightly different. Specifically, the traditional clock speed (in MHz) and Dhrystone (DMIP) per MHz are used as the performance measures for Intel x86 core and ARM classic core (SAP Archives, 2013; Toshiba Archives, 2011), while the SoC technology is tested with a CPU performance measure internally developed by Apple (Apple Archives, 2013, AnandTech, 2014). The M values of Intel x86 core and ARM classic core are based on the maximum performances achieved by the two technologies, while the M value of the SoC technology is based on the forecasted performance of octa-core SoC. As shown in the test results presented in Table 3, the Bass model provides fairly good statistical fits for the three technologies with the values of R-squared all greater than 0.8968. In addition, the values of all the parameters p and q are statistically significant at 5 percent level except for the parameter p of ARM classic core (with the p-value equal to 0.1218).One interesting application of the original Bass model is that some values of parameters p and q of analogous products are fairly close, which is formally verified by Sultan, Farley, and Lehmann (1990) with a meta-analysis of 213 products. With our relatively small sample sizes, it appears that this type of “macro patterns” of similarities among some values of parameters p and q across multiple technology generations may also exist based on our test results. As an example, for different generations of semiconductor technologies, the values of parameter p for the 0.22 micrometer, 0.18 micrometer, 0.15 micrometer, and 0.13 micrometer are rather close at 0.0006, 0.0006, 0.0008, and 0.0007, while the values of parameter q are relatively close at 0.1212, 0.1313, 0.1152, and 0.1292, respectively. This is an area which requires further research with larger sample sizes to explore a potential application similar to that of the original Bass model where the values of p and q of the technology S-curves derived from previous technology generations can be used to estimate the parameter values for a new technology generation.66The estimated values, however, still need to be adjusted by decision makers based on other relevant factors such as the nature of the technology, a firm's innovation capability, and market conditions, which is similar to the well documented process in applying the Bass diffusion model (Lilien and Rangaswamy 2003).Hard disk drives, semiconductors, and CPU for mobile devices are B2B products for which purchasing decisions are usually made by professional buyers/purchaser of an organization. As mentioned previously, it is generally agreed that the conjoint framework adopted in our model is suitable for both B2C and B2B products (Auty, 1995, Kivetz et al., 2004, Lilien & Grewal, 2012). This is because the typical issue of small sample sizes arising from a small population of purchasers under B2B settings does not invalidate research results since conjoint analysis is fundamentally performed at the individual level. Nevertheless, special caution still needs to be exerted to deal with potential measurement and implementation issues. For example, under the B2B settings, researchers may utilize an integrated approach with the combination of experimental work with choice modeling and analysis of real-world secondary data to enhance internal consistency and the likelihood to generate a joint preference among decision makers (Kivetz et al., 2004). If an empirical test is performed under the B2C settings, special attention should be paid to issues related to observability and complexity as discussed in Rogers (2003). For example, researchers may clearly describe a new performance attribute and its functionality with visual aids and user-friendly manuals to reduce complexity as well as to enhance observability.The test results for the three different types of technologies verify the effectiveness of our proposed use of Bass model to capture the time-based process of performance improvement along a technology S-curve. With the values of p and q, if the complete data on product prices and customer valuations are also available, the exact D-Day, V-Day, and bleak days can be derived based on our proposed model, as will be demonstrated in the next section.In this section, we will present a numerical example to demonstrate the applications of our proposed model, and discuss how to operationalize key model parameters in order to forecast the timing of technology disruption in practice.The numerical example is based on available data for the competitive market of rigid disk drives in the 1980s with the 8-inch disk drive as the sustaining technology and 5.25-inch drive as the disruptive technology. According to Christensen (1997), the prospect of the 5.25-inch architecture (introduced in 1980) did not look promising upon its introduction since the disruptive technology did not address the perceived needs of the mainstream market where minicomputer makers still preferred the 8-inch architecture (introduced in 1978) and other larger drives at that time. However, the 5.25-inch drive had features that appealed to the new desktop personal computer market segment just emerging in the early 1980s. Eventually, growth in the use of 5.25-inch drives occurred in two waves. The first followed creation of a new application: desktop computing, in which product attributes such as physical size, relatively unimportant in established applications, were highly valued. The second wave followed substitution of 5.25-inch disks for larger drives in established minicomputer and mainframe computer markets due to the rapidly increasing capacity of 5.25-inch drive. In the numerical experiment, the values of p and q are derived from the regression analyses presented in the previous section. The base prices and values of M for the two technologies are based on Christensen (1997) and Disk/Trend Report (1985–1999). The price decline function is based on the regression equation established by Komorowski (2009) for the hard disk drive industry. We also define the measure of “portability” as the reciprocal of the physical volume (in cubic feet) provided in Christensen (1997) for each of the two technologies.77The parameter values used in our numerical experiment are as follows:tS=1978,p1S=0.0063,q1S=0.3181,M1S=4721.39,F1So=57.80,F2So=3.05,tD=1980,p1D=0.0021,q1D=0.4076,M1D=10010.24,F1Do=43.73,F2Do=11.52,α=1andβ=1. The price decline function based on Komorowski (2009) isr(t)=10−0.2502(t−tj)+6.304.To minimize the effects of the absolute values of prices and customer valuations on consumer choices in the numerical experiment, we use the following modified utility function based on the Cobb–Douglas function:(8)UkD(t)=[vk1(F1Do+F1D(t))+vk2F2Do]/rD(t),andUkS(t)=[vk1(F1So+F1S(t))+vk2F2So]/rS(t),which is a monotonic variant of the utility function defined in (4). One advantage of the utility definition in (8) is that the D-Day and V-Day can be shown to be fully determined by the ratios of customer valuations (i.e., vk2/vk1) without the information about the exact values of customer valuations. By comparing the utilities received by customers in each segment from the disruptive and sustaining technologies, we may derive the critical times (D-Day and/or V-Day) given different values of customer valuation ratio (vk2/vk1) within the feasible range of the existence conditions, as presented in Fig. 4. For example, ifvN2/vN1=12andvE2/vE1=6, it can be estimated that the D-Day and V-Day are around the beginning of 1985 and the mid-year of 1986, respectively, for the disruptive technology (5.25-inch drive) to create an emerging market (desktop computers) and invade the mainstream market (minicomputers and mainframe computers).The analytical results derived from the above numerical experiment can also be linked to other models in the existing literature with different definitions of consumer tastes. For instance, Druehl and Schmidt (2008) assume that consumer tradeoff between two performance attributes is uniformly distributed. Given the density function of a particular probability distribution of consumer valuation tradeoff or a time-dependent function of demand S-curve (Adner, 2004), the analytical results in Fig. 4 can be used to estimate the critical times of disruption or the percentages of customers choosing the disruptive and sustaining technologies over time. For example, under the assumption of uniformly distributed valuation tradeoff between 2 and 12 (i.e., vk2/vk1 ∈ [2, 12]), it can be estimated that the market shares of the 5.25-inch and 8-inch drives would be 30 percent and 70 percent (customers with vk2/vk1 higher than 9, i.e. vk2/vk1 ∈ [9, 12], will choose 5.25-inch drive while others will choose 8-inch drive) in the beginning 1986 based on Fig. 4.Depending on the characteristics of the disruptive and sustaining technologies and consumers in different market segments, our model can also be modified to analyze the competitive situations under other assumptions such as horizontal differentiation (Adner & Zemsky, 2005) and different consumer choice rules. For example, based on the logit choice model (Anderson, de Palma, & Thisse, 1992), we may assume that the utilities received by consumers in market k ∈ {N, E} from choosing the disruptive and sustaining technologies areUkD=[vk1(F1Do+F1D(t))+vk2F2Do]/rD(t)+ɛkDandUkS=[vk1(F1So+F1S(t))+vk2F2So]/rS(t)+ɛkS, where ɛkDand ɛkSfollow independent extreme value distributions. Then the market shares of the disruptive and sustaining technologies, denoted by PkDand PkS, are(9)PkD=exp(UkD)exp(UkD)+exp(UkS)andPkS=exp(UkS)exp(UkD)+exp(UkS).Given the previously assumed parameter values, Fig. 5presents the market shares of the 5.25-inch drive in the new and existing markets based on the logit choice model wherevN1=0.01,vN2=0.12,vE1=0.015, andvE2=0.01. It is interesting to note that, while the 5.25-inch drive eventually captures both markets after 1988 according to the numerical experiment, the entire process is not a smooth ride for the disruptive technology at all. Due to the relatively small value of the ease factor (p1D=0.0021) of the 5.25-inch drive, there exists a period of bleak days where the performance and utility gaps between the 8-inch drive and 5.25-inch drive are increasing (instead of decreasing) right after the introduction of the disruptive technology in 1980, as shown in Fig. 5. This causes the 5.25-inch drive to even lose its initial market shares achieved in 1980 until around 1985. After the bleak days, however, the 5.25-inch drive is able to quickly close the performance and utility gaps due to its relatively large value of the network factor (q1D=0.4076) as more design and engineering issues of the disruptive technology are solved over time. As a result, the mass market is quickly captured in a relatively short period of time between 1985 and 1988, which would probably be too short for any firm, existing or new, to play catch-up if it decides to halt the initial development effort during the bleak days. It should be noted that the exact market shares and the pace for the disruptive technology to capture the new and/or existing markets may be different given different values of customer valuations and choice models. The basic pattern of market dynamics with the bleak days, however, is quite consistent given the same value of the ease and network factors used in the numerical experiment.

@&#CONCLUSIONS@&#
