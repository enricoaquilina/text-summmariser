@&#MAIN-TITLE@&#
A PSO-based rule extractor for medical diagnosis

@&#HIGHLIGHTS@&#
A PSO-based rule extractor called a PFHRCNN is proposed to overcome the bottleneck of the knowledge acquisition.The knowledge embedded in a trained PFHRCNN can be represented by a set of fuzzy If-Then rules.The performance of the proposed PFHRCNNs is demonstrated on three benchmark medical databases.

@&#KEYPHRASES@&#
Neural networks,Pattern recognition,Rule extraction,Fuzzy systems,PSO,

@&#ABSTRACT@&#
One of the major bottlenecks in applying conventional neural networks to the medical field is that it is very difficult to interpret, in a physically meaningful way, because the learned knowledge is numerically encoded in the trained synaptic weights. In one of our previous works, we proposed a class of Hyper-Rectangular Composite Neural Networks (HRCNNs) of which synaptic weights can be interpreted as a set of crisp If-Then rules; however, a trained HRCNN may result in some ineffective If-Then rules which can only justify very few positive examples (i.e., poor generalization). This motivated us to propose a PSO-based Fuzzy Hyper-Rectangular Composite Neural Network (PFHRCNN) which applies particle swarm optimization (PSO) to trim the rules generated by a trained HRCNN while the recognition performance will not be degraded or even be improved. The performance of the proposed PFHRCNN is demonstrated on three benchmark medical databases including liver disorders data set, the breast cancer data set and the Parkinson’s disease data set.

@&#INTRODUCTION@&#
Recently, neural networks have been widely applied to a variety of different fields. One of the most appealing aspects of neural networks is that they can inductively learn knowledge from given experimental data. Nevertheless, several popular neural networks suffer from lengthy training time. For example, multi-layer perceptrons (MLPs) incorporated with the backpropagation algorithm adopts an “instantaneous estimate” for the gradient of the error surface in weight space; therefore, it may zigzag its way about the true direction to a minimum on the error surface [1]. More seriously, it is very difficult to physically interpret a trained neural network. That is, the learned knowledge is numerically encoded in the parameters of a trained network. Obviously, this kind of knowledge representation does not give a meaningful expression of the qualitative aspects of human reasoning. This lack of explanation would make users (especially physicians) hesitate to make any important decisions simply based on the advices output from a black box.Fuzzy classification is a type of nonexclusive classification in which a pattern is assigned a degree of similarity to each class in a partition. Compared to neural networks, fuzzy systems have the advantage of providing a set of rules to justify their responses. In addition, laboratory tests and physical findings are prone to be incomplete or are apt to be distorted by measurement errors; therefore, the incorporation of fuzziness into class assignments further broadens their flexibility and robustness. However, fuzzy classifiers still encounter the same problem of the bottleneck of the knowledge acquisition. This leads to the need of machine learning systems which make possible the automatic generation of rules from available training data.Therefore, to overcome the bottleneck of the knowledge acquisition, several methods have been proposed for extracting either crisp or fuzzy rules directly from numerical data such as the nested generalized example (NGE) algorithm [2], the fuzzy adaptive learning control network (FALCON) [3], the fuzzy min–max neural network classifier (FMMC) [4], adaptive-network-based fuzzy inference systems (ANFIS) [5], HyperRectangular Composite Neural Networks (HRCNNs) [6], and the method proposed by Abe and Lan [7] (to name just a few here). Each approach has its own merits and disadvantages. For example, The NGE algorithm allows exceptions (i.e. counterexamples) to be stored inside hyper-rectangles, and exceptions to exceptions can be nested any number of levels deep. One possible consequence of this training strategy is that regions will fragment too much. In addition, the NGE algorithm produces neither If-Then rules nor decision trees. The FALCON is a five-layer network which integrates the basic functions of a fuzzy logic controller into a neural network structure. While many existing fuzzy systems adopt grid partitions in the input/output spaces, FALCON incorporated with the FALCON-ART learning algorithm partitions the input/output spaces in a more flexible way based on the distribution of training data so that it can avoid the combinatorial growing problem of grid partitions. The FMMC adopts a special expansion–contraction process to generate hyper-rectangles so that it can learn nonlinear class boundaries in a single pass through the data. Since it uses a user-defined threshold to control the maximum size of a hyper-rectangle, it does not have the flexibility to expand the generated hyper-rectangles as large as possible to include as many data points as possible. Similar to the FALCON, ANFIS is functionally equivalent to some fuzzy inference systems (e.g. Sugeno and Tsukamoto models). It has been proved that ANFIS is a universal approximator; however, it adopts the grid partitions so that the structure of the ANFIS may become huge for complicated problems. Although HRCNN can be guaranteed to achieve 100% correct recognition rate for the training data set, it may result in several ineffective hyper-rectangles that contain sparse data.In our previous work [6], we have developed a method for extracting crisp classification rules directly from numerical data. The method is based on applying the Supervised Decision-Directed Learning (SDDL) algorithm to train a HRCNN. The SDDL algorithm will sequentially generate a set of hyper-rectangles. Each generated hyper-rectangle which shows the existence region of several positive examples corresponds to a crisp If-Then rule. This kind of rule representation is very helpful to physicians because they require justifications for any diagnosis made by them, whether it arises from nature or computational intelligence. Although HRCNNS have many appealing properties (e.g., crisp If-Then rules, 100% correct recognition rate for training data, etc.), they suffer from one disadvantage. For some cases, HRCNNs may result in several ineffective classification rules which can only justify very few (e.g., one or two) positive examples. That is, those ineffective rules are with poor generalization ability. This motivated us to propose the PSO-based Fuzzy Hyper-Rectangular Composite Neural Network (PFHRCNN) which is proposed to overcome the disadvantages of HRCNNs. The basic idea is as follows. We first train a HRCNN to extract a set of crisp rules from a given data set. Then we apply the particle swarm optimization (PSO) to delete ineffective rules and fine-tune the remaining rules while the recognition performance will not be degraded or even can be improved. The particle swarm optimization (PSO) proposed by Kennedy and Eberhart has been receiving increasing amounts of attention from many different research fields due to its promising optimization ability and simplicity [8–10]. In the proposed PFHRCNN, a special encoding scheme was developed to apply PSO to implement some amendments (e.g., fuzzification and confidence factor) in order to compensate the side-effect resulted from the deletion of crisp rules.The remainder of this paper is organized as follows. Section 2 briefly reviews the HRCNNs and PSO. The PFHRCNN is introduced in Section 3. The simulation results are discussed in Section 4. Finally some concluding remarks are given in Section 5.HRCNNs are a kind of hybrid systems which integrate the paradigms of neural networks with the rule-based approach. A symbolic representation of a two-layer HRCNN is illustrated in Fig. 1. The mathematical description of a two-layer HRCNN with J hidden nodes is given as follows [6]:(1)Out(x̲)=f∑j=1JOutj(x̲)-η,(2)Outj(x̲)=f(netj(x̲)),(3)netj(x̲)=∑i=1nf((Mji-xi)(xi-mji))-nand(4)f(y)=1ify≥00ify<0where Mjiand mji∊R are adjustable synaptic weights of the jth hidden node,x=(x1,…xn)Tis an input pattern, n is the dimensionality of input variables, η is a small positive real number, and Out(x):Rn→{0,1} is the output function of a two-layer HRCNN with J hidden nodes. The parameters, Mjiand mji, can be physically interpreted as the high bound and the low bound along the ith dimension for a hyper-rectangle defined by them. Based on (1–4), one may find that the values of the synaptic weights of a trained HRCNN can be interpreted as a set of crisp If-Then rules. The If-Then classification rules extracted from a trained HRCNN with J hidden nodes can be represented as:(5)If(x̲∈[m11,M11]×…×[m1n,M1n])ThenOut(x̲)=1;⋮If(x̲∈[mJ1,MJ1]×…×[mJn,MJn])ThenOut(x̲)=1;ElseOut(x̲)=0;These rules can be interpreted as follows. If a data pattern,x=(x1,…xn)T, falls inside at least one of the J n-dimensional hyper-rectangles defined by [mj1,Mj1]×⋯×[mjn,Mjn] for j=1, … J, then the output of the network is one (i.e. Out(x)=1). The idea of using hyper-rectangles to represent rules can also be found in the nested generalized example (NGE) algorithm [2], the fuzzy min–max neural network classifier (FMMC) [4], the method proposed by Abe and Lan [7], and fuzzy ARTMAP [11]. These algorithms differ in their corresponding learning algorithms, computational complexity, and rule representation complexity.The supervised decision-directed learning (SDDL) algorithm is used to generate a two-layer HRCNN in a sequential manner by adding hidden nodes as needed [6]. First of all, training patterns are divided into two classes (1) a positive class from which we want to extract the concept and (2) a “negative class that provides the counterexamples with respect to the concept. A seed pattern is used as the base of the initial concept (i.e. an n-dimensional hyper-rectangle with arbitrarily small size). The seed pattern is arbitrarily chosen from the positive class. For example, if a pattern,x, is chosen to be the seed pattern, then the hyper-rectangle can be initialized to make M1i=xi+ɛ and m1i=xi−ɛ for i=1, …, n where the parameter ε is a small-valued positive real number. Then we try to generalize (expand) the initial concept (hyper-rectangle) to include next positive pattern. After this, we have to check whether there is any negative pattern falling inside the present hyper-rectangle in order to prevent the occurrence of overgeneralization. The following step is to fetch next positive pattern and to generalize the initial concept to include the new positive pattern. This process involves growing the original hyper-rectangle to make it larger to include the new positive pattern. After the process of generalization, again we use negative patterns to prevent overgeneralization. This process is repeated for all the remaining positive patterns. If there is any unrecognized positive pattern, another initial hyper-rectangle (hidden node) is generated and the process of learning is repeated again and again until all positive patterns are recognized. Therefore, as long as there are no identical data over different classes, we can obtain 100% correct recognition rate for training data at the end of the training procedure. For classifying multi-class patterns, a HRCNN is separately constructed based on its corresponding 2-class training data for each class.Although the SDDL algorithm guarantees that 100% correct recognition rate can be achieved at the end of training procedure, it may result in the generation of hyper-rectangles which can cover very few (even just one) positive examples. Such hyper-rectangles are then regarded as ineffective hyper-rectangles (i.e., ineffective rules). If we directly delete these ineffective crisp rules then the whole recognition performance would certainly degrade. Therefore, we need to adopt a compensation scheme to simultaneously trim the ineffective rules and keep the performance from being degraded or even can be improved.The particle swarm concept originated as a simulation of a simplified social system [8–10]. Particle swarm optimization (PSO) is similar to other evolutionary algorithms in the sense that the algorithm is initialized with a population of random solutions; however, it is motivated by the simulation of social behaviors rather than the natural selection and evolution mechanism. In PSO, the performance of these individuals is improved by cooperation, competition, and imitation among the individuals themselves through generations. PSO has undergone many modifications to improve its performance since its appearance in 1994. A good and complete introduction to these variations can be found in [10].In PSO, a particle is manipulated by the following two equations:(6)v̲i=w×v̲i+c1φ1×(p̲i-x̲i)+c2φ2×(p̲g-x̲i)(7)x̲i=x̲i+v̲iwhereviis particle i’s velocity through the parameter space,xiis the particle’s current position,pirepresents the best previous position of the ith particle,pgis the best position found by any individual of particle i’s neighborhood, w is the inertia weight. The parameters, c1 and c2, are the acceleration rates of the cognitive and social parts, respectively. The variables, φ1 and φ2, are two random variables with a uniformly distributed number between 0 and 1.The parameters, c1 and c2, determine the significance of personal experience,pi, and the social’s role model,pg, respectively. Similar to many other optimization algorithms, PSO must pre-specify its parameters such as the population size (i.e., the number of particles), the maximum velocity, the parameters, w, c1 and c2, and the topology of the swarm network. Eberhart and Shi suggested that the inertial weight decreases over time, typically from approximately 0.9 to 0.4 [10]. The performance of the PSO algorithm is greatly affected by the population size and the topology of the swarm.As mentioned in the previous section, HRCNNs sometimes suffer from the problem of generating several ineffective crisp rules. If we directly delete those ineffective crisp rules then the whole recognition rate will certainly degrade. To compensate the side-effect caused by the deletion of ineffective crisp rules, we can introduce the use of a confidence factor for each rule, incorporate some degree of fuzziness into class assignments, and adopt some kind of optimization scheme to fine-tune the remaining rules. These amendments are the contributions of the proposed PFHRNNs.The training algorithm for the PFHRCNNs involves the following three steps:•Step 1: Generation of Crisp RulesFor a 2-class classification problem, we use the SDDL algorithm to generate a HRCNN for the training data set which is consisted of positive examples and negative examples. This step results in a set of crisp rules which can explain the positive examples for the 2-class problem. As for classifying multi-class patterns, a HRCNN is separately constructed for each class. Suppose the number of hidden nodes corresponding to each class is Hk, k=1, …, K. That is, the kth class has resulted in HkIf-Then rules which are represented as Hk n-dimensional hyper-rectangles,[mj1(k),Mj1(k)]×…×[mjn(k),Mjn(k)],j=1,…,Hk.•Step 2: Optimization of Fuzzy RulesAs mentioned in the very beginning of this section, to compensate the side-effect resulted from the deletion of ineffective crisp rules, we can introduce the use of a confidence factor for each rule, incorporate some degree of fuzziness into class assignments, and adopt some kind of optimization scheme to fine-tune the remaining rules.First of all, we can use the following membership function to measure the degree with which a data pattern,x, belongs to a fuzzy hyper-rectangle defined by[mj1(k),Mj1(k)]×…×[mjn(k),Mjn(k)]for class k:(8)mj(k)(x̲)=e-sj(k)2(Perj(k)(x̲)-Perj(k))2(9)Perj(k)=∑i=1n(Mji(k)-mji(k))(10)Perj(k)(x̲)=∑i=1nmax(Mji(k)-mji(k),Mji(k)-xi,xi-mji(k))where sj(k) is a sensitivity parameter which regulates how fast the membership value, mj(k)(x), decreases as the distance betweenxand the hyper-rectangle defined by[mj1(k),Mj1(k)]×…×[mjn(k),Mjn(k)]. The parameter defined in (9),Perj(k), could be regarded as the value of the perimeter of the hyper-rectangle defined by[mj1(k),Mj1(k)]×…×[mjn(k),Mjn(k)]. If the data pattern,x, does not lie in the hyper-rectangle, then the hyper-rectangle will be expanded to include the data pattern,x. Therefore, the parameter defined in (10),Perj(k)(x̲), is the value of the perimeter of the expanded hyper-rectangle. Fig. 2gives a 2-dimensional example to interpret the meanings of (9–10). While the perimeter of the original rectangle defined by [0,4]×[0,2] is 12 (i.e.Perj(k)=12), the perimeter of the expanded rectangle incurred by the introduction of the data pattern (5, 3)Tbecomes 16 (i.e.Perj(k)((5,3)T)=16). Fig. 3illustrates an example of the membership function, mj(k)(x). Basically, if a data pattern falls inside the hyper-rectangle defined by[mj1(k),Mj1(k)]×…×[mjn(k),Mjn(k)], then its membership degree is 1.0. Otherwise, its membership degree will decrease as the distance between the data pattern and the hyper-rectangle. In Fig. 3, the rectangle is defined by [0,4]×[0,2]. Since the data pattern, (1, 2)T, falls inside the rectangle then its membership degree is 1.0 (i.e.,e-(12-12)2=1.0). On the other hand, the data point, (5, 3)T, falls outside the rectangle so its membership value decreases to approximate zero (i.e.,e-(16-12)2≈0when sj1=1.0 ore-0.2×(16-12)2≈0.04when sj1=0.2). In fact, the membership function defined in (8–10) has been introduced in our previous work [12–14] but we did not find a way to trim ineffective rules at that time.The inclusion of fuzzy logic into the rules does not guarantee that the classification performance can be surely improved. Therefore, we suggest incorporating a confidence factor, wj(k), to each rule in order to increase its flexibility. Then the location of the hyper-rectangle defined by[mj1(k),Mj1(k)]×…×[mjn(k),Mjn(k)]can be fine-tuned to increase its effectiveness.All these amendments are implemented by the use of the well-known optimization algorithm, PSO. The key issue for the use of PSO with the objective of deleting ineffective rules, fine-tuning the hyper-rectangles, finding the appropriate confidence factor for each rule while the recognition performance is not degraded or can be even improved is how to encode candidate solutions to the position vector of a particle in PSO.We use the following method to encode the all the amendments into a particle’s position vector. Each rule can be described by 2n+2 parameters,[mj1(k),Mj1(k)]×…×[mjn(k),Mjn(k)],sj(k),wj(k). Therefore, Hkrules have total Hk(2n+2) parameters to be optimized. We then cascade these parameters together into a∑k=1KHk(2n+2)×1column vector to represent a particle’s position vector as shown in Fig. 4. If a rule with the value of the product of wj(k) and mj(k)(x) is less than a pre-specified threshold (e.g., 0.2 in our simulations), then this rule is claimed to be unnecessary and must be deleted from the rule list. That is, this rule will not be counted in the computation of Eq. (11). The final output of the trained PFHRCNN is computed via Eqs. (11) and (12). The best particle is the particle with the largest recognition rate.Theoretically, the particles’ position vectors can be randomly initialized; however, in our simulations, it will take a huge number of generations to evolve a good solution. Therefore, we use the information from the extracted crisp rules to initialize the parameters of the hyper-rectangles,[mj1(k),Mj1(k)]×…×[mjn(k),Mjn(k)],fork=1,⋯,Kandforj=1,⋯,Hk. As for the initialization of the parameter, wj(k), it is initialized to be the ratio of the positive examples covered by the jth hyper-rectangle to the total positive examples for the kth class. The parameter, sj(k), can be randomly initialized to be a small-valued positive number or a pre-specified constant (e.g. 0.1 in our simulations).If a pre-specified number of generations is achieved or some kind of termination criterion is satisfied then we terminate the PSO updating procedure. At the end of the updating procedure, the best particle with the highest recognition performance is used to construct a PFHRCNN with the minimal number of fuzzy rules while the recognition performance is the highest one.•Step 3: ClassificationAfter step 2, a FHRCNN consisted of a set of fuzzy rules has been generated as shown in Fig. 5. Finally, an unknown data pattern is classified as the k*th class if the following conditions are satisfied:(11)Sk=maxj=1,⋯,Hlwj(k)mj(k)(x̲)(12)K∗=argmaxl=1,⋯,KSkwhere Skrepresents the degree with which the data pattern,x, belongs to the kth class. The firing strength of the jth rule for class k is the product of the membership degree,mj(k)(x̲), and the confidence factor, wj(k). The final decision is based on the maximum operator.To test the performance of the proposed PFHRCNNs, three medical benchmark data sets were downloaded from the repository of the University of California at Irvine, Repository of Machine Learning Databases (UCI) [15]. We briefly describe the three medical data sets used in our simulations as follows.1.Wisconsin Breast Cancer: The problem is to predict a tissue sample taken from a patient’s breast is malignant or benign. There are two classes, nine numerical attributes, and 699 observations. Sixteen instances are removed because they contain a missing attribute value.Liver Disorders: The problem is to predict whether a male patient has a liver disorder based on blood tests and alcohol consumption. There are two classes, six numerical attributes, and 345 observations.Parkinson’s Disease: This dataset is composed of a range of biomedical measurements from 31 people, 23 with Parkinson’s disease (PD). The main aim of the data is to discriminate healthy people from those with PD. There are 2 classes, 23 attributes and 197 instances.For each data set, data were randomly divided into a training data set consisting of half of the data and a testing data set consisting of the remaining half of the data for ten times. The parameters of PSO, c1, c2, and w, were respectively set to be 0.6, 0.6, and 0.1 for all the simulations. The population size and the upper limits of the number of generations were both set to be 100.Table 1tabulates the comparisons between the proposed PFHRCNN and the HRCNN. The mean value and the standard deviation value of the performances for the ten runs were reported in Table 1. Obviously, the trained PFHRCNNs outperformed the trained HRCNNs based on the comparisons of the number of rules and the average recognition performance. This confirmed that the amendments to HRCNNs were effective. The average computational time needed by the PFHRCNNs was 28.78s, 24.20s, and 13.77s for the Wisconsin breast cancer data set, the liver disorder data set, and the Parkinson’s disease data set, respectively. All these simulations were run on an Intel(R) Core(TM) i7 2.93GHz computer with 4GB RAM under Microsoft Windows 7 operating system.To investigate how the population size of the PSO affects the PFHRCNNs, we used another three different population sizes to rerun the optimization algorithms for all these three data sets. Fig. 6shows the performance achieved by the four different population sizes. The number of generations was set to be 100 for all the simulations. The main observations from Fig. 5 are the following.(1)For the liver disorder data set and the Parkinson’s disease data set, a large population size more or less improves the performance. A saturation phenomenon exists when the population size is 100. An even larger population size is not going to produce any significantly better results for the PFHRCNNs.For the Wisconsin breast cancer data set, a larger population size does not guarantee that the performance can be improved. The highest performance happens when the population size is 100.In our opinion, the best population size should be determined by experiments. It may vary with the complexity of the problem.In order to directly compare the simulation results achieved by the trained PFHRCNNs with some existing methods, we encapsulate the results provided by Lim et al. [16], Ramani and Sivagami [17], and Giotis and Petkov [18] (they reported the results of applying many machine learning algorithms against these three benchmark data sets) and insert PFHRCNNs in the appropriate location in the ordered list for each classification problem in Table 2. Obviously, the proposed PFHRCNN outperformed a number of well-respected classification methods on these three data sets. Tables 3–5tabulate the first two most representative rules for each data set, respectively. One may notice that why some ranges of input variables include negative regions but their feasible ranges are defined on positive ranges (e.g. the variables such as Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, etc., in the breast cancer data set). Actually, those negative values can be reset to be zero without degrading the performance since those variables will never violate the negative conditions. The negative regions are simply produced to make the generations of hyper-rectangles easier during the training procedures. By viewing Table 3, we can know that the most representative rules extracted for the breast cancer data set can be represented as follows:If(the clump thickness∈[0.555,9.622]∩…∩the mitosesε[-0.466,7.776]).Thenthe diagnosis is benign with confidence factor=0.928.Ifthe clump thickness∈[5.194,10.961]∩…∩the mitosesε[0.345,10.525]).Thenthe diagnosis is malignant with confidence factor=0.621.After the examination of the extracted prediction rules, several observations could be summarized as follows:1.The benign and malignant cells have their unique characteristics in morphology, as can be seen from their different thickness, size, shape, and cell margin. For example, a malignant tissue sample is prone to have larger cell size, more marginal adhesion areas, thicker chromatin, remarkable nucleoli, and active mitoses, etc.The densities of cell nuclei, chromatin, and nucleoli are representative of cell mitosis activity. The abnormal, uncontrolled mitosis is an important sign of malignancy. This is the reason why we choose these morphology characteristics as parameters in predicting malignancy.

@&#CONCLUSIONS@&#
This paper introduces a PSO-based rule extractor called PFHRCNN. The PFHRCNN was proposed to trim the crisp rule generated from trained HRCNNs while can maintain or even improve the whole recognition performance. The extracted rules can be represented by a set of If-Then rules which are easily interpretable to human users. Simulation results demonstrated the effectiveness of the proposed PFHRCNNs.