@&#MAIN-TITLE@&#


@&#HIGHLIGHTS@&#
We adapt the Tweedie distribution for modeling economic transactions.We address statistical and computational issues in parameter estimation.We develop an efficient exact algorithm for random variate generation.We empirically show the potential of the Tweedie model for anti-fraud analysis.

@&#KEYPHRASES@&#
Compound Poisson distribution,Exponential tilting,International trade,Lévy processes,Tempered stable distribution,

@&#ABSTRACT@&#
This paper shows the potential of the Tweedie distribution in the analysis of international trade data. The availability of a flexible model for describing traded quantities is important for several reasons. First, it can provide direct support to policy makers. Second, it allows the assessment of the statistical performance of anti-fraud tools on a large number of data sets artificially generated with known statistical properties, which must comply with real world scenarios. We see the advantages of adopting the Tweedie model in several data sets which are particularly relevant in the anti-fraud context and which show non-trivial features. We also provide a systematic outline of the genesis of the Tweedie distribution and we address a number of relevant statistical and computational issues, such as the development of efficient algorithms both for parameter estimation and for random variate generation.

@&#INTRODUCTION@&#
The regulatory framework of the European Union (EU) reserves to the EU institutions the responsibility of trade relations between the EU Member States and the non-EU countries (TFEU, 2012). It also gives mandates to both institutions and Member States to counter fraud and protect the financial interests of the Union (Council, 1995). This article is grounded on more than 15 years of work conducted in the anti-fraud context by the Joint Research Centre (JRC) of the European Commission (EC), in collaboration with the European Anti-fraud Office (OLAF), academic partners and relevant authorities in Member States. The ultimate target is the development of sound statistical methods for the analysis of international trade data, in order to detect customs frauds (e.g., under-valuation of import duties), trade-related infringements (e.g., money laundering) and cases of circumvention of EU trade regulations (e.g., anti-dumping and countervailing measures). The resulting signals are used in the definition of audit plans or for initiating investigations.The statistical tools that the JRC has implemented for anti-fraud analysis involve the inspection of the quantity of specific products which are imported in, or exported from, the EU market. It is crucial to rely on flexible statistical models for describing the distribution of these quantities for a large number of traded products. First, such models will provide direct support to the EU policy makers, in the form of tools for monitoring the effect of policy measures and for deciding how to react against international trade distortions originating in and outside the EU. For example, statistical models of trade can provide factual background for the official communications on trade policy (see, e.g., European Commission, 2012), or for the related preparatory technical documents (Lejeune et al., 2013).Another important goal for modeling traded quantities is the assessment of the statistical performance of alternative methods used for finding relevant patterns in international trade data. For anti-fraud purposes, outlier detection and robust clustering tools are typically required (see, e.g., Fogelman-Soulie, Perrotta, Piskorski, & Steinberger, 2008; Riani, Atkinson, & Cerioli, 2009; Cerioli, 2010; Perrotta & Kopustinskas, 2010; Cerioli & Perrotta, 2014). It is very difficult to derive analytical results for such methods in finite samples, especially when non-Gaussian distributions are involved. Therefore, the methods need to be compared, evaluated and eventually tuned on a large number of data sets artificially generated with known statistical properties, which must reflect the distributions observed in trade data. The compliance of such models with real world scenarios is of paramount importance, especially in the case of legal disputes, when statistical evidence is evaluated in Court. For example, Cerioli and Perrotta (2014) tackle the problem of assessing alternative robust regression clustering tools in a particular anti-fraud problem. They assume a Gamma distribution for simulating traded quantities, but with a rather ad hoc and product-specific choice of parameter values. Their approach, although effective in specific applications, is thus difficult to replicate in large-scale analyses of different products and lacks general probabilistic motivation. The adoption of nonparametric tools for density estimation is also problematic for similar reasons, as it does not allow for ready simulation of artificial data sets. Furthermore, it is not straightforward how to extend standard density estimation methods to the case of distributions with a non-negligible mass at (or close to) zero, as it often happens in the case of trade data.This article concentrates on the Tweedie distribution, a flexible three-parameter model which is fitted to the traded quantity of products of major importance for anti-fraud purposes and trade policy support. Similar models have been successfully adopted in different application fields, and seem to be particularly attractive in economics and finance (Babaei, Sepehri, & Babaei, 2015; Francq & Zakoian, 2013; Menn & Rachev, 2005; Rachev, Kim, Bianchi, & Fabozzi, 2011). The main applied contribution of our work is to show that the model that we propose is able to capture most of the basic features observed in international trade data. Such features are not easy to analyze, due to the combination of economic activities and normative constraints. Indeed, we typically have to face with markedly skew empirical distributions with heavy tails, a large number of rounding errors in small-scale transactions due to data registration problems, and structural zeros arising because of confidentiality issues related to national regulations. We see the advantages of adopting the Tweedie distribution in several data sets which are particularly relevant for the tasks sketched above. One is the category of the Petroleum oils and oils obtained from bituminous minerals, crude, of great importance for governments and policy makers. The second application area includes a range of fraud-sensitive products that are regularly monitored by the anti-fraud partners of the JRC.To achieve our applied goal we also face a number of methodological and computational issues. First, we provide a systematic introduction to the Tweedie distribution, unifying contributions which are currently scattered in the literature. We emphasize the stochastic genesis of this model either as an exponentially tilted stable or a compound Poisson distribution. We also describe how the Tweedie distribution can be embedded in the theory of Lévy processes, which has direct relevance to international trade. We argue that our systematic outline could be easily understood and provide motivation to applied scientists analyzing trade data. Then, we suggest new efficient computational algorithms both for estimating the parameters of the Tweedie model from observed trade data and for random variate generation in Monte Carlo studies. Computational performance of the estimation method represents a crucial issue in anti-fraud problems, as the model needs to be estimated on thousands of possible products. Similarly, the availability of efficient and easy-to-implement simulation algorithms is an important ingredient for performing large scale assessments of the effectiveness of statistical anti-fraud tools.The rest of the paper is organized as follows. In order to sharpen the focus of our work, in Section 2 we make some connections with approaches in Operations Research and Economic Theory dealing with international trade. The genesis and format of the trade data that we need to model are introduced in Section 3. Then, Section 4 gives an account of the Tweedie distribution, relates it to Lévy processes and motivates its use for trade data. In Section 5 we propose an efficient approach to parameter estimation and we provide efficient random variate generators from the Tweedie model. In Section 6 we apply the model to empirical cases of major importance in anti-fraud analysis, while Section 7 provides some comparisons with respect to competing models. In Section 8 we assess the stability of our models over time. Closing remarks are given in Section 9. Appendix A contains the pseudo-code of our simulation algorithms, while Appendix B reports descriptive statistics for the trade data that we analyze.The dynamics of international trade has been traditionally studied on the basis of economic theory models using methodologies from different disciplines, in particular Operations Research, Information Theory and entropy optimization principles. A classic starting point is a linear programming cost minimization model that relates the flows of commodities between two geographic regions to the production, consumption and costs of transportation (see, e.g., Harris, 1974). In this model, if qijis the quantity of product shipped from an origin i to a destination j and cijis the related transportation, eligible flows configurations are found by minimizing the objective function∑i∑jcijqij,subject to the constraints qij≥ 0,∑jqij=Qi.for each origin i, where Qi. is the total production at origin i (used as surrogate for the total export from i), and∑iqij=Q.jfor each destination j, where Q.jis the total consumption at destination j (used as surrogate for the total import at destination j). Additional constraints are often considered to address realistic trade patterns, a popular one being the entropy function−∑i∑jqijlnqij(see e.g. Erlander, 1977; 1982).A second research line applies entropy maximizing principles to various sets of assumptions of typical trade frameworks, to derive the most unbiased (or less informative) distributions for the trade quantities or for other trade statistics of interest. This is achieved by specifying a finite number of moment values and by choosing, out of all the probability distributions with these moment values, the one which maximizes an entropy measure. In the discrete case this is the Shannon entropy or (in case a prior probability distribution is also given) the Bayesian entropy. Lagrange multipliers are then used to solve the constrained problem. The principle, originally proposed for discrete data, was generalized to the continuous case by Jaynes (1968). Its application to modeling transportation and international trade are described in Kapur (1993, pp. 425–428). Other examples of use of the maximum entropy principle in international trade are discussed by Wilson (1970). The approach is flexible and can be also combined with rather complex assumptions on trade dynamics; see, e.g., the integration of Lotka–Volterra “predator–prey” type dynamics recently proposed by Fray and Wilson (2012), or the possibility to include transaction costs (Zhang, Liu, & Xu, 2012).To give an idea of the distributions that can be derived for modeling international trade with the maximum entropy principle let us now assume, following Kapur (1993), that Q.jis the total import at destination j in the current year; similarly, Qi. is the total export at origin i in the current year. In addition, let∑i∑jqij=Qand definepij=qij/Q,pi·=Qi./Qandp·j=Q.j/Q. We do not know the qijvalues for the current year, but we can assume to know averages from previous years and define a priori estimates for the unknown proportions pij, saypij*. In this setting, by maximizing the entropy function under the given constraints on past averages and on current total import/export values obtained from an external source (e.g. information on production/consumption, as above), we find the maximum entropy estimatesp˜ij=pij*pi·p·jaibj,where aiand bjare determined by usingai∑pij*p·jbj=1andbj∑pij*pi·ai=1. The maximum entropy estimatesp˜ijwill be finally used to infer the volumes qijtraded in the current year between each pair of countries.Although rooted in the same types of data, these research lines are different from our approach. Typically the goal is to build input–output models for the trading countries. For example, one can use the model to check if a certain policy objective, e.g. to increase GDP per capita, is feasible and what is the best trade setting to achieve it. Then, the trade data are used to validate the relationships between the model factors, calibrate the model parameters and study the dependence of the initial conditions on the model dynamics. The tools of sensitivity analysis can be an extremely useful support for such a purpose (Saltelli, Tarantola, Campolongo, & Ratto, 2004).On the other hand, the aims of our statistical perspective are to justify on solid grounds the choice of a certain density function for the trade data population and to fit its parameters on the basis of the observed trade sample data. Our selected model, i.e. the Tweedie distribution, has a clear interpretation according to the actual value of its estimated parameter and also a substantive motivation in terms of the generating economic process, as shown in Sections 4.1–4.3 below. Furthermore, we can also verify the quality and stability of our estimated models by checking their distribution for data chosen according to different criteria (randomly, in different time windows, etc.).To understand the main links between the two perspectives we recommend reading the excellent book of Kapur (1993, pp. 266–291), which also contains elegant mathematical programming formulations of relevant entropy maximization problems (pp. 553–564) and interesting links between the probability distributions derived through the maximum entropy approach and the frequencies in the cells of contingency tables (pp. 252–265). In particular, it shows how to derive the classical χ2 test for testing the independence of attributes in contingency tables from the maximum entropy principle. This and other relationships were brought up by Theil (1967) and, again in relation to international trade, also by Wilson (1970).Our data come from the official extra-EU trade statistics, Extrastat, extracted from the COMEXT database of Eurostat. We consider monthly aggregates of trade quantities for each product, country of origin and country of destination, registered in the period from August 2008 to July 2012. We construct one data set for each product. A glimpse into one of them is provided in Table 1, whereEU1,EU2,…denote the EU Member States andEU¯1,EU¯2,…are non-EU countries. Each observation in the data set is the amount of trade for the selected product that took place from a non-EU country to a Member State in a given month. This observation may be the result of a single transaction, or, more frequently, it is the aggregate over several transactions possibly involving different traders. The aggregates are built from the customs declarations collected from individuals or companies by the Member States, following a strictly regulated process (Eurostat, 2006). The products are specified according to a numeric code of the Combined Nomenclature (CN). We model data specified at the maximum level of accuracy generally accessible for both imports and exports, that is eight CN digits. This level of classification, containing more than 10,000 sub-headings, in general is sufficiently detailed to distinguish the products by their material, function and degree of processing.Depending on the product, the traded quantities are expressed in tons of net mass and/or in supplementary units (liters, number of items, etc.). The weight of packaging is not included in the net mass. With the aim of simplifying customs operations, small scale transactions may not be declared by the traders. The thresholds are fixed by Member States within the maximum limit of 1 ton for the net masses and 1000 euros for the values. However, the national administrations have to make estimations for trade below the thresholds, for which there are no common methodologies. In addition, when statistical units risk to disclose information on individual traders, the declared quantities and/or values may be hidden to the user by replacing them with a zero. Again, each Member State fixes the criteria to decide which data units should be treated as confidential. As a result of these issues, the data include monthly aggregates with quantity rounded to zero but of positive value and also (but less frequently) aggregates of positive quantity but value rounded to zero. Data quality is quite heterogeneous across countries and products and, for this reason, is subject to constant monitoring by the statistical authorities and customs services. However, only macroscopic outliers are removed or corrected.The typical combination of trading factors, regulation constraints and reporting errors (either fraudulent or not) gives rise to empirical distributions of quantities that are markedly skew, with heavy tails and a considerable number of transactions very close, or even equal, to zero. To be concrete, Fig. 1reports the empirical distribution of traded quantities, in millions of tons, for product “Petroleum oils and oils obtained from bituminous minerals, crude” (CN-27090090) in the selected time frame (August 2008 to July 2012, chosen to cover a sufficiently long period). This product is the most relevant one for the EU balance sheet, covering alone around 17 percent of all EU trade imports. Its distribution, which is described and analyzed in detail in Section 6.1, is the result of 6651 transactions, concerning imports of the selected product in all Member States from non-EU countries. With a slight graphical abuse, the black bin of the histogram depicted in the left-hand panel corresponds to 286 transactions for which the recorded quantity is exactly zero, due to rounding errors or—more often—to confidentiality issues. The long and slowly decaying right-tail is paramount, as is the spike of transactions for which the recorded quantity is positive but very close to zero. Indeed, in this example slightly more than 20 percent of the observations fall in the first strictly positive bin of the histogram, which corresponds to a traded quantity of about 45,000 tons. The largest transaction exceeds 4,220,000 tons. The empirical masses concentrated both at the origin and in a relatively small neighborhood of it have great influence on parameter estimation for the candidate statistical models which are fitted to such data, as well as on the assessment of model accuracy. Any credible modeling must deal carefully and appropriately with the small values, since they comprise a large part of the data set. We thus need a statistical model with enough flexibility to accommodate both these masses and the tail of the distribution.We emphasize that, although the qualitative tendency observed in international trade data is general, the quantitative precise specification of the features described above is product-dependent and cannot be anticipated without visual inspection of the data, which is impossible in routine anti-fraud applications. Therefore, our distributional model must fit adequately well to a large number of data sets without prior specification of any tuning constant. It is also impractical to model the rounding and recording errors precisely, given the currently available information, since regulations and controls still vary considerably from Member State to Member State. We find that our model based on the Tweedie distribution has the required flexibility for many products which are relevant in anti-fraud analysis.The Tweedie distribution has been popularized and analyzed at length by Jörgensen (1987), following the seminal ideas of Tweedie (1984). For some parameter values the distribution has been also introduced by Hougaard (1986). Further discussion is contained in Aalen (1992) and Barndorff-Nielsen and Shephard (2001). Hougaard (1986) gives the Laplace transform of the Tweedie r.v. X with the following parameterization:(1)LX(s)=E[exp(−sX)]=exp[(δ/α)(θα−(θ+s)α)],Re(s)>0,where the parameter space is given by(α,θ,δ)∈{]−∞,1]×]0,∞[×]0,∞[}∪{]0,1]×{0}×]0,∞[}and the boundary forα=0has been managed for analytical continuity.It is worth considering the family morphology as parameters vary. Since α crucially determines the characteristics of the Tweedie distribution, we will focus on the two fundamental subsets of its parameter space in Sections 4.1 and 4.2, respectively.We first assume that α ∈ ]0, 1]. Let Z be a positive Stable r.v. of index α (see Sato, 1999, for details). The Laplace transform of the scaled r.v.Y=(δ/α)1/αZisLY(s)=exp(−(δ/α)sα),Re(s)>0.In such a case, expression (1) may be rewritten asLX(s)=exp[−(δ/α)(θ+s)α]exp(−δθα/α)=LY(θ+s)LY(θ),i.e. an exponentially-tilted stable r.v. X with tilting parameter θ is achieved. See, e.g., Devroye and James (2014), Lijoi and Prunster (2014), and Favaro and Nipoti (2014) for details on this distribution.If fXand fYare the p.d.f.’s of X and Y with respect to the Lebesgue measure onR,we obtainfX(x)=exp(−θx)fY(x)LY(θ).SincefY(x)=∑n=1∞(−1)nδnn!Γ(−nα)αnx−nα−1I[0,∞[(x),where IBis the usual indicator function of a given set B (Sato, 1999), it followsfX(x)=exp(−θx+δθα/α)∑n=1∞(−1)nδnn!Γ(−nα)αnx−nα−1I[0,∞[(x).A number of special distributions are contained in the family for α ∈ ]0, 1]: the positive Stable r.v. Z is achieved forθ=0andδ=α,whileα=1/2yields the inverse Gaussian distribution. In addition, the Dirac mass atx=δis achieved ifα=1for any θ ≥ 0. Finally, the Gamma distribution with shape parameter δ and scale parameter (1/θ) is obtained as α↓0. The parameters α and θ thus rule the tail heaviness of the distribution. Specifically, for α ∈ ]0, 1], in the limiting case whereθ=0moments exist solely for order r < α, while for θ > 0 the moments of all orders exist, even if the right tail tends to be heavier as θ approaches zero.Whenα∈]−∞,0[,expression (1) may be rewritten as(2)LX(s)=exp[−(δθα/α)((1+s/θ)α−1)],i.e. the law of X may be expressed as a compound of a Poisson distribution, with parameter(−δθα/α),and Gamma r.v.’s with shape parameter(−α)and scale parameter 1/θ (Aalen, 1992). If {Gn}n ≥ 1 is a sequence of copies of such Gamma r.v.’s, while N is a Poisson r.v. with parameter(−δθα/α),X is thus stochastically represented as(3)X=L∑n=1NGn,with the assumption that X degenerates at zero ifN=0. Hence, in this case X displays a mixed distribution, given by a convex combination of a Dirac distribution (with mass at zero) and an absolutely continuous distribution. By exploiting the properties of Gamma r.v.’s, the distribution function of X isFX(x)=exp(δθα/α)I[0,∞[(x)+[1−exp(δθα/α)]∫−∞xgX(u)du,where gXis the p.d.f. of X conditioned to the event {N > 0}. From (2), we obtaingX(x)=exp(−θx+δθα/α)1−exp(δθα/α)∑n=1∞(−1)nδnn!Γ(−nα)αnx−nα−1I[0,∞[(x).Hence, gXcoincides with fXfor α ∈ ]0, 1] up to the factor[1−exp(δθα/α)]−1.Again, some special distributions are contained in the family: the non-central Gamma distribution of zero shape is achieved forα=−1,while the Poisson distribution is obtained asα→−∞. In turn, the Gamma distribution with shape parameter δ and scale parameter 1/θ is accomplished as α ↑ 0.In order to motivate the Tweedie model for trade data, it is interesting to provide the genesis of the distribution in the framework of Lévy processes. Informally speaking, a non-negative Lévy process is a stochastic jump process with non-negative, independent, time-homogeneous increments. See Sato (1999) for a formal definition. The Laplace transform of such a process, say Xtwith t ≥ 0, is given by the Lévy–Khintchine representationLXt(s)=E[−sXt]=exp[−tψ(s)],Re(s)>0,where t is the time parameter of the process, while the functionψ(s)=∫(1−e−sx)ν(dx)is referred to as the characteristic exponent of the Lévy process and ν is the so-called Lévy measure. The family of Lévy processes encompasses many remarkable special cases, such as the compound Poisson processes, the Gamma processes, the Stable processes, among others. Indeed, all non-negative Lévy processes are limits of compound Poisson processes.In trade, an observation corresponding to a given positive variable may be ideally seen as the result of a cumulative process in which the marginal contributions occur independently during time and at the same rate. Indeed, the COMEXT data introduced in Section 3 are precisely the result of such an aggregation, involving all the imports of the selected product in a given month. It is apparent that this concept may be modeled as a Lévy process, and the stochastic outcome at a given time is actually the observation on hand. Hence, it is interesting for our purposes to embed the Tweedie distribution in the framework of Lévy processes.When α ∈ ]0, 1[, by taking an exponentially-tilted stable Lévy measure such thatν(dx)dx=λΓ(1−α)x−1−αexp(−θx)I[0,∞[(x),where λ > 0, it follows thatψ(s)=λΓ(1−α)∫0∞(1−e−sx)x−1−αexp(−θx)dx=λα((θ+s)α−θα).Hence, the Laplace transform of the process Xtat time t is given byLXt(s)=exp[(λt/α)(θα−(θ+s)α)],Re(s)>0.By lettingλt=δfor a fixed t, the Tweedie r.v. for α ∈ ]0, 1[ may thus be considered as the “outcome” of an exponentially-tilted stable Lévy process. On the contrary, a non-stochastic process is actually involved in the caseα=1,since the Tweedie distribution degenerates to a Dirac mass in that situation (see Brix, 1999, p. 933).Subsequently, let us takeα∈]−∞,0[. In this case, a Poisson process of rate λ is running on time scale t and each jump is assumed to be a Gamma r.v. (independent of the past) with shape parameter(−α)and scale parameter 1/θ. This compound Poisson process Xtis actually the sum of the Gamma r.v.’s up to time t. The Laplace transform of the process Xtat time t is given byLXt(s)=exp[λt((1+s/θ)α−1)],Re(s)>0,and hence Xtis a Lévy process with characteristic exponent given byψ(s)=λ(1−(1+s/θ)α).By lettingλt=−δθα/αfor a fixed t, it is apparent that the Tweedie r.v. forα∈]−∞,0[may be considered as the “outcome” of this compound Poisson process.It is intuitively apparent from the discussion outlined above that the flexibility of the Tweedie distribution makes it an attractive candidate for modeling trade data sets which follow the qualitative pattern depicted in Fig. 1. According to its actual parameter values, it can accommodate tails of different length and thickness, as well as allow for the height and the position of the spike close to the origin.We emphasize that the Tweedie model is widely applicable to data from different countries and also of possible different quality concerning under-reporting issues. As this model fits automatically the presence of null and very small values, it does not require that the user specifies the actual threshold for rounding errors, nor the precise mechanism which generates under-reporting for small trade quantities. These are clearly appealing features for routine application to a wide range of products ranging from oil to textile goods and food. In addition, the contribution of each Member State to trade is often very different for each product, thus making the effect of state-specific regulations difficult to anticipate in general terms.The relationship with Lévy processes provides an additional motivation for the use of the Tweedie distribution with international trade data. In fact, the quantities recorded in a given temporal and spatial market are obtained as additive aggregates of transactions occurred at a smaller scale, up to the individual company level. This cumulative trading process may be represented by sums of increments of the typeXti−Xti−1, so that Xtbecomes the cumulative traded quantity of a given product up to time t. Therefore, modeling the generation mechanism of trade data through the Tweedie distribution may also represent an appealing choice from an economic point of view.As suggested by one referee, further insights on the substantial mechanism generating trade data can be achieved on the basis of the Lévy process representation through the stochastic measure considered by Brix (1999). Indeed, Brix (1999) emphasizes that this stochastic measure may be seen as a sum of infinitely many terms for α ∈ ]0, 1[, since the Lévy measure ν does not integrate, i.e.ν(]0,∞[)=∞. Hence in this case the Lévy process has infinitely many jumps and 0 is the accumulation point of the infinite sequence of jumps. On the other hand, forα∈]−∞,0[the stochastic measure is a sum of a finite number of terms, in such a way that the jumps follow a Gamma distribution with shape parameter(−α)and scale parameter 1/θ. The two different features may thus be taken to represent alternative trading schemes suitable for different markets. In fact, for many consumer goods it may be reasonable to assume that the traded quantities arise from a large number of transactions that take place almost instantaneously, as it happens in the case α ∈ ]0, 1[. On the other hand, for products like the Petroleum oils introduced in Section 3, the traded quantities are typically obtained through a limited number of relatively large transactions, corresponding to a finite number of jumps in the underlying stochastic mechanism, as is the case whenα∈]−∞,0[. These jumps might be potentially very high, but their effect is tempered by θ. The estimated values of α and θ in the Tweedie model can thus shed light on the actual trade process that rules the observed quantities. Examples of this interpretation are shown in Section 6.IfX1,X2,…,Xnrepresent n independent copies of a Tweedie r.v. X with α ∈ ]0, 1], the likelihood function (with respect to the Lebesgue measure) is given by(4)L(α,θ,δ)=exp(nδθα/α)∏i=1nexp(−θxi)∑j=1∞(−1)jδjxi−jα−1j!Γ(−jα)αj.In contrast, whenα∈]−∞,0[,M observations out of n are null, where M is a Binomial r.v. with parameters n and exp (δθα/α). By reindexing in such a way that the first(n−m)sample realizations are non-null, the resulting likelihood function (with respect to the Dirac measure at zero and the Lebesgue measure) is given by(5)L(α,θ,δ)=exp(nδθα/α)∏i=1n−mexp(−θxi)∑j=1∞(−1)jδjxi−jα−1j!Γ(−jα)αj.Obviously, the series in (4) and (5) should be truncated at a given value in order to practically handle the function. We may expect the resulting approximation to be accurate with few terms, at least when the observed xiare not too small or too large (Dunn & Smyth, 2005). Alternatively, if the series does not converge quickly, the suggestions provided by Palmer, Ridout, and Morgan (2008) and Dunn and Smyth (2008) for numerical inversion of the Laplace transform may be considered.In the analysis of international trade, data sets often contain thousands of observations. Furthermore, hundreds or thousands of data sets typically need to be scrutinized in sequence, both for routine inspection over different categories of products and for performance assessment of anti-fraud tools. The computational performance of estimation methods thus becomes a crucial issue. In order to set up a computationally efficient approach to the evaluation of the likelihood functions (4) and (5), we consider the generalization of the Inversion Theorem given by Barabesi and Pratelli (2015).First, let us assume that α ∈ ]0, 1]. In this case, if h is a positive measurable function defined onRsuch that 0 < E[h(X)] < ∞ and provided thatψh,X(t)=E[h(X)exp(itX)],where i is the imaginary unit, the expression(6)fX(x)=12πh(x)∫−∞∞exp(−itx)ψh,X(t)dtholds a.e. with respect to the Lebesgue measure onR. The integral in (6) eventually represents a Cauchy principal value. In this case, if θ ≠ 0 and by choosingh(x)=exp(cx)with c < θ, this representation reduces to(7)fX(x)=exp(−cx+δθα/α)2π×∫−∞∞exp[−itx−(δ/α)(θ−c−it)α]dt.After a little algebra and a change of variable in the integral, expression (7) gives rise to(8)fX(x)=12πδ1/αexp[−(δ1/αθ−ϕ)(δ−1/αx)+δθα/α]×ηα,ϕ(δ−1/αx),whereϕ=δ1/α(θ−c),while(9)ηα,ϕ(u)=∫−∞∞exp[−itu−(1/α)(ϕ−it)α]dt.If the real and the imaginary parts of the integrand are considered, by means of a change of variable, expression (9) may be rewritten as(10)ηα,ϕ(u)=2∫0∞exp[−kα,ϕ(t)cos(lα,ϕ(t))]×cos[−tu+kα,ϕ(t)sin(lα,ϕ(t))]dt,where(11)kα,ϕ(t)=α−1(ϕ2+t2)α/2andlα,ϕ(t)=αarctan(t/ϕ).In practice, the combination of (8) and (10) allows us to rephrase fXin a new integral form, in such a way that the integral solely depends on the parameter α, since ϕ may be pre-fixed at a given value—obviously, by choosingc=θ−δ−1/αϕ. It is clear that ϕ should be selected as a convenient value for the evaluation of the integral. As an example,ϕ=1could be a compromise choice, since this selection implicitly leads us to consider a Tweedie r.v. whose expectation equals to one in the integral part. In addition, fXis expressed in such a way thatγ=δ1/αappears as the “natural” scale parameter. Hence, the likelihood function may be rewritten as(12)L(α,θ,δ)=exp(nδθα/α)(2πδ1/α)n×∏i=1nexp[−(δ1/αθ−ϕ)(δ−1/αxi)]ηα,ϕ(δ−1/αxi).Finally, the caseθ=0—corresponding to the positive Stable distribution—may be handled by means of suitable methods existing in the literature.Whenα∈]−∞,0[,it is not possible to adopt immediately the generalization of the Inversion Theorem, since in this case the Tweedie r.v. displays a mixed distribution. However, it is feasible to apply the theorem to gX. By remarking that the Laplace transform of the r.v. X conditioned on the event {N > 0} is given by(LX(s)−exp(δθα/α))/(1−exp(δθα/α)),gXmay expressed as(13)gX(x)=12πδ1/αexp[−(δ1/αθ−ϕ)(δ−1/αx)+δθα/α]1−exp(δθα/α)×φα,ϕ(δ−1/αx),where(14)φα,ϕ(u)=∫−∞∞exp(−itu)[exp(−(1/α)(ϕ−it)α)−1]dt.In this case, (14) may be reformulated asφα,ϕ(u)=2∫0∞(exp[−kα,ϕ(t)cos(lα,ϕ(t))]×cos[−tu+kα,ϕ(t)sin(lα,ϕ(t))]−cos(tu))dt.Therefore, the likelihood function reduces to(15)L(α,θ,δ)=exp(nδθα/α)(2πδ1/α)n−m×∏i=1n−mexp[−(δ1/αθ−ϕ)(δ−1/αxi)]φα,ϕ(δ−1/αxi).We finally remark that the integral representations (8) and (13) also provide the basis for finding suitable expressions of the information matrix and its estimated counterpart.We now discuss alternative ways of generating random values from the Tweedie distribution. This task is an important ingredient in any Monte Carlo study which aims at assessing the statistical properties of anti-fraud tools (Cerioli & Perrotta, 2014), when applied to products for which the Tweedie distribution represents a satisfactory model.We start with the case α ∈ ]0, 1[, for which Hofert (2011) suggests both a naive algorithm and an improved version of it, even if these proposals are inefficient as δ increases. On the other hand, Devroye (2009) introduces a more efficient—even if complex to implement—algorithm, subsequently corrected and improved by Hofert (2011). Finally, Ridout (2009) considers numerical inversion of the Laplace transform. See also Bianchi and Fabozzi (2014).Barabesi and Pratelli (2015) propose a universal algorithm which combines efficiency and simplicity if applied to the Tweedie r.v. for α ∈ ]0, 1[ (see also Barabesi & Pratelli, 2014). Following their approach, let us consider the functiona(ρ)=exp(δθα/α)π∫0∞exp[−δkα,θ−ρ(t)cos(lα,θ−ρ(t))]dt,where kα, ϕ(t) and lα, ϕ(t) are defined in (11). Note that the function a(ρ) is defined forρ∈]−∞,θ[. Moreover, let(16)a1=a(−ρ1),a2=a(ρ2),v=a(0),(17)b1=min(1ρ1+ρ2loga2a1,1ρ1logva1),b2=max(1ρ1+ρ2loga2a1,−1ρ2logva2),(18)w1=a1exp(ρ1b1)Aρ1,w2=a2exp(−ρ2b2)Aρ2,w3=v(b2−b1)A,where(19)A=A(ρ1,ρ2)=a1exp(ρ1b1)ρ1+a2exp(−ρ2b2)ρ2+v(b2−b1)is the rejection constant of the algorithm, i.e. the expected number of iterations in the algorithm. We choose ρ1 and ρ2 as(ρ1*,ρ2*)=argminA(ρ1,ρ2),where minimization is taken under the constraint on a(ρ).Our proposed algorithm for simulating the Tweedie distribution as an exponentially tilted stable law, which makes use of the quantities defined in Eqs. (16)–(19), is detailed in Appendix A as Algorithm 1. It is apparent that our algorithm is efficient in terms of the rejection constantA(ρ1*,ρ2*),which is computed in Table 2for selected values of α, θ and δ. It is seen that the rejection constant of the algorithm suggested by Devroye (2009) is always larger (and often much more so) than that for Algorithm 1, in spite of the fact that we have provided our method in a very basic form. In fact, our algorithm could be further improved by using clever computational tricks, such as “recycling” to save the generation of a uniform random variate in the loop, and suitable “squeezes” based on the characteristic function of X to skip the direct computation of fXmost of the times. In any case, direct evaluation of fXmay be eventually avoided by means of the series method (Devroye, 1986), even if at the cost of a more involved algorithm.Whenα∈]−∞,0[,the stochastic representation (3) yields the efficient algorithm given in Appendix A as our Algorithm 2. Indeed, the sum of i.i.d. copies of Gamma r.v.’s is in turn Gamma distributed, so that Algorithm 2 only requires generation of Poisson and Gamma variates, which are widely available.This section describes two relevant sets of products on which we have satisfactorily fit the Tweedie model. Being able to anticipate the distribution of traded quantities for these products is important both for policy support and anti-fraud purposes. Other distributions have also been considered for the same data and the comparison is discussed in Section 7, together with additional insights on the fit, residual analysis and some computational details. The Matlab code that was used to analyze the data is available on request.International political economy is especially interested in the 27th chapter of the CN, including “Mineral fuels, mineral oils and products of their distillation; bituminous substances; mineral waxes”. In the period considered the chapter alone covered more than 27 percent of all EU imports. Within the chapter, we select the product of largest share (around 17 percent of all EU trade imports), i.e. “Petroleum oils and oils obtained from bituminous minerals, crude” (CN-27090090). The EU is highly import-dependent for this product, with major impact on the balance of payments. There is also academic interest for this product. Crude oil cannot be studied under the classical price theory assumptions relying on perfect competition. In fact, the product is quite heterogeneous, with an actor, OPEC, with big market power but not behaving as a typical cartel (the OPEC members often pursue different policy goals). The volume of available reserves is unclear and oil demand is variable, even within the EU. Furthermore, oil prices are greatly influenced by the rapid growth of demand from emerging countries and by the availability of unconventional resources (shale oil). There is little agreement in economic research about how all these factors impact on the demand and, therefore, on the oil quantities imported in the EU. In this complex context, sound empirical approaches are clearly needed to support economic modeling of demand and supply. Our trade quantity model offers a tool for assessing and validating different price-quantity proposals.Descriptive statistics of the Petroleum oil quantities, reported in Appendix B, give an idea of the order of magnitude of this trade. Perhaps surprisingly, given the nature of the product, a non-negligible amount of quantities are recorded as zero, in order to ensure confidentiality of the traders involved in such transactions. However, these null values cannot be modeled from a purely economic perspective since the precise incidence of confidentiality-related underreporting is unknown and regulations vary considerably from one market to another. The null quantities, together with the spike of transactions of relatively small quantities, are likely to influence the fit of any distribution to such data. Fig. 2displays the fit of the Tweedie model and the estimated parameters. Visual inspection shows the fit to be satisfactorily good. Indeed, the Tweedie distribution is effectively able to capture both the long tail of the empirical distribution and the considerable mass of observations close to zero. Furthermore, the negative value ofα^is coherent with the qualitative interpretation of the nature of the jumps that are expected to give rise to the traded quantities of this product. Further quantitative insight on the fit is reported in Section 7.Many different categories of goods were seized in the last years by OLAF investigations and joint customs operations. We select a number of particularly sensitive products, from numerous chapters of the CN, among those that have been object of official EU press releases, which can be retrieved from the database http://europa.eu/rapid of the European Commission. We also classify the products according to the main fraud area to which they were associated by the press releases, excluding those at risk of smuggling (e.g., cigarettes and garlic), because the fraudulent amounts of such products escape from the data recording process. Our first category is very heterogeneous, and is related to the counterfeiting problem. It includes products such as wine (in CN-2204), alcohol (in CN-2208), textiles (from CN-50 to CN-63), footwear (in CN-64), electrical and electronic apparatus (in CN-84 and CN-85), accessories of motor-vehicles (mostly in CN-8708) and watches (from CN-9108 to CN-9110). A second less heterogeneous category, with more than 80 fruit and vegetable products in CN-20, relates to the problem of mis-declaration of product or origin.We discuss here the findings of our empirical analysis of four products, two for each fraud category, but the results are similar for most of the sensitive products that we have classified. In order to study also the effect of the zeros on the fitted model, for each fraud category we select one product for which many null quantities are recorded and one without zeros. Combed wools and Wines with Protected Designation of Origin (PDO) or Protected Geographical Indication (PGI) are major products at risk of counterfeiting. Mushrooms are often improperly classified for evading duties, or their origin is misdeclared for bypassing theauthorized exporter’s quotas. In particular, many false classifications were reported for the ‘Agaricus’ type, even with implications on food safety. Cherries are an example of fruit found to contain or being preserved with sugar or alcohol, but declared otherwise. This relates to duty evasion but also to illicit sugar trade practices.Descriptive statistics for the four selected fraud sensitive products are in Appendix B, while Fig. 3shows the fit and the estimated parameters. From the histograms it is clear that the products have different data distributions. In two cases—depicted in plots (b) and (d)—there is also a non negligible mass of exactly null quantities that determine, as in the application to Petroleum oils, a negative estimate of parameter α. In the two other instances—plots (a) and (c)—all quantities are strictly positive, but there is a very large amount of observations close to the origin, thus implying that rounding to zero and confidentiality issues have a negligible impact for these products. As a result, we obtain positive estimates of α, which are coherent with the idea of a large number of almost instantaneous transactions typical of consumer goods, while the estimated values of θ are very close to zero. This suggests a stable-type shape of the empirical distribution which is automatically captured by our Tweedie model, without any tuning intervention. We thus see that, independently of the specific data generating mechanism, the Tweedie distribution is able to fit effectively both sides of the empirical distribution of traded quantities in all the selected anti-fraud examples.We now compare the fit of the Tweedie and that of other five candidate models. We define the alternative models by mixturing some popular skew distribution functions for positive values, listed in Table 3, with a Dirac mass at the origin. In most cases the skew distribution functions that we select were successfully applied to describe economic patterns, such as income distribution (see, e.g., Clementi & Gallegati, 2005; Fisk, 1961). The Gamma distribution is one of such alternative candidates. As described in Section 4, it is also a special case of the Tweedie model and, for that reason, it has been considered as a potential competitor in several application fields (see, e.g., Palmer et al., 2008).Classical models for skew distributions cannot be directly applied to products for which null quantities are observed. Since our goal is to provide comparison among models that could be routinely applied to international trade data without tuning intervention, we augment the standard two-parameter models with a spike at the origin. We thus fit the three-parameter mixture distribution(20)FX(x)=πI[0,∞[(x)+(1−π)GX(x),where GXis the distribution function of any of the two-parameter random variables listed in Table 3 and π ∈ ]0, 1[ is an additional parameter which represents the probability of observing a quantity rounded to zero. Therefore, this mixture may be seen as a somewhat ad hoc three-parameter extension of traditional models for skew distributions, defined in view of the specific structure of trade data and mimicking the stochastic representation (3) of the Tweedie distribution.We stress that it would be unfair to compare our approach with the standard two-parameter versions of the distributions given in Table 3, since they would not be able to allow for the presence of null values in many data sets. On the other hand, we frame our comparison in a “worst-case” scenario for the Tweedie model, since mixture (20) exploits some prior information about the structure of the data which is not available for the Tweedie distribution, placing the Dirac mass exactly where it is required, i.e. at the origin. Our reference distribution is also more parsimonious than the four-parameter generalized skew normal distribution proposed by Mazzuco and Scarpa (2015) to allow for bimodality in asymmetric patterns, which cannot model in any case the probability mass at zero. Incidentally, it should be remarked that the Tweedie distribution may even display bimodality, in addition to the automatic modeling of the zero mass forα∈]−∞,0](see Aalen, 1992). The Tweedie distribution itself could be used as an ingredient in mixture (20), if a less parsimonious four-parameter model is required due to the complexity of data. However, we do not investigate this possibility in detail since our goal is to compare relatively simple three-parameter alternatives for routine analysis of trade data sets.We estimate the three parameters in mixture (20) by means of Maximum Likelihood. We compare all models in terms of their maximized loglikelihood values and well-known divergence measures in Table 3, where the best values for each criterion are highlighted in bold. This allows an exhaustive analysis of the accuracy of the fit. In fact the likelihood evaluates the quality of the fit of the density function, while the other measures evaluate the divergence in terms of the distribution function. In particular, we use the Kolmogorov–Smirnov distance, which is perhaps the most natural candidate in the present context, and the Anderson–Darling distance, which places more weight on observations in the tails of the distribution. Since the Tweedie distribution function cannot be written in closed form, it is computed from samples of 10,000 randomly generated observations, simulated using the random number generator described in Section 5.2 and the parameter estimates obtained for each product. Note that the optimal model in terms of Maximum Likelihood also optimizes the BIC and AIC comparison criteria, given that all the alternative models considered in our comparison have the same number of parameters and that the sample size is fixed for each product. Moreover, since mixture (20) is not nested in the Tweedie model, the likelihood ratio tests of Palmer et al. (2008) cannot be carried out to formally compare the Tweedie model to its competitors.In terms of maximized loglikelihood, our results show that the Tweedie distribution provides the best fit, and in some cases by a large extent, while there is no clear evidence on the ranking of alternative models. For Combed wool and Preserved cherries, the fit of the Gamma version of the three-parameter mixture (20) is very close to that of the Tweedie distribution. However, in other cases it is the Weibull distribution which comes second, thus showing that it is not straightforward to specify a flexible and simple alternative to the Tweedie model which holds for all these data. Divergence measures confirm the general good fit of the Tweedie model. It is undoubtedly the best model for Combed wool and Preserved cherries. It can be considered the best performer also for Mushrooms Agaricus, since it optimizes the Anderson–Darling distance, which places more importance on the tail of the distribution, and it is very close to the best values for the other two divergences. For the remaining two products, the performance of the Tweedie fit is very close to the correspondent best model.As a further drawback of mixture (20), we note that for two products (Wine and Mushrooms) and the Lognormal version of mixture (20) the Matlab estimation algorithm failed to converge, despite our careful tuning of tolerance options. We take this outcome as a further drawback of standard Maximum Likelihood when applied to mixture (20), where there is a significant mass probability at one point in the domain of the distribution and the method may become unstable.On the other hand, the Maximum Likelihood approach to the Tweedie model described in Section 5.1 did not suffer from computational problems. Specifically, we truncated the series involved in (5) at 170 terms, although a much smaller number—such as 20 or 30—was generally sufficient for achieving convergence in most of the simulations we performed and data sets we analyzed. The Matlab function “mle” with options ‘MaxIter’ = 1000 and ‘MaxFunEvals’ = 2000 was used to maximize the likelihood. The starting values of the parameters were selected according to the moments of the empirical distribution, and eventually perturbed in case of no convergence of the algorithm. With these settings we did not experience convergence problems in the fitting process of the Tweedie distribution. Furthermore, we double-checked our results by implementing the likelihood equations as Mathematica functions. Eqs. (12) and (15) obviously involve integrals whose integrands are oscillating functions, in such a way that their oscillating behavior is ruled by the parameter ϕ. As suggested in Section 5.1, this parameter was generally set to unity and the required integrals were computed by means of the numerical integration routines provided by Mathematica in the standard setting (i.e., without adopting special tunings or options for oscillating integrands). The achieved values of the Maximum Likelihood estimates consistently matched for the two softwares, also in the case of quite extreme data sets with a considerable mass at zero. Therefore, we argue that the Tweedie model appears to be more stable than mixture (20) also from a computational point of view.We supplement our comparisons through some residual analysis on the fit of the different models. Fig. 4 reports the estimated quantiles under the six fitted models against the quantiles of the empirical distribution function (reported as the dashed straight line on the diagonal of each plot) for two products: Combed wool and Petroleum oils. Due to the spike at the origin, we do not plot the quantiles associated with probabilities smaller than 0.2. Furthermore, to facilitate interpretation, the estimated quantiles of the Tweedie model are represented with a thick black solid curve. Again, the Tweedie quantiles are computed from samples of 10,000 randomly generated observations, simulated using the random number generator described in Section 5.2 and the parameter estimates obtained for each product.We cannot expect one single model to uniformly dominate all the others over the entire support of the distribution. Nevertheless, it is seen that the fit of the Tweedie distribution is generally good and it is especially so in the right tail of the empirical distribution, even in the case of Petroleum oils for which other candidate mixtures (20) exhibit slightly lower divergences. These QQ-plots thus provide visual confirmation that the excellent global performance of the Tweedie model in Table 3 is not marked by isolated deviations due to some specific transactions.From our comparisons, we conclude that the Tweedie distribution offers an effective modeling solution even when compared to mixtures that incorporate prior information on the structure of the data, and that may be thus favored in the analysis of traded quantities. It's very flexible parameterization allows a simple and statistically sound representation of empirical data which can be useful in many anti-fraud and policy support settings. It also qualifies the Tweedie distribution as an ideal candidate for Monte Carlo simulations mimicking the real structure of international trade data.The Tweedie parameters estimated in Section 6 for Petroleum oils and the other fraud-sensitive products are based on data covering a four-years period (i.e. August 2008–July 2012). This time-length interval is usually classified in economics as “middle-term”. In order to validate the good behavior and the proper fitting of the Tweedie model also in the short-term, we have repeated the analysis on five shorter and consecutive sub-periods, at least for the products that guarantee a sufficient number of observations in each sub-period (namely Petroleum oils, Wine PDO and PGI protected and Mushrooms Agaricus). We provide here only the case of Petroleum oils (Table 4), since results and conclusions for the other products are very similar.The sub-period analysis highlights two important facts. First, the Tweedie model still offers the best fitting in terms of maximized loglikelihood (and then also in terms of BIC and AIC) in most of the sub-periods considered, when compared to the alternative candidate mixtures (20). Moreover, it guarantees also the highest average loglikelihood per observation, a value that can be considered for summarizing the general performance of a model over the five sub-periods. Second, the parameter estimates remain stable and do not appreciably differ from the values estimated for the whole period (see Fig. 2). This limited volatility of the coefficients reflects the substantial stability of the quantity distribution over the five sub-periods, as the minor changes in the descriptive statistics proves. Furthermore, following the discussion at the end of Section 4.4, it confirms that the trade mechanisms operating in these markets are essentially stable over time. So we can conclude that the choice of a specific time window does not affect the main substantial findings, as is desirable in our operative context.

@&#CONCLUSIONS@&#
We have shown the wide applicability to international trade data of a flexible parametric model based on the Tweedie distribution. Our proposal is particularly suitable for products of paramount importance in the EU market, especially for anti-fraud purposes. To this aim, we have solved a number of tricky statistical issues concerning model parameterization, estimation and random variate generation. These issues are important both for practical routine implementation of our model and for performing Monte Carlo evaluation of the properties of anti-fraud tools.We have developed this work in support of the EU decision-makers and law enforcement services. Nevertheless, we believe that the applicability of our model is much wider. For example, the precise description of the trade dynamics that it provides can help both national services in estimating trade balance and private companies in simulating the effect of market dynamics. Our focus has been on modeling the trade quantity, but this variable has direct, often linear, relation with capital flows and prices. Economic theory has widely studied how sensible fluctuations in the exchange rates can determine reactions in the trade prices and, in turn, changes in the trade quantities (see, e.g., Goldberg & Knetter, 1997; Arkolakis, Costinot, & Rodriguez-Clare, 2012; Burstein & Gopinath, 2015). Economic research has addressed these relationships with particular attention to strategic trade areas affected by imperfect competition, such as the petroleum oils. To be concretely applicable to trade, monetary and inflation policies, these studies need to be supported by empirical evidence that we argue can be found in data and models such as those addressed in this paper.In addition to the stated goals of policy support and data simulation, the availability of a close-to-reality model for trade quantities can also help to investigate how different conditions, or structural changes, in the international economy affect transactions in the EU market. As we have shown, this goal can be achieved through seeing whether the estimated values of the parameters of the Tweedie distribution remain stable over time, especially when different economic phases are considered. Similarly, the estimated parameters of the Tweedie model could be used to check homogeneity of trade within the same CN chapter.We close this discussion of other potential applications of our model with a mention of the COMTRADE database (http://comtrade.un.org), also formed by monthly aggregates built on the basis of the UN protocols. COMTRADE is geographically broader than COMEXT, as it collects data from all members of the United Nations, but the products are defined only at six-digits level in the Harmonized System (HS) classification. Therefore, COMTRADE contains less sub-headings than COMEXT (about 6000). It will be an interesting task for future research to investigate whether the approach presented in this work is applicable to these “less precise” aggregates or, in other words, to check at what extent the granularity of the classification impacts on the estimated model.