@&#MAIN-TITLE@&#
Multi-objective portfolio optimization considering the dependence structure of asset returns

@&#HIGHLIGHTS@&#
We investigate the portfolio optimization problem as a bi-objective problem.We employ stable distribution to capture fat tailed property of return series.Dependence structure is considered through an appropriate copula function.Two algorithms are designed based on MOPSO to tackle resulted MINLP.The numerical studies exemplify the effectiveness of the proposed algorithms.

@&#KEYPHRASES@&#
Metaheuristics,Portfolio optimization,Stable distribution,Copula function,Multi-objective particle swarm optimization,

@&#ABSTRACT@&#
Portfolio optimization context has shed only a little light on the dependence structure among the financial returns along with the fat-tailed distribution associated with them. This study tries to find a remedy for this shortcoming by exploiting stable distributions as the marginal distributions together with the dependence structure based on copula function. We formulate the portfolio optimization problem as a multi-objective mixed integer programming. Value-at-Risk (VaR) is specified as the risk measure due to its intuitive appeal and importance in financial regulations. In order to enhance the model's applicability, we take into account cardinality and quantity constraints in the model. Imposing such practical constraints has resulted in a non-continuous feasible region. Hence, we propose two variants of multi-objective particle swarm optimization (MOPSO) algorithms to tackle this issue. Finally, a comparative study among the proposed MOPSOs, NSGAII and SPEA2 algorithms is made to demonstrate which algorithm is outperformed. The empirical results reveal that one of the proposed MOPSOs is superior over the other salient algorithms in terms of performance metrics.

@&#INTRODUCTION@&#
Most of the improvements in the course of portfolio optimization are attributed to Markowitz (1952), who formulated the portfolio selection problem as a quadratic programming. He modeled the uncertainties associated with financial institutions through the incorporation of probability and optimization theory. Markowitz's model was based on two conflicting criteria: maximizing the return of a portfolio contrary to minimizing the risk of a portfolio. His model, however, was influenced by some restrictive assumptions. One of the main assumptions was that the joint distribution of the assets was followed by a multivariate normal distribution whereas asset returns empirically are not symmetrical (Fama & Roll, 1971; Mandelbrot, 1963a, 1963b). Hence, many researchers have tried to employ asymmetric and fat-tailed distributions like α-stable distribution (Chambers, Mallows, & Stuck, 1976; Kanter, 1975). α-Stable distribution is the generalization of a normal distribution, which allows high skewness, excess kurtosis, and occurrence of extreme values. Nolan (2003) gave some examples about the application of stable distribution in finance. Rachev, Stoyanov, and Fabozzi (2007a) also incorporated some distributional models like stable distribution with portfolio optimization and risk management context. Adcock (2014) considered portfolio selection problem for the case when asset returns were defined by multivariate skew-Student's t-distribution.Furthermore, many implementations, in practice, have used a covariance matrix to capture dependencies among the assets. While it can be cumbersome to make an accurate estimation of the covariance matrix, estimation errors significantly affect the resulting portfolio proportions (Fabozzi, Petter, Dessislava, & Sergio, 2007). There are also some kinds of dependencies like comonotonicity and rank correlation that are not captured by covariance matrix. Alternatively, copula functions provide a convenient tool to describe the stochastic dependence structure among the random variables. Copulas are a powerful tool because they link joint distribution to its margins in such a way that each margin can be modeled individually by a specific type of distribution (Kumar, 2011). Nelsen (1999) and Joe (1997) studied copula from statistical and mathematical perspectives. Embrechts, McNeil, and Straumann (1999) applied copula functions in finance for the first time. Cherubini, Luciano, and Vecchiato (2004) investigated the application of the mathematical copula in credit risk analysis and derivative pricing. Much research has been done to fit an appropriate copula to the financial data. Dobrić and Schmid (2005) examined various bivariate copulas to fit to the asset returns contained in DAX. They stated that neither Gaussian nor Archimedean copulas could describe the dependence structure. Actually, only Student's t copula with low degree of freedom produced acceptable results. Munnix and Schafer (2011) analyzed the tail-dependency structure in S&P500 constituents with a copula-based approach. They understood that Gaussian copula underestimates the negative tail dependence.Markowitz's model is also being questioned for its use of variance as a risk measure. Variance calculates both fluctuations above and below the expected value in the same way. However, in reality, only returns below the mean are undesirable and consistent with the notion of risk. Another drawback of variance is that since it does not recognize asymmetry property associated with asset return distributions, it can be only justified to elliptical distributions. To compensate this, some researchers have concentrated on using more practical risk measures such as MAD (Konno & Yamazaki, 1991), SMAD (Speranza, 1993), LPM (Bawa & Lindenberg, 1997), VaR (Linsmeier & Pearson, 2000), CVaR (Rockafellar & Uryasev, 2000), and CDaR (Chekhlov, Uryasev, & Zabarankin, 2000). Benati and Rizzi (2007) suggested a mixed integer linear programming model for portfolio optimization problem with VaR as a risk measure. They employed historical method to measure the risk. Overall, they showed that the model fell into the so-called NP-hard class. Rockafellar and Uryasev (2000) provided an attractive linear programming model for CVaR minimization problem, allowing the handling of portfolio with a large number of observations. However, they still used historical approach. Gaivoronski and Pflug (2004) approximated VaR by a Smoothed function called SVaR, which filtered out the local irregularities. They also pointed out that efficient frontiers constructed on the basis of other risk functions could not approximate mean-VaR efficient frontier properly. Goh, Lim, Sim, and Zhang (2012) introduced Partitioned VaR (PVaR) by using half-space statistical information to incorporate asymmetry in the distributions of asset returns. They discussed that the PVaR approach always generated better solutions respect to the traditional Markowitz's mean-variance approach. Kim, Giacometti, Rachev, Fabozzi, and Mignacca (2012) proposed a multivariate market model, which was defined by a mixture of the multivariate normal distribution and the tempered stable distribution. They derived closed-form solutions for the marginal VaR as well as the marginal Average VaR (AVaR) and applied them to optimize the portfolio with 29 stocks. We refer the readers to De Giorgi (2002), Kou, Peng, and Heyde (2013), Krokhmal, Palmquist, and Uryasev (2002), Krokhmal, Uryasev, and Zrazhevsky (2005),Pflug (2000), and Rockafellar and Uryasev (2013) for a detailed discussion on using different risk measures in portfolio optimization problem.Cardinality Constraint Mean Variance (CCMV) problem is classified as NP-complete problem (Bienstock, 1996), resulting in the exponential growth of the computation time with the size of the problem. Additionally, Eberle (2009) argued that scenario-based VaR optimization problem is NP-hard. So metaheuristics have received more attention to cope with this issue. Chang, Meade, Beasley, and Sharaiha (2000) investigated three heuristics, namely Genetic Algorithm (GA), Simulated Annealing (SA), and Tabu Search (TS) for the CCMV problem. They showed that cardinality constraint makes the efficient frontier to become discontinuous. Woodside-Oriakhi, Lucas, and Beasley (2011) examined Markowitz's model with additional cardinality and threshold constraints. They applied GA, SA, TS algorithms, which were hybridized with subset optimization. Albeit subset optimization problems were QMIP, but since they were dealing with a limited number of assets, they could be solved relatively quickly. The above authors compared their results with those of Chang et al., (2000) and understood that their heuristics gave better quality solutions, but at a more computational time. Chang, Yang, and Chang (2009) addressed portfolio optimization problem with different risk measures viz. variance, semi variance, MAD, and variance with skewness. They introduced a novel GA and verified it by three datasets collected from main financial markets. Deng, Lin, and Lo (2012) suggested an improved PSO for CCMV with bounding constraints. They presented computational results for different implementations of the algorithm containing up to 225 assets. They further argued that their algorithm is much more robust and effective than other PSO variants. Some other studies that exploited metaheuristic approaches to solve CCMV or CCMV with additional constraints are: GA (Soleimani, Golmakani, & Salimi, 2009), PSO (Golmakani & Fazel, 2011), SA (Crama & Schyns, 2003), and Ant Colony Optimization (Maringer & Kellerer, 2003).Multi-objective portfolio optimization problems have gained momentum in recent years. The main advantage of them is that they obtain the efficient frontier in a single run. Armananzas and Lozano (2005) adopted greedy search, SA, and ACO to deal with multi-objective CCMV. Anagnostopoulos and Mamanis (2009) examined the ability of three Multi-objective Evolutionary Algorithms (MOEAs), that is, NSGAII, SPEA2, and PESA to solve the problem. Their model was based on CCMV with additional class and quantity constraints as well as the replacement of variance with VaR and CVaR measures. They used the historical data of S&P100 to conduct the experiments. Jana, Roy, and Mazumder (2009) introduced a multi-objective model for portfolio rebalancing, which was enriched by an entropy objective function to produce a well-diversified efficient frontier. They took a trapezoidal possibility distribution into account as the possibility distribution of the returns, and used a fuzzy programming method to solve the model. Wang, Li, and Watada (2011) considered the mean-variance-VaR portfolio selection model. They utilized the property of fuzzy VaR to evaluate the future risk and designed a MOPSO algorithm to find the Pareto front. Here (Anagnostopoulos & Mamanis, 2011; Branke, Scheckenbach, Stein, Deb, & Schmeck, 2009; Skolpadungket, Dahal, & Harnpornchai, 2007) are some other publications, which treat the portfolio selection problem as a multi-objective problem.This paper contributes to the portfolio optimization context from two conspicuous aspects. First, we conduct a comprehensive analysis to model the stochastic dynamism of financial data. We exploit α-stable distribution together with a copula function to determine the dependence structure more accurately. Unlike most of the publications in the portfolio context for the incorporation of copulas in modeling dependencies that have focused on the bivariate case, this paper will consider a high number of involved dimensions, that is, up to 200 assets. Second, we develop two variants of MOPSO algorithms and compare them against two other well known algorithms, namely NSGAII and SPEA2, by employing a thorough evaluation procedure.The rest of the paper is organized as follows: Section 2 deals with the modeling of asset returns. Problem formulation and discussions about risk are brought in Section 3. The proposed algorithms are mentioned in Section 4. Section 5 talks about the validation of the algorithm. The computational results are provided in Section 6. Finally, the conclusions and future research are included in Section 7.As previously mentioned, return series are usually followed by a distribution with fatter tails than suggested by a normal distribution. This implies that extreme events are more likely to be occurring, which, in turn, accentuates on the use of a fat-tailed distribution. Stable distributions are a rich family of probability distributions that allow the occurrence of heavy tails and skewness.A random variable X is said to follow stable distribution if and only if for all n > 1, there exist constants cn> 0 and dn∈ R such that:(1)X1+X2+⋯+Xncn=dX+dnwhere, X1, X2, …, Xnare independent, identical copies of X, and=dmeans equally in distribution. Actually, Eq. (1) states that the shape of X is preserved under addition (Nolan, 2003). Since, in general, there is no any closed form expression for Probability Density Function (PDF) and Cumulative Distribution Function (CDF), stable distributions are uniquely defined by their characteristic function: φ(t; μ, γ, α, β) = exp[itμ − |γt|α(1 − iβsgn(t)Φ(t; α)]; where, Φ(t; α) = tan(απ/2) for α ≠ 1 and Φ(t; α) = ( − 2/π)log |t| for α = 1 in which, sgn(t) = 1 for t > 0, 0 for t = 0, and − 1 for t < 0. As it can be seen, stable distribution is described by four parameters: μ, γ, α, and β. μ ∈ R is the location parameter, γ ∈ R+ is the index of dispersion, α ∈ [0, 2] is the characteristic exponent, which controls decay in the tail of the distribution, and β ∈ [ − 1, 1] is the term of skewness. Clearly, these four parameters boost the flexibility of a stable distribution to include a number of distributions. For instance, when α = 2, Gaussian distribution; when α = 1, β = 0, Cauchy distribution; when α = 1/2, β = −1, Pearson distribution; when α = 1/2, β = 1, Levy distribution.In order to simulate the stable random variables, we follow the work of Chambers et al. (1976). Based on this, let γ0 = arctan(βtan(πα/2))/α, γ be uniformly distributed on ( − π/2, π/2), and W be an independent exponential random variable with mean 1. Generating of γ and W is straightforward. For this purpose, it is sufficient to set γ = π(U1 − 1/2), and W = −log U2, where U1 and U2 are two independent uniform random variables on (0, 1). Then we simulate the stable random variables according to Eq. (2):(2)Y={sinα(γ0+γ)(cosαγ0cosγ)1/α{cos(αγ0+(α−1)γ)W}(1−α)/αα≠12π{(π2+βγ)tanγ−βlog(π2Wcosγπ2+βγ)}α=1The growing attention to copulas in quantitative finance stems from their flexibility in modeling the distribution of multivariate random variables. The importance of copulas lies in the Sklar's theorem (Sklar, 1959), which states that a copula can decompose the joint distribution into two components: marginal distributions of each variable, and an appropriate copula function as a measure of dependence. Mathematically speaking, let H be an N-dimensional CDF with margins F1, …, FN. Then there exists an N-dimensional copula C such that, for x ∈ RN, we have H(x1, …, xN) = C(F1(x1), …, FN(xN)). Moreover, for anyu∈ [0, 1]N, we haveC(u1,…,uN)=H(F1−1(u1),…,FN−1(uN)); where,Fi−1is the generalized inverse of Fi. By applying the Sklar's theorem, we can write:(3)f(x1,…,xN)=c(F1(x1),…,FN(xN))∏i=1Nfi(xi)where, c(F1(x1), …, FN(xN)) = ∂NC(u1, …, uN)/∂u1…∂uNis the multivariate copula density, f(x1, …, xN) is the joint PDF of X1, …, XN, and fiis the PDF of Xi.The use of Maximum Likelihood method to estimate the parameters of margins and copula could be computationally intensive as it requires to jointly estimate them. In this study, we use the so-called “Inference Functions for Margins” (IFM) method, which is mentioned in Bouyé, Durrleman, Nikeghbali, Riboulet, and Roncalli (2000) to estimate the above parameters. For this purpose, letΘ= (θ1, …,θN,δ) such thatθ1, …,θNbe the vectors of the parameters corresponding to the margins,δbe the vector of the parameters associated with the copula, Lt(Θ) be the likelihood for observation t, and ℓt(Θ) be the log-likelihood of Lt(Θ). Considering the total T observations, we haveℓ(Θ)=∑t=1Tℓt(Θ). Regarding Eq. (3), we can write:(4)ℓ(Θ)=∑t=1Tlnc(F1(x1t;θ1),…,F1(xNt;θN);δ)+∑t=1T∑i=1NlnfN(xit;θi)Based on IFM, we first drive the estimation of the margins using Koutrouvelis technique, who employed the empirical approximation of characteristic function to estimate the parameters. Details about this method are in Koutrouvelis (1980, 1981). Now with the substitution of previous estimations, we have:(5)δ^=argmaxδ∑t=1Tlnc(F1(x1t;θ1),…,FN(xNt;θN);δ)In order to investigate whether the dependence structure of a multivariate distribution is appropriately captured by a copula function, we conduct the goodness of fit tests suggested by Genest, Quessy, and Rémillard (2006). To do this, we first introduce Rosenblatt's transform, which is a Probability Integral Transformation (PIT) on which the goodness of fit tests could be based on. PIT transforms a set of dependent variables into another set of uniformly independent variables. Rosenblatt's PIT of a copula C is the mappingR:(0,1)N→(0,1)N, which to everyu= (u1, …, uN) ∈ (0, 1)Nassigns a new vectorR(u)=(e1,…,eN)with e1 = u1 and for j ∈ {2, …, N}:(6)ej=∂j−1C(u1,…,uj,1,…,1)∂u1…∂uj−1/∂j−1C(u1,…,uj−1,1,…,1)∂u1…∂uj−1The main property of Rosenblatt's PIT is that ifuis distributed as C, we can checkH0*:R(u)∼C⊥instead ofH0:u∼C. In this paper we employ three test statistics: rank-based version of the Cramér–von Mises (ST), Anderson–Darling test based on Rosenblatt's PIT (AD), and Rosenblatt's PIT version of the Cramér–von Mises (ST(C)). The null hypothesis of these tests is H0: C ∼ C0; where, C0 belongs to a specific parametric family of copulas. The STtest statistic can be written as follows:(7)ST=∫[0,1]NC˜(u)2dC^(u)where,C˜is the empirical process given byC˜=T(C^−Cθ), in which Cθis the parametric estimation of C obtained under H0, andC^is the empirical cumulative distribution given by:(8)C^(u)=1T∑i=1T1(Ui1≤u1,…,UiN≤uN)The AD test statistic is given by:(9)AD=−T−1T∑i=1T(2i−1){log[G(χ(i))]+log[1−G(χ(T+1−i))]}where,χi=∑j=1N{Φ−1(R(uij))}2for i ∈ {1, …, T}, Φ−1 is the inverse of the cumulative distribution of a standard Gaussian variable, G is the cumulative distribution of a Chi-square random variable with N degree of freedom, and χ(1) ≤ ⋅⋅⋅ ≤ χ(T) are the order statistics corresponding to χ(1)…χ(T). Finally,ST(C)is defined as Eq. (10):(10)ST(C)=T∫[0,1]N{DT(u)−C⊥(u)}2dDT(u)where,DT(u)=1T∑i=1T1(R(Ui)≤u)The multi-objective portfolio optimization problem, which has been considered in this paper, can be expressed as follows (Problem 1):(11)minρ(x)Subject to(12)max∑i=1nxiμi(13)∑i=1nxi=1,(14)∑i=1nνi=K,(15)liνi≤xi≤uiνi,i=1,…,n(16)νi∈{0,1},i=1,…,n.In the above formulation, ρ(x) is the risk measure defined on the portfolio, n is the number of different assets, xiis the proportion of asset i in the portfolio, μiis the mean return of asset i, νiis a binary decision variable, which is equal to 1 if an asset i is held in the portfolio, and 0 otherwise, K specifies the number of assets that should be held in the portfolio, liand uirepresent the minimum and maximum proportions that can be held in an asset i, respectively. Eq. (11) minimizes the risk of portfolio while Eq. (12) maximizes the expected return of the portfolio. Eq. (13) is the budget constraint that ensures the proportions sum to one. Eq. (14) represents cardinality constraint and guarantees that K assets be exactly held in the portfolio. Eq. (15) is quantity constraint; whenever an asset i is held in the portfolio, νi= 1, the corresponding proportion must lie between li≤ xi≤ ui. And finally, Eq. (16) is the integrality constraint. In the following, we discuss more about the risk measures.In finance, risk has been used to characterize the situation in which an asset or portfolio is exposed to vulnerabilities and thus, enforces losses to the institutions. So, risk denotes the fact that losses can be incurred. Risk measures were introduced for the sake of quantifying these losses or under achievements. Mathematically, a risk measure ρ can be defined as below.Let the triple(Ω,F,P)specify some abstract probability space, where Ω is a set of random events,Fis a σ-algebra, andPis a probability measure, which belongs to a linear space ofF-measurable function, and X(x, ω) be the portfolio value. Then any mapping ρ: X → R∪{∞} is called a risk measure.The random variable X(x, ω) depends on the decision vector x as well as on some random events ω ∈ Ω. A risk measure ρ( · ) imposes a preference order ≽ on the random variables, namely: X ≽ Y if and only if ρ(X) ≤ ρ(Y). As previously mentioned, although the overwhelming influential models of portfolio selection consider variance as a risk measure, the shortcomings of this symmetrical risk measure are not hidden to anybody. Hence, researchers have turned to using downside risk measures, which could reflect a better notion of risk. Value-at-Risk (VaR) is one of the most popular downside risk measures. Exploiting this risk measure in portfolio selection problem has been increasingly adopted by both academicians and practitioners, particularly after the Basel Accord II in which banks and other financial organizations were obliged to use VaR in their reports. Methodologically, VaR is a number that the losses of a portfolio can exceed it with α% probability over the given planning horizon. Mathematically, VaR with a confidence level of α ∈ (0, 1) is defined as Eq. (17):(17)VaRα(X)=−inf{z|P(X≤z)≥α}In spite of its intuitive definition and easy interpretation, calculation of VaR is computationally extensive. Several methods have been proposed to estimate VaR in the literature; each with its own pros and cons. One may use historical simulation to estimate VaR. Notwithstanding simplicity in implementation, it could be unrealistic since future events may be different from the past observations. The delta-normal is another method, which is based on the assumption that return series reveal multivariate normal distribution. However, return series are leptokurtic and highly skewed, which are in contrast to the normality assumption. Another methodology to calculate VaR is Monte Carlo simulation. Although this method gives the advantage of considering an arbitrary stochastic process for the underlying returns, it is too time-consuming and to a large extent dependent on computer power. Here, we exploit the order estimator method to calculate risk. Let α ∈ (0, 1) and r1, …, rNbe the N realizations of the return distribution of portfolio. Then, if r[1] ≤ r[2] ≤ ⋅⋅⋅ ≤ r[N] be the ordered returns with an associated probability of occurrence pj, we have:(18)VaRα(X)=−inf{r[jα]|∑j=1jαpj≥α}Kennedy & Eberhart (1995a, 1995b) introduced PSO by mimicking the social behavior of animals, like birds flocking and fish schooling. In PSO, the whole population and potential solutions are called swarm and particles, respectively. PSO provides a guideline to iteratively encourage the particles toward promising locations. In the first step, PSO initializes randomly a number of particles to fly through the multi-dimensional search space. In each iteration, particle i records two values: position vector xi(t) = (xid(t), …, xid(t)) and velocity vector vi(t) = (vid(t), …, vid(t)). Each particle adjusts its movement by keeping track of its own experience (cognitive experience) as well as the experience of the whole particles in the swarm (social experience). At iteration t, the dth dimension of particle i modifies its coordinates using Eqs. (19) and (20):(19)vid(t)=wvid(t−1)+c1r1(pbestid−xid(t−1))+c2r2(gbestd−xid(t−1))(20)xid(t)=xid(t−1)+vid(t)where, pbestidand gbestdare the best solutions that have been achieved so far by particle i and the whole swarm in the dth dimension, respectively. r1 and r2 are uniformly random numbers in the interval [0, 1], and c1 as well as c2 are acceleration coefficients. According to Li and Engelbrecht (2007), a usual tuning for them is c1 = c2 = 1.494. w stands for inertia weight, which makes balance between local exploitation and global exploration. Many versions of PSO have been proposed in the literature. Interested readers can refer to Clerc and Kennedy (2002). Coello, Pulido, and Lechuga (2004), and Schutte and Groenwold (2005) for further details.The first proposed algorithm, herein called as AIMOPSO-CD, is the extended version of Improved-PSO and Adaptive-PSO to handle multi-objective problems. AIMOPSO-CD is adaptive in nature respect to inertia weight and adopts mutation operator with dynamic mutation rate to explore new search space and prevent trapping to the local optimal solutions. We use an external repository to contain non-dominated solutions found on the flight of the particles. Moreover, in order to obtain well-diversified solutions, we apply a dynamic mutation rate together with a crowding distance mechanism. Each particle is represented by a single point in N-dimensional search space. Each dimension also corresponds to an asset and takes a random variable that lies between liand ui. The particles are evaluated based on the non-domination concept. The performance of PSO highly depends on the inertia weight (w) values. In the starting iterations, the algorithm requires to attain an acquisition of the search space. So, greater values of w would be desirable. On the other hand, at the finishing steps, the algorithm needs to improve the quality of solutions; hence, smaller values for w are preferable. The proposed algorithm uses a time variant w. The updating equation for w is:(21)w(t)=wmax−q(t)(wmax−wmin)where,q(t)=|A(t)|/∑i=1tA(i)and|A(t)|is the cardinality of external repository at iteration t. In Eq. (19), it is possible for the magnitude of the velocity to converge into infinity. Therefore, it is usually confined to the interval [Vmin, Vmax]. Avoiding premature convergence, the particles undergo mutation operator with the probability of Pm. We adopt the dynamic mutation rate Pm(t) as below:(22)Pm(t)=12q(t)2As it can be observed, as the algorithm proceeds, Pm(t) gets smaller values to reduce disturbance. We use the mutation operator suggested by Zheng, Wu, and Song (2007), which acts both on velocity and position vectors:(23)υid′(t)=2rκvid(t)(24)xid′(t)=xid(t−1)+υid′(t)where, κ is the direction generated randomly κ ∈ { − 1, 1} and r is the random number in the interval (0, 1).The second proposed algorithm, herein called AMOMPSO, is a multi-objective multi-swarm particle swarm optimization. At each iteration, the particles are evaluated and then sorted based on domination count. Afterward, we start to divide the whole swarm into sub-swarms. The number of particles in each sub-swarm,N, should be pre-specified. The first particle in the swarm with the best domination count is then selected as the leader of the first sub-swarm. The selection of other particles to form the sub-swarms is based on two criteria: (1) Euclidean distance from the leader, (2) domination counts. The algorithm utilizes a parameter δ to make a balance between these two criteria. Indeed, to form the first sub-swarm,δ(N−1)percent of the particles with the highest Euclidean distances from the leader together with[(1−δ)(N−1)]percent of the particles with the worst domination counts are selected. TheseNparticles are then excluded from the swarm. The first particle from the remaining swarm is selected as the leader of the second sub-swarm. This particle together with(N−1)other particles forms the second sub-swarm. This procedure should be continued as long as the whole swarm is split into sub-swarms. Since the leaders of each sub-swarm, regardless of the first one, are the best only for their own sub-swarms; we called them as local leaders (ll). At this stage, each particle is updated according to Eqs. (25) and (26):(25)vid*(t)=wvid*(t−1)+c1r1(pbestid−xid*(t−1))+c2r2(llid(t−1)−xid*(t−1))(26)xid*(t)=xid*(t−1)+vid*(t)In this algorithm, if ll is repeatedly updated, the speed of convergence grows increasingly; thus the algorithm may be trapped in the local solutions. To avoid premature convergence in each iteration, ll is updated with a pre-defined probability. Moreover, premature convergence may be triggered by the rapid loss of diversity within the swarm. Hence, considering the largest Euclidean distance between ll and its neighborhoods to form the sub-swarms can promote diversity. It is interesting to note that since the best particle of the entire swarm is always selected as ll of the first sub-swarm, other particles in the sub-swarm fly across the promising regions by the guidance of ll. Additionally, in other sub-swarms, the particles are influenced by local optima solution (ll). Thus, a good exploration of the search space can be conducted. Furthermore, we adopt a popular selection scheme, namely, Adaptive Grid Archiving (AGA) method proposed by Knowles and Corne (2000, 2003). To be short, at iteration t, the AGA first updates the boundaries of the adaptive grid. Then a new particlePi(t)can enter into the archive if any of the following three possibilities occurs: (1)Pi(t)is non-dominated and the size of archive is not completed, (2)Pi(t)extends the range of the grid with respect to any objective dimension, (3) although the archive is full,Pi(t)is placed in a less crowded district than other particles in the archive. Our proposed algorithms are also equipped with a constraint satisfaction mechanism. Whenever the numbers of non-zero elements of a particle become greater than K, the algorithm selects an asset with the smallest proportion, and eliminates it from the portfolio. Conversely, if the number of assets within the portfolio is less than K, an asset is randomly selected and initialized with minimum proportional value li. To handle the budget constraint and the lower quantity constraint, a particle should be repaired by the following equation:(27)xi′=li+xi∑i∈Qxi(1−∑i∈Qli)where, Q is the set of all assets in the portfolio and x'iis the repaired particle. We utilize the reflection strategy proposed by Paterlini and Krink (2006) to satisfy upper quantity constraint. Based on this method, in initial steps, if a particle violates its boundaries, it is updated as below:(28)xi(t)=xi(t)−2(xi(t)−ui)Arriving at iteration j, the updating formula changes to:(29)xi(t)=uiAs it can be observed, there are two major discrepancies between the algorithms. While AIMOPSO-CD is designed based on the global PSO, AMOMPSO is established upon the local version of PSO with a new neighborhood topology. The second distinction between the algorithms has to do with the strategies that are exploited to manage the archive. AIMOPSO-CD uses crowding distance metric to prevent the particles from drifting toward preoccupied regions. However, AMOMPSO relies on AGA mechanism to prune the archive and improve its exploration capability.The pseudocodes for AIMOPSO-CD and AMOMPSO are presented in Figs. 1and 2, respectively.In this section, we examine the effectiveness of the algorithms by utilizing two problems in the literature. Benati and Rizzi (2007) and Yang (2011) offered a mixed integer linear programming formulation to obtain the optimal mean-VaR portfolio. Considering some modifications to adjust to our problem, the problem can be written as follows (Problem 2):(30)minx,y,ξξSubject to(31)∑i=1nxirij+M(1−yj)≥−ξ,j=1,…,J(32)∑j=1Jpj(1−yj)≤α,(33)∑i=1nxiμi≥G,(34)x∈P,yj∈{0,1},j=1,…,Jwhere,rijis the return of asset i in realization j, pjis the probability of realization j, ξ is the upper bound for VaR associated with the losses, α is the significance level, M is a big number, G is the lower bound for the expected return of portfolio, andPis the feasible space of our problem defined by Eq. (13) to Eq. (16). Considering Eq. (31), every time the return of the portfolio in realization j(∑xirij)becomes less than − ξ, yjtakes 0 and, therefore, 1 − yj= 1 in Eq. (32). So, the probabilities of such realizations are summed up. If the result is greater than α, then the portfolio would be infeasible. Eq. (33) guarantees that the portfolio return becomes at least G. We also verify the performance of the algorithms by solving Rockafellar & Uryasev (2000) CVaR minimization problem. Below, we combine their formulation with our problem (Problem 3):(35)minx,ξξ+1αJ∑j=1JyjSubject to(36)∑inxilij−ξ≤yj,j=1,…,J(37)∑i=1nxiμi≥G,(38)x∈P,yj≥0,j=1,…,J.where,lij=−rijis associated with the loss of asset i in realization j, and J corresponds to the total number of realizations.The validation process of the algorithms is as follows. First, we solve Problem 1 through the algorithms. We consider the expected return of the portfolio obtained by the algorithms as the lower bound (G) in Eqs. (33) and (37). Afterward, the Problems 2 and 3 are solved exactly by Cplex, and then the VaRs of these problems are compared against the VaR of the algorithms. We also conduct a cross comparison among the algorithms through different performance metrics:Spacing:S=(1/|Q|)∑i∈Q(di−d¯)2, where,di=mink∈Q∧k≠i∑m=1M|fmi−fmk|andd¯is the average of di.Spread:Δ=df+dl+∑i=1|Q|−1|di−d¯|df+dl+(|Q|−1)d¯, where, dfand dlare the Euclidean distance between the extreme solutions and the boundary solutions of Q.Hypervolume:HV=volume(∪i=1|Q|vi), where, viis the hypercube constructed by solution i and a reference point.Set coverage: C(Q, S) = |{s ∈ S|∃ q ∈ Q; q < s}|/|S|, where, Q and S are two non-dominated sets found by the algorithms.Generational Distance:GD=(∑i=1|Q|diq)1/q/|Q|, where, we set q = 2,di=mink∈R∑m=1M(fmi−fmk)2andfmkis the mth objective value of solution k in the reference set.The effectiveness of the proposed MOPSOs, NSGAII, and SPEA2 is investigated using three sets of data including 50, 100 and 200 assets. The datasets are referred to the daily returns of S&P500 from 16 June 2006 to 7 June 2010 with the total of 1000 observations. We check the robustness of the algorithms by defining different instances of the problem. Three values are set for K = 5, 10, and 15. In addition, to avoid achieving accidental results, we run each algorithm 20 times for each instance of the problem. Since there is no exact efficient frontier for the mean-VaR problem, in calculating the performance metrics, the reference sets are regarded as the best solutions obtained from all algorithms in all replicates for a specific problem instance. The tuning parameters of the algorithms are chosen through the experiments. The same parameter calibration is applied for NSGAII and SPEA2 to conduct a fair comparison. The population size and the number of iterations are set to 100 and 200, respectively. The intermediate crossover with the crossover rate of 0.8, and the Gaussian mutation operator with the mutation probability of 0.09 are considered. For AIMOPSO-CD, the maximum number of iterations, swarm size and external repository size are selected to be 200, 100, and 100, respectively. AMOMPSO uses the maximum allowable iterations of 200, the swarm size of 100, the external repository size of 100, and the sub-swarm size of 10. The parameters δ andrare set to be 0.7 and 0.8, respectively. Table 1 represents descriptive statistics for 16 randomly selected assets from S&P500. We observe that the skewness values are different from zeros, indicating the asymmetric properties of the data. The kurtosis values are also very larger than the normal kurtosis (which is equal to 3), resulting in fat-tailed distribution. The Jarque–Bera test statistics are large enough to reject the normal distribution hypothesis at the 5 percent significance level. The Shapiro–Francia test also confirms the conclusion following the Jarque–Bera statistic. Fig. 3shows dependence structure between the returns of two randomly selected assets. We have illustrated the historical returns, simulated returns obtained from the joint distribution of stable margins coupling with Gaussian copula, and simulated returns obtained from the joint distribution of Gaussian margins coupling with a Gaussian copula on the same plot to inspect the impact of different marginal distributional specifications. As it can be seen, there are some extreme values in the tails of the historical returns, which are determined by two circles, and cannot be captured by the simulated returns with Gaussian margins. However, tails of the simulated returns with stable margins are fat enough to capture these points. Next, we concentrate on the modeling of dependence structure among the raw returns. Table 2provides the estimated parameters of different types of copula for 2, 5, 10, 50, 100, and 200 sets of data. At the first glance, we find out that in bivariate case, the Student's t copula seems to be the best choice because it represents higher log-likelihood and smaller AIC and BIC. After that, Gumbel copula comes to the second place. As the number of assets increases, the ability of Archimedean copulas to capture the dependence structure among the assets diminishes. Thus, elliptical copulas seem to be a more appropriate choice to model financial data at higher dimensions. More especially, we observe that in all cases, the Student's t copula surpasses the Gaussian copula in the sense of AIC and BIC. To validate the choice of copula selected, we carry out the goodness of fit tests, which were mentioned in Section 2. p-Values of test statistics are summarized in Table 3. It can be observed that in all cases, p-values of all statistics fail to reject the null hypothesis of Student's t copula at the 1 percent significance level. Moreover, neither Gaussian nor Archimedean copulas successfully pass all three tests in any dataset. This brings us a good testimony to fit the data with the Student's t copula.In order to have a visual representation of the phenomenon in bivariate case, different estimated copula densities as well as contour curves of the joint distribution obtained by coupling the copula functions with stable margins are given in Fig. 4. This figure highlights the importance of utilizing a suitable copula function. Although stable distribution is flexible enough to capture the univariate randomness of a single asset, but unless it accompanies with a proper copula, the occurrences of joint extreme observations may be reflected accurately. The graphical illustration of the copulas also reveals that the Student's t copula is an appropriate copula as it allows for both positive and negative dependencies asymmetrically, and thus results in thick tailed joint distribution. It is also remarkable that Gaussian and Frank copulas approximate well the middle of the distribution but perform poor on the tails. In addition, Gumbel and Clayton copula exhibit only positive and negative dependencies, respectively. In the validation procedure, we first draw our attention to solve Problem 2. We notice that, in almost all situations, Cplex cannot find the optimal solution in reasonable time, even in the smallest problem instance. Although the conditions are somewhat better for Problem 3, the execution time is still high. Hence, we examine the validation of the algorithms by solving Problems 2 and 3 for a smaller dataset, that is 10 assets with K = 2. Concerning Table 4, we understand that the mean errors for all algorithms are acceptable. More specially, our proposed MOPSOs reveal fewer errors. The median values also explain that the errors have negative skewness. For illustrative purposes, we trace out the efficient frontier of the algorithms in Fig. 5. As it can be seen, efficient curves are non-smooth and discontinuous. These may be originated from non-convex property of VaR as well as cardinality constraint. We also find that in almost all cases, our proposed MOPSOs outperform NSGAII and SPEA2, at least from a macroscopic point of view. Particularly, MOPSOs exhibit their supremacy in producing non-dominated solutions over NSGAII in Fig. 5(9). However, there are exceptional cases in Fig. 5(7) and Fig. 5(4), where NSGAII and SPEA2 generate some better solutions than our proposed-MOPSOs, respectively. Moreover, we have plotted the efficient frontiers generated by the historical returns and the Gaussian margins in Fig. 6to compare them against the results obtained by the stable margins in Fig. 5. All of the curves have been produced by AMOMPSO. It is obvious that both of these methods underestimate the risk seriously. This implies that parametric approach is superior to the historical approach in the sense of accuracy, and the prevailing Gaussian distribution hypothesis is absolutely misleading. Tables 5–7 show a cross comparison in a quantitative way among the algorithms. Considering the Spacing metric, NSGAII and AMOMPSO perform well and nearly identical. They further win the other two algorithms in almost all instances. However, spread and hypervolume metrics indicate conflicting results. It can be observed that NSGAII outperforms AMOMPSO in 67 percent of instances with regard to the Spread metric while AMOMPSO gets better values than NSGAII in 78 percent of instances concerning hypervolume metric. This suggests that both of the algorithms are competitive in generating evenly distributed approximation of the efficient frontier.AIMOPSO-CD also defeats NSGAII and AMOMPSO in 23 percent and 12 percent of instances, respectively. SPEA2 reveals disappointing results in terms of Spread metric. It fails to defeat any algorithm at all. Hypervolume metric, on the other hand, brings promising news to SPEA2 and AIMOPSO-CD since they win the other two algorithms in some more instances. In a word, regarding the diversity characteristics, NSGAII and AMOMPSO are incomparable and both come at the first place since each wins the other in a specific metric and perform virtually the same in one metric. Generational distance metric in Table 6and set coverage metric in Table 7report the comparison on the results of the proximity characteristics. GD exhibits that AMOMPSO and AIMOPSO-CD win the other two algorithms in all instances. Moreover, in 56 percent of instances, AMOMPSO solutions have more proximity to the reference set than AIMOPSO-CD. Our proposed-MOPSOs’ performance is also preferable with regard to the overall average of set coverage metric. This metric reveals that AMOMPSO produces solutions more dominant than other algorithms’ solutions while being less dominated by them. After that, AIMOPSO-CD comes on the second place among the four algorithms. We also compare AMOMPSO and AIMOPSO-CD directly and find that, on average, 46 percent of the solutions produced by AMOMPSO dominate AIMOPSO-CD and 29 percent of them are being dominated by AIMOPSO-CD. Therefore, the performance of AMOMPSO is superior to the other algorithms in terms of proximity metrics. The execution time of the algorithms is also given in Table 7. One can understand that AMOMPSO and AIMOPSO-CD use up less CPU time and thus converge into the Pareto-optimal solutions much faster than SPEA2 and NSGAII. SPEA2 is the worst algorithm in CPU time consumption. This is because the truncation operator used in SPEA2 is computationally very expensive. Nevertheless, the standard deviation values show that SPEA2 is slightly firmer as compared to those of other algorithms. To put it in a nutshell, if both diversity and proximity characteristics are contemplated, we can reach at the conclusion that AMOMPSO can be no worse than the other three algorithms. It outperforms both SPEA2 and AIMOPSO-CD, and performs almost the same as NSGAII in terms of diversity metrics. It also wins NSGAII and SPEA2 in almost all instances considering the proximity metrics. Although AIMOPSO-CD was a bit better competitor, but a cross evaluation between AMOMPSO and AIMOPSO-CD proves that AMOMPSO surpasses AIMOPSO-CD in the sense of proximity metrics. It is notable that this superiority appears with more computational effort. The analysis further shows that SPEA2 has a fairly poor performance as it gets the last position in nearly all comparisons. However, it is noteworthy to say that all of the mentioned algorithms are capable to tackle our problem appropriately since their values are close to each other for all metrics. Last but not least, concerning the standard deviation of performance metrics, it is obvious that the robustness of the algorithms is verified.In this paper, we investigated the portfolio optimization problem in a more realistic manner. For that purpose, a two-step procedure was developed. In the first step, we concentrated on modeling the financial data. Unlike traditional multivariate normal assumption for the joint distribution of returns, we adopted stable distributions as the margins of the portfolio returns so as to capture the fat-tailed properties of financial data. We also examined the dependence structure among the assets by considering different specifications and calibrations of parametric copula functions. The empirical results showed that Archimedean copulas do a poor job, especially in dealing with high dimensional portfolios. The statistical goodness of fit tests also provided evidence in favor of the Student's t copula. In the second step, the extension of Markowitz's model was taken into account, where the risk was measured by Value-at-Risk and some additional constraints were added to the model. We proposed two multi-objective algorithms based on the particle swarm concept, and compared them against NSGAII and SPEA2. The results indicated that our proposed MOPSOs are highly competitive, surpassing the results obtained by the other algorithms. Particularly, they provide evidence of splitting the whole swarm into sub-swarms, and that evolving them promises a significant breakthrough over our problem with respect to the case where dealing directly with the whole swarm. Additionally, we studied how margins distributions influence portfolio optimization, and found that using Gaussian margins method and historical method underestimate the risk seriously. Finally, some remarks on possible future research of the paper are listed as below. One can conduct a variety of backtest procedures to examine the adequacy of the VaR models. Another important extension is the consideration of time series models, like ARMA-GARCH in the modeling of asset returns. Last suggestion is about using some other copula functions such as vine copulas to capture the inherent stochastic of data.

@&#CONCLUSIONS@&#
