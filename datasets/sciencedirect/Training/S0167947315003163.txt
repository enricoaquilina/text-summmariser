@&#MAIN-TITLE@&#
Estimating extreme tail risk measures with generalized Pareto distribution

@&#HIGHLIGHTS@&#
A new GPD parameter estimator is proposed.It is based on a nonlinear weighted least squares method.Under the POT framework, we estimate tail risk measures. Extensive simulation studies show the new method works well.

@&#KEYPHRASES@&#
Generalized Pareto distribution,Value-at-Risk (VaR),Conditional Tail Expectation (CTE),Tail VaR,Peaks over threshold (POT),Weighted nonlinear least squares,

@&#ABSTRACT@&#
The generalized Pareto distribution (GPD) has been widely used in modelling heavy tail phenomena in many applications. The standard practice is to fit the tail region of the dataset to the GPD separately, a framework known as the peaks-over-threshold (POT) in the extreme value literature. In this paper we propose a new GPD parameter estimator, under the POT framework, to estimate common tail risk measures, the Value-at-Risk (VaR) and Conditional Tail Expectation (also known as Tail-VaR) for heavy-tailed losses. The proposed estimator is based on a nonlinear weighted least squares method that minimizes the sum of squared deviations between the empirical distribution function and the theoretical GPD for the data exceeding the tail threshold. The proposed method properly addresses a caveat of a similar estimator previously advocated, and further improves the performance by introducing appropriate weights in the optimization procedure. Using various simulation studies and a realistic heavy-tailed model, we compare alternative estimators and show that the new estimator is highly competitive, especially when the tail risk measures are concerned with extreme confidence levels.

@&#INTRODUCTION@&#
The last few decades have witnessed an unprecedented increase in the size of datasets available, a phenomenon of massive data, in various applications such as finance, insurance, computer science and communications. Such large datasets now allow various quantitative risk analyses which were once thought infeasible to implement, including the investigation on rare but huge loss events occurring in the tail of the distribution. For instance, for financial institutions, extreme quantiles of the loss distribution are of great interest for both internal and regulatory purposes. Accurate estimation of such tail-related quantities however generally requires generating considerably large loss samples (see Section  5 for details), which may be very time-consuming for large complicated financial portfolios, and calculating those quantities is also known to be expensive because of the computing time and memory storage, as observed in the computer science literature, e.g., Liechty et al. (2003), Chen et al. (2000) and Munro and Paterson (1980); see Song and Song (2012) for a more detailed account on the issue of estimating extreme quantiles from massive datasets.In view of these difficulties, Extreme value theory (EVT) has received much attention as a modern tool to study tail quantities of the distribution, including the extreme quantiles. In the centre of EVT framework, the generalized Pareto distribution (GPD) emerged as the distribution of the exceedances above a sufficiently high threshold for arbitrary heavy-tailed loss data (Pickands, 1975). The standard procedure of fitting heavy-tailed data calls for a separate modelling of the tail region of the dataset using the GPD, a procedure commonly known as the peaks over threshold (POT); see, e.g., Embrechts et al. (1997). In finance and insurance applications, a main purpose of applying EVT under the POT is to determine common tail risk measures such as the Value-at-Risk (VaR) or Conditional Tail Expectation11In the literature, this is also known as the Conditional Value-at-Risk (CVaR) or Expected Shortfall (ES). The CTE will be formally defined later; see, e.g., McNeil et al. (2005) for a comprehensive discussion on risk measures.(a.k.a. Tail-VaR) from the fitted GPD. However, residing in the tail region, these risk measure estimates are highly sensitive to the estimated GPD parameters, and their volatility becomes larger as the required quantile level gets extreme. For example, according to Basel II (BCBS, 2006), the operational risk capital a bank should hold must cover unexpected losses with at least a 99.9% probability under the Advanced Measurement Approach (AMA), equivalent to VaR at a confidence level of 99.9%. Similarly, the credit risk capital under the Internal Ratings Based (IRB) approach also requires VaR 99.9%; furthermore, VaR 99.99% is used in the regulator’s backtesting analysis, assessing rare events occurring with probability 0.0001. Apparently, such extreme quantiles are highly sensitive to the estimation methods and small differences in the estimates could lead to a considerable impact on the financial position of banks, underscoring the importance of accurate GPD estimation.Estimating the GPD parameters is a long-standing problem and various approaches have been investigated in the literature. For example, the traditional maximum likelihood estimation (MLE) is discussed in Grimshaw (1993), Davison (1984) and Smith (1985). Pickands (1975) proposed a method using the sample order statistics; Hosking and Wallis (1987) used the method of moment (MME) and probability weighted moments (PWM). A generalized version of the MME was discussed by Ashkar and Ouarda (1996) and generalized probability weighted moments (GPWM) was proposed by Rasmussen (2001); Dupuis and Tsao (1998) developed a hybrid PWM. Juárez and Schucany (2004) introduced a minimum density power divergence method, and Zhang (2007) proposed a likelihood moment estimator (LME). Zhang and Stephens (2009) and Zhang (2010) surveyed some relevant contributions to the literature of the GPD parameter estimation by means of the Bayesian methodology. The reader is also referred to de Zea Bermudez and Kotz (2010) for a survey of various GPD estimators. More recently, Song and Song (2012) introduced a new–yet computationally simple and fast–GPD parameter estimator for large samples based on a nonlinear least square method that minimizes the sum of squared deviations between the empirical distribution function (EDF) of the sample and the theoretical GPD, which was reported to outperform other existing methods they considered.In this paper, we propose a new GPD parameter estimator under the POT framework to estimate tail risk measures at extreme quantiles. Our method is adapted from that of Song and Song (2012) and uses a nonlinear least squares method that minimizes the sum of squared deviations between the empirical distribution function and the theoretical GPD for the data exceeding the tail threshold. However, the proposed method uses a different object function and is better in its performance, and these are our contributions in the current paper. In particular, we first examine the method of Song and Song (2012) and point out its caveat, to show that their method is only applicable for the case where the tail of the loss sample is GPD distributed unconditionally. We address this issue and present a revised procedure. Second, in order to further improve the estimation, we introduce suitable weights in the revised optimization procedure using the weighted regression setup. Using the proposed procedure we estimate the VaR and CTE, the two popular tail risk measures, at extreme quantile levels. Using various simulation studies and a realistic heavy-tailed model, we compare alternative estimators and show that the performance of the proposed estimator is highly competitive compared to other existing estimators, especially for the risk measures with extreme confidence levels.This paper is organized as follows. The POT approach in EVT is briefly reviewed in Section  2. In Section  3, after reviewing existing methods, we describe a new GPD parameter estimator under the POT. Section  4 is devoted to numerical exercises, where we compare the performances of different methods in estimating tail risk measures under various heavy-tailed common parametric distributions. In Section  5 a more realistic loss model is considered for a similar exercise. Section  6 concludes the paper.We start with a brief account for Extreme Value Theory (EVT) focused on the peaks over Threshold (POT) framework; comprehensive treatments of book length on EVT can be found in, e.g., Embrechts et al. (1997) and Beirlant et al. (2006). Let us denote the tail or survival function of a continuous random variableXbyF̄(x)=1−F(x),0<x<∞. Then we say thatF̄(x)is regularly varying with index−γ<0, or simplyF̄∈R−γ, if(1)limx→∞F̄(xλ)F̄(x)=λ−γ,λ>0.Whenγ=0the tail is called slowly varying, orF̄∈R0. Using this definition we can write a regularly varying distribution asF̄(x)∼L(x)x−γwhereL(⋅)∈R0. We note thatf(x)∼g(x)meanslimx→∞f(x)/g(x)=1. Therefore the tail of regularly varying functions can be represented by power functions multiplied by slowly varying functions. Heavy-tailed distributions such as Pareto, Generalized Pareto, Log-gamma, Cauchy and Stable laws are examples of such functions. WhenF̄∈R−γwithγ>0, the distribution is said to be in the maximum domain of attraction of the Fréchet distribution (MDA(Φγ)in short), a member of the generalized extreme value (GEV) distribution family, representing the class of distributions with heavy tails.A fundamental result of EVT is the famous Pickands–Balkema–de Haan theorem (Balkema and De Haan, 1974 and Pickands, 1975) which states that, forF̄∈R−γ, the excess loss(X−u|X>u)from such a distribution with a large thresholdu>0converges to the Generalized Pareto distribution (GPD) with positive Pareto parameterξ>0. That is,(2)limu↑xFsup0<x<xF−u|Pr(X−u<x|X>u)−Gξ,σ(x)|=0,wherexF≤∞is the right endpoint ofFandGξ,σ(x)is the GPD distribution function. This implies that the excess distributionPr(X−u|X>u)converges to the GPD wheneverXis heavy-tailed distribution inMDA(Φγ). This theorem, therefore, provides the basis of the POT framework which enables a separate modelling for the tail part of the dataset with the GPD, starting fromu. Selecting the thresholdufor a given sample however has been considered to be a nontrivial problem and one should explore different candidates in practice. The distribution function of the GPD is defined as(3)Gξ,σ(x)=1−(1+ξσx)−1/ξ,x>0,ξ>0,whereσ>0is the scale parameter,ξis the shape parameter and1/ξ=γis the tail index. Sometimes the location parameterμ∈(−∞,∞)is added, in which case the distribution function is denoted byGξ,μ,σ(x)=Gξ,σ(x−μ)with the supportx>μ. The value ofξdetermines the shape of the GPD. Distributions withξ>0are classified heavy-tailed withE[Xk]being infinite fork>1/ξ. Among other properties, we note that, ifXisGξ,σ(y)distributed, its excess loss is again GPD distributed with parameter(ξ,σ+ξu). That is,(4)Pr(X−u<x|X>u)=1−(1+ξσ+ξux)−1/ξ,x>0,ξ>0.In a sense, this so-called stability of the GPD resembles the memoryless property of the exponential distribution, a special case of the GPD withξ=0.The VaR of a continuous random variableXat the100p%level is the100pquantile of the distribution ofX, denoted by(5)Qp(X)=FX−1(p).The VaR is a widely-accepted standard risk measure used in solvency and risk analyses in financial industry, withpclose to 1. However, it has also been criticized for lacking a certain property that desirable risk measures should meet, the criteria known as the coherent risk measure axioms; see Artzner et al. (1999). In this regard, the CTE has received much attention as an alternative, coherent tail risk measure. The idea of the CTE is to measure the average severity of the loss when the extreme loss does occurs, where the extreme loss is represented by the VaR. Formally, the CTE of the random variableXat100p%level is defined as(6)CTEp(X)=E[X|X>Qp(X)].When the POT approach is employed, one can write the underlying loss distribution as a combination of the body and tail parts with the latter modelled by the GPD. For this, we letF(x)be the distribution function of an arbitrary continuous distribution, and define the exceedance distribution of loss events overuby(7)Fu(y)=Pr(X−u≤y|X>u)=F(u+y)−F(u)1−F(u),which is assumed to beGPD(ξ,σ). Then we can rewriteF(x)as(8)F(x)=P(X≤x)=(1−F(u))Fu(x−u)+F(u)=(1−F(u))Gξ,σ(x−u)+F(u)=(1−F(u))Gξ,u,σ(x)+F(u),using the three-parameter GPD, and its estimated version by(9)Fˆ(x)=(1−Fn(u))Gξˆ,u,σˆ(x)+Fn(u),whereFnis the EDF andξˆ,σˆare the estimated GPD parameters from the sample. Assuming that the tail risk measures lie in the GPD realm, the VaR estimate is obtained from inverting (9) as(10)Q̂p(X)=Gξˆ,u,σˆ−1[1−1−p1−Fn(u)]=u+σˆξˆ[(nnu(1−p))−ξˆ−1],wherenuis the number of exceedances over the thresholduandnis the total number of observations. The CTE is determined from the fact that whenX−u|X>uisGPD(ξ,σ)distributed,X−u′|X>u′, withu′>u, isGPD(ξ,σ+ξ(u′−u))distributed. Hence, subject toξ<1for the mean to be finite, we obtain, forQp(X)>u,(11)CTEp(X)=E[X−Qp(X)|X>Qp(X)]+Qp(X)=σ+ξ(Qp(X)−u)1−ξ+Qp(X)=Qp(X)+σ−ξu1−ξ,ξ<1,which leads to its estimate(12)CTÊp(X)=Q̂p(X)+σˆ−ξˆu1−ξˆ=u+σˆξˆ[11−ξˆ(nnu(1−p))−ξˆ−1].The traditional maximum likelihood estimation (MLE) was studied in Grimshaw (1993), Davison (1984) and Smith (1985). The likelihood function however approaches infinity whenξ<−1, so the MLE exists only forξ>−1. Further, Smith (1984) showed that the resulting estimators for the GPD are consistent and asymptotically normal only forξ>−0.5. The computational difficulty and convergence issue of the MLE, even whenξ>−0.5, was discussed in Grimshaw (1993) and Hosking and Wallis (1987). In order to improve the estimation, Hosking and Wallis (1987) proposed the Method of Moments Estimation (MME) using the first two moments, but its performance is poor forξ≥1/2where the second moment does not exist. In the same paper, they also derived a probability weighted moments (PWM). He and Fung (1993) proposed the method of median (MED), which is found by equating the sample median of the two partial derivatives to the corresponding population medians. The likelihood moment method (LME) was proposed by Zhang (2007) to solve the numerical problems of the MLE. Unlike the MLE, the LME always exists and is computationally simple. In a separate paper, Zhang (2010) also suggested a Bayesian estimator, improved over its previous one in Zhang and Stephens (2009), which is more efficient and adaptive forξ>1by selecting a better prior distribution. Also see de Zea Bermudez and Kotz (2010) for a survey of various estimators.Recently, Song and Song (2012) proposed a new GPD parameter estimator for massive heavy-tailed data based on the POT framework. Their procedure, called the nonlinear least squares (NLS) estimator, consists of two steps with the first step designed to stabilize the shape parameterξestimation with an interim estimate. We review the NLS procedure in detail here as it is closely related to our proposed one. Suppose that we have a samplex1,…,xnof sizen, andnu<nobservations that are greater than the selected GPD thresholdu. Without loss of generality, it is assumedx1>x2>⋯>xn. In the first step, one finds the interim estimate(ξˆ1,σˆ1)using a nonlinear minimization:(13)(ξˆ1,σˆ1)=argmin(ξ,σ)∑i=1nu[log(1−Fn(xi))−log(1−Gξ,u,σ(xi))]2.HereFn(x)is the EDF andGξ,u,σ(x)is the distribution function of the three-parameter GPD. With(ξˆ1,σˆ1)as the initial values, the following second step runs another optimization:(14)(ξˆ2,σˆ2)=argmin(ξ,σ)∑i=1nu[Fn(xi)−Gξ,u,σ(xi)]2,which is the least square estimation for the distribution function of the GPD. The resulting(ξˆ2,σˆ2)is the final estimate of the GPD parameters. Song and Song (2012) commented that the direct application of (14) without the first step in (13) gives quite volatile estimates because, for high thresholds, the GPD distribution function values are concentrated on the interval very close to 1, making the global minimum hard to find in a stable manner. For various datasets simulated from standard heavy-tailed distributions, the two-step NLS procedure was reported to outperform other existing methods.We start this section by discussing a caveat of the NLS estimator of Song and Song (2012) as described in (13) and (14). In particular, while their idea of minimizing the squared deviations betweenFn(x)and the theoretical GPD overx>uis valid in light of the POT framework, their actual formation is only applicable when the underlying model’s tail fitsGPD(ξ,0,σ), unconditionally. To elaborate this point, consider the EDF values in (13) and (14) forx>u, which is the range over which the optimization is carried out. In this range,Fn(x)increases with the starting value ofFn(u), which is already very close to 1 becauseuis the GPD threshold in the tail; for instance, the starting value isFn(u)=0.98ifuis set to be the 98th sample quantile. In contrast, the value ofGξ,u,σ(x)in (13) and (14) always starts fromGξ,u,σ(u)=0as its support is always[u,∞). Hence the difference betweenFn(x)andGξ,u,σ(x)is not measured consistently, and we see that the NLS procedure is only sensible and applicable whenu=0, the case where the distribution is unconditionally GPD from the very beginning of the data. This discrepancy can also be rephrased using the excess loss notation. Looking at the object function (14),Gξ,u,σ(x)represents the conditional distribution ofX|X>uwhereas the EDFFn(x)is a distribution for the entireX. While the summation in (14) is carried out for the observations exceeding the threshold only, the issue remains unsolved in this second step because the EDF conditional onX>ucannot be created by simply restricting the acceptable range of the observations; the conditional EDF should be constructed on its own basis using truncation, as will be shown shortly. In this regard, the limitation of NLS procedure is that it does not properly employ the POT framework despite their claim of doing so. In their numerical exercises, Song and Song (2012) report that the performance of the NLS is superior to alternative estimators. However the comparison was made unfairly, because all the other methods are based on the POT framework whereas the NLS is not. Asσˆneeds to be obtained from conversion formulaσˆ=σˆ(u)(1−Fn(u))ξˆunder the POT framework, whereσ(u)is the scale parameter estimate from the truncated distribution (McNeil and Saladin, 1997), forgoing the POT element gives an advantage to the NLS method by unduly reducing parameter uncertainties.One way to compare the alternative estimators from a proper perspective is to compare their performances assumingu=0, which provides a level playing field for all estimators. For this, we carry out a simulation study where several estimators considered in Song and Song (2012) are compared for extreme VaRs when the underlying model isGPD(ξ,0,σ)with different(ξ,σ)values, of which the results are presented in Table 1. The performance for each estimator is evaluated using the root MSE (RMSE) and the absolute relative bias (ARB), where the latter is defined as the scaled difference between the estimator and true quantile,E(|θˆ−θ|/θ)for an estimatorθˆ. From the table, it is seen that, contrary to what (Song and Song, 2012) reported, the estimator of Zhang (2010) is superior to the NLS for VaR estimations, indicating that the performance of the NLS is not as good.We now revise the NLS method so that it can properly account for the POT framework. The key element in this revision is to minimize the squared distance betweenFn(x)andF(x)in (8), not the distance betweenFn(x)andGξ,u,σ(x)as done in (13) and (14). The revised first step of the NLS then should be(15)(ξˆ1,σˆ1)=argmin(ξ,σ)∑i=1nu[log(1−Fn(xi))−log(1−F(xi))]2=argmin(ξ,σ)∑i=1nu[log(1−Fn(xi))−log(1−(1−Fn(u))Gξ,u,σ(xi)−Fn(u))]2=argmin(ξ,σ)∑i=1nu[log1−Fn(xi)1−Fn(u)−log(1−Gξ,u,σ(xi))]2,followed by the revised second step:(16)(ξˆ2,σˆ2)=argmin(ξ,σ)∑i=1nu[Fn(xi)−F(xi)]2=argmin(ξ,σ)∑i=1nu[Fn(xi)−(1−Fn(u))Gξ,u,σ(xi)−Fn(u)]2=argmin(ξ,σ)∑i=1nu(1−Fn(u))2[Fn(xi)−Fn(u)1−Fn(u)−Gξ,u,σ(xi)]2=argmin(ξ,σ)∑i=1nu[Fn(xi)−Fn(u)1−Fn(u)−Gξ,u,σ(xi)]2.We call this revised two-step procedure “pot-NLS”, as opposed to the original NLS. Compared to the NLS method, the pot-NLS addresses the tail part appropriately by using a truncated distribution, well-aligned with the POT framework.Moreover, we observe that the second step optimization (16) can be further improved by adding suitably chosen weights for each squared deviance term, an idea from the weighted least squares regression. To find the suitable weight for each squared error, we recognize that the first line of the revised second step above is equivalent to applying the least squares method to the following regression setup:(17)F(xi)=Fn(xi)+ϵi,i=1,…,n,whereϵiis the error term withE(ϵi)=0, which is justified from the fact that the EDF is an unbiased estimate ofF(x)for allx; see, e.g., Rohatgi (1984). Now noting thatF(X)is a uniform distributed random variable for anyX, we can determine the weight for each response variableF(Xi)as the reciprocal of its variance. That is, assuming without loss of generality thatX1>⋯>Xn, the distribution ofF(Xi)is that ofUn−i+1:n, the(n−i+1)th order statistic of the uniform random variable, of which the distribution is known to beBeta(n−i+1,i)from the standard distribution theory. Because the first two moments of such beta variable are given by(18)E(Un−i+1:n)=n−i+1n+1,Var(Un−i+1:n)=i(n−i+1)(n+1)2(n+2),the weight forF(Xi)should be(Var(Un−i+1:n))−1, which yields the weighted version of the nonlinear optimization given in (16):(19)(ξˆ3,σˆ3)=argmin(ξ,σ)∑i=1nu[Var(Un−i+1:n)]−1[Fn(xi)−F(xi)]2=argmin(ξ,σ)∑i=1nu[i(n−i+1)(n+1)2(n+2)]−1[Fn(xi)−Fn(u)1−Fn(u)−Gξ,u,σ(xi)]2.We will call(ξˆ3,σˆ3), combined with the revised first step (15), the weighted NLS estimator under the POT (pot-WNLS in short). One advantage of the pot-WNLS estimator over the pot-NLS, as will be seen later, is that it estimates the extreme quantiles in a more stable manner because, asVar(Un−i+1:n)is increasing inifori≤n/2, larger weights are given forF(xi)values asximoves towards the tail side. In Fig. 1, the values ofVar(Un−i+1:n),i=1,…,n, and the weights in (19) are illustrated. We use the standard “optim” function in R, based on the default method of Nelder and Mead (1965), to implement the pot-WNLS estimator in our numerical exercises.We estimate the tail risk measures using different estimators from various parametric heavy-tailed distributions. Following the POT framework, we fit the GPD using the sample exceedances above a sufficiently high threshold to estimate the tail risk measures. For the risk measures, we compute the VaR and CTE at quantile levels ranging from 98% to 99.99%. We include 99.99%, despite its extremity, in light of its practical use as discussed in Introduction. As our interest is in risk measures rather than the parameter itself, we do not report the parameter results. The competing estimators we consider are the MED estimator by He and Fung (1993), the Pickands estimator by Pickands (1975), the LME by Zhang (2007), the Zhang’s method of Zhang (2010), our NLS method (pot-NLS) and its weighted version (pot-WNLS). The MME, PWM and MLE are not considered; the first two estimators are known to perform poorly except for narrow ranges ofξand may produce infeasible estimates (Zhang, 2007, 2010), and the MLE often cannot be obtained whenξ>0.5due to the singular observed information matrix as noted by Song and Song (2012), which is in agreement with Hosking and Wallis (1987).The simulated samples are generated from the GPD, Cauchy, Log-gamma and Pareto, respectively. For each sample we choose 97th, 98th and 99th sample quantiles for the thresholdsuand the estimated quantiles are VaR 98%, 99%, 99.9%, and 99.99% of the distribution. We consider different threshold values because the exact GPD threshold is unknown in real applications. Practical experience suggests that the thresholds are relatively large with the number of exceedances less than 5% of the sample, justifying our candidate threshold choices; see Chapter 7 of McNeil et al. (2005). We describe the simulation procedure for each distribution as follows.1.Generate a random sample of size 10,000 from the given distributionPick a thresholdu, taken to be the100pth sample quantileFit the GPD with the observations aboveuusing alternative estimators to estimateξandσ, which in turn yield the VaR estimate using (10)Repeat above steps 20,000 times to compute the RMSE and ARB. The true VaR for the distribution is obtained analytically or numerically.Note that the sample size of 10,000 allows us to have 100 to 300 observations for the GPD fit, reasonable numbers of exceedances to investigate extreme tails. The simulation results are presented in Tables 2 and 3. We discuss the results for each distribution below, but overall we see that the “MED”, “Pickands” are relatively poor estimators for all cases. The performance of the “pot-NLS” is about average. The “LME”, “Zhang” and “pot-WNLS” form a better estimator class in most cases. In particular, in terms of the RMSE, the “pot-WNLS” performs best for VaR 99.9% and 99.99% under all models considered, with one exception. In terms of the ARB, the “pot-WNLS” always performs best for the VaR 99.99% and best in most cases for the VaR 99.9%.Example 4.1GPD(ξ,σ).Cauchy(α,β).Example 4.3Log-gamma(α,β).Pareto(θ,α).Now we turn to the CTE estimation and compare the same six estimators for simulated datasets from the GPD, Pareto and Log-gamma distributions, respectively, with varying shape parameters. The simulation procedure is similar. We first pick the100pth sample quantile of the dataset as the thresholduand fit the exceedances to the GPD to obtainξˆandσˆ. The fitted GPD then gives the CTE estimate (12), which can be compared against the true CTE of the given model. We note that the true CTE values for all models have been derived from the corresponding distribution function and available in closed-from. The sample size is again 10,000 with 20,000 repetitions for each model. The simulation results of CTE 97%, 99% and 99.9% are presented in Tables 4–6, for each model. We do not consider CTE 99.99% as it is, being way larger than the VaR 99.99%, too volatile for such heavy-tailed distributions. One distinct consideration in the CTE estimation is that the CTE does not exist forξ>1due to the moment existence condition. Besides, as seen from (12), if one single estimate ofξˆout of 20,000 repetitions turns out to be too close to 1, even though it is strictly less than 1, it produces a prohibitively large CTE estimate and yields a ruinous impact on the bias and variance of the estimate, which prevents us from any realistic comparison. To appropriately compare these estimates therefore we have excluded all occurrences that giveξˆ>0.99in our RMSE and ARB computations. The number of occurrences excluded are separately recorded in the tables (coined as ‘Failure’) as this also provides valuable information about the estimate’s quality.Example 4.5GPD(ξ,σ).Example 4.6Log-gamma (α,β).Pareto(I) (θ,α).One may wish to compare the performance of different estimators for actual datasets. However, such a task is not easy as the true value of the tail risk measure is generally unknown. It could be argued that, provided that the sample size is large enough, the empirical tail measure may serve as a feasible substitute of the true measure, but the variability of the empirical risk measure can actually be substantial even for large samples. In fact, recognizing that the VaR and CTE are special cases ofL-statistics in the statistical literature, the asymptotic variance of the100p%sample quantile and CTE are given by (see, e.g., Staudte and Sheather (1990) and Manistre and Hancock (2005))(23)Var(Q̂p(X))≈p(1−p)n(f(Qp(X)))2and(24)Var(CTÊp(X))≈Var(X|X>Qp(X))+p(CTEp(X)−Qp(X))2n(1−p).Though these variances decay atO(n−1), when evaluated at finite samples, their values critically depend on confidence levelpand the tail thickness of the underlying distribution through its densityfor the CTE of the underlying distribution. Thus, for heavy-tailed datasets withp≈1, the variances of the estimates could easily produce unacceptably large values, invalidating further analyses. To avoid this situation, we consider in this section a realistic parametric model which models the entire range of some actual loss dataset, but still is in the MDA of the Fréchet distribution, so that the GPD under the POT framework can be properly applied. The model we consider is the Log Phase-type (LogPH) distribution class of Ahn et al. (2012), which is created via the exponential transformation of a Phase-type (PH) random variable, defined as the distribution of the time until absorption in a continuous time Markov chain with an absorbing state, as introduced by Neuts (1975). The class of PH distributions contains popular distributions such as the Exponential, Erlang, and their finite mixtures, and has attractive distributional properties. A notable property of the LogPH class is that it can approximate any non-negative distribution to any desired accuracy. The distribution function of the LogPH,Y, is(25)FY(y)=1−αeTlogy1,fory≥1,where1is apdimensional column vector consisting of ones,αis apdimensional row vector andTis ap×pinfinitesimal generator matrix. Ahn et al. (2012) showed that the LogPH distribution, being in the MDA of the Fréchet distribution, adequately models the entire range of the famous Danish fire insurance data22Retrieved from Dr. A. McNeil’s webpage: http://www.ma.hw.ac.uk/mcneil/data.html; see McNeil et al. (2005) for further discussions and other references on the Danish fire data.withp=2. Their estimated parameters areαˆ=(0.622,0.378)andTˆ=[−4.0003.5640.267−1.813].Fig. 2shows the LogPH fit of the dataset. The resulting model indicates that the moment exists only up to about 1.4, consistent with previous findings in the literature.For our purpose, we generate samples of size 10,000 with 20,000 repetitions from this LogPH model and compare the performances of various tail risk measure estimates. Tables 7 and 8present the results for the VaR and CTE estimation for this model, with the true tail risk measures which are obtained analytically as shown in the original paper. For the VaR estimation, 97th, 98th and 99th sample quantiles were selected as thresholduand we estimated the VaR 98%, 99%, 99.9%, and 99.99%. Our estimator “pot-WNLS” gives the smallest RMSE at VaR 99.99% for all threshold choices and has best results at VaR 99.9% and VaR 99.99% when the thresholduis 99th sample quantile. In other cases, “LME” and “Zhang” are slightly better. Overall, “pot-WNLS” again tends to outperform other estimators as we go towards the extreme quantile levels. Similarly, we estimated the CTE for the 97%, 99% and 99.9%, where the 95th, 98th and 99th sample quantiles were selected as the threshold. The numbers of Table 8 shows that our estimator “pot-WNLS” performs best at CTE 99% and 99.9% in terms of both RMSE and ARB, with the smallest numbers of failure at the same levels. Again, our estimator gives the best results as the threshold and quantiles go to extreme cases.We proposed a new procedure to fit the GPD and estimate tail risk measures at extreme quantile levels under the POT framework. Our method is adapted from the recent NLS estimator of Song and Song (2012) which uses nonlinear minimization of the squared error between the GPD distribution function and the empirical distribution. We critically examine the NLS method and point out its caveat. After an appropriate revision on the NLS, we further introduce its improved version by adding a suitable weight for each empirical distribution point. From extensive numerical studies, using both simulated samples and a more realistic loss dataset, we found that the performance of the proposed estimator is highly competitive in estimating extreme VaR and CTE for heavy-tailed data. In particular the proposed estimator performs better than alternative estimators as the shape parameterξof the GPD increases and the tail thresholdugets larger.

@&#CONCLUSIONS@&#
