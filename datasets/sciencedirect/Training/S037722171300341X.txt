@&#MAIN-TITLE@&#
Variable neighborhood search for minimum sum-of-squares clustering on networks

@&#HIGHLIGHTS@&#
A new Clustering problem on networks is introduced.The metaheuristic VNS is shown to be naturally customized for this problem.Numerical experience reported confirms that results than those reported by k-means are obtained.

@&#KEYPHRASES@&#
Minimum sum-of-squares clustering,Location on networks,Variable neighborhood search,

@&#ABSTRACT@&#
Euclidean Minimum Sum-of-Squares Clustering amounts to finding p prototypes by minimizing the sum of the squared Euclidean distances from a set of points to their closest prototype. In recent years related clustering problems have been extensively analyzed under the assumption that the space is a network, and not any more the Euclidean space. This allows one to properly address community detection problems, of significant relevance in diverse phenomena in biological, technological and social systems. However, the problem of minimizing the sum of squared distances on networks have not yet been addressed. Two versions of the problem are possible: either the p prototypes are sought among the set of nodes of the network, or also points along edges are taken into account as possible prototypes. While the first problem is transformed into a classical discrete p-median problem, the latter is new in the literature, and solved in this paper with the Variable Neighborhood Search heuristic. The solutions of the two problems are compared in a series of test examples.

@&#INTRODUCTION@&#
Cluster Analysis is a popular and powerful tool in Data Analysis [24,33] that provides a rich variety of challenging optimization problems [16]. The aim of Cluster Analysis is to partition a set of entities into clusters, so that entities within the same cluster are similar and entities in different clusters are different. The most well-known model is Minimum Sum-of-Squares Clustering (MSSC). In its basic form, MSSC assumes the entities to be points inRn; p points (called prototypes) are sought by minimizing the sum of the squares of the Euclidean distances separating the entities from their closest prototypes. MSSC, recently shown to be NP-hard in [2], can be solved exactly for data sets of moderate sizes (over 2300 entities) by column generation [3]. For larger data sets, heuristics are used, see [18,22] and the references therein.Whereas many clustering problems in Data Analysis are properly accommodated within such Euclidean framework (entities identified with points in the Euclidean space, entities closeness measured by the Euclidean distance), many data sets in complex systems in domains such as Sociology, Biology or Computer Science may be more naturally modeled with networks [1,6,7,14,31,34], where the entities are seen as the nodes, and the edges of the network model entities interaction. In such cases the length is usually called weight, in order to avoid confusion between some metric and nonmetric properties of the data. Each edge is assumed to have a positive weight, which in the simplest case always takes the value 1, indicating only that the two nodes associated with edges are linked. In many complex systems, however, the edges may have different cost (weight). Transportation and mobility networks, Internet, mobile phone networks, power grids, social and contact networks, and neural networks, are mentioned in [7] as contexts in which the network topology alone does not contain all the information, and community detection should take into account such different edges weights.In this paper we assume that the set V of entities to be clustered is the set of nodes of a connected and undirected network N=(V,E), where E is a collection of pairs of nodes, and each e∈E has positive length le. The set V of nodes is to be split into p disjoint subsets (clusters, communities), and p prototypes (one per cluster) are to be chosen so that all entities in a given cluster are close to their associated prototype. We propose to use a sum-of-squared-distance criterion, which leads us to address two versions of the problem, called V-MSSC and E-MSSC. The V-MSSC (vertex-MSSC) consists of selecting a subset V∗ of p entities (prototypes) within the set of vertices of the network, so that the sum of squares of the distances from each entity v∈V to its closest prototype is minimized. Closeness between any pair of entities u, v∈V is measured by the length d(u,v) of the shortest path connecting u and v. The E-MSSC (edge-MSSC), [12], has the same objective, but the prototypes are sought not only at vertices, but also at points along the edges of the network. This way, one may obtain with the E-MSSC clusters configurations which are impossible if only vertices are allowed to be prototypes. In other words, the clusters class which can be obtained by solving E-MSSC is richer than the one obtained by solving V-MSSC problems, and this may lead to more accurate clusters. We emphasize that we use prototypes as a means to cluster entities, though in some cases there is no direct physical meaning of the prototype locations: when applied to nonmetric data such as social networks, the prototype position on the edge that connects two entities, say Peter and Paul, has no physical meaning, but is used here as a tool to identify whether the two entities, say Peter and Paul, belong to the same cluster or not. It is interesting to note that the difference between vertex minimum sum-of-distances and edge minimum sum-of-distances (not squared distances) does not exist, since the optimal solutions of both are equivalent [15].The purpose of this paper is to introduce these two clustering models on networks, analyzing some structural properties, comparing them, and developing algorithmic tools to cope with data sets of nontrivial size. The rest of the paper is organized as follows. In Section 2 the V-MSSC and E-MSSC problems are formally stated. Some structural properties are presented in Section 3. In Section 4 we describe how VNS can be customized to address E-MSSC. The paper ends with a battery of numerical experiments in Section 5, comparing the output of both problems and of different variants of the VNS designed to solve them. Some concluding remarks and possible lines of future research are given in Section 6.A connected and undirected network N=(V,E), with node set V and edge set E, is given. Each edge e∈E has length le>0. The lengths are assumed to satisfy the triangle inequality, and thus they induce a metric on the set of nodes, namely, the shortest-path distance. We will consider the network N to be a spatial network: for any edge e=(u,v)∈E, and any x∈[0,1], the pair (e,x) will be identified with the point in edge e at a distance xlefrom u, and at distance (1−x)lefrom v. Let us denote with P(N) the set of all pairs (e,x), i.e., the set of points of the network N. A metric on the set P(N) is defined: for any two points x, y∈P(N), d(x,y) is the length of a shortest path connecting x and y. See [26] for details.The V-MSSC problem is defined as the problem of finding p prototypes from the set V of vertices such that the weighted sum of squares of distances from the nodes to their closest prototype is minimized. V-MSSC is easily formulated as a linear integer program. Indeed,•for each pair (u,v)∈V×V, define the binary variable xu,v, which takes the value 1 if entity u is allocated to prototype v, and zero otherwise,for each v∈V, define the binary variable yv, which takes the value 1 if entity v is chosen to be one of the prototypes, and zero otherwise.With this notation, V-MSSC is written as the Integer Program(1)min∑u,v∈Vd2(u,v)xuvs.t.∑v∈Vxuv=1∀u∈Vxuv⩽yv∀u,v∈V∑v∈Vyv=pxuv∈{0,1}∀u,v∈Vyv∈{0,1}∀v∈V.Obviously, Problem (1) is the classical p-median problem with the finite set V both as set of users and candidate sites for the facilities, and d2(u,v) as the distance from the user u∈V to the facility at v∈V. Hence, exact and heuristic algorithms as those described in [4,5,17,28] can be used to successfully address V-MSSC.The E-MSSC problem is analogous to the V-MSSC, but prototypes are sought along the edges: we seek a set of p prototypes from the set P(N) of points on edges such that the weighted sum of squares of distances from the nodes to their closest prototype is minimized:(2)min∑v∈Vωvd2(v;{x1,…,xp})s.t.x1,…,xp∈P(N),where non-negative weights ωv>0 are given for any v∈V. The distance d(v,X) from v to a non-empty finite set X of points of the network is defined as the distance to the closest point in X,(3)d(v,{x1,…,xp})=min1⩽j⩽pd(v,xj).Once prototypesx1∗,…,xp∗have been obtained, solving either (1) or (2), a partition {C1,C2,…,Cp} of the set V of entities is defined by allocating each entity v∈V to its closest prototype:(4)dxi∗,v⩽dxj∗,v∀v∈Ci,i=1,2,…,p.V-MSSC and E-MSSC differ in the space in which prototypes are sought, and thus they may yield different clusters. In [12], 1000 test instances were generated, each having 10 entities uniformly distributed on the line segment [0,1], and p=2 prototypes were sought. In 160 cases, the clusters obtained by the V-MSSC and E-MSSC models were different. It will be shown in Section 5 that this percentage is larger in general graphs, in which the E-MSSC yields clusters which may not be discovered by solving the V-MSSC.As discussed in Section 2, V-MSSC is a p-median problem with d2 as distance measure. We refer the reader to [4,5,17,27,32] for an analysis of p-median problems and algorithmic approaches. We present now some properties of the E-MSSC, extending the results given by the authors in [12]. These will be helpful in the design of the heuristic algorithm described in Section 4.We assume in what follows that the number p of prototypes to be located is strictly smaller than the number of nodes. Otherwise, the problem is solved in a straightforward manner, since locating prototypes at all nodes would yield objective value of zero, which is optimal.First we show that any optimal solution to E-MSSC yields p non-empty clusters, since, any prototype has at least one entity which is strictly closer to it than to any other prototype.Property 3.1Letx1∗,…,xp∗be an optimal solution to(2). Then, for anyxj∗there exists v∈V such thatdv,xj∗<d(v,xi∗)∀i≠j.Suppose that, for any v there existsxi(v)∗withdv,xj∗⩾dv,xi(v)∗such that i(v) is not equal to j. Take an arbitrary v0∈V such thatv0∉x1∗,…,xp∗. It is clear that the collection of prototypes replacingxj∗by v0 will yield a strictly lower objective value, which contradicts the optimality ofx1∗,…,xp∗.□Since any optimal solution minimizes the sum of increasing functions of the distance (point,prototype), one has the following.Property 3.2At any optimal solutionx1∗,…,xp∗, the shortest path from any node v∈V to its closest prototype cannot pass through any other prototype.Whereas prototypes are allowed to be located at the interior of edges, they cannot be concentrated on a given edge, since each edge can contain at most one optimal prototype in its interior, as stated in the following.Property 3.3Letx1∗,…,xp∗be an optimal solution to(2)and let e=(u,v)∈E. The interior of e contains at most one optimal prototype. If it contains one optimal prototypexj∗, then both endpoints u and v havexj∗as the closest prototype.By contradiction, suppose that the interior of e contains two optimal prototypes,xi∗,xj∗,xi∗≠xj∗. Without loss of generality, we assume thatxi∗is between u andxj∗, which is betweenxi∗and v. By Property 3.1, there exist vi,vj∈V such thatdvi,xi∗⩽dvi,xk∗∀kdvj,xj∗⩽dvj,xk∗∀k.By Property 3.2, u,v, the ending nodes of e are not optimal prototypes since the shortest path from viand vjto their closest facility would otherwise pass through another prototype.For all nodes v∗ that havexi∗as their closest facility, the shortest path from v∗ toxi∗passes through the end point u (otherwise, it would also pass throughxj∗, contradicting Property 3.2). Hence, if we replacexi∗by u in the set of prototypes, the objective function would be strictly decreased, contradicting the optimality ofx1∗,…,xp∗. Thus, it is not possible to have two different prototypes in the interior of the edge e.Now, suppose that only one prototypexi∗belongs to the interior of the edge e. In order to show that both u and v havexi∗as their closest prototype, suppose, by contradiction, thatxj∗exists withdu,xi∗>du,xj∗. For any node v∗∈V, havingxi∗as its closest prototype, it follows that the shortest path from v∗ toxi∗must pass through v and not through u, since otherwisexj∗would be closer thanxi∗to v∗. Hence, replacingxi∗with v in the set of prototypes, we would obtain another feasible solution with a strictly smaller objective value, contradicting the optimality ofx1∗,…,xp∗. This shows that both u and v havexi∗as their closest prototype.□By Property 3.3, given a node v, if an edge e adjacent to v contains in its interior some optimal prototypexj∗, then both endpoints of e, including v, must havexj∗as their closest prototype, and thus v cannot be an optimal prototype. This implies the following.Property 3.4If an optimal prototype is located at a node v∈V, then the interior of all edges adjacent to v contains no optimal prototypes.We end this section studying in more detail the case p=1, i.e., one single prototype is to be chosen, which can be seen as the centroid of the network. Of course this case has no direct application for clustering (just one cluster, namely, V, will be obtained), but it will be useful to design the algorithm for the general case.For p=1, it is easy to construct a finite dominating set, [25], i.e., a set known to contain an optimal solution, and thus global optimization of Problem (2) is reduced to inspecting a finite set of candidates. To construct such finite dominating set, it is important to recall that, given v∈V, the distance from v to a point x in a given edge e∈E with endpoints u1 and u2 is given byd(v,x)=min{d(v,u1)+d(u1,x),d(v,u2)+d(u2,x)}=min{d(v,u1)+d(u1,x),d(v,u2)+le-d(u1,x)}.Whether the minimum above is attained at the first or the second term depends on the relative position of point x with respect to the so-called bottleneck point, [26], z(v), defined as(5)z(v)=12(le-d(v,u1)+d(v,u2)).Formula (5) allows us to compute the distance from node v to any point in the edge e:•If z(v)⩽0, then the shortest path from v to any point x∈e passes through u2.If z(v)⩾le, then the shortest path from v to any point x∈e passes through u1.If 0<z(v)<le, then the shortest path from v to x∈e passes through u1 if x belongs to the sub-edge with endpoints u1 and z(v), and it passes through u2 if x belongs to the sub-edge with endpoints z(v) and u2.In the latter case, such z(v) will be called a bottleneck point. By definition, for such v, the distance from v to z(v) via u1 is equal to the distance from v to z(v) via u2, and then two shortest-paths exist from v to z(v).Given an edge e∈E, and v∈V the distance to v is an affine function on the two subintervals (possibly degenerate) in which the bottleneck z(v) splits e. Hence, within each such subinterval, the squared distance is a quadratic polynomial in the variable x. This process can be done on any given edge e for all nodes v, calculating all bottleneck points z(v), and splitting e into O(∣V∣) subintervals, such that, within each subinterval, each squared distance is a quadratic polynomial function, and thus the sum of the squared distances is also a quadratic polynomial function. In other words, the objective function of Problem (2) is a second-degree polynomial function in one variable on each such interval. Thus, the derivative of its minimum point should be equal to zero, if it belongs to the considered interval; otherwise its minimum is achieved at one of endpoints of the considered interval. Therefore, at each edge e, the objective function can have at most ∣V∣−1 local minima, which are obtained analytically. By doing this process for all edges, we obtain a finite dominating set D for Problem (2). The pseudo-code for finding dominating set D is given in Algorithm 1.Algorithm 1Finite Dominating SetFunctionFDS(N,E));1D=∅;2 for each e∈Edo3for each v∈Ndo4Calculate bottleneck z(v) according to (5)end5Sort all bottleneck values in nondecreasing order;6Form set S as a set of sub-edges obtained by splitting edge e by bottlenecks;7for each sub-edge eℓ from Sdo8Find the minimum objective value; denote the corresponding point with yℓ;9D=D∪{yℓ}endendSince the optimal point has to belong to the finite dominating set D, it can easily be obtained by its inspection.E-MSSC is a nonlinear optimization problem defined on a network. Finding a globally optimal solution may be done by inspecting, for all possible partitions C1,…,Cpof V, the objective value atx1∗,…,xp∗, where eachxj∗is the optimal solution to E-MSSC for p=1. Suchxj∗can be obtained, as described in Section 3, by inspecting a finite set of points. However, this approach is only applicable for networks of very small size.For large data sets, heuristic methods seem to be the only option. For that purpose, we propose a heuristic based on Variable Neighborhood Search metaheuristic (VNS) [29,20,30], although some other metaheuristics can be customized for this problem as well.VNS is a flexible framework for building heuristics to solve approximately combinatorial and global optimization problems. It exploits systematically the possibility of changing the definition of neighborhood structures within the search for a globally optimal (or near-optimal) solution. VNS is based on the following simple observations: (i) An optimum for one neighborhood structure is not necessarily optimal for another neighborhood structure; (ii) a global optimum is a local optimum with respect to all neighborhood structures; and (iii) empirical evidence shows that for many problems all local optima are relatively close to each other. The first property is exploited by using increasingly complex moves in so-called Variable Neighborhood Descent (VND) in order to find local optima. The second property suggests using more neighborhoods if the local optima found are of poor quality. Finally, the third property allows, once a local optimum is reached, to exploit this information to find a better local optimum in its vicinity. See [19,20,30] for further details and applications.Three important choices for the implementation of VNS are how a starting solution is generated, how the shaking is performed, and how local searches are implemented. We describe now such issues.The simplest way to build an initial solution is to follow an iterate process by randomly selecting points which do not violate the structural properties described in Section 3. Given a k-uple (k<p) of protopypes already selected, we say that a point of the network is feasible if, together with those prototypes previously chosen, Properties 3.3 and 3.4 are satisfied. We say an edge is feasible if it contains feasible points. In our randomized procedure, at each step one feasible edge is chosen at random, and then one feasible point in such edge is chosen at random. The process is repeated until p points are obtained.After calculating, as preprocessing, all-pairs shortest path distances, the set of feasible edges E1 initially contains all edges, and the set of feasible points N1 contains all points of the network. Once a prototype has been chosen, we fathom points and edges according to Properties 3.2 and 3.4. If the chosen prototype belongs to the interior of one edge, then we eliminate all points of such edge from the set of feasible points. If the prototype is equal to one endpoint, we eliminate such endpoint and all interior points of each edge adjacent with such prototype as well. After that, each edge that does not contain feasible points is excluded from the set of feasible edges. So, a random initial solution X is generated by the procedure described as Algorithm 2.Algorithm 2Find initial solution at randomFunctionRIS (N,E,p,X);1 Calculate all-pairs shortest distances;2X=∅; E1=E; N1=N;3 fori≔1,…,pdo4choose an edge e from the set E1 at random;5choose a feasible point y on the edge e at random;6X=X∪{y};7reduce sets E1 and N1;endSince using a good starting solution may be crucial to speed up the convergence of the procedure, we can enhance the quality of the solution obtained in Algorithm 2 by running a heuristic for the V-MSSC instead. Both strategies will be analyzed in the computational results reported in Section 5.Feasible solutions of E-MSSC are identified with sets X⊂P(N) of cardinality p. The distance between two solutions X1,X2 is equal to k if and only if the sets X1 and X2 differ exactly in k locations. A (symmetric) distance function ρ can be defined on the set of solutions asρ(X1,X2)=|X1⧹X2|=|X2⧹X1|∀X1,X2.Neighborhood structures are induced by the metric ρ, i.e., k locations of facilities (k⩽p) from the current solution are replaced with k locations that are not in the current solution. We denote by Nk, k=1,…,kmax(kmax⩽p) the set of such neighborhood structures, and by Nk(X) we denote the set of solutions forming neighborhood Nkof a current solution X. In our implementation kmax, the highest radius considered, is set to p.The most popular local-search approach for the Euclidean MSSC is the so-called k-means algorithm, [23]. The k-means is a location–allocation procedure, in which location and allocation steps are repeated until convergence: in the location step (the allocations are assumed to be given), the optimal locations for the p prototypes are obtained; later, in the allocation step (the prototype locations are assumed to be given), all entities are allocated to their closest prototype. The process is repeated until convergence is reached. The key property is that, in the location step, since the allocations are assumed to be fixed, the problem is split into p independent problems, namely, finding one optimal prototype for each cluster.Here we follow the very same strategy: we propose a location–allocation heuristic, Algorithm 3, in which we exploit the fact that, in the location step, the p independent subproblems to be solved are easy, since, as discussed in Section 3, they can be solved by inspection of the low-cardinality set of candidate points constructed as in Algorithm 1.Algorithm 3K-Net-Means algorithm (Net-KM) for the NMSSC problemFunctionNetKM (n,p,X);1Cj=∅, j=1,…,p;2 repeat3fori≔1,…,ndo4m(ui)←arg minxj∈xd2(ui,xj);m(ui)∈{1,…,p}5Cm(ui)=Cm(ui)∪{ui}end6RemoveDeg(n,p,C);7forj≔1,…,pdo8calculate prototype xjenduntilm does not change or ℓ=Maxit;Starting from a set of p initial prototypes (e.g. taken at random, as explained in Algorithm 2), users are assigned to their closest prototype (steps 3–5). m(ui) denotes the membership index of a user ui. Each user uiis assigned to the clusterCm(ui), where m(u1) is the index of a prototype closest to ui. In the case of ties, i.e., if there are more than one prototype with the same distance to ui, the one with the smallest index is chosen. Steps 7 and 8 are location steps, where prototypes xj, j=1,…,p are found for a given clusters Cj. More precisely, for each cluster a 1-prototype problem is solved Allocation step of K-Net-Means is repeated with the new locations of the prototypes. These steps are repeated until no more changes in assignments occur.When using this local-search procedure, we may face degeneracy[10,13] problems: when customers are allocated to prototypes, some prototypes may remain with no customers assigned. Obviously, if we move one of such prototypes to any node, we will strictly improve the objective value. So, if a degenerate configuration is obtained during this local-search procedure, all prototypes without nodes allocated can be moved randomly to remaining nodes that are not used already as a prototype, improving the objective value. The algorithm for removing degeneracy, as used in Algorithm 3, is described as Algorithm 4.Algorithm 4Removing degeneracyFunctionRemoveDeg(n,p,C);1 Find prototypes without users, i.e., find indices ℓwith Cℓ=∅, ℓ=1,…,q;2 ifq>0 then3for ℓ=1,…,qdo4Relocate prototype xℓ to random non-occupied node u.endendThe basic VNS rules, as described above, for solving the E-MSSC problem, lead to Algorithm 5.Algorithm 5Basic VNS for E-MSSCFunctionNet-VNS (x,kmax,tmax);1 Get an initial solution X;2 repeat3k←1;4repeat5X′← Shake(X,k)/∗ Shaking ∗/;6X″←NetKM(n,p,X′)/∗ Local search ∗/;7iff(X″)<f(X) then8X←X′; k←1/∗ Make a move ∗/;else9k←k+1/∗ Next neighborhood∗/;end10t←CpuTime ()untilk=kmax;untilt>tmax;The aim of this section is twofold: first, we want to explore whether the new model, E-MSSC, is essentially different from the V-MSSC, by checking if the clusters obtained are the same or not to those given when only entities (nodes) are allowed to be prototypes. In [12] some experiments were performed on line segment networks, showing that different clusters are obtained by V-MSSC and E-MSSC in around 20% of cases. We will show that, for more complex and larger networks, the differences are larger. Second, we want to explore the influence of the starting solution strategy, as described in Section 4.1.For solving V-MSSC we use a VNS based heuristics described in [17], called here VNS-0. As an initial solution, p nodes are selected at random. Then, the algorithm explores neighborhood structures, induced by metric ρ, (see Section 4.2) using Interchange (or vertex substitution) heuristic as a local search.For solving E-MSSC, three different VNS-based heuristic, called VNS-1, VNS-2 and VNS-3, are applied. Algorithm VNS-1 starts with the solution obtained by VNS-0, and then the local-search described in Algorithm 3 is performed. Algorithm VNS-2 uses as starting solution the one obtained by VNS-1, and then our Network VNS, Net-VNS (X,kmax,tmax), explained in Algorithm 5, is run. Finally, VNS-3 starts with a random initial solution, obtained by RIS(N,E,p,X), followed by our Net-VNS(X,kmax,tmax).All algorithms described in the previous paragraph have been tested on 40 p-median instances taken from the OR-Lib [8]. Each algorithm has been run on each instance 10 times for different choices of the random seed, with the time limit of 10seconds. The results, obtained with a personal computer with a 2.53gigahertz CPU and 3gigabytes of RAM, are reported in Tables 1 and 2. The first three columns are common to both tables. The first column, Instance, gives the name of the OR-Lib instances. Instances parameters, namely, the number n of entities, and the number p of prototypes sought, are given in columns 2 and 3 respectively.Effect of the initial solution. In Table 1 we provide numerical results which give us more insight into the behavior of the VNS algorithm VNS-0 for solving V-MSSC, as well as into the behavior of VNS algorithms for solving E-MSSC. More specifically, we were interested in exploring the ability of Algorithm 3 to improve a solution found by VNS-0 as well as the ability of VNS-2 to improve the output solution of VNS-1. Also, we wanted to check the influence of the initial solution on our Network VNS. For this purpose, for each chosen random seed, we compared the objective values of the solutions found by VNS-0, VNS-1, VNS-2, VNS-3, i.e. fVNS−0, fVNS−1, fVNS−2, fVNS−3, by calculating % deviations using the following formula:dev(a,b)=fa-fbfa·100.More precisely, for each chosen seed we calculated the following % deviations: dev (VNS-0,VNS-1), dev (VNS-1,VNS-2) and dev (VNS-3,VNS-2). The average values as well as standard deviations for each of these % deviations regarding ten different seeds are reported in Table 1.From Table 1 the following observations may be derived.1.VNS-2, which uses the most sophisticated strategy for building its starting solution (it solves heuristically the V-MSSC and then runs Algorithm 3) clearly outperforms VNS-3, which is initialized with a random solution. Indeed, only for instance pmed3 VNS-3 behaves better (average % deviation dev (VNS-3,VNS-2) is equal to-0.384). On the other hand, the overall average % deviation is 13.394, thus favorable for VNS-2. Therefore, the best known solutions for all instances, except one, were found by VNS-2. This stresses the importance of having a good initial solution for solving E-MSSC, in accordance with previous observations on solving the continuous p-median problem (also called multi-source Weber problem) [9,21] and MSSC onRn[18]. It is apparent that the best results for these problems are obtained if the algorithm takes its time to find the V-MSSC solution first, to be used as starting solution.VNS-2 does not significantly improve VNS-1: on 30 instances out of 40 the average % deviation dev (VNS-1,VNS-2) is equal to 0, while on the others the average % deviation is between 1.503 and 0.001. Therefore, the overall % deviation is very close to 0, i.e. 0.083. This means that the V-MSSC solution obtained with a standard VNS, followed by a local search, already yields excellent results.The average % deviation dev (VNS-0,VNS-1) on all instances is between 7.387 and 0, while the overall average % deviation is equal to 1.526. Furthermore, just on 16 instances out of 40 the average ratio is equal to 0. Therefore, we may conclude that the local search (Algorithm 3) is usually capable to improve the V-MSSC solution.Table 2 presents a comparison of V-MSSC and E-MSSC model. The first three columns are the same as those in the Table 1. Columns 4 and 5 report the results of two different heuristics: in column V-MS we have chosen as prototypes those p entities minimizing the sum of distances from the entities to the closest prototype, i.e., the optimal solution to the p-median problem, while in column V-MSS we report the value of the best solution of V-MSSC obtained in ten runs. In both cases, the problem is not solved exactly, but using the version of the VNS for solving p-median described in [17], i.e. VNS-0. The best objective values fV−MSand fV−MSSof the prototypes obtained when solving both problems are compared: column dev reports the % deviation between these two values (i.e., dev (V-MS,V-MSS)). The next column reports the best solution value (column E-MSSC) obtained by one of three different variants of VNS applied to the E-MSSC problem (VNS-1, VNS-2, VNS-3) in ten runs. The deviation of this value from the value reported in column V-MSS is reported in column 8. It should be emphasized that almost all values reported in column E-MSSC were found by VNS-2. The only exception is instance pmed2, on which VNS-3 found the best value. Finally, the last two columns analyze whether E-MSSC yields solutions which are not obtained when only entities are considered as prototypes. Column Node indicates whether the set of optimal prototypes, which corresponds to the value reported in column E-MSSC only contains nodes (and coincides with the optimal prototypes for the V-MSSC problem). Even of the set of optimal prototypes of E-MSSC and V-MSSC do not coincide, it may be the case that they yield identical clusters. This is reported in the last column, Same.From Table 2 the following observations may be derived.1.Comparing the values reported in columns V-MSS and E-MSS, one can observe that the difference between these values mostly depend on the value of p. For almost all instances with p=5 or 10, especially those with large n, the best obtained values for V-MSSC and E-MSSC are the same, as well as yielded clusters. On the other hand, for larger values of p, a significant lower value for E-MSSC is given, yielding different clusters than those obtained by V-MSSC.In 12 out of 40 instances, the prototypes of E-MSSC and V-MSSC coincide (see column Node). On the other hand, in 19 instances out of 40 (47.5%, see column Same), the clusters obtained by the two methods are the same. This ratio is much lower than the one reported by the authors in [12], whose preliminary results on clustering on the line showed that 80% of the cases considered gave the same partitions.The classical p-median problem usually yields very good solutions for V-MSSC, as seen when comparing columns V-MS and V-MSS, so they could be used almost indifferently to build the starting solution of VNS-2.Minimum Sum-of-Squares Clustering problem (MSSC) inRnis probably the most studied clustering problem in the literature. Only recently such MSSC model has been extended to networks, [12], which is a more natural framework for clustering problems from complex systems. In this paper we suggest a basic Variable Neighborhood Search approach for solving E-MSSC, namely, the MSSC when prototypes can be located at vertices or on edges of the network.Perturbations of the incumbent solution are obtained by using the symmetric difference between two sets (solutions) of common cardinality p, i.e., a random solution at distance k from the incumbent solution is the one that has k different elements. The well-known k-means algorithm is adapted to be used as a local-search routine.Different VNS-based heuristics developed differ in the way initial solutions are generated. From the computational analysis performed on test instances, several conclusions can be obtained. First, it appears that approximately in 52% of the cases, clusters obtained by E-MSSC and V-MSSC (when prototypes are restricted to be nodes of the network) are different. In addition, finding a good initial solution is shown to be crucial for getting a final solution of good quality.Several possible directions for future work exist. First, new VNS-based heuristics for solving E-MSSC problem, such as General VNS or Decomposition VNS can be developed; second, the proposed methodology can be extended to similar and more complex clustering models on networks, such as those including additional constraints; third, further testing of our VNS-based heuristic on larger and real world test instances would allow to support strongly our conclusions; finally, while the paper assumes the number p of clusters to be fixed, one may consider p as a decision variable, as done in [11] for the p-median problem.

@&#CONCLUSIONS@&#
