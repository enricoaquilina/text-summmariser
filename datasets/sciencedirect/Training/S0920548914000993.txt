@&#MAIN-TITLE@&#
SemQuaRE — An extension of the SQuaRE quality model for the evaluation of semantic technologies

@&#HIGHLIGHTS@&#
We define a quality model for semantic technologies by extending SQuaRE quality model.The model specifies a hierarchy of quality characteristics and measures with formulas.We show that the quality model is complete and successfully applied in practice.

@&#KEYPHRASES@&#
Quality model,Semantic technology,SQuaRE,

@&#ABSTRACT@&#
To correctly evaluate semantic technologies and to obtain results that can be easily integrated, we need to put evaluations under the scope of a unique software quality model. This paper presents SemQuaRE, a quality model for semantic technologies. SemQuaRE is based on the SQuaRE standard and describes a set of quality characteristics specific to semantic technologies and the quality measures that can be used for their measurement. It also provides detailed formulas for the calculation of such measures. The paper shows that SemQuaRE is complete with respect to current evaluation trends and that it has been successfully applied in practice.

@&#INTRODUCTION@&#
It is well known that software quality is a crucial need across domains (e.g., security and health) and technologies (e.g., operating systems and databases), and that to obtain high-quality software products, the specification and evaluation of quality are of pivotal importance [1].Semantic technologies provide new ways to express knowledge and data in machine processable formats that can be exploited by software agents. There are different types of semantic technologies [2], which can be used for different tasks and which fulfill different requirements. For example, ontology editors are used for implementing ontologies, whereas ontology matching tools are used for mapping concepts of one ontology to concepts of another. However, some tools also share a number of requirements related to different technology types, as is the case of ontology engineering environments. An example of such tool is Protégé,11http://protege.stanford.edu/.an ontology editor that provides additional functionalities such as reasoning (HermiT plug-in22http://protegewiki.stanford.edu/wiki/HermiT.) or querying (SWRL-IQ plug-in33http://protegewiki.stanford.edu/wiki/SWRL-IQ.). In recent years we have seen an exponential growth of semantic technologies, and in order to assess their quality, multiple evaluations of such technologies have been proposed, from general evaluation frameworks [3] to tool-specific evaluations [4,5] and even to characteristic-specific evaluations [6].In order to benchmark different tools, which is an important aspect of evaluation [7], evaluation results from various sources need to be integrated and compared. In this way, through the observation of the results of quality evaluations, the quality of different products is compared in terms of meeting the users' needs. This is particularly significant for the integration of the evaluation results of different types of tools and for the selection of tools according to their quality. However, since no consistent terminology for describing semantic technology quality exists, the comparison of evaluation results sometimes requires additional effort. For example, the results of a specific reasoning task obtained by Baader et al. [8] and by Glimm et al. [9] can be compared, but the fact that such results are presented with different terminologies, “labeling time” and “classification time” respectively, requires a deeper analysis of the evaluation process. Furthermore, in some cases different characteristics are evaluated, and the analysis of these results or their reuse can be misleading or impossible. An example of this occurs in the analysis of reasoning times by Meditskos and Bassiliades [10] and by Urbani et al. [11]; in both cases the results are impossible to integrate because when they refer to reasoning times, they refer to different reasoning processes.Software quality models provide the basis for software evaluation and give a better insight of the characteristics that influence software quality by specifying a consistent terminology for software quality and by providing guidance for its measurement. Evaluations from various sources which are driven by a quality model produce results that can be much more easily integrated and, hence, the comparison and benchmark of tools become easier. Generic software quality models exist (e.g., SQuaRE [12]) and in order to use a generic quality model in a specific domain, the model usually has to be extended to include the particularities of such domain [13,14].Various methods for extending quality models have been proposed in the literature [1,15]; they all follow a top-down approach, starting from general characteristics to concrete measures. However, as reported by some authors (e.g., Botella et al. [16]), identifying all the elements of a quality model can be a difficult task when using this approach. For some cases a bottom-up approach can be also applied, as those in which many evaluation results are available and from which the quality model can be extracted. An example of this case is the semantic technology field, in which various initiatives such as the SEALS project44http://www.seals-project.eu/.[17] and the Ontology Alignment Evaluation Initiative55http://oaei.ontologymatching.org/.have performed evaluations and provided evaluation results.Following the discussion above, it is important to have a quality model in the semantic technology domain that guides the evaluation of different semantic technology types. The goal of this paper is to define a quality model for semantic technologies that extends the SQuaRE quality model, starting from real semantic technology evaluations. The model here proposed has been evaluated in terms of completeness, flexibility, and applicability.The quality model for semantic technologies is a hierarchical quality model developed from the evaluation results provided by the SEALS project. This model describes a set of quality characteristics and sub-characteristics for six different types of semantic technologies: ontology engineering tools, ontology matching tools, reasoning systems, semantic search tools, and semantic web services; ontology annotation tools have been included into the model during the process of evaluation. For each of these sub-characteristics, a set of quality measures is specified, and formulas for each measure are provided. These measures are organized into hierarchy of base measures, derived measures, and indicators.The reminder of this paper is organized as follows. Section 2 gives an overview of existing generic software quality models and presents the elements of a software quality model with their definitions. Section 3 presents the current state of semantic technology quality specification. Section 4 describes the available top-down methods for extending software quality models. Section 5 describes how SemQuaRE, a quality model for semantic technologies, is defined; Section 6 presents the evaluation. And finally, Section 7 draws some conclusions and includes ideas for future work.According to the ISO/IEC 9126 standard [18], one of the processes in the software development lifecycle is the evaluation of software products in order to satisfy software quality needs, where software quality is evaluated by measuring software attributes. Those attributes can be grouped into categories that are called quality characteristics, and these quality characteristics can be later decomposed into quality sub-characteristics [12]. Furthermore, the ISO/IEC 9126 standard states that software product quality should be evaluated using a quality model.Various generic software quality models have been described in the literature: McCall's quality model [19]; Boehm's quality model [20]; the ISO 9126 quality model [18]; and the SQuaRE [12] quality model. Next, we give a brief description of the most recent software quality models.The International Organization for Standardization (ISO) identified the need for a unique and complete software quality standard and, therefore, produced the ISO 9126 standard for software quality. The ISO 9126 standard offers a complete view of software quality with definitions for all the quality characteristics and sub-characteristics that the standard describes.The ISO 9126 is a hierarchical quality model that defines three types of qualities: internal quality, external quality, and quality in use. The standard describes six main quality characteristics for external and internal quality, which in turn are decomposed into sub-characteristics, and also provides the internal and external measures for sub-characteristics. For quality in use, the standard proposes four characteristics.Although the ISO 9126 standard has been accepted and used successfully, some problems and issues related to its further use and improvement have been identified. These problems eventually arise mainly because of advances in technologies and changes in users' needs. As pointed out by Azuma [21], the main problem is due to issues on metrics (e.g., the existence of metrics that have no verified direct co-relation with quality characteristics but that are generally recognized as related to product quality; and the distribution of metrics information in several parts of the standard series, among others).With the object of harmonizing the content with the ISO 15939 standard [22] and taking into account the advancement in technology and environmental changes [23], the ISO 9126 standard has been redesigned and named SQuaRE (Systems and software Quality Requirements and Evaluation). The SQuaRE quality model [12] has the same hierarchical structure as the ISO 9126 quality model, with the difference that internal and external quality models are combined into a product quality model, which describes eight quality characteristics and thirty one quality sub-characteristics (Fig. 1), and is related to “static properties of software and dynamic properties of the computer system”. As for quality in use, SQuaRE defines it as “the outcome of interaction when a product is used in a particular context of use”, and describes five quality characteristics and nine quality in use sub-characteristics.Taking into account the quality models described in the literature, we can define a quality model as a set of quality characteristics, sub-characteristics, and quality measures of a product and the relationships between them.According to the ISO 15939 standard [22], software evaluation is performed with the goal of capturing some information about an attribute of a software product. In this process, a measurement method is applied, and the result is a base measure, which captures some information about an attribute. Two or more base measures can be combined in a measurement function in order to obtain derived measures. Finally, base and/or derived measures are later combined to obtain indicators, which are measures that provide evaluations of specific attributes. Therefore, the output of an evaluation process is a set of measures that, according to the SQuaRE standard, has to be specified in a quality model.Finally, for any of those measures a measurement method and measurement scale, called metric, can be defined [18]. Additionally, a unit of measurement can also be defined for all measures.As different quality models use different terminologies for their elements [24], we have based our terminology on the SQuaRE and ISO/IEC 15939 standards.Different types of semantic technologies have been described in the literature [2,3]; however, to the best of our knowledge there is no consistent terminology for describing these technologies and the tasks they are designed to perform. The current scope of our research is limited to five types of semantic technologies (which do not exhaustively cover every type of semantic technology), for which we give the following definitions: Ontology engineering tools, which are software for managing ontologies and their components either through a user or a programming interface; Ontology matching tools, which are software programs for finding correspondences (i.e., alignments) between the components of two ontologies; Reasoning systems, which are software programs for inferring logical consequences from ontology instances; Semantic search tools, which are software programs that provide answers to natural language queries by exploiting data semantics; and Semantic web services, which are software programs that support the use of semantic web services (i.e., web services enriched with semantic descriptions) for different tasks (e.g., discovery, composition and mediation, among others).Multiple evaluations of semantic technologies have been performed to this date, both conducted by individuals and by the community. In order to obtain the current state of specification of semantic technology quality, we have performed a literature review. This section presents such a review on semantic technology evaluations, which was performed according to the procedure described by Kitchenham [25].Next, we present the outcomes of each step described in the procedure:•Background. The purpose of the research is to review previous evaluations of semantic technologies, including the evaluation results, with the object of having an overview of the current state of the quality of semantic technologies with respect to specification.Research questions. We stated two research questions. RQ1: Is there a common approach for specifying the quality of semantic technologies? This literature review will allow us to observe if there is a unique reference or guideline followed by researchers to specify the quality of semantic technologies. RQ2: Which semantic technology characteristics are evaluated and which quality measures are used to evaluate them? The goal here is to observe the quality characteristics and measures used in different evaluations as well as the terminology used.Strategy for searching previous studies. When looking for research relevant to our questions, in order to identify those publications that deal with semantic technology evaluation, we decided to analyze the proceedings of the two most relevant conferences and those of the three most important workshops in the semantic web area that are focused on the topic of evaluation. These proceedings are: International Semantic Web Conference (ISWC) — all eleven editions (2002–2012); European/Extended Semantic Web Conference (ESWC) — all nine editions (2004–2012); International workshop on Evaluation of Ontology-based Tools (EON) — all six editions (2002–2004;2006–2008); International Workshop on Evaluation of Semantic Technologies (IWEST) — all two editions (2010, 2012); and International workshop on Scalable Semantic Web Knowledge Base Systems (SSWS) — all eight editions (2005–2012).In the first phase of the search, the title and abstract of each paper were inspected in order to identify those publications that could answer our research questions. This phase resulted in 179 publications.Study selection criteria and procedures. All publications that are identified in the previous step were thoroughly revised with the object of selecting those that answered the research questions. The final selection was made with respect to several criteria.First, the selected studies included only publications dealing with the following types of semantic technologies: ontology engineering tools, reasoning systems, ontology matching tools, semantic search tools, and semantic web service tools. These are the types of semantic technologies most frequently evaluated in the literature, and are in the current scope of our work. Second, the selected studies included publications discussing or presenting measures for semantic technology evaluation, as well as publications reporting on evaluations of semantic technologies. Finally, publications suggesting new algorithms (e.g., for reasoning or semantic web service discovery) that were evaluated in the publication were also selected.Data extraction strategy. While inspecting the publications collected for this literature review, the data related to research questions were extracted. Extracted data include the type of semantic technology evaluated, as well as measures used in the evaluation. Measures were extracted in their original terminology, which means that we took into account the different terminologies that various authors use for the same measure.The data allowed us to analyze the publications and to answer our research questions.Synthesis of the extracted data. In order to answer the research questions, the data extracted was synthesized. First, the measures were extracted in their original terminology and were classified according to the measure evaluated. This helped us to observe the use of terminology and see whether different terms are used for the same measures. Then, each extracted measure was aligned to a quality characteristic from SQuaRE, and all the measures extracted in the review were classified according to those characteristics and to types of semantic technologies. This permitted us to observe which quality characteristics were evaluated. Finally, the frequencies of each measure and characteristic were also taken into account. This permitted us to observe which measures and characteristics are most common.In total, we analyzed one hundred publications.66The complete list of publications is available for download at http://goo.gl/LzWu0T.Table 1shows the results of the analysis including, for each type of semantic technology, the number of papers in which each type is evaluated, and the quality characteristics used (shown in bold) that were extracted from SQuaRE standard. For every characteristic, a set of measures is presented and, in those cases where different terminologies were used, measures are grouped under a common name (shown in italics). For example, classification correctness for reasoner systems is represented as ‘number of successes’ and ‘number of solved tasks’. In some cases, however, we were not able to determine the exact classification (e.g., in the case of time behavior of reasoning systems). The number of papers in which a measure was found, and the number of papers that evaluated a specific characteristic and a tool type are shown in brackets. Numbers are omitted when only one occurrence appears.Next, we show the results of the literature review with respect to each research question.RQ1. We have found that no common approach for specifying the quality of semantic technologies exists and that no quality model or guideline has been developed in this domain. Furthermore, of all the publications analyzed, we have observed that only one publication provides instructions on the measures to be evaluated. In this publication, the authors propose a set of novel measures for ontology matching tools: comprehensive f-measure, STD score and ROC-AUR score, which have never been used before in evaluations [26]. Five publications provide some discussion on existing measures that have been used before, of which three are related to reasoning systems [7,27,28], and one to semantic web services [29] and ontology matching tools [30].RQ2. Table 1 shows all the information related to the characteristics being evaluated and the quality measures used, and we can outline several conclusions.When we see a specific type of semantic technology, we can observe that there is a variety of characteristics in the quality that are evaluated; i.e., in different publications we observe different characteristics evaluated. For example, while in some publications we find that time behavior of semantic web services is evaluated, in others only correctness is.When we study specific quality characteristics, we can see that there is a variety of measures for measuring them. This means that, although some publications evaluate the same quality characteristic, they measure it with different measures. An example of this can be the classification and satisfiability evaluations of reasoning systems.Different publications use different terminology for measures so their names are different even when they evaluate the same measures. This can be directly observed in Table 1 (e.g., in the case of classification correctness of reasoning systems).As discussed before, the quality of software products is of high importance; therefore, the specification and evaluation of quality is of great concern. Software quality models provide the basis for quality specification and evaluation; however, since there is neither a unique specification nor a quality model for semantic technologies, the current state of semantic technology evaluations is characterized by a large variety of characteristics, measures, and terminology of the quality evaluated. A possible consequence of this variety is that the integration and analysis of different evaluation results (e.g., when performing benchmarks) might lead to misleading and wrong conclusions. Therefore, without a common ground to specify the quality of semantic technologies, it is very difficult to compare them and to assess their quality.Software quality models (e.g., the ISO 9126 or SQuaRE ones) provide insight into characteristics that are generic [14]. However, different types of software products have characteristics specific to them and, therefore, the actual application of these software quality models usually requires refining the generic characteristics to conform to a specific software product or domain [13,14].As pointed out by Al-Kilidar et al. [31], some practical problems with ISO 9126 arose, namely, ambiguity in metric definitions and usability interpretation. Furthermore, the authors argue that a number of attributes and measures are missing, that some characteristics are too abstract, and that the standard itself is open to interpretations, all of which calls its usefulness into question, according to the authors. On the other hand, some authors suggest that, taking into account the nature of the product itself, some new sub-characteristics can be added, the definitions of existing ones can be changed, or some sub-characteristics can be eliminated from the model [16]. By following these suggestions, the problems previously mentioned can be overcome.For various types of applications, different authors have proposed software quality models in the domains of B2B [1], mail servers [32], web-based applications [33], e-learning systems [34], and ERP systems [16]. All those authors have used the ISO 9126 standard as the basis software quality model and have extended it to conform to the particular domain. Additionally, some authors have extended the SQuaRE quality model in the domains of web services [35] and software evolution [36].Software quality model extensions can be performed following two main approaches [37]: a top-down approach that starts from the quality characteristics and continues towards the quality measures, and a bottom-up approach that starts from the quality measures and defines the quality sub-characteristics related to each specific measure.Next, we describe some methods that we have found in the literature for extending software quality models.Franch and Carvallo proposed a method for customizing the ISO 9126 quality model based on a top-down approach [15], which they latter applied in constructing the quality model for mail servers [32]. After defining and analyzing the domain of interest, their method proposes several steps.In this first step, quality sub-characteristics are determined and, according to the domain, some new quality sub-characteristics are added while others are excluded or their definitions are changed. If needed, sub-characteristics are further decomposed according to some criteria, and a hierarchy of sub-characteristics is defined. Afterwards, sub-characteristics are decomposed into attributes, which are more concrete concepts that refer to some particular software attribute (i.e., observable feature). Furthermore, attributes not directly measurable are further decomposed into basic ones.After the quality entities have been defined (quality sub-characteristics and attributes), the relationships between these quality entities are explicitly defined. Three possible types of relationships are identified: collaboration means that increasing the value of one entity implies increasing the value of another entity; damage means that increasing the value of one entity implies decreasing the value of another entity; and dependency means that some values of one entity require that another entity fulfills some conditions. Finally, to be able to compare and evaluate quality, it is necessary to define metrics for all the attributes in the model.In order to build their quality model for B2B applications, Behkamal et al. proposed a method that customizes the ISO 9126 quality model in five steps [1]. The main difference with the previous method is that in Behkamal's approach, the quality characteristics are ranked by experts; thus, experts should provide weights for all quality characteristics and sub-characteristics, and these weights are later used to establish their importance, which can be time consuming and resource demanding. Another important difference is that Behkamal's approach does not contemplate defining relationships between quality entities (quality characteristics and attributes).At the time of writing this paper, we have not found any example of a bottom-up approach in the literature. In some scenarios, however, it can prove helpful to base the extension of the quality model on existing software evaluations and evaluation results when available. In some cases, both bottom-up and top-down approaches are important for building quality models [37]. Therefore, since a larger number of evaluation results are already available in the semantic technology domain, we have decided to follow a research methodology based on a bottom-up approach [38] to define a quality model for semantic technologies. Next, we present the steps of such methodology, including examples of each of them for the case of web browsers evaluation:1.To identify base measures. The output of evaluating a software product with some input data allows identifying the base measures.For example, in web browser evaluation, base measures could be page loading time, startup time, memory consumption, or number of open tabs.To identify derived measures. Base measures can be combined among them and/or with input data in order to obtain derived measures. Derived measures are defined in a way that they bring additional and meaningful information not provided by the base measures themselves; it is also possible to use one base measure in order to obtain more than one derived measure. In some cases it is possible to combine different derived measures to obtain other derived measures.A derived measure for web browsers could be memory consumption per open tab.To identify indicators. Indicators are measures related to a whole evaluation and are obtained by the aggregation of base measures and/or derived measures. As in the previous case, a base or derived measure can be used in more than one indicator. For each indicator, a scale and a unit of measurement should be specified.In the case of web browsers, an indicator could be the average startup time. This indicator belongs to a ratio scale, and it is measured in seconds.To specify relationships between measures. In this step, which can be performed in parallel with the previous ones, relationships between measures are expressed either in an informal way (e.g., the collaboration, damage and dependency categories proposed by Franch and Carvallo [15]), or more formally (e.g., with the formulas used for obtaining the measures, as proposed by Bombardieri and Fontana [36]). For every derived measure defined, it is recommendable to specify the formula (or set of formulas) that allows obtaining such derived measure from the base measures. Similarly, it is also advisable for any indicator to identify the formula that defines such an indicator based on other measures. Also, it is important to note that one indicator can be obtained by means of different formulas.Additionally, in order to have consistency in the quality model, all the lower level measures should be used for obtaining measures at the higher level. That means that the model cannot contain any base measure that is not used for obtaining any of the derived measures or indicators, or that an indicator cannot be obtained by means of a derived or base measure not defined in the model.In the web browser example, the formula for the memory consumption per open tab could be the ratio between the total memory consumption and the number of open tabs. The average startup time could be the average of startup times measured in the evaluation.To define domain-specific quality sub-characteristics. Every software product from a particular domain has some sub-characteristics that are different from other software products and those sub-characteristics, together with more generic ones, should be identified and precisely defined. Every indicator provides some information about one or several software sub-characteristics; therefore, based on the indicators defined in the previous step, software quality sub-characteristics are specified. It is not necessary that every quality sub-characteristic has only one indicator but rather a set of indicators that determines it. Thus quality sub-characteristics can be examined through several different indicators which can be combined to measure certain sub-characteristics. Finally, if needed, some quality sub-characteristics can be combined into more general ones.Similarly, as in the case of the hierarchy of measures, all indicators should be used for quality sub-characteristics in the model, and no quality sub-characteristic should be measured with different indicators to those already specified, nor should they be measured without any of the indicators assigned.A quality sub-characteristic of a web browser measured using the average startup time could be the browser time behavior.To align quality sub-characteristics to a quality model. In this step, the alignment with an existing quality model is established, i.e., the software quality sub-characteristics previously defined are related to others already specified in the existing model. Depending on the domain and nature of the software product, some new quality characteristics can be specified, or existing ones can be modified or excluded.The sub-characteristic defined for web browsers in the previous step can be aligned to the SQuaRE's time behavior sub-characteristic.This section describes each step of the definition of SemQuaRE, a software quality model in the domain of semantic technologies, by following the bottom-up approach and starting from evaluation results.As mentioned in Section 3, the scope of our work does not cover all types of semantic technologies and is currently limited to only those types evaluated in the SEALS project; i.e., SemQuaRE is currently restricted to ontology engineering tools, ontology matching tools, reasoning systems, semantic search tools, and semantic web service tools.The starting point to define software quality measures has been the set of evaluations performed in the international evaluation campaigns organized in the SEALS project. In these campaigns, 41 different tools from organizations in 13 countries have participated producing evaluation results for different types of semantic technologies.77http://about.seals-project.eu/downloads/category/1-.For each type of technology, different evaluation scenarios were defined, using different test data as input in each scenario. In this step we identified the base measures of each evaluation scenario (i.e., those outputs directly produced by the software during the evaluation).Due to space reasons, we cannot present thorough details of all evaluation scenarios. The complete overview of the quality model can be found on the SemQuaRE wiki.88http://semquare.oeg-upm.net.In this section, we just present the outcomes of each step for one concrete scenario, that of evaluating the conformance of ontology engineering tools. Conformance evaluations assess the degree in which the knowledge representation model of a tool adheres to the knowledge representation model of an ontology language according to the different ontology components (e.g., classes, properties) of such knowledge representation models.In this case, the evaluation process is based on the IEEE 829-2008 standard [39], and the evaluation data consists of different test suites, each containing a number of different test cases to be executed. These test suites are used for evaluating the conformance of ontology engineering tools, and each of their cases contains an Origin ontology (Oi), which is the ontology to be used as input.An execution of each of these test cases in a test suite consists in importing the file that contains an origin ontology (Oi) into the tool and then exporting the imported origin ontology from a test case to another ontology file (OiII), as shown in Fig. 2. The ontology (OiI) represents the ontology inside the tool, for which the state is unknown. Therefore, the comparison of two ontologies (Oiand OiII) gives an insight of the extent to which the tool observed conforms to the ontology model.The base measures obtained after a test case execution of conformance scenario of ontology engineering tools are:•Final ontology. The ontology produced by the tool when importing and exporting the origin ontology, i.e., the resulting ontology after importing and exporting the origin ontology.Execution problem. Whether there were any execution problems in the tool when importing and exporting the origin ontology.In this step, the base measures identified in the previous step were combined with the test data to obtain derived measures.In the conformance scenario of ontology engineering tools, based on the test data and the base measures of one test execution, the following derived measures were specified:•Information added. The information added to the origin ontology after importing and exporting it.Information lost. The information lost from the origin ontology after importing and exporting it.Structurally equivalent. Whether the origin ontology and the final one are structurally equivalent.Semantically equivalent. Whether the origin ontology and the final one are semantically equivalent.Conformance. Whether the origin ontology has been imported and exported correctly with no addition or loss of information.The derived measures previously defined give insight into additions or losses of information that are possible due to the tool's internal ontology model and to whether the semantics is preserved.From the derived measures in the conformance scenario of ontology engineering tools, the following indicators were obtained:•Ontology language component support. Whether the tool fully supports an ontology language component. This indicator has a nominal scale, with yes and no as possible values.Ontology language component coverage. The ratio of ontology components shared by a tool internal model and an ontology language model. This indicator has a ratio scale with values ranging from zero to one hundred, expressed in percentage.Ontology information change. The ratio of information additions or losses when importing and exporting ontologies. This indicator has a ratio scale with values ranging from zero to one hundred, expressed in percentage.When using only the base measures, the following indicator is obtained.•Import/export errors. The ratio of tool execution errors when importing and exporting ontologies. This indicator has a ratio scale with values ranging from zero to one hundred, expressed in percentage.Due to space reasons, we have presented only the outcomes for one evaluation scenario. Similar to the example presented above, we have used the bottom-up approach to define measures for other types of tools. Table 2summarizes the results obtained for each type of tool.99As some measures are repeated across the tools, the total number of measures is different than the sum of all measures.We have identified the relationships between measures in a formal way by defining the formulas used for obtaining all derived measures and indicators.For example, the formulas for the Information added (1) and Information lost (2) derived measures calculate the structural differences between the origin and final ontologies in terms of triples, i.e., in terms of all the components of the two ontologies:(1)finalontology−originontology(2)originontology−finalontologyThe formula for the Structurally equivalent (3) derived measure uses previously defined measures to determine whether there is a difference in the structure of the origin and final ontology:(3)informationadded=null∧informationlost=null.The formula for the Semantically equivalent (4) derived measure calculates if the origin and final ontology carry the same amount of information:(4)finalontology≡originontology.Finally, the formula for the Conformance (5) derived measure observes the conformance of the process of importing and exporting an ontology:(5)semanticallyequivalent∧¬executionproblem.Similarly, we have defined the formulas for the indicators obtained in the conformance scenario of ontology engineering tools. These formulas are the following: Ontology language component support (6), Ontology language component coverage (7), Ontology information change (8), and Import/export errors (9).(6)#teststhatcontainthecomponentwhereconformance=true#teststhatcontainthecomponent=1(7)#componentsintheontologylanguagewherecomponentsupport=true#componentsintheontologylanguage×100(8)#testswhereinformationadded≠null∨informationlost≠null#tests×100(9)#testswhereexecutionproblem=true#tests×100In some cases, a measure can be obtained using more than one formula. For example, the Ontology information change measure could be also obtained with Formula 10.(10)#testswherestructurallyequivalent=false#tests×100Similarly, we have defined the formulas for all the derived measures and indicators identified in every evaluation scenario for the different types of semantic technologies. Furthermore, all the formulas defined are completely consistent, i.e., all the base and derived measures are used, and all the formulas contain measures already specified.In this step, and starting from the indicators previously identified, we defined the set of quality sub-characteristics affected by those indicators. In some cases, we were able to reuse existing quality sub-characteristics but, in others, we had to define domain-specific ones.In the conformance scenario of ontology engineering tools, based on the measures and analyses presented above, we have identified three quality sub-characteristics:•Ontology language model conformance. The degree to which the knowledge representation model of the software product conforms to the knowledge representation model of an ontology language. It can be measured with–Ontology language component coverageOntology language component support.Ontology processing accuracy. The degree to which a product or system provides the correct results with the needed degree of precision when processing ontologies. It can be measured with–Ontology information change.Ontology processing maturity. The degree to which a system, product or component meets the needs for reliability while processing ontologies under normal operation. It can be measured with–Import/export errors.Fig. 3presents the base measures, derived measures, indicators, and quality sub-characteristics of the conformance evaluation for ontology engineering tools.In total, we have identified fourteen semantic quality sub-characteristics. Three of them are those described for the conformance evaluation, and the others are the following:•Ontology language interoperability. The degree to which the software product can interchange ontologies (importing and exporting an ontology using two different tools) and use the ontologies interchanged.Ontology alignment accuracy. The degree to which a product or system provides the correct results with the needed degree of precision when performing an ontology alignment task.Reasoning accuracy. The degree to which a product or system provides the correct results with the needed degree of precision when performing a reasoning task.Semantic search accuracy. The degree to which a product or system provides the correct results with the needed degree of precision when performing a semantic search task.Semantic web service discovery accuracy. The degree to which a product or system provides the correct results with the needed degree of precision when finding services that can be used to fulfill a given requirement from the service requester.Ontology interchange accuracy. The degree to which a product or system provides the correct results with the needed degree of precision when interchanging ontologies.Ontology processing time behavior. The degree to which the response and processing times and throughput rates of a product or system, when working with ontologies, meet requirements.Reasoning time behavior. The degree to which the response and processing times and throughput rates of a product or system, when performing a reasoning task, meet requirements.Semantic search time behavior. The degree to which the response and processing times and throughput rates of a product or system, when performing a semantic search task, meet requirements.Ontology alignment time behavior. The degree to which the response and processing times and throughput rates of a product or system, when performing an ontology alignment task, meet requirements.Ontology alignment maturity. The degree to which a system, product or component meets the needs for reliability while performing an ontology alignment task under normal operationBesides these domain-specific quality sub-characteristics, we have identified that the following general ones (of which two come directly from SQuaRE) can also be defined for semantic technologies:•Ease of use. The degree to which a product or system is easy to operate and control by users.Efficiency. The degree of resources expended by a product or system in relation to the accuracy and completeness with which users achieve goals.Satisfaction. The degree to which users' needs are satisfied when a product or system is used in a specified context of use.Ease of use is defined as a sub-characteristic of SQuaRE's Operability sub-characteristic and is related to product quality while, according to SQuaRE, Efficiency and Satisfaction are related to quality in use.Finally, we have also identified those sub-characteristics contained in others (e.g., Ontology alignment maturity is a sub-characteristic of Ontology processing maturity).All the quality sub-characteristics and indicators are completely consistent, i.e., all indicators are used to define quality sub-characteristics, and every quality sub-characteristic is measured using one or several indicators.Table 3shows all semantic quality characteristics and indicators defined for measuring them.Even though ISO 9126 is a widely adopted and used standard, it is now replaced by SQuaRE; that is why we have decided to adopt the latter (SQuaRE) for constructing the quality model for semantic technologies.In the previous step we have identified the set of quality sub-characteristics specific to semantic technologies. In this step, all the sub-characteristics identified were properly assigned to those sub-characteristics that already exist in the SQuaRE quality model, which are as highly comprehensive as those in the ISO 9126 [1]. For example, Reasoning time behavior is aligned to the Time behavior sub-characteristic, which is a sub-characteristic from SQuaRE that, by its definition, is highly related to Reasoning time behavior. As suggested by Franch and Carvallo [15], we have introduced, where needed, new sub-characteristics to be aligned with existing ones. An example of this is Functional compliance sub-characteristic, which is introduced as a sub-characteristic of SQuaRE's Functional suitability characteristic.Figs. 4 and 5show the hierarchy of quality characteristics and sub-characteristics for semantic technologies for product quality and quality in use respectively.

@&#CONCLUSIONS@&#
This paper presents SemQuaRE, a quality model for the evaluation of semantic technologies, which extends the SQuaRE software quality model. The SemQuaRE quality model provides a framework for the evaluation and comparison of semantic technologies, and it is particularly significant for the community in the following aspects:•It is a first step towards a consistent terminology for semantic technology quality, even if it is limited to its current scope.Within such scope, it gives comprehensive definitions of all its elements, i.e., quality characteristics and measures.Although some problems with the ISO quality models have been identified (as described by Al-Kilidar et al. [31]), SemQuaRE has introduced quality measures specific to semantic technologies; it has also specified formulas for all derived measures and indicators, which results in reducing ambiguities in the model and provides a detailed guidance of how to evaluate and measure the quality of semantic technologies.It serves developers of new semantic tools as a checklist for semantic technology quality requirements.SemQuaRE has been built using the bottom-up methodology described in Section 4. Although we have not found any bottom-up approach for extending quality models in the literature, some authors suggest that a bottom-up approach is important when building quality models [37], and we share such opinion. The bottom-up approach that we have used is not validated, so in the future we plan to perform an evaluation and provide details about the validation and usefulness of such approach.During the evaluation process, we have concentrated only on the literature review, i.e., limited venues that include the most relevant conferences and workshops in the semantic field and limited tool types. Although some venues were implicitly included in our study (some editions of the Ontology Alignment Evaluation Initiative1111http://oaei.ontologymatching.org/.and the Semantic Web Service Challenge1212http://sws-challenge.org/wiki/.have been addressed in the SEALS project and in the IWEST workshop series), we plan to extend SemQuaRE by analyzing other evaluation campaigns (e.g., the Semantic Web Challenge,1313http://challenge.semanticweb.org/.the Triplification Challenge,1414http://triplify.org/Challenge.or the Linked Data Cup1515http://i-semantics.tugraz.at/i-challenge/call-for-submissions.) and relevant journals, as well as by including other types of tools in order to get a more complete quality model.SemQuaRE is a hierarchical quality model, as those of the ISO 9126 and the SQuaRE standards are, and this is regarded as an important factor for clear and unambiguous quality models [1]. Furthermore, Bertoa et al. [14] argue that the structure and organization of the model influence its understandability and, in this sense, we can assume that SemQuaRE provides a high degree of understandability to its users; however, we plan to evaluate the understandability of SemQuaRE in future work.One line of future work includes building an ontology to describe SemQuaRE. Such an ontology could be used by developers as an information model and could take the form of a machine-readable artifact easily exploited by software in various applications.