@&#MAIN-TITLE@&#
A harmony search algorithm for high-dimensional multimodal optimization problems

@&#HIGHLIGHTS@&#
DDRA strategy for avoiding generating invalid solutions.Dynamic fret width (fw) strategy of two phases for balance global exploration and local exploitation.DIHS algorithm for improving the solution quality and search efficiency.Take-one strategy for a fast convergence to globally optimal solution is analyzed.

@&#KEYPHRASES@&#
Harmony search,High-dimensional multimodal optimization problems,Dynamic dimensionality reduction adjustment strategy,Wilcoxon signed-rank test,Update-success rate,Population diversity,

@&#ABSTRACT@&#
Harmony search (HS) and its variants have been found successful applications, however with poor solution accuracy and convergence performance for high-dimensional (≥200) multimodal optimization problems. The reason is mainly huge search space and multiple local minima. To tackle the problem, we present a new HS algorithm called DIHS, which is based on Dynamic-Dimensionality-Reduction-Adjustment (DDRA) and dynamic fret width (fw) strategy. The former is for avoiding generating invalid solutions and the latter is to balance global exploration and local exploitation. Theoretical analysis on the DDRA strategy for success rate of update operation is given and influence of related parameters on solution accuracy is investigated. Our experiments include comparison on solution accuracy and CPU time with seven typical HS algorithms and four widely used evolutionary algorithms (SaDE, CoDE, CMAES and CLPSO) and statistical comparison by the Wilcoxon Signed-Rank Test with the seven HS algorithms and four evolutionary algorithms. The problems in experiments include twelve multimodal and four complex uni-modal functions with high-dimensionality.Experimental results indicate that the proposed approach can provide significant improvement on solution accuracy with less CPU time in solving high-dimensional multimodal optimization problems, and the more dimensionality that the optimization problem is, the more benefits it provides.

@&#INTRODUCTION@&#
High-dimensional multimodal optimization problems are more and more often encountered in our real applications, especially due to the big data gained from and the complex problems to be solved in our real world. They are challenging in that the search space is very large due to the high dimensionality of the problem (e.g., >200), and the too large number of modals (i.e., too many local minima) among which only one is the globally optimal. In the case that problem is with more than 1000 dimensions and possibly infinite number of local minima, it is of great challenge on how to search for the globally optimal solution in an efficient time.In recent years, swarm intelligent algorithm casts a population of individuals to perform an effective heuristic random search in parallel with mutual learning process to realize global optimization for an optimization problem. It has received much attention in comparative to conventional mathematical optimization algorithms in that it is not limited by requiring substantial gradient information and not sensitive to initialization [1].As a typical swarm intelligent algorithm and characterized by simplicity, utilizing real-number encoding and fewer mathematical requirements and so forth, harmony search (HS) and its variants [2–15,51–54], mimicking the process of improvising a musical harmony, have been found to be potential in solving optimization problems. They have been applied to many fields of science and engineering successfully (e.g., pipe network design optimization problems [16], structural optimization problems [17,18], nurse rostering problems [19], economic load dispatch problems [20–22], PID controller optimization problems [23], location of wireless sensor networks [24], trajectory planning for robots [25], vehicle routing optimization problems [26,27], reliability problems [28], 0–1 knapsack problems [29], feature selection [30,31] and so on [32–46]).To our limited knowledge, the present HS and its variants have not been found applications to the high-dimensional multimodal optimization problems (e.g., the dimensionality is larger than 200). Possibly this is due to either the so high dimensionality and/or the so many modals (local minima). In this situation, algorithm should be very powerful in exploration; otherwise, the region of the globally optimal solution cannot be positioned; and it should be also very powerful in exploitation when the region has been positioned such that it can be very stable in the region with a good property of convergence to the globally optimal solution.For solving high-dimensional multimodal optimization problems, balancing exploration power and exploitation power is especially important. The exploration is to find new regions in search space [8], where population diversity plays an important role. The exploitation, which expects to obtain a high precision solution, means the computing power of the algorithm by using the information that has already been collected before. Therefore, exploration power is strongly required before locating into the region that contains the globally optimal solution. When the region has been found, the exploration power of algorithm should be degraded and the exploitation power should be enhanced. A gradual transfer from exploration to exploitation should be given for the search without a sharp cut.In this study, we propose a new HS algorithm called DIHS, which employs a new Dynamic-Dimensionality-Reduction-Adjustment (DDRA) strategy and dynamic fret width (fw) strategy. The DIHS can achieve a good balance between exploration and exploitation for solving high-dimensional multimodal optimization problems.The rest of this paper is organized in the following way: Section 2 introduces the standard HS algorithm. Take-one strategy for fast convergence to globally optimal solution is introduced and DIHS algorithm is proposed in Section 3. In Section 4, four parameters (Smax,Smin, HMS andfwmid) are investigated, sixteen high-dimensional benchmark functions and computation results about them are discussed, the convergence and robustness on DIHS are analyzed and a portfolio optimization problem is also used to investigate the performance of DIHS. Finally, conclusions are drawn in Section 5.The optimization problem to be solved is below:MinimizeXf(X),X=(x1,x2,…,xD)∈SS.t.xi∈[xLi,xUi],i=1,2,…,DwhereS⊆RD,XL=(xL1,xL2,…,xLD)andXU=(xU1,xU2,…,xUD)respectively are lower and upper bounds of the available search space, D is the dimensionality of the problem,xi(i=1,2,…,D)is decision variable.The implementation of standard HS algorithm for solving optimization problem is as follows:Step 1. Initialization of optimization problem and algorithm parameters: The optimization problem and the control parameters of HS algorithm are specified. Parameters include HMS, HM considering rate (HMCR), pitch-adjusting rate (PAR), fret width (fw) (fret width is called formerly bandwidth: bw) and the termination criterion (i.e., the maximum function evaluation times: MaxFEs).Step 2. Initializing the HM with a uniformly distributed random number in search space S. HM is a matrix of sizeHMS×D.Step 3. Improvising a new harmonyXnew=(x1new,x2new,…,xDnew)based on the following three rules:For each notexinew(i=1,2,…,D)Ifr1<HMCR, perform rule (a): Harmony memory considera-tion rule.Ifr2<PAR, perform rule (b): Pitching adjustment rule.Else perform random consideration rule (c) with probability1-HMCR.Endwherer1andr2are uniformly distributed random numberbetween 0 and 1.Step 4. If theXnewis better than the worst harmony in the HM, judged in terms of the objective function value,Xnewreplaces the worst harmony in the HM.Step 5. Checking the stopping criterion. If stopping criterion (MaxFEs) is meet, computation is terminated. Otherwise, Step 3 and Step 4 are repeated.Due to the high dimensionality and the multi-modality of the optimization problem, one needs to consider many problems in great detail such that the globally optimal solution can be reached with an efficient computation time: under the constraint that one cannot provide too large number of samples in HM, (1) in the initial search process, exploration power should be as large as possible such that one cannot lose the region of the globally optimal solution; (2) whenever the solutions in the HM are suboptimal in that they are close to the globally optimal solution, the search should be as fast as possible to reach the globally optimal solution, rather than destroyed by search strategy; (3) the search is a gradual process which requires a smooth transfer from initialization to convergence.For that purpose, and considering that the dimension is too high and the modals are possibly too many, we propose several strategies for efficiently finding the globally optimal solution with the HS approach.We consider an extreme case of solutions in the HM, referred to as extreme HM. Assume that we have reached suboptimal solutions in the HM beingHM=[X1X2⋮XHMS]=[y11x2⁎…xHMS⁎x1⁎y22…xHMS⁎⋮⋮⋮⋮x1⁎x2⁎…yHMSHMS…xD⁎…xD⁎⋮⋮…xD⁎]whereX⁎=(x1⁎,x2⁎,…,xD⁎)is the globally optimal solution. We need adjust only one dimension such that the new solution is exactly the globally optimal solution.We now consider two strategies to see which one is more probable (in probability) to reach to exactly the globally optimal solution directly, as the dimensionality D increases. The probability that the new solutionXnewis exactly the globally optimal solutionX⁎is referred to as success rate here.The strategies are take-one and take-all respectively. In the take-one search, the new solution is simply a solution in HM (referred to as the base solution) with an exception of its some dimension whose value takes the value of that dimension of any solutions in the HM; in the take-all search the new solution is generated with its each dimension taking the value of that dimension of any solutions in the HM. Detail of the two strategies is below.Take-all searchFori=1 to Dj=ceil[rand(0,1)×HMS];xinew=xijEndTake-one searchXnew=Xr(r is a random number in {1,2,…,HMS})a=ceil[rand(0,1)×D]j=ceil[rand(0,1)×HMS]xanew2=xaj,j∈U{1,2,…,HMS}For the take-all search, the probability thatxi⁎is chosen from i-th column[xi⁎,xi⁎,…,xi⁎,yii,xi⁎,…,xi⁎]Tof HM asxinewisHMS−1HMS, while altogether we have D columns, thus the success rate of the strategy, is(HMS−1HMS)D. Due to the fact that(HMS−1)/HMSis always less than 1, the success rate of the search is exponentially decreasing with respect to the dimensionality D.For the take-one search, one needs to select a solution randomly among the solutions in the HM, sayXnew←Xr=(x1⁎,…,yjr,xj+1⁎,…,xD⁎). We only choose one dimension ofXnewto adjust using harmony memory consideration rule. Obviously, the probability thatyjris chosen from (x1⁎,…,yjr,xj+1⁎,…,xD⁎) is1/D, and the probability thatxj⁎is chosen from the j-th column[xj⁎,xj⁎,…,yjr,…,xj⁎]Tof HM asxjnewisHMS−1HMS, thus the success rate is1D×HMS−1HMS.Fig. 1shows the success rate of the two strategies with respect to dimensionality D. The success rate of the take-all decreases exponentially while that of the take-one decreases inversely with respect to the dimensionality of the problem. Hence the take-one is substantially potential in a fact convergence from sub-optimality to global optimality compared to the take-all.In fact, the take-all search gains a new solution in which all dimensions are adjusted, which will be seriously possible to destroy the already obtained suboptimal solutions in the HM, while its counterpart, the take-one is a solution (the base solution) in the HM which is already suboptimal, with only one dimension adjusted, making it with more probability to reach to the globally optimal solution for the extreme HM situation.In our approach, the worst solution is kept as the base solution of the take-one search for substituting the worst solution in the HM with a new solution which is with better fitness.By referring the search being in the space defined by the solutions in the HM as exploitation and the search availably outside the space as exploration, take-one and take-all are two extreme cases of the exploitation where only one dimension is adjusted and all the dimensions are adjusted respectively. From above analysis, one can see that the take-one is much beneficial in exploitation for a fast convergence to globally optimal solution from suboptimal solutions compared to the take-all.The high search efficiency of the take-one comes from the assumption that solutions in the HM are really suboptimal (the extreme HM situation): only one dimension is not reached to the globally optimal solution while all the other dimensions are in optimal position. In this case, of course take-one is efficient for adjusting only one dimension (say, that dimension) to reach to globally optimal solution. However, in the early phase of the search, solutions in the HM are not as suboptimal as the extreme HM. Even in the late phase this is true since entering the extreme HM is too lengthy to be implemented, especially for a high dimensional optimization problem. From this view point, take-one is neither necessary nor efficient for a high dimension optimization problem.To tackle the problem, we adopt take-many search, in that some dimensions rather than only one dimension is adjusted. More precisely, the new solution is simply a base solution in HM with an exception of its many dimensions each of which takes the value of that dimension of any solutions in the HM.Take-many should be the one in which the number of dimensions to be adjusted is gradually from large to small as the search proceeds from the early stage to the final stage for search efficiency.We adopt a probabilistic principle for the selection of a dimension with the selection probabilitySP(t)gradually decreased from the early phase to the late phase of the search in the take-many strategy. This means that the number of dimensions adjusted is a fixed function of the search iteration t, but determined in probability, with the probability decreased with respect to t (see Fig. 2). We formulate the selection probability of a dimension as(1)SP(t)=Smax−(Smax−Smin)(tMaxFEs)kwhereSmaxandSminis the maximum and minimum selection probability, respectively; the exponent k is a positive integer which value determines the descent rate of SP (the larger the k is, the lower the descent rate is.k=3is recommended through lots of experiments).For variables which have been selected with SP, we perform harmony memory consideration rule (a) with rate HMCR and perform random generation rule (c) in search space with probability 1−HMCR; and some variables that have been recombined in the harmony memory consideration rule will further be pitch-adjusted with probability PAR using a two-stage dynamic fret width fw.Notice that the dimensionality-reduction strategy can enhance the success-rate and then improve convergence speed only with the harmony memory consideration rule (a). However, it cannot ensure avoiding the harmonies in the HM trapping into a local area that does not contain globally optimal solution, prematurely. And it also cannot ensure obtaining the high-precision globally optimal solution even if globally optimal solution is in the local area defined by solutions in the HM. If the random generation rule (c) has a large probability, it can help harmonies in the HM escape from a local area, however the random generation rule (c) is a completely blind search strategy whose success-rate is very low.Thus, in order to balance the global exploration and local exploitation, a new dynamic fret width is proposed. Because large fw and large PAR cause so severe disturbance in search space that it converges hardly for solutions, and small fw and small PAR cause too weak exploration ability to find globally optimal solution. Generally, in the early stage of search, it is required for fw with large value to increase the diversity of solutions so as to enhance the capability of finding unexplored area, but it is simultaneously required for PAR with small value to avoid the undue disturbance in search space. However, small fw value with large PAR value usually cause the improvement of best solutions in final stage of search process which algorithm converged to global optimal solution [5].To efficiently realize the balance between global exploration and local exploitation, the parameters fw and PAR in our algorithm are changed dynamically with the iteration t (as shown in Fig. 3). The expressions of fw and PAR are presented in equations (2)[5] and (3), respectively.(2)PAR(t)=PARmin+(PARmax−PARmin)×tMaxFEswherePARmaxandPARminrepresent the maximum and minimum pitch adjustment rate.(3)fw(t)={fwmax×(fwmidfwmax)(tMaxFEs/2)2,t≤MaxFEs2fwmid×(fwminfwmid)(t−MaxFEs/2MaxFEs/2)2,t>MaxFEs2wherefwmin<fwmid<fwmax.ThePAR(t)is increased from some small valuePARminto some large valuePARmaxfor fine tuning in the neighborhood of each new solution. Thefw(t)is divided into two stages: global exploration stage and local exploitation stage. In global exploration stage (t≤MaxFEs/2), fw possesses a large value relatively, which contributes to intensify disturbance ability in the search space so as to explore the undeveloped area that may contain the optimal solution. After the exploration stage, the area that contains global optimal solution may have been determined, at this time fw has nearly become equal tofwmid. Next, in the exploitation stage, small value of fw with large PAR contributes to intensify local exploitation power so as to obtain high precision global optimal solution.The pseudo-code of proposed DIHS algorithm for improvising a new harmony is as follows:The process of improvising a new harmony in DIHS algorithm.Xnew=Xworst;J=⌈rand×D⌉; //select an integer randomly from set {1,2,…,D}.Fori=1to DIf rand < SP(t) ori==JIf rand < HMCR //Rule (a)If rand < PAR //Rule (b)xinew=xinew±rand×fw(t)End IfElse  //Rule (c)xinew=xLi+rand×(xUi−xLi)End IfEnd IfEnd ForThe proposed DIHS algorithm is in the framework of a standard HS algorithm. The key difference between DIHS and HS are:(1) HS algorithm improvises a brand new harmony in which each dimension is generated based on three HS rules. DIHS algorithm chooses the worst harmony in HM as adjustment target for improving its fitness value, and a new dynamic dimensionality-reduction adjustment strategy is proposed to select some dimensions of the worst harmony with probability SP to adjust using three HS rules.(2) Parameter fw in HS is a fixed value. However, in DIHS algorithm, a dynamic fw is presented, which is changed dynamically in terms of Eq. (3). And the parameter PAR is also changed dynamically but it is fixed in standard HS algorithm.To verify the performance of DIHS algorithm, several numerical experiments are performed. In the experiments, parameter setting for the compared algorithms is shown in Table 1. Sixteen well-known functions [47–50] and their optimum solutions are listed in Table 2, which include 12 multimodal functions and four com-plex uni-modal problems (the expressions of these sixteen functionsare described in supplementary file S1).All the experiments were performed on Windows XP 32 system with Intel(R) Core(TM) i3-2120 CPU@3.30 GHz and 2 GB RAM, and all the program codes were written in MATLAB R2013a.In this section, we investigate the effect of parametersSmax,Smin, HMS andfwmidon the performance of the DIHS algorithm by numerical experiments. In the experiments, the parameters of DIHS are set as Table 1.(1)The full factorial experiments ofSmax{1, 0.8, 0.6, 0.4} andSmin{0.4, 0.3, 0.2, 0.1, 0.05, 0.03, 0.02, 0.01} onD=1000are investigated by employing sixteen benchmark functions (F1∼F16). Fig. 4(a), Fig. 4(b) and FS2-1∼Fig. S2-14 (FS2-1∼Fig. S2-14 is presented in supplementary file S2) respectively describe the changes on solution quality with different value of (Smax,Smin) for 16 functions.It can be seen from Fig. 4 and Fig. S2-1∼Fig. S2-14 that a largeSmaxvalue and a largeSminvalue cause deterioration (obtain large mean best fitness value) in the performance of DIHS algorithm because the probability that each decision variable is selected to adjust is large accordingly, which makes the update-success rate become very small (see Section 3.1) in exploitation stage. However, “the smaller, the better” for (Smax,Smin) is not a completely correct expression. In Section 3, we compared the performance of take-all search and take-one search, which shows that the take-all search has better solution quality than take-one search whenD<30andHMS=10(see Fig. 1). In many of trials with different (Smax,Smin) combinations, we found that, in the DIHS, the pairwise (Smax=200/D,Smin=20/D) is recommended forD>200and the pairwise (Smax=100/D,Smin=20/D) is recommended for20≤D≤200.(2) Sixteen benchmark test functions are employed to determine the value of HMS. In the test, the mean optimal fitness (is called Mean) of 30 times is shown in Table S2-1 (see supplementaryfile S2). It is indicated that the DIHS algorithm has good performance on most functions whenHMS=10. However, the difference of results is not very significant for most of functions when HMS is between 10 and 50. Considering the computing cost,HMS=10is recommended.(3) The value and position offwmidplay very essential role for achieving balance between global exploration and exploitation. We investigate the effect of parameterfwmidon the performance of DIHS so as to select a most suitable value (Value) forfwmidfrom set{(xU−xL)/102,(xU−xL)/103,(xU−xL)/104,(xU−xL)/105,(xU−xL)/106,(xU−xL)/107,(xU−xL)/108}, and at the same time, to determine a best position (Position) forfwmidfrom three candidate positions (MaxFEs/4, MaxFEs/2,3⁎MaxFEs/4). The full factorial experiments of pairwise (Value, Position) forfwmidare investigated by employing 16 benchmark functionsF1∼F16. In the experiments,D=500, each function is run independently for 30 times. Experimental results are shown in Table S2-2 (see supplementary file S2). Table 3presents the number of each pairwise (Value, Position) getting the best fitness value on all the 16 functions. It indicates that most of the functions can obtain the best solutions whenfwmid∈{(xU−xL)/104,(xU−xL)/105}and its best Position offwmidis equal to MaxFEs/2.To verify the performance of the proposed DIHS algorithm, we compare the DIHS algorithm with seven typical HS algorithms (HS, IHS, SGHS, NGHS, EHS, DSHS, and ITHS) and compare it with four state-of-the-art evolutionary algorithms (SaDE [57], CMAES [58], CoDE [59], CLPSO [60]) employing sixteen benchmark functions as shown in Table 2. MATLAB source codes of the HS, IHS, SGHS, NGHS, EHS, DSHS and ITHS were rewritten according to original paper, and the source codes of SaDE, CMAES, CoDE and CLPSO were downloaded from:http://ist.csu.edu.cn/paper%20and%20matlab%20code/Differential%20evolution%20with%20composite%20trial%20vector%20generation%20strategies%20and%20control%20parameters/CoDE.RAR[59].In simulation experiments, the dimensionality D is set to 200, 500 and 1000, respectively. To make comparison fairly, populations for all the competitor algorithms were initialized with the same random seeds and each algorithm adopts the same terminal condition (MaxFEs=5000×D); For each algorithm, each test function was run for 30 independent replications onD=200andD=500, and it was run for 10 times onD=1000(all algo-rithms take too long time to run for 30 times onD=1000_. Forexample, CMAES takes19000(s)×30≈158(h)_for functionF1.)_. The best fitness value (Best) and the worst fitness value (Worst) of each problem are recorded for all independent runs; the mean fitness value (Mean), standard deviation (Std), and mean CPU runtime (Runtime) of each benchmark problem for all independent runs are stored.(1) Solution quality is stored for comparison for the standard HS, NGHS, IHS, SGHS, ITHS, EHS, DSHS and the proposed DIHS algorithm. Sixteen test functions withD=1000are considered in this work. The Best, Mean, Worst, Std and the Runtime are shown in Table 4, in which the best results are shown in bold and underline. (The comparison of solution quality forD=200and 500 is presented insupplementary file S3:Table S3-1 and TableS3-2.)(2) Solution quality is stored for comparison for the four state-of-the-art evolutionary algorithms (SaDE [57], CMAES [58], CoDE [59], CLPSO [60]) and the DIHS algorithm. All the algorithms have the same number of decision variables and the same number of function evaluations. The sixteen test functions withD=1000are experimented respectively. Five performance metrics (Best, Mean, Worst, Std, and Runtime) for the five algorithms are given in Table 5, in which the best results are shown in bold and underline. (Situation forD=200_and 500 is presentedin supplementary file S3: Table S3-3 and Table S3-4.)(3) Convergence curves of mean optimal fitness values and boxplot figures of eight HS algorithms on functionF11(Griewank Shift function) are shown in Fig. 5, and those of the four evolutionary algorithms (SaDE, CMAES, CoDE and CLPSO) on functionsF11are presented in Fig. 6. (Those for other functions are shownin supplementary file S3).A detailed view of Table S3-1 (see supplementary file S3) indicates that DIHS outperforms HS, IHS and SGHS on all the benchmark functions (D=200) in terms of all the five metrics (Best, Mean, Worst, Std, Runtime). For functionsF1∼F3,F5∼F6,F8andF10∼F15, the DIHS algorithm performs better than other seven HS algorithms over all five performance metrics. For functionsF4, DSHS generates the best result; for functionF7, the best solution is obtained by ITHS; for functionF9, the winner is NGHS; EHS obtains the best results on functionF16. However, there is no significant difference between DIHS and the winner algorithm for functionsF4andF16. In Table S3-2(D=500), only in case of functionF9, the Worst and Std obtained by NGHS are better than that by DIHS. For other 15 problem instances, the advantages of DIHS algorithm are obvious in terms of the five performance metrics.Table 4 indicates that for all test instances (D=1000), DIHS is far superior to other variants of HS algorithm on all five performance metrics. From Table 4, Table S3-1 and Table S3-2, it can be noticed that, of eight comparison HS algorithms, the DIHS algorithm not only obtains the highest precision solution for most of instances, but also consumes the least CPU Runtime for all test functions.For the comparison with four evolutionary algorithms, it can be seen evidently from Table 5, Table S3-3 and Table S3-4 that, the DIHS is superior to CLPSO on all five performance metrics for all 16 functions, and the final solutions of DIHS algorithm are better than or comparable to those of SaDE according to the metrics. For functionsF2,F7,F9,F10andF16, the Best, Mean, and Worst solutions obtained with CMAES are better than or not significant differences with those obtained using DIHS algorithm. However, for others 11 functions, the Best, Mean and Worst solutions of DIHS are better than those of CMAES, and CMAES need much larger memory capability and much more expensive calculation cost than DIHS for all test functions (see Fig. 6(b)). Compared with CoDE, DIHS has obvious advantages in the Best, Mean, Worst and Std for most of functions. However, CoDE is slightly superior to DIHS on performance metric Runtime whenD=200andD=500.In Fig. 5(a), we can see obviously that DIHS algorithm converges faster than other variants of the HS algorithm, and the convergence curves is broken up into 2 stages: exploration stage (the first half of iteration) and exploitation stage (the second half of iteration). In the exploration stage, the convergence curve declines at a slower speed than that in exploitation stage. This is because, in first half of iteration, a large value of fw has strong disturbance capacity in search space, which is beneficial to explore new region; however, in the second half of iteration, the algorithm begin to concentrate on local exploitation, and small value of SP and fw contribute to enhance the success rate of update operation. As shown in Fig. 5(b), DIHS algorithm takes the least time among eight HS algorithms. From the box plots Fig. 5(c), we find that DIHS algorithm has the minimum degree of dispersion and has optimal fitness value for the entire test functions in eight HS algorithms.It indicates from Fig. 6 that, of five compared algorithms, the DIHS algorithm has the highest precision and it spends the least time. For functionF11, the CMAES and DIHS obtain nearly identical quality of solution; however, CMAES takes much more run time than DIHS.Non-parametric Wilcoxon Signed-Rank Test is employed to compare solution quality of eight HS algorithms and four other optimization algorithms in solving numerical optimization problems. The mean values (Mean) of all algorithms are used for multi-problem-based pairwise comparison.The Wilcoxon Signed-Rank Test is conducted at the 5% significance level(a=0.05)to judge whether the Mean obtained with the DIHS algorithm differ significantly from those of its competitors. Because the solution qualities for different functions are represented in different ranges, they are normalized to the interval[0,1]in order to make a fair comparison.Table 6presents the multi-problem-based pairwise statistical comparison results using DIHS and other eleven comparison algorithms, whereT+and T− are defined in literature [55,56]. “T+” denotes the number of cases in which the null hypothesis was rejected and DIHS displayed a statistically superior performance in this problem-based statistical comparison tests at the 95% significance level; “T−” indicates the number of cases in which the null hypothesis was rejected and DIHS algorithm displayed an inferior performance.As revealed by Table 6, whenD=200, 500 and 1000, over all the 16 benchmark instances, the DIHS performs better than all of the 11 comparison algorithms in 5% significance level. Furthermore, we observe that the larger value of the dimensionality D is, the more advantages over other HS algorithms the DIHS has.To further verify the high update-success rate of the DIHS (introduced in Section 3) in the exploitation by numerical experiment, we keep trace of the success rate (SR) of update operation. After a specified number of iterations: Total times (i.e., it is 1000 in our experiment), the SR is recalculated as follows.SR=#STotalwhere #S is the amount that theXworstis replaced successfully by the new solutionXnewin Total times iteration.In this experiment, the SR of DIHS algorithm is compared with that of standard HS algorithm. Fig. 7shows the SR curves of HS and DIHS algorithms for Griewank Shift function onD=1000. It is evident that the SR of DIHS is higher than that of HS algorithm throughout the search process. In the beginning stage, the SR curve of DIHS algorithm decreases sharply, then it keeps a relatively stable status in the middle stage, in the mid-to late phase, the SR curve of DIHS increases gradually, and in the final phase, it falls again.This is because, in the beginning, the fitness value of each harmony in HM is so poor that it is easy to find a better solution than current worst harmony in HM. So the SR is very high in the beginning of iteration. However, it declines quickly because of intense disturbance on multi-variables which make new solution cannot be superior to old solution. In the middle stage, SR is relatively steady so as to continue exploring the global optimal region of optimum solution with a large value of SP and large value of fw. In the mid-to late stage, the global optimal region of optimum solution may have been located and the main goal for the rest of time is to concentrate on finding high-precision solution; at this point, the parameter SP and fw has become very small, so the DIHS has high update-success rate in the final stage until the global optimal solution has been found.To further investigate the convergence process of the DIHS algorithm, we keep trace of the population diversity of the eight HS algorithms in the process of search. The population diversity is defined as follows,Diversity=1D∑i=1D1HMS∑j=1HMS(xij−x¯i)2wherex¯iis the mean value of i-th decision variable in HM.Griewank Shift function is employed for testing. The dimensionality D of each problem is set to 1000. The population diversity curves are shown in Fig. 8.As Fig. 8 shows, population diversity of SGHS algorithm falls sharply in the early stage, which make exploration ability weak prematurely, and the fluctuation scale of population diversity is so large in later phase of search that the algorithm is hard to concentrate all individuals of population on exploiting the high-precision solution. For HS, IHS, ITHS, NGHS, EHS, and DSHS algorithms, population diversity decreases very slowly and has not obvious change in the later stage, which make the individuals of population hard gather to the global optimal solution, so it is very hard to obtain high-precision global solution. The population diversity of DIHS keeps a sustained downward trend throughout the search process and it has not much large fluctuation, which make the DIHS algorithm keep stronger power to explore new-region in the early stage and to exploit the high precision solution in the later stage than other HS variants.We investigate the performance of our algorithm employing a portfolio problem-Nikkei 225. The Nikkei 225 is a stock market index for the Tokyo Stock Exchange (TSE), which consists of 225 companies (http://en.wikipedia.org/wiki/Nikkei_225). The mathematical model of portfolio problem is introduced in [61,62], which is a complex high-dimensional multimodal optimization problem.In the investigation, we apply DIHS algorithm to find an optimal portfolio weightxi(i=1,2,…,D)for Nikkei 225. At the same time, we keep trace of the sets of Pareto optimal portfolios obtained with DIHS algorithm and compute the mean Euclidian distance (MED) from standard efficient frontier to the frontier of each algorithm, variance of returns error (VRE) and mean return error (MRE) [62,63].We separately perform two tests to investigate the performance of DIHS on solving unconstraint and constraint portfolio problem. In test 1, the desired number of assets in the portfolio is not constraint, and the investment proportion of each asset is also unconstraint. The desired number K of assets in the portfolio equals 10 in test 2, the lower limitξiand the upper limitζiof investment proportion of each asset are equal to 0.01 and 1, respectively. The experimental results are shown in Table 7. The λ in Table 7 is the risk aversion parameter which is between 0 and 1. When λ is equal to 0, the goal of optimization is to maximize the return of investment regardless of the investment risk. In contrast, when λ is equal to 1, the goal of model is to minimize risk of the portfolio regardless of the return. For each different value of λ(0≤λ≤1), we keep trace of the corresponding optimal investment selection (return, risk) which will construct a curve of efficient frontier. The curve gives the best possible tradeoff of risk against return, which denotes the set of the optimal portfolios.Table 7 presents that the mean Euclidian distance, variance of returns error and mean return error of DIHS algorithm are less than that of compared PSO algorithm which showed good performance in literature [63]. In Fig. 9(a), the efficient frontier of DIHS almost entirely overlaps with the standard efficient frontier, and in Fig. 9(b) it is also very close to the standard efficient frontier for constraint portfolio optimization. It indicates from Table 7 and Fig. 9 that the DIHS algorithm is effective for solving the high-dimensional portfolio optimization problem.

@&#CONCLUSIONS@&#
