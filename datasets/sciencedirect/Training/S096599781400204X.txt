@&#MAIN-TITLE@&#
Improving the effect of fuzzy clustering on RBF network’s performance in terms of particle swarm optimization

@&#HIGHLIGHTS@&#
This paper quantifies the effect of fuzzy clustering in the design process of a typical RBF network.It is analytically shown that the fuzzy clustering acts to minimize an upper bound of the network’s square error.The PSO algorithm is used to minimize the upper bound and to provide an estimation of the network’s parameters.Finally, the widths and connection weights are further tuned using a steepest descent approach.

@&#KEYPHRASES@&#
Radial basis function neural networks,Fuzzy clustering,Particle swarm optimization,Armijo’s rule,Lipchitz continuity,Distortion function,

@&#ABSTRACT@&#
This paper proposes a novel training algorithm for radial basis function neural networks based on fuzzy clustering and particle swarm optimization. So far, fuzzy clustering has proven to be a very efficient tool in designing such kind of networks. The motivation of the current work is to quantify the exact effect of fuzzy cluster analysis on the network’s performance and use it in order to substantially improve this performance. There are two key theoretical findings resulting from the present work. First, it is analytically proved that when the standard fuzzy c-means algorithm is used to generate the input space fuzzy partition, the main effect this partition imposes to the network’s square error (i.e. performance index) can be written down in terms of a distortion function that measures the ability of the partition to recreate the original data. Second, using the aforementioned distortion function, an upper bound of the network’s square error can be constructed. Then, the particle swarm optimization (PSO) is put in place to minimize the above upper bound and determine the network’s parameters. To further improve the accuracy, the basis function widths and the connection weights are fine-tuned by employing a steepest descent approach. The main experimental findings are: (a) the implementation of the PSO obtains a significant reduction of the square error while exhibiting a smooth dynamic behavior, (b) although the steepest descent further decreases the error it finally obtains smaller reduction rates, meaning that the strongest impact on the error reduction is provided by the PSO, and (c) the improved performance of the proposed network is demonstrated through an extensive comparison with other related methods using a 10-fold cross-validation analysis.

@&#INTRODUCTION@&#
The radial basis function (RBF) networks constitute an effective architecture of artificial neural networks with a substantial number of applications in a wide range of research fields [3,8,23,27,28]. The RBF networks are capable of approximating functions or real systems by establishing the input–output relationships in terms of complex and highly nonlinear decision boundaries. These boundaries are sketched by a set of overlapping radial basis functions, also called receptive fields. Given that the input–output data set is designated as(xk;yk)k=1nwithxk∈Rpandyk∈R; the receptive fields are evaluated as follows,(1)fi(xk)=exp-‖xk-νi‖σi2(1⩽i⩽c)where c the number of hidden nodes (i.e. receptive fields),νi∈Rpthe center of the i-th basis function and σithe corresponding width. The estimated network’s output is,(2)y⌢k=∑i=1cwifi(xk)with wibeing the connection weight of the i-th hidden node. The network’s training process is based on determining effective input–output relationships that are provided by an optimal set of values of the parameters(νi,σi,wi)i=1c. So far, many scholars have investigated the use of fuzzy clustering in RBF network’s design.In [15], Pedrycz explored the output space by using linguistic labels that were obtained by ordinary fuzzy partitions or cluster analysis. To each linguistic label, he assigned weights that were used to guide the clustering process in the input space by a sophisticated modification of the fuzzy c-means model, called conditional fuzzy clustering (CFC). Wang et al. [26] improved the CFC algorithm by introducing a criterion to determine the proper number of clusters within each context and embedded this information into a supervised clustering scheme. Staiano et al. [22] created fuzzy clusters in the input space and linked each cluster to a linear regression model. Then, the network’s output was evaluated as the aggregate of the individual responses of the above regression models. Park et al. [14] performed fuzzy clustering by taking into account the dimensionality reduction of the input space.In this paper, the input space fuzzy partition is generated in such a way so that it will be able to reveal, as to a certain degree, the real data structure while at the same time it will correspond to a minimal square error of the network. To accomplish this task, a class of upper bounds of the network’s square error is developed, which consists of the following components: (a) all the parameters that define the network’s topology, and (b) a distortion function that defines the effect of the input space fuzzy partition on network’s performance. The resulting upper bound constitutes a highly nonlinear function and the particle swarm optimization is employed to minimize it with respect to all network’s parameters. To this end, the widths and connection weights are further tuned by using a steepest descent approach based on the Armijo’s rule [1].The work presented here is organized in five sections, including the introductory section. Section 2 briefly describes the basic features of the particle swarm optimization algorithm. Section 3 analyzes the proposed training algorithm. Section 4 illustrates the simulation experiments. The paper concludes in Section 5.Particle swarm optimization (PSO) constitutes a numerical optimization paradigm that has gained increasing popularity in recent years [9,16,18,29]. The basic design element is a q-dimensional real valued vectorρ∈Rqcalled particle that represents a possible, yet complete, solution of the problem at hand. The aggregate of all particlesP={ρ1,ρ2,…,ρ|P|}forms the swarm, where |P| denotes the swarm’s size. Each particle is assigned a vectorhi∈Rqcalled velocity and a group of informants Tithat includes λ other particles, which are randomly reselected in each iteration. The position corresponding to the lowest value of fitness function obtained so far by the particleρiis symbolized asρibest(t). The position associated with the lowest value of fitness function obtained so far by all particles belonging to Tiis denoted asρgbest(t). The velocityhicomes in the form,(3)hi(t+1)=ωhi(t)+φ1Φ(0,1)⊗ρibest(t)-ρi(t)+φ2Φ(0,1)⊗ρgbest(t)-ρi(t)where⊗is the point-wise vector multiplication,Φ(0,1)is a function that returns a q-th dimensional vector, the coordinates of which are randomly generated by a uniform distribution in [0, 1], φ1 and φ2 are constant positive numbers called the cognitive and social parameters, and ω is also a positive constant called the inertia factor. The position of each particle is updated as,(4)ρi(t+1)=ρi(t)+hi(t+1)A major drawback of the above version of the PSO is the lack of a mechanism responsible to control the magnitude of velocities that fosters the danger of swarm explosion and divergence.In this paper, the magnitude of velocities is controlled as indicated by the following procedure. For the j-th dimension of the particle’s space the largest and the lowest values of all particles are calculated as indicated next,(5)ρjmin=min1⩽i⩽|P|{ρij}andρjmax=max1⩽i⩽|P|{ρij}The maximum velocity associated with the j-th dimension is,(6)hjmax=ρjmax-ρjminℓwhere ℓ is a positive integer with typical values ℓ=2,3,4,5. Then, the velocities of all particles are restricted in the interval-hjmax⩽hij⩽hjmax, and the corresponding vector representation ishi∈[−hmax, +hmax], withhmax=h1maxh2max⋯hqmaxT. The use of Eq. (6) educes a maximum velocity restricted to an area that corresponds to the (100/ℓ)% of the particles’ search space.The main advantages of using PSO instead of other meta-heuristic algorithms are enumerated as follows:(a)It maintains a sound balance between the local and the global exploration of the searching space; therefore, it is able to overcome the problem of premature convergence [11,13,18].It is a parallel searching procedure and therefore, less vulnerable to getting trapped in local minima [13,16,19,24].It does not depend on the initialization; starting anywhere in the feature space, it ensures the convergence to an optimal solution [6,11,13,18].It employs probabilistic transition rules instead of deterministic ones; and therefore, it constitutes a stochastic optimization process, meaning that it is able to exhibit a more flexible and robust behavior [6,9,13,19].Next, a brief survey on the use of the PSO and its variants in designing RBF networks is presented. Feng [5] used the PSO to fine-tune all network’s parameters and to estimate the minimum number of hidden nodes, where the fitness function was the network’s square error. In [7], a modification of the PSO was introduced, according to which the inertia factor was linearly decreasing. Then, the widths and the connection weights were encoded in the particle’s structure. To deal with the over-fitting problems occurring during the training process, Luo et al. [10] proposed a conjunction of regression trees and PSO. The selected fitness function combined the network’s root mean square error and a term quantifying the computational complexity. The use of PSO in the development of adaptive RBF networks was investigated in [21]. The fitness function was a multiple-termed function that involved the network’s square error. In [24], the PSO was inserted into the fuzzy clustering algorithm and used to minimize the fuzzy quantization error of the fuzzy partition. The incorporation of cluster analysis and PSO was also discussed in [4], where there was developed a PSO-based clustering scheme that considered features of the patterns in the data set. Yang et al. [28] employed the PSO to estimate the antecedent parameters of RBF neural-fuzzy systems. The usage of multi-objective PSO in training RBF networks was inquired in [19].The proposed method consists of three sequential steps aiming toward developing a highly accurate RBF neural network structure. These steps are illustrated in Fig. 1.The first step quantifies the effect of the fuzzy clustering on network’s square error; where it is analytically proved that the fuzzy clustering acts, in a straightforward manner, to minimize an upper bound of the network’s square error. The second step involves the optimization of the aforementioned upper bound by utilizing the PSO. The implementation of the PSO derives an estimation of the network’s parameters. Finally, in the third step, the algorithm employs a steepest descent approach to further tune the basis function widths and the connection weights. The above steps are fully analyzed within the subsequent subsections.The clustering process is the standard fuzzy c-means applied in the input space. The corresponding objective function is(7)JFcM=∑k=1n∑i=1c(uik)m(dik)2where uikis the membership degree between the kth training sample and the ith cluster center, dikthe corresponding distance, andm∈(1,∞)the fuzziness parameter. The distance is calculated as,(8)dik=‖xk-νi‖2where ‖⋅‖ is the standard Euclidean norm, andνiis the center of the i-th cluster in the input space.Following the standard minimization procedure, the membership degrees and the cluster centers are calculated in terms of the following relations,(9)uik=∑j=1cdikdjk2m-1-1and(10)νi=∑k=1n(uik)mxk∑k=1n(uik)mThe efficiency of a fuzzy partition in revealing the underlying data structure depends on its ability to accurately reconstruct the training data vectors [16,24]. Since each vectorxkparticipates in all clusters with different membership degrees, all cluster centers contribute in this reconstruction process. The magnitudes of these contributions exclusively depend on the corresponding membership degrees. It appears that the fuzzy partition generates a prediction for a training samplexk, which is denoted asx⌢k∈Rand is calculated as shown by the next proposition.Proposition 1Given that the input space partition is defined in terms of Eqs.(9) and (10), the prediction of the training sample xk is calculated by the unconstrained minimization of JFcM as follows,(11)x⌢k=∑i=1c(uik)mνi∑i=1c(uik)mEq. (7) yields,(12)∇JFcM(xk)=2∑i=1c(uik)mdik∇dik(xk)=0where 0 is the zero vector. If the distance function in (8) is used,(13)∇dik(xk)=12‖xk-νi‖22(xk-νi)⇒∇dik(xk)=1dik(xk-νi)By substituting (13) in (12),(14)∑i=1c(uik)mdik1dik(xk-νi)=0⇒xk∑i=1c(uik)m=∑i=1c(uik)mνiReplacing the symbolxkby the symbolx⌢k, and solving with respect tox⌢k, Eq. (14) derives Eq. (11). This completes the proof of proposition 1. □According to Eq. (11), what the fuzzy partition “understands” about a training data vectorxkis its prediction vectorx⌢k. Bearing in mind that the link between the input space fuzzy partition and the receptive fields is provided by the cluster centers, then the effect of the fuzzy partition on network’s performance can be measured by two quantities. The first one arises when the data vectorxkis passed through the network and comes in the form of the estimated network’s outputy⌢kin Eq. (2). The second one is obtained when the prediction vectorx⌢kis inserted into the network,(15)χ⌢k=∑i=1cwifi(x⌢k)whereχ⌢k∈R. As usual, the network’s performance index is measured by the corresponding square error,(16)JSE(V,σ,w)=∑k=1n‖yk-y⌢k‖2=∑k=1nyk-∑i=1cwifi(xk)2whereV=[ν1ν2…νc]T,σ=[σ1σ2…σc]Tandw=[w1w2…wc].Proposition 2The functionf(ϑ)=e-ϑ2is Lipchitz continuous, with the global Lipchitz constant being equal toL=2/e.The proof is given in [25]. □In view ofProposition 2; given the input training sample xk and the corresponding predictionx⌢k, the following inequality is always true,(17)|fi(xk)-fi(x⌢k)|⩽Lσi‖xk-x⌢k‖Settingϑi(xk)=‖xk-νi‖σiandϑi(x⌢k)=‖x⌢k-νi‖σi, the result of proposition 2 implies that,(18)|fi(xk)-fi(x⌢k)|=e-(ϑi(xk))2-e-(ϑi(x⌢k))2⩽L|ϑi(xk)-ϑi(x⌢k)|=Lσi|‖xk-νi‖-‖x⌢k-νi‖|The triangular inequality gives,(19)|‖xk-νi‖-‖x⌢k-νi‖|⩽|‖xk-x⌢k‖|=‖xk-x⌢k‖Combining (18) and (19) derives the inequality in (17). This completes the proof of proposition 3. □To conclude the analysis and introduce the next proposition, the following vectors are defined,(20)ΔFk=ΔF1kΔF2k…ΔFck=f1(xk)-f1(x⌢k)f2(xk)-f2(x⌢k)…fc(xk)-fc(x⌢k)(21)S=1σ11σ2…1σcTProposition 4In view of (17), (20) and (21); the square error of the network given in (16) is upper bounded as follows,(22)JSE(V,σ,w)⩽2JUB(V,S,w)where(23)JUB(V,S,w)=∑k=1n‖yk-χ⌢k‖2+L2‖S‖2‖w‖2∑k=1n‖xk-x⌢k‖2-∑k=1n∑i<jwiΔFjk-wjΔFik2The proof is given in the Appendix. □According to the proposition 4, the functionJUB(V,S,w)defines a class of upper bounds for the square error. Its structure encompasses two components. The first one is directly related to the network’s topology and is provided by the network’s parametersV,σandw. The second one is the subsequent distortion function,(24)Q=∑k=1n‖xk-x⌢k‖2The distortion function Q measures the ability of the partition to accurately reconstruct the training samples, and as shown in [16,24] it is minimized during the clustering process. Based on the analysis so far, the following remark provides the main theoretical result of the presented analysis.Remark 1Upon the assumption that the widths and the connection weights have been fixed, the direct effect of the fuzzy c-means implementation is to minimize the upper boundJUB(V,S,w)through the minimization of the distortion function Q. In addition, the function Q possesses the following property: as the number of clusters increases the predictions obtained by the partition tend to the respective training vectors and in the limiting case they will be identical [16,24]; therefore,Q→0. Hence, as the number of receptive fields increases,∀i,kit holds thatfi(x⌢k)→fi(xk),ΔFik→0, andχ⌢k→y⌢k, which implies that the upper bound tends to the square error, meaning that the square error is also minimized.It thus appears that the minimization of the functionJUB(V,S,w)will provide a set of values for the network’s parameters that correspond to a minimal square error, while at the same time the respective fuzzy partition preserves the structure of the input training data. The above approach maintains a trade-off between the accuracy of the network and the effectiveness of the input space partition in revealing the underlying data distribution, meaning that the selection of the basis function centers is guided by both the structure of the training data and the minimization of the network’s square error.The objective here is to minimize theJUB(V,S,w)with respect to all network’s parameters in terms of the PSO algorithm. However, the presence of the membership degrees in (11) requires their compulsory estimation in each iteration. As the interest is focused only in the cluster centers, the expression ofx⌢kin (11) is reformulated by eliminating the membership degrees and handling the clusters centers in a straightforward manner. Specifically, the membership degree in Eq. (9) comes in the form,(25)uik=∑j=1c(djk)21-m-1(dik)21-m⇒(uik)m=∑j=1c(djk)21-m-m(dik)2m1-mSubstituting the above result in Eq. (11) obtains the reformulation of the vectorx⌢k,(26)x⌢k=∑i=1c∑j=1c(djk)21-m-m(dik)2m1-mνi∑i=1c∑j=1c(djk)21-m-m(dik)2m1-m⇒x⌢k=∑i=1c(dik)2m1-mνi∑i=1c(dik)2m1-mand using the distance function given in (8) modifies Eq. (26) as follows,(27)x⌢k=∑i=1c(‖xk-νi‖2)m1-mνi∑i=1c(‖xk-νi‖2)m1-mUsing the above expression, the functionJUB(V,S,w)in Eq. (23) involves only the cluster centers. Based on the above analysis, the dimension of the particles’ feature space is:q=cp+c+c=c(p+2), while the corresponding particle’s structure is illustrated in Fig. 2.According to the Remark 1, it is expected that as the size of the network gets larger the upper bound will be closer to the square error. For a small number of hidden nodes, however, the gap between the upper bound and the square error could be large. This would lead to great loss of flexibility and effectiveness in the choice of the network’s size. To solve this problem and gain better fitting capabilities avoiding any over-fitting problems, the widths and connection weights are further tuned by directly minimizing the network’s square errorJSE(V,σ,w). To accomplish this task, a steepest descent approach is employed, the initial conditions of which are provided by the PSO, soon after its convergence has been accomplished. First the following vector is defined,(28)z=z1,…,zc,zc+1,…,z2cT=w1,…,wc,σ1,…,σcTThen, the steepest descent is based on the Armijo’s rule [1], which for the t+1 iteration is,(29)z(t+1)=z(t)-η(t)∇JSE(z(t))with(30)∇JSE(z)=∂JSE∂z1,…,∂JSE∂zc,∂JSE∂zc+1,…,∂JSE∂z2cT=∂JSE∂w1,…,∂JSE∂wc,∂JSE∂σ1,…,∂JSE∂σcTThe partial derivatives in (30) can be easily calculated as,(31)∂JSE∂wi=(-2)∑k=1n(yk-y⌢k)fi(xk)(32)∂JSE∂σi=(-4)wi(σi)3∑k=1n(yk-y⌢k)fi(xk)‖xk-νi‖2The parameter η(t) in (29) is,(33)η(t)=βrwhereβ∈(0,1). In each iteration, the parameter r is the smallest positive integer such that,(34)JSE(z(t)-η(t)∇JSE(z(t)))-JSE(z(t))<-εη(t)‖∇JSE(z(t))‖2withε∈(0,1).To evaluate the proposed method, two kinds of experiments were conducted. The first experiment compared the proposed network to three other networks in terms of a 10-fold cross validation analysis. For each fold, 10 different initializations for all networks were used. The first network, called RBF 1, is trained by the algorithm developed in [25]. This algorithm constitutes a crisp input–output clustering scheme, which assumes constant widths and updates the basis function centers and connection weights in each iteration until convergence. The second network, called RBF 2, is trained by using the conditional fuzzy clustering (CFC) [15]. The third network, called RBF 3, estimates the radial basis function centers by performing a fuzzy clustering in the product space and then projects the centers onto the input space. The widths and the connection weights were estimated by minimizing the square error in (16) using the PSO algorithm.The second experiment was performed by randomly splitting each data set into a training set containing 60% of the data and a testing set containing the rest 40%. Again, 10 different random initializations were considered. This experiment concentrated mainly on studying the effect of PSO on network’s performance and comparing the proposed method with other algorithms that exist in the literature. Table 1reports the parameter setting of the PSO algorithm.For all networks that use fuzzy cluster analysis, the fuzziness parameter was fixed to m=2. The performance of the networks was measured in terms of the root-mean square error,(35)RMSE=JSE(ν,σ,w)nThis example considers two functions. The first one is two dimensional,(36)g(x,y)=x3+0.3x2-0.4x-ywithx,y∈[-1,1]and the second one is three dimensional arising in the form,(37)f(x,y,z)=(1+x-2+y-1.5+z-1)2withx,y,z∈[1,5]For both functions, 200 input–output data pairs were randomly generated. The first simulation case concerned the 10-fold cross-validation analysis. The comparative results are reported in Table 2.In view of Table 2, the following observations can be easily pointed out: (a) In all cases the proposed method obtained far better results in both the training and the testing data; (b) The standard deviations obtained by the proposed network are, in general, smaller than those obtained by the RBF 2 and RBF 3, which are based on pure fuzzy clustering. This observation supports the hypothesis that the way the PSO was implemented did not impose any negative influence to the dependence of the training process on initialization.The second experimental case concerned the 60–40% split of the data. The convergence capabilities of the PSO as far as the minimization of the function JUBis considered are illustrated in Fig. 3for both functions. Notice that the convergent behavior of the PSO is quite smooth. In addition, as the number of hidden nodes gets larger the resulting value of JUBbecomes smaller, something that it was expected in the first place. This behavior is also observed in the next data sets.The effect the PSO imposes on the overall behavior of the network was quantified as,(38)JUB∗=JUB(V,S,w)nThe simulations involved the simultaneous estimation of the RMSE and the functionJUB∗during the implementation of PSO, while as soon as the PSO converged the RMSE values continued to be coming from the steepest descent. This approach enables to clearly judge the individual dynamic performances of the PSO and the steepest descent, and the initial conditions provided by the PSO to the steepest descent, as well. The results for various numbers of hidden nodes are illustrated in Figs. 4 and 5.Notice that since theJUB∗and RMSE are simultaneously reported, the vertical axis of each figure has no label. TheJUB∗corresponds to the dashed line while the RMSE to the solid line. There are some interesting conclusions that can be reached by carefully observing Figs. 4 and 5. First, the behavior of the RMSE appears to be smooth and it strictly follows the behavior of the functionJUB∗. This fact directly implies that the minimization of the upper bound manages to minimize the square error, too. Second, the largest impact on decreasing the RMSE comes from the PSO, while although the steepest descent achieves a further reduction of the RMSE, it finally obtains smaller reduction rate. The use of the steepest descent leads to a better local minimum; a fact that agrees with the main characteristic of gradient descent approaches according to which, if they start with good initial conditions they are able to detect effective local minima.The next simulation concerned the study of the behavior of the distortion function Q during the fuzzy c-means implementation, which is important for the minimization of the JUB. Fig. 6illustrates the results of this simulation for different number of hidden nodes. Notice as the network moves on to a larger number of clusters the values of Q become smaller, a fact that agrees with the analysis reported in the previous section.Tables 3 and 4provide the comparison between the proposed algorithm and other methods existing in the literature, where the advantages of the proposed method can be easily verified.In this section, three data sets taken from UC Irvine Machine Learning Repository1http://archive.ics.uci.edu/ml/.1are considered. The first one is the Auto MPG, the second one is the Computer Hardware and the last one is the Concrete Compressive Strength data set. Table 5reports their characteristics and descriptions.The results of the cross-validation analysis are given in Table 6.To study the influence of the PSO and compare with other networks, each one of the three data sets was randomly separated into a training (60% of the original data) and a testing set (40% of the original data). Fig. 7depicts the convergent behavior of the PSO in terms of the fitness function JUBfor different number of hidden nodes. Again, the smooth dynamic evolution of the learning process is observed.The effect of the PSO implementation on the network’s RMSE for the training data is given in Figs. 8–10, where similar conclusions to the previous examples are extracted. Notice, in Fig. 9, although the steepest descent appears to obtain large reduction rate of the RMSE than in the rest data sets, this rate is kept much smaller than the one achieved by the PSO. This fact directly implies that the PSO performed remarkably well, obtaining RMSE values that reach quite close to the resulting local minimum. Thus, the initial conditions provided by the PSO to the steepest descent were very effective.The dynamic behaviors of the distortion function Q for the three data sets are depicted in Fig. 11.Next, the proposed method is compared to other algorithms that exist in the literature. This comparison is demonstrated in Tables 7–9. In addition, Table 9 includes one more simulation that concerns the implementation of a back propagation Multilayer Perceptron (MLP) neural network based on Levenberg–Marquardt optimization. One hidden layer used for the processing of the input data. The hyperbolic tangent sigmoid function transferred the output signal from the input to the hidden layer while a linear function was used for the activation of the output layer. The input data were used unprocessed and the training was controlled by the maximum number of the failed validation cased and the maximum number of epochs.This example considers a real life problem called distillate maturation process. Freshly distilled spirits are stored in oak casks for several years to improve their aroma and taste, a process known as distillate maturation process. The exact mechanism of this process has not been understood. There are two main reasons behind that [20]. First, there are many factors affecting the maturation result. Second, there is not any reliable chemical index to indicate the progress of the process, a fact that imposes difficulties in estimating the formation of the final blend. Traditionally, the above problems are resolved by the expert’s organoleptic testing of samples from barrels. Herein, the proposed algorithm is used to predict the quality of the aroma of wine distillate spirits. The system variables are defined as follows:x1={Barrel Usage}: the number of refills for each barrel (input variable),x2={Barrel Age}: the usage period of each barrel in years (input variable),x3={Distillate Age}: the period of each distillate contained in a specific barrel in years (input variable),y={Aroma} is the aroma of the matured distillate (output variable).The data set consists of 120 input–output data pairs, which can be found in [12,20]. The experimental comparison outcomes of the 10-fold cross-validation analysis are given in Table 10.As far as the second experiment is concerned (i.e. the random 60–40% split of the data) the convergent capabilities of the PSO are given in Fig. 12, while the behaviors of theJUB∗and RMSE are illustrated in Fig. 13for different numbers of hidden nodes. The effect of the PSO on the clustering process is depicted in Fig. 14, where similar observations to the previously studied examples can be extracted.Finally, the proposed network is compared with the one developed in [12], in terms of the normalized RMSE (NRMSE),(39)NRMSE=∑k=1n‖yk-y⌢k‖2∑k=1n‖yk-y¯‖2wherey¯=∑k=1nyknis the mean of the output values. Table 11depicts the comparative analysis. Notice that the best results come in the case of the testing data, while the algorithm exhibits highly accurate predictions.

@&#CONCLUSIONS@&#
A new methodology has been proposed for the efficient training of RBF neural networks. The method incorporates different types of information to obtain a network with improved performance. The novel features of the network resulting from this study are based on the development of a class of upper bounds of the network’s square error that involves the exact effect of the input space fuzzy partition. To overcome any difficulties related to the non-linear nature of the upper bound function, the particle swarm optimization was employed to effectively minimize it with respect to the network’s parameters. After the convergence of the particle swarm optimization, one more step is utilized to fine-tune the widths and connection weights and get a better local minimum. The resulting learning scheme manages to substantially improve the accuracy of the network. The experimental analysis indicated that the particle swarm optimization obtained a smooth convergent behavior, and strongly affected the minimization of the network’s square error.