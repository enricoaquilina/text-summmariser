@&#MAIN-TITLE@&#
A review on algorithms for maximum clique problems

@&#HIGHLIGHTS@&#
We provide an updated review of exact and heuristic algorithms for maximum clique problems.We classify the search strategies and put forward the key ingredients of the reviewed methods.This is aimed to encourage more research on new solution algorithms.This is aimed to motivate more applications of using clique approaches.

@&#KEYPHRASES@&#
Maximum clique problems,Exact algorithms,Heuristics,Applications,

@&#ABSTRACT@&#
The maximum clique problem (MCP) is to determine in a graph a clique (i.e., a complete subgraph) of maximum cardinality. The MCP is notable for its capability of modeling other combinatorial problems and real-world applications. As one of the most studied NP-hard problems, many algorithms are available in the literature and new methods are continually being proposed. Given that the two existing surveys on the MCP date back to 1994 and 1999 respectively, one primary goal of this paper is to provide an updated and comprehensive review on both exact and heuristic MCP algorithms, with a special focus on recent developments. To be informative, we identify the general framework followed by these algorithms and pinpoint the key ingredients that make them successful. By classifying the main search strategies and putting forward the critical elements of the most relevant clique methods, this review intends to encourage future development of more powerful methods and motivate new applications of the clique approaches.

@&#INTRODUCTION@&#
The maximum clique problem (MCP) is to find a complete subgraph of maximum cardinality in a general graph. Its decision version is among the first 21 NP-complete problems presented in Karp’s seminal paper on computational complexity (Karp, 1972). The MCP is among the most studied combinatorial problems.The MCP has a wide range of practical applications in numerous fields. Early applications can be found for instance in Ballard and Brown (1982); Barahona et al. (1992) and Christofides (1975) and are surveyed in Bomze et al. (1999) and Pardalos and Xue (1994). Nowadays, more and more practical applications of clique problems arise in a number of domains including bioinformatics and chemoinformatics (Malod-Dognin et al., 2010; Ravetti and Moscato, 2008), coding theory (Etzion and Östergård, 1998), economics (Boginski et al., 2006), examination planning (Carter et al., 1996; Carter and Johnson, 2001), financial networks (Boginski et al., 2006), location (Brotcorne et al., 2002), scheduling (Dorndorf et al., 2008; Weide et al., 2010), signal transmission analysis (Chen et al., 2010), social network analysis (Balasundaram et al., 2011; Pattillo et al., 2012), wireless networks and telecommunications (Balasundaram and Butenko, 2006; Jain et al., 2005). In addition to these applications, the MCP is tightly related to some important combinatorial optimization problems such as clique partitioning (Wang et al., 2006), graph clustering (Schaeffer, 2007), graph vertex coloring (Chams, Hertz, & Werra, 1987; Wu and Hao, 2012a), max-min diversity (Croce et al., 2009), optimal winner determination (Shoham et al., 2006; Wu and Hao, 2015), set packing (Wu et al., 2012) and sum coloring (Wu and Hao, 2012b). These problems can either be directly formulated as a maximum clique problem or have a sub-problem which requires to find a maximum clique.Given its theoretical importance and practical relevance, considerable effort has been devoted to the development of various solution methods for the MCP. On the one hand, effective exact methods have been designed mainly based on the general branch-and-bound (B&B) framework. These methods have the theoretical advantage of guaranteeing the optimality of the solution found. However, due to the inherent computational complexity of the MCP, exact methods can require a prohibitive computing time in the general case and are often applicable only to problems of limited sizes. On the other hand, to handle problems whose optimal solutions cannot be reached within a reasonable time, various heuristic and metaheuristic algorithms have been devised with the purpose of providing sub-optimal solutions as good as possible to large problems within an acceptable time. It is clear that exact and heuristic methods constitute two complementary solution approaches which can be applied to face different situations and fulfill different objectives. These two approaches can even be combined to create more powerful search methods.Since the Second DIMACS Implementation Challenge dedicated to Maximum Clique, Graph Coloring, and Satisfiability organized during 1992–1993 (Johnson and Trick, 1996), studies on these NP-hard problems are becoming more and more intense. In particular, significant progresses have been achieved regarding the MCP, its important generalizations (e.g., maximum vertex weight clique and maximum edge weight clique) and relaxations (e.g., quasi-clique and densest k-subgraph). Advances on new algorithms have helped to find improved results to benchmark problems and deliver effective solutions to new applications (social network analysis, protein structure alignment, wireless network etc.).At the same time, we observe that the two most influential surveys on the MCP date back to 1994 and 1999 respectively (Bomze et al., 1999; Pardalos and Xue, 1994). To the best of our knowledge, there is no updated review to report the newest advances achieved during the past 15 years. This paper thus aims to fill this gap by providing a detailed review of different solution approaches proposed in the recent literature for maximum clique problems. We will not only make a general and large survey of the most representative exact and heuristic algorithms, but also carry out an in-depth analysis of the studied methods to identify their most relevant ingredients that make these methods successful.Let G = (V, E) be an undirected graph with vertex set V = {1, …, n} and edge set E⊆V × V. A clique C of G is a subset of V such that every two vertices in C are adjacent, i.e., ∀u, v ∈ C, {u, v} ∈ E. A clique is maximal if it is not contained in any other clique, a clique is maximum if its cardinality is the largest among all the cliques of the graph. The maximum clique problem (MCP) is to find a maximum clique of a given graph in the general case. The clique number ω(G) of G is the number of vertices in a maximum clique in G.The maximum clique problem is strictly equivalent to two other well-known combinatorial optimization problems: the maximum independent set problem (MIS) and the minimum vertex cover problem (MVC). Given G = (V, E), an independent set (also called a stable set) I of G is a subset of V such that every two vertices in I are not connected by an edge, i.e., ∀u, v ∈ I, {u, v}∉E. The MIS is to determine an independent set of maximum cardinality. A vertex cover V′ of G is a subset of V, such that every edge {i, j} ∈ E has at least one endpoint in V′. The MVC is to find a vertex cover of minimum cardinality.LetG¯=(V,E¯)be the complementary graph of G such that{i,j}∈E¯if {i, j}∉E. One observes that C is a maximum clique of G if and only if C is a maximum independent set ofG¯,and if and only if V∖C is a minimum vertex cover ofG¯. An illustration of the relation between maximum clique, maximum independent set and minimum vertex cover is given in Fig. 1. Due to the close connection between the MCP and MIS, we will operate with both problems while describing the properties and algorithms for the MCP. Clearly a result which holds for the MCP in G will also be true for the MIS inG¯. This paper focuses thus on the MCP (and the MIS) while putting aside the MVC which itself has a large body of studies in the literature.On the other hand, one notices that some generalizations and relaxations of the MCP are attracting increasing attention recently due to their wide applications in some emerging areas like bioinformatics and social network analysis. We will discuss these cases in Section 5. The rest of this section is dedicated to the issue of problem formulations and complexity of the MCP.There are numerous studies on the formulation of the MCP. These studies are of great interest since they can lead to deep understandings of the problem and the discovery of new results of theoretical and practical nature. For a comprehensive review of the existing formulations of the MCP, the reader is refereed to Bomze et al. (1999); Butenko (2003) and Pardalos and Xue (1994). Below, we review some typical and recently developed formulations.The simplest formulation is given by the following binary program:(1)maximize∑i=1nxi(2)subjecttoxi+xj≤1,∀{i,j}∈E¯(3)xi∈{0,1},i=1,⋯,n.In this edge formulation, any feasible solution defines a clique C in G as follows: vertex i is in the clique if xi= 1 and otherwise xi= 0. The linear relaxation of this formulation is significant as well. If a variable xi= 1 holds for an optimal solution to the linear relaxation of the above formulation, then xi= 1 holds for at least one optimal solution to the integer formulation (Nemhauser and Trotter, 1975). Clearly this result can be used by an algorithm to reduce the explored search space when seeking an optimal clique.Let S denote the set of all maximal independent sets in G, an alternative formulation based on independent sets imposes that any clique of G can contain no more than a single vertex from any maximal independent set of G:(4)maximize∑i=1nxi(5)subjectto∑i∈sxi≤1,∀s∈S(6)xi∈{0,1},i=1,⋯,n.This formulation has the advantage that the expected gap between the optimal solution and its linear relaxation is small. However, it is difficult to enumerate all independent sets in an arbitrary graph. Furthermore, as the number of independent sets in the graph grows exponentially with the graph size, it is not trivial to solve the relaxation of the independent set formulation.The above edge formulation can be modified to model the maximum weight independent set problem (Shor, 1990), leading to a quadratically constrained problem:(7)maximize∑i=1nwixi(8)subjecttoxixj=0,∀{i,j}∈E(9)xi2−xi=0,i=1,⋯,n.This formulation combined with dual quadratic estimates generates very good computational results (Shor, 1990).A remarkable connection exists between the MCP and a certain standard quadratic programming problem (Motzkin and Straus, 1965). For a given set of vertices S⊆V, let xSbe the characteristic vector of S, that is a vector that satisfiesxiS=1|S|if vertex i ∈ S andxiS=0otherwise, for i ∈ V. Let Δ be the standard simplex in the n-dimensional Euclidean space ℜn:Δ={x∈ℜn:∑i=1nxi=1,xi≥0,i=1,…,n}.Let AG= (aij)i, j ∈ Vbe the adjacency matrix of the graph G, and for x ∈ ℜn, let g(x) = xTAGx. Then the global optimal solution x* to maxx ∈ Δg(x) is related to the clique number ω(G) by the following formula:(10)ω(G)=11−g(x*)≥11−g(x),∀x∈ΔAdditionally, S is a maximum clique of G if and only if its characteristic vector xSis a global maximizer of g on Δ.More recently, discretized formulations for the MCP are proposed in Martins (2011), leading to tight upper bounds on many benchmark graphs based on linear relaxations. These formulations use an additional set of auxiliary variables:(11)wq={1ifthecliquesizeisequaltoq,0otherwise,∀q∈Qwith Q = {qmin, …, qmax} (1 ≤ qmin, ω(G) ≤ qmax) being an interval containing all cliques’ sizes. Then the MCP is formulated as follows:(12)maximize∑i=1nxi(13)subjectto∑j∈N(i)xj≥(q−1)xi−(q−2)(1−wq),∀i∈V,∀q∈Q(14)∑i∈Vxi=∑q∈Qqwq(15)∑q∈Qwq=1(16)xi∈{0,1},∀i∈V(17)wq∈{0,1},∀q∈QN(i) in constraint (13) denotes the set of vertices adjacent to i in the graph. Constraints (15) and (17) impose that only a single clique size is determined. Constrains (13), (14) and (16) force that each of the q vertices belonging to the clique must have at least q − 1 neighbors in the clique. Constraint (13) can be further strengthened in an extended and discretized variable space, leading to an extended and discretized formulation to the MCP. This formulation requires the auxiliary variables {wq} and a new set of node variables that include an extra index with information on clique size:(18)xiq={1ifvertexiisinacliqueofsizeq,0otherwise,∀i∈V,∀q∈QUsing these variables, the following extended and discretized formulation for the MCP can be given:(19)maximize∑i∈V∑q∈Qxiq(20)subjectto∑q∈Qwq=1(21)∑j∈N(i)xjq≥(q−1)xiq,∀i∈V,q∈Q(22)∑i∈Vxiq=qwq,∀q∈Q(23)wq∈{0,1},∀q∈Q(24)xiq∈{0,1},∀i∈V,∀q∈QObviously, the number of variables and constraints of these formulations depends on the range of variation of an interval containing the clique number (ω) of the graph. When a short interval containing ω(G) is known and for sparse graphs, the LP relaxation of these discretized formulations are able to produce much stronger upper bounds for ω(G) than other formulations.The MCP is well studied from the complexity viewpoint, early complexity results on the problem being reviewed in Bomze et al. (1999) and Pardalos and Xue (1994). During the last 15 years, a number of advances on the research of the computational complexity of the MCP appeared, including inapproximability results, parametric complexity completeness, refined complexity analysis of exponential algorithms. Although a full review of these theoretical results is clearly beyond the scope of the paper, we provide a brief summary of some main results.The current best-known polynomial-time approximation algorithm achieves only an approximation guarantee of O(n(log log n)2/(log n)3) (Feige, 2004). On the other hand, the study of Engebretsen and Holmerin (2003) shows that the MCP is not approximable within a factor ofn/2O(logn/loglogn)under the assumption thatNP⊈ZPTIME(2O(logn(loglogn)3/2)). The work of Khot (2001) further demonstrates that the MCP cannot be approximated within a factor ofn/2(logn)1−γ′for some small constant γ′ > 0, assumingNP⊈ZPTIME(2(logn)O(1)). In Håstad (1999), it is shown that the MCP is not approximable within n1 − ε for any ε > 0, unless NP = ZPP. An improved result shows that the MCP is not approximable within n1 − ε for any ε > 0 unless NP = P (Zuckerman, 2006). Another study (Bourgeoisa et al., 2011) confirms that if there exists an exact exponential time algorithm for the related MIS with a worst-case complexity O*(γn) (γ < 2) where O*( · ) is O( · ) ignoring polynomial factors and n is the order of the graph, then for any ρ ∈ (0, 1], there exists a ρ-approximation algorithm for the MIS that runs in time O*(γρn). Recently, it is established that unless NP⊆SUBEXP, for every 0 < δ < 1, there exists a constant F(δ) > 0 such that the MCP has no Fixed-Parameter Tractable (FPT) optimum approximation with ratioρ(OPT)=OPT1−δin2OPTF·poly(|V|)time (Chitnis et al., 2013). This hardness result in the domain of parameterized complexity is further enhanced very recently in Chalermsook et al. (2013), confirming that, for any r larger than some constant ε > 0, any r-approximation algorithm for the MIS must run in at least2n1−ϵ/r1+ϵtime under the exponential time hypothesis.This section is dedicated to an in-depth review of the most recent and influential exact methods for the MCP. Unsurprisingly, most of them are based on the general B&B framework. They differ from each other mainly by (1) their specific techniques to determine the lower and upper bounds and (2) their branching strategies. Below, we begin with the general B&B scheme which is common to many studied methods and then review the improvements introduced by the most representative methods. Comprehensive overviews covering early exact methods prior to 1999 can be found in Bomze et al. (1999) and Pardalos and Xue (1994).An early and well-known exact algorithm (denoted by CP) is developed by Carraghan and Pardalos (1990) which is shown in Algorithm 1. Despite its simplicity, this algorithm constitutes an important step for exact solving of the MCP since it provides the basis for many later improved exact clique algorithms. For this reason, we discuss in detail the functioning of this algorithm and identify the key elements which impact critically its performance.The CP algorithm uses two key vertex sets: C (called current solution or clique) and P (called candidate vertex set or candidate set). C designates the clique under construction while P is a subset of V\C such that v ∈ P if and only if ∀u ∈ C, {u, v} ∈ E. In other words, each vertex of P must be connected to all the vertices of C (the reverse does not necessarily hold). Let N(v) be the set of the vertices adjacent to vertex v, then P can equivalently be defined by P = ∩v ∈ CN(v). Given the property of P, it is clear that any vertex v of P can be added to C to obtain a larger clique C′ = C∪{v}. This property constitutes one of the key foundations of Algorithm 1.The algorithm operates by calling recursively the function Clique(C, P) (starting with an empty clique C = ∅ and P = V, see lines 2 and 3, Algorithm 1) and uses a global variable C* to maintain the largest clique discovered so far (|C*| is thus the current best lower bound of the maximum clique). The function Clique(C, P) is mainly defined by two key components: the bounding (line 10) and branching (line 11) procedures. Precisely, given the current clique C and its corresponding P set, one observes that |C| + |P| naturally defines an upper bound for the maximum clique. As a consequence, if |C| + |P| ≤ |C*|, C cannot lead to a clique larger than C* and thus can be safely closed. Otherwise, the subtree rooted at the clique C needs to be further explored. In this case, a branching strategy is employed to determine the next vertex v ∈ P to be selected to expand the current clique C (line 11). In Carraghan and Pardalos (1990), this is achieved by selecting the first vertex v ∈ P to insert into the current clique. Initially, the vertices v1, v2, ..., vnof the original graph G are ordered in such a way that v1 has the smallest degree in V, v2 has the smallest degree in V\{v1} and so on. After each branching step, P is updated by the union P = P∩N(v) (line 14, Algorithm 1), making the required property of the set P always verified.Since the introduction of the CP algorithm, many refinements have been devised to improve its performance with a focus on two key issues. The first one is to tighten the upper bound of the maximum clique (other than to use the size of P) during the search for the purpose of more efficient subtree pruning. The second one is to improve the branching rule in order to choose the most promising vertex of P to expand the clique.Around these two issues, four typical methods can be found in the literature.(1)The first one uses an iterative deepening strategy, and tries to obtain improved upper bounds on the size of the maximum clique of P with the aid of information obtained in some previously computed smaller subgraphs (Section 3.2).The second one is based on vertex coloring, which serves both as a bounding strategy to approximate the upper bound, as well as a branching strategy to guide the choice of the vertex from P(Section 3.3).The third one is to directly reduce the set P by removing certain vertices which can no way extend the current clique to a maximum clique or by using special rules to identify vertices in P which are part of a maximum clique that contains the current clique C(Section 3.4).The fourth one combines vertex coloring and the most recent MaxSAT technology to obtain tighter upper bounds (Section 3.5).In what follows, we review representative works dealing with these issues.Östergård (2002) proposes an iterative deepening strategy which uses a basic idea similar to dynamic programming to improve the upper bound of the above CP algorithm. Let Si= {vi, vi + 1, …, vn} be a subgraph of the initial graph G including the vertices from vithrough vn. The CP algorithm seeks a maximum clique by first considering cliques in S1 that contains {v1, v2, …, vn} then cliques in S2 that contains {v2, …, vn} and so on. In Östergård’s algorithm (called Cliquer), this ordering is reversed: Cliquer starts on the smallest subgraph Sncontaining only vertex vnand determines a maximum clique of this graph (which is trivial). Then it considers Sn − 1 composed of two vertices {vn − 1, vn} and determines a maximum clique. As such, Cliquer iteratively finds a maximum clique for subgraphs Sn − 2, …, S1 and ends up with a maximum clique in the last subgraph S1 which is the original graph to be solved. Instead of using the size of P to bound the maximum clique, a new strategy is introduced in Cliquer using information from the previously computed maximum cliques of smaller graphs.Let c(i) (i = n, n − 1, …, k) be the size of the maximum clique of the subgraph Si, which is previously computed for each subgraph Sn, Sn − 1, …, Sk. Then we can use these c(i) values to define a new pruning strategy when searching on the subgraph Sk − 1. In fact, to obtain a clique greater than |C*| from the current clique C (and its associate P), one can safely prune the search if |C| + c(i) ≤ |C*| where i = min{j: vj∈ P}, since P is a subset of Siand c(i) is an upper bound to the maximum clique of the subgraph induced by P. With this technique, Cliquer is able to boost its performance significantly. Indeed, comparisons with several other approaches before Cliquer on random and DIMACS instances show that Cliquer is superior in many cases to the reference approaches. Cliquer, however, has not proved for the moment as successful as some sophisticated vertex coloring based exact algorithms like MCQ (Tomita and Seki, 2003) and BB-MaxClique (Segundo et al., 2011).To estimate the upper bound of the maximum clique, graph coloring techniques are frequently applied to the subgraph induced by the candidate set P. This is based on the general fact that if a graph can be colored with k colors, then the maximum clique in this graph is smaller or equal to k. As such, a smaller k for the subgraph induced by P corresponds to a better upper bound of the maximum clique (see Section 3.1). Since the number of color classes is a much more precise estimation than the number of vertices in P, using a coloring instead of |P| improves generally the upper bound and consequently reduces the size of the search tree. However, since coloring itself is NP-hard, finding k-coloring with k close to the chromatic number may be extremely difficult and time consuming. It is therefore important to find a proper trade-off between computing time and bound quality. In addition, vertex coloring can be also served in defining branching rules to guide the choice of the vertices from the candidate vertex set P during the search process.Basically, two different ways of using vertex coloring for branching and bounding are explored in the literature. The first one colors the initial graph only once before the B&B routine starts and uses this coloring throughout the search. This strategy has the main advantage of running the coloring algorithm only once. However, since the clique algorithm manipulates many and different subgraphs of the initial graph G, the coloring for G is not necessarily appropriate for bound estimation of these reduced subgraphs. The second strategy applies repeatedly a coloring algorithm to different subgraphs at different nodes of the search tree. This strategy makes it possible to obtain tighter bounds of the maximum clique of the subgraphs. However, coloring multiple graphs may be also time consuming. For the purpose of saving computing time, fast greedy coloring heuristics are usually employed.An early classic B&B algorithm (denoted by BT) which employs a heuristic coloring for finding maximum cliques can be find in Babel and Tinhofer (1990). This algorithm employs two strategies for bounding and pruning. The first strategy, which uses the number of vertices in P, is done exactly in the same way as the algorithm of Carraghan and Pardalos. In the second pruning strategy, the algorithm employs a coloring algorithm based on the greedy DSATUR procedure (Brélaz, 1979) to find a better upper bound. In addition, DSATUR also provides a heuristic maximum clique of P which is then used to update the lower bound of the original graph. At each node of the search tree, the algorithm first computes a coloring of the candidate vertex set P, then it chooses a vertex according to the lexicographical order and adds it into the current clique C.The MCQ algorithm (Tomita and Seki, 2003) goes one step further, based on the idea that the coloring of the subgraph induced by the candidate set P not only provides a bound on the size of the maximum clique, but also serves as a branching strategy. The main ingredient of MCQ is to use a numbering and sorting procedure to sort the vertices of P in an ascending order with respect to the color numbers at each branching step. The numbering and sorting procedure first employs a greedy coloring algorithm to color the vertices of P in a predetermined order as follows. For each coloring step, a vertex v ∈ P is inserted into the first possible color class, so that v is non-adjacent to any vertex already in this color class. If such a color class does not exist, a new color class is opened to insert v. After all vertices in P are assigned to their respective color classes, these vertices are sorted in an ascending order with respect to their color numbers, and then copied back to P according to this sorting order. Then, at each search step of the algorithm, MCQ selects a vertex v ∈ P in reverse order (the last vertex in the reordered set P belongs to the highest color class) and the color number associated with each vertex becomes an upper bound for the maximum clique in the remaining subgraph to be searched.There are two noteworthy differences in the use of coloring in the MCQ algorithm with respect to an early algorithm also using coloring (Babel and Tinhofer, 1990). First, instead of computing a coloring at each node of the search tree, the MCQ algorithm looks for a heuristic coloring only for a new branch. Second, at each step, a vertex in the candidate set is selected from the final color class of the coloring of the candidate set. With this strategy, the number of colors of the coloring for the candidate set tends to be reduced more quickly than if a vertex is chosen according to the lexicographical order.MCR (Tomita and Kameda, 2007) improves MCQ with a better initial sorting of vertices of the input graph G which is similar to that of the CP algorithm, but uses the same coloring process to compute the upper bound as in MCQ. In MCR, the vertices of G is ordered into a list L = (v1, …, vn) where vnis the vertex of minimum degree in G, vn − 1 is the vertex of minimum degree in G\{vn}, vn − 2 is the vertex of minimum degree in G\{vn − 1, vn}, and so on. Furthermore, the authors of MCR carried out additional computational experiments to confirm the usefulness of the initial sorting of vertices by comparing the performance of MCR with MCQ.The numbering and sorting procedure of MCQ is further improved in the MaxCliqueDyn algorithm (Konc and Janežič, 2007). This is based on the observation that a better upper bound is achieved, when the vertices in P are presented to the greedy coloring procedure in a non-increasing order of their degrees (Carraghan and Pardalos, 1990). The authors of MaxCliqueDyn realize that it is necessary to reorder only those vertices in the candidate set P with color numbers large enough to be added to the current clique C in a direct descendant order with respect to the color numbers. Any vertex v ∈ P with a color number below a threshold Kmin < |C*| − |C| + 1 will never be added to the current clique and is kept in the same order as it was presented to the coloring algorithm. In addition, MaxCliqueDyn dynamically recomputes the degree of vertices at some nodes near the root of the search tree, and re-orders the vertices in the non-increasing order with respect to their degrees before coloring these vertices. This coloring strategy makes MaxCliqueDyn faster than MCQ, especially for dense graphs.In Segundo et al. (2011), an exact bit-parallel algorithm (BB-MaxClique) for the MCP is proposed. BB-MaxClique uses an improved approximate coloring procedure which relies on a new implicit global degree branching rule to obtain tighter upper bounds to the candidate set P. The basic idea of this branching rule is to keep the vertices in the candidate set P in the same order as they are presented initially to the BB-MaxClique procedure (conveniently sorted by non-increasing degree) at each level of recursion so that the vertices in the candidate set P can be presented to the coloring procedure in a roughly non-increasing order with respect to their degrees. Moreover, BB-MaxClique makes full use of bit strings to efficiently compute basic operations during the search. Their experiments confirm that the implicit global degree branching rule prunes the search tree better than the two reference algorithms MCQ and MaxCliqueDyn for a wide range of DIMACS graph, notably for dense graphs.The greedy coloring procedure in the MCQ algorithm (Tomita and Seki, 2003) is further improved by MCS (Tomita et al., 2010), which uses a recoloring strategy to improve the coloring obtained by the greedy coloring procedure. The basic idea of MCS is that if a vertex v ∈ P with color number kv> kmin (kmin = |C*| − |C|) is moved to a color class with color index l ≤ kmin, then the number of vertices to be searched in the candidate set P can be reduced since all vertices with a color number below kmin will be pruned in the derived child subproblem. It is possible to reassign a vertex v belonging to a color class Ck(k > kmin) to a different color class Cj(j ≤ kmin) if the following two conditions hold:(1)There exists a vertex w ∈ Cjsuch thatN(v)⋂Cj={w},i.e., w is the only member of the neighbor set of v in Cj.There exists a color class Clsuch thatN(w)⋂Cl=⌀and l ≤ kmin, i.e., Cldoes not contain any neighbor of w.In this case, v is moved from its current color class to another color class Cjwith j ≤ kmin, the subproblem hanging from v will be pruned. Very recently, this recoloring strategy is integrated in a bit string framework, leading to an improved bit parallel exact algorithm (Segundo et al., 2013).B&B algorithms like MCQ, MCR, MaxCliqueDyn and BB-MaxClique need to compute a coloring at each branching step. Since finding a coloring is time consuming, frequently calculating colorings could affect the performance of these algorithms. Another class of B&B algorithms which use coloring as an aid for bounding and branching apply only once a coloring algorithm to the initial graph before the B&B routine starts and then use the obtained coloring on the permanent base during the search. A typical algorithm of this kind (denoted by DK) is shown in Kumlander (2005).The DK algorithm integrates a pruning rule similar to MCQ and a backtracking search technique which examines the graph in the opposite order of a standard B&B algorithm. The first step of the algorithm is to obtain a coloring c = {C1, C2, …, Ck} of the vertices of G and re-order the vertices by color classes, so that color classes will appear in the decreasing order with respect to the color index, i.e., V = {Ck, Ck − 1, …, C1}. One first considers all cliques that could be built using only the vertices of the first color class C1. Then one considers all cliques that could be built using vertices of C1 and C2, i.e., of the first and second color classes, and so forth. The ith step considers all cliques that contain vertices of {Ci, Ci − 1, …, C1}, the last considered subgraph being the original graph. The algorithm also uses a special array b to remember the maximum clique found for each level of the subgraphs during the backtracking search. So b[i] is the maximum clique for a subgraph formed by the vertices of {Ci, Ci − 1, …, C1}. The branching rule used by this algorithm is similar to MCQ, it always selects a vertex v ∈ P with the largest color number to insert into the current clique. So when one considers v to become the (j + 1)th vertex in the current clique and v belongs to the kth color class, the algorithm can prune the search if j + b[k] ≤ |C*| (C* is the largest clique found so far), since P is a subset of Ck∪Ck − 1∪⋅⋅⋅∪C1 and b[k] is an upper bound to the maximum clique of the subgraph induced by Ck∪Ck − 1∪⋅⋅⋅∪C1. Computational experiments on the DIMACS benchmarks show that this algorithm outperforms the CP and Cliquer algorithms.Another method to improve the basic CP algorithm is to directly tighten the candidate set P by removing certain vertices which cannot be used to extend the current clique C to a maximum clique or by fixing vertices in P which are part of a maximum clique of G that contains the current clique C. Following this line, three B&B algorithms, denoted by DF, χ and χ + DF, are studied in Fahle (2002). The basic idea of DF is to use two cost based domain filtering algorithms to “clean-up the candidate set” before each branching step. Given the current clique C, its corresponding candidate set P and the largest clique discovered so far C*, let NP(v) = {u ∈ P: {u, v} ∈ E}, the DF algorithm is based on the following filtering properties:•Lemma 1. For each v ∈ P such that |C| + |NP(v)| < |C*|, v cannot extend C to a maximum clique of G.Lemma 2. For each v ∈ P such that |NP(v)| = |P| − 1 is contained in any maximum clique of G that also contains C.Lemma 1 allows the algorithm to safely remove from P every vertex of degree less than |C*| − |C|, since such a vertex cannot be part of a clique larger than |C*| in G. With Lemma 2, one can displace every vertex of degree |P| − 1 from the candidate set P to the current clique C to increase the clique size.In the χ algorithm, to approximate the chromatic number of the candidate set P = ∩v ∈ CN(v), one computes four heuristic colorings of P and retains the coloring with the smallest number of colors for bounding. These four colorings are obtained using two different greedy coloring algorithms. Algorithm χ + DF combines the domain filtering strategy of DF and the coloring-based bounding strategy: for each vertex v ∈ P, compute with a coloring algorithm an upper bound of the chromatic number for the subgraph induced by all the neighbors of v in P. If this upper bound plus the size of the current clique is smaller than the lower bound of the graph, then v can be safely removed from P. The computational experiments reported in the paper show that the combined use of these two strategies outperforms the CP algorithm as well as approaches that only apply either of these techniques. Furthermore, χ + DF is shown to be able to solve to optimality 45 out of the 66 tested DIMACS instances.One obvious drawback of the filtering algorithm in χ + DF is the time required to compute the upper bound of the chromatic number of the subgraph induced by a vertex and its neighbors in the candidate set P. In Régin (2003), a constraint programming approach (CPR) is used to determine an upper bound based on a fast matching algorithm rather than a coloring algorithm. This new upper bound roughly corresponds to the number of independent sets of cardinality 2 in the subgraph induced by P. This bound is then used to remove some vertices from P after each branching step in the following way: select a vertex v and compute the upper bound using the matching algorithm for the subgraph induced by all the neighbors of v; if this upper bound plus the size of the current clique is smaller than the lower bound of the graph, then v can be safely removed from P. In addition to the filtering algorithm, CPR also uses the diving technique in conjunction with a MIP approach to solve subproblems and a specific strategy to select the vertex to expand the current clique. CPR is able to solve seven DIMACS instances to optimality for the first time.Exact methods based on vertex coloring use the number of colors required by the subgraph to approximate the upper bound of the maximum clique on the subgraph induced by the candidate set P. This approach is further improved by using methods developed for MaxSAT (Li and Quan, 2010). The resulting method (denoted by MaxCLQ) is based on the following proposition: If G can be partitioned into k independent sets (thus can be colored with k colors), and there are s disjoint inconsistent subsets of soft clauses in the independent set using MaxSAT encoding, then the maximum clique in the graph is smaller or equal to k − s. To obtain a tighter bound of the maximum clique on the subgraph G′ induced by P, MaxCLQ first partitions G′ into k independent sets in a similar way to MCQ (Tomita and Seki, 2003). Based on this partition, one first encodes the graph into a MaxSAT instance where each independent set is encoded into a soft clause, and then uses an dedicated approach to detect the number (denoted by s) of disjoint inconsistent soft clauses in the MaxSAT instance. Finally k − s is used as an upper bound to the maximum clique of the subgraph G′ induced by P. This approach leads generally to tighter bounds than the pure coloring based approaches. Experimental evaluations on the DIMACS benchmarks show the B&B algorithm integrating this MaxSAT based bounding technique is very effective and can even close one open problem (p_hat1000-3).Later, a complex approach combining MaxCLQ (Li and Quan, 2010) and MCS (Tomita et al., 2010) with the ILS algorithm (Andrade et al., 2012) is studied in Maslov et al. (2014). The ILS algorithm is used to obtain an initial solution to the maximum clique problem, which is then served as a lower bound to prune branches in the main B&B algorithm. The study shows that running ILS before MaxCLQ and MCS algorithms considerably reduces the total computing time for some hard DIMACS instances.In addition to the above algorithms, there are some other related studies. A branch and cut (B&C) algorithm for the equivalent MIS can be found in Rebennack et al. (2011). This algorithm is based on the edge-projection theory first introduced in Mannino and Sassano (1996) and further explored in Rossi and Smriglio (2001). An overview of various characteristics of effective B&C algorithms for the MCP and MIS is given in Rebennack et al. (2012). Some parallel and multi-threading exact algorithms appear very recently in the literature. For instance, a scalable and fault-tolerant solution for the MCP based on the MapReduce framework is presented in Xiang et al. (2013). The main idea of this approach is to first partition the graph into smaller subgraphs, which are then independently solved to optimality with a B&B method on different nodes of a computing cluster. Another recent multi-threading parallel algorithm adapts a B&B algorithm similar to MCQ and explores the multi-core parallelism with a bit-set encoding mechanism (McCreesh and Prosser, 2013). This algorithm achieves excellent performances on many DIMACS instances and some BHOSLIB instances.To conclude this section on exact algorithms, Table 1summarizes in chronological order the reviewed algorithms. One observes that very few exact algorithms report their results on the set of 40 BHOSLIB instances with the exception of the study reported in McCreesh and Prosser (2013). In Section 6, we provide more information about the relative performance of these exact algorithms.In addition to exact algorithms, notable progresses on heuristic algorithms for the MCP have been made in the recent years. In this section we review the most representative and effective MCP heuristics.The majority of greedy heuristics for the MCP are called “sequential greedy heuristics”. They generate a maximal clique by starting with an empty clique and then iteratively adding vertices using greedy rules until no further additions are possible, or by repeatedly removing vertices from a set of vertices which do not form a clique. A comprehensive survey of early greedy heuristics are provided in Bomze et al. (1999); Butenko (2003) and Pardalos and Xue (1994).For these early greedy heuristics, decisions on which vertex to be added in or moved out are usually based on some static information associated with the vertices in the candidate set like their degrees. However, static greedy algorithms can easily fall into the usual greedy traps due to their short-sighted nature. Several improvements to the static greedy heuristics have been proposed in the literature. For instance, three adaptive restart randomized greedy heuristics (denoted by AI, AW and NA) are studied in Jagota and Sanchis (2001). The main ingredient of these heuristics is a probabilistic greedy vertex selection rule combined with a multi-start strategy. During each run of the greedy algorithm, the probability distributions used by the selection rule are updated through a learning-like mechanism. QUALEX-MS (Busygin, 2006) is another deterministic iterated greedy algorithm. In QUALEX-MS, each vertex is assigned a weight which represents its importance towards inclusion in the evolving clique. Vertex weights are calculated on the basis of the coordinates of stationary points of nonlinear programs derived from the Motzkin–Straus nonlinear formulation of the MCP. The deep adaptive greedy search (DAGS) by Grosso et al. (2004) integrates swap moves and vertex weights into a two-phase greedy construction procedure. Two important vertex selection strategies are introduced in DAGS to avoid usual greedy traps. First, DAGS uses swap moves to transform the current partial clique into an equal-sized but more promising clique instead of using only the short-sighted add move operator. Second, DAGS adaptively adjusts the vertex weights used for vertex selection by a restart mechanism. This dynamic weighting technique guides the search towards less explored areas. Computational results show that DAGS is superior to QUALEX-MS for most of the tested DIMACS instances.Local search is the most successful framework for designing effective MCP heuristics. We provide here an in-depth analysis of the most influential heuristic algorithms mainly developed during the two past decades.When designing a local search algorithm for solving a particular problem, one has to define the search space to be explored, the evaluation function and the neighborhood function. This triplet forms a search strategy. We propose to classify the search strategies for the MCP into two categories.•The k-fixed penalty strategy: A target clique size k is provided and the goal is to find a feasible clique of size k (a k-clique). The search space Ω is usually composed of all vertex subsets of size k (called k-subsets) including both feasible and infeasible cliques. To approximate the maximum clique, one searches for legal k-cliques with increasing k values.The legal strategy: The search space Ω contains all legal cliques and the goal is to find a clique c ∈ Ω whose size is as large as possible.These two general strategies can be further classified according to the move operators used to explore the search space. For the k-fixed penalty strategy, existing local search algorithms mainly rely on either a constrained swap operator or a unconstrained swap operator which basically exchanges one (or more) vertex of the current clique with another vertex (or more vertices) out of the clique. As to the legal strategy, the algorithms typically employ three different move operators (add, swap and drop). Fig. 2summarizes this classification which is also used in this review to guide our presentation of local search algorithms.For the k-fixed penalty strategy, as noted in Friden et al. (1989), the maximum clique problem can be approximated by finding a series of k-cliques for increasing values of k (a k-clique is a clique of size k). Each time a k-clique is found, k is incremented by one and a new (larger) k-clique is sought. This process is repeated until no k-clique can be found. The last k-clique constitutes an approximation of the maximum clique of the graph. Consequently, the maximum clique problem comes down to the problem of finding k-cliques.In the k-fixed penalty strategy, a solution is represented as a subset C of k vertices. The evaluation function f(C) counts the number of edges induced by C, and the goal of the search algorithm is to maximize function f(C) such that f(C) reaches its largest value f(C) = k × (k − 1)/2, which means any two vertices of C are connected by an edge and C is a legal k-clique.To obtain a neighboring solution from the current solution C, one typically swaps a vertex in C with another vertex in V∖C. Basically, there are two different techniques to apply the swap operation in the literature. The first technique considers all the possible moves induced by C and V∖C; any vertex from C and any vertex from V∖C can take part in a swap operation. The second technique constrains the consideration of the two exchanged vertices to some specific subsets of C and V∖C. This technique is useful to reduce the computing time needed to explore the neighborhood.As early as 1989, the first tabu search application to the equivalent MIS (called STABULUS) is presented in Friden et al. (1989) using the k-fixed penalty strategy. STABULUS tries to minimize the number of edges contained in the current subset of k vertices to zero (notice that here we are solving the MIS in the complementary graph, see Section 2). The neighborhood employed by STABULUS is defined by the unconstrained swap move operator, which swaps a vertex u ∈ C with another vertex v ∈ V∖C. At each iteration, STABULUS selects the best swap move which decreases the most the evaluation function to generate the next neighboring solution. To determine whether a move is eligible, STABULUS first uses one tabu list T1 containing the |T1| last solutions. In addition, STABULUS uses two other lists T2 and T3 containing the last vertices which were removed from (or introduced into) the independent set for the purpose of preselection. This algorithm reported remarkable results at the time it was published. STABULUS was later improved in Fleurent and Ferland (1996b) by introducing a different tabu list and tabu tenure management mechanism. Instead of putting the visited solutions in the tabu list, each time a vertex u ∈ C is swapped with another vertex v ∈ V∖C, these two exchanged vertices are added into the tabu list and are forbidden to move for the next Tuand Tviterations, where Tu= 0.3 × |V∖C| and Tv= 0.3 × |C|.Recently, an adaptive multi-start tabu search algorithm (AMTS) is introduced in Wu and Hao (2011) which is based on the k-fixed penalty strategy and the constrained swap operator. The constrained neighborhood limits the swap move to two critical subsets X⊆C and Y⊆V∖C such that |X| and |Y| are as small as possible, and the resulting neighborhood contains always the best solutions of the unconstrained neighborhood induced by C and V∖C. The critical subset X⊆C identifies the vertices of the current solution C that are not in the tabu list and have the highest number of adjacent vertices in C, and set Y identifies the vertices of V∖C that are not in the tabu list and have the smallest number of adjacent vertices in C. A neighboring solution C′ is then obtained by swapping one vertex u ∈ X and one vertex v ∈ Y. The constrained neighborhood is both more focused and smaller-sized compared to the unconstrained swap neighborhood and largely improves the computational efficiency of the search procedure. This algorithm competes well with reference heuristics like RLS (Battiti and Protasi, 2001) and DLS (Pullan and Hoos, 2006).The algorithms presented in Andrade et al. (2012); Battiti and Protasi (2001); Jin and Hao (2015); Katayama et al. (2005); Pullan (2006); Pullan and Hoos (2006); Pullan et al. (2011) and Wu et al. (2012) follow the legal strategy. Here the search space Ω is defined by the set of all legal cliques. The function to be maximized is the clique size f(C) = |C|, and the neighborhood N(C) is given by subsets of legal cliques which can be reached by applying some eligible move operators. From the literature, we can identify three such operators: add, drop and swap. The add operator displaces to C a vertex in V∖C that is adjacent to all vertices in C. The drop operator deletes a vertex from C. The standard swap operator exchanges a vertex v in C with a vertex u in V∖C which must be connected to all vertices in C except v. Note that a swap move can be trivially decomposed into two separate moves, i.e., a drop move followed by an add move (Battiti and Mascia, 2010). Most local search methods using the legal strategy jointly rely on two or three of these move operators. For instance, methods that employ the add and drop moves include the tabu search algorithms (Gendreau et al., 1993), the RLS algorithm (Battiti and Protasi, 2001) and the KLS algorithm (Katayama et al., 2005) while examples of using all three moves (add, drop and swap) are the VNS algorithm (Hansen et al., 2004), the family of PLS (Pullan, 2006), DLS (Pullan and Hoos, 2006), CLS (Pullan et al., 2011) algorithms, the MN/TS algorithm (Wu et al., 2012), the BLS algorithm (Benlic and Hao, 2013) and the SBTS algorithm (Jin and Hao, 2015). Notice that in some rare cases, more general swap(a, b) could be also useful allowing to exchange a and b vertices between C and V∖C.For the algorithms using only add and drop moves, add moves are always preferred since they increase the clique size, drop moves are applied only when no add move exists. Associated with the add moves is the vertex set PA (which corresponds to the candidate set P of Section 3.1), which is composed of the vertices that are excluded from the clique C and connected to all the vertices of C: PA = {v: v ∈ V\C, {v, u} ∈ E, ∀u ∈ C}. Algorithms which rely on the add and drop moves differ from each other chiefly in the scheme of vertex selection and the prohibition mechanism applied to the performed moves.For instance, two early tabu search variants (denoted by DT and PT) using add and move are studied in Gendreau et al. (1993). The first variant is deterministic: when add moves are possible, one vertex v ∈ PA with the maximum adjacent vertices in PA is selected to join the current clique C. When no add move exists, one vertex v ∈ C that results in the largest PA is removed from C. The general prohibition rule adopted by the deterministic variant is as follows: Only vertices that leave the clique C are forbidden to move back to C during the prohibition period, vertices that join the clique C can leave C without restriction. The second variant (PT) is probabilistic: if add moves are possible, a random vertex in PA is selected for a possible addition, otherwise, the current solution C is a local optimum and additions are impossible, a number of randomly selected vertices in C are removed from it.Reactive Local Search (RLS) by Battiti and Protasi (2001) is a landmark algorithm to the MCP which applies the add and drop operations. Starting from an empty clique, RLS explores the search space of cliques by adding/removing one vertex to/from the current clique. RLS applies add moves whenever this is possible and selects a vertex with the highest number of adjacent vertices in PA to add to the clique C. If no allowed addition exists, RLS searches for an allowed vertex to drop from C such that its removal leads to the largest set of vertex additions to C. If no allowed moves are available, a random vertex is picked from C and is dropped. As soon as a vertex is moved, it is put into the tabu list and remains prohibited for the next T iterations. The prohibition period T is related to the amount of desired diversification, and is determined by feedback information from the search history. RLS also uses a dynamic restart strategy to provide additional long-term diversification, assuring that each vertex is eventually tried as part of the current clique. Computational results on DIMACS instances show that RLS performs better than its predecessors, both in terms of efficiency and quality.The k-opt local search (KLS) of Katayama et al. (2005) also relies on the add and drop operations. KLS is generalized from the basic 1-opt local search by moving multiple vertices at each iteration instead of a single vertex for the purpose of obtaining a larger clique. At each iteration of KLS, a variable number of k vertices are added to or removed from the current clique simultaneously by applying a sequence of add and drop move operations. The vertex selection rule of KLS is similar to that of the deterministic variant of the tabu search algorithm of Gendreau et al. (1993). Each time a vertex is added to (or removed from) C, it is forbidden to remove from (or add back to) C during this round of k-opt neighborhood search. A restart strategy is also employed by the KLS algorithm: The largest clique obtained in the previous execution becomes a new initial solution for the next round of k-opt neighborhood search. The reported results on 37 DIMACS benchmarks show that KLS remains competitive compared to RLS (Battiti and Protasi, 2001).For the algorithms relying on the add, swap and drop moves, add moves are always applied whenever this is possible as they are the only moves that augment the current clique. Drop moves are considered only when no add or swap move exists. Associated with swap moves is the vertex set OM, which is composed of the vertices that are excluded from the clique C and connected to all but one vertex of C:OM={v:v∈V∖C,|N(v)⋂C|=|C|−1}where N(v) = {j: j ∈ V, {j, v} ∈ E} is the set of vertices adjacent to v.The variable neighborhood search algorithm (VNS) of Hansen et al. (2004) uses a distance metric between two solutions T and T′ which is the difference in cardinality of T and T′. Then the neighborhood Nk(T) consists of all solutions at distance k from T. Three types of moves (add, swap and drop) are considered for the selected neighborhoods Nk, k = 1, …, kmax. An initial solution is obtained by a variable neighborhood descent (VND) heuristic, which is also used as the local search procedure of the VNS approach. VNS uses add moves in its descent phase and swap moves in a plateau phase. Upon reaching a local optimum with respect to both the add and swap neighborhoods, a new solution is randomly generated from the kth neighborhood by dropping k vertices from the current clique. Different selection rules are used by VND to guide the choice of the vertex to be added to the clique C. The results on a subset of the DIMACS instances show a globally good performance.The family of stochastic local search algorithms (dynamic local search (DLS) (Pullan and Hoos, 2006), phased local search (PLS) (Pullan, 2006) and cooperating local search (CLS) (Pullan et al., 2011)) employ the add and swap moves in their main search procedure and the drop moves for overcoming search stagnation. These algorithms alternate between a clique expansion phase and a plateau search phase. The expansion phase seeks to expand the clique by adding a vertex from PA. When the current clique cannot be extended, a plateau search phase is triggered during which vertices of the current clique are swapped with vertices in OM. If the current clique becomes expandable, the search switches back to the expansion phase. When no add and swap moves are possible, some perturbation strategies are applied to the clique C. DLS (Pullan and Hoos, 2006) follows this scheme and uses dynamic vertex penalties to guide vertex selection during the clique expansion and plateau search phases. The perturbation mechanism simply reduces the clique C to the last added vertex v, or adds a vertex v that is chosen at random and then removes all vertices from C that are not connected to v. To cope with graphs of different structures, PLS (Pullan, 2006) combines three sub-algorithms which use different vertex selection rules: random selection, random selection among those with the highest vertex degree or random selection within those with the lowest vertex penalty. In addition, the perturbation mechanism differs between the sub-algorithms. CLS (Pullan et al., 2011) further improves over PLS by following the idea of the hyper-heuristic scheme (Burke et al., 2003). CLS incorporates four low level heuristics which are effective for different instance types. The difference between these low level heuristics is their vertex selection techniques and the way they deal with plateaus. To further enhance the performance of CLS, relevant information is passed between low level heuristics in order to guide the search to particular areas of the search domain. DLS, PLS and in particular CLS show excellent performances on both DIMACS and BOSHLIB benchmarks.The study of Andrade et al. (2012) explores general swap operator in the context of the legal strategy: The (1, 2)-swap move, which exchanges a single vertex from C against two other vertices, and the (2, 3)-swap move, which removes two vertices from C and adds three new vertices. This study also shows that, given any maximal clique, a (1, 2)-swap move can be implemented in a linear time, while a (2, 3)-swap move can be implemented in O(mΔ), where m is the number of edges and Δ is the highest degree in the graph. These swap operators are integrated in an iterated local search (ILS) method. The results on the DIMACS and BHOSLIB benchmarks show that ILS performs very well on two large and difficult MANN instances. However, it is dominated by some best MCP algorithms on some large brock family instances from DIMACS as well as some BHOSLIB instances.The multi-neighborhood tabu search (MN/TS) (Wu et al., 2012) for the weighted and unweighted MCP is based on a combined neighborhood induced by the add, swap and drop moves. For the unweighted case, the vertex selection rule is based on a random scheme: For the add move, a vertex v ∈ PA is randomly picked to expand C, and for the swap move, a vertex v ∈ OM is randomly selected to exchange with the only vertex u ∈ C which is not connected to v, while for the drop move, a vertex is randomly removed from C. The general prohibition rule in MN/TS is similar to that of the deterministic tabu search procedure of Gendreau et al. (1993): Only vertices that leave the clique C are forbidden to move back to C during the prohibition period, vertices that join the clique can be removed without restriction. MN/TS obtains the best-known solutions for all the BHOSLIB instances and 78 out of the 80 DIMACS instances.The breakout local search (BLS) (Benlic and Hao, 2013) for the MCP and its vertex weight version uses the union of the add and swap moves in its search procedure and the drop and add-repair moves during its perturbation procedure. BLS explores first search areas around the current local optimum and displaces the search into a new distant area only if the search seems to be stagnating. This is achieved by its specific perturbation strategy which determines dynamically and adaptively the type and strength of perturbations. BLS reaches competitive results on both DIMACS and BHOSLIB instances.Very recently, a general swap-based tabu search (SBTS) algorithm (Jin and Hao, 2015) is introduced for the equivalent MIS. Each iteration of SBTS corresponds to either an intensification step or a diversification step by applying a general (k, 1)-swap (k ≥ 0) operator. Given an independent set S, (k, 1)-swap exchanges one vertex (which is strategically selected) in V\S against its k adjacent vertices in S. SBTS alternates dynamically between its (k, 1)-swap based neighborhoods to find improved solutions or escape from local optima. SBTS is currently the single heuristic able to attain the best-known result for all DIMACS and BHOSLIB instances.Evolutionary algorithms like genetic algorithms have long been a popular approach for the MCP. As opposed to local search, this approach uses a population of solutions that are improved via an “evolutionary” process. Early attempts are reported in 1990s. However, pure genetic algorithms are not effective for the MCP (Carter and Park, 1993). This approach is often enhanced by incorporating other techniques like local optimization. A first example is the genetic tabu search algorithm of Fleurent and Ferland (1996a) which combines a standard uniform crossover and a tabu search procedure. Another hybrid genetic algorithm (GENE) mixes a uniform crossover, a swap mutation and a descent method (Marchiori, 2002). A hybrid evolutionary algorithm with a guided mutation (EA/G) for the problem is introduced in Zhang et al. (2005). The HSSGA algorithm by Singh and Gupta (2006a) is a combination of a steady-state genetic algorithm, a randomized sequential greedy approach and the exact algorithm of Carraghan and Pardalos (1990) while IEA-PTS blends an impatient evolutionary algorithm and a probabilistic tabu search (Guturu and Dantu, 2008).Despite numerous attempts, the reported evolutionary algorithms for the MCP are not competitive compared to simpler local search algorithms. This may be partially explained by the fact that for the MCP, no meaningful recombination (or crossover) operator is known yet. Indeed it is not clear how random crossover operators can help the search to discover improved cliques. Even the most effective hybrid evolutionary algorithms for the MCP are basically driven by some local search procedures. However, better performances could be achieved if a semantically meaningful crossover for the MCP can be devised.Other popular heuristics for the MCP include ant colony optimization (Solnon and Fenet, 2006), artificial neural networks (Yang et al., 2009) and methods based on continuous optimization (Burer et al., 2002; Busygin, 2006). According to the reported results, the overall performance of these heuristics are not competitive compared with recent local search methods. Finally, it is worthwhile to mention that some recent edge weighting local search approaches for the equivalent minimum vertex cover problem, such as COVER (Richter et al., 2007) and EWCC (Cai et al., 2011), report very good results on the DIMACS and BHOSLIB benchmarks.To conclude this section, we summarize in Table 2the main reviewed heuristic algorithms (listed in chronological order). In Section 6, we provide more information about the relative performance of these heuristic algorithms.The MCP has some relevant generalizations and relaxations which are useful to formulate a number of practical applications where the standard MCP is not appropriate. These problems are not only gaining increasing popularity in a number of practical applications in particular in graph data analysis and mining, but also keeps garnering attention as a promising avenue for theoretical investigations.Given G = (V, E), let w: V → Z+ be a weighting function that assigns to each vertex i ∈ V a positive value. For a clique C of G, define its weight as W(C) = ∑i ∈ Cwi. The MVWCP is to determine a clique C* of maximum weight, i.e., ∀C ∈ Ω, W(C*) ≥ W(C) where Ω is the set of all possible cliques of the graph. The MCP can be considered as a special case of the MVWCP where the weight of each vertex is set equal to 1. It is clear that a maximum clique of the MCP does not necessarily lead to a maximum vertex weight clique of the MVWCP, and the MVWCP has at least the same computational complexity as the MCP.Some exact algorithms for the MVWCP come from and generalize previous methods designed for the unweighted MCP (Kumlander, 2004; Östergård, 1999). Three other B&B algorithms are introduced in Warren and Hicks (2006), which use weighted clique covers to generate upper bounds and branching rules. Several heuristics are also available to find sub-optimal solutions for the MVWCP: Augmentation algorithm (Mannino and Stefanutti, 1999), distributed computational network algorithm (Bomze et al., 2000), complementary pivoting algorithm (Massaro et al., 2001), hybrid evolutionary approach (Singh and Gupta, 2006b), adaptation to the weight version of the phased local search (Pullan, 2008) for the unweighted case, multi-neighborhood tabu search algorithm (Wu et al., 2012) and breakout local search (Benlic and Hao, 2013).Given an integer m and a complete graph G = (V, E), each edge {i, j} ∈ E being associated with a weight dij. The MEWCP is to find a clique C in G such that the sum of the weights of the edges in C is maximized and the number of vertices in C is no more than m. Like the MVWCP, the decision version of the MEWCP is NP-complete (Ausiello et al., 1999), as far as it reduces to the maximum clique problem. The MEWCP or its variants are also studied under other names: Maximum diversity (Martí et al., 2013; Palubeckis, 2007), maxisum dispersion (Kuby, 1987), MAX-AVG dispersion (Ravi et al., 1994), remote-clique (Chandra and Halldórsson, 2001), and maximum edge-weighted subgraph (Macambira, 2003).Early exact methods for the MEWCP can be found in Dijkhuizen and Faigle (1993) and Park et al. (1996). Three more recent examples of B&B algorithms are provided in Aringhieri et al. (2009); Martí et al. (2010) and Sørensen (2004) which can solve problems with up to 150 vertices. On the other hand, there are many heuristics devoted to the MEWCP: Tabu search (Aringhieri and Cordone, 2011; Macambira, 2003), iterated tabu search (Palubeckis, 2007), iterated greedy algorithm (Lozano et al., 2011), greedy randomized adaptive search procedure (Andrade et al., 2005; Silva et al., 2007), genetic algorithm (Feng et al., 2010), variable neighborhood search (Brimberg et al., 2009), scatter search (Gallego et al., 2009; Gortázar et al., 2010), path-relinking method (Andrade et al., 2005) and memetic search (Wang et al., 2014; Wu and Hao, 2013). Comprehensive surveys and comparisons of the most significant heuristic and metaheuristic methods for the MEWCP before 2011 can be found in Aringhieri and Cordone (2011) and Martí et al. (2013).Given a graph G = (V, E) and a real number γ ∈ [0, 1], a γ-quasi-clique (γ-clique for short) is defined as a subset of vertices S⊆V such that the number of edges in the subgraph induced by S is at leastγ(2|S|). Then the maximum γ-clique problem is to find a γ-clique of maximum cardinality in the given graph (Definition 1, see references like Bourgeois et al. 2012). An alternate definition of a γ-quasi-clique can also be found in the literature like Pei et al. (2005) and Zeng et al. (2007) where S⊆V is called a γ-quasi-clique if every vertex in S has at least γ(|S| − 1) adjacent vertices in S. Obviously, this definition is more restrictive than Definition 1 since S⊆V satisfying the minimum degree requirement is also a γ-quasi-clique according to Definition 1. However, the converse is not necessarily true as a γ-quasi-clique by Definition 1 could include vertices which have less than γ(|S| − 1) adjacent vertices in S. A more general definition is provided in Brunato et al. (2008) where S⊆V is called a (λ, γ)-quasi-clique if the number of edges in the subgraph induced by S is at leastγ(2|S|)and every vertex in S has at least λ(|S| − 1) adjacent vertices in S. The maximum γ-clique problem is shown to be NP-complete in Patillo et al. (2013) for any fixed γ satisfying 0 < γ < 1.Mathematical programming formulations, effective pruning techniques and exact methods are also proposed in the literature to solve the problem exactly. Mixed-integer programming formulations for the maximum γ-clique problem are described in Patillo et al. (2013) and report preliminary results obtained with a modern commercial MIP solver. A B&B algorithm for the problem is recently developed in Pajouh et al. (2014). There are several heuristics available to find sub-optimal solutions in various application scenarios. A heuristic algorithm (Matsuda et al., 1999) is developed to find large γ-cliques for classifying a large and mixed set of uncharacterized biological sequences. GRASP procedures are studied in Abello et al. (2002) for detecting large quasi-cliques in graphs representing telecommunications data. Two existing local search algorithms for the MCP are extended to the quasi-clique problem (Brunato et al., 2008). A greedy algorithm for a quasi-clique finding problem is proposed within the context of studying the human protein–protein interaction networks (Bhattacharyya and Bandyopadhyay, 2009).Given a graph G = (V, E) and an integer k ≤ |V|, the densest k-subgraph problem is to find a subgraph of k vertices with the largest number of edges in the subgraph. Its edge weight version (called the heaviest k-subgraph problem) is to find a k-vertices induced subgraph with the maximum total edge weight. The densest k-subgraph problem is NP-hard in the general case (Feige et al., 2001).There are some solution approaches for the problem. For instance, several integer programming formulations are studied in Billionnet (2005). A B&C algorithm to solve the problem exactly is introduced in Bourgeois et al. (2013). A variable neighborhood search algorithm is described in Brimberg et al. (2009) for an edge weight version of the problem.Finally, there are some other interesting models related to the MCP, such as the maximum k-plex problem (Balasundaram et al., 2011), the maximum k-club problem (Bourjolly et al., 2002) and the maximum/minimum edge neighborhood density clique problem (Martins, 2012).This section is devoted to performance evaluation of MCP algorithms. First of all, one must keep in mind that performance evaluation is a delicate task since many factors can affect such an assessment like the programming language and the data structures used to code the algorithm, the computing plate-form, the experimental protocol (stop condition, parameter tuning...). Moreover, evaluation criteria may also become a delicate issue since a criterion suitable for an exact algorithm may be unsuitable for a heuristic, and vice versa. Even for a given type of algorithms, several criteria would be possible. To simplify the presentation and highlight some main trends, we focus on two most popular indicators: the quality of solutions (main indicator) and the computing time (secondary indicator). In any case, the comparisons presented in this section are only for indicative purposes and should be interpreted with caution.For the purpose of computational assessment of MCP algorithms, there are two main benchmarks available in the literature. The DIMACS benchmark is historically the most popular while the BHOSLIB benchmark is more recent.The DIMACS benchmark set is created in the 1990’s for the second DIMACS challenge on Clique, Satisfiability and Graph Coloring (Johnson and Trick, 1996).11ftp://dimacs.rutgers.edu/pub/challenge/graph/">ftp://dimacs.rutgers.edu/pub/challenge/graph/.It is composed of 80 graphs and represents the standard set for evaluating MCP algorithms. The benchmark covers a number of real-world problems from e.g., coding theory, fault diagnosis problems and Keller’s conjecture, etc., in addition to randomly generated graphs and graphs where the maximum clique is hidden by incorporating low-degree vertices. The size of the DIMACS instances ranges from less than 50 vertices and 1000 edges up to more than 3300 vertices and 5,000,000 edges. According to the results reported in the literature, among these 80 DIMACS instances, the maximum clique is now known for most of them except for four graphs: three (large and dense) random graphs with at least 500 vertices (C500.9, C1000.9, C2000.9) and 1 structured graph (johnson32_2_4).The BHOSLIB benchmark set is composed of 40 graphs with hidden optimal solutions (Xu et al,, 2007),22http://www.nlsde.buaa.edu.cn/~kexu/benchmarks/graph-benchmarks.htm.which are translated from hard random SAT instances generated at the exact phase transition of the RB model (Xu et al., 2005). These instances have sizes ranging from 450 vertices and 17,794 edges up to 1534 vertices and 12,7011 edges. These instances are supposed to be hard theoretically for exact algorithms and only few exact algorithms (see for instance McCreesh and Prosser 2013) report results on these instances. On the other hand, several recent heuristic algorithms can attain the known optimal solutions for these instances with no particular difficulty (Benlic and Hao, 2013; Cai et al., 2010; 2011; Grosso et al., 2008; Jin and Hao, 2015; Pullan et al., 2011; Richter et al., 2007; Wu et al., 2012).In addition to these benchmarks, a set of instances from code theory (less used in the literature) is available at http://neilsloane.com/doc/graphs.html.To compare different exact algorithms for the MCP, we summarize in Table 3the computational results of 10 recent exact algorithms on the DIMACS benchmark. To be concise, we exclude the very easy instances that are solved by most of the reviewed algorithms in less than 2 seconds and the 10 hard instances on which few exact algorithms reports their results (C500.9, C1000.9, C2000.5, C2000.9, C4000.5, MANN_a81, hamming10-4, johnson32-2-4, keller6, and p_hat1500-3). The results of Cliquer, CPR, MaxCliqueDyn, MCR and MaxCLQ are directly extracted from Li and Quan (2010), while the results of χ + DF, MCS, BBclique and ILS&MaxCLQ are taken from the papers describing these algorithms and the results of MCQ are obtained from Konc and Janežič (2007). The running times shown in Table 3 are all adjusted and expressed in seconds of the computer used in Li and Quan (2010) to run MaxCLQ. The method for adjusting the running times is as follows. If the original paper provides machine benchmarking information on running the DIMACS MCP Machine Benchmark program (ftp://dimacs.rutgers.eduin directory /pub/dsj/clique) on the three benchmark graphs r300.5, r400.5 and r500.5, then the CPU time is transformed using the benchmark information. Otherwise, the CPU time is adjusted using the SPEC - Standard Performance Evaluation Corporation (www.spec.org). To interpret the data in Table 3, we should keep in mind that the results are often obtained under different time limits and the SPEC time conversion is not as precise as running the DIMACS Machine Benchmark program.Comparative studies of exact algorithms are available in a number of studies like Carmo and Züge (2012); Konc and Janežič (2007); Li and Quan (2010); Östergård (2002); Prosser (2012); Régin (2003); Segundo et al. (2013); 2011); Tomita and Kameda (2007); Tomita and Seki (2003) and Tomita et al. (2010). A summary of 10 most effective exact algorithms is provided in Table 3. From these results, one finds that the most effective algorithms are those that use vertex coloring for both bounding and branching like MCQ (Tomita and Seki, 2003), MCR (Tomita and Kameda, 2007), MCS (Tomita et al., 2010), MaxCliqueDyn (Konc and Janežič, 2007) and BB-MaxClique (Segundo et al., 2011), and the MaxSAT based algorithms like MaxCLQ (Li and Quan, 2010). In Carmo and Züge (2012); Li and Quan (2010) and Tomita and Seki (2003), extensive comparisons are presented between the coloring based algorithm MCQ and some other algorithms before MCQ, like Cliquer (Östergård, 2002) and χ + DF (Fahle, 2002), disclosing that MCQ is superior on a wide range of DIMACS instances. Given that χ + DF uses coloring only for bounding, the comparative results between MCQ and χ + DF also indicate that it is better to use coloring as an aid for both branching and bounding than for bounding only. MCR, MCS, MaxCliqueDyn and BB-MaxClique can be viewed as improved versions of MCQ by embedding more effective branching rules. When comparing the MaxSAT based algorithms like MaxCLQ and the coloring based algorithms like MCR, MCS and MaxCliqueDyn, one observes that MaxCLQ solves most of the large and dense DIMACS instances faster than other coloring based algorithms while the reverse is true for some small instances (Li and Quan, 2010; Maslov et al., 2014).Similarly, Table 4summarizes the results of 21 heuristics on 9 representative DIMACS instances. These instances are selected since they are known to be hard for most existing MCP heuristic algorithms and thus can be used to differentiate the compared heuristics. The results in terms of solution quality (i.e., best and average clique size) are directly extracted from the original papers while the average CPU running times are adjusted and expressed in seconds of the computer used by EWCC (Cai et al., 2011) using the same method as for exact algorithms of Table 3. The solution quality (i.e., clique size) is the primary criterion while the computing times are provided for indicative purposes.From Table 4 as well as the results reported by the original papers, one first observes that the newest heuristic SBTS (Jin and Hao, 2015) is the only method able to attain the best-known clique size for these 9 graphs (in fact this remains true for all DIMACS and BHOSLIB instances) while the other heuristics miss at least one best-known result. Secondly, algorithms based on the vertex penalty mechanism such as DLS (Pullan and Hoos, 2006), PLS (Pullan, 2006), CLS (Pullan et al., 2011) perform very well on the brock instances, but have troubles to find an optimal (or best known) solution for the large MANN instances. On the other hand, algorithms designed for the vertex cover problem (EWLS (Cai et al., 2010), EWCC (Cai et al., 2011), COVER (Richter et al., 2007)) and two other algorithms (VNS (Hansen et al., 2004) and KLS (Katayama et al., 2005)) are able to reach the optimal (or best known) solution for the two large MANN instances, but perform less well on some large brock graphs.In light of the computational results of Table 4 as well as the analysis in Cai et al. (2011); Jin and Hao (2015); Pullan and Hoos (2006); Pullan (2006); Pullan et al. (2011); Richter et al. (2007); Wu and Hao (2011), the dominant heuristics are DLS (Pullan and Hoos, 2006), PLS (Pullan, 2006), COVER (Richter et al., 2007), CLS (Pullan et al., 2011), EWCC (Cai et al., 2011), AMTS (Wu and Hao, 2011), SBTS (Jin and Hao, 2015). As shown in Pullan and Hoos (2006), DLS dominates previous state-of-the-art heuristics KLS (Katayama et al., 2005), RLS (Battiti and Protasi, 2001) and DAGS (Grosso et al., 2004) on several DIMACS instances. In Pullan (2006), PLS is compared directly with DLS using the assessment criteria listed in Pullan and Hoos (2006), showing a comparable or better performance than DLS for almost all DIMACS instances. In Richter et al. (2007); Cai et al. (2011); Pullan et al. (2011) and Wu and Hao (2011), EWCC, CLS, COVER and AMTS are compared with DLS or PLS, showing that they are as competitive as or even more efficient than DLS or PLS. Therefore, we can roughly conclude that EWCC, SBTS, DLS, PLS, CLS, COVER and AMTS are among the most effective heuristic algorithms currently available for the MCP.Based on this review, we now take on the challenge of discussing some perspective research directions.From the perspective of exact solution of the MCP, we consider two directions for further improvement. First, as shown in Section 3, the most popular exact algorithms use a (very simple) graph coloring procedure to estimate their upper bounds. Since the quality of the coloring impacts directly the quality of the bounds, it would be interesting to investigate recent and more powerful coloring algorithms. At the same time, since the coloring procedure is repeatedly called as a subroutine in the B&B process, a compromise between the coloring quality and the computing time needs to be reached.Second, it is possible to take advantages of both exact and heuristic methods and combine these two approaches. For instance, before the branch-and-bound routine starts, we could use a highly effective local search procedure to obtain some high quality initial solutions, which could be served then as lower bounds of the main B&B algorithm. Such a combination was very recently investigated in Maslov et al. (2014) and proves to be effective. In addition to this loose combination, we can consider more subtle cooperation between these two search methods. For instance, during the B&B process, for each active node of the search tree, one can call a (fast) local search procedure which determines a best possible path from the current node to the leaf of the search tree, leading thus to a hopefully good lower bound of the maximum clique.Other ways of integrating heuristics with exact methods can also be considered. For instance, information from high quality solutions found in several runs of a heuristic can be used to define smaller problems solvable by an exact algorithm. This is motivated by the observation that many optimization problems show some type of “backbone” structure in the sense that high quality solutions share a number of solution components with optimal solutions. We could use an effective local search method to collect some high quality solutions, then fix the vertices which are shared by all these high quality solutions. By excluding the vertices which are adjacent to all these preserved vertices shared by the high quality solutions, we obtain a subproblem which may be small enough to be solved by exact algorithms. Notice that such an approach does not always guarantee the completeness of the search.As shown in Section 4.2, local search is the most popular and globally the most effective heuristic approach for the clique problems. However, if we examine the existing local search algorithms on an individual base, we observe that very few single algorithm dominates all the other algorithms. This happens because when the search operators used in an algorithm are suitable to explore the structures of some graphs, these same operators may fail to handle other graphs with quite different structures. One possible way to mitigate this deficiency would be to incorporate multiple search operators or strategies within a single algorithm and equip the algorithm with a capacity to dynamically decide the most suitable operators to trigger during the search process. Moreover, it would also be interesting to explore a portfolio approach using several algorithms.From the population-based search perspective, as discussed in Section 4.3, the most effective algorithms of this family for the MCP are memetic methods which combine the genetic search framework and local search. Unfortunately, these complex hybrid methods are often surpassed by simpler local search heuristics for the MCP. One reason is that the recombination (crossover) operators used in these algorithms are not really meaningful with respect to the clique problem. On the other hand, it is now well acknowledged that with the memetic framework, a carefully designed recombination operator, i.e., able to recombine the “building blocks” of the given problem, constitutes a key factor for the effectiveness of the algorithm (Hao, 2012). Consequently, an interesting perspective to boost the performance of population-based search is to devise dedicated recombination operators. For this purpose, it is necessary to obtain a deep understanding of the properties and structures of the solutions in order to identify meaningful building blocks. Dedicated recombination operators can then be designed which can generate new promising solutions by blending properly existing solutions. Such an recombination operator will allow the memetic algorithm to make a difference when it is combined with a powerful local search procedure.Finally, exact methods could be used to help the heuristics to visit promising regions of the search space. For instance, as the maximum clique problem can be formulated as an integer programming problem, its LP relaxation can be solved easily by any LP solver. The resulting LP optimal solution could be a sources of useful information. One possibility would be to fix some variables according to the LP optimum and exclude them from the search space examined by the heuristic method. Such a strategy helps the heuristic method to intensify its search within more focuses areas.To conclude, the maximum clique problem and its generalized and relaxed variants are generic models which find more and more applications in numerous domains. Advances in exact and heuristic methods for these clique problems will help to find satisfying solutions to many practical problems. On the other hand, studies of challenging real-world problems will encourage the development of more effective solution methods. Given the increasing interest in clique problems and their applications, it is reasonable to believe that research in these domains will become even more intense and fruitful in the forthcoming years.

@&#CONCLUSIONS@&#
