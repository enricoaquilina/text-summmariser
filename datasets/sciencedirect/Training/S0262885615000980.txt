@&#MAIN-TITLE@&#
A framework for dynamic restructuring of semantic video analysis systems based on learning attention control

@&#HIGHLIGHTS@&#
A framework is proposed to restructure an initial video analysis system dynamically.The initial structure of the system must be hierarchical containing some units.The framework imposes a learning-based dynamic feature selection method on each unit.The system learns how to direct attention to the informative units to achieve a goal.The framework is tested for goal and card event detection in broadcast soccer videos.

@&#KEYPHRASES@&#
Attention control,Broadcast soccer video,Event detection,Q-learning,Semantic video analysis,

@&#ABSTRACT@&#
Current semantic video analysis systems are usually hierarchical and consist of some levels to overcome semantic gaps between low-level features and high-level concepts. In these systems, some features, descriptors, objects or concepts are extracted in each level and therefore, total computational complexity of such systems is huge. In this paper, we present a new general framework to impose attention control on a video analysis system using Q-learning. Thus, our proposed framework restructures a given system dynamically to direct attention to the blocks extracting the most informative features/concepts and reduces computational complexity of the system. In other words, the proposed framework directs flow of processing actively using a learning attention control method. The proposed framework is evaluated for event detection in broadcast soccer videos using limited numbers of training samples. Experiments show that the proposed framework is able to learn how to direct attention to informative features/concepts and restructure the initial structure of the system dynamically to reach the final goal with less computational complexity.

@&#INTRODUCTION@&#
Nowadays, a wide variety of digital videos such as movies, news and sport videos are available on the web, cell-phones, hard disks, and non-volatile memories. On the other hand, almost all users need semi- or fully automated tools for video analysis and management in different applications such as video indexing and retrieval, video summarization and event detection. Therefore, scientists and companies are trying to develop efficient methods and tools for content and semantic video analysis. In this section, we review semantic video analysis systems developed in recent years. Methods used in these systems are divided into two main approaches: (1) methods based on content analysis; and (2) methods based on semantic analysis. The second approach is more important and investigated in this paper but first, we review the first approach briefly. Then, the methods included in the second approach are reviewed and categorized into two categories: (1) heuristic based; and (2) learning based.Many of the previous systems for video analysis are content based. In other words, they are usually based on low-level audio-visual information. For example, the systems proposed for shot boundary detection (low-level video segmentation) [1], key-frame extraction [2,3], object/scene detection [4], object tracking [5,6], content-based video compression [7], video genre classification [8], video summarization based on affective video content [9], and event detection in general videos [10] are based on low-level processing. Such systems use heuristic (rule based) or statistical (learning based) models applied on low-level features (e.g., color, texture and motion) and sometimes mid-level features. Limitations of content-based methods that only use low-level features have been in the semantic gap between low-level features and high-level concepts [11]. Thus, the main drawback of these methods stems from that fact that low-level features cannot describe high-level semantics properly.Semantic video analysis can be performed in a hierarchical structure to fill the semantic gap between low-level and high-level features. To this end, intermediate descriptors (e.g., people, objects, and trajectory of moving objects) can be used to bridge the semantic gap. The current methods for semantic video analysis are divided into two main categories. The first category contains heuristic semantic video analysis systems. Many of semantic video analysis systems fall into this category. A lot of researches such as copy detection [12], event detection in sport videos [13–17] and anchor detection in news videos [18] use this approach for video analysis. In such systems, prior knowledge about the problem imposed by an expert is the key of the solution. Although these systems are usually more accurate and faster than learning-based systems (the second category), they depend on the expert's knowledge and thus are not fully automatic. Nowadays, users usually prefer to have fully-automatic systems; therefore, such systems are not of much interest.The second category contains the methods based on learning approaches. Although learning based approaches are very useful in many fields of pattern recognition and computer vision, there are still many challenges in using learning methods in semantic video analysis systems. One of these challenges is that concepts or events in a video may occur in very wide forms according to different conditions. Therefore, it is not easy to learn the system by a limited number of training samples. On the other hand, many of the interesting high-level concepts are rare. Therefore, learning approaches have to be trained by a limited number of training samples that do not present a given concept properly in the feature space. These systems usually suffer from low accuracy when there are not enough training samples. Such systems usually use a few heuristics to overcome these challenges. Additionally, these systems are usually proposed for specific situations (e.g., for a given type of sport); otherwise, they will not be able to analyze the video efficiently. Some of the recent event detection systems for sport videos such as [19–23] fall into this category. Almost all of these methods partition the video to high-level segments called play-break or highlights. Then, they extract low and/or mid-level features from each segment. Then, according to some training samples, a fully- or semi-supervised learning mechanism construct semantic relationships between low, mid and high-level features. Finally, the resulting system is fully- or almost fully-automatic. In contrast to the above mentioned systems, the system proposed in [24] uses a knowledge adaptation method to compensate shortcomings of the training samples by some other samples partially related to the target concepts. First, this system refines (adapts) the concepts (knowledge) extracted from the auxiliary samples using the knowledge extracted from the training samples. Then, the results of event detection based on the training samples and the refined auxiliary samples are fused to obtain the final decision.Many of the previous works, especially learning-based semantic video analysis systems have high computational complexity. One of the main approaches to reduce computational complexity and achieve an efficient system is feature selection. There are two basic categories for feature selection methods [25]: filter and wrapper. Filter methods rank the features based on a predefined criterion without any learning, thus, we call them unsupervised. In the wrapper methods, the feature selection criterion is the performance of the decision maker. Therefore, the wrapper methods are considered supervised. Recently, many supervised feature selection methods like evolutionary-incremental methods [26] were presented. However, limited number of training samples limits the use of supervised feature selection methods in semantic video analysis. In order to overcome this issue, a semi-supervised feature selection method was proposed for semantic video analysis [27]. This method exploits a lot of unlabeled video samples to find discriminative features for classification of a few labeled video samples. In fact, this method is a specific feature selection method which combines filter and wrapper methods.In this paper, we propose a framework for semantic video analysis systems based on learning attention control to reduce computational complexity. Indeed, the proposed method is based on a new dynamic feature selection method. The proposed framework is applied to a hierarchical system containing some Processing Units (PU). PU's are building blocks of hierarchical semantic video analysis systems that extract low-, mid- or high-level features. Our framework imposes a local attention control on each PU and therefore, restructures the system dynamically according to the inputs and goal of the system. Then, the hierarchical structure of the system propagates local attention to the overall structure in a top-down manner. Attention control at each PU directs the decision making process to generate more efficient results with lower computational complexity. Rest of the paper is organized as follows. In Section 2, a brief review of attention control methods is presented. Details of the proposed framework are presented in Section 3. In Section 4, we apply our proposed framework to broadcast soccer videos for event detection. Experimental results are reported in Section 5. Conclusions and future works are explained in Section 6.Attention control or selective attention is a multi-disciplinary concept which is considered in computer science, neurobiology, and psychology. Frintrop et al. [28] defined selective attention as “the mechanism in the brain that determines which part of the multitude of sensory data is currently of most interest”. In other words, attention control is a process to concentrate on some parts of input data selectively and ignore others. The cocktail party effect is the most common example for attention control: among a room full of unwanted voices and sounds, a person can concentrate and understand the voice of his/her friend.One of the main applications of attention control is in computer vision which is known as visual attention control. Computational complexity of problems related to the interpretation of image and video is usually very high; thus, computer vision has become an interesting application of attention control. In computer science, the most important aspect of attention control is the computational models for attention control. Therefore, in this section, we concentrate on this aspect and review some famous and common computational models of visual attention control.There are many computational models for visual attention control [28,29]. The oldest model, called Feature Integration Theory (FIT), was proposed by Treisman and Gelade [30]. Although FIT is the basic and oldest model for visual attention, the most famous and common models are Wolfe's guided search [31] and Itti's model [32]. These models are based on low-level features (e.g., color and edges) and usually used for rapid object detection in a natural scene.Although attention control can be categorized and reviewed from multiple viewpoints [28,29], we consider two of the most relevant viewpoints in this section. The first viewpoint is based on the driving factor of attention: stimulus-driven and goal-driven [28,29]. Stimulus-driven or bottom-up attention control is derived from raw data of sensors (e.g., pixels of an image). The result of stimulus-driven attention control is detection of the regions of interest that are called salient points. On the other hand, goal-driven attention control has a top-down mechanism which is derived from a certain goal. In other words, stimulus-driven attention control is a blind search to detect salient points, but goal-driven attention control searches the image to find a certain goal using a prior knowledge.Itti et al. [32] proposed a famous stimulus-driven attention control model to build a 2D saliency map. Itti's model is a general and complete version of the FIT model. This saliency map shows the importance level of each region of the image. Thus, the saliency map guides us to a limited number of regions that may contain a specific object.Borji et al. [33] changed this model to build a goal-driven mechanism of attention for object detection. This method has a hierarchical structure in three layers. In the first layer, a biased version of Itti's model is used for salient point detection. Then, a domain specific attention control mechanism is applied. This layer is based on mid-level features, usually object-based. Finally, decision making is the last layer which is performed based on the attended information in the second layer. In the last layer, decision making is based on a specific type of decision tree called U-tree which is learned by a reinforcement learning approach.Another viewpoint of attention control is based on the space of attention [34]: perceptual space and decision space. Attention in perceptual space deals with raw data gathered from sensors or features extracted from them. In this case, attention mechanism has to select some important sensors (features) to attend. Many current attention models such as the models presented in [32,33,35] use attention mechanism in perceptual space. In high level semantic video analysis, we have a few researchers that use attention control in perceptual space. For example, in [36], a method is presented about fast highlight detection and scoring in soccer videos using on-demand feature extraction. This method uses a bottom-up perceptual attention control in feature space to speed-up the procedure of summarization.Attention in decision space deals with output of decision makers (e.g., label of classes). In other words, attention control in the decision space is similar to an ensemble (fusion) of decision makers that combines some decision makers and discards others dynamically to make a final decision.In [37], a method called Active Decision Fusion Learning (ADFL) is proposed. This method assumes that each decision maker provides an output in the form of a probability distribution. ADFL tries to direct attention in the decision space and learn an active (dynamic) sequential selection of the decision makers in order to make the final decision. In other words, in a dynamic cascade classifier ensemble, ADFL learns the best sequence of decision makers in a cascade structure according to the on-line output of the decision makers. Markov Decision Process (MDP) is used to model a sequence of decision makers. Also, a continuous reinforcement learning method is used for learning of such MDP problem.After ADFL, Mirian et al. [38] proposed a framework called Mixture-of-Experts Task and Attention Learning (METAL) for attention mechanism in a continuous decision space. This framework is similar to ADFL but handles a dynamic ensemble of decision makers when each decision maker has a partial knowledge of the task. The proposed framework learns the task and the attention mechanism simultaneously. In other words, METAL learns the task while it learns how to attend the decision makers.Usually, the previous works presented in visual attention control are used in robot vision for image interpretation and object detection. There are a few researches about application of attention control in video analysis. Almost all of these researches are about detection of affective parts of video and determination of type and intensity of their effectiveness. However, early researches such as [39,40] are based on attention control on low-level features including sound energy, color and motion intensity; recent researches such as [41,42] present hierarchical and complicated methods to apply attention control into semantic video analysis. Recent methods try to fill the semantic gap using mid-level features. In fact, hierarchical analysis lets the system to extract content (mid-level features) from low-level features and then extract a high-level concept such as effectiveness from the analyzed content.According to our best knowledge, all of the previous methods of semantic video analysis using attention control are in the perception space. In contrast, our proposed method is a top-down attention control model both in perception and decision spaces. At the first look, overall prospect of our proposed method is similar to a perception-based model when a PU wants to select the best inputs actively. On the other hand, a PU tries to select a dynamic sequence of inputs while it makes decision at each step of this sequence. Thus, it has an attention control in the decision space. In fact, selection of a dynamic sequence of inputs during performance improvement of decision making causes a hybrid attention model both in the perception and decision spaces.The proposed framework tries to impose attention control on a pre-designed hierarchical system using machine learning methods to reduce computational complexity. In other words, our proposed framework restructures a given system actively to direct attention to the PU's extracting the most informative features/concepts. This framework is introduced in a hierarchical structure that consists of some PU's in different levels. Attention control of the framework is embedded locally at each PU using a simple mechanism. In fact, the proposed framework imposes attention control mechanism on each PU and thus, each PU is able to select its inputs according to a dynamic sequence. On the other hand, the structure of the system is hierarchical and output of some PU's at lower levels is used as input of some higher PU's. Therefore, the final system works based on an overall attention control. The structure of the framework and the mechanism of attention control are presented in this section.The structure of the system, that we apply our proposed framework to, is hierarchical. Thus, the system may include multiple processing levels and there are some PU's at each level. A PU at each level may use the outputs of some or all of the PU's in the lower levels. The designer of system defines the number of levels and the PU's at each level. Additionally, the designer defines the connections between PU's. However, the designer can put a PU in the ith level if the inputs of this PU are not produced by any PU at the jth level (j≥i) and the output of this PU is not connected to any PU at the jth level (j≤i). Also, the designer can use a fully connected structure where a PU at the ith level is connected to all compatible PU's located at the lower levels (1st, 2nd,…, (i-1)th). It is important that the designer connects only compatible PU's because the output of some PU's at the lower levels may be incompatible (from viewpoint of format) with the input of some PU's at the higher levels. For example, if the output of a PU at the lower level is a matrix, it is incompatible with a PU at the higher level that uses a scalar as the input.The proposed framework may remove connections between PU's permanently or dynamically during learning of attention control but does not create new connections between PU's. Therefore, the designer has to define all possible and compatible connections between PU's at the initial state if he does not have prior knowledge about proper structure of the system. The connections defined by the designer will be used as the initial connections between PU's in the learning phase.From viewpoint of graph theory, the structure of the system in the proposed framework is a specific directed acyclic graph in which all directions are bottom-up. In other words, there is no connection from a PU at a higher level to a PU at a lower level. Because many of the current approaches for semantic video analysis systems are hierarchical (level-by-level), this constraint of our proposed framework is consistent with almost all current semantic video analysis systems. Fig. 1shows the structure of an example video analysis system that is in the form of our proposed framework. This system has 4 levels and the initial connections between PU's are shown by arrows.We define an adjacency matrix for the initial structure (connections) denoted by AMinit. AMinitdescribes the initial structure of the system completely. According to the defined constraint on the proposed structure, AMinitis always a lower (left) triangular matrix. For the structure depicted in Fig. 1, AMinitis a 9×9 matrix as shown in Table 1.In the proposed framework, we can define a cost for each PU denoted by PUC. Cost of a given PU determines the expected cost of that PU to process input and generate output. In this paper, we define the cost of a given PU (PUg) as the expected total processing time of PUgdenoted by PUTgtotal. If self-processing time of PUgis denoted by PUTg, PUTgtotalis calculated by:PUCg=PUTgtotal=PUTg+∑i|PUiiscalledbyPUgPUTitotal.PUTitotalis the total expected processing time of PUiwhere the output of PUiis used by PUgas an input. In other words, the total expected processing time of PUgis equal to self-processing time of PUgplus sum of the total expected processing time of all PU's that are called by PUg. In this case, because PUgmay call different PUiin different conditions dynamically, PUTgtotalvaries according to how many and what PU's are called by PUg. Consequently, we assume that PUTgtotal=PUTgif PUgis at the first level.It is important that PUTgis the expected self-processing time of PUgto process some inputs, make a decision and generate output. In other words, PUTgdoes not contain the processing time of providing inputs for PUg. The processing time related to providing inputs for PUgis equal to sum of PUTitotalwhile PUi's provide input for PUg.In the proposed framework, we impose a top-down attention control on a hierarchical system using machine learning approaches. Attention control controls the connections between lower PU's and higher PU's. In other words, each PU may use some PU's at lower levels and discard others dynamically.To achieve this goal, we build the final system by a bottom-up hierarchical approach. The PU's at the first level do not need learning attention control. In fact, these PU's extract low-level audio-visual features from video without any attention mechanism. Thus, at first, learning attention control is applied to all PU's at the second level. Similarly, learning attention control is applied to PU's at higher levels, level-by-level (see Algorithm 1 for details). In this algorithm, we assume that the system has lev levels and the number of PU's at level i is NPUi.Algorithm 1The proposed algorithm in a macro view to impose top-down learning attention control.Algorithm 1 builds a hierarchical structure of attention control. This algorithm uses an algorithm called DynamicFeatureSelection. DynamicFeatureSelection shows a micro view of our proposed framework that uses a learning attention control mechanism for dynamic feature selection at a PU. It gets PUgand ConSet and calculates GTDFSg. GTDFSgis the Guidance Table for Dynamic Feature Selection (GTDFS) that is usable for dynamic feature (input) selection at PUg. Hierarchical utilization of learning attention control mechanism at each PU results in a system with top-down learning attention control mechanism.We assume that a given PU named PUgis initially connected to a set of PU's denoted by ConSet={PUi} where AMinit(i,g)=1. It means that PUgmay use the output of one or some of {PUi} as input. In other words, we have a feature set {fi} where fiis the output of PUi. Additionally, we assume that fiis a number or symbol from a finite and countable set. In some cases, output of PUimay be a vector. In this case, we assume that the output vector of PUiis a unique bag of features and we still denote it by fi. Therefore, all elements of this bag of features are always considered together as a unique feature.Now, attention control mechanism has to select the best sequence of features from {fi} actively to maximize the expected value of a utility function. In ordinary feature selection methods, the order of the final selected features is not important and the selected features are the same for all situations. Despite ordinary feature selection methods that find a static set of best features, our proposed method finds the best sequence of features dynamically.In our proposed method, we select features step by step. At each step, all of the selected features are used as a bag of features for decision making. Then, according to the output of the decision maker (PU), another feature may be selected or feature selection may stop. When feature selection stops, the output of the decision maker using all selected features is the final output of PU.We use the Q-learning method to perform dynamic feature selection at every PU. Q-learning is a famous method for reinforcement learning [43]. As depicted in Fig. 2, reinforcement learning approaches are usually used for the training of an agent to take actions in either a static or dynamic environment. The agent takes an action at the current state, and then the state of the agent changes to a new state. Additionally, the agent receives a feedback called reward from the environment for the taken action. Reward is generated by a critic that we can consider it as a utility function.Reinforcement learning approaches help the agent to learn how to maximize the expected reward (utility). Q-learning is a model-free method that learns an action-value function in a Markov Decision Process (MDP) environment. The action-value function determines the expected utility of taking a given action in a given state and following a fixed policy thereafter.In our problem, the agent is a PU and the environment is the training set at a given feature space. States, actions and rewards (utility function) are defined below.For complete modeling of our dynamic feature selection problem by an MDP, we have to define states, actions and a utility function. We assume that there are n features in {fi}. Additionally, we assume that the output of PUgis an element of a finite and countable set. Therefore, we denoted that different output values/symbols of PUgby oj. {oj} is the set of different possible values/symbols that may be used as the output of PUgand we assume that it has m elements.Next, we define each state as a pair of (fi,oj). Additionally, we define another state called initial state denoted by ∅s. Initial state shows that no feature is selected yet. (fi,oj) shows that the latest selected feature at the previous state is fiand the output of the decision maker using all of the previously selected features is oj. According to the above explanations, the set of states is denoted by St where the total number of states is n×m+1.St=∅s,f1,o1,f1,o2,…,fi,oj,…fn,omThen, we define an action as selection of a feature from {fi}. At the current state (fi,oj), we select another feature fk(take an action), make a decision using fkand the previously selected features to generate olas the output. Thus, (fk,ol) is the next state. Consequently, we define another action denoted by ∅ato terminate the feature selection process. When ∅ais selected as an action, feature selection ends. According to the above explanations, we define the action set Ac with the following n+1 elements:Ac=∅a,f1,…,fi,…,fn.According to the above definition of states and actions, Q-table will be a (n×m+1)×(n+1) matrix as shown in Table 2. Q-table is the memory of the agent about preference of each action in each state.Q-learning essentially learns the first order Markov models. A first order Markov model has just a memory of size one for each state–action pair. In other words, a two dimensional matrix may save the total memory of the agent about all state–action pairs. In this case, the current state depends only on the previous state. For higher order Markov models, a higher dimensional matrix can serve as the memory of the agent.In our problem, selection of a feature at a given state does not only depend on the previous state (indeed the previous feature), but also depends on the complete set of features selected from the initial state to the current state. Although our problem to select the best set of features is not essentially a first order Markov model, we use a two dimensional matrix as the Q-table. In this case, we have to change the ordinary Q-learning method to adapt it to our problem.Our proposed definition for states and actions is the first step towards changing a non-first order Markov model to a first order Markov model, but it is not sufficient. In order to adapt Q-learning, we use an auxiliary variable called LSF (List of Selected Features). LSF helps us to remember the history of the states visited and the actions taken at the previous states.Additionally, we change the definition of the utility function to a new definition. In our proposed method, the utility function determines the expected utility of selecting a given feature in a given state, adding the selected feature to current LSF, and makes decision based on the new LSF. In other words, the utility of the current state is calculated based on the utility of decision making using the current sequence of selected features (LSF). Utility of LSF may be defined as a combination of error rate and total processing time of decision making based on LSF. For example, we may define utility (Ut) as:Ut=−ErPUg+PUTgtotalwhereErPUgand PUTgtotalare the error rate and the expected total processing time of PUgrespectively, when PUguses the features listed in LSF for decision making.According to the above problem definition, the problem is modeled by a first order Markov model with n×m+1 states, n+1 actions, an auxiliary data structure LSF, and a new utility function.According to the above explanations, our problem is not a first order MDP problem; thus, we change the ordinary Q-learning method to be adapted with our high order MDP problem. This method is similar to the ordinary Q-learning method presented in [43], but there are two main differences between the ordinary Q-learning and our proposed learning method.The first difference is in the update mechanism of Q-table. In the ordinary Q-learning method, with the assumption of the first order MDP problem, Q-table is updated at every step of episode. In this case, in every steps of episode, agent gets a reward from the critic for its action taken in the current state and it updates Q-table. But in our proposed method, Q-table is updated at the end of episode, when we get the utility (reward) of the dynamic feature selection. In this case, we update all the Q-values of state–action pairs of the episode simultaneously at the end of episode.The second difference is in the mathematical formulation of updating Q-table. In the ordinary Q-learning method, Q-table is updated in every steps of episode by [43]:Qsj,aj=Qsj,aj+αrj+γmaxkQsj+1,ak−Qsj,ajwhere rjis the reward observed after performing ajin sj, going to the next state sj+1 and following the optimal policy thereafter (selecting the best action in sj+1). α and γ, the learning rate and discount factor, are the parameters of learning algorithm.The max operator in the above equation lets us follow the optimal policy in the next state. In this case, we have a standard first order MDP problem. Thus, we can follow an optimal policy in each step of an episode, without attention to the policy of the previous state. Also, we can calculate rj(the reward of each step related to performing ajin sj) of each step separately. However, our problem is not a real first order MDP and we cannot calculate rjseparately for each step of episode. Our problem is a high order MDP that we follow an ε−greedy policy in all the states of an episode during the learning phase. Thus, we do not use the max operator and calculate the total utility (reward) of episode, from the initial state ∅sto the final state in which ∅ais selected as an action in that state. The total utility of episode is denoted by Ut and applied to the Q-values of all the state–action pairs of episode. Additionally, we use ε−greedy policy for action-value updates during an episode. In other words, after performing ajin sjand going to the next state sj+1, we are not following the optimal policy thereafter. Thus, mathematical formulation for updating Q-table in our proposed learning method is as:∀sj,ajthatvisitedintheepisode:Qsj,aj=Qsj,aj+αUt+γQsj+1,aj+1−Qsj,aj.Above explanations are about the main differences of our proposed learning algorithm with the ordinary Q-learning method. In the following, we describe details of our proposed method.Our proposed method for learning the attention control in the dynamic feature selection algorithm is shown in Algorithm 2. Like an ordinary Q-learning method, we use a (m×n+1)×(n+1) matrix as Q-table for learning. The Q-table is initialized by small random values. The Q-learning method updates the Q-table by an iterative algorithm. A learning iteration is a procedure that starts from the initial state (∅s) and ends at the state in which ∅ais selected as an action in that state. In other words, LSF is always empty at the start of the learning episode and it contains the final set of selected features when the learning episode ends. At each learning episode, some elements of the Q-table are updated simultaneously when state transitions ends.In the proposed method, Q-learning updates the Q-table iteratively using the training samples. In fact, after a state transition in the proposed combined space, the decision maker (PU) is evaluated on the training samples using the selected features (LSF). The training samples are shuffled randomly and each training sample is used in a learning episode. We can use the training samples a few times if necessary (e.g., in the case of lack of enough training samples).There are two action selection policies in Q-learning: a policy for learning and a policy for living (when learning finishes and agent is ready for test). Action selection policy during learning is ε−greedy, but when learning phase finishes, the action selection policy is greedy (selection of the best action in each state). During learning, we let the agent to explore the space and evaluate different actions (even non-best actions) at a given state by ε−greedy. But when learning finishes, we assume that the Q-table contains almost exact preference of each action at a given state; therefore, we select the best action surely by greedy policy [43].The ε−greedy method selects the best action with probability of 1−ε and selects a random action with probability of ε where 0≤ε≤1 [43]. When ε approaches 1, the selection strategy becomes fully random, and when ε approaches 0, the selection strategy is fully greedy. In this paper, we use a common value for ε, i.e., ε=0.1.When learning finishes, we save the best action of every state in the GTDFSg. In fact, GTDFSgguide us to select the best feature at each state dynamically for PUg. Finally, we remove the rows of GTDFSgthat not accessible from the initial state ∅s. It means that some rows of GTDFSgrelated to the PU's which are not called by PUgat all are removed. Thus, the final GTDFSgis a well-defined and abstract table for dynamic feature selection.Algorithm 2The proposed algorithm for learning of dynamic feature selection in a PU.We present an example to illustrate our proposed method further. Assume α=0.1, γ=0.1, and a 5×3 Q-table as shown in Table 3for PUg. Assume that in an episode, we start with ∅sand then select f1 as the first feature while the output of decision maker with f1 is o1. Then, we select f2 as the second feature while the output of decision maker with {f1,f2} is o2. In this step, we select ∅aand the procedure of feature selection and decision making is finished. Here, we assume that Ut=−0.2.According to the proposed algorithm, the highlighted elements of Q-table in Table 3 will be updated as below:row:1,column:2:0.116=0.15+0.1−0.2+0.1×0.1−0.15row:2,column:3:0.078=0.1+0.1−0.2+0.1×0.8−0.1row:5,column:1:0.71=0.8+0.1−0.2+0.1−0.8.Accordingly, the updated Q-table is shown in Table 4.In this section, we apply our proposed framework in an event detection system for broadcast soccer videos. The proposed event detection system tries to detect goal and card event using cinematic features.At first, we design the initial structure of the system. In Fig. 3, the initial structure of the system, all usable PU's and their possible connections are depicted. It seems that the initial structure of the system is almost fully connected; however, there are some non-plausible connections in the structure. In fact, we do not connect the PU's that are not consistent from the view point of input/output format. Thus, the initial structure does not need high prior knowledge about connections of PU's; our proposed method will find the best connections.In Fig. 3, simple lines show loose connections that may be eliminated after learning but double lines show tight connections. Tight connections are always connected and will not be affected by attention control learning. Tight connections are based on prior knowledge about the system. In the remaining of this section, we explain details of each PU and their connections.All PU's at the first level perform a low-level processing on video frames. These PU's extract common low-level features in soccer video analysis systems. Table 5shows brief explanations of these PU's that extract basic features. By the way, we normalize the output of all mentioned PU's to the range [0,1].Essence of some PU's including RGB, HSV, RGBH and HSVH is simple and clear, but the PU's named SHSVH, 3DRGB and EgH need more explanations.In SHSVH, first the image is segmented into 3×3 segments like the method proposed in [16], and then 16-bin histogram of H, S and V components of each segment is calculated. The final output of SHSVH is obtained using a concatenation of the histograms of all segments. Therefore, the size of the output of SHSVH is 48×9.In 3DRGB, we calculate 3D RGB histogram of the image. Despite the ordinary RGB histogram, constructed by a concatenation of the histograms of the R, G, and B channels, a 3D RGB histogram is constructed by quantizing the 3D RGB color space and counting the number of pixels in each voxel of the resulting volumetric RGB space [36]. Here, we quantize R, G, and B channels to 4 levels. Therefore, 4×4×4=64 color voxels are generated and the length of the 3D RGB histogram is 64. Computational complexity of the 3D RGB histogram is more than the ordinary RGB histogram but the 3D RGB histogram may describe color information more precisely than the RGB histogram.EgH presents a low-level texture feature of the frames using histograms of edges in different scales and orientations. To this end, we use 2D Discrete Wavelet Transform (DWT) in 4 levels. At each level, 2D DWT generates detail coefficient matrices for horizontal, vertical and diagonal edges. Therefore, we achieve wavelet coefficients in 4 scales and 3 orientations. Each coefficient matrix is binarized by the Otsu method [44] and then, the number of edges (white pixels) is counted. Finally, we concatenate the numbers of edges in different scales and orientations to achieve a feature vector with 12 elements.PU's at the second level are SBD (Shot Boundary Detection), SrD (Shirt Detection) and VTR (View Type Recognition). In this section, we explain details of algorithms employed by these PU's.Shot boundary detection is a low-level processing that partitions the video into low-level parts. There are many methods for shot boundary detection [45,46]. The main idea for shot boundary detection is based on calculating difference of features in consecutive frames. We use this idea in SBD. In other words, we get feature vectors of two consecutive frames and calculate a difference measure of the feature vectors. Then, two adaptive thresholds are applied to the calculated difference measure to detect shot boundary and classify shot transition (i.e., cut or gradual).We assume that Fiand Fi+1 are two feature vectors related to frames i and i+1, respectively. We calculate DMias the difference measure by:DMi=∑jFij−Fi+1j.Now, we decide if a transition has occurred between frames i and i+1 and if so, what the type of the shot transition is: cut or gradual. We use a thresholding method to do this:OSBD=0notransitionifDMi≤T1SBD1gradualifT1SBD≤DMi≤T2SBD2cutifT2SBD≤DMiwhere OSBDis the output of SBD. In the above equation, T1SBDand T2SBDare two adaptive thresholds which are calculated based on the training samples. In fact, shot transitions are detected by T1SBDand the detected shot transitions are classified by T2SBD. T1SBDis adaptively determined to achieve a 100% recall for shot transition detection while T2SBDis adaptively determined to achieve a high accuracy rate for shot transition classification.The proposed method for shirt detection is based on detection of the color of the players' or referee's jersey. Our proposed method is similar to the method presented in [16]. In this method, we need some meta-data imported by the user. This meta-data is about the color of the shirt of the players of teams A and B, goal keepers of teams A and B and referee. User imports five patches of shirt images of size 40×40pixels for each of the mentioned people. In this case, there are 8000 sample pixels for each color. Then, the pixels for each color are modeled in the HSV color space by two thresholds on each color axis.The input of SrD is a frame of video stream and the outputs of SrD are 5 binary images. In this PU, after color modeling, an input image (frame) in the HSV color space is investigated to detect 5 colors. Therefore, 5 output binary images depict all objects of a specific color (shirt) in the input image.For noise reduction, the closing morphological operation is applied to each binary image and small objects are removed. A small object is an object whose size is smaller than 0.001 of the size of the image. Then, the sum of the areas of the remaining objects in each binary image is calculated as the output. Therefore, the output of SrD is a vector of length 5. Finally, we normalize the output vector using the image size.Although SrD is placed at the second level, there is no attention mechanism inside this PU. SrD uses only the image in the HSV color space.Soccer video shots are usually divided into four types: (1) far view (or long-shot), (2) medium view (or medium-shot), (3) close-up, and (4) out-field [47]. In our proposed method, the view type of a frame is recognized by a k-nearest neighbor approach. Thus, the input of VTR is a frame and the output is a scalar from {1,2,3,4} that determines the view type of the input frame. At first, we extract proper features from the training samples according to the current LSF. Then, the k-nearest neighbor algorithm with k=3 is used for view type recognition. Mahalanobis distance is used as the distance measure in the k-nearest neighbor method. Mahalanobis distance is the generalized form of Euclidean distance that uses the variance of the training data along the principle components to compute the distance between two points. Mahalanobis distance is an adaptive version of Euclidean distance and is data-driven.There is only a PU at the third level called LgD (Logo Detection). In broadcast soccer video, replays provide another chance to review the important events for viewers. The replays may be utilized for event detection and summarization of broadcast soccer videos. Usually, logo detection is used to separate replays from other parts of video, because replays are usually sandwiched by two logo transition at the start and the end of the occurrence.In our proposed method, logo detection is performed by a CART (Classification and Regression Tree) [48]. At first, we extract proper features from the training samples (frames) according to the current LSF. Some extracted features (e.g., RGB histograms) are in the form of feature vectors. Therefore, we expand all feature vectors and concatenate them to create the total feature vector as the input vector. CART is used to learn logo detection based on the input feature vector. The output of LgD is 1 if logo is detected in the input frame, otherwise the output is 0. This method is very fast and efficient for logo detection.PU's at the forth level are HLD (Highlight Detection), GKD (Goal Keeper Detection), PyD (Player Detection) and RfD (Referee Detection). In fact, PU's at the forth level analyze the content of video to detect objects or people. In this section, we explain details of algorithms employed by these PU's.HLD segments a high-level video based on the detection of the replay segments. HLD is based on heuristics; thus, dynamic feature selection method is not used in it. In fact, the input of HLD is a full length video and the output is a few features about highlight segments. HLD is called iteratively for the video to find all highlights.The proposed method for highlight detection is based on replay segmentation. According to the output of LgD, all of the frames between two consecutive logo appearances are detected as replay. After replay segmentation, some shots before each replay are selected and combined with the corresponding replay to compose a highlight. The number of selected shots before a replay is related to the length of the corresponding replay. The total length of a highlight is denoted by LHLand computed as:LHL=LPlay+LReplaywhere LPlayis the total length of the selected shots (in seconds) before the replay and LReplayis the total length of the replay (in seconds). LPlayminis the minimum length of LPlaydetermined by:LPlaymin=max10,LReplay.According to the above equation, LPlayminis always equal or greater than 10s. Now, the minimum value of LPlayis computed but the final value of LPlaydepends on the view type of the first shot of this segment. The first shot of the play part must be a far view shot, because each important event in the soccer video starts with a far view shot that shows the scene of the event. According to Fig. 4, the start point of each highlight is the same as the start point of the first far view shot while the length of the play part is equal or greater than LPlaymin.The output of HLD is composed by: (1) the start point and the total length of the highlight, (2) the start point and the length of the replay, and (3) the start point of all shots in the highlight.Our goal in GKD, PyD and RfD is detection of goal keepers, players and referee in a close-up view respectively. In the proposed methods for GKD, PyD and RfD, we use a CART decision tree to achieve our goal. The input of these PU's is an image (frame) and their output is a scalar that determines the existence of a close-up of goal keepers, players, or referee.The main purpose of GKD is detection of goal keeper of teams A or B in a close-up view. Output of GKD is 0 when no goal keeper is detected in the image. When the goal keeper of teams A or B is detected, GKD generates 1 or 2 respectively as the output.In PyD, similar to GKD, we detect players of teams A and B. Therefore, the output of PyD is 0, 1 and 2 for no detection, detection of a player of teams A and detection of a player of team B, respectively.In RfD, our purpose is detection of the referee. Therefore, the output of RfD is 0 when the referee is not detected and it is 1 when the referee is detected.PU's at the fifth level are GED (Goal Event Detection) and CED (Card Event Detection). These PU's extract events from highlights based on the outputs of the lower PU's. Both of GED and CED use the same method for event detection but we do not combine them in a single PU because our proposed dynamic feature selection may select different feature sequence for each one. In the following, we explain our proposed method for goal and card event detection.In the proposed method for goal event detection, first, the video is partitioned to high-level segments by HLD: highlights and non-highlights. Therefore, the first input of GED is the output of HLD. Then, we only consider highlights for goal event detection (ignore other parts of video) and therefore, we always use the output of HLD in GED. Then, the outputs of LgD, GKD, PyD, RfD and VTR from each shot of highlights are used conditionally according to the proposed dynamic feature selection method.The proposed method for goal event detection is based on the CART decision tree. The proposed CART uses the following post-processed features obtained by the connected PU's at the lower levels:•Length of replay.Total length of shots containing close-up of goal keeper.Total length of shots containing close-up of players.Total length of shots containing close-up of referee.Total length of close-up shots before replay.Total length of out-filed shots before replay.For recognition of the view type of a shot or detection of the referee, it is not necessary to process all frames of the shot. In GED, view type recognition and referee detection are performed based on 5 frames selected from 10%, 30%, 50%, 70%, and 90% of a shot duration. After view type recognition or referee detection for the selected frames, the majority vote method is used to decide about the view type of the shot or existence of the referee.According to the above explanations, GED investigates each highlight independently to detect a goal event. The output of GED is 1 when it detects a goal event in a highlight and 0 otherwise.The proposed method for card event detection is similar to our proposed method for goal event detection. In other words, the first input of CED is the output of HLD. Then, we consider highlights to extract some high level features for card event detection. In CED, we detect card events (yellow/red) in a given highlight using a CART, but we cannot determine which player receives the card and what color the card is. Therefore, the output of CED is 1 when it detects a card event in a highlight and 0 otherwise.

@&#CONCLUSIONS@&#
