@&#MAIN-TITLE@&#
The analytic hierarchy process with stochastic judgements

@&#HIGHLIGHTS@&#
We develop an extension of the AHP for stochastic judgments.Integrates stochastic multi-criteria acceptability analysis with the AHP.Uses the scaled 1/x distribution to represent pairwise comparisons.Simulations show that judgmental consistency is robust to imprecision.Simulations show that acceptability indices are sensitive to imprecision.

@&#KEYPHRASES@&#
Decision analysis,Multicriteria,Analytic hierarchy process,Uncertainty,Simulation,

@&#ABSTRACT@&#
The analytic hierarchy process (AHP) is a widely-used method for multicriteria decision support based on the hierarchical decomposition of objectives, evaluation of preferences through pairwise comparisons, and a subsequent aggregation into global evaluations. The current paper integrates the AHP with stochastic multicriteria acceptability analysis (SMAA), an inverse-preference method, to allow the pairwise comparisons to be uncertain. A simulation experiment is used to assess how the consistency of judgements and the ability of the SMAA-AHP model to discern the best alternative deteriorates as uncertainty increases. Across a range of simulated problems results indicate that, according to conventional benchmarks, judgements are likely to remain consistent unless uncertainty is severe, but that the presence of uncertainty in almost any degree is sufficient to make the choice of best alternative unclear.

@&#INTRODUCTION@&#
The analytic hierarchy process (Saaty, 1990) is a widely-used method for multicriteria decision support based on a hierarchical decomposition of a decision problem into multiple criteria, the assessment of preferences using pairwise comparisons, and an aggregation of these pairwise preferences into an overall evaluation of the alternatives. While a number of practical and theoretical aspects of the AHP have proved controversial (see for example the discussion in Belton & Stewart (2002)), it has found widespread application and acceptance in practice (e.g. Vaidya & Kumar, 2006), to the extent that it may well be among the most-frequently applied of currently available methods for decision support.At the heart of the method is a nine-point semantic scale used by decision makers to express their preferences for one alternative over another on a particular criterion, and for how much one criterion is valued over another. It is clear that sometimes these assessments will be subject to uncertainty – meaning that the decision maker (DM) does not possess the necessary information to describe or deterministically predict the inputs required by the AHP (see Durbach & Stewart (2012) for a review of uncertainty in multicriteria decision support). Although the standard AHP method does not directly treat uncertainty or imprecision in its inputs, a number of extensions have been proposed to address this issue, using for example fuzzy set theory (Boender, de Graan, & Lootsma, 1989; Buckley, 1985; Laarhoven & Pedrycz, 1983), interval arithmetics (Salo & Hämäläinen, 1995), and various stochastic techniques (Hauser & Tadikamalla, 1996; Saaty & Vargas, 1987).This paper adds to that body of work by introducing a simulation-based method for representing imprecise or uncertain pairwise comparison information from one or more DMs through stochastic distributions, and a computational method to treat this information in the analysis. The method is a variant of stochastic multicriteria acceptability analysis (SMAA; see Lahdelma, Hokkanen, & Salminen (1998), Lahdelma & Salminen (2001), Tervonen, Hakonen, & Lahdelma (2008)), an inverse-preference methodology applied here to the case of the AHP. The resulting SMAA-AHP can be used with arbitrary independent or dependent distributions for the comparisons, and is based on Monte Carlo simulation from probability distributions appropriately defined over any uncertain pairwise comparisons and a subsequent collection of statistics summarizing the performance of each alternative. SMAA-AHP is related to other simulation-based methods, most notably Hauser and Tadikamalla (1996), but presents additional information to the DM, defines uncertainty regions differently, and uses a different distribution for the uncertain judgements. SMAA-AHP also allows more flexible representation of weight constraints and can also be used with missing preference information.The remainder of the paper is organized as follows. Section 2 reviews uncertainty modelling in the AHP. Section 3 describes the SMAA-AHP method. Section 4 demonstrates the method using a small example. Section 5 discusses the advantages and potential problems with the method, guided by the results of a simulation study. A final section concludes the paper.In the following, we consider a decision problem consisting of I alternatives, each evaluated on K criteria. Letzikbe the evaluation of alternative i in terms of criterion k, according to some suitable performance measure. In the standard AHP the DM performs pairwise comparisons at each node of the objectives hierarchy, expressing their preferences for one alternative over another on a particular criterion, or for how much one criterion is valued over another. The pairwise preferenceaijkfor alternative i over alternative j on criterion k represents the ratio between evaluationszik/zjk, expressed on a discrete scale from 1 to 9 (where 1 means equal preference and 9 denotes absolute preference). Where convenient, we drop the criterion subscript and refer simply to the pairwise evaluationaij. The same approach is used to compare the importance of criteria, in which case we refer to a pairwise preferenceaijfor criterion i over criterion j representing the ratio between trade-off weightswi/wj. In cases where pairwise comparisons can be assessed precisely, a number of ways have been proposed to aggregate these into global measures of performance (Belton & Stewart, 2002). Most commonly, the eigenvector corresponding to the largest eigenvalue of the (I×IorK×K) pairwise comparison matrixA=[aij]is extracted (the so-called priority vector), and a global evaluation formed by a simple weighted sum.Our concern is with decision making situations in which the pairwise evaluationsaijk(and consequently computed values forzikandwj) are uncertain. Early research into the modelling of probabilities in the AHP was largely concerned with deriving relationships between the distributional form of the uncertain pairwise judgements and the distributions of the marginal evaluations contained in the priority vector (Basak, 1989, 1991; Saaty & Vargas, 1987; Vargas, 1982). Subsequent probabilistic AHP models (Banuelas & Antony, 2007; Basak, 1998; Hauser & Tadikamalla, 1996; Levary & Wan, 1998; Levary & Wan, 1999) have focused on using Monte Carlo simulation to randomly generate pairwise evaluations from the distributions specified by decision makers. These approaches all follow the same basic approach, first expressed by Hauser and Tadikamalla (1996). The decision maker expresses pairwise comparisons in the usual way i.e. using the same 1–9 scale as for deterministic AHP, except that these comparisons are allowed to be random variables with associated probability distributions. Hauser and Tadikamalla generated random judgementsaij∗uniformly on the interval[aij-daij,aij-daij], with d an uncertainty factor, before transforming any values less than one usingf(aij∗)=1/(2-aij∗). Further restrictions may be placed on the types of distributions if necessary. Next sets of random pairwise judgements are generated using Monte Carlo simulation. For each set of randomly generated evaluation matrices the priority vector is computed. Repeating this process many times gives a distribution of priorities for each alternative, which can be used to rank the alternatives, in most cases using the mean of the distribution.Most authors make small embellishments around this general process. Levary and Wan (1998) incorporate scenarios into their model (see also Levary & Wan (1999)). Decision makers thus assess different (possibly stochastic) judgemental matrices for each scenario. Their simulation approach first generates a random number to specify which scenario is being used, and then generates further random numbers specifying the pairwise judgements within each scenario. Basak (1998) uses a Bayesian approach to integrate expert judgements with the decision maker’s prior probabilistic assessments. Pairwise judgements are simulated by drawing from the posterior distributions. Banuelas and Antony (2007) add a sensitivity analysis phase to investigate the influence of the probabilistic judgements on the consistency index. As mentioned above the primary distinction between existing simulation-based AHP methods and SMAA-AHP is the additional information that is presented to DMs, which can be useful in facilitating a greater understanding of the decision problem and progressing towards a final decision. We discuss this information in the presentation of the SMAA-AHP given in the following section.In SMAA-AHP, the DMs may express their comparisons on a discrete scale from 1 to 9 or use arbitrary positive values. The DMs can give their pairwise comparisons either as precise values, as in AHP, or as intervals to express imprecise or uncertain preferences. The DMs can give the lower and upper bounds of the intervals explicitly, or express them as[aij/dij,aijdij]whereaijis the geometric mean of the interval anddij⩾1is the so-called imprecision factor of their pairwise comparison. For example, the interval [0.5,8] corresponds to the pairwise comparison 2 with imprecision factor 4. The imprecision factor is a meaningful way to express uncertainty on a ratio scale, where all values should be positive.When the DMs express their pairwise comparisons, it should be checked that these are sufficiently consistent. In the original AHP, where pairwise comparisons are expressed deterministically, a popular approach for evaluating consistency is to compareλ1, the leading eigenvalue of an assessed pairwise comparison matrix, with I, the leading eigenvalue obtained from anI×Imatrix of perfectly consistent judgements (in the sense thataik=aijajk,∀i,j,k). To provide a measure of the severity of this deviation,(λ1-I)/(I-1)is compared with the mean inconsistency value derived from many randomly generated reciprocal matrices of the same size. An inconsistency ratio of 0.1 or less is generally stated to be acceptable (Saaty, 1990), meaning that the inconsistency of the observed pairwise comparisons should be no more than 10% of what would be observed, on average, from completely random judgements. For an interval-based analysis, a natural analogue would be to suggest that the geometric mean of the comparisons should have a inconsistency ratio below 10%. Note, however, that this benchmark, as well as the general use of the inconsistency ratio, has been strongly criticized (see in particular Bana e Costa & Vansnick (2008)).After each DM has given his/her pairwise comparisons, we combine them into intervals[aijmin,aijmax]whereaijminis the minimal value that any DM has expressed andaijmaxis the maximal value. We represent then the aggregated comparison values by stochastic variables with suitable probability distributions. Technically, it is possible to use arbitrary distributions. However, in the absence of information about the distribution, we apply the truncated and scaled1/xdistribution. The PDF (probability density function) of the scaled1/xdistribution is given byf(x)=α/xwhenx∈[xmin,xmax], and zero elsewhere. The scaling coefficientα=1/ln(xmax-xmin)is determined so that the integral over the PDF equals one.The motivation for using the scaled1/xdistribution to represent pairwise comparisons in an interval is that this distribution allocates equal probability mass for all sub-intervals[x/d,xd]corresponding to the same imprecision factor d. For example, given a pairwise comparison interval [0.5,8], the scaled1/xdistribution allocates equal probability mass of 1/4 for each of the subintervals[0.5,1],[1,2],[2,4], and[4,8]. If the interval is degenerate, i.e.aijmin=aijmax, we use Dirac’s delta function (the unit impulse function) as the distribution.After representing the aggregated pairwise comparisons by suitable distributions, we analyse the performance of each alternative through stochastic simulation by simultaneously drawing pairwise comparisons from their corresponding distributions and computing the score for each alternative as in AHP. Observe that even if the pairwise comparisons given by each DM are (sufficiently) consistent, comparisons drawn from the combined intervals can be inconsistent. It is easy to reject during the simulation comparisons whose inconsistency ratio exceeds some threshold. However, inconsistent comparisons can also be included in the computations, because the random inconsistencies do not introduce any systematic bias to the results.During the simulation we collect statistics about the weights at different nodes of the hierarchy, the overall score of the alternatives, and their ranking. In particular, we collect the following statistics: the number of times alternative i obtains rank r (Bir), the number of times alternative i scores better than alternative j (Cij), the sum of scores for alternative i at node t in the hierarchy (Sit), the sum of the weights for criterion k during the iterations when alternative i obtained first rank (Wik), and the number of times alternative i obtained first rank using its central weights (Pi, computation of which requires a second simulation round after the central weights have been computed). A sufficient number of simulation runs K to obtain 95% confidence limits of 0.01 around most outputs (most notably, acceptability) is approximately 10,000 (Tervonen & Lahdelma, 2007). Based on the statistics, we compute the following descriptive measures for evaluating the alternatives:•Average criterion score for different alternatives at each criterion node. This generalizes the corresponding crisp AHP criterion scores to consider imprecise comparison values. The average criterion score is computed assti=Sit/K.Average overall scorefor different alternatives. This generalizes the crisp AHP overall score to consider imprecise comparison values. The average overall score is simply the average criterion scoresitfor the root node t.The rank acceptability indexbirmeasures the variety of different preferences for which alternative i obtains rank r. The rank acceptability indices can be used for ranking the alternatives roughly, or for finding compromise alternatives in case no alternative obtains sufficient acceptability for the first rank. Potential compromise alternatives are those with high acceptability for the best ranks. Alternatives that obtain high acceptability for the worst ranks should be avoided. The rank acceptability index is computed asbir=Bir/K.The first rank acceptability indexbi1measures the variety of different preferences that make alternative i most preferred. In other words, the acceptability index measures how widely acceptable the alternative is. The acceptability index can be interpreted as the share of people voting for the alternative, assuming that the applied distribution for comparison values represents the voters’ preferences. Zero acceptability means that the alternative is inefficient, i.e. no preferences make it best.The central weight vectorwicdescribes what kinds of weights are favourable for alternative i, i.e. make it most preferred. The central weights can be presented to the DMs in order to help them understand how different weights correspond to different choices with the assumed preference model. The central weights are undefined for inefficient alternatives. The central weights are computed aswikc=Wik/Bi1.The pairwise winning indexcijis the probability for alternative i to score better than alternative j considering the uncertainty in the preference statements. The pairwise winning indices are computed ascij=Cij/K.The confidence factorpicis the probability for alternative i to be most preferred when the central weight vector for that alternative is applied. In other words, the confidence factor measures if the performance of the alternative has been assessed accurately enough, so that it can be selected under favourable preferences between criteria. The confidence factors are computed aspic=Pi/K.To illustrate the SMAA-AHP method, we reconsider the AHP example originally used in Saaty (1990). First we consider the problem with precise comparison values, then with imprecise comparisons, and finally with missing comparisons.Tables 1 and 2show the pairwise comparisons assessed between the criteria and alternatives respectively. Only the upper triangle of each reciprocal comparison matrix is presented. The criterion comparison matrix in Table 1 is a little inconsistent with consistency indexCI=0.14and inconsistency ratioIR=11%, as is the comparison matrix for criterion four (CI=0.10,IR=20%). The remaining comparison matrices haveIR<10%, the conventional benchmark (Saaty, 1990).Using the standard AHP approach, alternative A obtains the highest score (normalized to sum to 100) of 40 followed by B at 36 and C at 24. Suppose now that the inputs above are not precise but are effectively expectations (or other forms of ‘best guesses’) around which substantial uncertainty exists. For simplicity we use the same uncertainty factordij=1.5for all comparisonsaij.Output from the SMAA-AHP model is shown in Table 3. Alternative A remains the most likely candidate for the first rank, with a first rank acceptability of 86%. However alternative B, which obtainsb1=14%, is also potentially optimal. In practice the DMs could either decide that this result is conclusive enough, or determine that more accurate evaluation of the alternatives may be necessary. Similar central weights are observed for the two potentially optimal alternatives – to be expected because these are restricted by the pairwise comparisons between criteria. The confidence factors show that under favourable preferences alternative A is very likely (95%) the most preferred one. In contrast alternative B receives only 30% confidence, meaning it is unlikely to be the best option even when its central weights are applied. Choosing B would require more precise comparisons to be collected, and for that new information to favour B.The use of simulation allows for the collection of detailed statistics on consistency and its effect on results. Basic consistency information is summarized in Fig. 1, which shows empirical distributions of inconsistency ratios for each of the seven pairwise comparison matrices. Clearly the greatest inconsistencies lie in the pairwise comparisons between criteria and within criterion 4, and to a lesser extent within criterion 1 and 6. For all of these, the conventional 10% upper bound on consistency is regularly violated (in 82%, 83%, 26%, and 26% of simulations, for the four cases respectively) by the matrices simulated as part of SMAA-AHP. This provides the user with information on the consistency of the preferences implied by the combination of initial judgements and imprecision factor d, either in the form of the full empirical distributions or summarized as the proportion of simulated judgments violating some consistency benchmark such as 10%.Of particular interest is whether these inconsistent judgements are linked to preferences for particular alternatives. This information can be conveyed in a number of ways. Fig. 2shows the average first rank acceptability indices for alternatives as a function of the observed inconsistency in each comparison matrix. It is important to note that because consistency refers to the pairwise comparison matrix as a whole, two simulated matrices with the same inconsistency ratio may differ substantially in their pairwise judgments, and hence the ranking of alternatives and subsequent acceptabilities that are derived from them. Therefore, to ease interpretation, the averages shown in Fig. 2 have been smoothed with a simple local smoother using a neighbourhood of the closest 10% of observations and a cubic inverse distance function to weight those observations, although our conclusions are relatively insensitive to these choices. Our aim in doing so is to provide a summarized view of how preferences change, on average, with increased inconsistency.In Fig. 2(a), which uses the same imprecision factord=1.5for all judgements, it is clear that acceptabilities remain roughly the same regardless of the consistency of simulated judgements. However, this need not always be the case, particularly where uncertainty is not symmetric around the initial point judgement. Fig. 2(b) provides an illustration of results obtained if only those between-criteria comparisonsa12,a15, anda25are uncertain and these are generatedU[0.5,2]; all within-criteria comparisons are still subject to an imprecision factord=1.5. In that case the overall acceptability of alternative B increases marginally to 18%, but the bottom panel of Fig. 2(b) reveals that much of this increase in acceptability is derived from inconsistent simulated between-criteria judgements, and that under highly inconsistent between-criteria assessments alternative A attains higher acceptability, on average, than B.Where acceptabilities are sensitive to judgmental consistency, two broad responses are possible. The first is simply to reject any simulated matrices whose inconsistency is above a certain threshold (e.g. the usual benchmark of 10%). A second option, arguably more closely aligned with the goal of decision support, is to interrogate the inconsistent judgments that are favouring a particular alternative, in an attempt to understand the sources of the inconsistency and ultimately to resolve these, either through more precise evaluations or rejecting certain judgements. Simulated comparison matrices favouring each alternative may be collected and stored as part of the SMAA process, so that those above a certain consistency threshold can be easily identified and summarized. To illustrate, in Table 4we summarize 250 between-criteria comparison matrices which obtain a inconsistency ratio above 0.17 and hence, from Fig. 2(b), can be seen to increasingly favour alternative B. Here,a12=2.93anda52=8.68so that it would appear thata51should be greater than 1 and indeed perfect consistency would implya15=2.93/8.68=0.34. The geometric mean of the inconsistent simulated comparisons indicates, however, a strong preference for criteria 1 over 5 (a15=5.41). At this stage the decision maker would need to decide whether the inconsistent simulated judgments are merely artifacts of the simulation that can be summarily rejected, and/or whether further, more refined assessments are required.Finally, we demonstrate the use of SMAA-AHP in the case where comparison information between criteria is completely missing. We represent this missing information using non-negative normalized weights that follow a joint uniform distribution. Table 5shows the output from the revised SMAA-AHP model.Average overall scores indicate that A increases its superiority, while B and C appear almost equally good. Rank acceptability indices show how the increased uncertainty in the comparisons is reflected as increased uncertainty in the ranking. The confidence factors show that both A and B can be chosen under favourable preferences, but C is likely not the best alternative under any preferences. The central weights identify what kind of trade-off ratios between the criteria make each alternative most preferred. We can see that different alternatives are favoured by dramatically different weights. For example, alternative C would require about 40% of the weight to be placed on criterion 3 alone. The pairwise winning indices reveal the surprising fact that while B is superior to C when competing about the first rank, C is preferred to B on average. An analysis of consistency information is not provided here, but would be conducted as for the partial-information case described above.The purpose of the simulation study is to evaluate the influence of imprecision on model results. In particular we wish to evaluate the rate at which the consistency of the resulting judgements and the ability of the SMAA-AHP model to discern the best-performing alternative deteriorate as imprecision, as captured by the imprecision factor d, increases in magnitude.The basic structure of each simulation run is as follows:1.Select problem size parameters (number of alternatives I and criteria K) and generateK+1pairwise comparison matrices.Select an imprecision factor d and select which of the pairwise comparison matrices are to be imprecise. Modify the appropriate pairwise judgements to reflect imprecision.Simulate the application of SMAA-AHP to the resulting imprecise decision problem.Evaluate model results.By performing a number of simulation runs at each combination of parameters, aggregate statistics can be collected and the mean performance of the SMAA-AHP model assessed and compared statistically across a range of simulated conditions. Further details on the simulation structure are given below.For each criterion k, pairwise comparisonsaijare generated by first simulating the evaluation of alternative i on criterion k, denotedzik, for all i and k. The evaluations are drawn fromU[0.1,0.9]and normalized so that each alternative’s scores sums to one over all criteria. This ensures that all initial ratio judgements will lie between 1/9 and 9 as per the AHP scale, and that all alternatives are non-dominated. Pairwise comparisons are computed from the resulting normalized evaluationsaij=zi/zj, where for convenience we have dropped the subscript k. Pairwise comparisons of criterion importance are done in the same way, except that no normalization is required. All initial judgements are therefore perfectly consistent.We introduce imprecision into the simulated pairwise comparisons via the imprecision factor d described in Section 3. This factor is left as a parameter of the simulation, taking on a range of values between 1 and 5. We also differentiate between imprecision involving evaluations of alternative performance and imprecision involving evaluations of criterion importance. This gives three conditions: (a) both pairwise comparisons of criterion importance and pairwise comparisons of performances on each criterion are imprecise; (b) pairwise comparisons of criterion importance are imprecise but pairwise comparisons of performances on each criterion are not; and (c) pairwise comparisons of performances on each criterion are imprecise but pairwise comparisons of criterion importance are not. For simplicity, the same values of d are used for both types of imprecision.Once the imprecision factor d has been chosen, a single iteration of the SMAA-AHP model involves replacing each pairwise comparisonaijin the upper triangular part of the pairwise comparison matrix with a randomly drawn valueaij∗from the scaled1/xdistribution defined between[aij/d,aijd]. In this case,xmax/xmin=d2and soα=1/ln(d2). We do not generate random values in the lower triangular part of the pairwise comparison matrix so as to preserve the relationaij∗=1/aij∗, and also allow the generated values to lie beyond the usual AHP 1–9 scale. Once allaij∗have been generated, scores are computed as in the usual AHP model. That is, for each pairwise comparison matrix we extract the eigenvector corresponding to the largest eigenvalue, and aggregate the resulting scores using an additive model. Alternatives are ranked in descending order of global score, and we note which alternative is ranked in each position (for later calculation of acceptability indices).In implementing the SMAA-AHP model, acceptability results are based on 10,000 iterations of the procedure described above, following results in Tervonen and Lahdelma (2007). Note that the pairwise judgements generated by SMAA-AHP i.e. theaij∗, may well be inconsistent. We do not reject inconsistent judgements at this stage; rather we simply compute the average inconsistency and report how this varies with changes to the imprecision factor d as well as other parameters of the simulation.Information is collected on the following outcome measures:1.b1: Mean acceptability of the first-ranked alternative;b1-b2: Mean difference between the acceptability of the first-ranked and second-ranked alternatives, giving some indication of the degree to which a best alternative can be discerned;Pr(bi1>0): Mean proportion of alternatives with non-zero acceptability indices, bearing in mind that all alternatives are by construction Pareto optimal.IR: Mean inconsistency ratio evaluated across all matrices for whichd>1.Pr(IR>0.10): Mean proportion of imprecise matrices (i.e. those for whichd>1) with inconsistency ratios above the benchmark of 0.10.We carry out the following three sets of experiments:1.The imprecision factord∈{1.1,1.2,1.4,1.6,1.8,2,2.5,3,4,5}is varied, holdingI=10andK=8fixed and with both types of imprecision present.The type of imprecision present (criterion importance, alternative performance, or both) is varied, for each element in a subset of imprecision factorsd∈{1,1.5,2,2.5,3,4,5}and holdingI=10andK=8fixed.The problem size parameters governing the number of alternatives and criteria are varied, using four different problems sizes:(I=5,K=4);(I=10,K=16);(I=20,K=8); and(I=20,K=16)with both types of imprecision present and for each element in a subset of imprecision factorsd∈{1,1.5,2,2.5,3,4,5}.For each combination of simulation parameters 50 simulation runs are performed, giving standard errors of at most 0.05 for any group means discussed in the results.

@&#CONCLUSIONS@&#
In this paper we have introduced the SMAA-AHP method for representing uncertain or imprecise information through stochastic distributions in the AHP and a simulation approach for analysing the resulting model. The method is suitable for group decision-making problems, where it is difficult to agree on precise pairwise comparisons. A particular strength of the method is that it allows flexible modelling of different kinds of imprecision, uncertainty, or missing preference information. This may be useful in many decision problems where information is gradually refined during the process. The multiplicative uncertainty factor of SMAA-AHP is a natural way to express the uncertainty symmetrically around a midpoint, and has (in comparison to the uniform distribution, see for example Hauser & Tadikamalla (1996)) the advantage of allocating equal probability mass for equal proportional sub-intervals. However, the SMAA approach is not constrained by this choice, and any realistic distribution may be used (see Tervonen, van Valkenhoef, Baştürk, & Postmus (2013) for details).One concern that users of the AHP may raise about the use of a simulation-based method such as SMAA is the potential for generating inconsistent judgements. Inconsistent judgements may of course be screened out using rejection sampling, but our simulation results indicate that they are unlikely to arise at all (given precise point judgements i.e. entirely due to the simulation process) unless imprecision is fairly severe. The simulations results also show that with even quite limited imprecision there tend to be several potentially optimal alternatives, with the difference between the largest and second-largest acceptability decreasing rapidly with imprecision. This means that when imprecision is present in almost any degree the choice of best alternative will not be clear. In some instances imprecision may be resolvable by further discussion with the DM; in others it may be an unavoidable element of the decision problem (for example, when it results from uncertainty about future events). In either case, the SMAA methodology provides a set of output measures that can be presented to DMs, so that they can better understand the kinds of preferences that support the selection of each of the alternatives. Even where progression to a final choice is not immediately possible this information can help DMs to gain greater insight into the choices facing them.