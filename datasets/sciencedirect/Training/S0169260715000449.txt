@&#MAIN-TITLE@&#
Orthogonal moments for determining correspondence between vessel bifurcations for retinal image registration

@&#HIGHLIGHTS@&#
This paper presents an algorithm for registration of retinal images using orthogonal moment invariants as features.As orthogonal moments are invariant to translation, rotation and scale (TRS); moment invariants features around a vessel bifurcation are unaltered due to TRS and are used to determine the correspondence between reference and test retinal images.The TRS in test retinal image with respect to reference retinal image is estimated using Similarity transformation.The test retinal image is aligned with reference retinal image using the estimated registration parameters.The experimentation is carried out on 4 databases as DRIVE, STARE, VARIA and database provided by local government hospital.

@&#KEYPHRASES@&#
Diabetic retinopathy,Periodic screening,Registration,Orthogonal moment invariants features,Similarity transformation,

@&#ABSTRACT@&#
Retinal image registration is a necessary step in diagnosis and monitoring of Diabetes Retinopathy (DR), which is one of the leading causes of blindness. Long term diabetes affects the retinal blood vessels and capillaries eventually causing blindness. This progressive damage to retina and subsequent blindness can be prevented by periodic retinal screening. The extent of damage caused by DR can be assessed by comparing retinal images captured during periodic retinal screenings. During image acquisition at the time of periodic screenings translation, rotation and scale (TRS) are introduced in the retinal images. Therefore retinal image registration is an essential step in automated system for screening, diagnosis, treatment and evaluation of DR. This paper presents an algorithm for registration of retinal images using orthogonal moment invariants as features for determining the correspondence between the dominant points (vessel bifurcations) in the reference and test retinal images. As orthogonal moments are invariant to TRS; moment invariants features around a vessel bifurcation are unaltered due to TRS and can be used to determine the correspondence between reference and test retinal images. The vessel bifurcation points are located in segmented, thinned (mono pixel vessel width) retinal images and labeled in corresponding grayscale retinal images. The correspondence between vessel bifurcations in reference and test retinal image is established based on moment invariants features. Further the TRS in test retinal image with respect to reference retinal image is estimated using similarity transformation. The test retinal image is aligned with reference retinal image using the estimated registration parameters. The accuracy of registration is evaluated in terms of mean error and standard deviation of the labeled vessel bifurcation points in the aligned images. The experimentation is carried out on DRIVE database, STARE database, VARIA database and database provided by local government hospital in Pune, India. The experimental results exhibit effectiveness of the proposed algorithm for registration of retinal images.

@&#INTRODUCTION@&#
Retinal image registration is fundamental in diagnosis and monitoring of diabetic retinopathy in order to follow progress of the diseases [1]. Diabetic retinopathy is one leading cause of blindness resulting from long term diabetes. Retinal vessels and capillaries become weak due to long term diabetes. This result into leakage of lipid and blood from retinal vessels which is evident by presence of exudates, microaneurysms, and hemorrhages on the retina causing loss of vision and eventually leading to blindness. If detected in early stages the complications of DR can be prevented with proper examination and treatment. The risk of subsequent blindness caused due to DR can be reduced by 50% if DR is detected in early stages in periodic retinal screenings [2]. Dupas et al. evaluates algorithms and computer assisted diagnostic systems for grading diabetic retinopathy in Ref. [3]. An automated diagnostic system for detection of DR is presented in Ref. [4]. The system extracts retinal features and detects and analyzes early clinical signs associated with the diseases as microaneurysms, and hemorrhages. Kedir et al. present an automated system for detection of microaneurysms based on semi-supervised learning approach in Ref. [5]. A review of computer aided diagnosis systems and available methods of retinal feature extraction for automated analysis are presented in Ref. [6]. The retinal images captured during subsequent periodic screenings are compared using automated systems to follow the progress of the diseases. The retinal images captured during different periodic screenings need to be registered before they can be compared. Estimating spatial correspondence between two retinal images can be difficult as retina is curved surface and retinal image is projection of such surface captured by an un-calibrated camera. Also the nonvascular region of a healthy retina is homogeneous whereas varieties of pathologies appear on the affected retina. The pathologies present on the retina can vary with time and progression of diseases result in poor quality images with indistinguishable vasculature. Also varying illumination conditions might be present during image acquisitions or different modalities may be used during periodic screening [7,8]. Image artifacts such as rotation, translation and scale are introduced during retinal image acquisition due to several reasons as variations in sitting position of the patient, chin cup position, head orientation and distance between eye and camera [9]. Also as retina is spherical in nature, parallax will be introduced in the images causing translation of the retinal landmarks. These issues need to be addressed as an essential step for development of automated system for diagnosis and monitoring of Diabetic Retinopathy.The existing techniques for retinal image registration can be broadly divided into two categories such as intensity based methods and feature based methods [10]. In intensity based methods, certain similarity measure or objective function based on mutual information, cross correlation, phase correlation, gradient correlation, least mean square error are optimized using pixel intensities [11,12]. Matsopoulos et al. employed image transformation models as affine, bilinear and projective along with downhill simplex, simulated annealing and genetic algorithms for registration of angiographic images retina [13]. Ritter et al. used mutual information as similarity measure combined with simulated annealing for registration of temporal images of retina [14]. The limitations of such methods are the high computational efforts involved in search and optimization procedures [10]. The performance of the intensity based techniques is affected due to non uniform contrast within the image as well as presence of large homogeneous (texture less) area in the image. Therefore Chanwimaluang et al. propose a hybrid approach combining area based and feature based method. The retinal vasculature is segmented using entropy thresholding. Translation is estimated by maximizing mutual information between binary images. Further two features as landmark points and sampling points are used for affine/quadratic model estimation [15].The feature based methods use prominent landmarks present in the image and derive the correspondence of these landmarks by maximizing a similarity measure derived from the landmarks. The methods based on salient point matching perform well even if large homogeneous area is present in the image [7]. The application of feature based methods for image analysis can be found in Ref. [16]. The paper describes application of Random Sample Consensus (RANSAC) to the location determination problem (LDP) of known landmark points. Implementation details and comparison of histogram based keypoint descriptors is presented in Ref. [17]. Stewart et al. propose a dual bootstrap iterative closest point algorithm [18]. The method starts from low order estimate which are accurate over small regions called bootstrap regions, expand the bootstrap regions and verifies if the higher order transformation model can be used. The algorithm is initialized by automatically matching vessel bifurcations as landmarks of the retinal vasculature. In Ref. [19] bifurcations of the retinal blood vessels are detected and orientations of adjacent blood vessels are computed. An orientation based feature is used for matching the bifurcation points detected in reference and test retinal image. Further Bayesian Hough-Transform is used to arrange the transformations to their corresponding likelihoods. Can et al. model the retina as rigid quadratic surface with unknown parameters, and twelve parameter inter image transformation is computed based on this model. The parameters are derived by matching features of the retinal vasculature [20]. Ryan et al. present an algorithm for registration of retinal images with an intrinsic set of control points whose correspondence is not known in [21]. Similarity transformation is used for matching all possible combinations of control points. Further registration is performed using linear regression to optimize similarity, bilinear or second order polynomial transformations. Estimation of correspondence based on Normalized Mutual Information as similarity measure in retinal images is applied in Ref. [22]. The image pairs registered are subtracted to represent progression DR. In Ref. [23] the retinal vessel is tree is extracted, thinned and approximated as short linear segments. These short elements are elastically deformed to match the elements of test retinal image to be registered. Ojala et al. proposed local binary pattern (LBP) based features to represent textures defined by histogram of LBP code for each pixel in the region under consideration [24]. These features recognize certain uniform local binary patterns which are fundamental properties of local image texture and their occurrence histogram. The LBP based features are robust in terms of gray scale variations [24]. Chen et al. propose a partial intensity invariant feature descriptor for estimation of correspondence of landmark points in Ref. [25]. The corner points are used as landmarks which are uniformly distributed over the image domain. A method based on artificial immune system (AIS) is proposed in [26]. The AIS is used for locating optimum set of candidate points as well as for defining their correspondence in the test image. Automatic alignment of two images using sparse features is proposed in Ref. [27]. Robust Hybrid Image Matching (RHIM) algorithm is used to optimize feature correspondence and spatial transformation for image registration. In Ref. [28] vessel bifurcations are used as landmark points for matching. A feature vector comprising of directional gradients along 0°, ±45° and 90° directions around the labeled bifurcation is used for estimating the correspondence between reference and test retinal images.This paper presents an algorithm for registration of retinal images. The vessel bifurcation points present in the retinal image are used as intrinsic control points for registration. The proposed work focuses toward identifying the one out on n vessel bifurcation points detected in reference retinal image that matches with the vessel bifurcation point detected in test retinal image using local statistical descriptors computed around the vessel bifurcation points. Each vessel bifurcation point detected in retinal image is uniquely represented by third order moment invariants descriptors. The moment invariants descriptors exhibit invariance to TRS and thus can be effectively used as features for registration. The coordinates of the matched vessel bifurcation pairs are used to estimate transformation between reference and test retinal image. The registration parameters (TRS) have been estimated using pairs of matched vessel bifurcation points. Further the test retinal image is aligned with reference retinal image using estimated registration parameters. To evaluate the effectiveness of the proposed method the experimentation is carried out retinal images from DRIVE database, STARE database, VARIA database and local government hospital database. The performance of the proposed algorithm is measured in terms of mean error and standard deviation of the labeled vessel bifurcation points in the aligned images. The paper is organized into five sections. Introduction is followed by Section 2 elaborating the steps of the proposed algorithm. Section 3 describes the experimentation and results. Section 4 presents the conclusion followed by references.The proposed algorithm uses orthogonal moment invariants features for determining correspondence between vessel bifurcations in the reference and test retinal images to estimate the TRS present in the test retinal image with respect to reference retinal image. Fig. 1shows the flow chart of the proposed method. Initially the retinal blood vessels in the reference and test retinal images are enhanced using a set of twelve, directional matched filter kernels and segmented using automated entropy thresholding. The segmented retinal images are thinned using morphological operations so that the retinal blood vessels are of one pixel width. The vessel bifurcations are detected in the segmented, thinned reference and test retinal images. As described in Section 2.2 the genuine vessel bifurcations are retained and labeled in the input grayscale reference and test retinal images. Further orthogonal moment invariants features of an area selected around the labeled vessel bifurcations are computed for reference and test retinal images. The correspondence between the vessel bifurcations of reference and test retinal images is established using orthogonal moment invariants features and metrics as Hellinger Distance and Pearson Correlation Coefficient. Based on set of matched vessel bifurcations, first order similarity transformation is applied to estimate registration parameters as TRS present in the test retinal image. Further the test retinal image is aligned with reference retinal image using estimated registration parameters. The stages of the algorithm are described in detail in the subsequent subsections.Retinal vasculature is the most prominent anatomical structure in the retinal images and is extensively used in the registration algorithms. An extensive study of retinal vasculature segmentation techniques is found in Ref. [29]. A large number of registration algorithms use retinal vasculature for extracting the control points as it is spread over large area of retina, its location is unaffected except due to some diseases, and provides sufficient information for localization of control points [15].Initially the reference and test retinal images are resized to a same size and converted to square for further processing. Further the red, green and blue channels are separated from the input color fundus retinal images. The green channel image is used for further processing as it contains more information and is less subject to nonuniform illumination [30]. Fig. 2(a) displays a typical input color fundus image from DRIVE database and the corresponding green channel retinal image is shown in Fig. 2(b).The enhancement of retinal blood vessels using 2-D matched filters was initially proposed by Chaudhari et al. in Ref. [31]. As retinal blood vessels have limited curvature, they are represented as piece-wise linear segments. Further the cross sectional profile of pixel intensity of these linear segments has Gaussian shape. Also the width of the retinal blood vessels reduces as they move away from the optic disk [29]. These attributes of the retinal blood vessels influence the design of shape and size of the filter kernel applied to enhance the retinal blood vessels. The shape of the filter kernel is designed to match intensity profile of the retinal blood vessel which has Gaussian shape [31]. The size of the kernel is decided by the length in which the retinal vessel can be considered as linear [31]. The basic matched filter kernel is described in Eq. (1) as(1)k(x,y)=−exp−x22σ2,for|x|≤3σ,|y|≤L/2where σ defines spread of the intensity profile, L defines kernel size. We have used spread of the kernel, σ=2 and length of the kernel, L=9 as mentioned in Ref. [31]. Twelve matched filter kernels, 15° apart spanning 0–180° orientations are applied to enhance retinal vessel in all orientations. Fig. 2(a) shows typical input color fundus image from DRIVE database, the corresponding green channel retinal image is illustrated in Fig. 2(b) and (c) displays retinal image enhanced using set of twelve matched filter kernels.Further to enhancement of retinal blood vessels using matched filters the co occurrence matrix of MF enhanced retinal image is computed. The co occurrence matrix of an image represents the transition of intensity between adjacent pixels, indicating spatial structural information of the image. The elements of the co occurrence matrix represent probability of joint co occurrence of pixels intensities than values of the individual pixels intensities [32].For the matched filter (MF) enhanced retinal image f(x,y) of size M×N pixels and quantization level k, the spatial relationship of a pixel pair (i,j) with specified pair (d,θ), the joint probabilities can be expressed as(2)P(i,j,d,θ)={C(i,j)|(d,θ)}where C(i,j) represents the co occurrence probabilities between gray level i and j and is given by(3)C(i,j)=P(i,j)∑i=1k∑j=1kP(i,j)0≤C(i,j)≤1The co occurrence matrix of MF enhanced retinal image is constructed by considering inter pixel distance d=1 and orientation θ=0°.The segmentation of retinal blood vessels is performed using automated entropy thresholding technique described in Refs. [15,31,33]. We have used entropy obtained from co occurrence matrix of the MF enhanced retinal image for vessel non vessel pixel classification.Entropy of co occurrence matrix of MF enhanced image is given by(4)H=−∑i=0k−1∑j=0k−1P(ij)logP(i,j)where k is the number of quantization levels of the image. Let ti,jbe the (i,j)th entry of the co occurrence matrix, then the probability of co occurrence P(i, j) of gray levels i and j is(5)P(i,j)=ti,j∑i=0k−1∑j=0k−1ti,jAs seen from Fig. 3, a threshold s such that 0≤s≤k−1, partitions the co occurrence matrix into four quadrants namely A, B, C and D. The normalized probabilities of individual quadrants of co occurrence matrix of MF enhanced retinal image are given as(6)P(i,j)A=ti,j∑i=0S∑j=0Sti,j(7)P(i,j)B=ti,j∑i=0S∑j=S+1k−1ti,j(8)P(i,j)C=ti,j∑i=S+1k−1∑j=S+1k−1ti,j(9)P(i,j)D=ti,j∑i=Sk−1∑j=0S−1ti,jThe second order entropy of the object (retinal blood vessels) can be defined as(10)HA2(s)=−12∑i=0S∑j=0SP(i,j)Alog2P(i,j)ASimilarly, background entropy can be given as(11)HC2(s)=−12∑i=S+1L−1∑j=S+1L−1P(i,j)Clog2P(i,j)CThe threshold s which maximizesHT2(s)gives optimal threshold for vessel non vessel pixel classification. The threshold obtained automatically from entropy–threshold curve as shown in Fig. 4is used for vessel non vessel pixel classification. Fig. 5(a) illustrates the corresponding segmented retinal image from DRIVE database.The segmented retinal image obtained after automated entropy thresholding has isolated pixels as well as some of the retinal capillaries are broken. Median filter of size 3×3 is used to remove isolated pixels. Length filtering is applied to connect the disconnected retinal vessels as shown in Fig. 5(b). Further the segmented retinal image is thinned such that retinal vessels are of one pixel width and vessel connectivity is also maintained. Fig. 5(c) displays a typical thinned retinal image. The thinned retinal image is further used for detecting vessel bifurcations.The proposed algorithm uses vessel bifurcations as intrinsic control points for registration. A feature based registration algorithm based on point matching requires sufficient number of control points to be available in both the images. Bifurcations of retinal blood vessel are natural features and prove to be suitable intrinsic control points to guide registration as they are both salient and accurately detectable [21]. Further the vessel bifurcations are locatable even in case of retina with pathology.The vessel bifurcations are located in the binary thinned retinal image with one pixel vessel width. If the retinal vessels are of mono pixel width then the vessel bifurcations appear like ‘T’ shaped structures when viewed in their 3×3 neighborhood [19]. Fig. 6(a) shows typical vessel bifurcations marked in binary, thinned, mono pixel width retinal vasculature. The marked vessel bifurcations along with their neighborhood are displayed in Fig. 6(b)–(e). In the binary mono pixel width retinal vasculature, the vessel bifurcations have at least three or four neighbors with intensity value equal to one [9]. Initially we have detected all such candidate pixels representing probable vessel bifurcations. The width of the retinal vessels affects the detection of vessel bifurcations typically in case of primary vessels where a cluster of adjacent pixels may get detected for one vessel bifurcation. A typical vessel bifurcation associated with a relatively thick retinal vessel is labeled in thinned retinal image as shown in Fig. 7(a). The neighborhood of the vessel bifurcation is displayed in Fig. 7(b). For the vessel bifurcation marked in Fig. 7(a) four pixels are detected as probable vessel bifurcation as displayed in Fig. 7(c). The candidate pixels detected as probable vessel bifurcation pixel are labeled in the corresponding grayscale retinal image as shown in Fig. 7(d). The intensity profiles along vertical direction for all four candidate pixels are plotted and a local minimum is detected as illustrated in Fig. 7(e). The pixel corresponding to local minimum is retained as genuine pixel associated with the corresponding vessel bifurcation and is labeled in grayscale retinal image as displayed in Fig. 7(f). The retinal blood vessels can be considered as piecewise linear due to their limited curvature. The cross sectional profile of the pixel intensity of these linear segments exhibits Gaussian curve [31]. Thus pixel along the centerlines of the blood vessels will exhibit local minima and will truly represent the bifurcation point associated with the retinal vessels. Therefore we have retained the pixel associated with local minima as genuine pixel representing the corresponding vessel bifurcation.The crossover points of two retinal vessels may also get detected as probable vessel bifurcations as the pixels associated with the vessel crossovers also have three or four neighbors in its D8 connectivity with intensity value equal to one. A typical crossover point of retinal vessels is marked in Fig. 8(a). The neighborhood of the labeled vessel crossover point is displayed in Fig. 8(b). Two candidate pixels are detected as probable vessel bifurcations as illustrated in Fig. 8(c). The distances between these two detected candidate pixels is less than d pixels and therefore both are discarded. All the candidate pixels detected as probable vessel bifurcations which are located at distance lesser than d from each other are not considered as genuine vessel bifurcations. Further the image is divided into four quadrants. We have ensured that at least one vessel bifurcation is retained in every quadrant. If there are no vessel bifurcations detected in a quadrant, the candidate pixels detected as probable vessel bifurcations placed at a distance lesser than d are retained as genuine vessel bifurcations.The genuine vessel bifurcations retained are marked in the corresponding grayscale retinal image for further processing. Fig. 9(a) shows retained genuine vessel bifurcations in a typical image from DRIVE database, Fig. 9(b) displays retained genuine vessel bifurcations labeled in a typical image from STARE database. The retained genuine vessel bifurcations in a typical image from VARIA database are illustrated in Fig. 9(c) and (d) shows retained vessel bifurcation in a typical image from local hospital database. These retained vessel bifurcations are used as control points for further processing. The number vessel bifurcations associated with primary and secondary vessels are relatively more than the number of bifurcations associated with retinal capillaries.After locating the vessel bifurcations in grayscale image, a square area of size fixed size L×L, is selected around every vessel bifurcation and moment invariants features are computed for the selected area. Therefore for a given vessel bifurcation (x, y) of a typical retinal image I, selected square area of size L×L around bifurcation (x, y), can be represented asSx,yL. The average width of retinal vessels affects the choice for size of area considering that the detected vessel bifurcation is located on the centerline of the retinal vessel. The selected area should contain pixels belonging to vessels as well as to the background [34]. Considering average vessel width of nine pixels as mentioned in Ref. [34], we have selected area of size 21×21 pixels around the vessel bifurcation have computed moment invariant features of the same.We have used orthogonal moment invariant features to represent the selected area around the retained genuine vessel bifurcations. Moments of a function are scalar quantities which describe the function and capture the significant attributes of the same. They can be also considered as projections of the function onto a polynomial basis [35]. The moment invariants features are invariant to image TRS and therefore uniquely represent the image function under consideration even in presence of image artifacts.The reference retinal image f(x, y) and test retinal image t(x, y) captured during periodic screenings are related as(12)t(x,y)=D[f(x,y)]where D is the degradation operator representing various forms of degradation introduced in the image during acquisition. The features of the retinal image which are invariant to degradation D, can be used for registration of the retinal images. Computing such features can be considered as determining an invariant functional I defined on the space of image function which is invariant with respect to degradation operator D such that(13)I[f(x,y)]=I[t(x,y)]=I[D(f(x,y))]This property is called as invariance. Along with invariance the functional I must also have discriminability property. The functional I will have distinct values for the objects belonging two different classes due to discriminability property. To increase the discriminability of the invariants, a set of invariants features are used generating an invariants vector and each object is represented by a point in n dimensional feature space. We have used orthogonal moment invariants features to represent every retained genuine vessel bifurcation. These features exhibit invariance to TRS, as well as they are unique for every vessel bifurcation demonstrating discriminability property.Moment invariants were initially introduced by Hu [36]. For a selected image areaSx,y21around a typical vessel bifurcation (x, y) from a given retinal image f, the 2-D moment of order (p+q) of is defined as(14)mpq=∑i∑jipjqfSx,y21(i,j)forp,q=0,1,2…wherefSx,y21(i,j)represents grayscale value at the pixel (i, j). The summation is performed for the spatial coordinates of image area selected around the vessel bifurcation. According to uniqueness theorem, moments of all the order exists and the moment sequence (mpq) is uniquely determined by the functionfSx,y21(i,j). Conversely (mpq) uniquely determinesfSx,y21(i,j). The central moment is given by(15)μpq=∑i∑j(i−i¯)p(j−j¯)qfSx,y21(i,j)wherei¯=m10/m00andj¯=m01/m00.The invariance of the central moment to translation is achieved by shifting the object such that its centroid(i¯,j¯)coincides with the origin of the coordinate system.Scaling normalization is realized by normalizing the moment. Since low order moments are less affected by noise the normalization is often performed by a proper power of μ00. The normalized central moments are given as(16)ηpq=μpqμ00γ;forp,q=0,1,2,…where γ=(p+q/2)+1; for (p+q)=2, 3, ….Rotation invariance was introduced by Hu and he proposed seven moment invariants to an in-plane rotation [36]. This set of seven invariants is derived from second and third order moments. The seven moment invariants are given by Equation (17)–(23).(17)ϕ1=η20+η02(18)ϕ2=(η20−η02)2+4η112(19)ϕ3=(η30−3η12)2+(3η21−η03)2(20)ϕ4=(η30+η12)2+(η21+η03)2(21)ϕ5=(η30−3η12)(η30+η12)[(η30+η12)2−3(η21+η03)2]+(3η21−η03)(η21+η03)[3(η30+η12)2−(η21+η03)2](22)ϕ6=(η20−η02)[(η30+η12)2−(η21+η03)2]+4η11(η30+η12)(η21+η03)(23)ϕ7=(3η21−η03)(η30+η12)[(η30+η12)2−3(η21+η03)2]+(3η12−η30)(η21+η03)[3(η30+η12)2−(η21+η03)2]We have used these seven moment invariants features described by Eqs. (17)–(23) to represent the image areafSx,y21(i,j)around every labeled vessel bifurcation of input grayscale retinal image f(i, j). For a given 2-D functionfSx,y21(i,j)around a typical vessel bifurcation (x, y), orthogonal moment invariants features describe the attributes of the corresponding probability density function. η10 and η01 are the mean values, η20 and η02 are the variances of horizontal and vertical projections and η11 is the covariance between them. Second order moments indicate orientation of the image. Skewness of probability density function represents deviation of the respective projection with respect to mean. Skewness is derived from second and third order moments. The skewness of horizontal projection is given asη30/η203and skewness of vertical projection is given asη03/η203[37]. As the orthogonal moment invariants features are based on statistical information contained infSx,y21(i,j), they exhibit invariance to image translation, scale and rotation.Fig. 10(a) displays a typical vessel bifurcation marked in the grayscale retinal image. The region of interestSx,y21, selected for computing moment invariants features of the vessel bifurcation is shown in Fig. 10(b). The plot of orthogonal moment invariants features for the corresponding labeled control point is given in Fig. 10(c). The input retinal image is rotated by 5° in counterclockwise direction and same vessel bifurcation is labeled as shown in Fig. 10(d). The region of interestSx,y21around the labeled vessel bifurcation in rotated image is displayed in Fig. 10(e) and plot of moment invariant features of the labeled control point is illustrated in Fig. 10(f). Orthogonal moment invariants features computed forfSx,y21(i,j); area selected around a typical vessel bifurcation numerically describe the vessel bifurcation (x, y) and its neighborhood (21×21) independent of TRS introduced in the image f (i, j) [34]. Therefore for the same vessel bifurcation in Fig. 10(a) and (d) the moment invariants features are unaltered due to rotation which can also be seen from the plots of Fig. 10(c) and (f).Fig. 11(a) illustrates two distinct vessel bifurcations labeled in the grayscale retinal image. As seen from Fig. 11(b) and (c), moment invariants features are dissimilar for distinct vessel bifurcations of the same input image. Orthogonal moment invariants features offSx,y21(i,j)are uniquely determined byfSx,y21(i,j)and vice versa. As orthogonal moment invariants demonstrate invariance to TRS along with ability to discriminate between distinct vessel bifurcations of the same input retinal image, we have established the correspondence between the vessel bifurcations of reference and test retinal images using these features.The correspondence between vessel bifurcations detected in the reference and test retinal image is established using orthogonal moment invariants features and metrics as Pearson Correlation Coefficient and Hellinger Distance.The vessel bifurcations are detected in segmented, thinned retinal images and genuine vessel bifurcations are retained and labeled in corresponding grayscale retinal images. Let A represent set of genuine vessel bifurcations retained in reference retinal image fR(i, j) given as(24){a1(x1,y1),a2(x2,y2),⋯am(xm,ym)}∈A,suchthat(x1…m,y1…m)∈(i,j)Let B represent set of genuine vessel bifurcations retained in test retinal image fT(i, j) given as(25){b1(x1,y1),b2(x2,y2),…bn(xn,yn)}∈B,suchthat(x1…n,y1…n)∈(i,j)It is observed that a genuine match of vessel bifurcation in A may be present in the D8 neighbors of vessel bifurcations retained in B[38]. Therefore we have considered D8 neighbors of vessel bifurcations retained in B for determining matched vessel bifurcation pairs from reference and test retinal images.(26){(b1(xr1,ys1)∈N8(b1)),(b2(xr2,ys2)∈N8(b2)),…(bn(xrn,ysn)∈N8(bn))}∈Bwhere r and s indicate location of the corresponding vessel bifurcation. The moment invariants for vessel bifurcations of reference retinal image are computed considering image functionfRSx,y21(i,j)whereSx,y21represents region of interest of size 21×21 pixels around the all vessel bifurcations of reference retinal image contained in set A. For m vessel bifurcations of reference retinal image the feature space of orthogonal moment invariants features for reference retinal image is given by(27)ϕA={ϕa1,ϕa2…ϕam},∀ai∈A;i=1:mwhere ϕai, i=1:m, are orthogonal moment invariants features representingfRSxi,yi21(i,j)selected around vessel bifurcation ai(xi, yi)∈A.If n vessel bifurcations are retained in test retinal image, the moment invariants features of all n vessel bifurcations along with their D8 neighbors form the feature space for test retinal image. This can be represented as(28)ϕB={ϕbij,…ϕbnj},∀bij∈B;i=1:n;j=1:9Correspondence between reference and test retinal images is established by comparing moment invariant features of (n×9) image areas represented byfTSx,y21(i,j)belonging to n vessel bifurcations an their D8 neighbors in test retinal image with the moment invariants features computed for m image areas represented byfRSx,y21(i,j)associated with m vessel bifurcations retained in reference retinal image. Statistical metrics as Hellinger Distance (HD) and Pearson Correlation Coefficient (PCC) are used to establish the correspondence between orthogonal moment invariants features of vessel bifurcations from reference and test retinal image. Hellinger Distance (HD) can be considered as a difference measure between two probability density functions [39]. This is given as(29)HD=1N∑i=1Nxi2−yi221/2where xiand yidenote realizations of the random variable X and Y, N is the number of available sample pairs.Pearson Correlation Coefficient indicated departure of two random variable (X, Y) from independence. It was first introduced by Galton [40] and is defined as(30)PCC=1N−1∑i=1Nxi−μxσxyi−μyσyThe above mentioned measures are used for establishing correspondence between elements of ϕAand elements of ϕB.For every ϕap∈ϕAthe closest point ϕbq∈ϕBis estimated as(31)min(HD(ϕap,ϕbq))=1N∑i=1N(ϕap2−ϕbq2)21/2∩max(PCC(ϕap,ϕbq))=1N−1∑i=1Nϕapi−μϕapiσϕapiϕbqi−μϕbqiσϕbqiThe closest match of ϕap∈ϕAand ϕbq∈ϕB, provides correspondence between ap(xp, yp)∈A and bq(xq, yq)∈B, the vessel bifurcations between the reference and test retinal images. This provides set C of coordinates of matched vessel bifurcations in reference and test retinal images given as(32)C={(api(xpi,ypi))≡(bqj(xqj,yqj))};∀i=1:m1,j=1:m1where m1 is the number of vessel bifurcations for which the match is obtained as described in Eq. (32). The vessel bifurcations for which more than one match is obtained are discarded. Fig. 12(a) displays a reference retinal image with labeled genuine retained vessel bifurcations along with their corresponding vessel bifurcations in the test retinal image as shown in Fig. 12(b). Further we have estimated registration parameters (TRS) in the test image with respect to reference retinal image by applying similarity transformation to the coordinates of matched vessel bifurcations of reference and test retinal images.After determining correspondence between vessel bifurcations of reference and test retinal images, the deformation between reference and test retinal image is modeled using first order similarity transformation. The deformation in retinal images is due to translation between eye and camera, slight rotation (head orientation) and some scaling effects caused due to motion in direction of optical axis [9]. Similarity transformation is a first order transformation that represents rotation, translation and uniform scale using four parameters [21]. If ap(xR, yR), ap∈A is a vessel bifurcation of the reference retinal image represented by fR(x, y) and bq(xT, yT), bq∈B is the corresponding vessel bifurcation of the test retinal image represented by fT(x, y) such that {(ap(xp, yp))≡(bq(xq, yq))}∈C then the transformation matrix relating the coordinates of ap(xR, yR) and bq(xT, yT) is given as(33)xRyR=Δxαcosβ−αsinβΔyαsinβαcosβ1xTyTwhere Δx and Δy represent translation, α gives the scale and β represents the rotation present in the test retinal image fT(x, y) with respect to the reference retinal image fR(x, y).If apiand bqjare the matched vessel bifurcations respectively such thatC={(api(xpi,ypi))≡(bqj(xqj,yqj))}∀i=1:m1,j=1:m1From the pairs of matched vessel bifurcations, (api, bqj)∈C, an estimate of registration parameters for subsequent image alignment can be obtained. For m1 matched vessel bifurcations, a set G containing estimates of registration parameters is obtained such that(34)G={Δxi,Δyi,αi,βi}Computing median parameters of set G gives good approximation of registration parameters. Therefore estimate of TRS in the test retinal image, fT(x, y) with respect to the reference retinal image fR(x, y) can be obtained as(35)Gmed={Δx,Δy,α,β}medFurther the test retinal image fT(x, y) is aligned with reference retinal image fR(x, y) using the estimated registration parameters as translation (Δx, Δy), rotation (β) and scale (α).For implementation and testing of the proposed algorithm the computational facility with Windows XP, MATLAB 7.10 (R2010a), intelcore2duo, 2.09GHz, 2GB RAM was used. The performance of the proposed algorithm is evaluated on DRIVE, STARE, VARIA databases and database obtained from local government hospital. The detailed description of DRIVE, STARE, VARIA and hospital database is given in following subsections.The DRIVE (Digital Retinal Images for Vessel Extraction) [41] database comprise 40 retinal images (seven of which present pathology) taken with Cannon CR-5 nonmydriatic 3-CCD camera with a 45° field of view (FOV). Every image is captured at 584×565 pixels, 8 bits per color plane.STARE (Structured Analysis of the Retina) database [42] database contains 20 colored retinal images, with 700×605 pixels and 8 bits per RGB channel, captured by a TopCon TRV-50 fundus camera at 35° FOV.The VARIA database [43,44] includes 233 images of 139 individuals forming 59 pairs. The images have been acquired with a TopCon non-mydriatic camera NW-100 model. The optic disc is centered in the image with a resolution of 768×584.This database has been provided by Sassoon hospital, a local government hospital in Pune, India. The database consists of 50 images forming 20 image pairs captured at different times out of which some images contain pathologies. The images are of size 1200×1200 pixels, 8 bits per RGB channel captured by Canon CF-60 DSi digital fundus camera at 40° FOV.During acquisition of the retinal images the distortion (TRS) is introduced due to variations in the patient sitting positions, tilting of head, chin cup movement, and change in distance between eye and camera. Also the parallax error due to spherical nature of retina introduces translation in the image. The DRIVE and STARE databases do not contain image pairs. Therefore we have incorporated the imaging artifacts by introducing known TRS in images to generate test images; while the database provided by local government hospital and VARIA consists of image pairs in which inherent TRS is present between the given image pair.For DRIVE and STARE database a random set of TRS is introduced to generate test images. The artifact introduced due to head tilting of the patient is simulated by introducing random rotation of −5°, 2° and 7° in the images of DRIVE and STARE databases. The effect of change in the sitting position of the patient and chin cup movement is simulated by introducing random vertical and horizontal translations (Δx1=20, Δy1=20); (Δx2=13, Δy2=18); (Δx3=17, Δy3=22) and scale is changed to 0.8, 0.9 and 1.1 to simulate movement of the patient in the direction of the principal axis. Fig. 13(a) and (d) shows typical retinal images from DRIVE and STARE database respectively. Corresponding test images generated by introducing translation of 20 pixels in horizontal and vertical directions, rotation of −5° and scale of 0.9 are illustrated in Fig. 13(b) and (e) respectively. Fig. 13(c) and (f) exhibits the unregistered pair of reference and test retinal images superimposed from DRIVE and STARE database respectively. Similarly 20 image pairs are generated for each random set of TRS introduced from DRIVE and STARE databases each.The reference and test retinal images are resized to same size. As described in Section 2.1, retinal blood vessels are segmented and thinned so that retinal vessels are of mono pixel width. Further vessel bifurcations are detected in both the images and genuine vessel bifurcations are retained as per Section 2.2. The retained vessel bifurcations are labeled in the corresponding grayscale retinal images. In case of test retinal image, all the D8 neighbors of the retained vessel bifurcation are also labeled in the grayscale test retinal image. An image area of 21×21 pixels is selected around every labeled bifurcation pixel in grayscale retinal images. Orthogonal moment invariants features of all the selected image areas are computed as per Section 2.3. The correspondence between vessel bifurcations associated with reference images and test images is established using metrics as Pearson Correlation Coefficient, Hellinger Distance as explained in Section 2.4. Minimum Hellinger Distance and maximum Pearson Correlation Coefficient is obtained for matched vessel bifurcations. Table 1lists the average percentage ratio of vessel bifurcations matched to vessel bifurcations detected for 20 image pairs generated as described earlier from DRIVE and STARE database respectively. After establishing correspondence between vessel bifurcations of image pairs, the amount of TRS in the test image is estimated using similarity transformation elaborated in Section 2.5. The estimated values of TRS are compared with the amount of TRS introduced while generating test images. Tables 3 and 4list the percentage absolute error in estimated values of TRS for 20 image pairs from DRIVE and STARE database respectively. The average percentage error for DRIVE database in estimating rotation is observed as 5.07%; for translation 6.10% (horizontal direction), 5.65% (vertical direction); and for scale 3.71%. For STARE database the average percentage error in estimating rotation is 5.54%; translation is 6.03% (horizontal direction), 6% (vertical direction); and scale is 3.83%.The test retinal image is aligned using estimated registration parameters. Fig. 14(a) shows a typical retinal image pair from DRIVE database superimposed and Fig. 14(b) shows the corresponding registered retinal image pair superimposed. Similarly a typical retinal image pair from STARE database is displayed in Fig. 15(a) and corresponding registered and aligned retinal image pair is displayed in Fig. 15(b).The performance of registration is measured in terms of mean error and standard deviation in pixels between the set of matched pairs of vessel bifurcation points. The error between a pair of matched vessel bifurcation a(xR, yR) and b(xT, yT) is computed as(36)e=(xR−xT)2+(yR−yT)2For a given image pair with n pairs of matched vessel bifurcation points, the mean error is given as(37)e¯=∑i=1n(xRi−xTi)2+(yRi−yTi)2nThe mean error of 2.39 pixels and standard deviation of 1.97 pixels is obtained for DRIVE database and the mean error of 2.97 pixels and standard deviation of 2.26 pixels is obtained for STARE database as presented in Table 5.The database obtained from local government hospital and VARIA consists of 20 and 59 image pairs respectively. In each image pair, test image has inherent TRS with respect to reference retinal image. Fig. 16(a) and (b) displays images from VARIA database with inherent TRS with respect to each other. The corresponding pair of retinal images superimposed is illustrated in Fig. 16(c). The image pair from local hospital database is shown in Fig. 17(a) and (b). Fig. 17(c) shows corresponding superimposed retinal image pair. As described in Section 2 the registration parameters are estimated for the images pairs from VARIA and database obtained from local government hospital. The images from VARIA contain relatively lesser number of vessel bifurcation points as compared to other databases used for experimentation. Table 2presents the average percentage ratio of vessel bifurcations matched to vessel bifurcations detected for the image pairs from VARIA and hospital database. Further the registration parameters are estimated and test image is aligned. The registration and alignment results of typical retinal image pairs from VARIA and local hospital database are presented in Figs. 18(a), (b) and 19(a), (b)respectively. The proposed algorithm yields mean error of 2.83 pixels and standard deviation of 2.04 pixels for VARIA database while for database obtained from local government hospital the mean error obtained is 3.05 pixels and standard deviation is 2.17 pixels as presented in Table 5. A typical retinal image pair not successfully registered is shown in Fig. 20. The performance of the algorithm is also evaluated against use of lesser number of matched control points for registration. The mean error and standard deviation is computed by reducing the number of matched vessel bifurcations used for registration to 90%, 80% and 60% of the total available matched vessel bifurcations. The results for all four databases are presented in Table 6. The standard deviations obtained by using 60% of matched vessel bifurcations are reported as 3.17, 3.22, 4.83 and 2.74 for DRIVE, STARE, VARIA and local hospital database respectively.Also though the vessel bifurcations are translated due to parallax, the moment invariant features representing the bifurcation will remain unchanged. The proposed algorithm identifies the genuine matched vessel bifurcation pair using the moment invariants features. In case of DRIVE and STARE databases the average error in estimating introduced translation is less than 2 pixels. The mean registration error for all four databases used is less than 3 pixels except 3.05 for hospital databases. Therefore error introduced due to parallax is minimized considerably.The time required for registering a typical image pair includes the time consumed for segmentation and thinning of retinal vasculature, detection of genuine vessel bifurcation points, establishing correspondence between the bifurcations of image pair and estimating the registration parameters. For a typical image of size 584×565 the time required for segmentation of retinal vasculature, thinning and detection of vessel bifurcations is approximately 11s. If m and n are number of retained vessel bifurcations in reference and test retinal image respectively, the number of comparisons of orthogonal moment invariants feature vectors required to identify the genuine match for every vessel bifurcation is m×n×8 as each bifurcation from the reference image is compared with all D8 neighbors of every bifurcation in test image. As the size of the feature vector is (7×1) the time required for identifying the one out of n matching vessel bifurcation is reduced. Given a typical segmented thinned retinal image from DRIVE database with 32 vessel bifurcations retained, the time required to align the image pair is approximately 10s (32×32×8 comparisons). For a typical segmented image from STARE database with 28 vessel bifurcations retained, the total time required for establishing correspondence between vessel bifurcations of reference and test retinal image, estimating registration parameters and alignment is around 8s. The registration of a typical segmented image pair from hospital database requires 9.5s. The images from VARIA contain relatively lesser number of vessel bifurcation points as compared to other databases used for experimentation. For VARIA database approximately 7s are required for registering the typical image pair.

@&#CONCLUSIONS@&#
The proposed algorithm describes moment invariants features for registration of retinal images. The correspondence between vessel bifurcation points of test and reference retinal images is established by identifying the one out of n vessel bifurcation points from reference image that matches with the vessel bifurcation point from test retinal image using third order moment invariants features. The registration parameters (TRS) are estimated using first order similarity transformation applied to coordinates of matched pairs of vessel bifurcation points. The test retinal image is aligned with reference retinal image based on estimated registration parameters. The performance of the algorithm is reported for four databases. The average error in registration reported for all four databases is less than three pixels with maximum standard deviation of 2.26 considering all matched pairs of control points. Similarly the algorithm yields maximum deviation of 4.83 considering 60% of matched pairs of control points. From the reported results the moment invariants features are proved effective for retinal image registration. The algorithm also works satisfactorily even in case of lesser number of control points.Even though the resolution, size and average contrast in the images from four databases used in experimentation are different, some of the images are affected by non uniform illumination as well as some images contain pathologies, this algorithm works effectively.In registration algorithms, most of the computations are required for matching the control points. In the present algorithm the process of finding matched pair of control points involves identification of one out of n bifurcation point based on comparison of feature vectors. This apparently takes lesser computations and reduces the overall registration time.From the experimental results it indicates that orthogonal moment invariants features are proved to be effective for registration of retinal images considering overall accuracy, computational complexity and availability of lesser number of control points.There are no conflicts of interests.