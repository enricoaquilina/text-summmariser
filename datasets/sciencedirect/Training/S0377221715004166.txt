@&#MAIN-TITLE@&#
Scheduling for data gathering networks with data compression

@&#HIGHLIGHTS@&#
Data gathering networks with data compression are studied.The scheduling problem is to transfer all data in given time at the minimum cost.The decision version of the problem is proved to be NP-complete.Polynomial-time heuristics are proposed and tested experimentally.

@&#KEYPHRASES@&#
Scheduling,Data gathering networks,Data compression,

@&#ABSTRACT@&#
This paper analyzes scheduling in a data gathering network with data compression. The nodes of the network collect some data and pass them to a single base station. Each node can, at some cost, preprocess the data before sending it, in order to decrease its size. Our goal is to transfer all data to the base station in given time, at the minimum possible cost. We prove that the decision version of this scheduling problem is NP-complete. Polynomial-time heuristic algorithms for solving the problem are proposed and tested in a series of computational experiments.

@&#INTRODUCTION@&#
Data gathering wireless sensor networks (WSNs) find a broad variety of applications in monitoring the environment, surveillance and other areas (Akyildiz, Su, Sankarasubramaniam, & Cayirci, 2002). A network consists of a set of measuring sensors and a base station to which the collected data should be transferred. As wireless sensors have only limited power and computing capability, these resources have to be carefully managed to ensure network efficiency. Lower energy usage or network delay can be achieved by applying specific protocols of communication (Ergen & Varaiya, 2010; Kumar & Chauhan, 2011; Shi & Fapojuwo, 2010; Wu, Li, Liu, & Lou, 2010). Another approach is constructing scheduling algorithms for the whole data gathering application. On the basis of the divisible load theory, algorithms were created for minimizing data gathering time (Choi & Robertazzi, 2008; Moges & Robertazzi, 2006) and maximizing network lifetime (Berlińska, 2014). Column generation algorithms for maximizing network lifetime were also proposed (Alfieri, Bianco, Brandimarte, & Chiasserini, 2007; Rossi, Singh, & Sevaux, 2013). One more method for increasing network performance is compressing the gathered data before sending it to the base station in order to decrease the communication time. Since general compression algorithms are often not applicable for sensor networks because of scarce resources, several compression algorithms were specifically designed for WSNs (Kimura & Latifi, 2005). Compressive sensing theory was applied to data gathering wireless sensor networks, taking advantage of the correlations among the sensor data, in paper (Luo, Wu, Sun, & Chen, 2009). This research direction became very popular, resulting in more advanced data gathering approaches based on compressive sensing (Cheng, Ye, Jiang, Wang, & Wang, 2013; Wang, Tang, Yin, & Li, 2012; Xiang, Luo, & Rosenberg, 2013; Xu, Wang, & Wang, 2011).In this paper we do not aim at constructing one more method of compressing the data gathered by a wireless sensor network. Instead, our goal is to provide and analyze a simple theoretical model of data gathering networks with data compression. In our model each node of the network is able to preprocess the gathered data in order to compress it before sending to the base station, at monetary or energy cost proportional to its size. Let us note that this model can be applied not only to wireless sensor networks but also to wired networks whose nodes collect some data or obtain them as a result of computations. We analyze the problem of sending the whole data gathered by the network to the base station in given time, at the minimum possible cost of data compression. We prove that the decision version of this problem is NP-complete. Three polynomial-time heuristics are proposed. Their performance and its dependence on system parameters are tested in a series of computational experiments.The rest of this paper is organized as follows. In Section 2 we propose a model of data gathering networks with data compression and formulate the scheduling problem. Its computational complexity is studied in Section 3. The following section describes the algorithms we propose for solving the problem. Section 5 comprises the results of computational experiments. The last section is dedicated to conclusions.We analyze a data gathering network consisting of a set of m identical nodesP={P1,…,Pm}and a single base station to which the collected information should be passed. The words node, sensor and processor will be used interchangeably. The size of data gathered on node Piis αi. Each node can preprocess its data, using application-specific knowledge, to decrease its size to γαiin time Aαi, where 0 < γ < 1. However, this comes at cost fαi. A node that compresses its data can start sending it only after the compression process is finished. Although all sensors are identical, their locations are different, and hence, the communication capabilities are not the same for all of them. Sensor Pitransmits data of size x in time xCi. Only one sensor can communicate with the base station at a time. Constructing a schedule for data gathering in the described network consists in choosing which nodes should compress their data and what should be the order of communications with the base station. Our goal is to create a schedule for transmitting all data in given time, at the minimum possible cost. This problem can be formulated as follows.Problem 1(DG-Compr-OptF). Given m nodes, their parameters A, f, γ, (Ci)1≤i≤m, (αi)1≤i≤mand a rational number T, what is the minimum cost necessary to send all data to the base station within time T? What schedule allows for achieving this cost?In the next section we will analyze the complexity of the following decision version of problem DG-Compr-OptF.Problem 2(DG-Compr). Given m nodes, their parameters A, f, γ, (Ci)1≤i≤m, (αi)1≤i≤mand two rational numbers T, F, is it possible to send all data to the base station within time T at cost not greater than F?As all nodes of the network execute the same data gathering application, it is often the case that the amounts αiof collected data are the same for all sensors. Therefore, we will also study the complexity of the following subproblem representing this situation.Problem 3(DG-Compr{αi=α}). Given m nodes, their parameters A, f, γ, (Ci)1≤i≤m,αi=αfor 1 ≤ i ≤ m and two rational numbers T, F, is it possible to send all data to the base station within time T at cost not greater than F?In this section we study the computational complexity of the formulated scheduling problems. Let us start with the observation that DG-Compr is a selection problem, i.e. if a subsetP′⊂Pof processors that compress their data is given, it is easy to check if it allows for constructing a feasible solution. Indeed, the cost of compression for givenP′can be computed straightforwardly and compared with F. It remains to check if the shortest possible schedule forP′is not longer than T. Node Pjis ready to start communication with the base station at time rj, whererj=0ifPj∉P′andrj=AαjifPj∈P′. As all sensors send their data sequentially, constructing the shortest schedule is equivalent to solving problem 1|rj|Cmax, i.e. scheduling on one processor with job release times and makespan criterion. This can be done in O(mlog m) time by sorting the processors according to nondecreasing rj(Błażewicz, Ecker, Pesch, Schmidt, & Węglarz, 2007).We will now prove that problem DG-Compr{αi=α}, and hence the more general problem DG-Compr, is NP-complete. Afterwards, it will be shown that problem DG-Compr{αi=α} can be solved in polynomial time if all nodes have the same communication rate (Ci=Cfor all 1 ≤ i ≤ m). Finally, we will prove that problem DG-Compr remains NP-complete even if all nodes have the same communication rate and data compression takes no time.Proposition 1Problem DG-Compr{αi=α} is NP-complete.It is obvious that DG-Compr{αi=α} belongs to NP. Indeed, as explained above, given a subset of processors that compress their data, we can create a schedule and verify its feasibility in O(mlog m) time, which is polynomial.We prove that DG-Compr{αi=α} is NP-complete via a reduction from the NP-complete Partition problem (Garey & Johnson, 1991), which is defined as follows.Problem 4(Partition). Given a finite set of m integersB={b1,…,bm},is there a subsetB′⊂{1,…,m}such that∑i∈B′bi=∑i∉B′bi=∑i=1mbi/2?Given an instance of Partition, we construct an instance of DG-Compr{αi=α} in the following way. Letα=f=1,Ci=bifori=1,…,m,A=∑i=1mbi/2,F=m. Parameter γ can be an arbitrary positive number smaller than one. LetT=(1+γ)∑i=1mbi/2. Note that F is so big that in order to solve the problem it is sufficient to find a schedule of length not exceeding T. LetP′be the subset of nodes which compress their data in some solution of the analyzed instance of DG-Compr{αi=α}. LetB′denote the set of indices of nodes contained inP′. The processors from subsetP∖P′can start communications at time 0, whereas the processors contained inP′can start sending their data only after time Aα. Hence, the schedule length is(1)t(B′)=max{∑i∉B′Ciα,Aα}+∑i∈B′Ciγα=max{∑i∉B′bi,∑i=1mbi/2}+γ∑i∈B′bi.If∑i∉B′bi<∑i=1mbi/2,then(2)t(B′)=∑i=1mbi/2+γ∑i∈B′bi>(1+γ)∑i=1mbi/2=T.For∑i∉B′bi>∑i=1mbi/2we obtain(3)t(B′)=∑i∉B′bi+γ∑i∈B′bi=γ∑i=1mbi+(1−γ)∑i∉B′bi>γ∑i=1mbi+(1−γ)∑i=1mbi/2=(1+γ)∑i=1mbi/2=T.Finally, if∑i∉B′bi=∑i=1mbi/2,then(4)t(B′)=∑i=1mbi/2+γ∑i∈B′bi=(1+γ)∑i=1mbi/2=T.Hence, the required schedule of length not exceeding T exists if and only if there exists a subsetB′⊂Bsuch that∑i∈B′bi=∑i∉B′bi=∑i=1mbi/2,which ends the proof.□Problem DG-Compr is NP-complete.Problem DG-Compr{αi=α} can be solved in O(1) time ifCi=Cfor 1 ≤ i ≤ m.Let us note that in a system with equal data sizes and equal communication rates the schedule length and cost depend only on the number n of nodes that compress their data. In order to achieve cost not exceeding F, it must hold that n ≤ nf, where(5)nf=⌊F/(fα)⌋.The n nodes compressing their data can start communications after time Aα, whereas the remainingm−nsensors can start sending data at once. Hence, the schedule length for given n is equal to(6)t(n)=max{(m−n)Cα,Aα}+nCγα.We will now distinguish two cases to compute the minimum length of a schedule with cost not exceeding F.1.Let us first assume thatn≤m−A/C. This means that(m−n)Cα≥Aαand hence,(7)t(n)=(m−n)Cα+nCγα=(mC−(1−γ)nC)α.As1−γ>0,t(n) decreases with increasing n and has the smallest value whenn1=min{⌊m−A/C⌋,nf}nodes compress data.Ifnf≥m−A/C,letn≥m−A/C. In this case the schedule length equals(8)t(n)=Aα+nCγαand increases with growing n. Hence, it is best to choosen2=⌈m−A/C⌉processors to use compression.It follows from the above considerations that in order to solve problem DG-Compr{αi=α} withCi=Cfor 1 ≤ i ≤ m it is enough to compute values t(n1) and t(n2) (if n2 ≤ nf) and compare them with the maximum allowed schedule length T. These operations take O(1) time.□Problem DG-Compr is NP-complete even ifCi=Cfor 1 ≤ i ≤ m andA=0.We will again construct a reduction from the Partition problem. LetC=f=1,αi=bifori=1,…,m,A=0,F=∑i=1mbi/2. Parameter γ can be an arbitrary positive number smaller than one. LetT=(1+γ)∑i=1mbi/2. LetP′be the subset of processors compressing their data in some solution of the above instance of DG-Compr and letB′denote the set of indices of nodes inP′. As data compression takes no time (A=0), the schedule length for givenB′is(9)t(B′)=∑i∉B′bi+γ∑i∈B′bi.If∑i∈B′bi<∑i=1mbi/2,then(10)t(B′)=γ∑i=1mbi+(1−γ)∑i∉B′bi>γ∑i=1mbi+(1−γ)∑i=1mbi/2=(1+γ)∑i=1mbi/2=T.For∑i∈B′bi>∑i=1mbi/2the cost of data compression is(11)f(B′)=∑i∈B′bi>∑i=1mbi/2=F.Finally, if∑i∈B′bi=∑i=1mbi/2,then we obtain a schedule with cost(12)f(B′)=∑i∈B′bi=∑i=1mbi/2=Fand lengtht(B′)=∑i∉B′bi+γ∑i∈B′bi=(1+γ)∑i=1mbi/2=T.Thus, a feasible solution of the scheduling problem exists if and only if a solution of the corresponding instance of Partition exists, which ends the proof.□In this section we propose scheduling algorithms for solving problem DG-Compr-OptF. As we explained at the beginning of Section 3, if subsetP′of nodes that compress their data has been selected, the shortest possible schedule can be constructed in O(mlog m) time. Therefore, all presented algorithms consist in constructing some candidates for subsetP′and checking if the obtained schedule lengths are acceptable. A feasible candidate solution with the lowest cost is selected as the final result.The optimum solution of problem DG-Compr-OptF can be easily found in exponential time. The algorithm checks all 2msubsetsP′⊂Pand solves the corresponding instances of problem 1|rj|Cmax to find all feasible solutions. Thus, the algorithm runs in time O(2mmlog m). This can be reduced by checking subsetsP′in the minimal change order (Kreher & Stinson, 1998) in the following way. The first subset isP′=∅,which means thatrj=0for all j in the corresponding instance of 1|rj|Cmax, and the nodes can communicate with the base station in an arbitrary order. Every time we move to the next subsetP′(which can be generated in O(m) time), value rjis changed for only one index j. Therefore, instead of sorting the whole set of processors according to rjin time O(mlog m), we move a single processor to the correct place in the list of communications from the previous step, in time O(m). As computing the schedule length for a given order of communications takes O(m) time, the whole exact algorithm for solving problem DG-Compr-OptF runs in time O(m2m).As the exact algorithm has exponential complexity and hence cannot be used in many practical situations, we also propose greedy heuristics. The idea of the first algorithm, which will be called greedy-Cα, is to include in subsetP′the nodes for which timeCiαi(1−γ),by which communication becomes shorter due to data compression, is as big as possible. This way, we may obtain a feasible solution using a small subsetP′. Thus, the algorithm sorts the processors according to non-increasing values of Ciαi. Let us denote the ith processor in the sorted sequence by P[i]. At the beginning of the algorithmP′=∅. If this allows for creating a feasible schedule, the algorithm has found the optimum solution with cost 0 and returns it. In the opposite case, the main part of the algorithm begins. In the ith iteration processor P[i] is inserted to subsetP′and the new schedule length is computed. If it is not greater than constraint T, then the current solution is returned. In the opposite case the algorithm compares the current schedule with the shortest schedule already found. If adding processor P[i] to subsetP′did not deliver a new best solution, this processor is removed fromP′. This procedure continues until a feasible solution has been found or m steps have been made. Let us note that for some instances the algorithm may find no feasible schedule although such a schedule exists.The idea of the second greedy algorithm, called greedy-α, is to start with compressing the smallest data parts, in order to keep the compression costs low. Thus, the processors are sorted according to non-decreasing αi. The remaining steps of the algorithm are the same as in greedy-Cα. Again, the algorithm always finds the optimum solution in the case when no compression is necessary, and is not guaranteed to find feasible solutions for all instances.As it is probable that one of the two above algorithms is better for some type of input data, and the other one for a different type of instances, we will also analyze their combination. Algorithm greedy-min consists in running algorithms greedy-Cα and greedy-α and choosing the better of the two solutions found.Let us now analyze the computational complexity of the proposed heuristics. Both algorithms greedy-Cα and greedy-α first sort processors in time O(mlog m) and then construct at mostm+1schedules, starting with the case when no processors compress their data, i.e.P′=∅. Each of the following subsetsP′is constructed from the previous subsetP′by adding one processor and, optionally, removing one processor. Thus, as explained in Section 4.1, sorting processors is not necessary for computing the order of communications, and the feasibility of the current solution can be tested in O(m) time. Hence, the running time of algorithms greedy-Cα and greedy-α, and in consequence greedy-min, is O(m2).To finish this section, let us remind that in order to run the proposed heuristics in an actual data gathering network, the base station needs the information about the amounts of data collected by all nodes. If these amounts are known in advance, a schedule may be found and sent to the sensors before the data gathering application starts. In the opposite case each sensor must report to the base station the size of data it gathered. Then, the base station has to compute a schedule and inform all sensors whether or not they should compress their data. As the sizes of necessary messages are very small and the computational complexity of the algorithms is low, we assume that the time necessary to perform these actions is negligible in comparison to the whole application running time.In this section we present the results of computational experiments conducted to examine the quality of solutions generated by the scheduling algorithms described in Section 4.2. All algorithms have been implemented in C++ in Microsoft Visual Studio 2012. As the heuristics are not guaranteed to find feasible solutions, we report on the percentage of solved instances as well as the average quality of found solutions and its 95 percent confidence interval (Frank & Althoen, 1994). Schedule quality is measured by the ratio F/F*, where F is the cost of the solution found by the analyzed heuristic algorithm and F* is the optimum, minimal cost found by the exact algorithm described in Section 4.1. Hence, smaller number means better quality. Let us remind that it is possible thatF*=0for some instances, which means that no data compression is necessary to fit in the time limit. However, as explained in Section 4.2, in this case we also haveF=0for all proposed heuristics. Therefore, we defineF/F*=1ifF*=0.Unless stated otherwise, we use the following reference experiment setting. The system comprisesm=20nodes. The compression rate isA=1E−6 seconds/byte, and the compression cost isf=1/byte. Compression efficiency parameter isγ=0.4,which means that data compression is very effective. Such a low value was chosen in order to create difficult instances, for which it is profitable to compress data on many nodes. Node communication rates Cjare selected randomly from the uniform distribution U[1E−8, 1E−7] (in seconds per byte) and the amounts of data αjare chosen from the uniform distribution U[1E6, 1E8] (in bytes). In order to generate only instances having feasible solutions, the following procedure was applied. After selecting the system parameters, the minimum possible schedule length T* was computed by an exponential algorithm analyzing all subsetsP′⊂P. Then, constraint T was set to δTT* for some δT≥ 1. In the reference experiment configuration we useδT=1.1. Each point on the charts presented in the further text represents an average of 100 instances. In total, 3700 test instances were generated and solved.In the first set of experiments we analyzed the influence of parameter δTon the performance of the algorithms. The results are presented in Fig. 1. Value δTdetermines to a large degree the number of feasible solutions. Indeed, forδT=1only the shortest possible schedules are acceptable. On the other hand, when δTbecomes very big, all solutions fit in time constraint T. Hence, it could be expected that the fraction of instances solved by algorithm greedy-Cα grows with increasing δT, as shown in Fig. 1a. It turned out that algorithm greedy-α (and, in consequence, greedy-min) is surprisingly effective in finding feasible schedules. Out of all experiments presented in this work only two instances were not solved by this algorithm, both withδT=1. Therefore, we decided not to present the fraction of instances solved by greedy-α and greedy-min on the charts. It seems that starting with compressing small amounts of data is a safer way of solving the problem than trying to decrease the communication time as much as possible, as in algorithm greedy-Cα. Indeed, selecting big values of Ciαioften means compressing large amounts of data αi, which makes the message sent by sensor Pimuch shorter, but also delays its communication until time at least Aαi, which is also quite big. This may lead to idle times in communication with the base station and obtaining too long schedules. The effect of introducing idle intervals in communication by algorithm greedy-Cα will be also visible in some of the further experiments.The quality of solutions delivered by all the three algorithms is best when δTis very small or very large. This can be explained in the following way. WhenδT=1,the shortest possible schedule must be found. In most cases there are at most a few feasible solutions, with similar costs. Thus, if a solution is found, then its quality is good. When δTincreases, there is more freedom in constructing the schedules and their quality gets worse. However, when constraint T is big, data compression may become unnecessary. Indeed, in 87 percent tests withδT=1.20and 99 percent instances withδT=1.25no processor compressed its data in the optimum solution. As described in Section 4.2, all our algorithms construct the optimum schedule in this case. Hence, average solution quality is very close to 1 forδT=1.25and the confidence intervals are so short that they are not visible on the plots.It is worth noting that although the quality of solutions found by algorithms greedy-α and greedy-Cα is comparable, the quality of algorithm greedy-min is substantially better, which confirms our supposition that none of the two basic greedy algorithms dominates the other for all (or almost all) test instances. Still, it seems that the performance of algorithm greedy-Cα is less stable than that of greedy-α or greedy-min, as the confidence intervals obtained for greedy-Cα are usually much longer than for the other two algorithms. These observations will be confirmed by the results of the remaining experiments.Fig. 2presents the results of experiments with changing parameter γ. Let us remind that small γ means that data compression is very effective and significantly decreases the amount of data to be sent, whereas when γ is big, only a little time can be gained by compressing the data. We noticed that for γ > 0.6 data compression is often disadvantageous: 82 percent of instances withγ=0.7and all instances withγ=0.8were solved optimally by executing no data compression at all. Thus, the algorithms reach quality 1 forγ=0.8or greater. When γ gets smaller, the optimum schedules are more complicated and hence, more difficult to construct. In consequence, the quality of solutions created by the heuristics is worse. It seems that the value of γ does not influence the quality of algorithm greedy-Cα when γ ≤ 0.7, as no trend is visible in the plot and all confidence intervals intersect. The reason may be that the fraction of solved instances increases for growing γ. Solving a larger number of more difficult tests may counteract the effect of improving solution quality for easier instances.The quality of the generated schedules for different values of parameter A is shown in Fig. 3. It turned out that when A ≥ 2E−6, the compression is so slow in comparison to communication that it is not profitable at all. Hence, all algorithms find the optimum solutions. For very fast compression (A=1E−7, 2E−7) very good results are obtained by algorithm greedy-Cα. This is caused by the fact that the delays introduced by compression of big pieces of data are quite small and do not lead to idle times in communication. Compressing data on a few processors gives smaller costs than compressing many small parts of data which give smaller gain in communication time. The quality of greedy-min is very similar to greedy-Cα, which means that greedy-α only very rarely outperforms greedy-Cα in this case. ForA=5E−7 we observed a huge drop in the fraction of instances solved by algorithm greedy-Cα (see Fig. 3a). It seems that this is a critical value for which long idle times in communication appear as a consequence of compressing too many large pieces of data and increase the schedule length. As a result, the quality of algorithm greedy-min is almost the same as for greedy-α, because only a small fraction of solutions can be improved by greedy-Cα. When A reaches 1E-6, smaller numbers of nodes are selected for compression by algorithm greedy-Cα, the percentage of solved instances increases, and greedy-min again clearly outperforms greedy-α.In the last set of experiments we analyzed the influence of processor number m on the performance of the heuristics (see Fig. 4). As could be expected, increasing m makes the problem more difficult to solve. Indeed, when the number of processors is small, at most a few of them can compress their data if idle times in communication are to be avoided. Good schedules are then easy to find. When the system size increases, the optimum schedules involve more compression and the quality of found solutions is worse. It may seem that the quality of solutions generated by greedy-Cα does not significantly change for m > 15. However, this is a result of decreasing fraction of solved instances, similar to what was seen in Fig. 2a for small γ. Algorithm greedy-Cα finds no feasible solutions for the most difficult tests with big m.

@&#CONCLUSIONS@&#
In this work we studied scheduling for data gathering networks with data compression. We proposed a mathematical model of such networks, formulated scheduling problem DG-Compr-OptF and proved the NP-completeness of its decision version. A subproblem that can be solved in O(1) time was singled out. We proposed an exponential exact algorithm for solving DG-Compr-OptF, as well as three greedy heuristics running in time O(m2). The algorithms were tested in a series of computational experiments. It turned out that algorithm greedy-α is very good at finding feasible solutions of the problem. It failed in only 2 of 3700 instances, which is less than 0.06 percent. Algorithm greedy-Cα fails much more often, but is creates better solutions than greedy-α for many instances. Therefore, the average quality of the combination algorithm greedy-min is substantially better than the average qualities of its both components. For almost all of our test instances the cost of the solution obtained by greedy-min was smaller than 1.5 times the optimum cost. We showed that the quality of schedules delivered by all algorithms depends on the system parameters. It is best when only a small number of processors should compress their data or when compression is very fast.Future research in this area may include analyzing the approximability of the formulated scheduling problem. Another interesting direction is constructing heuristics for solving the problem in systems with heterogeneous compression speeds and costs.