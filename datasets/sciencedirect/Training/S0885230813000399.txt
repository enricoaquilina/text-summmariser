@&#MAIN-TITLE@&#
Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery

@&#HIGHLIGHTS@&#
We present our approach to unsupervised training of HMM-based speech recognizers in domains where transcriptions do not exist.Our approach can be viewed as an extension of existing maximum likelihood HMM training.We propose an iterative learning algorithm using existing HMM trainers and recognizers.We demonstrate the utilities of the approach on applications in topic identification and keyword discovery.

@&#KEYPHRASES@&#
Unsupervised training,Keyword discovery,Self-learning,Speech recognition,Topic identification,

@&#ABSTRACT@&#
We present our approach to unsupervised training of speech recognizers. Our approach iteratively adjusts sound units that are optimized for the acoustic domain of interest. We thus enable the use of speech recognizers for applications in speech domains where transcriptions do not exist. The resulting recognizer is a state-of-the-art recognizer on the optimized units. Specifically we propose building HMM-based speech recognizers without transcribed data by formulating the HMM training as an optimization over both the parameter and transcription sequence space. Audio is then transcribed into these self-organizing units (SOUs). We describe how SOU training can be easily implemented using existing HMM recognition tools. We tested the effectiveness of SOUs on the task of topic classification on the Switchboard and Fisher corpora. On the Switchboard corpus, the unsupervised HMM-based SOU recognizer, initialized with a segmental tokenizer, performed competitively with an HMM-based phoneme recognizer trained with 1h of transcribed data, and outperformed the Brno University of Technology (BUT) Hungarian phoneme recognizer (Schwartz et al., 2004). We also report improvements, including the use of context dependent acoustic models and lattice-based features, that together reduce the topic verification equal error rate from 12% to 7%. In addition to discussing the effectiveness of the SOU approach, we describe how we analyzed some selected SOU n-grams and found that they were highly correlated with keywords, demonstrating the ability of the SOU technology to discover topic relevant keywords.

@&#INTRODUCTION@&#
The training of a speech recognizer is a way of modeling a speech corpus and as such is a way of imparting structure to the speech data. We exploit this structure by working with the tokens representing sounds or sound patterns that the recognizer produces. Typically the structure is imposed by having a transcription and a pronunciation dictionary of the speech corpus. The premise of our work is that even in the absence of a transcription and a dictionary one can effectively create models for speech applications that exploit the existing sound pattern structure of speech. The approach we take results in the development of sound units that are iteratively optimized for the acoustic domain of interest using the maximum likelihood (ML) criterion. We refer to these sound units as self-organizing units or SOUs. There is a growing interest in unsupervised training methods for speech, and HMM-based approaches are not the only option. An alternative approach for the unsupervised modeling of speech was originally developed by Park and Glass (2005) and more recently by Jansen et al. (2010) which is a non-parametric approach that is related to the Dynamic Time Warping (DTW) technology for speech recognition. Just as HMM recognizers differ significantly from DTW recognizers, similar differences exist between these two approaches.Below we discuss the difficulties for topic classification that may be encountered if a recognizer is trained using transcribed data collected at mismatched conditions. We then focus on some of the details of the SOU recognition systems. Many spoken language processing applications, such as spoken topic classification, use automatic speech recognition (ASR) as the first stage of processing to convert speech into tokens for down-stream processing. This works well when the characteristics of the classification data match that of the recognizer, which requires transcribed training data from the domain of the classification data. In some cases, transcribed speech may not be available in the specific domain, channel or language. While it is possible to use a tokenizer from a different language or channel, the system performance will be highly dependent on the level of mis-match between the tokenizer training and the classification data.In Hazen et al. (2007), significant degradation is reported on topic classification experiments when using a Hungarian phoneme tokenizer on English data even when both the tokenizer training and test data were on telephone channels. In addition to potential language mis-match, results from robust speech recognition suggest that mis-matches in environmental conditions, such as channel or noise could also cause significant performance degradation.An alternative is to build an acoustic tokenizer in an unsupervised fashion. Because transcribed data are no longer needed, it is possible to train the tokenizers using the domain-specific data related to classification, and thus significantly reduce the potential for mis-match. The GMM-tokenization (Zissman, 1993) is a special case of tokenization training without transcription and has been successfully applied in language identification.In Belfield and Gish (2003), we introduced the idea of unsupervised training of a multi-frame tokenizer. We built a topic classifier using an unsupervised segmental Gaussian mixture model (SGMM) to tokenize automatically derived multi-frame segments. This mixture of segmental models was the result of clustering multi-frame segments. The multi-frame segmental units were critical because they act like phones and their n-grams captured keywords which were needed for topic discrimination. However, the segmental technology is limited by its inability to benefit directly from improvements made in the more traditional HMM-based speech-to-text (STT) systems. Improvements, such as vocal tract length normalization, speaker adaptation, discriminative training, among others have been shown to improve recognition significantly but will need to be “re-invented” for segment models. On the other hand, using the traditional HMM-based recognizer as tokenizer would require transcribed training data.To benefit both from the unsupervised training in the segmental approach, and the progress made in HMM-based speech recognition technology, we propose in this paper an unsupervised HMM training approach that jointly optimizes the observation likelihood and the label sequence without any transcribed training data. Under this framework, training can be iteratively performed using existing HMM recognition tools. The HMMs can transcribe the audio into a sequence of self-organizing speech units (SOUs) using only untranscribed speech for training, and the resulting unit sequence can be used for topic identification (TID). One significant advantage of completely unsupervised training is that there will not be any mis-match between training and test, because the untranscribed test data can, if needed, be added to acoustic training. Because SOU training uses a standard HMM training framework, it benefits from improved ASR techniques such as speaker adaptation and context modeling. If some transcribed audio is available, semi-supervised training (Lamel et al., 2002; Zavaliagkos et al., 1998; Ma and Schwartz, 2008; Li et al., 2007) can be used, which is similar to our unsupervised approach with the exception that the model would be initialized using a small set of transcribed audio.Our first set of topic identification11We are in fact doing a mix of topic verification and classification. We apply the term “identification” in a broad sense that includes both verification and classification.experiments were performed on a ten-topic subset from the Switchboard corpus with performance measured by the average equal error rate (EER) across the topics. The proposed approach resulted in an EER of 12.5% that outperformed the Hungarian phoneme recognizer (16.7% EER) as well as an English phoneme recognizer trained with 1h of transcribed speech (14.7% EER). With the addition of context dependent acoustic models and lattice re-scoring in SOU recognition and lattice-based n-gram SVM features for TID, our TID EER was improved from 12% to 6.6%. To better understand the TID EER, we also performed an oracle experiment using the phoneme sequence from the true transcripts which gave a TID EER of 1.2%. In addition to the Switchboard corpus, we also performed TID experiments on the Fisher corpus for comparison with published TID results.Transcribing audio into SOUs also makes it possible to perform topic keyword discovery. In Nowell and Moore (1995), a dynamic programming approach was proposed to discover topic-relevant phoneme sequences. A similar approach has also been proposed in Park and Glass (2005) and Zhang and Glass (2010) that searches for “similar” regions of speech from in-topic audio recordings. In this paper, we discuss how to discover topic-relevant SOU-ngrams via features selected by the SVM-topic classifier.We describe the unsupervised HMM learning algorithm in Section 2. We then describe the SOU implementation, and the downstream topic classification system in Section 3. Experimental results are reported in Section 4. We then describe our work in keyword discovery in Section 5. In Section 6, we summarize the paper and describe possible further improvements.Denote the HMM parameters as θ=[θam, θlm], which include both the acoustic model parameters, θam, and the language model parameters, θlm. In typical supervised HMM training using maximum likelihood criterion, the acoustic model likelihood, p(X|W, θam), and language model likelihood p(W|θlm) are denoted as two separate sets of parameters because the parameters can be easily decoupled and maximized separately.22Parameter estimation using other criteria, such as discriminative training will have different formulation.To simplify our notation in this paper, we merge the two maximization into a single one over θ on the joint likelihood, p(X, W|θ) such that the maximum likelihood (ML) parameter estimation finds the parameter,θˆsup, that maximizes the joint likelihood of observation X and the label sequence W.Mathematically, we can express the ML parameter estimation as(1)θˆsup=argmaxθp(X,W|θ),in which both the acoustic observation X and transcription W are known in training time. If one views the representation of speech as a big network of multi-state HMMs, the word sequence simply imposes constraints on the state sequence allowable for each training utterance, and the language model probability can be captured by the transition probability between certain word end and word begin nodes. In the case of unsupervised training in which the label sequence W is not known, we maximize the joint likelihood by searching not only over the model parameters but also all possible label sequences. That is, W becomes a variable to be optimized. The unsupervised ML parameter estimation becomes,(2)θˆunsup=argmaxθmaxWp(X,W|θ),(3)θˆunsup=argmaxθmaxWp(X|W,θ)p(W|θ)The maximization over both the label sequence (language model term) and the acoustic model likelihood in Eq. (3) balances the acoustic likelihood and label sequence structure. At one possible extreme, one can use different symbols for each frame to maximize the acoustic likelihood at the expense of high entropy in the symbol sequence. At the other extreme, one can define only one symbol to maximize the language model likelihood but this will result in low acoustic likelihood.33We assume the number of parameters per label is fixed.The balance between these two is influenced by the choice of the initial label sequence and the complexity of the acoustic and language models.Eq. (3) maximizes over two sets of variables, θ and W, which can be performed iteratively. At each iteration, we keep one set of variables fixed while maximizing over the other set and then alternate between them. So, at the ith iteration, the two maximization steps are:1.Find the best parameters θion the previously found label sequence Wi−1.(4)θi=argmaxθp(X,Wi−1|θ).Find the best word sequence Wiby using the previously estimated parameters θi.(5)Wi=argmaxWp(X,W|θi),Comparing Eqs. (1) and (4), it is obvious that Step 1 (Eq. (4)) is simply the regular supervised HMM training (both acoustic and language model) using the newly obtained transcription Wi−1 as reference. Finding the best word sequence in the second step would suggest a Viterbi recognition pass, although recognition is usually viewed as finding the most likely label sequence over the posterior probability, p(W|X, θ). However, it is easy to show that the same sequence also maximizes the joint likelihood p(X, W|θ) as in Eq. (5). So, Eq. (5) effectively expresses the recognition of a new transcription using the updated parameters θi.44We ignored all the approximations typically associated with practical recognition systems, such as using language modeling weights, language model smoothing, or pruning.The iterative learning process is illustrated in Fig. 1, in which the process starts with an initial sequence of labels, followed by repeated model training and decoding. This suggests that once we have initialized the process, we can use existing recognition tools to perform unsupervised HMM training.The process can be initialized either with an initial model or an initial label sequence. Obtaining an initial model (without an initial sequence) with something like a flat start in an HMM is more difficult because there is really no information to differentiate between the units. Instead, it is easier to use another tokenizer (not necessarily an HMM) to create an initial label sequence. In many iterative maximization schemes, the quality of the initialization can have significant impact on final model quality. For the proposed unsupervised training, the initial label sequence is particularly important because it also defines the set of units to be learned. Multi-frame phone-like units are consistent with the ASR acoustic representations, and their n-grams can capture keywords that are critical for topic classification.The use of phone-like units does not preclude us from using bigger units in recognition. Similar to supervised STT where words are mapped to a sequence of phonemes via a dictionary, one can form bigger word-like units via a mapping to a sequence of basic units.The training process of the classification system includes three stages. The first stage creates an initial sequence for HMM training. There are multiple approaches for initializing the system with phone-like units, such as using a different recognizer trained from another language. In a sense, that is not truly unsupervised because it uses a model trained with supervision. Instead, we have taken a sequential learning approach to build a segmental tokenizer using a polynomial segmental Gaussian mixture model (SGMM). The details of the SGMM training will be described below in Section 3.1.1.The second stage is the unsupervised HMM training that iterates between optimizing the model parameters and the label sequences. It should be noted that after initialization, only the label sequences are passed from the segmental tokenizer to the HMM trainer while other auxiliary information such as segment boundaries is ignored.The final stage is the SVM classifier training. Again, only the label sequences are passed from the HMM recognizer.The first component of the segmental tokenizer is an adaptive segmenter, which partitions speech at boundaries of spectral discontinuities. The quantification of discontinuity is corpus specific and is inferred by a dynamic programming algorithm with the creation of statistical models for the corpus under consideration as described in Cohen (1981). The segmentation process is immediately followed by fitting each segment with a polynomial (quadratic in our application) trajectory model, whose parameters are used to compute the pairwise distances between segments for segment clustering. The distance of a pair of segments is the total area between the trajectories of the segments under comparison. This distance is applied to a binary centroid splitting algorithm that is widely used in frame-based mixture models.The basic idea for the segmental trajectory model is to jointly model a sequence of consecutive frames and as such model a sound instance. The process is illustrated in Fig. 2. The segmentation process cuts the audio into a sequence of variable length, multi-frame segments, each represented by a polynomial trajectory across time. The clusters of segments represent collections of sound units and any individual cluster is a collection of variants of a particular sound. We use the clusters as the basis for generating an SGMM, which is trained with the EM algorithm. Note that a segmental GMM term is a bit different from the usual Gaussian mixture model term whose mean is a fixed point in the cepstral space and is constant over time. Each term of an SGMM is a Gaussian whose mean is a vector trajectory in the cepstral feature space and varies over time to represent time varying characteristics of a sound. The mathematical basis for SGMMs has been discussed previously in Gish and Ng (1993, 1996). The SGMM becomes a “speech recognizer” or tokenizer when, for a test segment, the index of the mixture term with the maximum likelihood is generated as a token.In particular, for a segment in an utterance extending from frame i to j inclusively, we denote this segment asXij=[xi,xi+1,…,xj],where xtis the acoustic features for the tth frame. We define an M-component segmental Gaussian mixture model (SGMM) over the segment,Xijf(Xij)=∑n=1Mwngn(Xij).When using SGMM as an initialization to SOU training, M would be the number of SOUs. Each component, gn, is defined as the product of Gaussian distributions on the separate frames with a time-invariant covariance Σn, but the mean parameter μnfollows a trajectory in normalized time. Specifically,gn(Xij)=∏t=ijN(xt;μn(τt,i,j),Σn)where τt,i,jis the normalized time within the segment given byτt,i,j=t−ij−i.For any test segment, the symbol/token it generates in the recognition process isnoutput=argmaxnwnfn(Xij)The token sequences generated by the SGMMs are passed as the true “transcript” for HMM training.An SGMM tokenizer for initialization is one way for providing an initial transcription of the training audio in order to get the iterative training started. Alternative initializations can be achieved by using recognizers from other languages. However, while these recognizers do represent the use of transcribed audio, they can be considered as being a legacy resource. If the acoustic domain of the legacy system was similar to that of the audio at hand then it could be quite useful. An acoustic domain mismatch could reduce the utility of such a recognizer.The SGMM is trained entirely within domain. However, it is a fairly simple model with only one Gaussian per SOU. We have looked at generalizations that include using multiple Gaussians per SOU as well as an HMM approach to initializations. This work is not yet complete and we do believe such approaches should improve performance over the current SGMM initialization.Our work uses the state-of-the-art BBN Byblos system (Matsoukas et al., 2006) to model the units derived from the SGMM. Byblos includes advanced signal processing techniques, such as vocal tract length normalization (VTLN) and heteroscedastic linear discriminative analysis (HLDA) feature transformation. Byblos training does not require time alignment. Instead, iterative alignment and model estimation are applied with the first few iterations using simpler models followed by more complex models. Because we used the phonemes (or SOUs) as words, non-crossword models are equivalent to context-independent (or mono-phone) models. To build context dependent models in Byblos, we create “phoneme” classes to drive the decision-tree based phoneme-state clustering. What “linguistic questions” can we use to drive the decision tree? In our current work, we cluster the 64 SGMM's into 16 classes to act as “phoneme classes” for decision tree context clustering of quinphones. More complex models, including cross-word models, are trained after clustering. Speaker adaptive training (SAT) is also applied to create models for unsupervised speaker adaptation. While discriminative training is implemented and available, our current experiments used only the maximum likelihood training.55We have started to look at discriminative training for the SOU model which should provide greater distinction between the SOU units. Our early work has thus far only provided small gains but we believe it is still worth considering.Details about the Byblos training can be found in Matsoukas et al. (2006).In addition to acoustic models, bigram and trigram language models are learned using the label sequences generated by the segmental tokenizer.With the trained acoustic models and language models, the tokenization of training (also test) audio into SOU sequences is no different from regular phoneme recognition. When an HMM is trained, many models are created and their application, as we have previously noted, is typically done with an initial set of simpler models applied to the data followed by a pass of more complex models. The multi-pass approach is for computational efficiency and is the same for an SOU system as for a phonemic system. Whether working with phonemes or SOUs, the key idea is that the sound units are capturing the syllables, words or phrases that will be important in distinguishing topics. In our earlier work (Gish et al., 2009), we tokenized the audio using only non-crossword models with bigram language models. We observed a performance improvement using the more complex cross-word quinphone model as reported in Siu et al. (2010). These quinphones are used to re-score SOU lattices generated by mono-phone models.The step after tokenization is the SVM classifier training. For topic classification, audio cuts are tagged as in-topic or out-of-topic. Note that data used for training the classifiers do not need to be the same data used for training the HMMs but can be so if needed. We begin by tokenizing both the in-topic and out-of-topic audio into SOU sequences. We then extract from these SOU sequences the SVM classification features which are SOU n-gram statistics normalized by inverse-document-frequency (IDF). In this paper, our n-gram features are trigrams of SOUs, and a document for computing IDF is a conversation side. We then apply the more sophisticated SVM feature building techniques for generating useful long-span n-gram features (Campbell and Richardson, 2007).The n-gram statistics can either be extracted from recognition 1-best hypotheses as in Gish et al. (2009), or more generally, from the quinphone re-scored lattices (Siu et al., 2010), which are soft counts based on the lattice posterior probabilities. However, using soft counts from lattices for classification tasks was previously introduced by other researchers (Hazen and Margolis, 2008). The use of soft counts creates a problem for estimating the IDF weighting. We experimented with soft IDF weighting as proposed in Wintrode and Kulp (2009) as compared to using a simple posterior threshold on the soft token counts. We found that the simple thresholding gave better performance.The SOU TID evaluation process is quite similar to that of a typical TID system using words. The test audio cuts are first tokenized into SOU token sequences (or lattices). Classification features are extracted and passed to the SVM topic classifier to generate TID scores. Note that instead of using the SVM as a hard decision classifier, we extracted the SVM scores, each of which is a function of the distance of the test sample to the decision boundary, as the classification scores in our computation of TID EER.In this paper, we report two sets of experiments on topic identification on two different corpora.Our first set of experiments was performed on the Switchboard-1 corpus, which consists of telephone conversations between strangers discussing one of 70 pre-assigned topics. Each conversation is approximately 5min long. The most frequent ten topics were selected as target topics with the remaining 60 topics denoted as the non-target topic set. A set of 96 conversation sides (4h) was randomly selected from the non-target topic set for SOU training. The selected target topics, the number of training and test conversation sides (for TID) are listed in Table 1. TID decisions were made per conversation side. Two non-overlapping sets of target-specific impostor data were defined for each target topic. Each contains 600 tests randomly selected from the combination of the non-target topic set which is not used in HMM training, as well as data from the other target topics. One set was used as the negative examples in SVM classifier training and the other set was used as impostor data in classification evaluation.The experiment started with segmentation, which was based on spectral power from 14 frequency bands with a maximum segment duration of 50 frames. The details are described in Gish et al. (2009). Vocal tract length normalized (VTLN) cepstral features and their first derivatives were generated for each segment that was then modeled with a quadratic trajectory model. The clusters were created using the k-means algorithm with initial centroids obtained using binary centroid splitting. These k-means clusters were further refined with 3 iterations of EM training. This resulted in 64 mixture components.The SGMMs were used to tokenize the acoustic training data as the initial label for HMM training. This was followed by multiple iterations of unsupervised HMM training. Byblos used 5-state left-to-right HMMs trained with the maximum likelihood criterion. Sixty-dimensional acoustic features were generated by transforming the concatenation of 9 frames of the 13-dimensional cepstral features and its energy via an HLDA transform. We used a 512 component state-tied-mixture model (STM) for the monophone model and 64 Gaussian components for the cross-word state-cluster tied mixture (SCTM). Both speaker independent acoustic models and speaker adaptive training (SAT) models were created. During each iteration of unsupervised training, the acoustic training data were decoded using the newly learned parameters to generate new unit sequences. This decoding involved one pass of unadapted decoding followed by the HLDA adaptation, unsupervised MLLR adaptation and another pass of adapted decoding, all using only the STMs.After the HMMs were trained, recognition was performed on all the classifier training and test data using both the STMs and the context dependent SCTMs as described in Section 3. We trained the SVM using the regression mode of the publicly available LIBSVM package (Chang and Lin, 2001).To be able to compare performance with other published results on topic classification, we also performed larger scale experiments on the Fisher corpus similar to those set up in Hazen et al. (2007), which includes 40 topics. The topics and the amount of training and evaluation on-topic data are shown in Table 2. The data set includes a total of 244h for topic training and 114h for evaluation. The Fisher experiments were un-optimized in that the system is used “out-of-the-box”, using exactly the same settings used in the Switchboard experiments, without any adjustment for the large data set and its variations in amount of topic training data.

@&#CONCLUSIONS@&#
In this paper, we presented our novel approach to unsupervised HMM learning using SOUs, which can be a valuable option for speech applications in domains of limited transcribed data. We presented results in topic identification that showed SOUs can be competitive with systems trained with a limited amount of in-domain data and outperform those trained from mis-matched data. We described some of the improvements in SOUs, including context dependent acoustic models and lattice-based n-gram features that resulted in significant TID EER reduction. We further explored the relationship between some selected SOU sequences and their corresponding English words. We showed that these SOU n-grams are consistently mapped to the same English keywords, and SOU n-grams selected as TID features were representing topic keywords. Thus, the SOU TID system can also be used for unsupervised keyword discovery.One direction we are currently exploring is the building of bigger units during the SOU acoustic training. This is analogous to building a pseudo-word recognizer instead of phoneme recognizer which could further improve the SOU consistency and thus, improve downstream topic ID performance.