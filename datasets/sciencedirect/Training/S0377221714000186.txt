@&#MAIN-TITLE@&#
Maintaining the Regular Ultra Passum Law in data envelopment analysis

@&#HIGHLIGHTS@&#
We discuss limitations of the maintained assumption of convexity in DEA.BCC measure of efficiency is inconsistent with the Regular Ultra Passum Law.We relax the assumption of convexity in DEA.We develop a non-parametric piecewise linear estimator of S-shape production.Using simulated data, we show that our new method overcomes BCC biases.

@&#KEYPHRASES@&#
Data envelopment analysis (DEA),S-shaped production function,Convex hull estimation,Isoquant estimation,

@&#ABSTRACT@&#
The variable returns to scale data envelopment analysis (DEA) model is developed with a maintained hypothesis of convexity in input–output space. This hypothesis is not consistent with standard microeconomic production theory that posits an S-shape for the production frontier, i.e. for production technologies that obey the Regular Ultra Passum Law. Consequently, measures of technical efficiency assuming convexity are biased downward. In this paper, we provide a more general DEA model that allows the S-shape.

@&#INTRODUCTION@&#
The non-parametric data envelopment analysis (DEA) approach envelops observed data with a piecewise linear frontier. The characteristics of a DEA model are derived from a number of maintained assumptions imposed on the technology. A typical estimator used in DEA is the BCC-estimator (Banker, Charnes, & Cooper, 1984), which assumes the estimated production possibility set is a polyhedral set that allows variable returns to scale. As a consequence, the BCC-estimator assumes that marginal product is non-increasing, which violates standard microeconomic theory where marginal product initially increases but diminishing returns eventually set in. In particular, if data reflects the Regular Ultra Passum (RUP) law (Frisch, 1965, Chapter 8), the BCC-estimator will be biased downward.Definition 1The RUP law. Let a single output y be produced from a vector of m inputs x according to a production functionF(x,y)=0. This production function obeys the RUP law if∂ε(x,y)∂xi<0,i=1,…,mwhere the functionε(x,y)is the scale elasticity, and for some point(x1,y1)we haveε(x1,y1)>1, and for some point(x2,y2), wherex2>x1,y2>y1, we haveε(x2,y2)<1.The problem with the BCC-estimator is that the supporting hyperplanes for envelopment can overestimate inefficiency for points that should be projected to the local non-convex segments of the true frontier characterized by increasing returns to scale.1See Fig. 2 later in the paper for a geometric comparison of the BCC- and the proposed S-shaped estimator.1In this paper, we are concerned with production technologies satisfying the RUP condition where the BCC-estimator is biased because such technologies are not convex in input–output space.2It is argued in Banker et al. (1984) that any point located in the interior of a strongly efficient facet with a supporting hyperplane given by{(x,y)|uty-vtx-uo=0}will have the local scale characteristics determined from the sign ofuo. Hence, as stated in (29a), page 1087 in Banker et al. (1984) we have “Increasing returns to scale⇔uo<0”. In other words, the convex hull estimator proposed in the BCC model will in general accommodate estimation of local scale characteristics of both increasing, constant and decreasing returns to scale. However, if the true production function satisfies the RUP law with monotonically decreasing scale-elasticity then the convex hull estimator may provide biased efficiency scores for observations below most productive scale size (mpss). The possible radial contraction of the input vectors from such observations towards the boundary of the convex hull estimator may provide exaggerated estimates of possible input contractions. This could, e.g., happen if such an observation is dominated by a point on a facet spanned entirely by observations below mpss, where some observations are close to the origin and all other observations are close to being mpss.2Furthermore, existing measures of scale efficiency will be biased due to the improper projection to production impossibilities. The main contribution of this paper is the development of an approach that is capable of measuring inefficiencies for production possibilities in a non-convex homothetic and S-shaped technology. A non-convex S-shaped technology is characterized as follows: along any expansion path an expanding DMU with low activity will have a high scale elasticity greater than one. As the unit expands its activity the scale elasticity will decrease and will approach optimal scale size with an elasticity equal to one. Further expansion will imply decreasing returns with a scale elasticity less than one and approaching zero.3Recently, Olesen and Petersen (2013) suggested a method designed to provide a local (in the sense of a fixed input mix and a fixed output mix) estimation of lower and upper bounds on mpss. This suggested method requires “two additional maintained hypotheses which imply that the DEA-frontier is consistent with smooth curves along rays in input and in output space that obey the Regular Ultra Passum (RUP) law, i.e. monotonically decreasing scale elasticities”, (abstract). The purpose of the approach suggested in this paper is different. We argue that the convex hull estimator is not consistent with standard microeconomic production theory that posits an S-shape for the production frontier, i.e. for production technologies that obey the Regular Ultra Passum Law.3A twice differentiable nicely convex-concave production functionh(x)in the terminology suggested by Ginsberg (1974) is an example of an S-shaped technology for the single input single output case. A nicely convex-concave production function satisfies the following assumptions (i)h(0)=0, (ii)h(x)⩾0,x∈[0,∞), (iii)h′(x)>0,x∈(0,∞), (iv) there existx∗∈(0,∞)such thath″(x)>0,x∈(0,x∗)andh″(x)<0,x∈(x∗,∞), (v) there exists ax¯,x∗<x¯<∞, such thath(x¯)=x¯×h′(x¯). By reference to Ginsberg’s PhD thesis it is argued in Ginsberg (1974) that a nicely convex-concave production function will have an average product being non-negative and increasing until it reaches its maximum atx¯. Forx>x¯the average product will decrease for increasing x. It is easy to prove that this implies that the scale elasticity is monotonically decreasing, i.e. the production function satisfies the RUP law.4Frisch (1965, p. 89) discusses the economic theory of increasing marginal productivity that exists in a first stage of production at low levels of the variable input. In standard Principles of Microeconomics courses, production is illustrated with an S-shaped total product curve. The initial stage of increasing marginal returns is usually attributed to increases in specialization and division of labor. For example, Parkin (2014) argues that most production processes exhibit increasing marginal returns but eventually all production processes exhibit diminishing marginal returns.4Several non-convex models exist in the literature (e.g., the FDH-model of Afriat (1972), Deprins, Simar, and Tulkens (1984), the Petersen-Bogetoft approach, Petersen (1990), Bogetoft (1996)), and Jeong and Simar (2006), Kuosmanen (2001) but these models are not well-suited to estimate an S-shaped production structure because any non-convex shape can result from these estimation procedures. In other words, we are looking for an estimation procedure that allows only non-convexities that are reflected in an S-shaped production structure.5One of the referees has brought to our attention that a related paper entitled “Modeling Non-convex Production Frontiers: An Application to the Manufacturing Sector of China” by Sung-ko Li was presented at the North American Productivity Workshop VI in Houston in 2010. The paper is apparently unpublished. With one input and one output it seems to use an FDH approach below and a BCC approach above mpss.5For simplicity, we focus on production technologies that are homothetic. The concept of a homothetic production function was first introduced in Shephard (1953, page 30) as a monotonic transformation of a linear homogenous production function. With a homothetic production structure we can smooth the obtained structure of the estimated isoquant because homotheticity implies that the shape of the isoquants is identical. This allows us to maintain convexity in input (and output space) and to allow non-convexities in input–output space.In order to move between input space and output space, we propose estimating individual isoquants assuming selective input convexity using a simplified order-m estimation procedure (Cazals, Florens, & Simar, 2002) where we avoid replications. The order-m estimation procedures include a conditional estimation model maintaining selective convexity of the input sets.6See Ruggiero (1996) and Podinovski (2005).6Under the assumption of homotheticity, we can aggregate inputs (and outputs) allowing us to move to aggregate input–output space where we can impose an S-shape.The rest of the paper is organized as follows. In Section 2 we define the production technology, from an input orientation using an input distance function. The assumption of homotheticity is presented and the implication for input aggregation is discussed. Notably, the assumption of homotheticity allows us to generate any isoquant from a base isoquant and hence, derive a well-defined index of aggregate input. Section 3 is devoted to the estimation of the base isoquant using a conditional estimator. We also discuss criteria for selecting a well-estimated isoquant among all possible base isoquants to aggregate inputs. This isoquant is used for the aggregation of inputs. In Section 4, we develop a model to estimate a piecewise linear S-shaped frontier. Using simulated data in Section 5, we show that our method overcomes the inherent problems of standard DEA and provides better estimates of inefficiency when the true technology obeys the Regular Ultra Passum Law. The last section concludes with directions for future research.Let us consider a production environment where a vector of s inputsX=(x1,…,xs)is used in the production of one output Y. We represent the production technology with the input setL(Y)=X∈R+s:XcanproduceYwhich has isoquant(1)IsoqL(Y)={X:X∈L(Y),λX∉L(Y),λ∈[0,1)}Since we assume that only one output is produced, we can define a production function as(2)ϕ(X)=max{Y:X∈L(Y)}The input distance function (Shephard, 1970) is then defined as(3)DI(Y,X)=max{γ:X/γ∈L(Y)}which provides an alternative characterization of the technology sinceDI(Y,X)⩾1⇔X∈L(Y). Finally, the index of technical efficiency proposed by Debreu (1959) and Farrell (1957) that serves as basis for DEA is given as(4)FI(Y,X)=min{γ:γX∈L(Y)}whereFI(y,x)=DI(y,x)-1.In this paper, we seek to place additional structure on the production technology. In particular, we assume that production is homothetic.Definition 2A production functionϕ(X)is homothetic ifY=ϕ(X)=F(g(X))whereF():R+→R+is monotonically increasing andg(λX)=λg(X), i.e.,g()is positive homogeneous of degree one and continuously differentiable (see Shephard, 1970).g()is denoted the core function.From the definition, we see that a homothetic production function can be represented as a production process whereby the input vector X can be aggregated into a one dimensional input indexg(X), i.e. output is determined from the level of aggregate input (see Färe & Lovell, 1988 for a more general result).Proposition 1Assume a homothetic technology with one output. The distance function evaluated at(1,X)is equal to aggregate input defined from the core function in the homothetic production function multiplied by a constant k, i.e.DI(1,X)=k×g(X),k∈R+Letϕ(X)=F(g(X))withF-1=f. We know thatL(y)={X:F(g(X))⩾y}={X:g(X)⩾f(y)}Furthermore,DI(1,X)=max{γ:X/γ∈L(1)}=max{γ:X/γ∈{X:g(X)⩾f(1)}}=max{γ:g(X/γ)⩾f(1)}=max{γ:1γg(X)⩾f(1)}=max{γ:g(X)⩾γf(1)}=(f(1))-1×g(X)□Proposition 1 establishes that the dimensionality of DEA models can be reduced under the assumption of homotheticity. In addition, homotheticity allows us to span the production technology fromL(1)(see Shephard, 1970, p. 34).(5)L(Y)=f(Y)f(1)L(1)whereL(1)is the input set associated with the unit isoquant andf(Y)is the inverse scaling function. Input sets can be theoretically generated from a base input set by the scaling function that depends only on the level of output and not the input mix. From (5), we also have(6)IsoqL(Y)=f(Y)f(1)IsoqL(1)This shows that we can generate any isoquant from the unit isoquant. More generally, we could choose any output level and its associated isoquant to serve as the base. Here, we choose the unit isoquant for expositional convenience only. In the next section, we consider the estimation of a base isoquant and provide guidance on selecting a well-estimated base for aggregation purposes.One useful method for estimating any isoquant is the order-m estimation procedure (Daraio & Simar, 2005). The input distance functionDI(y,x), defined in (3) is expressed relative to the input setL(y)and the basic idea in the order-m procedure is to regard this input setL(y)as the support of a conditional density functionL(y)={x:FX|Y(x|y)>0}. The corresponding support for the joint input output densityHX,Y(x,y)is the production possibility set T, i.e.T={(x,y):HX,Y(x,y)>0},HX,Y(x,y)=Pr(X⩽x,Y⩾y)=Pr(X⩽x|Y⩾y)Pr(Y⩾y)=FX|Y(x|y)SY(y), whereSY(y)=Pr(Y⩾y). For a fixed level of outputyoletX1,…,Xmbe m i.i.d. random input vectors generated fromFX|Y(.|yo), i.e. all input vectorsXi,i=1,…,mare random vectors that can produceyowith a strict positive probability. Assuming selective (local) convexity of the input sets, the random input set of order-m7Daraio and Simar (2005) use the phrase “random production set of order-m for units producing more than y:TmC(yo)=Conv(x,y)∈R+m+1|x⩾Xi,y⩾y0,i=1,…,mHowever, this set is not a production set since it is unbounded in the output dimensions. We use the related input setLmC(yo).7for units producingyo,LmC(yo)is defined as:(7)LmC(yo)=Conv[{x|x⩾Xi,i=1,…,m}]The locally convex order-m input efficiencyθmLC(x,y)is defined as (Daraio & Simar, 2005, p. 17, (3.2.))θmLC(x,y)=EX|Yθ̃mLC(x,y)|Y⩾ywhereθ̃mLC(x,y)=infθ|θx∈LmC(y)To obtain the estimatorθˆmLC(x,y)=E^X|Y(θ̃mLC(x,y)|Y⩾ybased on a sample of n observations we apply the empirical version ofFX|Y(.|yo)asF^X|Y,n(x|y)=∑i=1n1(Xi⩽x,Yi⩾y)∑i=1n1(Yi⩾y), where1()is the indicator function.θˆmLC(x,y)can be approximated by a Monte-Carlo procedure: Sample m observationsX1,b,…,Xm,bconditional on output being greater thanyo=1with replacement. For each of the n observations find the inverse distance function valueθ̃mLC,b(Xl,1),l=1,…,nrelative to an input setConv[{x|x⩾Xi,b,i=1,…,m}]. Redo this estimationb=1,…,Band take the average of the obtained scores as the estimator, i.e.θˆmLC(Xl,1)≈B-1∑bθ̃mLC,b(Xl,1),l=1,…,n. From these scores we obtain an estimated input setL^mLC(1)as(8)L^mLC(1)=ConvθˆmLC(X1,1)×X1,…,θˆmLC(Xn,1)×Xn+R+s.A simplification of the order-m estimator is the conditional estimator of the base isoquant, which avoids the replications by choosingm=n.In the simulations presented later in the paper, we use this conditional estimator instead of the order-m estimator. The base isoquant can be estimated using this conditional model solving the following linear programs8In model (9) and (11) we use a formulation where the slacks are explicitly included. We need these slacks to distinguish between two types of index of aggregated input based on the input distance function, see footnote 11.8(9)θˆCLC(Xl,Ybase)=minθ-ε(1,…,1)ss.t.θXl-∑j=1nλjXj-s=0∑j=1nλj=1λj=0ifYj<Ybaseλ∈R+n,s∈R+s,θ∈Rl=1,…,n, whereεis a non-Archimedian andYbase=1in this section and where again the estimatorL^CLC(1)of the input set is derived as(10)L^CLC(1)=Convθ^CLC(X1,1)×X1,…,θˆCLC(Xn,1)×Xn+R+sThis model appears in the efficiency literature to control for exogenous inputs (Ruggiero, 1996), selective convexity (Podinovski, 2005) and as the conditional estimator (Daraio & Simar, 2005). In this formulation, units that are not observed producing at least the base amount (in this case, one) are not allowed in the solution space. Hence, we simply envelop all input vectors with observed output at least equal to one. Notably, we replace the standard assumption of convexity with selective input convexity of the input sets:Axiom [Selective input convexity]: If(X′,Y′)∈T,(X″,Y″)∈T,Y″>Y′⇒λ(X′,Y′)+(1-λ)(X″,Y′)∈T,λ∈[0,1].Our primary reason for using the conditional model is not to estimate efficiencies but to exploit homotheticity to aggregate multiple inputs into a one-dimensional input index. Hence, we estimate each isoquant using the conditional estimator and choose the “best” isoquant that has good coverage in the sense that (i) we want as many observations playing an active role of spanning the frontier, (ii) we want the cone spanned by these observation to be as large as possible and (iii) we want the observations to be spread out across the cone as uniformly as possible. After choosing the isoquant that best meets the desirable criteria, we then estimate the distance of each observation to this isoquant as an index of aggregated input.To ease the presentation of the proposed methodology, we chose the unit isoquant as the base in our discussion above. We now provide guidelines for how to choose the output level with the most useful information. Using the conditional estimator relative to a given output level y we only include input vectors from observations with an output level at least equal to this y. We would like to have as many observations as possible available for spanning the isoquant, which tends to suggest a low output level. However, observations producing output much larger than y may not provide any additional information. If we knew the positions and the shapes of the true isoquants we would look for a specific isoquants (a y level) where (i) we have many observed points on or just above the isoquant and (ii) where the observed points are spread out evenly along the full isoquants. Unfortunately, we do not know the locations and the shape of the true isoquants. Hence, we have to rely on an estimator, and in this case we will use the conditional estimator defined above. For each observed output levelYj,j=1,…,n, we use the conditional estimatorsθˆCLC(Xl,Yj),l=1,…,nwhich provides us with the estimatorsL^CLC(Yj)of all n input sets corresponding to all n output levels. As base isoquant we now choose the specific output level which performs reasonably well according to the following three criterias9Of course, other criteria may be relevant. We leave for future research the evaluation of isoquant coverage.9:1.A distribution of the angle coordinates of the observed data points on the conditional piecewise linear estimator of the isoquant, which mimics the uniform distribution on the empirical support of the angle coordinates for the whole data set. As a measure of the amount of deviation of the empirical distribution from the uniform distribution we suggest the volume between the two distribution functions.A large number of observed data points is located on the conditional piecewise linear estimator of the isoquant.A large number of observed data points is projected to the envelopment of the points on the frontier, i.e. are located in the input space within the cone spanned by the points that spans the isoquant.The selection process for choosing the “best” base isoquant is implemented as follows. Estimate alln×nconditional scores providingθˆCLC(Xl,Yj),j,l=1,…,n, where this score is missing, ifYl<Yj. For a given observed output levelYjokeep the input vectorsXlifθˆCLC(Xl,Yjo)is non-missing and ifθˆCLC(Xl,Yjo)=1. Let the number of input vectors that satisfy these two conditions benYjoand let the different angles-vectors in the polar representation of these input vectors beηl,l=1,…,nYjo. The resulting set of angles corresponds to observations on the estimated isoquant at output levelYjo. For the case of two inputs (this is the case covered in the included simulations) we only have one angle in the polar representation of the input vectors. Hence, we sort the angles and plot the pointsηj-SLSR-SL,jnYjo,j=1,…,nYjo, whereSL,SRare the left and right endpoint of the support of the angle distribution (empirical estimates), see Fig. 1. The deviation of this empirical distribution from the uniform distribution is measured as the area between the 45 degree line and the piecewise linear curve going through thesenYjopoints, starting at the origin and ending at(1,1).10The area under the piecewise linear curve determined from the absolute deviation between the two curves can be determined as a sum of areas of a combined rectangle and a triangle. Consider three point in this deviation space(ηj,zj),j=1,2,3whereηj+1>ηjand letz2>z1andz3<z2. The area spanned by(ηj,zj),j=1,2is12(z2-z1)+z1(η2-η1)=12(z2+z1)(η2-η1). The area spanned by(ηj,zj),j=2,3is12(z2-z3)+z3(η3-η2)=12(z3+z2)(η3-η2). Hence, the areas, except for the first and the last triangle are determined as∑j=2nYo12(zj+zj-1)(ηj-ηj-1). Similarities to the Gini coefficient are apparent.10Fig. 1 illustrates this deviation for isoquant 750 used as the best base isoquant in the simulation study presented below in Section 5.Suppose we have identifiedY∗=Ykas the output level associated with our chosen base isoquant. Then, an index measure of aggregate11We distinguish between an index of aggregated inputs from (11) with or without slacks with at least one component being strictly positive present in the optimal solution. As we will see in Section 5 we can nicely recover the true efficiency for data points without slacks with at least one component being strictly positive.11input isxˆ(Xl,Yl)=θCLC(Xl,Y∗)-1forl=1,…,nwhereθCLC(Xl,Y∗)is determined as the solution to the following linear program12It is left for future research to investigate how sensitive the obtained results are to the exact choice of base isoquants. As suggested by one of the referees it would be interesting to illustrate how much the results deteriorate if the base isoquant was chosen as being the one with the lowest output level.12:(11)θCLC(Xl,Y∗)=minθ-ε(1,…,1)ss.t.θXl-∑j=1nλjXj-s=0∑j=1nλj=1λj=0ifYj<Y∗λ∈R+n,s∈R+s,θ∈Rl=1,…,n.Our estimate of aggregate input using (11) allows us to analyze an estimator of the S-shaped technology in the single (aggregate) input single output case.13Maintaining the RUP law requires that the scale elasticity is monotonically decreasing for increasing production. However, it is well known that along any facet above most productive scale size (mpss) we will have constant marginal product and decreasing average product (Førsund & Hjalmarsson, 2004), which seems to imply that the RUP-law is violated. However, this violation disappears asymptotically, see Olesen and Petersen (2013).13With homothetic production, of course, we can also aggregate multiple outputs into a single output aggregate. Let the true production possibility set (PPS) be denotedTS, and we will assume that the boundary ofTSis an S-shaped graph from a convex-concave functionh(x), in the sense that we can divide the input axis into two parts[0,x∗]and[x∗,∞)where the production function is convex (concave) on the first (second) interval. Hence, the marginal product is monotonically non-decreasing in[0,x∗]and monotonically non-increasing in[x∗,∞). Following the terminology suggested by Ginsberg (1974) we will in addition require that there exists at least onex¯,x∗⩽x¯<∞, such thath(x¯)=x¯×h′(x¯). An example of such a true S-shaped production function is the twice differentiable nicely convex-concave production function proposed by Ginsberg (1974) and mentioned in the introduction. We denote(x∗,h(x∗))the inflection point and clearly any(x¯,h(x¯))is an mpss.In this section we propose two different approaches to estimate a piecewise linear S-shaped frontier. In the first simple approach we assume that the inflection point is located on or slightly below the mpss.14Only in special cases is the mpss assumed to be located close to the inflection point. Later in this section we provide an estimator of the inflection point.14An integral part of the second and more complicated approach is an estimation of the inflection point.The first approach denoted “the digging approach” modify the BCC convex hull estimatorT^BCCby digging out or removing a part of this convex hull. If observations are present below the mpss that support an S-shape then removing part ofT^BCCbelow mpss will provide an estimator with an S-shaped boundary. In general, we know that for input and output above the mpss, the BCC estimator works well, because of the true concave shape of the production function in this part of the input output space. On the other hand, we may encounter a data set where the structure of data below mpss and above the inflection point only can be recovered by the BCC estimator if we split the sample and only work on data above the inflection point. Hence, following a strategy of modifying the BCC estimator, it seems natural to dig out part of the convex hull from the origin to the mpss.To be more precise, we will dig out a certain convex hull of observed data point that satisfies the following:•the convex hull is spanned by the mpss observation with the smallest input level and all FDH efficient observations with input levels below this mpss observation. These observations reflect the convex IRS part of the technology,the convex hull is constructed such that no observed data point is located above the frontier (or equivalently, no points are located in the interior of this hull).Fig. 2illustrates this idea using 6 input–output observations generated from an “S-shaped” data generating process (DGP). Observations,A,Eand F are BCC-efficient and observation E is mpss. The basic idea behind the digging approach is to determine a subset of all FDH-efficient DMUs “below” mpss which determines a convex hullT^Dig, where none of these DMUs belongs to the interior of this hull. An estimatorT^Sof the PPS with an S-shape with an efficient boundary being piecewise linear is now available asT^BCC⧹T^Dig≡T^S, i.e. the convex hull BCC estimator of the PPS minus the convex hullT^Dig. In Fig. 2 the estimatedT^BCCis the convex hull of observationsA,E,Fset added toR+×R-(strong input and output disposability).T^Sis estimated asT^BCC⧹T^Dig, whereT^Digis the convex hull of the observationsA,B,C,D,E.The piecewise linear strongly efficient frontier in Fig. 2 ofT^Sis ABCDEF. In Fig. 3the FDH step function is included. Since only FDH efficient observations are allowed to influence the estimation we may have data points present only within the four triangles bounded by the FDH-steps and the frontier ABCDE. ObservationsH,I,J, and K are indicated below facet CD. Observation I is consistent with the frontier ABCDE in the sense that if I is present then we simply “dig” a deeper hole intoT^BCCproviding the efficient frontier as ABCIDE. This is possible without changing the rest of the hull because observation Iis above the extension of both facet BC and facet DE. Hence, including I on the frontier still gives us a monotonic non-decreasing marginal product moving from C to I to D. Notice however, that neither H norJ,Kare consistent with the frontier BCDE. These three additional observations share the characteristic of being either below the extension of facet BC or below the extension of facet DE, or both.The determination of this inverted convex hull is unfortunately not unique. This is illustrated in Fig. 3 by the data point L being below the extension of the facet CD. Hence, we cannot include L on the S-shaped frontier and at the same time maintain thatA,B,C,Dand E all are on the frontier. However, we may include L as being on the frontier if we remove C from the frontier, as indicated by the dashed convex hull. Hence, we have a choice. Either C or L is efficient, but not both. Below we will partly resolve this non-uniqueness of the solutions by searching for the solution that maximizes the number of FDH-efficient data points on the frontier.15In the case of a tie between several alternative solutions we suggest using each alternative solution to evaluate the efficiency of the observed points.15,16Several approaches seem to be possible to resolve this non-uniqueness. The approach proposed in this paper, i.e. select one of the solutions that maximizes the number of FDH efficient DMUs located on the frontier, reflects one of the DEA model characteristics, namely that we choose the most conservative frontier, placing as many observations on the S-shaped frontier as possible. As suggested by one of the referees an alternative approach would be to choose the solution with the smallest volume of the PPS, truncated to the support of the data set, in the aggregated input–output space.16Assuming that we know the position of the inflection point a simple procedure to determine an arbitrary inverted convex hull is as follows:Step 1.Generate the FDH efficiency scores for all units below the inflection point. Remove all FDH-inefficient point.Without loss of generality, let(X1,Y1)=(0,0)and the firstn′-1data points(Xj,Yj),j=2,…,n′be the FDH-efficient data points and project each of these points towards higher input levels within the convex hull of these DMUs, using the following program(12)max∑k=1n′θks.t.∑j=1n′λj,kXj-θkXk=0k=1,…,n′∑j=1n′λj,kYj-Yk=0k=1,…,n′∑j=1n′λj,k=1k=1,…,n′λ∈R+m×n′,θk∈R,∀kFor each optimalθk∗>1, at most two components amongλ1,k,…,λn′,kare strictly positive in any optimal basic solution. Remove one of these two FDH-efficient DMUs from the sample, but never remove the two DMUs with the largest and the smallest input value.Do steps 2–3 untilθk∗=1, for all index k in the remaining set of DMUs.We denote this approach the filling approach, because after “digging” an inverted convex hull including all FDH-efficient DMU, we modify the hull by making the hull smaller by removing FDH-efficient DMUs that force some other FDH efficient DMUs to be located in the interior of the hull.Assuming that we know the position of the inflection point this approach will provide us with an inverted hullT^Digand will thereby provide a piecewise linear estimator of the S-shaped technology given asT^S≡T^BCC⧹T^Dig. However, since the estimator is not unique the determination of which FDH-efficient data points to include on the increasing returns to scale part will depend on the which point we choose to delete in step 3 above and in what sequence such points are removed.We now consider an alternative procedure that provides a piecewise linear estimator of the S-shaped technology using the inverted convex hull that maximizes the number of FDH efficient data points on the S-shaped frontier, i.e. that maximizes the number of “S-shaped efficient” data points. An integral part of this procedure is an estimation of the inflection point; we seek as an inflection point below mpss one that allows for an estimated S-shaped frontier with a maximum number of FDH efficient data points on the frontier. An estimator of an S-shaped frontier is a sequence of linear segments, where each segment either connects the origin to one of the input output combinations or connects two of the input output combinations. In addition this sequence of linear segments must be connected, and all observations must be located on or below the curve defined from the union of all these linear segments. Finally, to get the correct shape of this curve, we require that it is a convex-concave curve.Testing a given observation as a candidate for the inflection point involves several conditions. Among all possible estimators of such an S-shaped piecewise linear frontier we search for an estimator that satisfies (i) the candidate for an inflection observation must be located on this curve, (ii) the part of the curve going from the origin to this potential inflection observation must be a convex curve, (iii) the part of the curve on or above this potential inflection observation must be a concave curve, and (iv) the segment of the curve to the left of the candidate of the inflection observation must have a greater slope that the slope of the segment of the curve to the right of this observation.To illustrate some of these conditions, let us assume that our sample consists of n FDH-efficient data points(Xj,Yj)∈R+2,j=2,…,n+1, and let us add the origin as the first observation, i.e.(X1,Y1)=(0,0). Assume thatXj+1>Xj,j=1,…,nand that we want to test(Xn,Yn)as a candidate for an inflection observation. The piecewise linear curve going from(Xj,Yj)to(Xj+1,Yj+1),j=1,…,nis an acceptable estimator ifYj+2-Yj+1Xj+2-Xj+1>Yj+1-YjXj+1-Xj,j=1,…,n-2,Yn+1-YnXn+1-Xn<Yn-Yn-1Xn-Xn-1The firstn-2inequalities are necessary to satisfy (ii) and the last inequality is necessary to make sure that conditions (iii) and (iv) are satisfied. We do not have to care about the requirement that all observations must be located on or below the curve, since all n observations are located on the curve in this example.In general, we are searching for a curve of connected line segments through a subset of the data points(Xj,Yj),j=1,2,…,n+1, starting at(X1,Y1)=(0,0)and ending at the estimator of the inflection point(Xn,Yn). This problem resembles the so-called traveling salesman problem (TSP) (Dantzig, Fulkerson, & Johnson, 1954), which consists of finding the shortest path through a set of data points, never visiting a data point more than once and returning to the starting point. In our problem the length of the path does not matter and it is not required that we return to the starting point. But it is required that we start at data point 1 and end at data point n. Secondly, we do not require that the path covers all data points. In fact, we expect only a subset of data points to be covered, but we maximize the number of data points visited up to and including the inflection point. Thirdly, the(i+1)th edge is required to have a larger slope compared to the ith. Finally, all observations have to be on or below the path.As in the TSP we use binary variablesbij, wherebij=1if the edge from observation i to j is used, otherwisebij=0,i,j∈{1,…,n}. We only consider abij=1as feasible ifj>i, since data are assumed to be sorted on input levels and a feasible convex path never will go from i to j, wherej<i. A requirement of at most one path into the kth data point and at most one path out of the lth data point is modeled with slightly modified assignment constraints:(13)∑k=1nbkl+slIntoNode=1,l=2,…n(14)∑l=1nbkl+skOutOfNode=1,k=1,…n-1whereslIntoNode⩾0,∀l,skOutOfNode⩾0,∀k. IfslIntoNode=1orslOutOfNode=1then the lth data point is not on the “path” that constitutes the convex part of the S-shaped estimator of the production function.17Notice that ifslIntoNode=slOutOfNode=0,∀lthese constraints are the well known assignment constraints from the LP-formulation of the TSP-problem making sure that one arc into and out of every node are used in a feasible solution.17Ifbkl=1,∀k,l,k<l,k≠1,l≠nthen an edge out of k and into l is used and we require that an edge into k and an edge out of l must be used, i.e.∑i=1nbik=1,∑j=1nblj=1⇒skIntoNode=slOutOfNode=0, or(15)skIntoNode+slOutOfNode⩽(1-bkl)M2,∀k,l,k<l,k≠1,l≠nwhereM2is a large number (hereM2must be greater than 2). Ifb1l=1then an edge into data point l is used and we require that an edge out of l must be used, i.e.∑j=1nblj=1implying thatslOutOfNode=0or(16)slOutOfNode⩽(1-b1l)M2,l∈{2,…,n-1}Ifbkn=1then an edge out of k is used, and we require that an edge into k must be used, i.e.∑i=1nbik=1⇒skIntoNode=0, or(17)skIntoNode⩽(1-bkn)M2,k∈{1,…,n-1}We prefer an estimator of the convex part of the frontier with as many data points on the frontier as possible. Hence, we maximize the sum of the binary variablesbij. Model (18) presents the full optimization problem:(18)max∑i=1n∑j=1nbijs.t.Yk-YjXk-Xj-Yj-YiXj-Xi-sijk=0i,j,k∈{1,…,n},i<j,j<k(18.1)sijk+(2-bij-bjk)M1⩾0i,j,k∈1,…,n,i<j,j<k(18.2)∑k=1nbkl+slIntoNode=1l∈{2,…,n}(18.3)∑l=1nbkl+skOutOfNode=1k∈{1,…,n-1}(18.4)skIntoNode+slOutOfNode-(1-bkl)M2⩽0∀k,l,k<l,k≠1,l≠n(18.5)slOutOfNode-(1-b1l)M2⩽0l∈{2,…,n}(onlyb1l)(18.6)skIntoNode-(1-bkn)M2⩽0k∈{1,…,n-1}(onlybkn)(18.7)Yt-YnXt-Xn-Yn-YkXn-Xk+dkterm=0k∈{1,…,n-1}(18.8)dkterm+(1-bkn)M1⩾0k∈{1,…,n-1}(18.9)bij=0i,j∈{1,…,n},i⩾j(18.10)bij∈{0,1}∀,i,jslIntoNode⩾0,skOutOfNode⩾0dterm∈Rn-1The constraints (18.1) and (18.2) are included to only allow a sequence of edges with increasing slope as feasible, whereM1is a large number. The constraints (18.8) and (18.9) are included to require that the slope starts to decrease when passing through the inflection point, i.e. when moving from the convex part to the concave part of the S-shaped frontier. The structure in (18.8) allows only one edge into data point n. Let us denote the starting point of this edge into data point n asko, i.e.bkon=1. For allk≠ko(18.9) implies redundant constraints,dkterm⩾-M1. Given the inflection point, the “termination” data point of the convex part denotedXt,Ytis determined as the data point with the maximal rate of transformation relative to the estimator of the inflection point, i.e.(19)(Xt,Yt)wheret=argmaxj>nYj-YnXj-Xn.Unfortunately, (18) does not provide a path from the origin to the inflection point with no uncovered data point above the path. Hence we have to supplement (18) with the following cutting procedure.1.Solve (18).For eachbij=1loop through all data points with the input component larger thanXiand smaller thanXjand check if any such data point is located “above” the facet spanned by[(Xi,Yi),(Xj,Yj)].If any such observation is above this facet then add the constraintbij=0to (18) and goto step 1.This procedure will either terminate with a feasible solution providing a convex part from origin to the inflection point with all data points on or below the path, or with a status being integer infeasible,18To give an illustration of a situation where the solution of (18) eventually fails with a status of being integer infeasible, consider the Fig. A1 in Appendix A with observationsA,B,C,D,E, all being FDH efficient. We choose observation C as our candidate for the inflection point, which implies that we need to find a piecewise linear convex curve from the origin to C, where C has to be located on this curve. This is clearly impossible without having observation B located above the piecewise linear convex curve. The position of the observation C in this figure reflects the position of observation 64 in the simulation, and as reported in Table 1, using this observation as the inflection observation implies that (18) eventually terminates with the status of integer infeasibility.18in which case no path exists with the required characteristics.This approach was applied to simulated data in the next section. In anticipation of our results see Fig. 4, which shows estimate of the S-shape technology with the endogenous inflection point identified as data point 66. In the next section, we analyze our approach using simulated data.We will show that our approach is capable of recovering the true S-shaped technology while simultaneously providing better estimates of technical inefficiency. Assuming one output, two inputs and homotheticity we generate data according to the following data generating processes (DGP). We specify a generalized production function (Zellner & Revankar, 1969)Y=ϕ(X)=F(g(X))where the scaling law isF(z)=151+e-5ln(z)and the linear homogenous core function is a Constant Elasticity of Substitution function:g(x1,x2)=βx1σ-1σ+(1-β)x2σ-1σσσ-1, withβ=0.45andσ=1.51. Data are generated for 1000 DMUs as follows; inputs are generated in polar coordinates as anglesηand modulusωuniformly distributed on0.05,π2-0.05and[0,2.5], respectively. Output is generated from the generalized production functionF(g(ωcosη,ωsinη)). Inefficiency is added to the input vectors withX=eθ×(ωcosη,ωsinη), whereθis a random variable from a truncated normal distribution with standard deviation 0.2.We sort the data on output and estimate (11) for each of the 1000 output levels; the solution space for each estimated isoquant is conditioned such that only DMUs with outputs greater than or equal to the ith DMUs output,i=1,…,1000are included. We thus obtain 1000 input oriented scores for each isoquant. If the input oriented score has additional slack in either input dimensions the score is assigned the value “missing”. Based on the these results, we identify for each potential base isoquant only those data points that span the conditional isoquants (i.e. only observations with input oriented score equal to one with no additional slack are included). For this simulation, we consider two criteria for choosing our base isoquant. Firstly, we are looking for the particular isoquant with as many observation on the frontier as possible. Secondly, we search for an isoquant with an empirical distribution of the angles of these data points spanning the frontier as close as possible to a uniform distribution. Taken together, we are looking for isoquants that have a lot of data points that uniformly span the isoquant. For our simulation, we identified numerous isosquants that performed well on both criteria. This result, while not surprising, is encouraging: the selection of a base isoquant for our input aggregation is robust. For our analysis, we chose isoquant 750 as our base isoquant.The inverse of the input oriented efficiency scores relative to isoquant 750,θcLC(Xl,Y750)-1,∀lare now used as indexes of aggregated input. Initiating the estimation of the inflection point and the S-shaped piecewise linear production function (one aggregated input, one output) we first remove the observations with positive slacks present in the estimation of the aggregate input index (181 observations) and observations that are FDH inefficient (732). That leaves us with a data set of 88 observations of which 16 (71) are above (below) mpss. We sort these 88 observations according to the input level, i.e. observations1,…,71are below the mpss and observations73,…,88are above the mss. First, we estimate the BCC efficiency scores based on the sample of observations from 72 to 88 since this concave part of the production function is unaffected of the exact choice of inflection point. Next, we endogenously determine which data point below mpss is the inflection point. For fixedi∈{1,…,,71}we go through the following steps:1.We focus on observation72-ias the candidate for the inflection point.Ifi>1, the BCC model is solved including only observations strictly above the candidate inflection observation and below or on the mpss, i.e. observations72-i+1,…,72. We count the number of observations on the frontier and estimate the termination marginal product in (19) withn=72-i.Ifi=1, only the mpss observation is strictly above the candidate inflection observation and below or on the mpss which means that only one observation is on the frontier. We estimate the termination marginal product of the facet from observation72-1to observation 72.Focusing on72-ias the candidate for the inflection point we solve (18) above (including the sequence of cuts) to determine a sequence of binary variables indicating a path through a number of data points below data point72-istarting at the origin and ending at data point72-iand with a monotone non-decreasing slope of the line segments along the path and no data points above the path. The optimal solution (assuming that one exists) will provide the count of data points on this convex part of the frontier.In addition we require that the slope of the line segment from data point72-i-1to data point72-iis greater than or equal to the termination marginal product estimated in step 2.Finally, we add the counts of data points on the frontier from step 1 and step 2.As an example, consideri=6with observation 66 as our candidate for an inflection point. First, we focus on the “Number of data pointsq1on the concave part of the frontier from the inflection point up to mpss”, i.e. data points66,67,…,72where five data points are located on the frontier. Next, focus is on “Number of data pointsq2on the convex part of the frontier up to the inflection point” which are the data points1,2,…,66where 27 data points are on the frontier, which implies that a total of 32 data points are on the S-shaped frontier from the origin up to the mpss.Table 1summarizes the results for ten different candidates for the inflection observations.The results of the analysis indicate that either observation 63 or observation 66 is a good choice for the inflection.19Analyzing observation 71 as candidate for the inflection point was terminated after 50 cuts. At termination, only 15 observations were on this infeasible frontier, implying that observation 71 is not a promising candidate for the inflection point.19Next, we solve (18) to estimate the S-shaped production frontier using observation 66 as the inflection point. As shown in Fig. 4, we are able to obtain a good approximation of the true underlying S-shaped technology.The next step is an analysis of the performance of the aggregation procedure to recover the true inefficiency. For this analysis we use observation 66 as the inflection point. The relationship between the distance function and the core of the production function is expressed in Proposition 1, i.e. assuming a homothetic structure,DI(yo,X)=k×g(X),k∈R+, wherek=(f(yo))-1,f()=F-1(), andF()is the scaling function. To compare the estimated radial input oriented efficiency scores based on the aggregated input with the true efficiency scores based on the original two dimensional input vector we need to estimate the conversion factork=(f(output750))-1,f(x)=F-1(x).F(x)=151+e-5logx⇒f(y)=e-15log15-yy, andk=e-15log15-9.842839.84283-1=0.87874, where 9.84283 is the level of output of the 750th data point.Figs. 5a and 5billustrates the performance of the recovery of the true scores after aggregating the inputs. In Fig. 5b all 1000 observations are included, while only the 819 observations with no slacks in the score estimation are included in Fig. 5a. It is clear from the plots that the estimated scores are biased downwards as expected for most of the observations. Especially in Fig. 5a we observe that all estimated scores are below the true scores except for one “outlier”, which turns out to be the smallest observation in the sample.Given our data generating process, we know that the BCC model will not perform well given the assumption of convexity. Nonetheless, we compared our estimates and the BCC estimates of efficiency with the true efficiency for contextual purposes. Four measures were used for the comparisons: mean squared and mean absolute deviations between estimated and true efficiency and the Pearson and Spearman rank correlation coefficients. The performance of the BCC estimator was poor; the mean squared (absolute) deviation was 0.195 (0.345) and the correlation (rank correlation) between estimated and true efficiency was only 0.178 (0.197). The results for our estimator were better. Using all observations, the mean squared (absolute) error was 0.377 (0.113) while the correlation (rank correlation) was 0.226 (0.755). However, when we include only those observations when the aggregate input was defined without additional slack, the results are much better. In this case, the mean squared (absolute) error was only 0.00305 (0.0242) while the correlation (rank correlation) was 0.934 (0.961). While this is not surprising given the data generating process, the results suggest that the degree of bias assuming convexity can be very high.Given our data generating process we will expect to see, for large samples, an estimated frontier that reveals the S-shape of the production function. To provide some insights on what sample size is necessary to get an S-shaped estimator with a estimated efficiency scores being highly correlated with the true scores, we repeat the simulation presented above for sample size50,100,200,400,600and 800. The four measures of performance mentioned above are used to evaluate the simulation, i.e. (i) the mean squared deviation, (ii) the mean absolute deviation, (iii) the correlation, and (iv) the rank correlation. The results from these additional simulations are presented in Table 2–4. For each sample size we include two sets of results. In the first column for fixed sample size only those observations when the aggregate input was defined without additional slack are used. In the second column for fixed sample size all observations are used. In the second row we include information on “Number of points compared”. First of all we clearly see a very nice coverage for most (all) sample sizes, if only the first set of results is considered. However, as expected we see declining performance for decreasing sample size. Secondly, the second set of results performs less satisfactory. However, we do observe a rather high rank correlation. Thirdly, for sample size 600 a base isoquant is chosen with rather few FDH efficient points on the lower part of the S-shape. This apparently decreases the performance of the estimator. For the first set of results we see that all four performance measures are worse than the corresponding measures for both sample size 400 and 800. This suggest that the choice of base isoquant probably should include an analysis of this phenomenon. This is, however, left for future research. Finally, for sample size 50 we see a low correlation (0.08568) if we include all observations. This is caused by an “outlier-effect” from the smallest observation in the sample. Removing this effect20Resetting the estimated efficiency score to one for the smallest of the 50 observation results in the following performance measures: (0.01026,0.06968,0.86034,0.74434).20“restores” a correlation of 0.86034.A maintained hypothesis of convexity in input–output space is often used in DEA estimations of efficiency scores. However, convexity is not consistent with standard microeconomic production theory that posits an S-shape for the production frontier. In this paper we have outlined an approach that allows for an estimation of efficiency from an S-shaped technology for the multiple inputs and one output case. To simplify, we have assumed that the technology is input homothetic. This assumption has allowed us to split the estimation procedure into two parts, (i) an aggregation procedure based on the structure of input homotheticity, and (ii) a joint estimation of the inflection point and a piecewise linear S-shaped structure for one aggregated input and one output.As an estimation procedure for individual isoquants we propose, assuming selective input convexity, the use of a simplified order-m estimation procedure. In theory, any input isoquant can be used as the base isoquant used to aggregate inputs utilizing the input homotheticity. Relative to this base isoquant, an index of aggregated input can be estimated as the inverse distance function value of any observed input vector. We have argued that in practice it is important to choose an isoquant which performs reasonably well according to the following two criteria: (i) the empirical distribution of the angle coordinates of the observed data points should mimic a uniform distribution, and (ii) a large number of observed data points should be located on the conditional piecewise linear estimator of the isoquant. To facilitate the choice of a base isoquant with good coverage we have proposed an estimation of all possible isoquants using a simplified version of the order-m estimation procedure. The simplification used is a conditional estimator of the base isoquant, which avoids the replications.Taking advantage of the reduced dimensionality (one aggregated input and one output) we have developed a model to estimate a piecewise linear S-shaped frontier where the aggregate input axis is divided to allow a production frontier that is convex and concave. In other words, we have assumed that the boundary of the true PPS is S-shaped in the sense that we can divide the input axis into two parts, where the frontier is convex (concave) on the first (second) part. Consequently, the convex hull estimator is too large and we have proposed a “digging approach” where we remove the part of the PPS that violates this S-shape. This digging approach is formulated as a joint estimation of the inflection point and the convex part of the frontier from the origin to the inflection point.Using simulated data in Section 5, we have shown that our method overcomes the inherent problems of standard DEA and provides better estimates of inefficiency when the true technology obeys the Regular Ultra Passum Law. According to an anonymous reviewer, our approach provides a suitable fit to the S-shape generated using the simulated data. We concur with the reviewer that a useful direction for future research would be a comparison using real-world data to statistically test differences between our approach and the BCC model. This will further our understanding of the trade-offs between using a theoretically better model that is more complex to implement.It is well known that the CCR- and the BCC-model provide downward biased estimates of the frontier if the true technology is convex. Maintaining that the true production possibility set is either a convex cone or a convex set extended by free disposability implies that a convex cone or a convex hull estimator will be downward biased. This is a consequence of only including one-sided radial inefficiency residuals in the non-parametric estimation. This characteristic of the estimated frontier is not globally present with the proposed S-shaped estimator. The sign of the bias is still negative for observations that are (much) larger than the true mpss (in the two dimensional space with aggregated input and a scalar output), but for observations far below mpss we could encounter en envelopment with a positive bias. As an illustration of this phenomenon, consider Fig. 3. If the true S-shape is located very close to the piecewise linear curveA,B,C,D,Eand if we choose the estimator that locates observation L on the frontier, i.e. the piecewise linear estimator isA,B,D,L,E,Fthen this estimator postulates feasible input–output combinations in the triangleB,C,D. However, combinations within this triangle are not all feasible in this situation.The approach proposed in this paper has two apparent shortcomings. First and foremost we have assumed input homotheticity which may or may not be a reasonable assumption. Hence an important extension of the approach is to allow at least for some kind of deviation from pure input homotheticity. Secondly, to simplify the presentation we have assumed only one output. Generalizing the approach to the case of multiple input multiple outputs is another important area for future research. Unfortunately, a straight forward approach based on the joint assumption of both input and output homotheticity requires some rather restrictive additional assumptions. As noted in Färe and Primont (1995) the notion of inverse homotheticity was introduced by Shephard (1970, pp. 255–257), where it is shown that this structure is sufficient for both input and output homotheticity. This result is generalized in Färe and Primont (1995), where it is shown that we have inverse homotheticity if and only if the technology exhibits simultaneous input and output homotheticity.21Färe and Primont (1995) present additional “mild” conditions under which this is true.21Hence, extending our approach to multiple inputs and multiple outputs is straightforward if the technology simultaneously exhibits input and output homotheticity, i.e., inverse homotheticity and if these “mild” additional conditions are maintained. This seems to be a natural starting point for a generalization of the approach in this paper to the case of multiple inputs and multiple outputs.The BCC model is only a possibility to estimate technical inefficiency in DEA. The proposed method could be modified to measure inefficiency by using alternative models, e.g., the Directional Distance Function (Chambers, Chung, & Färe, 1998), which is a generalization of the BCC model, the Enhanced Russell Graph or Slacks-Based Measure (Pastor, Ruiz, & Sirvent, 1999 and Tone, 2001), weighted additive measures as the Range Adjusted Measure (Cooper, Park, & Pastor, 1999) or the Bounded Adjusted Measure (Cooper, Pastor, Borras, Aparicio, & Pastor, 2011), or, more recently, the modified Directional Distance Function (Aparicio, Pastor, & Ray, 2013).

@&#CONCLUSIONS@&#
