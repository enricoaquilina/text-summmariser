@&#MAIN-TITLE@&#
Hallucination of facial details from degraded images using 3D face models

@&#HIGHLIGHTS@&#
A 3D model-based algorithm for face hallucination at any pose and illuminationA method for including non-local effects (e.g. blur) in 3D analysis-by-synthesisThe algorithm combines low spatial frequency information with details of a 3D model.Transfer of high spatial frequency details for hallucination on the level of poresOcclusion handling and seamless texture reconstruction

@&#KEYPHRASES@&#
Face hallucination,3D models,Model-based deblurring,Occlusions,Faces,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Difficult imaging conditions due to blur, low resolution, noise, partial occlusions or non-uniform lighting are frequently encountered in many real-world applications, for example in law enforcement if a suspect has to be recognized in low quality image material. A number of image processing algorithms recover and enhance information that is present in the image, yet mostly invisible to the human eye. For example, deconvolution strives to invert the effect of blurring. Often the blur kernel (or point spread function, PSF) is unknown and has to be estimated from the image. A survey of this so-called Deblurring by Blind Deconvolution problem can be found in [1], and some more recent publications in this field include [2] and [3]. For low resolution video, deconvolution can be combined with methods to merge data from all frames [4,5]. On the other hand, structures that are degraded due to occlusion can be partly recovered with image inpainting methods [6].These methods can be applied to any image material, because they make only very general assumptions about the image content. However, if it is known that part of an image shows a human face, it is possible to add new information that was not in the image to begin with. If the lower half of a face is occluded, we can still safely assume that a mouth and a chin have to be added, and we can estimate their pose angle and lighting from the upper half of the face. The same is true for blurred regions: even if the eye and eyebrow are only dark spots in the input image, we can fill in eyeball, iris, eyelashes and all other details of human eyes. Fig. 3shows how this can be done with our proposed algorithm.Given a mathematical model of the expected content of the image, such a model-based image enhancement can be achieved. In this paper, we rely on the 3D Morphable Model (3DMM) [7] as a statistical description of the natural shapes and textures of faces. It is important to stress that the added image detail cannot be more than an educated guess, based on the prior information about faces on the one hand, and all the remaining information that is in the image on the other hand.One solution would be to fill in the details from the average face, or any other random face. Our algorithm goes one step further by exploiting correlations in the set of human faces: After fitting the 3DMM to the degraded image, we obtain a best fit, then from this best fit we take the details and render these into the image, both in the case of blurred and partly occluded faces (Fig. 2). This idea is along the lines of 3D shape reconstruction from single images using 3DMMs [7], where the model is fitted to colors of pixels, and gives an estimate of depth. Recently, it has been shown that this inference is consistent with human expectation: when viewers see a frontal view of a face and then a choice of profiles that are all geometrically consistent with the front view, they tend to prefer those profiles that were calculated by the 3DMM [8], even if the choice includes the ground truth profiles of the face.With this caveat, model-based inference of missing information may be a useful tool to obtain high quality images or 3D face models from degraded input data.For images of human faces, the model-based fill-in of facial details has become known as Hallucinating Faces[9]. For a recent survey, see [10]. For low resolution images, AAMs have been used to fill in missing details [11].In contrast to this work, Baker et al. proposed a model that does not account for shape differences explicitly, and that uses a maximum a posteriori (MAP) estimator to estimate the high-resolution levels of a Gaussian Pyramid of registered images [9]. In a two-level approach based on global Eigenfaces and a local patch-based nonparametric Markov network, Liu et al. achieved very significant improvements in image resolution [12,13]. A separation of global face hallucination and local feature hallucination has been proposed in [14]. For face hallucination in video, Dedeoglu et al. use spatio-temporal consistencies and a domain-specific prior [15].Soon after their initial development, Active Appearance Models (AAMs) were used to reconstruct missing structures in occluded regions [16]. Reconstruction of facial images both in case of partial occlusion and low resolution using a 2D Morphable Face Model, which bears similarities with AAMs, has been presented in [17,18].Using separate reconstruction modules for 2D shape and texture that account for global structure and local detailed texture [19] can reconstruct occluded regions in images of faces. Another approach that is able to fill in occluded regions uses asymmetrical Principal Component Analysis (PCA) [20].All of these algorithms use a statistical model of 2D faces restricted to poses that are close to frontal. Larger variations in pose have been handled by using a Gabor wavelet decomposition of faces and a set of linear mappings between wavelet features in different poses [21], or by exploiting large datasets, recent image matching techniques and MAP estimation [22].Unlike these image-based methods, this paper proposes a 3D approach that is intrinsically invariant to changes in pose, size, illumination and other image parameters. Our strategy is to fit a 3DMM [7] to the input image with a novel fitting algorithm that is robust to the effects of blurring by explicitly simulating image blur in an analysis-by-synthesis. The algorithm works on any blur level, and estimates the appropriate level automatically. In addition to the facial details estimated by the 3DMM reconstruction (equivalent to an MAP estimate [23]), the algorithm adds details, such as eyelashes and pores, from other faces to obtain high resolution results.A method for hallucinating 3D facial shapes from low resolution 3D scans using an radial basis function (RBF) regression that predicts curvatures and displacement images at high resolution from their low resolution version was presented by [24]. Unlike our algorithm, their input data are 3D, and no texture is used.A major challenge in using a 3DMM for face hallucination or 3D reconstruction from low resolution images is to adapt the cost function to blurred input data. This challenge also occurs for generative face models in 2D, such as AAMs. An algorithm to make AAMs applicable to low resolution images was presented as Resolution Aware Fitting (RAF) [25]. Similar to our method, they include an explicit model of the downsampling or blurring in their cost function, and they compute the image difference in terms of pixels of the input image space and not in the shape-free texture space, as standard AAM would do. However, besides being a 2D approach which is restricted to frontal views, their way of treating the blur function is fundamentally different from ours: RAF is based on a Taylor expansion of the effect of the degrees of freedom of the AAM on the blurred image, so these degrees of freedom are a first order perturbation of the blurred image. In contrast, our approach treats the effect of blurring as a perturbation of the imaging process.In this paper, Sections 2 and 4 summarize components that were introduced in earlier versions of the 3DMM fitting algorithm [7,23], but they include a number of relevant, yet unpublished implementation details. On this technical basis, the entire Sections 3, 4.1, 5 and 6 describe new algorithms (Fig. 4.)Novel contributions of this paper are:•A 3D model-based algorithm for face hallucination at any pose and illumination,A method for handling blur in 3D analysis-by-synthesis,A self-adapting estimate of blur levels,An algorithm that combines low spatial frequency information from the input image with mid-level details of the model,Transfer of high spatial frequency details across faces for face hallucination on the level of eyelashes and pores,An algorithm that treats occlusions in the fitting process, and that produces seamless textures that combine details extracted from the image with those inferred by the 3DMM. Note that the occluded pixels have to be marked manually, similar to image inpainting [6].The input data for our algorithm are an image, 5-7 feature point coordinates and, if the face is partially occluded, a binary occlusion mask. The output is a textured 3D face and a rendering of this face into the input image. Please note that the feature point coordinates can also be from automated feature detectors [26].The 3DMM, of 3D faces ([7]) is a statistical model that captures the range of natural faces in terms of 3D shapes and textures. It is derived from a dataset of 3D scans of faces. The crucial step is to establish dense point-to-point correspondence of all faces with a reference face. Then, shapes and textures of all m individual faces i∈{1,...,m} in the database are represented by shape and texture vectors [7](1)Si=X1,Y1,Z1,X2,…,Xn,Yn,ZnT(2)Ti=R1,G1,B1,R2,…,Rn,Gn,BnTformed by the coordinates and colors of all n vertices of the reference model. In our model, m=200 and n=75,972.In this face space representation, convex combinations of face vectors are equivalent to morphs of the database faces, and will therefore have a natural face-like appearance:(3)S=∑i=1maiSi,ai∈0,1∑i=1mai=1(4)T=∑i=1mbiTi,bi∈0,1∑i=1mbi=1.Using PCA, the distribution of database faces can be described in terms of arithmetic means, unit-length eigenvectors and standard deviations for shape and texture:(5)s¯,si,σs,iandt¯,ti,σt,iWith this, we can rewrite Eqs. (3), and (4) in a new basis (see [7]) with the coefficients α and β:(6)S=s¯+∑i=1m−1αisi,(7)T=t¯+∑i=1m−1βiti.The fact that PCA eigenvectors are orthogonal is not used here. The relevance of PCA in our algorithm is to reduce the number of dimensions, and to obtain an estimate of the prior distribution – a multivariate Gaussian – for regularization to avoid overfitting. For linear combinations (6) and (7) the prior probabilities are:(8)pα1,...,αm−1∼e−12∑i=1m−1αi2σs,i2,pβ1,...,βm−1∼e−12∑i=1m−1βi2σt,i2.In an analysis-by-synthesis loop, the fitting algorithm [7,23] finds the shape and texture vector from the Morphable Model that fits the image best in terms of pixel-by-pixel color difference between the synthetic image Imodel(rendered by computer graphics rasterization), and the input image Iinput:(9)EI=∑x,yIinputx,y−Imodelx,y2.The squared differences in all three color channels are added in EI. We suppress the indices for the separate color channels throughout this paper. The optimization is achieved by an algorithm presented in [7,23]. The algorithm optimizes the linear coefficients for shape and texture, but also 3D orientation and position, focal length of the camera, angle, color and intensity of directed light, intensity and color of ambient light, color contrast as well as gains and offsets in each color channel.For the optimization to converge, the algorithm has to be initialized with the 2D feature coordinates (qx,j,qy,j) of at least 5 feature points j. The 2D distance between the initialization positions and the current positionsxkj,ykjof the corresponding vertices kjof the model forms an additional cost function:(10)EF=∑jqx,jqx,j−xkjykj2.A third contribution to the overall cost function is a regularization term that avoids overfitting. The regularization term measures the Mahalanobis distance of the current solution from the average face using PCA:(11)Ereg=∑iαi2σs,i2+∑iβi2σt,i2+∑iρi−ρ¯i2σr,i2,where ρ denotes the rendering parameters such as pose angle, 3D translation, illumination and contrast.ρ¯iis the starting value for these parameters, and σR,ian ad-hoc estimate of the expected standard deviation. Together, these contributions form a cost function(12)Etotal=EI+μFEF+μregEregwith weights μF, μregthat are important for initialization and stability, and that are reduced during the process of the iteration. The optimization problem is solved iteratively using a Stochastic Newton Descent algorithm [7]. The following parameters are optimized:•Shape coefficients αi,Texture coefficients βi,3D orientation and position,Focal length of the camera,Ambient light (RGB channels),Direction and color of directional light (RGB channels),Color contrast, gains and offsets in RGB channels.Calculating the image difference term EI(9) on the entire image or on all pixels of the foreground in each iteration would be very time consuming. The main idea of Stochastic Newton Descent is to consider only a random subset of points in each iteration, and proceed in small steps towards the optimum. For this approximation of EI, we could proceed in the following way:•Render the image and select a subset of pixels (x,y), which would of course bring no speedup,Select a random subset of vertices i of the 3DMM, calculate their image positions (xi,yi) and evaluate EIthere, orSelect a random subset of triangles k of the 3DMM, calculate their centers (Xk,Yk,Yk) in 3D, project them to image positions (xk,yk) and evaluate EIthere.As in [7,23], we chose option 3 for two reasons:1)The analysis-by-synthesis requires surface normals and their derivatives to account for shading effects, and these normals are the easiest to compute on centers of triangles.Each triangle k can be assigned an area akin the image space, and by setting the probability of choosing k proportional to akin the random selection procedure, the expectation value of the approximated cost function is equal to EI.akis calculated in the starting position and once every 1000 iterations on the entire face. Triangles that are invisible due to self-occlusion (z-Buffer) obtain ak=0. The approximated cost function is then [7](13)EK=∑k∈KIinputxk,yk−Imodel,k2which involves the following calculations:•3D position of center of triangle k, using Eq. (3) for all 3 vertices,Rigid transformation of triangle center, given the current estimate of pose angles,Perspective projection, which yields the image position (xk,yk) of the triangle center,Surface normal of the triangle, computed from the corner positions in 3D,Surface reflectance (i.e. RGB vertex color) using Eq. (4),Phong shading, including cast shadows (see below),Color space transformation (offset, gain, color contrast).If the algorithm was used for rendering, the color value Imodel,kwould be rasterized to pixel (xk,yk). Please note that in Eq. (13) the 2D position (xk,yk) only appears in Iinput(xk,yk), not in Imodel,k. The latter is the appearance of the model triangle no matter where it is located in the image.In this paper, we call the effects of the image formation process local if the appearance of a pixel depends only on one surface point of the mesh or perhaps its neighborhood on the mesh. In contrast, non-local rendering effects occur whenever vertices that are far apart on the mesh have influence on the same image point. In this terminology, blurring is a non-local effect because the color of a pixel depends not only on the shading of the 3D surface point that is projected to that pixel, but also on its neighbors and – this is the crucial point – also on other vertices of the mesh that happen to be rendered close by. This is the case whenever there is a depth discontinuity in the rendered image, for example along the ridge of the nose in a half-profile, or along the silhouette of the face in front of a background. A more mathematical formulation would be that the mapping from image positions to a surface parameterization is not continuous. If it were continuous, the effect of blurring could be simulated by blurring the surface texture with a spatially varying and non-isotropic filter that accounts for the effects of perspective distortion. However, due to its non-local nature, image blur is more challenging.In the rendering pipeline described in the previous section (the analysis part of the analysis-by-synthesis) blurring would be formulated in the following way: Let Imodel(x,y) be the synthetic image of the current 3DMM reconstruction. Then, a blurred image is obtained by an image-space operator(14)Imodelx,y↦φImodelx,y.Existing 3DMM fitting algorithms, such as [7,23], cannot handle non-local, patch-wise image modifications.Our strategy is to simulate the effect of φ on a rendered image Imodel(x,y) in the current iteration step, compare it to the un-filtered rendered image and store the difference Δjfor each vertex j and each RGB channel. This is done prior to the optimization and once every 1000 iterations. The difference is precomputed since its value is based not only on vertex j but rather on multiple, potentially non-adjacent vertices of the 3D face model(15)Δj=φImodelxj,yj−Imodelxj,yj.Note that Δjis attached to a vertex j and not to a pixel position, because it is computed on the model and describes the change of the model appearance in this vertex when it is rendered and modified by φ. The screen position xjand yjof vertex j on the input image Iinputvaries during the reconstruction process, because of the adjustment of the shape and pose of the 3D face model.In the cost function described in Eq. (13), which is based on triangle centers,Δ¯kof the center is interpolated from the values Δjof the 3 triangle vertices. With thisΔ¯k, EKis(16)EK=∑k∈KIinputxk,yk−Imodel,k+Δ¯k2.As mentioned above,Δ¯kdescribes the color difference of triangle center k and a non-local color value modification of the same triangle center. To verify Eq. (16), we can combine Eqs. (15) and (13). The triangle version of Eq. (15) is(17)Δ¯k=φImodelxk,yk−Imodelxk,yk.Note that there is no fundamental difference between the triangle-based and vertex based version. The triangle-based values are only interpolations of the vertex-based values for the corners of each triangle (average with weight 1/3). The reason for dealing with triangles is that in each iteration the surface normal has to be updated, and surface normals cannot be computed on individual vertices without knowledge about the neighbors.After substituting Eq. (17) in Eq. (16) we can writeEK=∑k∈KIinputxk,yk−Imodel,k+φImodel,k−Imodel,k2EK=∑k∈KIinputxk,yk−φImodel,k2.This equation shows that the extension of the error function leads to a non-local modification of the analysis-by-synthesis loop.In this paper we focus on blurred input images. With a new variable b describing the blur level, let(18)φblurImodel,j,bdescribe the effect of blurring the reprojected face model for each vertex j (or triangle center k). Hence Δjis the vertex difference between the rendered and blurred model and the non-blurred rendering. φbluris implemented by filtering the rendered image Imodelwith a 3-tap binomial filter iteratively, and b is the number of iterations. The filtering process is separable, thus different blur levels for horizontal and vertical directions can be treated. Other forms of anisotropic filtering or PSFs from motion blur are easy to implement in our approach by changing φblur.To account for a diffusion of the background color into the face region along the silhouette, we render the face into the input image before blurring and computing Δj.Low spatial frequency images may be found if images of reasonable size are blurred due to defocus or motion blur. More often, however, they occur if the image size is small. In the previous section, the operator φ() has simulated the first case by means of a blur operation (convolution). In our algorithm, effects of image size, sampling and aliasing are less obvious.Unlike the cost function (Eq. (9)), which involves a summation over pixels (x,y), the triangle-based functions in Eqs. (13) and (16) seem to be independent of image size. For each triangle center, the estimated color is computed and compared to the color in Iinput(xk,yk). If Iinputis small, many triangles will be compared to the same pixel. Our novel algorithm (Section 3) alleviates this problem becauseΔ¯k, which simulates the effect of blurring and proper down-sampling, brings the colors of model points closer to what is found in Iinput, so the multiple triangles that are mapped to the same pixel will also have more and more similar colors as b increases.Still, we have found that the optimization algorithm works better if small images are upsampled with a Gaussian kernel to a standard minimum size of 400×400pixels for the face region. The reason is that the calculation of image gradients is more reliable if the relevant structures of Iinputare well above the pixel resolution. It is easy to determine how much the image has to be scaled up since the positions of 5 feature points are available for initialization of the fitting algorithm anyway (Section 2).As mentioned above, it is necessary to specify the blur level in the vertex blur function φblurwhich is used by the new error function (see Eq. (16)). To obtain the best reconstruction results, this blur level should be equal to the blur level of the input image. Thus, an identification and measurement metric for the blur level of a given input image is required.In this paper we use the blur metric proposed in [27]. This metric is based on the smoothing effect of sharp edges by measuring the spread of edges in an image. To detect the edges, a Sobel filter is applied to the luminance of pixels. In order to separate the gradient image from noise and insignificant edges, a thresholding is then applied. The start and end positions of an edge are defined as the locations of the local luminance extrema closest to the edge [27]. In other words, the distances between the nearest local maxima and minima to an edge are used as the local blur measure for the current edge location. By averaging the local blur values over all edges, the global blur metric of the image is calculated.Given this blur metric on the input image, we still need to find out the appropriate blur level for the model. The measured overall blur depends on the size and the content of the images, so that two images with the same blur level may have different overall blur measures. This could be partially avoided by using machine learning algorithms on images with controlled blur levels. In this paper we propose a novel self-adapting blur measurement that has the potential to operate on a wide variety of input images (different sizes, illuminations, blur levels).After the first set of iterations of the fitting algorithm, we have a very conservative first estimate of head shape (estimate head) and an estimate of the rendering parameters (e.g. camera pose, focus, lighting conditions). Rendered into Imodel(x,y), this estimated head is already roughly aligned with the input face in Iinput(x,y). A binomial 3-tap low-pass filter is applied subsequently on the rendered image Imodelto simulate different levels of blur. The 3DMM makes it easy to know where facial regions with significant edges in Imodelare, for example in the eye and mouth region. The blur metric is calculated only in these regions of Imodeland Iinput.To estimate the appropriate blur level b, the low-pass filtering is applied sequentially b times to Imodeluntil the blur measures on both images are approximately equal.Due to the limited texture resolution of the 3D scanner and residual errors in the calculation of correspondences when the 3DMM was built, the textures of our 3DMM faces tend to be blurry. For the self-adapting blur calculation described above, we apply a sharpening operator to the texture of the estimate head (Fig. 5). The sharpened texture is created by using an unsharp masking filter [28].Still, the resolution of the estimate head is limited. If the blur measure of the input image is equal to or even smaller than the blur measure of Imodel, no blur compensation is done by the analysis-by-synthesis process: b=0 and all Δj=0.To evaluate the reconstruction quality of the proposed non-local 3DMM fitting algorithm with the unmodified method [23], input images with different blur levels are reconstructed and compared with the reconstructions from unmodified input images. The Multi-PIE database [29] is used for this task, since it contains many different views of a large number of persons. For evaluation, all images are blurred by filtering and downsampling using a Gaussian image pyramid with different levels [30] and expanding to the original image size. In this way, the lowpass filter used to generate the input images differs from the binomial filter kernel used in the analysis-by-synthesis method.If G0 is the unfiltered image of the image pyramid, we measured the blur intensity of the expanded images of levels G1, G2 and G3[30] with the method described in Section 3.2. The average of the estimated blur levels for G1 is 4, for G2 is 12 and for G3 is 34. For a consistent terminology, we use this metric to describe the blur intensity of the input images instead of the stage of the image pyramid.Figs. 6 and 7show examples of reconstructed 3D faces. The input images for the first row of Fig. 6 are shown in Fig. 8c. The other examples of Figs. 6 and 7 are also reconstructed from an estimated average blur level of 12.Fig. 6 illustrates the quality improvement with respect to 3D shape. With no blur compensation, the reconstructions show obvious shape artifacts compared to the original and the blur-compensated reconstructions, for example a strong bulge above the eyebrows (first row), small chin, big lips and missing hump on the nose (second row).Also the quality of the texture estimate is improved substantially by the proposed method (Fig. 7). Obvious enhancements are visible especially in the eye regions (shape and color of pupils and eyebrows).During a manual evaluation of about 500 3D reconstructions, no evidence was found that the proposed method generates reconstructions with lower quality than the unmodified algorithm. In contrast, the quality of shape and texture was improved in most cases (as in Figs. 6 and 7) and comparable to the reconstructions from the unblurred input images.For an objective evaluation of the reconstruction quality we use the Mahalanobis distance. This distance compares 3D face reconstructions in PCA space, taking into account how much the faces vary in different directions in face space in terms of shape or texture.As described in Section 2, the shapes and textures of 3D face reconstructions are described by linear combinations of eigenvectors (see Eqs. (6) and (7)). The Mahalanobis distance calculates the difference of two reconstructions by using the coefficientsα→andβ→of the linear combinations. Hence(19)dsα→ref,α→sample=∑k=0mαref,k−αsample,k2σs,k2describes the Mahalanobis distance for shape and(20)dtβ→ref,β→sample=∑k=0mβref,k−βsample,k2σt,k2the distance for texture coefficients. Hereα→refandβ→refare the shape and texture coefficients of the reference reconstruction (unblurred image) andα→sampleandβ→sampleare the coefficients of the reconstruction from blurred images. σs,iand σt,iare the standard deviations for shape and texture from PCA calculation along every principal component. For evaluation, we reconstructed all 249 images of persons in the first session of the Multi-PIE database. This dataset includes faces from different ethnic groups (Afro-American, Asian, Caucasian, Indian), and some are wearing glasses. We reconstructed all persons from the original and blurred images with 3 different blur levels (average estimated blur level of 4, 12 and 34, see Section 3.2) without and with blur compensation incorporated by our model. Fig. 8 shows an example of the input images.The shape and texture coefficients of the unblurred reconstruction are used as reference, and we compare these to the coefficients of the uncompensated reconstruction and the proposed blur-compensated reconstruction using Mahalanobis distance.The average Mahalanobis distance of all 249 reconstructions for the 3 different blur levels for texture and shape is shown in Fig. 9. In all cases the average distance between the blurred and the ground truth image decreases with increasing blur levels. Especially the shape reconstructions get closer to the unblurred input image. Even in slightly blurred input images (estimated blur level n=4, Fig. 8b), the Mahalanobis distance decreases in 70.28% (175 of 249 cases) of the 3D reconstruction concerning the shape coefficients and in 73.89% (184 of 249 cases) concerning the texture coefficients. For higher blur levels (estimated blur level n=12, Fig. 8c) the blur compensation becomes more visible. Thus for shape, in 95.58% (238 of 249 cases) the blur-compensated reconstructions are closer to the reconstruction of the unblurred input image. Also the texture coefficients are in 81.92% (204 of 249 cases) closer to the unblurred reconstruction. In input images with strong blur (estimated blur level n=34, Fig. 8d) we decrease the Mahalanobis distance concerning shape in 84.73% (211 cases) and in 71.88% (179 cases) concerning texture.The result of the model fitting algorithm is a textured 3D model of the face. The texture vector T contains one set of RGB color values per vertex, and it is the optimal linear combination of database vectors (Eq. (4)).However, it is desirable to have true u,v texture mapping with high-resolution textures, for the following reasons:•The resolution with texture vectors that have one color per vertex is relatively low. The definition of these vectors was adapted to the resolution of the database scans. After fitting the model to high quality photos, details such as eyelashes can be captured only with a high resolution u,v texture.The linear combination (4) has only a limited number of degrees of freedom and it can only reproduce structures that are found in at least one of the database faces. Details (eyelashes, birthmarks) cannot be reproduced with the model-based approach directly.Even on blurred images, there may be individual characteristics on a low spatial frequency domain that are not in the degrees of freedom of the 3DMM, for example larger blemishes, or facial hair.The linear combination of texture vectors cannot capture these individual details from the photo, so the following texture extraction procedure [7] maps them to the model.Let TTE(u,v) be an RGB texture for the facial mesh. The resolution of TTEmay be any value that is appropriate to capture the details seen in the image. For each vertex j, we define a texture coordinate uj,vj.In order to extract TTEfrom an image, we rely on the fact that the image position xj,yjof each vertex j is known after fitting. The corners of a triangle k of the mesh have x,y coordinates in image space and u,v coordinates in texture space. For each texel (integer pair u,v) in the texture triangle, we calculate the barycentric coordinates, and use the same coordinates to calculate the corresponding point x,y in the triangle in image space. TTE(u,v) is then obtained by sampling the image in the non-integer position x,y using bilinear interpolation between 4 adjacent pixels.With the procedure described so far, all illumination effects in I(x,y), including specularities and shadows, would simply be mapped on the surface, so new illuminations and new poses could not be rendered correctly. Illumination-corrected texture extraction [7] solves this problem by inverting the effects of lighting in each texel. After fitting, the pose and the illumination of the face are known, since pose and illumination are among the parameters that are optimized. Also, the surface normal of each point is known. Given I(x,y), the algorithm inverts the effect of color contrast, subtracts the specular reflection using the surface normal, and finally inverts the effect of Lambertian shading. As a result, the algorithm outputs the reflectance values in each color channel and stores them in TTE(u,v). Subsequent rendering will then again multiply the reflectance with the Lambertian shading, add specularities and change the color contrast to obtain a realistic view in new rendering conditions. Note that the algorithm will exactly reproduce the input image I(x,y) when the textured face is rendered with the estimated pose and lighting of the photo. However, texture extraction from a low resolution input image would remove the details introduced by the 3DMM, so we need a modified procedure.As mentioned in this Section individual characteristics can occur on a low spatial frequency domain that are not in the degrees of freedom of the 3DMM. We use a modified texture extraction to improve the texture estimated by the 3DMM (vector T in Eq. (4)).In the first step, we calculate an enhanced input image(21)IEx,y=IInputx,y+IModelx,y−φIModelx,ythat contains all the image details that are in IModelbut are washed out in φ(IModel). These are texture details, for example the iris or the sharp edge of the eyebrows, but also details due to shading, for example at the nose. Both are missing in IInputand φ(IModel). Examples of IEare shown in Figs. 10 and 11.With this enhanced input image, we perform illumination-corrected texture extraction as described in the previous section:(22)IExy↦TETEu,v.Note that the illumination correction inverts the effects of shading, so the color of the nose will be constant skin color again, as desired.Although the methods described in the previous sections already provide substantial enhancements to blurred input images, they cannot go beyond the level of detail represented in the 3DMM (see Section 2). The fine structures of hair or skin, including pores and slight dermal irregularities, are not recovered.To cope with this drawback, we propose an additional method which adds details above the spatial frequencies captured by the 3DMM. These facial details are derived from a database of high-resolution photos of faces and are transferred to new individuals during a postprocessing step of the 3DMM fitting. The basic idea is to find one matching face (see Section 5.2) in a high-resolution photo collection for each low-resolution input image and to use it as a basis for the transfer of skin features. In Section 5.3 a modification to this approach is described, which does not search for an entire face that matches the one in the input image, but to find matching facial regions and thus combines the skin information of different faces. Currently our high-resolution database contains 221 individual faces (79 female and 142 male persons) from the Multi-PIE face database [29].While Scherbaum et al. [31] applied the makeup of one person to the face of another one, the fundamental idea of our approach is to transfer facial details, so that the formerly low resolution texture Ti,L(u,v) of person i gets transformed into a high-resolution texture. In the following, the result of this process is denoted as Ti,L→H(u,v).Before any facial details can be transferred, it is necessary to extract them from a low resolution texture Tj,L(u,v) and a corresponding high-resolution texture Tj,H(u,v) of a person j. We assume that the facial details are equivalent to the difference between the low and the high-resolution textures of the same person. These high spatial frequencies are stored in the texture Tdiff(u,v), so that(23)Tdiffu,v=Tj,Hu,v−Tj,Lu,v,where Tj,H(u,v) is the extracted texture from a high-resolution image of person j, while Tj,L(u,v) originates from a low resolution image of the same person j. Both images are stored in our facial database. Nevertheless, the low resolution image is mostly just a Gaussian blurred high-resolution image. The details of the texture extraction of our 3DMM are described in Section 4.1.Since the 3DMM guarantees a dense point-to-point correspondence between each texel (u,v) of all fitted individuals, which is a key prerequisite for a successful transfer of details, the difference texture Tdiff(u,v) can be added to the extracted (low resolution) texture Ti,L(u,v) of any other face to create a convincing high-resolution texture Ti,H(u,v). Therefore the transferred texture Ti,L→H(u,v) can be simply written as(24)Ti,L→Hu,v=Tj,Hu,v−Tj,Lu,v+Ti,Lu,v=Tdiffu,v+Ti,Lu,v.An example of a typical Tdiff(u,v) texture is shown in Fig. 12. While in Fig. 12a the whole cylindrical texture mask is displayed, Fig. 12b allows a more detailed look at the region of the right eye as well as its eyebrow. Not only the patterns of the eyelashes and brows were extracted, but also the high frequency differences of the dermal texture.Since it is unusual that the extracted information of one face perfectly represents the missing details of another face, in many scenarios it is necessary to locate pairs of textures that are similar to each other. Therefore the PCA coefficientsβ→inwhich belong to the low resolution face texture of the input image are compared with the coefficients of each low resolution sampleβ→dbof our facial database. For this comparison we calculate the Mahalanobis distancedtβ→in,β→db(see Eq. (20)) betweenβ→inand every availableβ→dband transfer the facial details from that individual, where the computed distance is minimal. Quantifying the distances in PCA space to detect the nearest neighbor instead of computing the absolute differences ensures that the most significant features are regarded for the similarity measurement.To preclude that similar eye, nose and mouth regions between a female and a male face lead to a transfer of beard stubble into the female face, all images in the database are labeled, so that only the details from individuals of the same gender are taken into consideration during the distance measurements and finally for the transfer of facial details. Please note that the region based approach presented in Section 5.3 does not presuppose a labeling of the high-resolution images of our database.The best fitting candidates based on the computed Mahalanobis distance between the low resolution input image Ti,L(u,v) and every face in the database are shown in Fig. 13a and b while those candidates which differ a lot from the input image are shown in Fig. 13c and d. The left part of each image in Fig. 13 shows the original photo of each candidate. The final result Ti,L→H(u,v) after transferring the facial details by adding the difference texture Tdiff(u,v) to the input image is depicted in the right part.Even though the transfer of details in Fig. 13c and d for images with a high Mahalanobis distance adds unwanted facial features to the input image, the strong point-to-point correspondence which is guaranteed by the 3DMM (see Section 2) prevents completely unconvincing results. As can be seen for example in Fig. 13c, where the photo on the left, which is used as high-resolution image to extract the facial details, differs a lot from the low resolution input image in Fig. 1a.Sometimes it is challenging to find a face where all facial regions (eyes, nose, mouth, ears, etc.) are similar enough at once to be useful for the texture transfer. For example there may be strains of hair on the forehead of some individuals from the image database while there is no hair on the forehead of the person in the input image. Therefore we implemented an approach to assemble the details of different facial regions by looking for best matching areas and combine the details of several individuals in the process of the high-resolution texture transfer. This is done by computing the Mahalanobis distance not only for the whole face but also for each region separately. To avoid visible transitions at the border of the different regions a texture blending is applied.Searching for matching facial regions instead of looking for similar looking faces provides an additional advantage: If we compare whole faces like in Section 5.2 local differences were sometimes overruled by global similarities. Sometimes this led to errors like the transfer of beard stubble into a female face. By labeling the high-resolution images based on the person's gender, we could avoid this kind of artifacts to some extent. Nevertheless, applying a region based approach to detect the best matches provides a solution to this problem without the necessity of labeling the images of the database.Nevertheless, artifacts may appear if a global similarity of two segments is co-occurring with strong local differences. This was occasionally observed in the region of the eyes or the eyebrows. An example of this artifact is shown in the close-up of the iris in Fig. 14a, where the artificially added details look like misplaced contact lenses.To handle these remaining imperfections we apply an image warping to the difference texture Tdiff(u,v) so that it fits better to the low resolution input texture Ti,L(u,v). As a basis for this transformation the optical flow between an artificially sharpened image of Ti,L(u,v) and the corresponding high-resolution texture Tj,H(u,v) from the database is estimated. The result after the application of the warped difference texture is shown in Fig. 14b.The result of the described transfer of texture details is shown in Fig. 15. While we obtain already strong enhancements with the model based deblurring (see Section 4.1) as is presented in Fig. 15c, the transfer of details improves the visual quality even more (see Fig. 15d).Although the transferred details for the small hairs of the eyebrow do not match perfectly to the blurred eyebrows of the input image, our algorithm provides a visually convincing result as can be seen at the top of Fig. 15d. Furthermore the fine skin structure of the lips is well transferred into the formerly blurred input image (see Fig. 15d).Nevertheless we would like to point out that it is not invariably the case that all features in the face are perfectly restored. For example, in Fig. 14a the recovery of the iris is not as good as in Fig. 15d, even though the corresponding Mahalanobis distances for the eye regions do not differ significantly from each other. Even after the application of the optical flow and image warping operation (see Fig. 14b), which has been described in Section 5.4, the reconstruction in Fig. 15d is superior. Accordingly a future improvement might be to reduce the size of the different regions and to combine all regions in a tree structure from coarse to fine. This should allow our approach to adapt better to even small differences between the facial textures.Another common problem that influences the quality of the reconstructed 3D models is occlusion of facial regions, for example due to sunglasses, hats, scarfs, beards or hair covering parts of the face. The presence of such facial occlusions is quite common in real-world applications and lead to visible artifacts in the 3D reconstruction (see Fig. 16). They affect the reconstruction and the texture extraction step.Without explicit handling of occlusions, the fitting algorithm tries to simulate the color of occluded areas, which differ in most cases significantly from skin-colors, by changing the lighting conditions and choosing a linear combination of textures that reproduces the appearance of the occluder as good as possible (see Fig. 16c). Since lighting estimation is a crucial step for the 3DMM, the reconstructed texture from input images with occluded facial regions have poor quality.If relevant regions like eyes or mouth are completely or partially hidden, the estimation of the 3D shape may also be affected. In the post-processing step of texture extraction, the occluding object is mapped directly on the 3D shape and generates wrong texture maps as a result. In this paper, we take occlusions into account both in the fitting algorithm and in texture extraction.The occluded regions of the face have to be detected and marked. In our approach, this is done manually using a paintbrush tool. We have experimented with automated methods to detect occlusions, but due to the fact that the 3DMM can partly adapt to non-face pixels even with relatively conservative settings (regularization), it remains unclear how an automated criterion could distinguish what is part of a human face and what is not [32].The occluded pixels are stored in a binary occluder image mask that has the same size as the input image. Fig. 17shows an example. Note that it is not a problem if pixels of the background are also marked as occluders, because these are not considered in the cost function (Eq. (13)).The algorithm has to be initialized with the feature coordinates of at least 5 feature points and – this is new – with an occluder image mask. In the reconstruction algorithm the binary mask image is taken into account during the calculation of the image difference term EK(see Eq. (13)).The pixel position of each of these triangles is calculated by rendering and rasterizing the color value Imodel,kto pixel (xk,yk) (see Section 2.3). Then, the visibility of a subset of triangles is tested consecutively. If the pixel position of one of these triangles is occluded, meaning that this pixel is marked in the occluder image mask, the current triangle is rejected and another triangle is chosen randomly. This is repeated until a non-occluded triangle is found. Since the visibility test is done for all triangles of the subset, in the end the cost function in Eq. (13) is determined only on visible triangles.An explicit handling of occluded facial regions is also necessary for the texture extraction algorithm, to prevent occluding objects from being mapped on the reconstructed 3D shape.As described in Section 4.1, the texture map assigns a 2D texture coordinate to every vertex in the 3D face reconstruction. In a first step, we determine the occluded regions on the texture map, by generating an occluder texture mask for the texture map automatically from the given occluder image mask. The occluder texture mask is very similar to the occluder image mask explained above. The only difference is that the occluder image mask describes which pixel of the input image is occluded and the occluder texture mask describes whether a vertex in the texture coordinate is visible or not (see Fig. 18for comparison). To calculate an occluder texture mask, the rendering parameters estimated by the fitting algorithm are used to reproject the reconstructed 3D face into the original image space. Afterwards we have calculated every pixel position (xn,yn) in the input image of each of the n=75,972 vertices. With the occluder image mask we can check the visibility of every vertex and can mark it in the occluder texture mask.After classification whether a vertex in the 2D texture coordinate is visible or not, it is necessary to fill up the missing data with plausible texture information. For this we propose two methods for occluded texture hallucination.The first algorithm uses the calculated texture from the 3DMM to fill up the missing texture data (see Fig. 19b and c). One drawback of this method is the lower resolution compared to the extracted texture from the input image. Especially in highly textured regions, such as the eyes or the mouth, the decreased quality of the estimated texture becomes salient. To avoid this, the second method utilizes the high symmetry of faces by mirroring texture from the visible half to the occluded regions, if possible (Fig. 19d and e). In cases where it is not possible to mirror texture the first algorithm is used as fallback option.One remaining problem in both occluded texture reconstruction methods are visible seams along transitions between extracted and reconstructed texture (see Fig. 20). These artifacts originate from slightly different color, structures and overall brightness. We address this by using Poisson image editing [33] for the reconstruction of the texture. The principle of this gradient-based stitching algorithm is fusing the derivatives of signals instead of stitching the signals themselves. An advantage of this method is that the intensity differences between the derivatives are relative, and not absolute as in the original signals. Thus, differences in the amplitude of the two signals have no influence in their gradient fields.In our texture reconstruction approach, we use Poisson image editing to stitch the extracted texture and the reconstruction of the occluded texture (either the calculated texture from the fitting algorithm (see Section 2) or the mirrored texture). See Fig. 21for an example.Figs. 22 and 23show typical results of 3D face reconstructions with hallucination from occluded input images. The examples consist of artificially generated occlusions and non-artificial occlusions (e.g. hair covering parts of the face or glasses). Unoccluded input images (ground truth) are depicted in the first row in Fig. 22. The second row shows the related input images with artificially generated occlusions. 3D reconstructions of the occluded input images are shown in the last row and the reprojected and relighted reconstructions are shown in the third row. Fig. 23 illustrates reconstructions from image with natural occlusions. Shape and texture can be reconstructed despite occlusions. The first row shows input images with occlusions and the second row shows the reprojected and relighted 3D reconstruction. The 3D reconstruction is depicted in the third row.

@&#CONCLUSIONS@&#
