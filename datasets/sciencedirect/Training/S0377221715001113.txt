@&#MAIN-TITLE@&#
Network construction problems with due dates

@&#HIGHLIGHTS@&#
Two new network construction problems are introduced.Complexity results are presented.MILP formulations and a branch-and-bound algorithm are developed.Novel lower bounding technique based on Steiner tree relaxations is used.Computational experiments use random instances and real-life data.Branch-and-bound algorithm is currently the best exact method.

@&#KEYPHRASES@&#
Scheduling,Network design,Network construction,Emergency restoration,Integrated network design and scheduling,

@&#ABSTRACT@&#
A network needs to be constructed by a server (construction crew) that has a constant construction speed which is incomparably slower than the server’s travel speed within the already constructed part of the network. A vertex is recovered when it becomes connected to the depot by an already constructed path. Due dates for recovery times are associated with vertices. The problem is to obtain a construction schedule that minimizes the maximum lateness of vertices, or the number of tardy vertices. We introduce these new problems, discuss their computational complexity, and present mixed-integer linear programming formulations, heuristics, a branch-and-bound algorithm, and results of computational experiments.

@&#INTRODUCTION@&#
We consider the type of network construction problems introduced in Averbakh (2012) and Averbakh and Pereira (2012). The base model used in Averbakh and Pereira (2012) and in this paper is as follows. A transportation network needs to be constructed by a server (construction crew) that is initially located at one of the vertices (the depot). The server can build edges of the network with a constant speed, and can travel within the already constructed part of the network with infinite speed (in practice, travel times are typically negligible with respect to construction times). It is required to develop a construction schedule (an order of building the edges) that minimizes some scheduling objective that is a function of the recovery times of vertices, where the recovery time of a vertex is the time when the vertex is reached by the server for the first time and becomes connected to the depot.As discussed in Averbakh and Pereira (2012), this base model is applicable in a variety of application settings, most notably: (a) when an extension of an existing transportation network needs to be constructed (then, the depot represents the existing network), and (b) in emergency situations when some local part of a transportation network (e.g., subway system, road/railway network, mine system, etc.) has been damaged/destroyed by a disaster (e.g., flood, earthquake, hurricane, landslide, etc.) or terrorist activity that led to disconnection of some vertices/parts of the network from the rest of the network, and it is required to plan restoration of the damaged/destroyed edges to restore access to the disconnected vertices (then, the depot represents the survived main network). For a discussion of these applications, the reader is referred to Averbakh and Pereira (2012).The specific problem studied in Averbakh and Pereira (2012) was to minimize the sum of the (weighted) recovery times of the vertices (the Flowtime Network Construction Problem). In this paper, we assume that vertices have due dates for recovery, and consider two problems: minimizing the maximum lateness of the vertices and minimizing the number of tardy vertices. These models are particularly important in emergency situations when some vertices are disconnected from the main network as a result of a disaster or terrorist activity, and people/equipment can be trapped there. A deadline or due date for recovery of a vertex may represent a self-sustainability limit of the vertex, i.e., a limit on the time that the vertex can remain disconnected from the rest of the network without risk to people/equipment trapped there. We introduce these problems, discuss their computational complexity, and present mixed-integer linear programming (MILP) formulations, heuristics, a branch-and-bound algorithm, and results of computational experiments based on randomly generated instances and instances generated from data on infrastructure restoration works after the 2010 Chilean earthquake (Chilean instances). The experiments show that our branch-and-bound algorithm outperforms CPLEX applied to available MILP formulations. For example, the branch-and-bound algorithm was able to solve all Chilean instances to proven optimality within seconds, while CPLEX applied to our MILP formulations ran out of memory for most of the Chilean instances.We note that network construction problems such as in Averbakh (2012) and Averbakh and Pereira (2012) and in the present paper combine network design and scheduling elements. They also have some routing aspects, in the following sense: (a) The problems are the limiting case of the routing-scheduling problems where the travel speed of the server is finite but very large with respect to the construction speed; (b) at any instant, the only edges available for construction to a server are those edges that are adjacent to the current position of the server in the modified network where the already constructed parts are shrunk to points, and thus the problems can be viewed as routing-scheduling problems in a dynamically changing network. Property (b) is particularly important in multi-server/multi-depot settings such as those considered in Averbakh (2012); the polynomial algorithms for path networks in Averbakh (2012) heavily use this property.Let us give a brief overview of related work where issues of scheduling construction/restoration activities in a network were studied. Guha, Moss, Naor, and Schieber (1999) considered the power outage recovery problem where it is required to restore connectivity between customer vertices and generator vertices in an electric power network when some relay vertices fail. The repair of the failed vertices occurs in stages where due to a budget constraint only a subset of failed vertices can be repaired in every stage, and the goal is to minimize the total weighted waiting time of disconnected customers. For this problem, Guha et al. (1999) presented a number of theoretical approximation results.Averbakh (2012) showed that a very broad class of network construction problems with multiple servers and depots is solvable in polynomial time if the underlying network is a path.Nurre, Cavdaroglu, Mitchell, Sharkey, and Wallace (2012) consider integrated network design and scheduling (INDS) problems where capacitated arcs are installed in a network to increase the maximum flow that can go through the network, and it is required to schedule the arcs’ installation on a set of parallel identical work groups. The objective is to maximize the cumulative (over a finite time horizon) weighted flow through the network. An integer programming formulation, valid inequalities, and heuristic dispatching rules are proposed. Cavdaroglu, Hammel, Mitchell, Sharkey, and Wallace (2013) consider similar problems involving a set of interdependent networks. Nurre and Sharkey (2014) consider INDS problems where the performance of the network is evaluated based on such characteristics as the maximum flow value, minimum cost flow value, shortest path length (with single or multiple destinations), and the minimum spanning tree length. They prove that the considered problems are NP-hard, and present a heuristic algorithmic framework based on dispatching rules. The INDS problems considered in Nurre et al. (2012), Nurre and Sharkey (2014) and Cavdaroglu et al. (2013) also combine network design and scheduling decisions; they are general and can capture a variety of settings and objectives including some connectivity-based objectives, e.g., the sum of the weighted recovery times of vertices. However, the specific INDS models considered in Nurre et al. (2012), Nurre and Sharkey (2014) and Cavdaroglu et al. (2013) do not seem directly suitable for representing the objectives considered in the present paper, i.e., the maximum lateness or the number of tardy vertices, as they are based on the overall network characteristics (the maximum flow value, the minimum spanning tree length, etc.), rather than on the lateness of individual vertices with respect to their due dates. From the modeling perspective, a difference between the INDS problems from Nurre et al. (2012), Nurre and Sharkey (2014) and Cavdaroglu et al. (2013) and the network construction problems from Averbakh (2012), Averbakh and Pereira (2012) and the current paper is that the models from Nurre et al. (2012), Nurre and Sharkey (2014) and Cavdaroglu et al. (2013) do not assume that the servers (construction crews) can travel only in the already constructed parts of the network. This makes the solution spaces different. From the solution perspective, the difference is often not important in the single-server/single-depot case (Averbakh and Pereira, 2012), but is important in multiple-server/multiple-depot case as it affects optimal solutions and the optimal objective value (Averbakh, 2012).Elgindy, Ernst, Baxter, Savelsbergh, and Kalinowski (2014) and Engel, Kalinowski, and Savelsbergh (2013) study incremental network design problems where at each stage one edge is installed, and the objective is to minimize the total length of the shortest path (Elgindy et al., 2014) or the minimum spanning tree (Engel et al., 2013) over all stages. Connectivity issues do not arise in these problems since in the initial network the vertices of interest are assumed to be already connected. Kalinowski, Matsypura, and Savelsbergh (2015) consider similar problems where the objective is to maximize the cumulative maximum flow in the network over all stages. This problem can be viewed as a special case of the problem considered in Nurre et al. (2012).The structure of the paper is as follows. In Section 2, we formally introduce our problems. In Section 3, we study their computational complexity. In Section 4, we develop MILP formulations. In Section 5, we describe heuristics used for obtaining initial solutions, and develop lower bounds to be used in a branch-and-bound approach. In Section 6, we present some structural results, and in Section 7 we describe a branch-and-bound algorithm. Results of computational experiments are discussed in Section 8. Some conclusions are stated in Section 9.LetG=(V,E)be a connected network with the set of verticesV={1,2,…,n}and the set of undirected edges E. To avoid confusion, nodes of the network G will always be called vertices, reserving the term “nodes” to nodes of branch-and-bound search trees. For each edge {i, j} ∈ E, let cij> 0 be the length of the edge,cij<+∞. The edges of the network represent connections (e.g., roads) that have to be constructed. A server (construction team) that is initially located at vertex 1 (the depot) can build edges with the speed of 1 unit of length per unit of time. The server can travel within the already constructed (connected) part of the network with infinite speed; that is, at any time, the server can immediately relocate to any point of the already constructed portion of the network. The server starts working at time 0. The instant Ciwhen the server reaches a vertex i for the first time is called the recovery time of the vertex; this is the instant when the vertex becomes connected to the depot. A due date diis associated with each vertex i > 1; vertex 1 (the depot) does not have a due date. We assume that vertices are indexed according to non-decreasing due dates, i.e. d2 ≤ d3 ≤ ⋅⋅⋅ ≤ dn. ValueCi−diis called the lateness of vertex i. A vertex with positive lateness is called a tardy vertex. Our goal is to find a possible schedule of constructing the edges so as to minimize a scheduling objective. This problem will be referred to as Problem L if the objective is to minimize the maximum lateness of the vertices, and Problem T if the objective is to minimize the number of tardy vertices. LetZL*andZT*be the optimum objective values for Problems L and T, respectively.Edges constructed before all vertices have been recovered are called essential. Clearly, there is an optimal solution without preemption, where the server does not interrupt construction of an edge once it has been started. After a choice of essential edges has been made, the order of building other edges is not important. Observe that there is always an optimal solution where the essential edges form a spanning tree, which allows us to structure the set of feasible solutions as follows: it is required to choose a spanning tree of essential edges and an optimal order of constructing the essential edges.We also discuss Problem F where it is required to find a solution that meets all due dates, or to show that no such solutions exist. This problem corresponds to the situation where the due dates are interpreted as strict deadlines. Clearly, to solve Problem F, it is sufficient to solve Problem L or T and check whether the optimal objective value is greater than 0. However, we will see that often Problem F is easier to solve than Problem L or T, and therefore is worth a separate discussion.It is relevant to highlight here the main common and different elements of the solution approaches in this work and in Averbakh and Pereira (2012). The set of feasible solutions in our problems and that of Averbakh and Pereira (2012) is the same, but the objective functions are of a very different nature. Hence, the logical structure of the branch-and-bound algorithm in Averbakh and Pereira (2012) and in the present paper is similar, but the lower bounding techniques are very different. In Averbakh and Pereira (2012), the main lower bounds are based on the cumulative matroid theory developed in Fischetti, Laporte, and Martello (1993) and matroidal relaxations. In this paper, we develop an original lower bounding technique for Problem L based on evaluating optimal values for some Steiner tree subproblems. The key observation is that although there are exponentially many lower bounds that can be obtained this way, the best of them can be obtained using only a linear number (instead of exponential) of Steiner tree subproblems. To make the approach practical, the optimal values of the Steiner tree subproblems are evaluated approximately. Also, in this paper we introduce some fast node evaluation/fathoming rules for Problem L that do not have analogs in Averbakh and Pereira (2012) and that significantly improve the performance of the branch-and-bound algorithm. Additionally, we prove new structural properties that are needed to justify dominance tests in the branch-and-bound algorithm; for the problems in Averbakh and Pereira (2012) this was unnecessary. In the basic “slot-based” MILP formulations in the present paper, the constraints and variables that model the set of feasible solutions are the same as in one of the formulations from Averbakh and Pereira (2012), but variables and constraints that relate feasible solutions with the corresponding objective function values are entirely different. Also, we develop time-indexed MILP formulations; time-indexed formulations were not used in Averbakh and Pereira (2012). The experimental part of the work is based not only on randomly generated instances as in Averbakh and Pereira (2012), but also on instances derived from the data on infrastructure restoration works after a real-life disaster—the 2010 Chilean earthquake.To minimize the amount of repetition but at the same time make the paper self-contained, throughout the text, if we use ideas/constructions similar to those used in Averbakh and Pereira (2012), we will provide a brief description with a reference to Averbakh and Pereira (2012) for a more in-depth discussion.Theorem 1Problems L, T, and F are strongly NP-hard.We use a reduction from the Steiner tree problem which is known to be strongly NP-complete (Garey and Johnson, 1979).Problem STEINER. Given a networkG′=(V′,E′)with the set of vertices V′ and a set of positive-length edges E′, a set of terminal vertices VT⊂V′ and a positive number K, is there a connected subnetwork of G′ of length not greater than K that spans all terminal vertices in VT?If the answer to the Problem STEINER question is “Yes”, then clearly there is a required subnetwork which is a tree.Reduction: Given an instance〈G′=(V′,E′),VT,K〉of Problem STEINER, the corresponding instance of Problem L or T or F is obtained as follows. The networkG=(V,E)is the same asG′=(V′,E′); one of the vertices from VTis selected as the depot; all other vertices from VThave due dates equal to K, and all vertices from V∖VThave due dates+∞(or a sufficiently large number). Then, clearly, the optimum objective value for this instance of Problem L (or Problem T) is not greater than 0 if and only if the answer to the original instance of Problem STEINER is “Yes”, and if and only if Problem F is feasible.□If network G is a tree, Problem L (and therefore Problem F) can be solved in O(nlog n) time, and Problem T is strongly NP-hard. Problem T is strongly NP-hard even if G is a spider network (a network that can be represented as a union of several paths having exactly one common vertex) with all edges of length 1.Interpret the edges as jobs, and their lengths as processing times. The tree defines natural out-tree precedence constraints for the jobs. Minimizing the maximum lateness of n jobs with precedence constraints can be done in polynomial time; e.g., the general O(n2) Lawler’s algorithm (Brucker, 2007, p. 62) is applicable. In fact, the discussion in Brucker (2007, pp. 67–69), implies that in the case of equal release times (or no release times) this can be done inO(nlogn+q)time, where q is the number of precedence relations describing the precedence constraints. In our problem, since we do not have release times, we can assume that all of them are equal to zero. In this case, the algorithm from Brucker (2007) works as follows. First, the due dates are modified as described in Brucker (2007, p. 68); this takesO(n+q)time. Second, the O(nlog n) Jackson’s rule is applied using the modified due dates. Sinceq=n−1for out-tree precedence constraints, the overall complexity is O(nlog n).Strong NP-hardness of Problem T follows from strong NP-hardness of the scheduling problem (1|chains;pj=1|∑Uj) (Brucker, 2007; Lenstra and Kan, 1980); to get a reduction consider spider networks with one central vertex (depot) and several paths (“legs”) emanating from it that do not have common vertices other than the depot. The “legs” model the chain precedence constraints.□If network G is a path, Problem T can be solved in polynomial time; for example, a direct application of the results of Averbakh (2012) (developed for much more general types of problems on paths) leads to an algorithm with O(n2) complexity.For any integer k, m, k ≤ m, let [k:  m] denote the set{k,k+1,…,m}. Let a pair (i, j) (that will be interpreted as an arc) represent the edge {i, j} constructed in the direction from i to j. Let E′′ be the set of arcs (i, j) such that {i, j} ∈ E and j ≠ 1. We will use|E′′|·(n−1)boolean variables(1)xijk={1if(i,j)isthekthconstructedarc;0otherwise,and continuous variables tk,k∈[1:n−1],Tj, j ∈ V∖{1}, z. Variable tkis interpreted as the recovery time of the kth recovered vertex from V∖{1}. Variable Tjis interpreted as the recovery time of the vertex j ∈ V∖{1}. The MILP formulation is as follows.(2)minimizez;subject to(3)z≥Tj−dj,j∈V∖{1};(4)∑k∈[1:n−1]tk=∑j∈V∖{1}Tj;(5)tk=∑k′∈[1:k]∑(i,j)∈E′′cijxijk′,k∈[1:n−1];(6)Tj≥tk−M(∑k′<k∑i:(i,j)∈E′′xijk′),k∈[1:n−1],j∈V∖{1};(7)∑i:(1,i)∈E′′x1i1=1;(8)∑(i,j)∈E′′xijk=1,k∈[2:n−1];(9)∑k∈[1:n−1]∑i:(i,j)∈E′′xijk=1,j∈V∖{1};(10)xijk≤∑k′∈[1:k−1]∑i′:(i′,i)∈E′′xi′ik′,(i,j)∈E′′,i≠1,k∈[1:n−1];(11)xijk∈{0,1},(i,j)∈E′′,k∈[1:n−1].The constant M in (6) can be any sufficiently large constant that is greater or equal to the recovery times of all vertices, e.g.,M=n·(max{i,j}∈Ecij).Theorem 2(2)–(11) is a valid formulation for Problem L.Constraints (7)–(8), (11) ensure the interpretation ofxijkas in (1). Constraint (10) ensures that the order of constructing the arcs is feasible. Constraints (7)–(10) ensure that arcs (i, j) withxijk=1for somek∈[1:n−1]form an out-tree rooted at vertex 1; notice that according to (9) there is exactly one such arc among all arcs entering any vertex j from V∖{1}, and such arcs cannot form directed cycles due to (10) and (9). Constraints (5) ensure the meaning of tk. Constraint (6) ensures that ifxijk′=0for all k′ < k and all i such that (i, j) ∈ E′′, then Tj≥ tk. This implies that if j is the kth recovered vertex from V∖{1}, then Tj≥ tk; in factTj=tk,taking into account (4). The constraint (4) is in fact unnecessary, but it ensures that in the optimal solution the values of variables Tjindeed represent the recovery times of the vertices. Then, (2) and (3) define the objective.□We note that variablesxijkand constraints (7)–(10) are the same as in the second formulation for the flowtime network construction problem from Averbakh and Pereira (2012). This is natural because these variables and constraints define the set of feasible solutions which is the same as in the problem from Averbakh and Pereira (2012), and the modeling approach here is quite standard for scheduling problems (variablesxijkchoose essential edges and assign them to slots in the sequence). Variables tk, Tj, z and constraints (3)–(6) are new, as they model the specific objective function (maximum lateness) of the problem.Formulation (2)–(11) will be referred to as Formulation 1-L, or the basic MILP formulation for Problem L.We will use|E′′|·(n−1)boolean variablesxijkand continuous variables tk,k∈[1:n−1],Tj, j ∈ V∖{1}; the interpretation of these variables is the same as in the previous section. Also, we will usen−1boolean variables yi, i ∈ V∖{1},yi={1ifnodeiistardy;0otherwise.The MILP formulation is as follows.(12)minimize∑j∈V∖{1}yj;subject to(13)Myj≥Tj−dj,j∈V∖{1};(14)yj∈{0,1},j∈V∖{1},and constraints (4)–(11). The constant M can be any sufficiently large constant that is greater or equal to the recovery time and lateness of any vertex, e.g.,M=n·(max(i,j)∈Ecij)−min{0,d2}(some due dates may be negative).Theorem 3(12)–(14), (4)–(11) is a valid formulation for Problem T.Constraints (4)–(11) are the same as in the formulation for Problem L and have the same justification. Constraint (13) along with (12) ensure the meaning of yjand define the objective.□Formulation (12)–(14), (4)–(11) will be referred to as Formulation 1-T, or the basic MILP formulation for Problem T.Now we present time-indexed formulations for Problems L and T. The main motivation to use time-indexed formulations for scheduling problems is that they typically produce stronger linear programming relaxations at the expense of having more variables (Savelsbergh, Uma, and Wein, 2005; Sousa and Wolsey, 1992).Assuming that all edge lengths are integer, let Tmax  be an upper bound on the largest recovery time of a vertex. For example, the length of the maximum spanning tree of G can be used as Tmax . We will use Tmax |E′′| boolean variablesxijt={1ifarc(i,j)∈E′′isinthesolutionanditsconstructioniscompletedexactlyattimet;0otherwise,(i,j)∈E′′,t∈[1:Tmax].To keep the formulation consistent with the format used for time-indexed formulations in Sousa and Wolsey (1992), we use coefficientsaijt′tthat identify the time slots during which the server is constructing an arc (i, j) in order to finish it at time t:aijt′t={1ift′∈[t−cij:t−1];0otherwise.For any node j ∈ V, let spjbe the shortest path distance between the node j and the depot. The time-indexed formulation of Problem L uses also a continuous variable z and is as follows:(15)minimizez;subject to(16)z≥∑i:(i,j)∈E′′∑t∈[1:Tmax]t·xijt−dj,j∈V∖{1};(17)∑i:(1,i)∈E′′x1ic1i=1;(18)∑i:(i,j)∈E′′∑t∈[1:Tmax]xijt=1,j∈V∖{1};(19)xijt≤∑i′:(i′,i)∈E′′∑t′∈[1:t−cij]xi′it′,(i,j)∈E′′,i≠1,t∈[1:Tmax];(20)∑(i,j)∈E′′∑t∈[1:Tmax]aijt′t·xijt≤1,t′∈[0:Tmax−1];(21)xijt=0,(i,j)∈E′′,t∈[1:spj−1];(22)xijt∈{0,1},(i,j)∈E′′,t∈[1:Tmax].Constraints (16) force the objective to be equal to the maximum lateness. Constraint (17) ensures that the first constructed edge is adjacent to the depot. Constraints (18) guarantee that each vertex is recovered. Constraints (19) ensure that vertex i has been recovered before constructing an arc that starts at i. The combination of constraints (18) and (19) ensures that no directed cycles appear. Finally, constraints (20) guarantee that there are no preemptions and that at most one edge is being constructed in each time slot. Constraints (21) reflect the observation that vertex j cannot be recovered before time spj.Formulation (15)–(22) will be referred to as Formulation 2-L.To obtain a time-indexed formulation for Problem T, we define coefficientsCjtwhich take value equal to 1 if t > djand 0 otherwise. Then objective (23) with constraints (17)–(22) represent a valid formulation for Problem T.(23)minimize∑(i,j)∈E′′∑t∈[spj:T]Cjt·xijt;Formulation (23), (17)–(22) will be referred to as Formulation 2-T.As discussed in Section 2, a feasible solution is represented by a choice of essential edges which form a spanning tree and the order of their construction. A sequence(v1,v2,…,vp)of p distinct vertices is called a feasible p-sequence, or just a p-sequence, if for eachk∈[1:p−1],at least one of the edges{vi,vk+1},i ∈ [1: k], is present in G, andv1=1. Any feasible solution naturally defines an n-sequence of recovered vertices. Also, we consider that an n-sequence(v1=1,v2,…,vn)uniquely defines a feasible solution (a spanning tree of essential edges with an order of their construction) by assuming that for eachk∈[1:n−1],nodevk+1is recovered by constructing the shortest of the edges that connectvk+1withv1,…,vk,with some unambiguous rule (e.g., lexicographic) for breaking ties. According to these definitions, the n-sequence defined by a feasible solution defines, in turn, a (possibly different) feasible solution of the same or even better quality. Thus, we need to find either the best feasible solution or the best n-sequence, and the search can be conducted in the space of n-sequences as well as in the space of feasible solutions. In the following, it will be convenient to refer loosely to elements of both spaces as “solutions”; the meaning will always be clear from the context.For Problem L, if a spanning tree of essential edges has been selected, then an optimal order of constructing the essential edges can be obtained in O(nlog n) time, since Problem L on a tree can be solved in O(nlog n) time (see Observation 1). This motivates the first heuristic for Problem L on a general network, which we call Heuristic MST: Find a minimum spanning tree, and solve Problem L on this spanning tree.For Problem T, we define the following greedy heuristic. When choosing the next vertex to recover, ignore the vertices that already cannot be reached before their due dates, and go to that of the remaining unrecovered vertices which takes the least time to reach. The idea of ignoring the jobs that cannot be completed before their due dates is quite common for scheduling problems that seek to minimize the number of tardy jobs, see e.g. Brucker (2007), Section 4.4.3.We also define a local search, where the neighbors of a solution are obtained by changing the position of one vertex in the corresponding sequence of recovered vertices. The neighborhood of the current solution is explored until an improving solution is found, which becomes the new current solution.For Problem L, exploring a neighbor sequence includes the “tree-restricted optimization” step. That is, when we explore the neighborhood of the current solution (sequence) S trying to obtain a better solution, for each considered neighbor S′ we obtain the corresponding spanning tree of essential edges and solve Problem L on it (see Observation 1 in Section 3). The current solution S is considered as a member of its neighborhood, so the tree-restricted optimization step applies to the current solution too and is performed before other neighbors are considered.To obtain an approximate solution for Problem L, we used Heuristic MST with the subsequent local search. For Problem T, we used the solution obtained by the greedy heuristic above with the subsequent local search, and the solution obtained by Heuristic MST for Problem L with the subsequent local search based on the objective of Problem T. The best of the obtained approximate solutions defines the initial upper bound UB0 for the optimal objective function value.Suppose we compute the maximum lateness (the number of tardy vertices) assuming that the recovery time of each vertex is equal to the shortest distance from the vertex to the depot. Clearly this is a lower bound forZL*(forZT*).A similar bound can be computed for any node of the branch-and-bound search tree, in which each node represents some initial k-sequence (a1,…,ak) of recovered vertices (with their recovery times). A lower bound for the node of the search tree would be the maximum lateness (or the number of tardy vertices) where instead of the recovery time of each unrecovered vertex we use the recovery time of akplus the shortest distance from the unrecovered vertex to the closest of the already recovered vertices.Theorem 4For any i ∈ [2: n], let Pi be the length of a minimum Steiner tree on the network G with terminal vertices 1,...,i. Then,Pi−di≤ZL*.For an arbitrary solution, let j be the vertex from [2: i] that is recovered last in this solution; then Cj≥ Pi, and therefore the latenessCj−djof the vertex j is at leastPi−dj≥Pi−di,since vertices are indexed according to non-decreasing due dates.□Let P be the length of a minimum spanning tree in G. Observe thatP=Pn. Therefore, valueLB2=P−dnis a valid lower bound forZL*.The Steiner tree problem is NP-hard, but there are various efficiently computable lower bounds for its optimal objective value. Suppose that lower boundsPi′are known for values Pi,i∈[2:n−1]. Then, the valueLB3=maxi∈[2:n−1](Pi′−di)is a valid lower bound forZL*.The lower bounds LB2 and LB3 are extended straightforwardly for computing lower bounds for nodes of the search tree in the branch-and-bound approach. To do so, for the initial k-sequence(a1,…,ak)that defines a node of the search tree, we compute LB2 and LB3 for the modified network where verticesa1,…,akare blended together in a single vertex a′ and for any other vertex ai,i∈[k+1:n],only the shortest of the edges {ai, aj}, j ∈ [1: k] is retained and becomes the edge {ai, a′}. The due dates of verticesak+1,…,anin the modified network are reduced by the recovery time Ckof node ak; the node a′ is considered as the depot and does not have a due date. Note that the largest of the latenesses of the already recovered verticesa2,…,akis also a lower bound for the considered node of the search tree. Then, the lower bound LB2 (LB3) for the node of the search tree is the larger of the bound LB2 (LB3) obtained for the modified network and the largest of the latenesses of the already recovered verticesa2,…,ak.RemarkFrom the proof of Theorem 4, we see that for any set A⊂V of vertices that includes the depot, ifP^is the length of the minimum Steiner tree in the network G with A the set of terminal vertices anddmaxAis the largest of the due dates of vertices from A, thenP^−dmaxAis also a lower bound forZL*. There are exponentially many ways to choose the set of terminal vertices A, thus there are exponentially many lower bounds that can be obtained in this way. However, this bound cannot be stronger thanPi−diwhere i is the index of the vertex from A with the largest due date, becausedi=dmaxAandP^≤Pi. Therefore, if optimal minimum Steiner tree values are taken instead of approximations, no bound obtained in this way can be stronger than the maximum of LB2 and LB3. Therefore, our bounds LB2 and LB3, that use onlyn−1sets of terminal vertices, capture everything that can be obtained from the exponential number of possible ways to choose the set of terminal vertices.In this section, we obtain some structural properties that will subsequently be used to justify dominance tests for the branch-and-bound algorithm and to develop additional rules that allow us to fathom some nodes of the decision tree without branching.We say that a p-sequence S′ is lexicographically smaller than a p-sequence S′′ if for some i ≤ p the firsti−1vertices in S′ and S′′ are the same, and the ith vertex of S′ has a smaller index than the ith vertex of S′′. Recall that non-depot vertices are indexed according to non-decreasing due dates, d2 ≤ d3 ≤ ⋅⋅⋅ ≤ dn, so for any i, j ∈ V∖{1}, di< djimplies i < j.For a p-sequence S, let T(S) and CC(S) denote the recovery time of the last recovered vertex of S and the current cost of S, respectively, where the current cost of S is the maximum lateness of vertices in S in the case of Problem L, and the number of tardy vertices from S in the case of Problem T.We say that a p-sequence S′′ dominates a p-sequence S′, if S′′ has the same set of recovered vertices as S′, the inequalities T(S′) ≥ T(S′′) and CC(S′) ≥ CC(S′′) hold, and either at least one of these inequalities is strict or S′′ is lexicographically smaller than S′. If a p-sequence S is not dominated by any other p-sequence, we say that S is non-dominated.For an n-sequence S, let S(p) denote the p-sequence of the first p vertices in S, 1 ≤ p ≤ n.An n-sequence S is called p-dominated for some p ∈ [2: n], if there is a p-sequence S′ such that S(p) is dominated by S′. An n-sequence S is called uniformly non-dominated, if it is not p-dominated for any p ∈ [2: n].Observation 3For Problem T, if n-sequence S is not p-dominated for some p ∈ [2: n], then it is not p′-dominated for any p′.Suppose that a p′-sequence S′ dominatesS(p′),p′. Then, replacingS(p′)with S′ in S produces an n-sequence that p-dominates S.□Note that for Problem L a similar statement would not be correct. For Problem L, an n-sequence S may be not p-dominated but still p′-dominated for some p′.For Problem T, if an n-sequence S is non-dominated, then it is uniformly non-dominated.For Problem L, suppose that an n-sequence S is p-dominated for some p ∈ [2: n], and is not p′-dominated for any p′ > p. Suppose that S(p) is dominated by a p-sequence S′. Then, if we replace S(p) with S′ in S, the obtained n-sequence is still not p′-dominated for any p′ > p.Straightforward.□For Problems T and L, there exists an optimal solution (n-sequence) which is uniformly non-dominated.The proof will be constructive; we will show that it is possible to obtain an optimal uniformly non-dominated n-sequence starting with any optimal n-sequence. Suppose that an optimal n-sequence S′ is given.Stage 1. If there is another n-sequence S′′ that dominates S′, clearly S′′ is also optimal. Replace S′ with S′′. Continue similarly. After a finite number of iterations, we will have an optimal n-sequence S that is not dominated by any other n-sequence (since dominance induces a strict partial order). If we are dealing with Problem T, this is a uniformly non-dominated optimal n-sequence according to the corollary from Observation 3, and we are done. If we are dealing with Problem L, continue to the next stage.Stage k,k=2,3,…. Let S be the optimal n-sequence obtained from the previous stage. If S is uniformly non-dominated, we are done. Otherwise, find the largest p such that S is p-dominated. Replace S(p) in S with a p-sequence that dominates S(p). According to Observation 4, after this replacement S is still not p′-dominated for any p′ > p. Continue similarly; after a finite number of iterations, S will not be p-dominated any more (but may still be p′-dominated for some p′). Then, go to the next stage.Clearly, after at mostn−1stages, we will have a uniformly non-dominated n-sequence. Each stage has a finite number of iterations.□For Problem L, any non-dominated n-sequence S has the following property: If a vertex j is positioned in S immediately after vertex i and i > j, then the edge (i, j) belongs to E and cij< ckj for any vertex k that is positioned in S earlier than i.Suppose that a vertex j is positioned immediately after vertex i in S, and i > j. Notice that this implies di≥ dj, since the vertices are indexed according to non-decreasing due dates. If the edge (i, j) does not belong to E or cij≥ ckjfor some vertex k that is positioned in S earlier than i, then swapping vertices i and j in S would result in a sequence S′ such that S is dominated by S′, so S cannot be non-dominated.□The general structure of the branch-and-bound algorithm is similar to that developed in Averbakh and Pereira (2012) for the Flowtime Network Construction Problem, but there are a number of modifications and new elements.The algorithm works by growing a sequence S of recovered vertices, branching on the choice of the next vertex to recover (to append to S). A new vertex to recover is chosen from the set of unrecovered vertices that can be connected to an already recovered vertex by an edge from E. It is assumed that a new vertex is recovered by constructing the shortest of the edges that connect it to the already recovered vertices, with some rule for breaking ties, so S defines a subtree E(S) of already constructed edges.A combination of the best-first and depth-first strategies is used for branching. Whenever a node of the search tree is selected by the best-first strategy, we explore the corresponding subtree of the search tree using the depth-first strategy for at most 10,000 branching steps or until the subtree is completely explored, whichever happens first, and again return to the best-first strategy. Thus, there are at most 10,000 depth-first steps for each best-first step. The motivation for such a mixed strategy is discussed in Averbakh and Pereira (2012).As a lower bound for a node of the search tree, we use the maximum of the lower bounds LB1, LB2, and LB3 described in Sections 5.2 and 5.3 for Problem L since all three bounds are easily computable, and the lower bound LB1 for Problem T. The heuristics described in Section 5.1 are also used at some nodes of the search tree to attempt improving the currently available upper bound. In our implementation, we chose to apply these heuristics for one out of every 1000 explored nodes of the search tree.For the computation of the lower bound LB3, we need to use some efficiently computable lower boundsPi′for the values Pi. We used the lower bound for the minimum Steiner tree value proposed in Shore, Foulds, and Gibbons (1982). Namely, if vertices1,2,…,iare the terminal vertices, we compute two numbers,a=∑k=1iminj∈[1:n]ckj,b=(∑k=1iminj∈[1:i]ckj)−(mink∈[1:i],j∈[1:i]ckj),assuming thatcij=+∞wheni=jor {i, j}∉E, and then min {a, b} is a valid lower bound for the length Piof a minimum Steiner tree (Shore et al., 1982). Note thata<+∞since G is connected. We note that the quality of this lower bound is typically poor when the number of terminal vertices is small with respect to n (i.e., for small values of i), but improves when the number of terminal vertices grows. Of course, there are other known lower bounds for the minimum Steiner tree value in the literature; perhaps a combination of bounds may be more effective for some instances, but we wanted to avoid over-complication of the algorithm.Initially, the best found solution is the solution obtained heuristically as described in Section 5.1.Before a node of the search tree is explored, a dominance test is applied to the node. The purpose of the dominance test is to fathom nodes that definitely cannot lead to better solutions than some other nodes that already exist or will be created in the future. If the node fails the dominance test, or if its lower bound is not less than the cost of the best already discovered solution, it is fathomed and never explored again.To perform the dominance test for a node that corresponds to a partial solution (p-sequence) S′, some heuristics are applied to find another p-sequence S′′ that dominates S′. If we find S′′ that dominates S′, then S′ fails the dominance test, and we fathom the corresponding node; otherwise, S′ passes the dominance test, and the node can be further explored. Fathoming nodes as a result of the dominance test may lead to eliminating some optimal solutions from the search space; however, Theorem 5 guarantees that there is at least one optimal solution that will not be eliminated by the dominance tests. To perform the dominance test for Problem L, we use the following heuristics:(a)Optimize the order of constructing the essential edges defined by S′. These edges form a subtree of G, therefore, this can be done in |S′|log |S′| time according to Observation 1, where |S′| is the number of already recovered vertices for the partial solution S′.Find the minimum spanning tree in the subnetwork of G induced by the already recovered vertices, and solve Problem L on this spanning tree.Change the position of the last vertex in S′ by trying to insert it in one of the previous positions.For all partial solutions S′′ obtained by the previous heuristic, optimize the order of constructing the edges in the corresponding subtree of essential edges.For Problem T, we use only the heuristic described in (c). Heuristics similar to the above were used for dominance tests in Averbakh and Pereira (2012).As an additional heuristic for dominance tests, we also compare S′ with previously explored partial solutions that have the same set of recovered vertices. As it may be memory-intensive to store the information about all previously explored partial solutions, we store it only as long as there is available memory.The need for the dominance tests comes from the following considerations. The problem is inherently very difficult because it combines aspects of job scheduling (time ordering) and vehicle routing/network design (based on distances and network structure). In our branch-and-bound scheme, branching is done on time ordering of recovery of vertices, while spacial connection aspects are incorporated through the lower bounds. Without the dominance tests, for a node of the search tree which is characterized by the corresponding set of recovered vertices and values T(S) and CC(S), there may be huge number of other nodes with the same set of recovered vertices and the same (or worse) values T(S) and CC(S). The dominance tests attempt to remove or reduce this degeneracy, thus trimming substantially the search tree.For Problem L, we used also the following observation that allows us to obtain the optimal objective value for some nodes of the search tree without branching.Observation 5Consider a node of the search tree that corresponds to some partial solution. If the modified network obtained by blending together the already recovered vertices and deleting unnecessary edges as described in Section 5.3 is a tree, then solving the corresponding instance of Problem L (described in Section 5.3) on this tree using Observation 1 allows to obtain the optimal objective value for this node of the search tree without further branching from this node.The following result allows us to fathom some nodes of the search tree for Problem L without branching.Suppose that S is a partial solution of length p (p-sequence) for Problem L, 1$. Let j be the smallest-indexed vertex fromS¯=V∖S,and r be the last vertex in S. If j < r andmina∈S∖{r}caj≤minb∈S¯∪{r}cbj,then S cannot be extended to a non-dominated n-sequence.Suppose that the condition of the theorem holds, and let a feasible sequence S′ be an extension of S. Let i be the vertex that is positioned immediately before j in S′. Then, sincei∈S¯∪{r},we have j < i and mink ∈ S∖{r}ckj≤ cij, i.e. S′ is not non-dominated according to Theorem 6.□Before exploring a node of the search tree while solving Problem L, we check the condition of Theorem 7; if it holds, we fathom the node and never explore it. According to Theorem 7, we do not lose any non-dominated solutions, and thus do not lose the uniformly non-dominated optimal solution guaranteed by Theorem 5. We also check the condition of Observation 5, and if it holds, we obtain the optimal objective value for this node and the corresponding solution as described in Observation 5, and never branch from this node. Our computational experiments show that these two elements significantly improve the performance of the branch-and-bound algorithm for Problem L.All the algorithms in this paper have been coded in ANSI C++ and compiled with GCC version 4.0.1. The Academic ILOG CPLEX library, version 12.4, is used to solve linear programming and mixed integer linear programming problems, with default settings for CPLEX parameters.The tests were carried out using an Intel Core 2 Duo 2.33 gigahertz. processor with 2 gigabytes RAM running the Mac OS X 10.4.11 operating system. Neither the implementation, nor the compiler make use of threads or other forms of parallel code, which makes them effective for a single processor, and the running time of each algorithm was limited to 600 seconds of clock time.Instances are sparse networks generated to resemble road networks, as follows. First, n vertices with integer coordinates are selected randomly in the 1000 × 1000 Euclidean grid; they will be the vertices of the instance. Then the edges of the instance are generated using the following procedure. In the first stage of the procedure, a spanning tree for the selected vertices is obtained by generating its edges one by one randomly inn−1steps. At each step, the potential edges for inclusion in the spanning tree (currently a forest) are the straight-line segments joining pairs of vertices such that they:(a)do not have internal intersections with already included edges;do not create cycles with already included edges, that is, have endpoints in different connected components.When the set of potential edges that satisfy conditions (a) and (b) is determined, selection of one of them is done randomly using a probability distribution based on the lengths of the potential edges; a potential edge e has the probability of being selectedKe∑e′∈EpKe′,where Epis the current set of potential edges, and Keis the reciprocal of the length of e.In the second stage of the procedure, after a spanning tree has been generated in the first stage, additional ⌈0.75n⌉ edges are generated one by one. At each step, the set of potential edges is defined by the condition (a) above (condition (b) is no longer applicable). Selection of one potential edge at each step is done randomly using the same probability distribution as above. If at some point there are no potential edges and the instance is not finished yet, the instance is discarded, and the generation process starts from scratch.For each generated network, the initial location of the server is chosen randomly among the vertices of the instance, and due dates are generated using two parameters, the relative range of due dates (RDD) and the average tardiness factor (TF), similar to the scheme proposed in Crauwels, Potts, and Wassenhove (1998). For each vertex, a due date is generated from the integer-valued uniform distribution on[P(1−TF−RDD/2),P(1−TF+RDD/2)]where P is the length of the minimum spanning tree of the network G.For Problems L and T, for each tested problem size n and each combination of values for the parameters TF and RDD from {0.2, 0.4, 0.6, 0.8}, five instances were generated, leading to 80 instances per instance size. Instances for Problem F will be discussed in Section 8.5.An instance for Problem L withn=10and RDD=0.8 is visually illustrated in Figs. 1and 2. Fig. 1 shows the minimum spanning tree, and Fig. 2 shows the tree of essential edges. Solid lines represent the edges of the minimum spanning tree in Fig. 1 and the essential edges in Fig. 2, while the dotted lines represent the remaining edges. Nodes are numbered in the optimal order of recovery, where node 0 corresponds to the depot.The instances obtained using the procedures described above are called random instances.For the time-indexed MILP formulations, the number of variables is not polynomial as it is proportional to Tmax . Time-indexed formulations can be used only for instances where edge lengths are relatively small integers. To evaluate the time-indexed MILP formulations we also created scaled instances, by scaling the lengths of edges of the random instances so that they are positive integers not greater than B, for values of B from {10, ~25, ~50}, and by scaling the due dates. This was done as follows. If the maximum of the edge lengths of a random instance is Cmax , then for any edge of the instance, its length is multiplied by (B/Cmax ) and the result is rounded up. The due dates are multiplied by (MST1/MST0) and then rounded up, where MST0 and MST1 are the lengths of the minimum spanning trees in the original random instance and the obtained scaled instance, respectively. The scaled instances are used only for comparison between the time-indexed and basic MILP formulations, and the corresponding results are reported only in the tables in Appendix 2 in the electronic Supplementary material. In the tables in the main body of the paper, scaled instances are not used.For instances of Problem T, we define the following quantities:(24)gapT=UB−LBn·100(25)gapT0=UB0−LB0n·100(26)relGapT=UB−LBUB0−LB0+ϵ·100(27)gapTLB0=ZT*−LB0n·100(28)gapTUB0=UB0−ZT*n·100Here, UB and LB are respectively the best upper and lower bounds obtained by the exact method under consideration (CPLEX or branch-and-bound), andϵis a small constant, set to10−6,to avoid division by zero; UB0 is the initial upper bound obtained by the heuristic from Section 5.1 for Problem T; LB0 is the initial lower bound (LB1 computed before branching).Table 1presents the results of applying CPLEX to the basic MILP formulation (Formulation 1-T) for Problem T. For each problem size (n) and a combination of values for TF and RDD, we report the number of instances (out of 5) where CPLEX reached and verified optimality (“#”), the average running time in seconds (“t(s)”), and the average gap (“gapT”) computed using (24). We did not use UB in the denominator of (24) because the gaps defined in this way would not be informative: for most instances that are not solved to optimality,LB=0and thus such gap would be 100, even if UB is small. The running time for each instance is equal to the clock time spent by the algorithm, or 600 seconds if the time limit was reached before verifying optimality.The results suggest that CPLEX applied to Formulation 1-T can successfully handle instances with sizes up to 12 vertices given the imposed time limit, has mixed performance for sizes aroundn=15,and is not effective for sizes 20 and larger. In fact, CPLEX usually runs out of memory for sizes greater than 20. Also, we observe the dependence of the performance on parameters TF and RDD. Instances with less variability and tightness of due dates are typically harder to solve with CPLEX, presumably because for such instances it is harder to obtain strong lower bounds. An exception is the case of very tight instances (TF=0.8) where lower variability (RDD) appears to decrease the difficulty, apparently because more vertices become clearly unrecoverable by the due dates. For problems where all due dates are relatively large, CPLEX is often unable to find a non-zero lower bound.To compare performances of the basic and the time-indexed formulations for Problem T, we applied CPLEX to both formulations for the scaled instances forB=10,25,50. The results forB=10andB=25are reported in Tables 11S and 12S in the electronic Supplementary material. We see that, as expected, the performance of the time-indexed formulation strongly depends on B. For small edge lengths (B=10), the time-indexed formulation outperforms the basic formulation, but still becomes unreliable for n ≥ 20. For medium edge lengths (B=25), the time-indexed formulation is comparable to the basic formulation (or, perhaps, is still slightly better), becoming unreliable for n ≥ 15. For larger edge lengths (B=50), the basic formulation is clearly better than the time-indexed formulation; we do not provide the results forB=50because for many instances CPLEX runs out of memory for the time-indexed formulation.Table 2presents the results of the proposed branch-and-bound algorithm. The results are limited to instances with n ≥ 25 because smaller instances were solved within one second. For each problem size and combination of values for TF and RDD, we report the number of instances (out of 5) where the method found an optimal solution and verified optimality (“#”), the average running time in seconds (“t(s)”), the average number of explored nodes in the search tree (“nodes”) in millions, and the average gap (“gapT”), where the gap for each instance is computed using (24).The results show that the branch-and-bound method clearly outperforms CPLEX applied to any of our MILP formulation. With the used time limit, it is reliable up ton=30for loose instances (TF=0.2) and up ton=50for tight instances (TF=0.8). Generally, the algorithm becomes more effective with an increase in variability and tightness of due dates. Again, the case of very tight due dates (TF=0.8) is an exception: in this case, more variability results in higher difficulty, likely for the same reason as discussed above for CPLEX.Table 3presents some additional statistics. The column “#” reflects the number of instances solved to proven optimality for each size (out of 80). The table reports values relGapT,gapT0,gapTdefined in (24)–(26) averaged over all 80 instances, and valuesgapTLB0andgapTUB0defined in (27) and (28) averaged over only those instances that have been solved to proven optimality. The valuegapT0reflects the gap before branching between the initial upper and lower bounds; the value relGapTreflects how strongly the branch-and-bound phase reduces this initial gap. The valuesgapTLB0andgapTUB0reflect the quality of the initial lower and upper bounds, respectively.Table 4provides valuesgapT0for large instances (as before, averaged over 80 instances for each size). We see that generally the initial gap does not change much with the size of the instances for n > 40. This seems to contradict the observation that in Table 3 valuesgapTLB0that reflect the quality of initial lower bounds increase rather quickly with n. To explain this, observe that valuesgapTLB0are averaged over instances solved to proven optimality, while valuesgapT0are averaged over all 80 instances (this also explains whygapT0≠gapTLB0+gapTUB0for n ≥ 35). More detailed analysis of the results shows that valuesgapTLB0increase with n for tight instances(TF=0.8)but decrease with n for loose instances(TF=0.2); since only few of the larger instances (n ≥ 45) solved to proven optimality are loose, this results in a misleadingly exaggerated increase of the reported valuesgapTLB0.We also see that the quality of initial upper bounds is significantly better than the quality of initial lower bounds. The fast heuristic for Problem T from Section 5.1 provides a reasonable quality of approximation. Values relGapTand comparison of valuesgapT0and gapTindicate that the branch-and-bound algorithm is quite successful in reducing the initial gap between the lower and upper bounds.For instances of Problem L, we define the following quantities:(29)gapL=UB−LBUB+dmin·100(30)gapL0=UB0−LB0UB0+dmin·100(31)relGapL=UB−LBUB0−LB0+ϵ·100(32)gapLLB0=ZL*−LB0ZL*+dmin·100(33)gapLUB0=UB0−ZL*UB0+dmin·100(34)gapLUB−MST=UBMST−ZL*UBMST+dmin·100Here, UB and LB are respectively the best upper and lower bounds obtained by the exact method under consideration (CPLEX or branch-and-bound); dmin is the smallest due date (according to our choice of indexing the verticesdmin=d2);ϵ=10−6; UB0 is the initial upper bound obtained by the heuristic from Section 5.1 for Problem L (a combination of Heuristic MST with local search); LB0 is the initial lower bound (the best of LB1, LB2 and LB3 computed before branching); UBMST is the upper bound provided by Heuristic MST (without local search).To explain the presence of dmin in the denominators of the formulas (29) and (30) and (32)–(34) for gapL,gapL0,gapLLB0,gapLUB0,gapLUB−MST,notice that both LB and UB can be negative for Problem L. Thus, for example, gapLcomputed without dmin in the denominator would be meaningless since it could be negative or greater than 100. Observe thatZL*≥−dmin,and thereforeUB≥−dmin. Since−dminis a trivial lower bound forZL*,the lower bound LB obtained by the method satisfiesLB≥−dmin. Therefore, 0 ≤ gapL≤ 100. A similar argument applies togapL0,gapLLB0,gapLUB0,andgapLUB−MST.Table 5presents the results of applying CPLEX to the basic MILP formulation (Formulation 1-L) for Problem L. We group instances with different values of TF together, because Problem L is invariant under an overall shift in due dates and thus TF values are irrelevant. For each problem size and a value for RDD, we report the number of instances (out of 20) where CPLEX reached and verified optimality (“#”), the average running time in seconds (“t(s)”), and the average gap (“gapL”). The running time for each instance is equal to the clock time spent by the algorithm, or 600 seconds if the time limit was reached before verifying optimality.The results suggest that CPLEX applied to Formulation 1-L can successfully handle instances with sizes up to 12 vertices given the imposed time limit, has mixed performance for sizes aroundn=15,and is not effective for sizes 20 and larger. We observe the dependence of the performance on due dates variability (RDD): instances with less variability of due dates are harder to solve.To compare performances of the basic and the time-indexed formulations for Problem L, we applied CPLEX to both formulations for the scaled instances forB=10,25,and reported the results in Tables 13S and 14S in the electronic Supplementary material. As expected, the performance of the time-indexed formulation quickly deteriorates with increase in edge lengths. For small edge lengths(B=10),the performances of both formulations are comparable, but forB=25the basic formulation outperforms the time-indexed formulation.Table 6presents the results of the proposed branch-and-bound algorithm. We report quantities similar to those in Table 5, and the average number of explored nodes of the search tree (“nodes”) in millions. The results suggest that the branch-and-bound algorithm is successful for instances with up to40−45vertices with the imposed time limit, has a mixed performance for sizes around 50, and performs poorly for n ≥ 60. It clearly outperforms CPLEX applied to any of our MILP formulation. For instances not solved to optimality, the quality of approximation usually deteriorates with the increase in variability of due dates (RDD).Table 7presents additional statistics. The column “#” reflects the number of instances solved to proven optimality for each size (out of 80). The table reports values relGapL,gapL0,gapLaveraged over all 80 instances, and valuesgapLLB0,gapLUB0,andgapLUB−MSTaveraged over only those instances that have been solved to proven optimality. The interpretation of these quantities is the same as for similar quantities for Problem T.Table 8provides valuesgapL0for large instances. We see that the initial gap on average slightly decreases as n grows. More detailed analysis of the results reveals that the initial gap strongly depends on the variability in due dates: instances with more variability (larger RDD) show higher values ofgapL0.Small values ofgapLUB0in Table 7 indicate that the heuristic for Problem L suggested in Section 5.1 is very efficient. However, valuesgapLUB0andgapLLB0for n > 40 should be interpreted with caution: they are averaged over only the easier instances that were solved to proven optimality. This explains the decrease in reported valuesgapLLB0with n for n > 45, even though the valuesgapL0do not change significantly. The larger the size, the smaller the proportion of successfully solved instances, and the more biased are the results based on averaging over such instances. Also, by comparing valuesgapLUB0andgapLUB−MST,we see that local search significantly improves the quality of solutions obtained by Heuristic MST.RemarkIn our experiments, the bound LB2 was usually (but not always) stronger than both LB1 and LB3. We however believe that this is specific to our instance-generating procedure and our choice of approximation to the minimum Steiner tree value, and with a different instance generator this may not be the case.As an MILP formulation for Problem F, we used the basic MILP formulation for Problem L with one additional constraint: z ≤ 0. CPLEX stops as soon as UB ≤ 0 (then, a feasible solution is found) or LB > 0 (then, no feasible solution exists).As a branch-and-bound algorithm for Problem F, we used the branch-and-bound algorithm for Problem L, with the same bounds and heuristics, but with the following additional rules: Any node of the search tree with the corresponding lower bound greater than 0 is fathomed, and the algorithm stops if UB ≤ 0, reporting feasibility.The instance-generating procedure used to generate random instances was aimed at producing diverse instances for Problems L and T. However, the combinations of values for the parameters TF and RDD that were used for Problems L and T would be inappropriate for Problem F, because any combination of values from {0.2, 0.4, 0.6, 0.8} for TF and RDD, except forTF=0.2withRDD=0.6orRDD=0.8,would produce only infeasible instances, since the largest due date would be smaller than P and the infeasibility would be detected by the branch-and-bound algorithm even before branching after computing LB2. Therefore, we used only parameter value combinationsTF=0.2with RDD ∈ {0.6, 0.8} andTF=0with RDD ∈ {0.2, 0.4, 0.6, 0.8} (6 combinations), and for each size and each combination generated 10 instances (60 instances for each size).Table 9presents the results with CPLEX applied to the MILP formulation. For each problem size and a combination of values for TF and RDD, we report the number of successfully solved instances (out of 10) and the average running time in seconds (“t(s)”). The results suggest that CPLEX is effective forn≤12−15,has mixed performance for 15 < n ≤ 30, and is not effective for n > 30.Table 10presents results for the branch-and-bound algorithm. For each size n, we report the number of instances (out of 60) solved successfully by the branch-and-bound algorithm (row # bb), and the number of instances where feasibility or infeasibility was detected before branching based on the initial lower and upper bounds (row # no bb). The results suggest that the branch-and-bound is effective for instances up ton=40and is clearly more effective than CPLEX. Moreover, we observe that for a significant proportion of instances (about 75 percent on average) feasibility or infeasibility was detected before branching, i.e., in negligible time, and the proportion of such instances does not change much with the increase in problem size. This indicates that Problem F is significantly easier than the exact solution of Problem T or Problem L.RemarkIn the electronic Supplementary material for the paper (Appendix 1), we present the results of computational experiments with instances generated from the street network of Winnipeg, Canada. The results exhibit the same patterns as the results for random instances discussed above.The 2010 Chile earthquake occurred on Saturday February 27, 2010. It had a magnitude of 8.8 on the moment magnitude scale. The affected area makes up for around 80 percent of the total population of Chile. The earthquake triggered a tsunami that also affected the coastal towns in south-central Chile.After the earthquake a large part of transportation infrastructure required repairs or complete reconstruction. The total expenditure of the ministry of public works in emergency and reconstruction operations was estimated in over 150 billions of Chilean pesos.The Chilean government and the CIGIDEN (the Chilean center for natural disaster management) provided us with detailed information on the damages to the Chilean transportation network and the different reconstruction works conducted by the government, including their budgets (ISP, 2010; IRM, 2011). Based on this information, we obtained a reconstruction network for our problem, as follows.First, we identified all continental Chilean cities. According to the Chilean government, any municipality with over 5000 inhabitants is considered to be a city. 228 cities were identified, along with their population sizes.Second, we identified the network of major public roads between the cities. We call this network a base network.Third, we identified the roads that required reconstruction work. These roads will be called affected.Fourth, we identified the connected components of the network obtained from the base network by deleting the affected roads. These connected components are the vertices of the reconstruction network; the population of such a vertex is the total population of cities in the corresponding component. The affected edges of the base network that connect different components are the edges of the reconstruction network; as the “length” of such an edge, we take the reconstruction expenditure for the corresponding road. In our opinion, the reconstruction expenditure for a road is a better proxy for the time required to repair the road than the actual length of the road. If several affected edges connect two components, we take one with the smallest reconstruction expenditure. As the depot, we chose the vertex that corresponds to the component that contains the city of Santiago (the capital of Chile). This is also the vertex with the largest population. The reconstruction network has 53 vertices and 76 edges.Based on this reconstruction network, we generated 80 instances of Problem L which we call Chilean instances. All Chilean instances have the reconstruction network as network G, and differ from each other only by due dates. The due dates are generated randomly using the same generation scheme as for the random instances, five instances for each combination of values TF and RDD, with the following difference: The 52 due dates generated for an instance (there are 52 non-depot vertices in the reconstruction network) are assigned to the vertices of the reconstruction network based on the population sizes associated with the vertices, the smallest due date being assigned to the non-depot vertex with the largest population, the second smallest due date being assigned to the non-depot vertex with the second largest population, and so on. We did not generate instances of Problem T because, in our opinion, Problem T is not sufficiently natural in the context of this example.The main results of computational experiments with the Chilean instances can be summarized as follows. Our branch-and-bound algorithm solved all Chilean instances to proven optimality rather quickly (the largest running time was 6 seconds). On the contrary, using CPLEX with our MILP formulations turned out to be ineffective for Chilean instances. None of the Chilean instances was solved to proven optimality within the allocated time (600 seconds) using the basic MILP formulation, with CPLEX running out of memory for most instances. When we applied CPLEX to the time-indexed formulation after scaling the Chilean instances as described in Section 8.2 withB=10,only 2 out of 80 instances were solved within 600 seconds, with CPLEX running out of memory for most instances.We note that the reconstruction network has some special structure, it is sparse and has some “junction” nodes (nodes whose deletion would decompose the network into relatively large disconnected parts). It appears that this structure is the reason why our branch-and-bound algorithm is effective for the Chilean instances in spite of their relatively large size (53 vertices): many elements of our algorithm (e.g., some dominance tests, the lower bound LB2, Heuristic MST) are based on “approximating” the network with a spanning tree, and the structure of the reconstruction network appears to make it more amenable to such an approximation than instances generated randomly as described in Section 8.2. Table 11provides the results of applying our branch-and-bound algorithm to Chilean instances; statistics similar to those in Tables 6 and 7 are reported, with the exception of the values gapLand relGapLwhich are not provided as they are equal to 0.We note that the Chilean instances illustrate that there are real-life disasters that result in instances of our problems that cannot be handled by CPLEX with the available MILP formulations, but can be handled by our branch-and-bound algorithm. To additionally illustrate this point, we note that for Winnipeg instances discussed in the electronic Supplementary material for the paper, the average disaster radius needed to generate instances of size 30 vertices (the size where CPLEX with our MILP formulations is ineffective but our branch-and-bound algorithm is still effective) is about 3 miles which is entirely realistic for extreme events such as earthquakes, landslides, floods, etc.

@&#CONCLUSIONS@&#
We have introduced and studied two new network construction problems, with the objectives of minimizing the maximum lateness of vertices (Problem L) and the number of tardy vertices (Problem T). Also, we discussed the feasibility problem (Problem F) where it is required to find a schedule that meets all due dates or to show that no such schedules exist. We presented complexity results, MILP formulations, fast heuristics, problem-specific lower bounds, and branch-and-bound algorithms. The main conclusions are:•The problems are strongly NP-hard on general networks. On trees, Problems L and F are polynomially solvable, but Problem T is strongly NP-hard.The MILP formulations can be used for solving small instances (n≤12−15).The branch-and-bound algorithms for Problems T and L are much more efficient than CPLEX applied to our MILP formulations and can be used for solving medium instances with30−50vertices for Problem T and40−55vertices for Problem L depending on the degree of variability in due dates and (for Problem T) on the tightness of due dates. Special structure of the network can make the branch-and-bound algorithm even more effective (e.g., Chilean instances). There are real-life extreme events that result in restoration networks of the size where applying CPLEX with available MILP formulations is problematic but where the branch-and-bound approach is still effective.The fast heuristics obtain approximate solutions with a reasonable quality of approximation for Problem T and with a very good quality of approximation for Problem L. They can be used for large instances.Problem F is considerably easier than the exact solution of Problems T or L. Fast heuristics and the initial lower bounds for Problem L detect feasibility or infeasibility for a large proportion of instances for Problem F without branching very quickly.We also point out that, as discussed in Averbakh and Pereira (2012), many network construction problems arising in practice are relatively small or medium even if they arise in the context of large transportation networks, as they correspond to planning construction of an extension of an already existing network or repairing a locally damaged part of an existing network. This observation emphasizes the importance of the exact methods, such as those developed in this paper, that can solve small/medium instances to optimality.A natural direction for further research would be to develop better solution approaches for the considered problems. Our goal in this work was to develop some reasonable exact computational scheme for these new problems and to test it on some natural set of instances. There are some obvious possibilities for further refining our approaches, such as using multiple lower bounds for estimating the minimum Steiner tree value in computation of the lower bound LB3 for Problem L, using more sophisticated heuristics for obtainingupper bounds, fine-tuning CPLEX parameters instead of using the default values, etc. However, we feel that such refining would be more appropriate at the application stage, where the methods are applied in a specific application setting. As for further research, we believe that of interest are the following two directions: (a) Developing substantially stronger lower bounds based on deeper understanding of the combinatorial structure of the problem that involves interplay of scheduling and spacial connections; (b) Developing conceptually different exact approaches, e.g., branch-and-bound with branching on the choice of essential edges instead of branching on the recovery order (an idea suggested by one of the anonymous referees). We hope that this paper will trigger new research on these problems.