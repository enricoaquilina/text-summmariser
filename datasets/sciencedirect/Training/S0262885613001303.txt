@&#MAIN-TITLE@&#
Selection of a best metric and evaluation of bottom-up visual saliency models

@&#HIGHLIGHTS@&#
Introduced a set of experiments to judge the biological plausibility of visual saliency models.Introduced a novel method to evaluate saliency map comparison metrics using a database of human fixation maps.Employed the introduced method to identify the best saliency map comparison metric.Examined nine well-known models of visual saliency using the best metric to identify the best visual saliency models.

@&#KEYPHRASES@&#
Bottom-up saliency mechanism,Best comparison metric,Computer vision,Human visual system,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
It has been shown that analyzing or storing all information entering the human eye at every moment is beyond the capabilities of the human visual system (HVS) [3]. Controlling fixations and saccades of the eye, the visual saliency mechanism enables the HVS to focus its limited perceptual and cognitive resources on the most “prominent” locations of the scene [4]. These locations are often referred to as “salient points” or “interesting points” in the literature [5]. The HVS gathers information mostly at the fixation points and little information is collected in saccades [6]. In addition to increasing the functionality of the HVS, the saliency mechanism helps the visual perceptual system organize visual information faster [1,5,7].The saliency activity of the HVS is an interaction between two mechanisms, bottom-up and top-down saliency [1,3,5]. Bottom-up saliency is a fast and purely stimulus driven mechanism (independent of any high-level visual task), which biases the observer towards selecting locations in the scene based on the saliency of the locations only [5]. In this case, the saliency of a stimulus can be defined as the state or quality of standing out relative to other stimuli in the scene. Top-down saliency is a slower, memory driven process, directing visual attention based on activities in which the human neural system is engaged [3]. Herein, the goal is to find visual saliency models that can mimic human observers' behavior; however, models of the saliency mechanism of the HVS can be employed in different applications of computer vision. Most of the computer vision algorithms depend upon scanning a scene from left to right or top to bottom to locate objects of interest. Saliency mechanisms offer a reasonably fast method to find regions in the scene that contain key information and may include the object of interest. Applications of the saliency mechanisms comprise, but are not limited to, automatic target detection, robotics, image and video compression and advertising.There are several “Machine Vision” models for visual saliency, and it is important to know which models perform the best in mimicking the saliency mechanism of the HVS. In an extensive survey of visual saliency models, Borji and Itti [8] point out the importance of creating a standard set of experiments to evaluate visual saliency models and judge their biological plausibility. Moreover, there are many evaluation metrics in the literature for comparing different saliency mechanisms, or saliency mappings. Results of different evaluation metrics often disagree [8,9]. Contributions of this paper are as follows:1.A method is given to evaluate saliency map comparison metrics using a database of human fixation maps.Different comparison metrics are investigated and ranked using 1.Ten well-known models of visual saliency are examined using the best metric from 2 to identify the best saliency models.It is worth mentioning that finding the best comparison metric is also necessary for training various machine vision algorithms. Comparison metrics are usually used as the objective function in algorithms in which a visual saliency model is trained based on a human fixation/saliency map database. It is critically important to choose a metric that can discriminate between a good and a poor set of saliency data.Visual saliency models have often been validated against human eye movement data. In a study at the MIT Computer Science Artificial Intelligence Laboratory, “Learning to Predict Where Humans Look” (LPWHL) [10], eye tracking data of 15 human observers on 1003 images of different scenes was collected. Observers free viewed the images. Gaze tracking paths and the first 5 fixation locations were recorded for each viewer. For every image in the database, a continuous saliency map was found by convolving a 2-D Gaussian filter over the fixation locations of all observers. As an example, an image, human observers' fixations, and its saliency map are shown in Fig. 1(a), (b) and (c). A binary map showing the 20% most salient pixels in the image is demonstrated in Fig. 1(d). The database created in the LPWHL study will be used in this paper to find the best comparison metric and also to examine visual saliency models.In what follows, Section 2 summarizes the main steps of bottom-up visual saliency models as well as ten visual saliency models that will be evaluated in this paper. In Section 3, three new saliency map comparison metrics are introduced and six published saliency map comparison metrics are explained. An evaluation procedure to evaluate comparison metrics is introduced in Section 4, and all metrics are ranked accordingly. Employing the best comparison metric, all visual saliency models are examined on their performances on a database of human observers' fixation data, and the best models are identified in Section 5. Finally, Sections 6 and 7 provide discussion and conclusions.There has been increasing effort to present computational principles of the HVS saliency mechanism in the last decades. One of the first models was proposed by Koch and Ullman in 1985 [11], which is based on the “Feature Integration Theory” of Treisman and Gelade [12]. According to this theory, features are extracted early, automatically, and in parallel across the visual field, while objects are identified separately and only at a later stage, which requires focused attention. Treisman and Gelade assumed that at the first step of visual processing in the HVS, several feature spaces such as color, orientation, spatial frequency, brightness, and direction of movement are initially extracted from the scene. Then, feature spaces are processed in parallel to generate feature saliency maps that are later integrated in a saliency map. They claimed that without focused attention, features cannot be related to each other.According to Harel et al. [13], models of the bottom-up visual saliency can be organized into the following three stages, illustrated by Itti et al. [14] in Fig. 2:1.Extraction: Given an image of the scene, several feature spaces such as image intensity, orientation and color are extracted by linearly filtering the input image.Activation: Computing feature saliency maps (activation maps) from feature spaces.Normalization/Combination: Normalizing feature saliency maps and combining them together to form the saliency map.Ten well-known bottom-up saliency mechanisms are briefly summarized herein.Itti et al. [14] developed the Koch and Ullman model and proposed one of the first complete implementations of human visual saliency models. They considered one intensity, two colors, and four orientation feature spaces, which are among the most important feature spaces based on work by Wolfe et al. [15]. Then, feature spaces are analyzed with six sets of radii for center and surround circles, which results in forty two feature saliency maps. Itti et al. [14] proposed to normalize feature saliency maps before combining them together and presented a new normalization method, which is the main difference from the Koch and Ullman model. Normalizing and summing feature saliency maps corresponding to each elementary feature space results in three “conspicuity maps”. Finally, the saliency map is calculated as an average of the three conspicuity maps. The Itti et al. model has been the basis of many newer models of the saliency mechanism of the HVS. Itti and Khoch [3] further developed [14] and introduced a more efficient feature saliency map normalization method. The Itti and Khoch [3] model of visual saliency mechanism is examined in this paper.Meur et al. [16] proposed a model based on the architecture of the Koch and Ullman model [11] to overcome the following drawbacks of the classical models of visual saliency such as the Itti et al. [14] model:1.Applying several normalization steps during the processIneffective normalization methodsIgnoring or overlooking some aspects of HVS saliency mechanismHou and Zhang [17] introduced the spectral residual approach. Their method is based on the general property of natural images, described by the 1/f law. This law states that the amplitude of the averaged Fourier spectrum,A(f), of the ensemble of natural images is proportional to 1/f, in which f is the frequency. They use this law to find the statistical similarities between input images and calculate the residual spectrum, which they called the bottom-up saliency map (M) of the input image (I). This method can be summarized in the following steps:(1)Af=AmplitudeFI(2)Pf=PhaseFI(3)Lf=logAf(4)Rf=Lf−Hn∗Lf(5)Mxy=G∗F−1eRf+Pf2whereF.is the 2D Fourier transform,Gis a 2D Gaussian filter to smooth the saliency map,Hnis a n×n matrix defined by:Hn=1n211...11⋮⋱11and * indicates the convolution of two terms.Harel et al. [13] introduced a new approach for modeling bottom-up visual saliency. To calculate the feature saliency map (M) corresponding to a feature space (F), the authors first generated a fully connected graphG, obtained by connecting every two pixels inF. Then, a weight w was assigned to the edge from pixel (k,l) to (p,q), defined by(6)wklpq=logFklFpqexp−k−p2+l−q22σ2where the first term on the right is called the dissimilarity betweenF(k,l) andF(p,q); and the second term is a Gaussian function to increase the weight of two close pixels and decrease the weight of pixels which are far from each other. Subsequently, a Markov chain is defined onGby normalizing the weights of the outbound edges of each node/pixel to 1. The equilibrium distribution of the Markov chain reflects the fraction of time a random walker would spend at each node if he were to walk forever [13]. It would naturally accumulate mass at nodes that have high dissimilarity with their surrounding nodes. This equilibrium distribution is considered the feature saliency map in the Harel, et al. approach. A linear combination of all feature saliency maps results in the final saliency map.Zhang et al. [18] proposed a visual saliency model using a Bayesian framework from which bottom-up saliency is defined as the self-information of visual features, and prior information emerges as the pointwise mutual information between the features and the target when searching for a target. Differing from most of the visual saliency models, Zhang et al. defined saliency based on the natural image statistics instead of considering the image of interest only. Let the binary random variable C represent whether or not a pixel location L belongs to the target. To calculate the feature saliency map value at pixel location (k,l) (M(k,l)), using the corresponding feature spaceF(k,l), they offered the following equation:(7)logMlk=−logpF=Fkl+logpF=FklC=1+logpC=1L=klThe first part on the right of Eq. (7) (−log[p(F=F(k,l))]) is known as self-information of the random variableFwhen it takes the valueF(k,l). It increases when the probability of a feature decreases (the rarer a feature, the more informative it is). The second part is a log-likelihood term which supports feature values that are consistent with the prior knowledge about the target. The third part on the right of Eq. (7) is independent of visual features and reflects the prior knowledge about where the target is more likely to appear.Achanta et al. [19] introduced a simple definition for computing the feature saliency mapMusing the feature spaceFas follows(8)M=μF−Fwhcwhere μFis the mean value ofFandFwhcis a smoothed version ofFwith a 2-D Gaussian kernel. They used the Euclidian length of the vector {M1(l,k), M2(l,k),..., Md(l,k)} to combine d feature saliency maps together and compute the saliency map value at pixel location (k,l).Bruce and Tsotsos [20,21] proposed a saliency mechanism model based on the principle of maximizing information sampled from a scene. Information in their model is computed using Shannon self-information of each local image patch.Goferman et al. [22] proposed to employ the following four basic principles of the HVS saliency mechanism in the visual saliency model:1.Local low-level considerations, to promote regions which differ from their immediate surroundings.Global considerations, to suppress frequently occurring features.Visual organization rules, which state that visual forms may possess one or several centers about which the form is organized.High-level factors, for example including human face match detection in the visual saliency model. They used the Euclidian distance between feature values and positions to define dissimilarities between two pixels.Xiaodi et al. [23] introduced a new image descriptor named image signature to create saliency maps. They used Discrete Cosine Transform (DCT) to define the image signature (IS) of gray scale imageIas(9)ISI=signDCTIand defined the saliency (M)(10)M=G∗X¯∘X¯whereX¯is the inverse discrete cosine transform ofISand the operator ∘ is the Hadamard (entrywise) product operator andGis a 2-D Gaussian kernel to smooth the saliency map.Emami and Hoberock [9,24] introduced a model based on the Itti and Khoch visual saliency model [3] with a new normalization method. They used the weighted volume trapped between the feature saliency map and a surface parallel to the ij plane passing through the point [l, k, M[l,k]] as the normalized value MNormalized[l,k] of the feature saliency map at [l,k], given by:(11)MNormalizedlk=∑i=1I∑j=1Jflkij×Mlk−Mij,whereflkis a Gaussian weight function centered at pixel [l,k], and bothMandfare matrices of size I×J.Some researchers define the goal of attention modeling to be finding a model which minimizes the error in locating human observer's fixations [8]. However, the most common approach is finding a model which predicts a human eye saliency map [2,10,13,16,25]. The method to compute human saliency maps is explained in the next section. Accordingly, all comparison metrics are designed to find similarities between a saliency map calculated by a visual saliency model and a same size reference map. In this paper, the saliency map computed with a visual saliency model is referred to as the Predicted Saliency Map (PSM), denoted by saliency valuesM(k,l). The reference maps are extracted from the LPWHL database [10], which are called Reference Human Saliency Maps (RHSMs), and are denoted by saliency valuesN(k,l). 1≤k≤K and 1≤l≤L, where K and L are the height and width of the maps in pixels, respectively. In this section, three new metrics along with six already published metrics to compare saliency maps are explained.Rearranging a PSM and an RHSM in ‖.‖ to KL length vectors m1 and m2, we define cosine of the angle between two maps as:(12)Cosθ=m1m2m1.m2where 〈.,.〉 is the inner product of the vectors and is the vector 2-norm. Cosθ=1 indicates two maps are identical. We introduce Cosθ as a measure of similarities between two maps.We define Score2 as the average of an RHSM values at the first (highest) N peaks of a PSM, given by Eq. (13):(13)Score2=∑i=1NMkli/Nwhere (k, l)iis the pixel location of the ith peak of the PSM (ith fixation of the visual saliency model). A large Score2 value shows that the N highest salient points found by the algorithm are the prominent locations of the image found by human observers. Therefore, the larger is Score2, the better is the performance of the visual saliency mechanism. Herein, Score2 is introduced to find resemblances between a PSM and the RHSM, when N=5.The difference map is defined as the difference between the PSM computed for an image and the image RHSM. We suggest treating the difference map as a vector with length K×L and using its vector 1-norm as a comparison metric to find dissimilarities between two maps, as follows:(14)DM=∑k=1K∑l=1LΜkl−Nkl/K×LNDM defined as:(15)NDM=1−DM,is introduced in this paper as a measure of similarities between two maps.In [26] the authors used the number of correct objects detected in the first 5 fixations of a saliency model as a measure of its performance, where, local maxima of the PSM are considered as fixation points. Hit rate is defined as follows:(16)HitRate=NumberofcorrectHitsNIn [10] images are not divided into objects of interest and background. However, the 20% most salient pixels of the image based on its RHSM is selected as the foreground, see Fig. 1(d), and the rest is called the background of the image. Accordingly, if a fixation point happens to be in the foreground, it is considered as correct object detection (0≤Hit Rate≤1). Herein N=min(5,Number of peaks in the PSM).In addition to “Hit Rate”, the ability to find the most salient object in the image in the first four fixations is employed in [26] to evaluate visual saliency models. In this paper, the global maximum of the RHSM is considered as the most salient location of the image, and the minimum distance (d) of the first four fixations of a visual saliency model is defined as a metric to evaluate its performance. Since in this paper comparison metrics are defined to measure similarities between two maps, DS is defined as follows:(17)DS=1−dL2+K2as a comparison metric (0≤DS≤1and the larger is DS the closer are two maps).The correlation coefficient of two saliency maps defined in Eq. (18) is used in [16,27,28] to find the linear relationship between two maps.(18)CC=∑k,lM1kl−μ1×M2kl−μ2∑k,lM1kl−μ12×∑k,lM2kl−μ22where μiis the mean value of the map Mi. Normalizing CC in the [0 1] interval produces the Normalized Correlation Coefficient, NCC, defined by:(19)NCC=1+CC/2NCC=1 implies that two maps are either exactly equal or are different by a constant value.One of the commonly used saliency map comparison metrics in the literature is the ROC area [2,13,17,20,23,25,29,30]. This metric determines how well salient and non-salient regions of the image can be discriminated by their saliency value in a PSM using a simple threshold [25]. Similar to “Hit Rate”, the 20% most salient pixels of the image based on its RHSM are selected as salient regions of the image, and the rest are called non-salient. Also, a binary map is created by thresholding the PSM. The threshold is increased gradually from the minimum of the map to its maximum, which changes both the hit rate (labeling salient locations as salient by the PSM) and false alarm rate (labeling a non-salient location as salient). The ROC is a curve that plots the false alarm rate as a function of the hit rate (0≤Hit Rate & False alarm rate≤1). The area under the ROC, ROC Area, is a well-known measure of similarity between two saliency maps.The Kullback–Leibler divergence value introduced in Eq. (20) is used as a measure of dissimilarities between two maps in [16,31].(20)KLph=∑xpxlogpxhxwhere p(x) is the predicted probability density function calculated from the PSM. h(x) is the probability density drawn from the RHSM of the image. KL is generally used to measure distance between two probability distributions. NKL, defined as(21)NKL=1−KLis used in this paper to find similarities between two maps.Define average fixation saliency (M¯fix) obtained when sampling the PSM at the fixations of human observers, and average saliency (μM) as the mean of the PSM. Then Score[32] is defined as:(22)Score=M¯fix−μM/μMScore is also used in [33,34] and a similar metric is used in [35]. In this paper, Score values are normalized to lie in the interval [0 1].There are several saliency map comparison metrics in the literature, but it can be easily shown that results from these in ranking different saliency models do not agree. The best visual saliency model identified by one comparison metric might show poor results when evaluated by another metric. Therefore, before ranking visual saliency models, it is important to identify which metric could be considered the best. The database created in [10] is used herein to design an evaluation procedure for comparison metrics. The longest dimension of each image in the data base is 1024 and the other dimension varies from 405 to 1024, with the majority having 768 pixels.It is commonly accepted that the best saliency map for an image is the one created using human observers' fixation data. Using the LPWHL database [10], two fixation maps are defined for each image herein. Randomly selecting a simple majority of fixations for a given image, a reference fixation map is created. The remaining fixations for that image are used to generate another map called the human fixation map. Saliency maps are generated from these fixation maps by convolving them with a 2-D Gaussian function, namely the Gaussian function used in [10]. The saliency map computed from the human fixation map is called the Human Saliency Map (HSM), and the saliency map computed from the reference fixation map is designated the RHSM. Accordingly, we propose for our first criterion a good comparison metric would be expected to find HSMs analogous to the corresponding RHSMs.Observers in the LPWHL study might have had different priorities during the experiment. It has been proved [6] that given the same image, fixations and patterns of saccade do change for different questions that were asked of an observer prior to viewing the image, which is believed to be a property of top-down saliency [5,6]. Accordingly, we believe it is important to select fixations randomly for the reference fixation map, instead of choosing all fixations of a fixed sample of observers.In this paper, it has been assumed that the worst fixation map for a specific image is a random selection of image points that are designated as “random fixation points” when in fact they typically would not be true fixation points. Such a map is generated for each image by randomly selecting some locations across the image as “fixations”. Similar to HSMs and RHSMs, a map is created convolving this random fixation map with the 2-D Gaussian function used in [10], and it is designated a Random Saliency Map (RSM). Our second criterion for a good comparison metric is that it should clearly distinguish an RSM for a given image as dissimilar from an RHSM for that image. Fig. 3shows an image (a) from [10] along with its original saliency Map with 75 fixation points (b). Samples of RHSM (c), HSM (d) and RSM (e) for NRef.fix.=50 are also shown with the average of RHSMs (f), HSMs (g) and RSMs (h) over 100 samples. The higher the number of fixations used in creating RHSM and HSM, the closer are these maps to the original saliency map. Although HSMs created using a small number of fixations do not seem similar to the original saliency map, the average RHSMs and HSMs (over 100 samples) are very similar to the original saliency map. As expected, the average of the RSMs is still a random map, and different regions are highlighted randomly Fig. 3(h).In this Section, for each image with fixation points from [10], reference fixation maps are created using NRef.fix.=40,45, …, 70 fixations chosen randomly from the fixation database. The remaining fixations in the database (35, 30,…, 5 fixations) are used to create human fixation maps. Then for each image, a number of random pixels are selected equal to the number of fixations chosen for the human fixation map. For each fixation number, all comparison metrics are employed to compare RSMs and HSMs with the corresponding RHSMs. This process was repeated 100 times, with seven NRef.fix.values for all images in the database (702,100 repetitions) and the results are illustrated in Table 1. 100 repetitions are chosen so that the results do not change as the number of repetitions increases from 70 to 100.Since the number of salient pixels usually varies in different saliency maps, Judd et al. [36] suggest matching the histogram of the saliency maps created for an image with the histogram of the reference saliency map of the image before comparing them together. This way the number of salient pixels in different saliency maps would be very close together. It creates a fairer basis for comparing saliency maps. Fig. 4shows an RHSM with 50 fixations (a) for the image shown in Fig. 3(a), HSM and RSM (b) and (d) with 25 fixations, and the HSM and RSM after histogram matching (c) and (e). Cumulative frequency distributions with 256 bins for these maps are plotted in Fig. 4(f). For simplicity in analyzing graphs, in the cumulative frequency distribution curves, the numbers of zeros in the maps are not counted.As demonstrated in Fig. 4(f), although the number of fixations in the sample HSM and RSM are equal, and fixation maps are convolved with the same Gaussian filter, the number of salient points in the RSM is almost twice the HSM. This is caused by the fact that in the random fixation maps, fixations are distributed widely and usually far from each other. On the other hand, in the human fixation maps, fixations are distributed mostly around salient locations in the central parts of the map. The number of salient pixels in HSM and RSM will be close to the number of salient pixels in the RHSM after histogram matching, as shown in Fig. 4(f). RHSMs, HSMs and RSMs are normalized to the [0 1] interval before histogram matching. Two approaches were taken in our investigations: RSMs and HSMs were compared with the corresponding RHSMs before and after histogram matching.All comparison metrics compute scalar values in the [0 1] interval. Good metrics are expected to produce high values close to 1 when comparing HSMs with RHSMs, and low values close to 0 when comparing RSMs with RHSMs. Accordingly, we designate the best comparison metric as the one which discriminates the best between HSMs and RSMs. We use thresholding to determine how HSMs are discriminated from RSMs by comparison metrics. The threshold for each metric is given by:(23)Threshold=ArgminThr∫0Thry1dx+∫Thr1y2dxwhere y1is the HSM histogram and y2is the RSM Histogram, respectively; and Thr stands for threshold (metric result histograms are created using 1000 bins). For example, the NCC threshold value is 0.621 for which NCC produces results larger than this threshold for 688,915 of HSMs (out of 702,100) and produces results smaller than this threshold for 693,443 of RSMs. Accordingly, we consider NCC as correctly classifying 688,915 HSMs and 693,443 RSMs, and the rest (13,185 HSMs and 8,657 RSMs) are misclassified. The percentage of misclassifications and related threshold values are shown in Table 1 for each of the metrics. We designate the best metric as that which produces the lowest misclassification percentage.Abbreviations for each metric are given in Section 3. Table 1 ranks the metrics according to their misclassification percentage, with evaluation rank 1 being the best. Table 1 shows that without histogram matching, the best metrics are NCC and Cosθ, which result in misclassification percentages of 1.56% and 1.89%, respectively. However, we believe matching the histograms of the saliency maps with RHSMs is essential in creating an impartial evaluation. Accordingly, since Score creates minimum misclassification error after histogram matching (0.634%), we designate Score as the best metric for comparing saliency maps. On the other hand, NKL (Kullback–Leibler divergence) with 21.45% and 43.56% misclassification, without and with histogram matching respectively, is the worst metric. NKL compares the probability density functions of the saliency maps. After histogram matching probability density functions of RSMs and HSMs would be very similar to each other, such that NKL cannot distinguish between them. As shown in Eqs. (12) and (18), the Cosθ and NCC formulations are very similar. Therefore, we expected them to produce similar results. Table 1 demonstrates that their results are very similar for both cases.All visual saliency models summarized in Section 2 have been applied to all the 1003 images in the LPWHL database [10]. Their codes were downloaded from their websites, except for the CC model [16] for which the authors provided us with their saliency maps on the LPWHL database. Fig. 5gives pictorial saliency results for one image before and after histogram matching. Abbreviations for each of these methods are given in Section 2.As discussed before and demonstrated in Figs. 4 and 5, the number of highlighted (salient) pixels returned by each visual saliency model for an image varies greatly. As shown in Fig. 5, histogram matching helps us find locations that each model finds most salient. Accordingly herein, histograms of the PSMs (predicted saliency maps) are matched with the RHSM histograms before comparison. In this section, the RHSM for each image in the database is computed using all of the fixation points for that image. Saliency model comparison results without histogram matching can be found in [9].PSMs for each image are compared with the RHSM for that image using all comparison metrics. Box-plots of the Score (the highest ranked saliency map comparison metric for the case with histogram matching) results are shown in Fig. 6. To check if there are statistically significant differences between models, first the normality of their Score values are checked using the Shapiro–Wilk method [37]. Score values of none of the results are normally distributed with a significance level of 0.05. Accordingly, one of the best non-parametric methods, the Wilcoxon test [38], was used, and results are given in Table 2with 0 for no statistically significant difference and 1 for significant difference.In each box in Fig. 6, the central horizontal red line is the median. The higher the median, the better the performance of the visual saliency model. The lower and upper edges of the box mark the 25th and 75th percentiles, q1and q3respectively, and the horizontal whiskers show ±2.7σ (or 99.3% coverage if the data are normally distributed). In these boxplots, data-points larger than q3+1.5×(q3−q1) or smaller than q1−1.5×(q3−q1) are drawn as outliers (magenta pluses). The horizontal dotted line (Score=0.0553) shows the threshold for Score in Table 1 that discriminates RSMs from HSMs. Accordingly, saliency maps for which Score is larger than the threshold are considered more similar to the corresponding RHSM, the larger indicating the more similar. Maps for which Score is less than the threshold line are closer to an RSM than the corresponding RHSM. For example, using the threshold value of 0.0553, 661 saliency maps (out of 1003) computed by the GBVS algorithm were found more similar to an RSM than the corresponding RHSM.Fig. 6 shows GBVS outperforms other models and EH is ranked 2nd based on the Score comparison metric. All medians are under the threshold line, which indicates the majority of the PSMs created by each model are classified as RSMs. The Wilcoxon test found no significant difference between GBVS and EH, which are statistically different from the rest of the models as demonstrated in Table 2. CC, IS and CASD differences are not significant, as well as the difference between IK, SUN and SR.Saliency comparison metric averages and the rankings based on them in parentheses (the lower the rank number, the better) for all visual saliency models are shown in Table 3. The number of maps classified as RSM based on each metric and the ranking based on the number of RSM maps in parentheses are also shown in Table 4. The right column of this table shows the average number of maps classified as RSM based on all saliency comparison metrics.Tables 3 and 4 show that the GBVS model outperforms other visual saliency models based on Score and also based on the average number of maps classified as RSM over all metrics. EH is ranked 2nd and FTSRD is ranked lowest in these tables. As a further note, the rankings based on Score are very similar to the overall ranking based on the average RSMs for all metrics. Also, after histogram matching, saliency map comparison metrics tend to agree more on ranking visual saliency models compared to the case without histogram matching in [9]. Accordingly, we select GBVS (Graph Based Visual Saliency) as the best bottom-up visual saliency model.Judd et al. [36] and Borji et al. [39] stated that visual saliency models that create blurrier saliency maps usually are ranked higher than models that create saliency maps with sharp edges. Also, maps that are biased towards the image center tend to gain better results than others at predicting HRSMs. In this section, we optimize the level of blurriness and center-bias of each visual saliency model by varying appropriate parameters characterizing these effects and choosing the parameter values that maximize the performance of the visual saliency model. This creates the opportunity to compare visual saliency models at the best levels of blurriness and degree of center-bias for each model. For blurriness, PSMs are convolved with 2D Gaussian filters with σ=10, 20, 30, …, 150pixels to produce a map designatedMBlurred. Then a weighted Center-Map (CM) is added to a weightedMBlurredto produce a new saliency mapMNew, using the following equation as suggested in [36]:(24)Mnew=w×CM+1−w×MBlurredwhere w=0,0.1,0.2,…,1, and the closest 2D Gaussian blob to the average of all RHSMs shown in Fig. 7is selected as theCM; however, the histogram of the center-map is matched to the histograms of the RHSMs before using (24). w=0 indicates that the new map is identical to the blurred map, and w=1 indicates the new map is equal to the center map.Exploring histogram matching, the optimized level of blurriness and center-bias of each visual saliency model by varying the variance σ of the Gaussian filter and the weight w in Eq. (24) is studied in this section. As an example of this study, Fig. 8shows a sample image, its RHSM, PSMs produced by the SR visual saliency model, blurred and center-biased saliency maps, and their Score results. The histograms of the resulting saliency maps are also matched to the histograms of the corresponding RHSM before comparison.Note in Fig. 8 that for the shown image, SR benefits from a certain degree of blurriness, with the highest value of Score=0.0981 for σ=10 and w=0. After modifying PSMs using Eq. (24), they are compared to the corresponding RHSM using Score (as the best metric for the histogram matching process). The optimum Gaussian blurring value for σ of each visual saliency map, the best weight values for adding the center-map, the visual saliency models new average Score, the standard deviations and their percentages of improvement are shown in Table 5.As demonstrated in Table 5, all visual saliency models can improve their performance with blurring and adding a center biased map. GBVS performed better than other models; however, all visual saliency models produce 614 to 667 minimum number of maps classified as RSM which are 61.2% to 66.5% of the images in the database. This indicates that visual saliency models under histogram matching produce relatively poor results when evaluated by Score. This also indicates that there is clearly an opportunity for an improved visual saliency model. GBVS used the minimum of the center map (50%), and FTSRD and SR used the maximum (100%). This means that the center map outperforms FTSRD and SR in mimicking human observers.In this section, we test visual saliency mechanisms on a benchmark of 54 synthetic images used in [39], 6 of which are shown in Fig. 9first row. CC [16] is not included here because we did not possess the computer codes for CC. Since there are no reference saliency maps available for the synthetic images dataset, reference fixation maps were generated by manually selecting 1 to 5 pixels at the central parts of the salient regions, depending on the number of salient regions in the image. Then, reference saliency maps were created by convolving the fixation maps by a 2D Gaussian filter with proper σs that highlight the salient regions, with results depicted in Fig. 9 second row. In Fig. 9, images are sorted based on their average Score values from all saliency models from high at the left to low at the right. All visual saliency models could find the salient regions in Fig. 9(a), and none of the models could detect the salient parts of (f).To evaluate and rank saliency model performance on the synthetic images, first the histograms of the PSMs for each image were matched to the corresponding reference saliency map. Then PSMs were compared to reference maps using Score, and results are depicted in Fig. 10and Table 7. The Shapiro–Wilk test shows that Score results for none of the models are normally distributed, with a significance level of 0.05. Accordingly, the Wilcoxon method is used to analyze the difference between models, and the results are shown in Table 6.Table 6 demonstrates that GBVS, CASD and IS are not found statistically different by the Wilcoxon test; however, we believe that the differences between IS and the first two ranked models are significant. This is a drawback of non-parametric statistical models like Wilcoxon, that when the number of data points is small, their results are not as reliable as when there are many data points. Also, no significant differences between EH results and AIM and IS were found, as well as between SUN, SR, FTSRD and IK.As shown in Fig. 10 and Table 7, GBVS with 6 maps classified as RSM (11.1%) produces the best PSMs for the synthetic image database. CASD and EH are 2nd and 3rd, respectively, with RSM percentages of 18.5 and 25.9, respectively. Models ranked 4th to 6th are very close in RSM percentages (around 42%). The saliency models FTSRD, SR and SUN, with Score means of 0.268, 0.217 and 0.232, have 61.1%, 62.9%, and 66.7% of maps classified as RSM, which rank them lowest. Accordingly, we select GBVS as the best bottom-up visual saliency model on synthetic images. As demonstrated in Table 7, the standard deviations for EH and IS are so large that their 25th and 75th percentiles in Fig. 10 meet the lower and the upper bounds of the data (0 and 1), respectively.Comparison of Table 7 with Table 5 shows bottom-up visual saliency models performed much better on synthetic images than natural images. This is because, synthetic images contain solely bottom-up features and information, e.g. change in color or orientation. Also, reference saliency maps created for synthetic images highlight only salient regions of the image and include all such regions. In the RHSMs created in the LPWHL database [10], sometimes salient regions close to the boundary of the image are missed, and non-important parts of the images close to the center are highlighted in the saliency maps.As suggested in [36,39] for fair model comparison, PSMs were convolved with 2D Gaussian filters with σ=5, 10, …, 25pixels and then were compared with the corresponding reference saliency maps using Score after histogram matching. Since, most of the saliency maps highlight parts other than the center, analyzing center bias would not create useful results. Accordingly, only the level of blurriness is studied here. The optimum Gaussian blurring level for each visual saliency model, their new average Score values, the number of maps classified as RSM, and their percentages of improvement are shown in Fig. 11and Table 8.Fig. 11 and Table 8 show that blurring improves the performance of most of the visual saliency models. After blurring, with 9.25% improvement compared to Table 7, CASD reaches a mean of Score of 0.846 with only 9.25% of maps classified as RSM. However, we see that the performances of GVBS and IS visual saliency mechanisms deteriorates with blurring. Improvement percentages are calculated in comparison with means of Score in Table 7.

@&#CONCLUSIONS@&#
