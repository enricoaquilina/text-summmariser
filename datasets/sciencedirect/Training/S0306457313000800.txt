@&#MAIN-TITLE@&#
An investigation into the application of ensemble learning for entailment classification

@&#HIGHLIGHTS@&#
Feature extraction for the processing of supervised entailment classification.Comparison of various ensemble methods to single learning approaches.Consideration of a novel heterogeneous/homogeneous ensemble combination for the problem area.

@&#KEYPHRASES@&#
Entailment,Classification,Ensemble learning,

@&#ABSTRACT@&#
Textual entailment is a task for which the application of supervised learning mechanisms has received considerable attention as driven by successive Recognizing Data Entailment data challenges. We developed a linguistic analysis framework in which a number of similarity/dissimilarity features are extracted for each entailment pair in a data set and various classifier methods are evaluated based on the instance data derived from the extracted features. The focus of the paper is to compare and contrast the performance of single and ensemble based learning algorithms for a number of data sets. We showed that there is some benefit to the use of ensemble approaches but, based on the extracted features, Naïve Bayes proved to be the strongest learning mechanism. Only one ensemble approach demonstrated a slight improvement over the technique of Naïve Bayes.

@&#INTRODUCTION@&#
Recognizing Textual Entailment (RTE) is based on the task of deciding, given two text fragments, whether the meaning of one text fragment (referred to as the hypothesis) is entailed (can be inferred) from the other fragment (referred to as the text) (Dagan & Glickman, 2004). This area has relevance to a number of Natural Language Processing (NLP) based tasks such as Question Answering (Harabagiu & Hickl, 2006), Summarization (Lloret, Ferrández, Munoz, & Palomar, 2008) and Machine Translation (Padó, Galley, Jurafsky, & Manning, 2009). Research in this area has been advanced by a series of challenges starting in 2005, which ran initially as PASCAL RTE challenges but since RTE4 have become NIST challenges. Each RTE challenge provided an annotated training and test set which allowed for a common form of standard benchmarking by the participants. The classical identification of entailment is considered as a form of logical entailment. However, the usual consideration is that textual entailment between a text (denoted by T and hypothesis (denoted by H) may hold based on human judgement, even if this inference could not be shown as a logical entailment based on logical reasoning and computational semantics (Dagan, Dolan, Magnini, & Roth, 2009). A related area of research focusses on the semantic inference process of learning general simple entailment rules whereby a rule describes a directional semantic inference between two predicates (Bernant, Dagan, Goldberger, & Adler, 2012). Such rules can of course assist in solving the general problem of textual entailment.There are many different approaches to this task and the use of various NLP components have included lexical similarity measures, anaphora resolution, paraphrasing, syntactic graph alignment, named entity recognition, semantic parsing and logical inference based on model theoretic approaches. Various RTE challenges have depended on one or more the aforementioned processes. Resources have included WordNet (Fellbaum, 1998), DIRT (Lin & Pantel, 2001) and other forms of world knowledge such as Wikipedia. There has been strong focus on considering the process of entailment as a Machine Learning (ML) classification task, and various approaches have been considered to find appropriate feature value vector representations for the entailment pair, e.g. in RTE3, 13 out of 26 teams considered some form of Machine Learning (ML) classification. Over the course of the challenges there has been a wide range of accuracies. The most successful approaches have reached impressive levels of accuracy. For example, Hickl and Bensley (2007) achieved 80% accuracy on the RTE3 test set, however this was achieved as a two stage potentially iterative process involving being able to select the best discourse commitments (a set of simple propositions that either the hypothesis or text show to be true) and applying entailment classification to the best choice of these aligned commitments. This process requires the use of numerous linguistic techniques and requires both the use of finite state transducers and machine learning.In general, accuracies for the different RTE challenges have ranged from 50% to 60% on RTE1 (17 submissions), from 53% to 75% on RTE2 (23 submissions), from 49% to 80% on RTE3 (26 submissions) and from 45% to 74% on RTE4 (26 submissions) (Bar-Haim et al., 2006; Dagan, Glickman, & Magnini, 2005; Dagan et al., 2009; Giampiccolo, Magnini, Dagan, & Dolan, 2007) and from 54% to 73% for RTE5 (21 submissions) based on an evaluation of a test set (Bentivogli, Dagan, Dang, Giampiccolo, & Magnini, 2009). However, when one considers a one pass linguistic process of feature extraction and machine learning based classification, the highest accuracy achieved was at most 64% for RTE2 and 67% for RTE3.3Only the RTE2 and RTE3 overview of proceedings details the usage of machine learning based classification in individual submissions.3Higher accuracies are usually achieved by more intricate processes which require more than one stage of linguistic processing or were not based on machine learning techniques. The level of accuracies reflect that entailment is not a trivial problem. With the exception of RTE4, the participants were provided with a training set. They were not constrained to only considering the training data provided by the given challenge and usually allowed for consideration of data from previous challenges, which was requisite in the case of RTE4. The data sets were always independent of each other, the only similarity was data drawn from a number of natural language processing applications such as Question Answering (QA), Information Retrieval (IR), Information extraction (IE) and Document Summarization (SUM). There have been subsequent challenges after RTE5 but they have been more specialized concerning the specific task being evaluated with regards to a given application setting. The IBM Watson deep QA system provided state of the art performance on the RTE6 challenge (Ferrucci, 2012) which is part of an Update Summarization task. However the focus of this challenge is on finding sentences in a corpus that entail a given hypothesis, so it is very different from the previous challenges involving classification. In addition, the latter system depends on the automatic construction of a semantic knowledge base, PRISMATIC from a large number (30GB) of factual documents (Fan, Kalyanpur, Gondek, & Ferrucci, 2012). Table 1shows examples of true and false entailments drawn from each of the NLP applications present in the RTE2 development data.We focus on a one-stage process which does not require any computationally intensive steps, and which provides a simple feature extraction to capture similarity/dissimilarity features between the entailment text and hypothesis. The linguistic processes to facilitate this require only tokenization, tagging and dependency parsing. The only knowledge resource that we consider is the use of WordNet. Based on such feature vectors, we wish to assess what is the best performing classifier or combination of classifiers, not only on one entailment data set but on a range of data sets. In this respect, we wish to determine which method provides the best generalization performance as seen by an evaluation over a range of data sets drawn from the various RTE1 to RTE5 challenges.The use of supervised classifiers in entailment has been prevalent but they generally focus on only one method of which decision trees or support vector machines are often a popular choice. One area that has received little attention is the application of ensemble learning which is the central focus of this paper. There has been some usage of ensemble based methods in previous challenges (for example Ferrés and Rodríguez (2007) applied AdaBoost whereas Kozareva and Montoyo (2006) considered Stacking and Voting). However, their utilization has been based on little or no justification and no thorough consideration has been made as to a choice of ensemble method and to their effectiveness. Ensemble learning refers to the process by which multiple learning machines are trained and their outputs combined, treating them as a committee of decision makers or predictors. The principle is that the committee decision, with individual predictions combined appropriately, should have better overall accuracy, on average, than any individual committee member. Numerous empirical and theoretical studies have demonstrated that ensemble models very often attain higher accuracy than single models (Brown, 2010). The generalization error of a classifier can be decomposed into a bias term, a variance term and an intrinsic noise term (Kohavi & Wolpert, 1996). The bias measures the expectation of how close the learning algorithm’s average prediction matches a target whereas the variance measures how much the prediction varies per data set from the average over all possible data sets. Mechanisms for ensemble learning usually concentrate on reducing either the bias and/or variance in the generalization error. A related interpretation to the bias-variance analysis of error (Brown, Wyatt, & Tiňo, 2005a) is that a necessary and sufficient condition for an ensemble to be more accurate than any of its members is that it consists of diverse and accurate models (Hansen & Salamon, 1990). Two models are considered diverse if they are not correlated in their errors and accurate if the classification accuracy is better than random guessing (Kuncheva, 2004). There has been numerous metrics proposed for the measure of diversity (Brown, Wyatt, Harris, & Yao, 2005b). There are two general stratagems to ensemble learning methods: the first is concerned with generating base models where only one learning algorithm is employed (homogeneous learning) and the second is based on methods where different learning algorithms are applied to generate the base models (heterogeneous learning). In this paper, we consider both approaches and assess empirically which approach is best suited to the problem of entailment classification. The paper consists of the following sections, Feature Extraction, Ensemble Methods, Experimental Evaluation and Analysis and Conclusions.The process of extracting features was influenced primarily by various lingustical features considered by Inkpen, Kipp, and Nastase (2006) for the RTE2 data challenge. Numerous other studies have focused on the extraction of similarity/dissimilarity features in a one pass based feature extraction and classification process. The range of possible similarity/dissimilarity features based on linguistic processing is immense, and consequently two different studies never focus on the exact same set of features. We chose this approach primarily as it allowed for a computationally lightweight extraction process with a reasonable level of accuracy as shown for RTE2 test data set of 58%. We extended this feature extraction process to allow for other features that we considered likely to be of relevance to the entailment classification process. Features that we chose required the application of linguistic processing mechanisms including tokenization, tagging, parsing and WordNet but did not require more intricate mechanisms such as graph alignment.By utilizing such a method, we were able to evaluate a diverse range of single and ensemble based learning methods. Feature extraction is based on the following NLP based pipeline utilizing entailment pairs shown in Fig. 1. Features are either nominal or numeric (which are identified by the num prefix). Features are categorized into a number of broad categories: lexical, similarity, relational, and semantic. Table 2summarizes the various features. Note that a number of ordinal features have a numeric counterpart. In the case that a feature is nominal, we indicate its potential choice of values as a set of values where “…” indicates that the maximum value is dependent on the data set and as such is open-ended. However, as shown for the features extracted for the RTE2-dev in Table 3, the range of possible values is not high.A number of features are identified as counts in both absolute and normalized form (normalized relative to the hypothesis). These are the number of content words (words excluding stop words) in common, the number of stop words in common (stopWordOverlap, numStopWordOverlap), the number of words in common (wordOverlap, numWordOverlap) the number of verbs in common (verbOverlap, numVerbOverlap) and nouns in common (nounOverlap, numNounOverlap). A number of nominal features have values indicating whether both the text and hypothesis share a negation polarity context modifier (MacCartney, Grenager, de Marneffe, Cer, & Manning, 2006). These include simple negation (not), downward-monotone quantifiers (no, few), restricting prepositions (without, except) and superlatives (e.g. tallest).The features in this categorization are based on the usage of WordNet and in the case of one feature, distributional similarity and corpus statistics (Corley and Mihalcea, 2005). The WordNet based features include synonym match, antonym match and meronym match and the use of verb entailment relations: verb cause and verb entailment. These features are expressed in both absolute and normalized form. Distributional similarity is based on utilizing pre-computed information content files from the British National Corpus provided by the WordNet::Similarity project (Pedersen et al., 2004). The similarity between T and H is calculated based on Eq. (1.1) where maxSim is the maximum similarity between a word w∊H and any word w in T and idf(w) is the inverse document frequency for the word. We calculated the inverse document frequency of terms based on an analysis of the NYTimes annotated corpus (Sandhaus, 2008), which is a large corpus with just over 1.8million documents.(1.1)sim(T,H)=∑w∈HmaxSim(w)*idf(w)∑w∈Hidf(w)If sim(T,H)>similarity_threshold, we set the feature similarity flag distSim=1 (0 otherwise).The number of skip bigrams in common to the text and hypothesis and the number of skip bigrams consisting of noun or verbs only, both in normalized form, were identified. A skip bigram is any pair of words in their sentence order, allowing for arbitrary gaps between the pair.For example, given the entailment pair, with text, T1 and the hypothesis, H1T1 “Guerrillas killed a peasant in the city of Flores.”H1 “Guerrillas killed a civilian.”the set of possible skip bigrams in H1, allowing for lemmatization, is {“guerrilla kill”, “guerilla a”, “guerilla civilian”, “kill a”, “kill civilian”, “a civilian”}.Based on dependency parsing, we considered each grammatical dependency pair obtained based on the identified dependency relations in the text and hypothesis. The dependency relation is expressed by the tuple:relation head/POShead modifier/POSmodifierwhere POShead and POSmodifier denote the part of speech of the head and modifier element in the relation respectively. In computing relation overlap, we use the tuple as it is, and also as a dependency pair (Head, Modifier), to cover the cases when the same lemma appear in different grammatical relations, possibly also with different parts of speech. An example of a full relation in H1 expressed as a tuple is:nsubj kill/VBD guerrilla/NNPSwhich has corresponding dependency pair (kill, guerilla).Tuples and dependency pairs are used to compute the following four features:•Absolute number of overlapping pairs between the test and the hypothesis.Normalized number of dependency pair overlap (absolute number divided by the number of tuples generated for the hypothesis).Absolute number of overlapping relations.Normalized number of overlapping relations.A special feature numNegateVerbs based on dependency relations is determined by a normalized count of negated verbs that appear only in the hypothesis and not in the text.Semantic features are derived based on two features: the first feature determines the number of named entities that are shared in common between the text and hypothesis. The second measure is based on the use of semantic role labeling. Based on an adapted approach derived from Wang, Li, Zhu, and Ding (2008), we calculate a numeric semantic role similarity based on shared semantic roles, where a semantic role has form Arg0, Arg1, etc. (based on Propbank semantic roles). In our approach, we only considered one predicate in the sentence, which is the verb below the root node in the dependency tree. The verb predicate and its arguments with semantic role labels constitute the verb frame for this verb. Given text T and hypothesis H, let Rhbe the semantic roles in H (associated with the main predicate) and Rtbe the semantic roles in T (associated with the main predicate). Let {r1,r2,…,rk} be the set of K common semantic roles between H and T based on the main predicate.Tt(ri)={ti1t,…,ti|Tt(ri)|t}is the set of argument terms of T in role ri. The similarity between Tt(ri) and Th(ri) is:(1.2)rsim(Tt(ri),Th(ri))=∑jtsim(tijt,ri)|Tt(ri)|where(1.3)tsim(tijt,ri)=1,iftijt∈Tt(ri)and∃tikh∈Th(ri)|tijtandtikhare related,0otherwiseThe overall semantic role similarity is given by:(1.4)fsim(T,H)=∑i=1krsim(Tt(ri),Th(ri))|Rh|Two argument terms are considered related if based on the WordNet hierarchy, they share either a synonym, hypernym, hyponym, holonym or meronym relationship. For example, in the previous entailment pair example, there is one verb frame for T1 associated with predicate kill and argument term sets Tt(Arg0)={guerilla}, Tt(Arg1)={peasant}. H1 has the verb frame with predicate kill and argument term sets Th(Arg0)={guerilla}, Th(Arg1)={civilian}. The common semantic roles are {Arg0,Arg1}.As a result of rsim(Tt(Arg0),Th(Arg0))=1 and rsim(Tt(Arg1),Th(Arg1))=0 the value for fsim(T,H))=0.5 which is the overall semantic role similarity.We considered the following choice of single learning methods, which have been considered in a number of different evaluations for entailment classification in Table 4. We give the WEKA class name and description of any options that are applicable. For brevity in the text we refer only to the WEKA class name and show option settings in brackets The WEKA realization of support vector machines was based on the sequential minimal optimization (SMO) algorithm, which uses a polynomial kernel by default. From a perspective of heterogeneous learning, the various learning methods have various different learning biases, so they are also well suited for heterogeneous learning. As we did not expect any of the entailment pairs to be very close in similarity to each other the number of nearest neighbours in IBk was set either to 11 or 15. So in total 6 different base methods were considered. In the case of the other classifiers, the default settings were utilised.We considered a range of ensemble homogenous approaches, many of which depend on some form of feature or instance data randomization. The devised mechanisms are generally well suited to high variance unstable learners, of which decision trees are a prime example of Nearest neighbours, Naïve Bayes and Support Vector machines tend to be stable but biased learners (Ting, Wells, Tan, Teng, & Webb, 2011). In Bagging (Breiman, 1996) one samples the training set, generating random independent bootstrap replicates to generate training sets. A classifier is constructed for each of these training sets and aggregates their classifications by a simple majority Vote in the final decision combination. In the random subspace method (RSM) (Ho, 1998b), classifiers are constructed in random subspaces of the data feature space. These classifiers are usually combined by simple majority Voting in the final decision combination. Skurichina and Duin (2002) investigated the method in comparison to Bagging for different training set sizes and levels of feature redundancy. They showed that the method is appropriate for small training sets with a high level of redundant features. García-Pedrajas and Ortiz-Boyer (2008) indicate that some of the feature subsets may lack the discriminatory ability to separate classes but this may be less of an issue in the case of a two class entailment problem.In Random Forests (Breiman, 2001), which is an ensemble technique specific to decision tree learning, a number of random trees are grown, where for each node split a random selection is made from a fixed number of attributes. Random tree outputs are also combined using majority Voting. AdaBoost (Freund & Schapire, 1996) has no random elements and grows an ensemble in an iterative fashion where base models are trained on successive reweightings of the training set where the current weights depend on the performance of the ensemble. The focus on the reweighting is to apply more emphasis on difficult instances to classify correctly. Rotation Forests (Rodríguez, Kuncheva, & Alonso, 2006) generate an ensemble classifier by randomly splitting the feature space into subsets for each classifier and applying principal component analysis to each subset or group of features. The principal components from each subset are assembled to form a rotation matrix which is used to transform the training data for the given base classifier. The mechanism was shown to outperform Random Forest, Bagging and AdaBoost. Table 5provides details concerning the various homogeneous methods and the possible flag settings that we utilize. The option −I indicating the number of base classifiers in ensemble is not shown, as it is always an option available for each method and in the experiments has the same setting for each method. The ensemble method, Decorate (Melville & Mooney, 2003), is an iterative mechanism based on enhancing diversity into an ensemble by adding base models where for each base model, artificial examples are added to the training set for the models and the model is added to the overall ensemble if the training error is not increased. The ensemble makes predictions in a similar fashion to Voting based on Averaging discussed in the next section. The setting for the −E flag which indicates the desired ensemble size is the same setting as for the −I flag.The most well known heterogeneous learning approach is the meta-modeling approach of Stacking (Wolpert, 1992). The common approach to Stacking as devised by Ting and Witten (1999) in essence follows this process: during training, all base classifiers are evaluated via J-fold cross-validation on the original dataset. Basically, a cross-validation splits the dataset into J; equal-sized folds, then uses J−1 folds for training and the remaining fold for testing. This process is repeated J times so that each fold is used for testing exactly once, thus generating one prediction for every example in the data set. Each classifier’s output is therefore a class probability distribution for every example. The concatenated class probability distributions of all base classifiers in a fixed order, followed by the actual class value, forms the meta-level training set for Stacking’s meta classifier. After training the meta classifier, the base classifiers are retrained on the complete training data. Ting and Witten (1999) proposed multi-response linear regression as the meta-learner which learns a linear regression model for each class. Seewald (2002) derived an improved mechanism referred to as StackingC also based on a multi-response regression, with the difference that each linear model uses features based on only the relevant specific class distributions (whereas the original approach by Ting and Witten uses all class probabilities for each linear model). We focused on this latter form of Stacking. A simpler approach to combining heterogeneous classifiers that we also considered is through Voting (where the WEKA class is referred to as Vote). The default form of Voting in WEKA is based on averaging of individual class probabilities for each base classifier and selecting the class with maximum probability. Due to the heterogeneous nature of the application of Stacking, there has been no detailed theoretical study outlining how and when it is effective, and studies tend to focus on the nature of the combiner and the ensemble set. It is thought though that its effectiveness lies in its ability to reduce bias (Sharkey, 1996). The general consensus on how to form an ensemble suited for Stacking is to employ base learning methods with varying learning biases.This section describes how experiments were carried out, the data sets utilized, and provides an evaluation of single learning methods, homogeneous learning methods and heterogeneous learning methods.The process of Information extraction and entailment classification is based on the use of embedded GATE (Cunningham, 2002). GATE embedded is an open source object-orientated framework developed in Java to provide embedded language processing functionality in diverse applications. It supports a number of processing resources such as sentence detection, tokenization, tagging and through a plugin, the Stanford dependency parser (De Marneffe, MacCartney, & Manning, 2006). We utilised an OpenNLP plugin within GATE (http://opennlp.apache.org/) to apply named entity recognition. The possible named entities were drawn from the following set of named entities: {PERSON,NAME,LOCATION,CURRENCY,DATE,TIME}. The process of semantic role labelling was based on the use of ClearTK (Ogren, Wetzler, & Bethard, 2008). To analyze textual content, a GATE processing pipeline was constructed. The first pair part of the pipeline performs the appropriate feature extraction for each entailment pair and creates WEKA instance data for each pair. Once feature extraction is complete, the second part of the pipeline provides an evaluation framework which performs an evaluation of a number of single and ensemble based classification methods. Entailment classification is based on the GATE Learning plugin, which supports a small number of WEKA (Witten & Frank, 2005) based classifiers. We modified the latter plugin to allow for the use of any WEKA classifier including a number of ensemble based methods. Also, where we considered an attribute to be numeric, we identified it in WEKA as a numeric attribute rather than considering all attributes to be nominal (which was the case in the original plugin). The evaluation performs a 1×10 fold cross-validation (CV) for each learning algorithm and calculates the macro-averaged F1-measure (F1cv), which is the F1-measure averaged over the 10 folds. For each fold, the recorded F1-measure is the harmonic mean of the precision and recall. The similarity_threshold for the distSim feature was kept at 0.7 for all experiments. Experiments were carried out on a Intel core i5 m480 2.67GHz with 4G RAM.The data sets are drawn from the RTE1 to RTE5 challenges. For the purposes of evaluation we consider training and test sets as simply evaluation data sets so that in total we have 9 data sets. Each data set is independent of each other and there is no duplication of any of the entailment pairs across data sets. Table 6provides details concerning the size of each of the data sets and average number of words in the text and the hypothesis per entailment pair. In the case of comparing single models, we also considered an extended set of datasets, based on 4 additional datasets (RTE1-random, RTE2-random, RTE3-random, RTE5-random} where each data set consisted of 400 randomly selected instances drawn from a combination of the training and test data for the respective RTE challenge. The additional data sets allowed for a paired t-test with a higher number of degrees of freedom, as described in 4.3.This subsection concentrates on experiments based on single classification models only. Initially, we considered whether the inclusion of semantic features (neOverlap and numSemanticRoleMatch) improved the performance of single models. Our reason for the non-automatic inclusion of these features, was that the additional processing required to extract the features, was slow. This was due to the large level of memory overhead associated with the use of such features, as the requisite models need to be loaded into memory. Table 7presents the average F1cv for each feature set which included all features plus 0 or more features drawn from the set of semantic features {neOverlap,numSemanticRoleMatch}. The average was taken over the 6 base methods for each data set and this was repeated for each of the four different feature sets. We considered if any of the feature set comparisons showed significant improvement in accuracy as measured by comparing the average F1cv measure for each data set as shown in Table 7, using 2-tailed paired t-tests (at a significance level p<0.05). This statistical test was based on 9 sample points in the case of 9 data sets and 13 for the extended set. This was not shown to be the case either when we considered only the original data sets or when we considered the extended set. As such, we excluded the use of semantic features from further evaluation as we wish to minimize the level of required linguistic processing.For feature sets excluding semantic features, Table 8shows the results of the individual based methods, and the average value for the given method, averaged across all data sets. In addition, the maximum value per data set and the index of the method returning the maximum is shown. The maximum accuracy varied quite considerably per data set from a high of 0.53 for RTE1-test and a high of 0.67 for RTE3-dev. Clearly based on averages, NaiveBayes outperforms all other methods. However, it only returned the maximum accuracy in 5 out of 9 the original data sets.The following table, Table 9summarizes for how many datasets a given method denoted by a row entry outperforms another method under the column heading based on a comparison of the F1cv. This count in row a column b added to the count in row b column a is less than or equal to the number of data sets in total (allowing for ties). In brackets is shown for how many data sets was the comparison shown to be statistically significant based on a paired t-test comparison of the F1-measures per cross validation fold (significance level p<0.05) based on the 10 sample points.It is clear that the number of data sets where the mean comparison was statistically significant was often far fewer than the number of data sets. This was a consequence of a large standard deviation in the observed F1-measure across folds for each method e.g. the standard deviation in F1-measure for NaiveBayes was 0.091 for the RTE2-dev data set. The large noted variation in standard deviation in F1 across folds for two individual methods means that a paired t-test will not show any statistical difference unless the mean values are very different. (A better comparison may have been based on using a 10×10 cross fold validation where we repeated a 1×10 fold validation 10 times for different stratifications of the data and compared the average per each validation. However, 10×10 cross fold validation would have been extremely time-consuming given the number of methods that we investigated). As a consequence, we are only able to draw any conclusions based on statistical tests by comparing the means of F1cv across all datasets. For all subsequent experiments we focus on this latter measure, as a determiner as to whether one method outperformed another method. The variation in F1cv measure across data sets was also more sensitive to statistical comparison due to the much smaller standard deviation in the sample points in comparing means. In addition, it served our intention to measure the generalization capability of a method. This process was similar to the previous comparison involving the inclusion of semantic features in feature sets. This test showed that NaiveBayes was the only method to statistically outperform all other methods. None of the other methods was shown to outperform any other at a statistically significant level. In addition, it returned the maximum accuracy for 5 out of 9 data sets.We considered the approach of Random Forests, and the techniques of Rotation Forest, Bagging and AdaBoost as applied to the decision tree learner C4.5 and the hybrid decision tree learner NBTree. We also considered the method of RSM as applied to nearest neighbours in addition to decision trees as this was shown to be beneficial by Ho (1998a). For each ensemble method we set the flag −I to 25 to indicate the presence of 25 base classifiers in the ensemble. There may be further gain in accuracy for a greater size of ensembles but if any gain is to be shown by a particular ensemble method, it should occur in the case of such an ensemble size (Opitz & Maclin, 1999).An initial evaluation assessing the technique of RSM considered the performance based on the size of random subspace sizes at 20% of all features and 50% of all features (the number of features recommended by Ho (1998b)), as applied to IBk for K=11 and K=15, C4.5 and NBTree. Fig. 2shows the performance of RSM as applied to IBk (−K 11), IBk (−K 15), C4.5 and NBTree. The accuracy was slightly higher for IBk (−K 11), IBk (−K 15) and J48 for 20% of features than 50% of features (with only RTE1-dev being the exception to this general pattern in the data sets). The comparison for NBtree showed a more mixed level of performance, although the average for 50% of features for NBtree was higher than for 20% of features.Table 10shows the average F1cv for each RSM method, and by way of comparison, the average F1cv for the base methods. If the average was statistically significantly greater, the value is highlighted in bold. Table 10 highlights that random selection of features was able to show improvements for both unstable learners such as J48 and NBTree and also for the stable learner IBk.We evaluated how the other methods applicable to enhancing decision tree learning performed in comparison to random subspacing. Tables 11 and 12provides a summary of this comparison for J48 and NBTree respectively. An average F1cv that was significantly higher than the baseline method is shown in bold, whereas an average that was significantly worse is shown in italics. In the case of J48, the average for Bagging and RotationForest was higher than the base method J48 and there were data sets for which either of two methods returned the highest F1-measure, however overall they did not match the performance of random subspacing.We surmise that a number of our features may be actually redundant as shown by the relative strength of RSM method. Skurichina and Duin (2002) indicate that RSM may be beneficial in the presence of a high level of redundant features. The performance of Bagging reflects that the fact that Bagging does not increase the bias in error and may reduce the variance (Breiman, 1996). However, it did not reach the performance level of RSM. Boosting may be particularly weak in the presence of a high number of redundant features although we have no evidence to support this conjecture (it was previously noted however that Boosting is often much weaker than Bagging in the presence of high classification noise where many instances are misclassified (Dietterich, 2000)). RotationForest did not improve upon the performance of Bagging and RandomForest returned a weaker performance than the base method. Decorate also improved upon the baseline method but not significantly. The results for RandomForest were lower than expected based on the previous studies which have shown an improvement in this technique over Bagging (Breiman, 2001).The variable parameter in RandomForest is the number of randomly chosen features (−K) that are allowed to be set for a split in construction of a decision tree. By default in WEKA this is set to log2(N) where N is the number of attributes. We considered whether the impact of a different setting for the number of attributes had an influence on the performance of RandomForest, as shown in Fig. 3. Allowing for some variation per data set, there did not appear to be any major improvement in choosing a value different to the default, which is 6 in this study.The result for RotationForest was also lower than expected. This is a more intricate method than random subspacing and how its performance is impacted by the presence of redundant features has not been considered, nor has a direct comparison to RSM. A key parameter was the size of subsets which by default in WEKA was set to 3. We considered whether allowing for different feature subsets size had a bearing on its performance. This is controlled by two flags −G which set the minimum size of a group of features and –H which set the maximum size of a group of features. Fig. 4did not show a marked improvement using a different setting from the default setting of “−G 3 −H 3”, although for individual datasets there were some improvement in F1cv.We considered the five learning methods (IBk (−K 15), NaiveBayes, J48, NBTree, SMO) as the base classifiers for heterogeneous ensembles. We did not consider IB(−K 11) as on average its performance was shown to be very similar to IBk(−K 15). The heterogeneous combination method was either StackingC or Vote. The number of folds for cross-validation in StackingC was set at 2. Given the noted improvement shown in the performance of the application of RSM to NBTree, IBk and J48, we also considered ensembles which replaced J48, IB(−K 15) and/or NBTree by RSM utilizing the same base method. This led to 8 possible ensemble member sets as shown in Table 13.The usage of a particular ensemble member set is denoted by an identifier added as a subscript to the heterogeneous combiner name. In such a case where RSM was introduced, we are applying a form of ensemble which had 3 levels, where one or more base learners is replaced by the homogeneous ensemble learning mechanism and the overall combination is based on one of the heterogeneous combiners. Tables 14 and 15show the results for StackingC and Vote, with the ensemble set identifiers denoting the ensemble set in usage. The baseline comparison is based on the ensemble set denoted by the subscript identifier a. Any value that is statistically better than the baseline is shown in bold and in italics if it is statistically worse. Results did not show any clear benefit in applying RSM to any of the base classifiers in the case of StackingC, as the average was no greater than StackingCa which did not utilize the RSM method. With the exception of StackingCb, the accuracy was reduced in comparison to StackingCa, but the value was only statistical lower in the case of StackingCe.The highest performing approaches of StackingCa and StackingCb had accuracy less than NaivesBayes but this was not shown to be statistically lower. StackingCa had a greater accuracy than Naïve Bayes for three of the datasets and lower in the case of the other six. StackingCb returned a similar performance. In general, it would appear that StackingC could not compensate for the fact that NaiveBayes was significantly better than all the other base methods considered for ensemble formation. We speculate that the application of RSM to base models possibly lead to an overall increase in the bias of a Stacked ensemble even if for the individual method the variance is reduced, which ultimately reduced the effectiveness of the Stacking method.Table 15 shows the results of the combination based on Voting. In general, it was seen that a greater accuracy was achievable in combining base methods and the highest average accuracy was shown for Voted where the average was 0.603, which was higher than NaiveBayes by 0.008, although this difference was not statistically significant. In the latter case, the accuracy was also higher for 7 out of 9 of the data sets. So in the case of Voting, there was some benefit in combining ensembles, which have base methods that have been enhanced using the application of RSM.

@&#CONCLUSIONS@&#
In this paper, we considered a simple framework for entailment classification based on the supervised learning of feature vectors based on the simple extraction of similarity/dissimilarity features between the text and hypothesis elements of an entailment pair. Based on this framework, we provided a detailed empirical study into what level of performance a user could expect from such a framework, by considering a range of standard single and ensemble based classifiers. We considered five different learning algorithms in this study, which were chosen, based on their distinctness and the fact that previous ensemble studies have shown them to be appropriate for improving their performance.We showed that a number of homogeneous learning methods were able to enhance the performance of unstable learners and in this respect, RSM proved the most effective. RSM was also able to improve the performance of the stable learner nearest neighbours. The benefit of the RSM method is attributed to the likelihood that potentially a high number of features were not discriminatory in terms of entailment classification and in fact may be redundant. The high level of such features is indicative of why other ensemble methods which have shown to improve unstable learners such as decision trees and hybrid decision trees did not show a marked improvement in accuracy in our results.However, as measured by the performance across data sets, none of the homogeneous methods were more accurate than Naïve Bayes. As Naïve Bayes is a stable form of learning, none of the homogeneous ensemble mechanisms was suited to enhancing its performance. We considered heterogeneous learners and found that a hybrid mechanism that combined heterogeneous ensembles using Voting based on averaging as the combination mechanism with the RSM applied to certain unstable base learners, was able to show an improvement over Naïve Bayes. However, the improvement was neither large nor was it significant. In Section 5, we attribute the lack of improvement shown by heterogeneous ensembles to a lack of diversity.Consequently, the study supports the view of Glickman et al. (2005) that “the uncertain nature of textual entailment calls for its explicit modelling in probabilistic terms”, wherein they also considered Naïve Bayes for entailment but utilized it for a different entailment problem. In this respect, combining Naïve Bayes with slightly weaker methods for entailment did not prove to be advantageous.As described in Section 5, the presence of high levels of redundant or irrelevant features limits the level of classification accuracy regardless of which single or combination model approach is adopted. This issue concerning the high level of redundant and irrelevant features was surprising, given that we chose a strong representation of various lexical, syntactic and similarity features, many of which are considered in other studies. This indicates that much larger feature sets need to be considered and an assessment given to the relevancy and redundancy of each feature.A possible mechanism to improve ensemble performance is to consider much larger initial feature sets and utilize methods that only retain sufficient accurate and diverse base classifiers, trained on feature sets which maximize relevancy and minimize redundancy. This could be achieved through integrating a feature selection method into the choice of base classifiers to retain in the ensemble. Given the relative strong performance of Naïve Bayes, it would also be interesting to pursue an ensemble based mechanism which allowed for its usage, as most methods do not as it is a stable learner. One exception is the approach proposed by Rodríguez and Kuncheva (2007) who were able to apply an ensemble approach based on the use of random oracle which proved effective with Naïve Bayes, a mechanism we will consider in future work. A very different approach to entailment focuses on the use of graph alignment mechanisms. We are also interested to see whether a semantic graph approach relying on heuristic functions to judge entailment, could, in combination with machine learning mechanisms, allow for a potentially more diverse set of classifiers and a better realization of ensemble combination.