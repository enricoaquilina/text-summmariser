@&#MAIN-TITLE@&#
Robust mixed-integer linear programming models for the irregular strip packing problem

@&#HIGHLIGHTS@&#
Two new nesting MIP models are proposed, compared and assessed.Direct geometry and no-fit-polygon convex decomposition are used.General models as they easily support pieces with complex geometries.Easier to implement than previous models in the literature.Outperform the best exact approach known in the literature.

@&#KEYPHRASES@&#
Packing,Cutting,Nesting,MIP models,

@&#ABSTRACT@&#
Two-dimensional irregular strip packing problems are cutting and packing problems where small pieces have to be cut from a larger object, involving a non-trivial handling of geometry. Increasingly sophisticated and complex heuristic approaches have been developed to address these problems but, despite the apparently good quality of the solutions, there is no guarantee of optimality. Therefore, mixed-integer linear programming (MIP) models started to be developed. However, these models are heavily limited by the complexity of the geometry handling algorithms needed for the piece non-overlapping constraints. This led to pieces simplifications to specialize the developed mathematical models. In this paper, to overcome these limitations, two robust MIP models are proposed. In the first model (DTM) the non-overlapping constraints are stated based on direct trigonometry, while in the second model (NFP−CM) pieces are first decomposed into convex parts and then the non-overlapping constraints are written based on nofit polygons of the convex parts. Both approaches are robust in terms of the type of geometries they can address, considering any kind of non-convex polygon with or without holes. They are also simpler to implement than previous models. This simplicity allowed to consider, for the first time, a variant of the models that deals with piece rotations. Computational experiments with benchmark instances show thatNFP−CMoutperforms both DTM and the best exact model published in the literature. New real-world based instances with more complex geometries are proposed and used to verify the robustness of the new models.

@&#INTRODUCTION@&#
Two-dimensional (2D) irregular packing problems, also known as nesting problems, are cutting and packing problems in which, as in all cutting and packing problems, small items or pieces have to be cut from a larger object (or packed inside a larger object) so that the unused regions of the large object, usually designated as waste, are minimized. According to Bennell and Oliveira (2008), what differentiates irregular packing problems from the other cutting and packing problems is that the pieces are not regular, i.e. a non-trivial handling of geometry is involved. In the 2D irregular strip packing problem, only two dimensions of the material to cut are relevant for planning purposes. The goal is to cut all pieces while minimizing the used length of the large object. According to Wäscher, Haußner, and Schumann (2007) this is an input minimization problem and, within this category of problems, it can be classified as 2D Open Dimension Problem. These problems derive from rather important applications in, among others, the textile, garment, furniture, footwear and metalwork industries, being economically very relevant.The scientific community has paid a lot of attention to irregular packing problems and rather complex and sophisticated heuristic methods have been developed and recently published. Good examples of fairly recent and high performance approaches are: 2DNEST (Egeblad, Nielsen, & Odgaard, 2007), ILS(QN) (Imamichi, Yagiura, & Nagamochi, 2009), FITS (Umetani et al., 2009), ELS (Leung, Lin, & Zhang, 2012) and SA-SMT (Sato, Martins, & Tsuzuki, 2012). They are all rather complex algorithms, based on metaheuristics and with more than one level of search, incorporating nonlinear programming models, constructive heuristics and local search methods.However, the best heuristic method known up to now was proposed by Elkeran (2013). It starts with a pairwise clustering algorithm, to group congruent polygons together in pairs, followed by a guided cuckoo search with two phases: the first one is responsible for minimizing the board length whilst the second one handles the overlap minimization problem (as in several previously presented methods). The cuckoo search itself is a populational metaheuristic incorporating a local random walk and a global explorative random walk using Lévy flights. With the cuckoo search in charge of moving a piece inside the board to a less overlapping position, a guided local search algorithm is used to escape local minima.The heuristic methods present an increasingly higher implementation complexity and, despite the increasingly better results obtained for commonly used benchmark instances, by nature they are not able to prove solution optimality or even to provide any measure of distance to the optimal solution. This theoretical drawback led to an increasing interest of researchers on exact methods, namely based on mathematical programming models, for irregular packing problems. To write mathematical programming models it is needed to write constraints ensuring that pieces do not overlap and are completely contained inside the board. Baldacci, Boschetti, Ganovelli, and Maniezzo (2014) use a raster approach to deal with the geometric constraints and discretizes both the large object and the pieces into a bit-matrix and imposes a cover constraint to each position of the matrix. The accuracy of this model depends on the resolution of the bit-matrix. Actually, Baldacci et al. do not deal with the strip packing problem but with an irregular multiple stock-size cutting stock problem, reason why the published results are not comparable with the other cited methods.In what concerns the strip packing problems, Toledo, Carravilla, Ribeiro, Oliveira, and Gomes (2013) proposed the Dotted-Board Model where the placement points on the board are restricted to a set of integer positions. Binary variables are assigned to the placement of piece types on a given point of the board and this model proved to be rather efficient being able to solve to optimality instances till 21 pieces of 7 different types and 56 pieces of just 2 different types. However, it should be noticed that these are optimal solutions in what concerns the grid used to discretize the board. Continuous placement models do not have this drawback but are able to optimally solve significantly smaller instances. The three published models that use continuous decision variables for the placement of the pieces resort to the concept of nofit polygon to write the non-overlapping constraints. The nofit polygon is a polygon derived from aggregating two component polygons that represents the pieces and that captures the geometric characteristics of both pieces (Bennell & Oliveira, 2008), transforming the analysis of the relative position of two pieces in the much simpler analysis of the relative position of one point and one polygon. From a mathematical programming point of view, the feasible placement region of a given piece is the set of points that are in the exterior or on the frontier of the nofit polygons of that piece and all other remaining pieces. Focusing the attention in just a pair of pieces, the difference of the three models is on how the exterior of the nofit polygon is described. Therefore Carravilla and Ribeiro (2005) and Gomes and Oliveira (2006) use a set covering constraint to describe the exterior of the nofit polygon and Fischetti and Luzzi (2009) and Alvarez-Valdes, Martinez, and Tamarit (2013) use a set partitioning constraint. They differ on how the partitions are created, being the horizontal slices approach of Alvarez-Valdes et al. (2013) the best continuous exact approach known up to date. With this model it was possible to solve to optimality instances to up to 16 pieces in 5 hours of computation time.The complexity of the mathematical programming models depends heavily on how the geometry layer of the problem is dealt with. In practice, all the models impose constraints to the pieces or board geometry, either by discretizing the pieces and/or the board, or by considering simplified piece geometries, as dealing only with convex pieces or not allowing holes in the pieces, pushing the models away from the real-world needs. Part of the models’ limitations come from the limitations of the algorithms available to build the nofit polygons (Bennell & Oliveira, 2008). If, for pieces represented by convex polygons, building the nofit polygon is just a matter of ordering the edge angles, when non-convex pieces are present both the sliding algorithms and Minkowski sums algorithms face hard, sometimes insurmountable obstacles like narrow entries, similar edge slopes and piece holes. This leads to time consuming, complex and numerically unstable algorithms.In this paper, to overcome these limitations and produce robust mathematical programming models, two directions are proposed. The first one does not use the nofit polygon but instead derives non-overlapping constraints based on direct trigonometry (Bennell & Oliveira, 2008). The second one decomposes the pieces into convex parts and then computes nofit polygons for the convex parts. The two approaches are robust both in terms of numeric stability and on the type of geometries they can address, considering any kind of non-convex piece with any type of holes inside them.The remainder of this paper is organized as follows. After this first introductory section, Section 2 presents the basic geometric definitions used along the paper. In Section 3 the two new mathematical programming models are presented, including some variants based on valid inequalities and variable reduction strategies, and in Section 4 computational experiments on benchmark instances commonly used in the literature are presented. The first computational experiments aim at comparing the new models and their variants and the second experiments aim at comparing the best new model against the best model published in the literature. In a second phase the computational experiments focus on new geometrically more complex instances, aiming to prove the flexibility and robustness of the proposed models.This section presents a formal definition of the irregular strip packing problem and the geometric definitions that will be used along this paper.The two-dimensional irregular strip packing problem can be formally defined by a set of pieces of m distinct types, each one described by a polygon Pi,i=1,…,m,that have to be placed, in an amount of diunits, on a large rectangle board characterized by a height H and a length L. For the sake of legibility and comparability the models will be presented and tested considering that piece rotations are not allowed, as in all previous exact approaches to this problem. However, an extension of the models allowing different orientations for each piece will be proposed and discussed in Section 3.3. As the length L is not fixed, the board can be considered as having “infinite” length and the problem’s goal is to minimize L, i.e. the length of the board that is necessary to cut all demanded pieces. In practice, this corresponds to minimizing the amount of raw-material used to satisfy a given order of irregularly shaped pieces.The problem constraints are of three types:1.each piece type i has to be cut in the demanded quantities di, in a total ofN=∑i=1mdipieces;pieces must not overlap, i.e.int(Pj)⋂int(Pl)=∅,∀j,l=1,…,N;j≠l;pieces must be completely contained inside the board, i.e.:int(Pj)⋂int(rect(L,H))=int(Pj),∀j=1,…,N;where int () stands for the topological interior and rect(L,H) for the rectangle of length L and height H.In the new models proposed in this paper each piece i is not described by a single polygon but it is decomposed and represented by a set of convex polygons, the parts of piece i, that may or may not overlap, allowing to represent convex and non-convex pieces, as well as pieces with holes.Each part of piece i is represented by a list of (clockwise) ordered points in the plane that represents its vertices. One of the vertices of one of the parts of piece i is chosen to be its reference point, denoted by (xi, yi).liminandlimaxare defined as the distances in the x-axis from xito the leftmost and rightmost vertex in piece i, respectively. Analogously,himinandhimaxare defined as the distance in the y-axis from yito the vertices of piece i that are closest and farthest from the origin, respectively. Fig. 1illustrates a piece decomposed in two convex polygons, its reference point and the corresponding measures forlimin,limax,himinandhimaxas well as the coordinate system and its origin.To ensure that pieces i and j do not overlap, each one of the parts of piece i must not overlap each one of the parts of piece j and vice-versa. Therefore, it is only necessary to detect if a part of a piece overlaps a part of another piece. As stated in Bennell and Oliveira (2008) two methods for the detection of the overlaps between two pieces are the D-function, that uses direct trigonometry and the nofit polygon.Given an oriented edgeab¯,wherea=(ax,ay)andb=(bx,by)and a pointr=(rx,ry),the D-function gives the relative position of the point with respect to the oriented edge and is defined by Eq. (1).(1)Dabr=(ax−bx)(ay−ry)−(ay−by)(ax−rx).Fig. 2shows the different values that Dabrcan have and its implications. IfDabr=0(Fig. 2a), point r is over the line defined byab¯; if Dabr< 0 (Fig. 2b), point r is on the right side of lineab¯; and if Dabr> 0 (Fig. 2c), point r is on the left side of lineab¯. This particular relationship between the sign of the D-function and the right-left position of point r is dependent on where the origin of the coordinate system is defined (Fig. 1). In our implementation, as other authors (in the literature), for historical reasons we consider that the origin of the coordinate system is on the left-top corner (as it happens with screens and pixel numbering), i.e. x-coordinates grow to the right and y-coordinates grow downwards. This has an impact on the interpretation of the result of the D-function, e.g. a negative value of the D-function means that the point is on the right side of the edge if and only if the top-left origin for the coordinate system is considered.The D-function is used to analyze the intersection of two polygons that represent two pieces by evaluating the relative position between the edges of one polygon and the vertices of the other one. If pieces i and j are convex, consider Kito be the set of edges of piece i. Given an edge e ∈ Ki, the D-function can be used to verify if the vertices of piece j are on the right side of e. If it is true for at least one edge e ∈ Ki, then piece i does not overlap piece j.The nofit polygon of two pieces i and j, denoted by NFPij, is the locus of all the points where the reference point of piece j cannot be placed without overlapping piece i. Since the pieces are defined as sets of convex parts, the NFPijcan be defined as the set of all nofit polygons of parts of i and parts of j, that is, each element of NFPijis the nofit polygon of p and q, where p ∈ Piand q ∈ Pj. Note that as all the parts of pieces i and j are convex polygons all elements of NFPijare also convex polygons and they might overlap.To build the nofit polygon of convex pieces, the Cuninghame-Green (1989) algorithm can be used as exemplified in Fig. 3. In the method, given two pieces i and j, the NFPijcan be built ordering the edges of the fixed polygon (that represented the piece i) on clockwise orientation and the orbital polygon (that represented the piece j) edges on counter-clockwise orientation (Fig. 3a). Then, the edges are translated in order to start at the same point (Fig. 3b). Finally, the edges are concatenated in an increasing angle order (Fig. 3c).By using the NFPijit is possible to verify if piece i does not overlap piece j just by checking if the reference point of j is on the border or outside the NFPij.A mathematical formulation for an irregular cutting problem must ensure that the pieces are entirely inside the board and they do not overlap. The first condition can be easily satisfied by using the pieces geometric information presented at Fig. 1. To guarantee that the pieces do not overlap two different approaches are proposed. The first one uses only the information provided by the geometry of the pieces leading to a Direct Trigonometry Model presented in Section 3.1. The second approach is based on the NFP covering structure presented in Section 2, resulting in the NFP Covering Model presented in Section 3.2.Some valid inequalities have been developed for each model and are described in the respective section. Section 3.4 presents the bounds used in the models and includes a short discussion about their importance.This section presents a mixed integer programming model to solve the irregular strip packing problem based only on direct trigonometry to infer the set of feasible solutions. Some variable reductions and valid inequalities are introduced in order to eliminate redundant constraints and part of the symmetric solutions.The first set of constraints must ensure that the pieces do not overlap. Consider two pieces i and j. Piece i does not overlap piece j if either all the vertices of i are at the right side of an oriented edge of piece j or the vertices of piece j are at the right side of an oriented edge of piece i. Note that two pieces composed by several parts do not overlap if all the pairs of parts of these pieces do not overlap. The D-function in Eq. (1) is to build these constraints.Consider that the pointsak=(axk,ayk)andbk=(bxk,byk)represent respectively the initial and final vertices of edge k ∈ Kip, where Kipis the set of edges of part p of piece i. Consider alsogxirqjandgyirqjto be the horizontal and vertical distances between (xi, yi), the positioning point of piece i, and the vertex r of part q of piece j. Using the D-function (1) we can write inequality (2).(2)Dabg=(axk−bxk)(ayk−gyirqj)−(ayk−byk)(axk−gxirqj)≤0.If this inequality is satisfied, the pieces i and j are either separated or touching. Moreover the distance between each vertex of part q of piece j and the reference point of piece j must be taken into account. Considergxjrqjandgyjrqjthe vertical and horizontal distances between the reference point of piece j (xj, yj) and vertex r of part q of piece j:gxjrqj=xr−xjandgyjrqj=yr−yj. Then,gxirqj=xj+gxjrqj−xiandgyirqj=yj+gyjrqj−yi. The constantsgxjrqj,gyjrqj,gxirqjandgyirqjare illustrated in Fig. 4. Using this information on inequality (2), inequality (3) is obtained.(3)(axk−bxk)(ayk+yi−yj−gyjrqj)−(ayk−byk)(axk+xi−xj−gxjrqj)≤0⇒Cijpqkr+(axk−bxk)(yi−yj)+(ayk−byk)(xi−xj)≤0,whereCijpqkris a constant defined by(axk−bxk)(ayk−gyjrqj)−(ayk−byk)(axk−gxjrqj).Note that it is not necessary to create constraints invoking that all the vertices of q (or p) are on the right side of one line of p (or q). Indeed, given a set of points and a specific line, constraint (3) associated with them differs only on the constantCijpqkr. By this fact, only the constraint with the largest constantCijpqkrneeds to be inserted in the model. This rule reduces substantially the number of constrains on the model, leading to a faster search. Then the following constraints prevent two parts from overlapping.(4)Cijpqk+(axk−bxk)(yi−yj)+(ayk−byk)(xi−xj)≤0,whereCijpqkis a constant defined bymaxr{Cijpqkr}.However, only one inequality needs to be satisfied for each part p to ensure that the pieces do not overlap. Consider the variablevijpqkthat is 1 if the inequality for edge k of part p of piece i is satisfied with respect to part q of piece j, and 0 otherwise. These line constraints (4) are formulated as follows:Cijpqk+(axk−bxk)(yi−yj)+(ayk−byk)(xi−xj)≤(1−vijpqk)Mijpqk,i=1,…,N,j=1,…,N,i≠j,p∈Pi,q∈Pj,k∈Kip,whereMijpqkis a number large enough to make the inequality always valid whenvijpqkis equal to zero. This number is estimated in Section 3.1.4.As it is not acceptable that the pieces overlap, a constraint to ensure that exactly one constraint related to a given pair of parts of different pieces is satisfied must be created. It is important to reinforce that it is guaranteed that if all the parts of different pieces do not overlap, the pieces do not overlap. The following constraint ensures that one inequality of edge k, part p of piece i or of part q of piece j is satisfied. Clearly, feasible solutions with more than one edge active for the same pair of parts can exist, but these solutions are not precluded by activating only one of these edges.∑k∈Kipvijpqk+∑k∈Kjqvjiqpk=1,1≤i<j≤N,p∈Pi,q∈Pj.The constraint that the pieces must be entirely contained inside the board, can be ensured by the innerfit rectangle represented by the following constraints.limin≤xi,i=1,…,N,himin≤yi≤H−himax,i=1,…,N.The next constraints are imposed to ensure that the used length L of the board will be minimized.xi≤L−limax,i=1,…,N.The Direct Trigonometry Model is presented in (5)–(11).(5)minL(6)s.t.limin≤xi≤L−limax,i=1,…,N,(7)himin≤yi≤H−himax,i=1,…,N,(8)Cijpqk+(axk−bxk)(yi−yj)+(ayk−byk)(xi−xj)≤(1−vijpqk)Mijpqk,i,j=1,…,N,i≠j,k∈Kip,p∈Pi,q∈Pj,(9)∑k∈Kipvijpqk+∑k∈Kjqvjiqpk=1,1≤i<j≤N,p∈Pi,q∈Pj,(10)(xi,yi)∈R2,i=1,…,N,(11)vijpqk∈{0,1},i,j=1,…,N,i≠j,k∈Kip,p∈Pi,q∈Pj.This model does not need special geometric structures, as the nofit polygons, to be built. This characteristic is interesting since by using these simpler structures it can be easier, compared to more complex models, to add new constraints to the model or to change the existing ones.It is possible to eliminate redundant constraints by finding the sets of points and lines that lead to the same solution space. Consider part p of piece i and part q of piece j as in Fig. 5. Notice that the same solutions can be reached either if the constraint related to line α is active or if the constraint related to line β is active. In other words, if part p of piece i and part q of piece j have lines with the same orientation and opposite directions the line β can be removed from Kjqset (or α can be removed from Kipset). Therefore, the binary variablevjiqpβ(orvijpqα) is eliminated reducing the number of elements in the sum of variables on constraint (9) and one non-overlap constraint (8) is eliminated. The elimination of these redundant constraints keeps the model’s optimality.Collinear lines of different parts of a piece can be represented by the same line in the model, since it leads to the same solution space. Consider a piece i that is composed by three parts, as illustrated in Fig. 6. As the linesα∈Kip1andβ∈Kip3are collinear, the variable β can be removed fromKip3set and α can be included in the set. These modifications change constraints (8) and (9) eliminating a variable from the model for each pair of collinear lines. Note that line γ needs to be maintained.Reducing the number of variables and constraints, the model size and complexity are also reduced. These reductions lead the solution method to perform a faster search over the solution space.In the proposed formulation items of the same type are considered as different items, implying that a huge number of symmetric solutions are computed. This can be avoided by imposing that xi≤ xjfor all i < j ∈ N if pieces i and j are of the same type. These constraints ensure that the pieces will respect a precedence order and significantly reduce the symmetry of the solution space.The value of the big-M in constraint (8) must be estimated. Ifvijpqk=1,the value ofMijpqkis not important since it will be multiplied by zero. However, ifvijpqk=0,then the following equation holds:Mijpqk≥Cijpqk+(axk−bxk)(yi−yj)+(ayk−byk)(xi−xj).The value ofMijpqkdepends on the placement positions of pieces i and j, the variables xi, xj, yiand yj. To eliminate the variables from the equation we take the board height H and an upper bound for the board lengthL¯as upper bounds for(yi−yj)and(xi−xj),respectively. The coefficients(axk−bxk)and(ayk−byk)are also replaced by their absolute value. By making these substitutions we obtain the following equation for big-M that guarantees thatMijpqkis large enough to make constraint (8) valid.Mijpqk≥Cijpqk+|axk−bxk|H+|ayk−byk|L¯.The upper bound on the board length (L¯) is defined as the sum of the lengths of all the pieces. This value is clearly too high compared to the best solution, leading to doubts about the importance of the estimation of the big-M term. However, two facts must be taken into account. The first is that with the estimation of the big-M, it is possible to apply the model to instances of any size, avoiding numerical errors. The second is that as we had verified in our experiments, a tight M in the formulation generally makes it hard to the solver to find good quality solutions at the beginning of the search. A further discussion about upper bounds is provided in Section 3.4.Not using advanced geometric structures in DTM also bring some drawbacks. The number of variables and constraints in the model is directly correlated with the number of vertices of each pair of convex polygons that represented the piece that compose the different pieces that must be placed. This conclusion led us to use advanced structures, as the nofit polygon, as described in the next section.The DTM is relatively easy to build and uses only basic geometric informations. However, for the cases where several pieces are decomposed in several convex parts, the model may become difficult to solve because of the number of constraints and variables.In this section, a model based on the nofit polygon is proposed. This approach has at most the same number of constraints and variables as the DTM. Some valid inequalities proposed for the DTM can also be used in this model. Moreover, a new set of valid inequalities can be introduced in the model, pruning symmetric solutions of the search space and leading therefore to a faster search.The main difference between the NFP Covering Model and the DTM is in how the non-overlapping constraints are tackled.Consider the nofit polygon NFPijgenerated from pieces i and j as defined in Section 2.3.2.NFPij=⋃p=1,...,QijNFPijp,whereNFPijpis the part p of NFPijand Qijis the number of parts of NFPij. To ensure that the pieces do not overlap, the reference point of piece j must be outsideNFPijp,for allp=1,...,Qij.Considering thatKijpis the set of directed edges ofNFPijp,these constraints are ensured by imposing, for each part of NFPij, that the reference point of piece j is at the right side of exactly one edge inKijp. In order to build these constraints, the D-function (Eq. (1)) is used. Consideraijp=(aij,xp,aij,yp)andbijp=(bij,xp,bij,yp)as two consecutive vertices of theNFPijp. Consider also xiand yithe variables which represent the point where piece i is placed on the board.(12)(aij,xp−bij,xp)(aij,yp−gij,y)−(aij,yp−bij,yp)(aij,xp−gij,x)≤0⇒C¯ijpk−(aij,xp−bij,xp)gij,y+(aij,yp−bij,yp)gij,x≤0,where gij, xand gij, yare respectively the horizontal and vertical distance between the reference points of pieces i and j, i.e.,gij,x=xi−xjandgij,y=yi−yj,andC¯ijpk=(aij,xp−bij,xp)aij,yp−(aij,yp−bij,yp)aij,xp.Inequality (12) ensures that the reference point of piece j is on the right side or over the line defined by the verticesaijpandbijpof theNFPijp. Since only one line must be activated to avoid the overlap between pieces, the following constraints are imposed.C¯ijpk−(aij,xp−bij,xp)(yj−yi)+(aij,yp−bij,yp)(xj−xi)≤(1−vijpk)Mijpk,i=1,…,N−1,j=i+1,...,N,p∈Qij,k∈Kijp,∑k∈Kijpvijpk=1,i=1,…,N−1,j=i+1,...,N,p∈Qij,where the variablevijpkis 1 if the reference point of piece j is on the right side or over the line k ofNFPijpand 0 otherwise. The same discussion presented in Section 3.1.4 can be applied to the estimation of theMijpkterm. These constraints are sufficient to ensure that the pieces do not overlap.To ensure that the pieces are entirely inside the board, constraints (6) and (7) of the DTM are imposed in this model as constraints (14) and (15).The complete NFP Covering Model formulation is presented in (13)–(20).(13)minL(14)s.t.limin≤xi≤L−limax,i=1,…,N,(15)himin≤yi≤H−himax,i=1,…,N,(16)C¯pkij−(aij,xp−bij,xp)(yi−yj)+(aij,yp−bij,yp)(xi−xj)≤(1−vijpk)Mijpk,i=1,…,N−1,j=i+1,…,N,p∈Qij,k∈Kijp,(17)∑k∈Kijpvijpk=1,i=1,…,N−1,(18)j=i+1,...,N,p∈Qij,(19)(xi,yi)∈R2,i=1,…,N,(20)vijpk∈{0,1},i,j=1,…,N,k∈Kijp,p∈Qij.It is clear that the symmetry breaking constraints presented in Section 3.1.3 are valid for this case, since they are related only with the feasible placement positions of the pieces reference points. Some other important inequalities are presented in the next section.As in the DTM model, some valid inequalities and variable eliminations can be performed in order to reduce the model size.Recalling that there is a binary variable is assigned to each edge of each convex component p of each nofit polygon NFPij, the first variable reduction comes from assigning the same binary variable to two collinear edges belonging to components p and q of the same NFPij. The situation is similar to the one presented in Fig. 6, considering now the nofit polygon parts instead of parts of the pieces.Some valid inequalities are also proposed in order to reduce the search space. These inequalities are driven by the geometric characteristics of the different NFP parts. Consider two parts, p and q, of a given NFP as shown in Fig. 7, consider also a line k defined by two consecutive vertices of polygon p and the line e defined by two consecutive vertices of polygon q. If these lines are parallel or if, for all feasible placements of polygons p and q on the board, these lines intersect outside the board, then valid inequalities of three different types can be derived based on them.The first set of valid inequalities comes from the observation that there are cases in which if a constraint, corresponding to the support line of a given edge, is active, then a constraint corresponding to the supporting line of an edge of another part of the NFP must also be active, otherwise the problem would be infeasible. We call the later a slave line as this happens whenever the domain of this second line constraint covers all the domain of the first line constraint. Fig. 7b illustrates a case where the constraint corresponding to line β is activated (slaved) whenever the constraint corresponding to line associated to θ is active. Therefore, the constraintvijpβ≥vijqθcan be included in the model.Two lines can be classified as covering lines (Fig. 7c) if, when merging the regions defined by the corresponding constraints, the entire board is covered. This means that for any feasible solution at least one of these constraints must be active. Therefore, in the case depicted in Fig. 7c the constraintvijpβ+vijqγ≥1must be added.The last set of valid inequalities refers to disjoint lines (Fig. 7d) and models the cases when two variables cannot be active at the same time. Specifically, if two lines of different NFP parts define a disjoint region, i.e. if the regions do not intersect, then their corresponding constraints cannot be activated at the same time (see α and θ in Fig. 7d). In this case, the constraintvijpα+vijqθ≤1must be added to the model.The procedure to find the valid inequalities is presented in Algorithm 1.As previously stated, these models suppose that piece’s orientation is fixed, i.e. no rotations are allowed. However, in many real-world applications pieces may have multiple orientations. In some applications any orientation is feasible, but considering this continuous rotation in the model would make it non-linear and out of the scope of our current research. In other applications a discrete and pre-defined set of orientations is possible for each piece. To consider this case each piece will be replicated as many times as the number of admissible orientations. Each one of the replicas will be treated in the models as a different piece, and therefore from now on i will stand for a piece in a given orientation. Variable δi∈ {0, 1} will stand for the placement or not of piece i on the layout. Let also consider the existence of S sets Γs, each one of them containing indices of pieces that are mutually exclusive, as it happens when they represent different orientations of the same initial piece (Fig. 8).To impose that only one piece of each set Γsis placed on the layout constraints (21) have to be added to the models.(21)∑i∈Γsδi=1,s=1,…,SAdditionally, the non-overlap constraints between two pieces in a particular orientation will only be activated if those orientations are the ones chosen by the model for those pieces. To achieve this, constraints (9) and (17) are replaced by constraints (22)–(24) and (25)–(27), respectively.(22)∑k∈Kipvijpqk+∑k∈Kjqvjiqpk≥δi+δj−1,1≤i<j≤N,p∈Pi,q∈Pj,(23)∑k∈Kipvijpqk+∑k∈Kjqvjiqpk≤δi,1≤i<j≤N,p∈Pi,q∈Pj,(24)∑k∈Kipvijpqk+∑k∈Kjqvjiqpk≤δj,1≤i<j≤N,p∈Pi,q∈Pj.(25)∑k∈Kijpvijpk≥δi+δj−1,i=1,…,N−1,j=i+1,…,N,p∈Qij,(26)∑k∈Kijpvijpk≤δi,i=1,…,N−1,j=i+1,...,N,p∈Qij,(27)∑k∈Kijpvijpk≤δj,i=1,…,N−1,j=i+1,...,N,p∈Qij.Note that the constraints (23) and (24) (or (26) and (27)) are not necessary to guarantee the feasibility of the solution, but they eliminate symmetries of the model, improving its performance. This modeling strategy does not only handle piece rotations but can also be used to address irregular packing problems in which the board is fully limited (both the length and the width are given). According to the typology of Wäscher et al. (2007) it is the case of the irregular Knapsack Problem and the irregular Placement Problem. In these cases constraints (21) are satisfied as equal or less than one instead of equal to one.Defining better bounds for the problem may help the task of proving optimality. This section describes how the lower and upper bounds used in the models were calculated.Lower bounds are defined with simple computations using the pieces and the board height. The length of a solution will always be greater than or equal to the length of the longest piece, and then this is assumed as a lower bound for the problem. Another lower bound that can be used is the length that would be used if the optimal solution had no waste of material to perform the cut. This value is obtained by dividing the total area of the pieces by the height of the board and is a simple generalization of the lower bound of Martello, Monaci, and Vigo (2003) for the two-dimensional rectangular strip packing problem. The maximum of these two lower bounds is used in the models aiming to give more information for the solution method and then leading to a faster convergence.Unlike the lower bounds, estimating upper bounds as tight as possible does not lead to a faster convergence of the solution method. This happens because tighter upper bounds lead the branch-and-cut method to spend more time finding feasible solutions. Despite this, it is not useless to estimate upper bounds, since good upper bounds make the process of defining the variable domains and the big-M estimation automated for any instance dimensionality (see Section 3.1.4). Moreover, for some instance sizes, huge upper bounds for the board length could lead to numerical instability. In our experiments, a big-M large enough to solve all instances, including a large instance as albano, produces numerical instability to solve small instances as three, fu5, shapes_4 For this reason, the used upper bound for the length,L¯,is instance dependent, it is the sum of all pieces’ length, i.e.L¯=∑i=1,…,N(limin+limax). This upper bound on the length avoided the numerical instability for all the instances used in our computational experiments.Computational experiments were run to evaluate the performance of both DTM andNFP−CM. Variations ofNFP−CMwere tested in order to show the effect of using the proposed valid cuts. We compared the results of our approach with the results presented in Alvarez-Valdes et al. (2013). The authors presented results for the Fischetti and Luzzi (2009) model (HS1) and a variation of it with lifted bounds (HS2). They also presented the results of an extension of Gomes and Oliveira (2006) compaction model (GO). In their article, the authors concluded that the HS2 model outperforms the HS1 and GO model and then only the results of HS2 will be compared with our best model. In addition, we present the results of our method for larger instances from the literature and new instances based on real world applications.Three sets of instances were used in our experiments. The first set is the same used in Alvarez-Valdes et al. (2013) and was used to compare the performance of the proposed models and to compare the results of our best model against the HS2 model presented in Alvarez-Valdes et al. (2013). Classical instances of strip packing problems compose the second instance set. This instance set shows the effectiveness of the models for larger instances. Finally, the last instance set is composed by new instances based on a real world application, where small pieces can be positioned inside holes of larger pieces.To build the proposed models, given the height of the board and the vertices of the pieces, we have a pre-processing phase. In this phase, the pieces were divided in the minimum number of convex parts by the Greene’s partitioning algorithm proposed by Greene (1983) and implemented in Cgal (2015). In order to build the nofit polygons, the ordering edges algorithm presented in Cuninghame-Green (1989) was used. Unlike other approaches, which also decompose non-convex pieces into convex components to generate the respective (convex) nofit polygons, these convex parts are not merged in a single nofit polygon but directly used in the model.The computational experiments were performed on a Intel Core i7-2600 with 16 gigabytes of memory using Ubuntu 12.04 operating system. To solve the proposed models we use the CPLEX 12.6 optimization tool with C/C++ programming language and default settings. All instances were run until optimality was proven or a time limit (denoted tl, defined as 3600 seconds) was reached. This computational environment and time limit are similar to the ones used in Alvarez-Valdes et al. (2013).In this section the performances of the two models proposed in this paper, DTM andNFP−CM,are analyzed and compared. ForNFP−CMwe show the importance of the valid cuts by comparing the solutions obtained with the valid cuts (NFP−CM) and without them (NFP−CMnc). Only the set of instances present in Alvarez-Valdes et al. (2013) was used in this phase.Table 1shows the results obtained by DTM andNFP−CMfor this set of instances. In columns ub and lb we present the upper and lower bounds reported by CPLEX on termination. The solution gap, computed according to the formula(UB−LB)UBis given in column gap. Column time gives the computational time, in seconds, reported by CPLEX.To have a better overview of the models comparison, we build a performance profile graph (Dolan & Moré, 2002), using the computational time as a performance measure. In this graph, each model is represented by a curve. A point (x, y) in a curve of a model m means that the computational time model m took to solve (100 × y) percent of the instances is at most x times the computational time the fastest model took to solve them. To build the graphs in Fig. 9we used all the 35 instances, but, if a model could not find an optimal solution (i.e. gap greater than 0), we considered that the time it spend on solving the instance was “infinite”. To build the graphs in Fig. 10we used only the 20 instances for which the models could compute the optimal solution. In Figs. 10 and 9, the graph at the right is a zoom of the left portion of the graph at the left.In Table 1 and Fig. 9, when we compare the computational time spent by the models to solve all 35 instances we can see that both DTM andNFP−CMncare the fastest models for 26 percent of the instances, whereasNFP−CMis the fastest model for 57 percent of the instances. Besides that, DTM was able to solve to optimality 20 instances (57 percent),NFP−CMncsolved 25 instances (71 percent) andNFP−CMsolved 27 instances (77 percent). When we compare the computational time spent to solve only the 20 instances for which all models computed optimal solutions (see Fig. 10), both DTM andNFP−CMncare the fastest models for 45 percent of the instances, whereasNFP−CMis the fastest model for 65 percent of the instances.In Table 1 we can also see that, with the exception of instances threep3, poly1a0 and dighe1, in all the other five instances for whichNFP−CMcould not find an optimal solution (J1-10-20-3, fu, J1-14-20-1, J1-14-20-2 and J1-14-20-4), it obtained either an equal or a smaller gap than the other two models.Therefore,NFP−CMhas a better performance than the other two models, at least for this set of instances. We can also see that the performance ofNFP−CMncis better than the performance of DTM. This behavior was expected since in DTM only the geometric information about the pieces parts is used to build the model while the other proposed models use the information about the nofit polygon. Moreover, the performance ofNFP−CMis better than the performance ofNFP−CMncbecause the latter does not have inequalities associating the parts of each nofit polygon used to build the model. As expected, the amount of information used to build the model is related with its performance. However, in situations where there are no geometric tools to generate the nofit polygon or the nofit polygon generator cannot handle all the pieces characteristics, the DTM can be attractive to solve the problem. Furthermore, it is easier to include additional constraints in a model which depends on less information to be built, this fact also makes DTM attractive.SinceNFP−CMperformed better, we will only use it on the remaining sections to make performance comparisons.To assess the performance ofNFP−CMwe will compare it with the HS2 model which is the best model presented in Alvarez-Valdes et al. (2013). To do this, we use only the instances reported in Alvarez-Valdes et al. (2013).Table 2shows the results obtained byNFP−CMand the results of HS2 reported in Alvarez-Valdes et al. (2013). In columns lb and gap we have the lower bound and solution gap (given by(UB−LB)UB) for each solution computed. Column time gives the computational time, in seconds, reported by CPLEX. It is important to notice that, although the computer environment used in Alvarez-Valdes et al. (2013) is similar to the one we used, they are not the same (the main difference being the CPLEX version – in Alvarez-Valdes et al. (2013) version 12.1 was used). Therefore, we must be cautious when comparing the models performances.As can be seen in Table 2, the HS2 andNFP−CMapproaches obtained almost the same number of optimal solutions within the allowed time limit (25 and 27, respectively). In general, for instances with up to nine pieces the results obtained with both models are very similar, except for threep3 and threep3w9, instances where either HS2 orNFP−CMwere better. However,NFP−CMis significantly faster than HS2 for instances with 10 or more pieces, even considering the difference between the CPLEX version. According to IBM,11http://www-01.ibm.com/software/commerce/optimization/cplex-performance – Accessed 12.12.14.CPLEX 12.6 (used to runNFP−CM) is on average 40 percent faster than CPLEX 12.1 (used to run HS2). For these instances, HS2 presented better results only for the instance J1-10-20-3. Furthermore, even for the instances in which the time limit was reached, the gaps were smaller forNFP−CM. The optimal solutions obtained byNFP−CMthat were not obtained by HS2 are displayed in Fig. 11.This section presents a performance analysis ofNFP−CMapplied to solve larger instances. Two instance sets were used to perform these experiments: larger instances from the literature and new real world based instances. These experiments are detailed in Sections 4.3.1 and 4.3.2.To assess the performance ofNFP−CMwhen solving larger instances, we first use 10 classical instances from the literature with 16 or more pieces. The results can be seen on Table 3.From Tables 1 and 3 we can see that, given a time limit of 3600 seconds,NFP−CMcan find an optimal solution for most of the instances with up to 12 pieces (except instances threep3, J1-10-20-3 and fu). When the number of pieces if greater than 12,NFP−CMcan find a feasible solution for all instances with up to 30 pieces. For instances with more than 30 pieces,NFP−CMcannot find a feasible solution. This indicates that, when the instance is small (at most 12 pieces),NFP−CMhas a good chance of solving it to optimality. On the other hand, if the instance is big (more than 30 pieces), it is probableNFP−CMwill not be able to even find a feasible solution.SinceNFP−CM(as well asNFP−CMncand DTM) can deal with pieces with holes without any special structure needed to build the model, we propose some instances based on a metal cutting layout from the industry. These instances are characterized by large pieces with holes where some small pieces from the instances can be placed. The real pieces were approximated by polygonal forms as presented in Fig. 12. The instances generated with these pieces were named metal.Four instance sets were designed using these pieces, called metal0, metal1, metal2 and metal3. Details about these instances are given in the Appendix A.Table 4shows the results obtained byNFP−CMfor the metal sets of instances. Fig. 13shows the pictures of the optimal solutions obtained byNFP−CM.As we can see in Table 4,NFP−CMwas able to solve to optimality all metal instances with up to 10 pieces. For instances with a number of pieces between 10 and 45,NFP−CMwas always able to find a feasible solution. For instances with more than 45 pieces, it could not find a feasible solution. This is consistent with the results reported in Sections 4.1 and 4.3.1.When considering piece rotations it is not possible to make comparisons with previously published exact methods because none of them allows piece rotations, i.e. that pieces may be placed with an orientation among a set of given feasible orientations. However, in order to provide grounds for future research in the field, experiments were run with the models “NFP−CMwith rotations” and “DTM with rotations” using a set of smaller instances allowing two different orientations for each piece: the original (rotation of 0 degrees) and the orientation corresponding to a rotation of 180 degrees. It should be noticed that allowing piece rotations significantly increases the number of variables and the size of the model, and therefore only small instances can be tackled.As we can see in Table 5, as expected, the solutions are always equal or better than the ones when no rotations are allowed, but the time needed to prove solution optimality increases quickly and quite soon it is not possible to prove it.

@&#CONCLUSIONS@&#
In this paper two new mathematical programming models for the two-dimensional irregular packing problem were presented. These models aim to overcome the limitations of previous models in what concerns the geometry of the pieces they are able to deal with. The first model (DTM) states the piece non-overlapping constraints using direct trigonometry, while the second model (NFP−CM) firstly decomposes the pieces into convex parts and then states the non-overlapping constraints using the nofit polygons between these convex parts, as a covering of the actual non-convex nofit polygons. For theNFP−CMvalid inequalities were developed.The computational experiments were divided in three phases: first the new models were tested over a set of 35 commonly used benchmark instances; secondly the models were compared against the best model known in the literature, HS2 by Alvarez-Valdes et al. (2013), over the same classical set of instances; and finally the robustness of the new models was proven by running experiments over a set of new real-world based set of instances, incorporating geometric characteristics not dealt yet by previous models. From the first phase of experiments resulted the supremacy of the modelNFP−CMthat, within the time limit of one hour, was able to solve until optimality 77 percent of the instances and, for the instances in which optimal solutions were achieved by more than one model or variant, it was the fastest model for 57 percent of the instances. For the 8 instances in which optimality was not proven,NFP−CMwas able to generate a feasible solution and, in 5 of those instances, with a gap smaller than the other models. In the second phase, when comparing the newNFP−CMmodel against HS2,NFP−CMsolves more problems until optimality (27 against 25), clearly faster for instances with 10 or more pieces (around 13 times faster, already taking into account the different versions of CPLEX used) and, when optimality is not guaranteed, with smaller gaps. In the third phase of experiments it was possible to successfully solve new instances where the pieces have holes (the only geometric characteristic not addressed in the previous experiments), solving until optimality instances until 10 pieces and generating feasible solutions for instances with up to 45 pieces. The gaps are high, but it is well-known that irregular strip packing problems have very poor lower bounds.The new models presented can effectively address irregular strip packing problems with any kind of geometry that can be described by a polygon, without any approximation or simplification, being therefore robust in what concerns piece geometry. Equally important is the fact that they resort to much simpler geometric tools than previous models, easier to implement and without numerical stability problems. The downside of mathematical programming model based approaches is still the size of the instances that are possible to solve until optimality and, given the poor quality of the available lower bounds, feasible solutions are not better than feasible solutions generated by sophisticated (meta)heuristics in the same amount of time. Lower bounds for irregular strip packing problems are clearly a very difficult but relevant topic of future research.In many real-world applications pieces may have a pre-defined set of orientations. This possibility was considered as an extension of the proposed models and required the replication of each piece as many times as the number of admissible orientations and an additional binary variable for each of these replicas. The size of the instances that were possible to solve when considering the possibility was fairly small and efficiently tackling rotations in exact approaches is a challenge for future research.