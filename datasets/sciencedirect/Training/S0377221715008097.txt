@&#MAIN-TITLE@&#
Two Bayesian approaches to rough sets

@&#HIGHLIGHTS@&#
We propose a unified framework to review, classify and examine fundamental issues of Bayesian approaches to rough sets.We identify two classes of Bayesian approaches to probabilistic rough sets and three fundamental issues.We explore a theory of three-way decisions as a tool for building ternary classifiers.We weave existing results into a coherent study of Bayesian approaches to rough sets.

@&#KEYPHRASES@&#
Bayesian classification rough sets,Bayesian confirmation rough sets,Confirmation-theoretic rough sets,Decision-theoretic rough sets,Probabilistic rough sets,

@&#ABSTRACT@&#
Bayesian inference and probabilistic rough sets (PRSs) provide two methods for data analysis. Both of them use probabilities to express uncertainties and knowledge in data and to make inference about data. Many proposals have been made to combine Bayesian inference and rough sets. The main objective of this paper is to present a unified framework that enables us (a) to review and classify Bayesian approaches to rough sets, (b) to give proper perspectives of existing studies, and (c) to examine basic ingredients and fundamental issues of Bayesian approaches to rough sets. By reviewing existing studies, we identify two classes of Bayesian approaches to PRSsand three fundamental issues. One class is interpreted as Bayesian classification rough sets, which is built from decision-theoretic rough set (DTRS) models proposed by Yao, Wong and Lingras. The other class is interpreted as Bayesian confirmation rough sets, which is built from parameterized rough set models proposed by Greco, Matarazzo and Słowiński. Although the two classes share many similarities in terms of making use of Bayes’ theorem and a pair of thresholds to produce three regions, their semantic interpretations and, hence, intended applications are different. The three fundamental issues are the computation and interpretation of thresholds, the estimation of required conditional probabilities, and the application of derived three regions. DTRS models provide an interpretation and a method for computing a pair of thresholds according to Bayesian decision theory. Naive Bayesian rough set models give a practical technique for estimating probability based on Bayes’ theorem and inference. Finally, a theory of three-way decisions offers a tool for building ternary classifiers. The main contribution of the paper lies in weaving together existing results into a coherent study of Bayesian approaches to rough sets, rather than introducing new specific results.

@&#INTRODUCTION@&#
In Bayesian data analysis, a prior probability is used to capture our belief about an event or a hypothesis before observing the evidence or data, and the Bayes’ theorem is used to update the prior probability into a posterior probability through a likelihood when evidence becomes available. Bayesian methods have been widely applied in many fields for decision making and classification under uncertainty (Liu, Hua, & Lim, 2015). Pawlak rough set theory provides another approach to data analysis (Pawlak, 1982, 1991). Rough set analysis identifies decision rules and dependencies from data for decision making and classification. In generalized probabilistic approaches to rough sets, rules are typically studied and characterized in probabilistic terms (Grzymala-Busse, Marepally, & Yao, 2010; Pawlak, 1999; Tsumoto, 2002; Yao, 2003, 2008). There are close connections between Bayesian inference and rough set theory. A reviewer of this paper concisely summarized, “The relationship of rough sets with probability theory has been a matter of debate ever since the rough sets were first proposed in 1980’s. Over last three decades a number of researchers have made significant contributions to the study of relationship between the two theories. These contributions have not only helped us understand the rough sets better, but they have also provided useful extension of the rough set theory and in some cases created new ways of reasoning that combine concepts from rough sets and probability theory.”The first probabilistic rough set (PRS) model, called the 0.5-probabilistic rough set model (Yao, 2007a), was proposed by Wong and Ziarko (1985) and Pawlak, Wong, and Ziarko (1988). A threshold of 0.5 on probability is used to define probabilistic lower and upper approximations, or equivalently three probabilistic regions, of a set. The threshold 0.5 can be intuitively interpreted based on the notion of the majority rule. Wong and Ziarko (1986b) late generalized the model by using a pair of thresholds (α, 0.5). Based on Bayesian decision theory, Yao (2007a), Yao and Wong (1992), and Yao, Wong, Lingras, Ras, Zemankova, and Emrich (1990) proposed a generalized probabilistic model, called a decision-theoretic rough set (DTRS) model, by considering a pair of thresholds (α, β) on probabilities for defining probabilistic approximations. The pair of thresholds can be systematically calculated based on the well established Bayesian decision theory, and interpreted in terms of more practically operable notions such as cost, risk, benefit etc. Herbert and Yao (2008) integrated game theory and DTRS model to introduce a new PRS model known as a game-theoretic rough set (GTRS) model.Based on the notion of graded set inclusion, Ziarko (1993) introduced a variable precision rough set (VPRS) model by using a pair of thresholds on a set-inclusion function. The derived approximations are equivalent to a special case of the DTRS model. The main results of the model were later explicitly re-expressed in terms of thresholds on probability instead of a set-inclusion function (Ziarko, 2002).Ślȩzak and Ziarko (2002,2005) introduced a Bayesian rough set (BRS) model. A prior probability is used as a threshold for defining three regions. They also suggested to compare two likelihoods directly when neither posterior probability nor prior probability is derivable from data. Ślȩzak (2005) further drew a natural correspondence between the fundamental notions of rough sets and statistics. The set to be approximated corresponds to a hypothesis and an equivalence class to a piece of evidence; the three probabilistic regions correspond to the cases that the hypothesis is verified positively, negatively, or undecided based on the evidence. Based on such a correspondence, Ślȩzak introduced a rough Bayesian model (Ślȩzak, 2005), in which probabilistic approximations are defined based on a pair of thresholds on the ratio of the prior and posterior probabilities.Greco, Pawlak, and Słowiński (2004a,2004b) first introduced Bayesian confirmation measures into rough set theory to study decision rules and decision algorithms. Greco, Matarazzo, and Słowiński (2005;2008) argued that it may be insufficient to consider only probability values when formulating a PRS model. As a result, they introduced a parameterized rough set model by considering a pair of thresholds on a Bayesian confirmation measure, in addition to a pair of thresholds on probability. They presented and analyzed systematically Bayesian confirmation measures in constructing PRSs. Moreover, they explicitly showed that Pawlak rough sets, VPRSs (Ziarko, 1993), BRSs (Ślȩzak & Ziarko, 2002, 2005) and rough Bayesian sets (Ślȩzak, 2005) are special cases of their parameterized model. A problem to be solved is how to systematically determine a pair of thresholds on a Bayesian confirmation measure.An important problem of PRSs is the estimation of the conditional probability. The rough membership function (Pawlak, Skowron, Yager, Fedrizzi, & Kacprzyk, 1994) is a simple way to do it, but of limited value due to the requirement of a large-sized sample. Kotlowski, Dembczynski, Greco, and Słowiński (2008) and Kotlowski and Słowiński (2013) suggested a statistical model in which probabilities are estimated based on the maximization of a likelihood function. Yao and Zhou (2010) introduced a naive Bayesian rough set (NBRS) model by slightly modifying results from the standard naive Bayesian model for classification. An equivalence class is described by a vector of attribute values. Through an application of Bayes’ theorem, the estimation of the posterior probability is turned into the estimation of the likelihood based on the naive probabilistic independence assumption of attributes.Motivated by rough set classification with three regions, Yao (2009,2010,2011) introduced the notion of three-way decisions. Specifically, similar to the concepts of accepting a hypothesis, rejecting a hypothesis, or further testing in statistical testing (Wald, 1945), the three regions can induce positive rules for accepting an object to be an instance of a concept, negative rules for rejecting, and boundary rules for deferring a definite decision.In a series of papers, Pawlak (Greco et al., 2004b; Pawlak, 1999; 2002) advocated another research direction in studying connections between Bayesian methods and rough set approaches. He re-interpreted some of the results of Bayesian data analysis in the context of rough sets. Every decision rule is associated with two conditional probabilities, called accuracy and coverage (Tsumoto, 2002). While the accuracy corresponds to posterior probability, the coverage corresponds to the likelihood in Bayesian methods. In other words, Pawlak used Bayes’ theorem to explain the probabilistic relationship between conditions and decisions in decision rules.Each of these studies focuses on a specific perspective on PRSs. They are complementary to each other and, working together, they provide the main ingredients of a general framework for studying Bayesian probabilistic approaches to rough sets. Although such a framework emerges from the vast amount of studies, the specifics of the framework have not been fully examined, discussed and analyzed. The main objective of this paper is therefore to present one such general framework.There are two parts of our framework. The first part is a classification of existing PRSs into two categories, namely, Bayesian classification rough sets and Bayesian confirmation rough sets, as shown in Fig. 1. There are connections and differences between the parameterized model (Greco et al., 2005, 2008) and our framework. While the parameterize model is a single model with two components, we explicitly divide them into two classes of models, with each class intended for a different type of applications. The second part is the identification of three fundamental issues: (a) determination of thresholds, (b) estimation of conditional probability, and (c) application of three regions. The framework enables us to have a comprehensive understanding of the evolution of PRSs, perspectives of different PRS models, differences between these models and their intended applications.It should be pointed out that specific results in this paper are not entirely new and have been examined and discussed in many other papers. The main contribution of the paper is to integrate these results coherently into a complete whole within a common framework and to present them, first time, in a single paper. To achieve our goal, the rest of the paper is organized as follows. Section 2 briefly summarizes the main results from existing studies of Bayesian approaches to rough sets and identifies three fundamental issues. We give reasons for our preference to a categorization consisting of two classes of models, namely, Bayesian classification rough sets and Bayesian confirmation rough sets. Following the formulation of parameterized rough sets given by Greco et al. (2005;2008), Section 3 presents a formulation of Bayesian confirmation rough sets, or confirmation-theoretic rough sets, and discusses their differences from DTRS models. Section 4 presents a different type of formulation of Bayesian classification rough sets based on the result of the DTRS models. This section focuses on the issue of interpreting and determining the required thresholds based on Bayesian decision theory. Section 5 examines the problem of estimating probabilities based on Bayes’ theorem. After discussing the general form of a BRS model, an NBRS model is presented. A special case of the NBRS model, called a binary probabilistic independence rough set (BPIRS) model, is derived. Section 6 interprets the three regions of Bayesian classification models based on the notion of three-way decisions. We demonstrate applications of Bayesian classification rough sets for building ternary classifiers. Section 7 provides concluding remarks.In this section, we briefly summarize fundamental results of Pawlak rough sets and PRSs. We identify two basic classes of models, namely, Bayesian classification rough sets and Bayesian confirmation rough sets. We also point out three basic issues in studying Bayesian approaches to rough sets. To emphasize the semantic interpretation of PRSs with three-way decisions (Yao, 2009; 2010), our formulation directly uses three pair-wise disjoint positive, boundary, and negative regions, instead of a pair of lower and upper approximations.Bayes’ theorem plays a crucial role in developing Bayesian approaches to rough sets. Let H denote a hypothesis and E a piece of evidence. The Bayes’ theorem is expressed asPr(H|E)=Pr(E|H)Pr(E)Pr(H),where Pr(H) is the prior probability of H, Pr(H|E) is the posterior probability after observing E, and Pr(E|H) is the likelihood. Bayes’ theorem provides a way to update probability based on evidence. Through Bayes’ theorem, a difficult to estimate probability Pr(H|E) is expressed in terms of an easy to estimate likelihood Pr(E|H). This makes Bayes’ theorem particularly useful in data analysis and pattern classification.Let R ⊆ U × U be an equivalence relation on a finite universe of objects U, namely, R is reflexive, symmetric, and transitive. The pairapr=(U,R)is called an approximation space. The equivalence relation R induces a partition of U, denoted by U/R. The basic building blocks in constructing rough set theory are the equivalence classes of R. For an object x ∈ U, the equivalence class containing x is given by[x]R=[x]={y∈U∣xRy}. We omit the subscript R when the equivalence relation R is understood.In the classical Pawlak rough set theory (Pawlak, 1982, 1991), approximations of a set are defined based on qualitative set-inclusion and overlapping relationships between an equivalence class and the set. For a subset C ⊆ U representing a concept, its rough set approximations are given by three pair-wise disjoint regions, the positive region POS(C), the boundary region BND(C), and the negative region NEG(C):(1)POS(C)={x∈U∣[x]⊆C},BND(C)={x∈U∣[x]∩C≠∅,[x]¬⊆C},NEG(C)={x∈U∣[x]∩C=∅}.Semantically, the three regions may be interpreted as follows (Pawlak, 1991; Yao, 2007b). The equivalence relation R is practically defined by a set of features or attributes: two objects are equivalent if and only if they have the same features or the same attribute values, namely, they are described by the same description. Rough set theory thus provides a way to classify objects into three regions based on their descriptions. That is, rough set theory leads to a ternary classifier (Yao, 2010), which is complementary to widely used binary classifiers. For this reason, we prefer the formulation by three regions to the formulation by a pair of lower and upper approximations, where the lower approximation is the positive region and the upper approximation is the union of positive and boundary regions.Bayesian classification rough sets generalize the results from DTRS model proposed by Yao (2007a), Yao and Wong (1992), and Yao et al. (1990). We consider the degree of overlap between an equivalence class [x] and a set C. Specifically, a conditional probability is used to measure the degree of overlap and a pair of threshold values α and β with 0 ≤ β < α ≤ 1 on the probability is used to define three probabilistic regions.Let Pr(C|[x]) denote the probability of an object belonging to C given that the object is in [x]. A fundamental result of Bayesian classification rough sets is (α, β)-probabilistic positive, boundary and negative regions defined by Yao (2007a), Yao and Wong (1992), and Yao et al. (1990):(2)POS(α,β)(C)={x∈U∣Pr(C|[x])≥α},BND(α,β)(C)={x∈U∣β<Pr(C|[x])<α},NEG(α,β)(C)={x∈U∣Pr(C|[x])≤β}.The pair of thresholds can be systematically computed from loss or cost functions in making three-way classification decisions. For the caseβ=α,the three regions are defined slightly differently (Yao, 2007a; Yao & Wong, 1992; Yao et al., 1990). For simplicity, we only consider the case when β < α.Unlike the qualitative Pawlak approximations, probabilistic approximations introduce certain levels of error in both the positive and negative regions. More precisely, Pawlak regions and (α, β)-probabilistic regions are linked together by:(3)POS(C)⊆POS(α,β)(C),BND(α,β)(C)⊆BND(C),NEG(C)⊆NEG(α,β)(C).One obtains larger positive and negative regions by introducing classification errors in trade of a smaller boundary region so that the total classification cost is minimum (Yao, 2011). Considering the errors introduced, the three regions are semantically interpreted as the following three-way decisions (Yao, 2009, 2010, 2011). We accept an object x to be a member of C if the conditional probability is greater than α, with an understanding that it comes with a below(1−α)-level acceptance error and associated cost. We reject x to be a member of C if the conditional probability is less than β, with an understanding that it comes with a below β-level rejection error and associated cost. We neither accept or reject x to be a member of C if the conditional probability is between α and β; instead, we make a decision of deferment. The boundary region does not involve acceptance and rejection errors, but it is associated with cost of deferment. The three probabilistic regions are obtained by considering a trade-off between various classification costs.Greco et al. (2005,2008) first applied Bayesian confirmation measures in their parameterized rough sets, which will be discussed in the next subsection. By taking only the Bayesian confirmation component of the parameterized model, one arrives at Bayesian confirmation rough sets or confirmation-theoretic rough sets (Zhou & Yao, 2013). That is, Bayesian confirmation rough sets are a special case of parameterized rough sets. In this case, a Bayesian confirmation measure is used to quantify the relationship between an equivalence class and a set. A pair of thresholds on the Bayesian confirmation measure is used to define three pair-wise disjoint region.Let c( ·, ·) denote a Bayesian confirmation measure (see detailed discussions in Section 3). The value c([x], C) is the degree to which the evidence y ∈ [x] confirms the hypothesis y ∈ C, where y ∈ U. Given a Bayesian confirmation measure c( ·, ·) and a pair of thresholds (s, t), with t < s, on the confirmation measure, three (s, t)-confirmation regions are defined by:(4)POSC(s,t)(C)={x∈U∣c([x],C)≥s},BNDC(s,t)(C)={x∈U∣t<c([x],C)<s},NEGC(s,t)(C)={x∈U∣c([x],C)≤t}.They are pair-wise disjoint.Typically, a Bayesian confirmation measure involves the posterior probability Pr(C|[x]) and the prior probabilityPr(C). For example, given the following Bayesian confirmation measure,cd([x],C)=Pr(C|[x])−Pr(C),and a pair of threshold (s, t) with 0 ≤ t < s ≤ 1, three (s, t)-confirmation regions can be defined:(5)POSC(s,t)(C)={x∈U∣Pr(C|[x])−Pr(C)≥s},BNDC(s,t)(C)={x∈U∣t<Pr(C|[x])−Pr(C)<s},NEGC(s,t)(C)={x∈U∣Pr(C|[x])−Pr(C)≤t}.Since the posterior probability Pr(C|[x]) is not a Bayesian confirmation measure, Bayesian classification rough sets are different from Bayesian confirmation rough sets. In addition, while the class of Bayesian classification rough set models is uniquely defined by the posterior probability Pr(C|[x]), there are many different classes of Bayesian confirmation rough set models defined by different Bayesian confirmation measures.Greco et al. (2005,2008) proposed a parameterized rough set model that has a Bayesian classification component and a Bayesian confirmation component. We review their results in terms of three regions.Suppose c( ·, ·) is a Bayesian confirmation measure. For two pairs of thresholds, (α, β) with 0 ≤ β < α ≤ 1 on the posterior probability and (s, t) with t < s on the Bayesian confirmation measure c( ·, ·), three ((α, β), (s, t))-parameterized regions are defined as:(6)PPOS(α,β,s,t)(C)={x∈U∣Pr(C|[x])≥α∧c([x],C)≥s}=POS(α,β)(C)∩POSC(s,t)(C),PBND(α,β,s,t)(C)={x∈U∣(Pr(C|[x])>β∨c([x],C)>t)∧(Pr(C|[x])<α∨c([x],C)<s)}=(POS(α,β)(C)∩NEGC(s,t)(C))∪(BND(α,β)(C)∪BNDC(s,t)(C))∪(NEG(α,β)(C)∩POSC(s,t)(C)),PNEG(α,β,s,t)(C)={x∈U∣Pr(C|[x])≤β∧c([x],C)≤t}=NEG(α,β)(C)∩NEGC(s,t)(C).That is, both positive and negative regions are the common parts of the corresponding (α, β)-probabilistic approximations and (s, t)-Bayesian confirmation, and the boundary regions is the union of the two boundary regions and the disagreement parts of (α, β)-probabilistic approximations and (s, t)-Bayesian confirmation.Parameterized rough sets combine Bayesian classification rough sets and Bayesian confirmation rough sets (Greco et al., 2005, 2008). For a bounded Bayesian confirmation measure, by removing the requirement t < s and setting(7)s=min{c([x],C)∣x∈U},t=max{c([x],C)∣x∈U},one immediately obtains Bayesian classification rough sets. On the other hand, by removing the requirement β < α and settingα=0andβ=1,one obtains Bayesian confirmation rough sets. Greco et al. (2005,2008) gave conditions under which Pawlak rough set model, VPRS model (Ziarko, 1993), and rough Bayesian model (Ślȩzak, 2005) can be obtained, respectively.In the previous subsections, we have classified the main results of Bayesian approaches to rough sets into three classes of PRSs. One can observe features that are common to the three classes, namely, the use of a pair or two pairs of thresholds, the use of posterior probability Pr(C|[x]), and the division of the universe into three pair-wise disjoint regions. This immediately suggests the following three basic issues in a study of Bayesian approaches to rough sets:1.interpretation and computation of thresholds, namely, (α, β) for a classification model, (s, t) for a confirmation model, and both pairs for a parameterized model;estimation of conditional probability Pr(C|[x]);interpretation and applications of three regions in data analysis.A careful examination of the three issues would establish a basis for understanding various existing PRS models and for developing new models. The rest of this paper focuses on these issues by restricting our investigation to two different Bayesian approaches to rough sets, namely, Bayesian classification rough sets and Bayesian confirmation rough sets. Reasons for having such a limited scope are explained as follows.Bayesian classification and Bayesian confirmation represent two important types of Bayesian inference for data analysis. Both of them make use of Bayes’ theorem and both employ the same technique of using a pair of thresholds to produce three regions. However, their semantic interpretations and, hence, intended applications are very different. Bayesian classification rough sets are motivated by Bayesian inference for drawing conclusions based on available evidence. They deal with the approximation of a set by three probabilistic regions. In contrast, Bayesian confirmation rough sets are motivated by Bayesian weighting of evidence. Equivalence classes are treated as pieces of evidence that are put into three confirmation regions. As shown in Fig. 1, Bayesian classification rough sets are suitable for classifying objects, and Bayesian confirmation model are suitable for feature selection (Zhou & Yao, 2013). The separation of Bayesian approaches into two basic class seems to be reasonable. In addition, as also shown in Fig. 1, the two basic classes enable us to capture most existing PRS models.Our preference to two separate classes of Bayesian approaches to rough sets is due to an emphasis on the differences of two types of Bayesian inferences, as well as simplicity. In each model, we only have a pair of parameters. On the other hand, in the parameterized model (Greco et al., 2005, 2008), there are four parameters. The parameterized model faces the challenges of choosing an appropriate Bayesian confirmation measure, a pair of thresholds on the confirmation measure, and a pair of thresholds on the posterior probability. The interrelationships between the four parameters must be considered. Since probabilistic approximation regions and Bayesian confirmation regions represent semantically different concepts, semantics interpretation of the three regions of the parameterized model is another challenge. Greco et al. (2005,2008) interpreted the posterior probability as defining a rough membership function and a Bayesian confirmation as a relative rough membership function. Thus, the parameterized model is simply a combination of two rough membership functions.By studying two separate models independently, we reduce the complexity. Since a combination of results from two models can produce the results of the parameterized rough set model, at the same time, we do not suffer too much on the loss of generality. According to the three fundamental issues, we weave together results from existing studies to produce a more complete picture of Bayesian approaches to rough sets. The main contribution of the paper does not lie in the introduction of new results, but in chaining them together into an integrated whole. More precisely, the Bayesian confirmation component is separated from the parameterized model (Greco et al., 2005, 2008) to produce a Bayesian confirmation rough set model; the results from DTRS models (Yao, 2007a; Yao & Wong, 1992; Yao et al., 1990) are used to interpret and compute a pair of thresholds for a Bayesian classification model; the techniques used in existing BRS models (Greco et al., 2005, 2008; Greco et al., 2004a, 2004b; Ślȩzak, 2005; Ślȩzak & Ziarko, 2002, 2005; Yao & Zhou, 2010) are adopted and extended to estimate probability based on Bayes’ theorem and inference; a framework of three-way decisions (Yao, 2009, 2010, 2011) is adopted to interpret rules from the three regions for ternary classification.The formulation of Bayesian confirmation rough sets draws results from studies on Bayesian confirmation theory (Earman, 1992; Festa, Galavotti, & Pagnini, 1999; Fitelson, xxxx; Steel, 2007) studies on parameterized rough sets (Greco et al., 2005, 2008), and related PRSs (Ślȩzak, 2005; Ślȩzak & Ziarko, 2002, 2005). We show differences of the (s, t)-confirmation regions from the (α, β)-probabilistic approximations of Bayesian classification rough sets. For simplicity, we assume that, whenever a ratio is used, a probability in the denominator is not zero.In a qualitative interpretation of Bayesian confirmation theory (Earman, 1992; Festa et al., 1999; Fitelson, xxxx; Steel, 2007), evidence E confirms hypothesis H, disconfirms H, or is neutral with respect to H whenever the posterior probability Pr(H|E) increases from the prior probability Pr(H), decreases from Pr(H) or is unchanged from Pr(H). Festa et al. (1999) referred to this interpretations of confirmation as P-incremental confirmation, to reflect the fact that the hypothesis H is confirmed by the evidence E when the initial probability of H increases as an effect of E. This qualitative notion of P-incremental confirmation can be precisely defined as follows:{EconfirmsHiffPr(H|E)>Pr(H)oriffPr(H|E)Pr(H)>1,Eisneutralw.r.tHiffPr(H|E)=Pr(H)oriffPr(H|E)Pr(H)=1,EdisconfirmsHiffPr(H|E)<Pr(H)oriffPr(H|E)Pr(H)<1.In this definition, the condition Pr(H|E) > Pr(H) is equivalently expressed as Pr(H|E)/Pr(H) > 1, assuming that Pr(H) ≠ 0.Bayesian confirmation theory also relies on the Bayes’ theorem for the computation of Pr(H|E). More specifically, two additional representations can be further derived. According to Bayes’s theorem, we havePr(H|E)Pr(H)=Pr(E|H)Pr(E). Thus, one can re-express conditions for qualitative confirmation by using likelihood and the probability of evidence:{EconfirmsHiffPr(E|H)Pr(E)>1,Eisneutralw.r.tHiffPr(E|H)Pr(E)=1,EdisconfirmsHiffPr(E|H)Pr(E)<1.Let Hcdenote the complement of hypothesis H. It follows thatPr(H)+Pr(Hc)=1. By the computation of the probability of evidence,(8)Pr(E)=Pr(E|H)Pr(H)+Pr(E|Hc)Pr(Hc),we have,(9)Pr(E|H)Pr(E)>1⟺Pr(E|H)Pr(E|H)Pr(H)+Pr(E|Hc)Pr(Hc)>1⟺Pr(E|H)Pr(E|Hc)>1.It follows that the conditions for confirmation can also be equivalently expressed through a likelihood ratio:{EconfirmsHiffPr(E|H)Pr(E|Hc)>1,Eisneutralw.r.tHiffPr(E|H)Pr(E|Hc)=1,EdisconfirmsHiffPr(E|H)Pr(E|Hc)<1.The three distinct but equivalent expressions of the conditions of confirmation provide an understanding of Bayesian confirmation from different perspectives. The first focuses on the comparison of the prior and posterior probability of H, the second focuses on whether E is more probable conditionally on H than it is unconditionally, and the third focuses on the likelihood ratio.When applying qualitative Bayesian confirmation theory to rough sets, one may view the set C as a hypothesis that an object is in C and an equivalence class as evidence that an object is in the equivalence class. This immediately leads to a definition of three qualitative Bayesian confirmation regions, namely, the positive confirmation region, the non-confirmation (or neutral) region, and the negative confirmation (or disconfirmation) region:(10)POSC(C)={x∈U∣Pr(C|[x])>Pr(C)}={x∈U∣Pr([x]|C)Pr([x]|Cc)>1},BNDC(C)={x∈U∣Pr(C|[x])=Pr(C)}={x∈U∣Pr([x]|C)Pr([x]|Cc)=1},NEGC(C)={x∈U∣Pr(C|[x])<Pr(C)}={x∈U∣Pr([x]|C)Pr([x]|Cc)<1}.These three regions were introduced and studied by Ślȩzak and Ziarko (2002,2005) in a BRS model. They interpreted them as PRS approximations of C. We argue that they have a very different semantic interpretation and it may not be appropriate to interpret them straightforwardly as probabilistic approximations of C.The three confirmation regions and the three (α, β)-probabilistic approximation regions are similar in form and both are defined in probabilistic terms. It is very tempting to treat them as the same by viewing Pr(H) as a threshold on the posterior probability (Ślȩzak & Ziarko, 2002, 2005). However, it must be realized that there are significant semantic differences between them. Similar to Pawlak approximation regions, the three confirmation regions defined by Equation (10) are of a qualitative nature. They are defined based on whether an equivalence class confirms, is neutral with respect to, or disconfirms C. There is no consideration of the degree of confirmation. In contrast, the (α, β)-probabilistic approximation regions are of a quantitative nature. According to Bayesian confirmation theory, one may interpret the three confirmation regions as follows. For an equivalence class inside the positive confirmation region POSC(C), the evidence that an object is in the equivalence class positively confirms the hypothesis that the object is in C; evidence from the negative confirmation region negatively confirms, or disconfirms, the hypothesis; evidence from the boundary region neither confirms nor disconfirms, or is neutral with respect to, the hypothesis. In some sense, the three confirmation regions are a classification of various pieces of evidence according to their confirmation of the set C. They may not be viewed directly as approximations of C. In contrast, (α, β)-probabilistic regions are a classification of objects as approximation of C. For these reasons, we refer to one as confirmation regions and the other as probabilistic approximation regions.A quantitative conception of Bayesian confirmation theory employs a Bayesian confirmation measure that quantifies the degree to which evidence confirms hypothesis. There is no general agreement on a quantitative measure of confirmation (Talbot, 2001). Many Bayesian confirmation measures have been proposed and studied (Festa et al., 1999; Fitelson, xxxx). A number of properties to be satisfied by such measures have been introduced and examined. Festa et al. (1999) suggested that a Bayesian confirmation measure c(E, H) must satisfy the condition of probability increment, that is,c(E,H)=f(Pr(H|E),Pr(E)),must be an increasing function of Pr(H|E) and a decreasing function of Pr(H). Corresponding to the previously discussed three forms of qualitative confirmation, the following three measures are P-incremental confirmation measures:(11)cd(E,H)=Pr(H|E)−Pr(H),cr(E,H)=Pr(H|E)Pr(H)=Pr(E|H)Pr(E),cr+(E,H)=Pr(E|H)Pr(E|Hc),where a similar notational system of Festa is used.Fitelson (xxxx) argued that a confirmation measure must be consistent with qualitative interpretation of confirmation in the sense that(12)c(E,H){>0,Pr(H|E)>Pr(H)=0,Pr(H|E)=Pr(H)<0,Pr(H|E)<Pr(H)A measure satisfying this property is called a relevance measure (Fitelson, xxxx). Corresponding to the three representations of qualitative Bayesian confirmation, the following Bayesian confirmation measures are relevance measures:(13)cd(E,H)=Pr(H|E)−Pr(H),cnr(E,H)=Pr(H|E)Pr(H)−1=Pr(E|H)Pr(E)−1,cnr+(E,H)=Pr(E|H)Pr(E|Hc)−1,clr(E,H)=logPr(H|E)Pr(H)=logPr(E|H)Pr(E),clr+(E,H)=logPr(E|H)Pr(E|Hc).Confirmation measures cnrandcnr+may be viewed as normalized version of crandcr+that satisfy the constraint given by Equation (12). Additional Bayesian confirmation measures and their interpretations can be found in Festa et al. (1999), Fitelson (xxxx), and Greco et al. (2005,2008).When applying a Bayesian confirmation measure to define confirmation regions, one may expect that evidence in the positive region must confirm the hypothesis beyond a certain level, evidence in the negative region must disconfirm the hypothesis beyond another level, and evidence in the boundary region fails to confirm or disconfirm the hypothesis beyond the required levels. In the context of rough set theory, the value c([x], C) is the degree to which the evidence y ∈ [x] confirms the hypothesis y ∈ C, where y ∈ U. Consider a Bayesian confirmation measure c([x], C) and a pair of thresholds (s, t), with t < s, on the confirmation measure. Following the work of Greco et al. (2005)2008) on a parameterized model, three (s, t)-confirmation regions are defined by:(14)POSC(s,t)(C)={x∈U∣c([x],C)≥s},BNDC(s,t)(C)={x∈U∣t<c([x],C)<s},NEGC(s,t)(C)={x∈U∣c([x],C)≤t}.They are pair-wise disjoint.Although quantitative Bayesian confirmation regions are defined in a similar way as (α, β)-probabilistic approximation regions, there are several difficulties with the former. Recall that there is no general agreement on a Bayesian confirmation measure. Choosing an appropriate confirmation measure for a particular application may not be an easy task. Greco et al. (2005,2008); Greco et al. (2004a,2004b) introduced and analyzed a class of monotonic Bayesian confirmation measures. They also suggested that this class of confirmation measures is appropriate in the context of rough set theory. The ranges of the values of different confirmation measures are different. This makes it an even more difficult task to interpret and set the thresholds for the desired levels of confirmation or disconfirmation. For different confirmation measure, we may need to lay out different guidelines for setting thresholds.Consider now a minimal requirement of (s, t)-confirmation regions. According to intended interpretation of Bayesian confirmation, it is reasonable to require that (s, t)-confirmation is consistent with qualitative confirmation. That is, the pair of thresholds must be chosen so that it satisfies the following conditions: if evidence quantitatively confirms a hypothesis beyond a level s, the evidence must qualitative confirms the hypothesis; if evidence quantitatively disconfirms a hypothesis below a level t, the evidence must qualitatively disconfirms the hypothesis. If c([x], C) is a relevance measure, the condition t < 0 < s will satisfy this requirement. In general, the reverse implications are not true. This immediately leads to the following linkage between qualitative and quantitative confirmation regions:(15)POSC(s,t)(C)⊆POSC(C),BNDC(C)⊆BNDC(s,t)(C),NEGC(s,t)(C)⊆POSC(C).The linkage of qualitative and quantitative confirmation regions is a kind of reverse relation of the linkage of Pawlak and (α, β)-probabilistic approximations. One may conclude that Bayesian confirmation theory provides a new class of PRS models. It is important to distinguish the class of Bayesian classification rough set models and the class of Bayesian confirmation rough sets models.For the three representations of qualitative Bayesian confirmation, they produce the same confirmation regions. This is no longer true for their corresponding quantitative confirmation measures. For confirmation measurecd([x],C)=Pr(C|[x])−Pr(C),we have the following three regions:(16)POSC(s,t)(C)={x∈U∣Pr(C|[x])−Pr(C)≥s},BNDC(s,t)(C)={x∈U∣t<Pr(C|[x])−Pr(C)<s},NEGC(s,t)(C)={x∈U∣Pr(C|[x])−Pr(C)≤t}.Based on the range of the values of cd, it is reasonable to require that t < 0 < s, with [s, 1] for positive confirmation, (t, s) for neutral confirmation, and[−1,t]for negative confirmation. For confirmation measure,cr+([x],C)=Pr([x]|C)Pr([x]|Cc),one can immediately obtain the three regions introduced by Ślȩzak (2005) in a rough Bayesian model:(17)POSC(s′,t′)(C)={x∈U∣Pr([x]|C)Pr([x]|Cc)≥s′},BNDC(s′,t′)(C)={x∈U∣t′<Pr([x]|C)Pr([x]|Cc)<s′},NEGC(s′,t′)(C)={x∈U∣Pr([x]|C)Pr([x]|Cc)≤t′}.The pair of thresholds (s′, t′) oncr+is different from the pair (s, t) on cd. It is reasonable to require that t′ < 1 < s′, with[s′,+∞)for positive confirmation, (t′, s′) for neutral confirmation, and [0, t′] for negative confirmation. To some degree, it might be easier to interpret thresholds on confirmation measure cd, as it is the difference between posterior and prior probability. Interpretations of thresholds on confirmation measures such ascr+,clrorclr+are not as intuitive.In existing studies on variations of confirmation-theoretic rough set models, much attention has been paid to mathematical constructions and formal properties of various notions. It is important to closely examine the semantics of these variations, including BRSs (Ślȩzak & Ziarko, 2002, 2005) and rough Bayesian sets (Ślȩzak, 2005). The term “Bayesian rough sets” is preferred, as it indicate a combination of Bayesian methods and rough set theory, or applications of Bayesian methods to rough set theory, in a similar way that the term of “probabilistic rough sets” is used to denote a combination of probabilistic methods and rough set theory or applications of probabilistic methods to rough set theory. In other words, BRSs are interpreted as Bayesian approaches to rough sets, involving Bayesian statistics, Bayesian inference, Bayesian decision theory, and Bayesian confirmation theory.The plurality of Bayesian confirmation measures imposes another practical difficulty in applying confirmation-theoretic rough set models. Unlike DTRS models, there is, in general, a lack of guidelines and systematic procedures for interpreting and computing the required thresholds. As future research, one may study theories and methods for interpreting and computing a pair of thresholds in a Bayesian confirmation rough set model.Qualitative and quantitative Bayesian classification rough set model can be developed by following the formulation of Bayesian confirmation rough set models of the last section. That is, we simply replace a Bayesian confirmation measure with Pr(C|[x]) and use a pair of thresholds (α, β) on Pr(C|[x]). In this section, we review an alternative formulation based on decision-theoretic models suggested by Yao (2007a), Yao and Wong (1992), and Yao et al. (1990). The formulation not only produces (α, β)-probabilistic approximations but also provides a systematic procedure for determining the thresholds based on Bayesian decision theory.Bayesian decision theory is a fundamental statistical approach that makes decisions under uncertainty based on probabilities and costs associated with decisions. Following Duda and Hart (1973), the basic ideas of Bayesian decision theory are briefly summarized. LetΩ={w1,…,ws}be a finite set of s states and letA={a1,…,am}be a finite set of m possible actions. Let λ(ai|wj) denote the loss, or cost, for taking action aiwhen the state is wj. Let Pr(wj|x) be the conditional probability of an object x being in state wjgiven that the object is described by x. For an object with description x, suppose action aiis taken. Since Pr(wj|x) is the probability that the true state is wjgiven x, the expected loss associated with taking action aiis given byR(ai|x)=∑j=1j=sλ(ai|wj)Pr(wj|x).The quantity R(ai|x) is also called the conditional risk.Given a description x, a decision rule is a function τ(x) that specifies which action to take. That is, for every x, τ(x) takes one of the actions froma1,…,am. The overall risk R is the expected loss associated with a given decision rule. Since R(τ(x)|x) is the conditional risk associated with action τ(x), the overall risk is defined byR=∑xR(τ(x)|x)Pr(x),where the summation is over the set of all possible descriptions of objects. If τ(x) is chosen so that R(τ(x)|x) is as small as possible for every x, the overall risk R is minimized. Thus, the optimal Bayesian decision procedure can be formally stated as follows. For every x, compute the conditional risk R(ai|x) fori=1,…,mand select the action for which the conditional risk is minimum. If more than one action minimizes R(ai|x), a tie-breaking criterion is used.A DTRS model is constructed by a straightforward application of the Bayesian decision theory. With respect to a subset C ⊆ U, we can form a set of two statesΩ={C,Cc}. To derive the three regions in rough set theory, the set of actions is given byA={aP,aB,aN},where aP, aB, and aNrepresent the three actions in classifying an object x, namely, deciding x ∈ POS(C), deciding x ∈ BND(C), and deciding x ∈ NEG(C), respectively. The loss function is given by a 3 × 2 matrix:C(P)Cc(N)aPλPP=λ(aP|C)λPN=λ(aP|Cc)aBλBP=λ(aB|C)λBN=λ(aB|Cc)aNλNP=λ(aN|C)λNN=λ(aN|Cc)In the matrix, λPP, λBPand λNPdenote the losses incurred for taking actions aP, aBand aN, respectively, when an object belongs to C, and λPN, λBNand λNNdenote the losses incurred for taking these actions when the object does not belong to C.The expected losses associated with taking different actions for objects in [x] can be expressed as:(18)R(aP|[x])=λPPPr(C|[x])+λPNPr(Cc|[x]),R(aB|[x])=λBPPr(C|[x])+λBNPr(Cc|[x]),R(aN|[x])=λNPPr(C|[x])+λNNPr(Cc|[x]).The Bayesian decision procedure suggests the following minimum-risk decision rules:(P)IfR(aP|[x])≤R(aB|[x])andR(aP|[x])≤R(aN|[x]),decidex∈POS(C);(B)IfR(aB|[x])≤R(aP|[x])andR(aB|[x])≤R(aN|[x]),decidex∈BND(C);(N)IfR(aN|[x])≤R(aP|[x])andR(aN|[x])≤R(aB|[x]),decidex∈NEG(C).Tie-breaking criteria should be added so that each object is put into only one region.ByPr(C|[x])+Pr(Cc|[x])=1,we can simplify the rules based only on the probabilities Pr(C|[x]) and the loss function λ. Consider a special kind of loss functions with:(19)(c0).λPP≤λBP<λNP,λNN≤λBN<λPN.That is, the loss of classifying an object x belonging to C into the positive region POS(C) is less than or equal to the loss of classifying x into the boundary region BND(C), and both of these losses are strictly less than the loss of classifying x into the negative region NEG(C). The reverse order of losses is used for classifying an object not in C. Under condition (c0), we can simplify decision rules (P)–(N) as follows. For the rule (P), the first condition can be expressed as:(20)R(aP|[x])≤R(aB|[x])⟺λPPPr(C|[x])+λPNPr(Cc|[x])≤λBPPr(C|[x])+λBNPr(Cc|[x])⟺Pr(C|[x])≥(λPN−λBN)(λPN−λBN)+(λBP−λPP).Similarly, other conditions of the three rules can be expressed as:R(aP|[x])≤R(aN|[x])⟺Pr(C|[x])≥(λPN−λNN)(λPN−λNN)+(λNP−λPP),R(aB|[x])≤R(aP|[x])⟺Pr(C|[x])≤(λPN−λBN)(λPN−λBN)+(λBP−λPP),R(aB|[x])≤R(aN|[x])⟺Pr(C|[x])≥(λBN−λNN)(λBN−λNN)+(λNP−λBP),R(aN|[x])≤R(aP|[x])⟺Pr(C|[x])≤(λPN−λNN)(λPN−λNN)+(λNP−λPP),R(aN|[x])≤R(aB|[x])⟺Pr(C|[x])≤(λBN−λNN)(λBN−λNN)+(λNP−λBP).By introducing three parameters:(21)α=(λPN−λBN)(λPN−λBN)+(λBP−λPP),β=(λBN−λNN)(λBN−λNN)+(λNP−λBP),γ=(λPN−λNN)(λPN−λNN)+(λNP−λPP),we can express concisely the decision rules (P)–(N) as:(P)IfPr(C|[x])≥αandPr(C|[x])≥γ,decidex∈POS(C);(B)IfPr(C|[x])≤αandPr(C|[x])≥β,decidex∈BND(C);(N)IfPr(C|[x])≤γandPr(C|[x])≤β,decidex∈NEG(C).Each rule is defined by two out of the three parameters.The conditions of rule (B) suggest that it may be reasonable to impose the constraint α > β so that the boundary region may be non-empty. By setting α > β, we obtain the following condition on the loss function (Yao, 2007a):(22)(c1).λNP−λBPλBN−λNN>λBP−λPPλPN−λBN.The conditions (c0) and (c1) imply that 1 ≥ α > γ > β ≥ 0. In this case, after tie-breaking, the following simplified rules are obtained  (Yao, 2007a):(P)IfPr(C|[x])≥α,decidex∈POS(C);(B)Ifβ<Pr(C|[x])<α,decidex∈BND(C);(N)IfPr(C|[x])≤β,decidex∈NEG(C).The parameter γ is no longer needed. From the rules (P), (B), and (N), we get the (α, β)-probabilistic positive, negative and boundary regions in Equation (2).The formulation of a DTRS model not only produces three probabilistic regions, but also provides a theoretical basis for and a practical interpretation of the PRSs. The thresholds are systematically calculated from a loss function that can be easily interpreted in more operable terms, including profits, risk, cost, etc.The interpretation of the required threshold and the process of deriving the thresholds from loss functions can be demonstrated by using email spam filtering as an example (Zhao & Zhu, 2005; Zhou, Yao, & Luo, 2014). In this case, there are two states regarding an incoming email: C (P) denoting a Legitimate email and Cc(N) denoting a Spam. There are three actions: aPfor accepting the email, aBfor making a deferred decision (i.e., neither accept nor reject the email due to insufficient information), and aNfor rejecting the email.A three-way email spam filtering system produces three folders, the accepted folder, the deferred folder, and the rejected folder. Suppose that a user views the accepted folder immediately, delays the processing of deferred folders, and deletes the rejected folder without viewing. A loss function is therefore interpreted as the costs of taking the corresponding actions. Generally speaking, a higher cost occurs when misclassifying a legitimate email as a spam; it could result in losing vital information for a user. On the other hand, misclassifying a spam to be a legitimate email brings unnecessary costs of processing the spam. Cost also occurs when a delayed decision is made. The costs depend very much on a particular user’s subjective evaluation about various actions and the tolerance of different types of errors. Different users may give different values to reflect the cost of losing a legitimate email, processing a spam in the accepted folder, or delaying processing of the deferred folder.Tables 1and 2give the loss functions of two users, User 1 and User 2, respectively. User 1 is more concerned about losing a legitimate email and at the same time about processing a spam immediately. In comparison, User 2 is not so much concerned. The loss functions of both users satisfy conditions (c0) and (c1). The pair of thresholds α1 and β1 for User 1 is calculated according to Equation (21) as:α1=(λPN1−λBN1)(λPN1−λBN1)+(λBP1−λPP1)=10−5(10−5)+(5−0)=0.50,β1=(λBN1−λNN1)(λBN1−λNN1)+(λNP1−λBP1)=5−0(5−0)+(90−5)=0.06.The pair of thresholds α2 and β2 for User 2 is calculated as:α2=(λPN2−λBN2)(λPN2−λBN2)+(λBP2−λPP2)=8−5(8−5)+(5−0)=0.38,β2=(λBN2−λNN2)(λBN2−λNN2)+(λNP2−λBP2)=5−0(5−0)+(15−5)=0.33.It follows that β1 < β2 < α2 < α1. As expected, the thresholds of User 2 are within the thresholds of User 1, which shows that User 1 is much critical than User 2 regarding both incorrect acceptance and rejection. Consequently, User 1 would have smaller accepted and rejected folders but a large deferred folder. In contrast, User 2 would have lager accepted and rejected folders but a smaller deferred folder. Different filtering options are tailored to meet individual requirements in terms of the minimum overall cost.Bayesian inference, based on Bayes’ theorem, is used to find procedures for computing posterior probability.In the (α, β)-probabilistic approximations, the posterior probabilities are not always directly derivable from data. In such cases, we need to consider alternative ways to calculate their values. A commonly used method is to apply the Bayes’ theorem:(23)Pr(C|[x])=Pr(C)Pr([x]|C)Pr([x]),wherePr([x])=Pr([x]|C)Pr(C)+Pr([x]|Cc)Pr(Cc).It reduces the problem of estimating the posterior probability Pr(C|[x]) of class C given [x] into estimating the prior probability Pr(C) of class C, and the likelihood Pr([x]|C). There are many methods to estimate likelihood from data, which makes PRS models practically useful.In Bayesian classification, one often uses a monotonically increasing function of the conditional probability to construct an equivalent classifier. One of such functions is the odds defined byO(·)=Pr(·)1−Pr(·). The odds version of Bayes’ theorem is given by:O(C|[x])=Pr(C|[x])Pr(Cc|[x])=Pr([x]|C)Pr([x]|Cc)·Pr(C)Pr(Cc)=Pr([x]|C)Pr([x]|Cc)·O(C).It shows how to update a prior odds O(C) into a posterior odds O(C|[x]) through the likelihood ratioPr([x]|C)Pr([x]|Cc). An advantage of the odds form is that it is no longer required to estimate the probability of an equivalence class [x]. For the positive region, we have:Pr(C|[x])≥α⟺O(C|[x])≥α1−α⟺Pr([x]|C)Pr([x]|Cc)≥Pr(Cc)Pr(C)·α1−α.Similar expressions can be obtained for the negative and boundary regions. Consequently, we can express the three regions in terms of the likelihood ratio as:(24)POS(α,β)(C)={x∈U∣Pr([x]|C)Pr([x]|Cc)≥Pr(Cc)Pr(C)·α1−α},BND(α,β)(C)={x∈U∣Pr(Cc)Pr(C)·β1−β<Pr([x]|C)Pr([x]|Cc)<Pr(Cc)Pr(C)·α1−α},NEG(α,β)(C)={x∈U∣Pr([x]|C)Pr([x]|Cc)≤Pr(Cc)Pr(C)·β1−β}.The conditions about the likelihood ratio are obtained by the same transformation of the original thresholds α and β with an adjustment of the prior odds of C.Another widely used monotonically increasing transformation of probability function is the logit transformation defined bylogit(Pr(·))=log(O(·))=logPr(·)1−Pr(·). By applying logarithm function to Eq. (24), we can express the three regions as:(25)POS(α,β)(C)={x∈U∣logPr([x]|C)Pr([x]|Cc)≥logPr(Cc)Pr(C)+logα1−α},BND(α,β)(C)={x∈U∣logPr(Cc)Pr(C)+logβ1−β<logPr([x]|C)Pr([x]|Cc)<logPr(Cc)Pr(C)+logα1−α},NEG(α,β)(C)={x∈U∣logPr([x]|C)Pr([x]|Cc)≤logPr(Cc)Pr(C)+logβ1−β}.Again, conditions about value of logit function are obtained from the same transformation of the original thresholds α and β with an adjustment of the prior odds.Both the likelihood ratio and the logarithm of likelihood ratio are examples of Bayesian confirmation measures. By looking at the forms of the last two new but equivalent definitions of probabilistic approximations, one may confuse them with Bayesian confirmation regions. Since the prior odds of C is involved, the threshold are in fact not on the corresponding Bayesian confirmation measure, but on a Bayesian confirmation modified by the prior odds.Naive Bayesian classification provides an effective method to estimate the likelihood by representing an object as a feature vector and assuming that the features are probabilistically independent. Its application to rough set theory leads to an NBRS model (Yao & Zhou, 2010).In an information tableS=(U,At,{Va∣a∈At},{Ia∣a∈At}),the information function Iamaps an object in U to a value of Vafor an attribute a ∈ At, that is, Ia(x) ∈ Va. An equivalence relation RAcan be defined with respect to a subset of attribute A⊆At. For an attribute-value pair (a, v), a ∈ At and v ∈ Va, one can construct an atomic formulaa=v. The meaning ofa=vis interpreted as a subset of objects defined by:m(a=v)={x∈U∣Ia(x)=v}.For a subset of attributesA={a1,a2,…,an}⊆At,an equivalence class [x] of the equivalence relation RAis defined by the formula⋀i=1i=nai=Iai(x),that is,[x]=m(⋀i=1i=nai=Iai(x))=⋂i=1i=nm(ai=Iai(x)).Thus, the description of an equivalence class [x] is a feature vectorDes([x])=(va1,va2,…,van).With the feature vector representation, we need to estimate a joint probabilitiesPr([x]|C)=Pr(v1,v2,…,vn|C)and a joint probability ofPr([x])=Pr(v1,v2,…,vn). In practice, it is difficult to analyze the interactions between the components of [x], especially when the number n is large. A common solution to this problem is to calculate the likelihood based on the naive conditional independence assumption (Good, 1965). That is, we assume each component viof [x] to be conditionally independent of every other component vjfor j ≠ i. Although this assumption may seem overly simplistic, many empirical studies showed its effectiveness for classification problems (Domingos & Pazzani, 1996; Langley, Wayne, & Thompson, 1992; Zhang, 2005).Formally, the probabilistic independence assumptions are given by:(26)Pr([x]|C)=Pr(v1,v2,…,vn|C)=∏i=1i=nPr(vi|C),Pr([x]|Cc)=Pr(v1,v2,…,vn|Cc)=∏i=1i=nPr(vi|Cc).By inserting them into Eq. (25), we can compute the logarithm of the likelihood ratio as:(27)logPr([x]|C)Pr([x]|Cc)=log∏i=1i=nPr(vi|C)Pr(vi|Cc)=∑i=1i=nlogPr(vi|C)Pr(vi|Cc).Now, the three probabilistic regions can be computed by:(28)POS(α,β)(C)={x∈U∣∑i=1i=nlogPr(vi|C)Pr(vi|Cc)≥logPr(Cc)Pr(C)+logα1−α},BND(α,β)(C)={x∈U∣logPr(Cc)Pr(C)+logβ1−β<∑i=1i=nlogPr(vi|C)Pr(vi|Cc)<logPr(Cc)Pr(C)+logα1−α},NEG(α,β)(C)={x∈U∣∑i=1i=nlogPr(vi|C)Pr(vi|Cc)≤logPr(Cc)Pr(C)+logβ1−β}.The individual probabilities can be estimated from the frequencies of the training data:Pr(vi|C)=|m(ai=vi)∩C||C|andPr(C)=|C||U|,where | · | denote the cardinality of a set. Thus, a simple method for probability estimation is obtained in an NBRS model (Yao & Zhou, 2010).We introduce a binary probabilistic independent model for a specific classification problem to show the usefulness of NBRSs. In this model, all the feature vectors are binary valued. That is, an object x is represented by[x]=(v1,v2,…,vn)wherevi=1if feature i is present for the object x andvi=0if i is not present for x.Letpi=Pr(vi=1|C)denote the probability of a feature presenting in an object belong to class C, andqi=Pr(vi=1|Cc)denote the probability of a feature presenting in an object belong to class Cc. We can rewrite Eq. (27) as:(29)∑i=1i=nlogPr(vi|C)Pr(vi|Cc)=∑i=1i=nlogPr(vi=1|C)viPr(vi=0|C)1−viPr(vi=1|Cc)viPr(vi=0|Cc)1−vi=∑i=1i=nlogpivi(1−pi)1−viqivi(1−qi)1−vi=∑i=1i=nwivi+w0,wherewi=logpi(1−qi)qi(1−pi),i=1,2,…,n,andw0=∑i=1i=n1−pi1−qi. With these notations, the three probabilistic regions can be computed by:POS(α,β)B(C)={x∈U∣∑i=1i=nwivi≥logPr(Cc)Pr(C)+logα1−α−w0},BND(α,β)B(C)={x∈U∣logPr(Cc)Pr(C)+logβ1−β−w0<∑i=1i=nwivi<logPr(Cc)Pr(C)+logα1−α−w0},NEG(α,β)B(C)={x∈U∣∑i=1i=nwivi≤logPr(Cc)Pr(C)+logβ1−β−w0}.The value wimay be viewed as a weight of feature i, and w0 is added to simplify the representation of the equations.The three probabilistic regions can be used to build a ternary classifier for three-way classification. Under certain condition, the introduction of a third choice makes a ternary classifier more advantageous than a binary classifier. In this section, we investigate situations where a ternary classifier is preferred.There are extensive studies on inferring classification rules from rough set approximations (Grzymala-Busse, 1988; Grzymala-Busse et al., 2010; Polkowski & Skowron, 1998). Majority of them focuses on rule discovery methods and characterization of rules in terms of statistical measures such as generality, accuracy, coverage etc. (Brzezinska, Greco, & Slowinski, 2007; Leung, Wu, & Zhang, 2006; Pawlak, 2002; Tsumoto, 2002; Wong & Ziarko, 1986a; Yao & Zhong, 1999). However, there is an insufficient treatment on semantics and implications of the discovered rules. Recently, the notion of three-way decisions has been adopted to interpret rules derived in rough set theory. That is, classification rules in rough set theory produce a ternary classifier (Yao, 2009).Consider (α, β)-probabilistic positive, boundary and negative regions defined by Equation (2). They provide three-way classification of objects with respect to C, as shown in Fig. 2. We accept an object x to be a member of C if the conditional probability is greater than or equal to α, that is, Pr(C|[x]) ∈ [α, 1]. We reject x to be a member of C if the conditional probability is less than or equal to β, that is, Pr(C|[x]) ∈ [0, β]. We neither accept nor reject x to be a member of C if the conditional probability is in between of α and β, that is, Pr(C|[x]) ∈ (β, α); instead, we make a decision of deferment.Let Des([x]) denote the description of objects in the equivalence class [x]. Based on the probabilistic three regions, we can derive the following three types of positive, boundary and negative rules for classification:(30)Des([x])⟶PC,if[x]⊆POS(C);Des([x])⟶BC,if[x]⊆BND(C);Des([x])⟶NC,if[x]⊆NEG(C).Although these three types of rules have the same form, they have different interpretations, and hence lead to different decisions and actions. To be consistent with the interpretation of three regions, we can make one of three decisions when classifying an object. A positive rule is a rule for acceptance: if [x] ⊆ POS(C), we accept x to be an instance of C, that is, we accept x ∈ C. A negative rule is a rule for rejection: if [x] ⊆ NEG(C), we reject x to be an instance of C, that is, we reject x ∈ C. A boundary rule is a rule for deferment: if [x] ⊆ BND(C), we neither accept nor reject x ∈ C; instead, we defer such a definite decision. Accordingly, we also call the three types of rules the acceptance, deferment and rejection rules. By applying the three types of rules, one can easily classify an object through a decision of acceptance, rejection or deferment. The result is a ternary classifier.Ternary classifiers derived from PRSs can be used in many applications. Consider the example of email spam filtering discussed earlier. A positive rule indicates that an incoming email can be accepted as legitimate immediately, a negative rule indicates that an incoming email is rejected and is considered to be a spam immediately, and a boundary rule indicates that a deferred decision is made on an incoming email due to insufficient information and further analysis is required.Three-way decisions, or ternary classifiers, have been introduced and applied in many fields (Yao, 2010). For instance, Wald (1945) considered a sequential hypothesis testing model, in which a pair of thresholds is used for accepting a hypothesis, rejecting a hypothesis, or further testing based on two thresholds on probability values. Robinson (2003) suggested to add a boundary region marked unsure in email spam filtering problem, in addition to the binary legitimate/spam classification results to reduce the misclassification error. Pauker and Kassirer (1980) proposed a threshold approach to clinical decision making. A pair of a “testing” threshold and a “test-treatment” threshold on probability is used, with testing threshold determining whether to perform a diagnose test on the patient, and test-treatment threshold determining whether to treat the patient immediately. Similar approaches can also be found in many other real world problems, such as data warehouse (Ślȩzak, Wróblewski, Eastwood, & Synak, 2008), information retrieval (Li, Zhang, & Swanb, 1999), Bayesian significant test in statistical process control (Woodward & Naylor, 1993), decision making in environment management (Gebrezgabher, Meuwissen, & Oude Lansink, 2014; Goudey, 2007) and rough set based programming (Atteya, 2015). The approach introduced in this paper gives a new rough set interpretation of ternary classification.In Bayesian classification, a monotonically increasing function of Pr(C|[x]) is called a discriminant function (Duda & Hart, 1973). There are typically two ways to use a discriminant function for classification. One is to use the discriminant function to rank objects and let a user to classify objects by reading through the ranked list. The other is to set a threshold. This produces a binary classifier: objects whose values are above or equal to the threshold are accepted as instances of the class and whose values below the threshold are rejected. The Bayesian decision theory can be used to systematically compute the threshold (Duda & Hart, 1973).Formally, Bayesian two-way classification can be described as follows. Let γ ∈ [0, 1] denote a threshold on the posterior probability Pr(C|[x]). One can divide the set of objects into two regions as approximations of C, namely, the γ-probabilistic positive and negative regions:(31)POSγ(C)={x∈U∣Pr(C|[x])≥γ},NEGγ(C)={x∈U∣Pr(C|[x])<γ}.They in turn induce two types of rules, that is, the positive rules for acceptance and the negative rules for rejection. Note that the probability Pr(C|[x]) can be estimated by the same methods discussed in the last section.The differences between binary and three-way classifications are demonstrated in Figs. 2 and 3. As stated in Yao (2011), a three-way classification is advantageous under the condition β < γ < α. In this case, for the two intervals [0, β] and [α, 1], binary and ternary classifier make the same decision of acceptance and rejection, respectively, and for interval (α, β), a ternary classifier chooses a deferment rather than an acceptance or a rejection. It follows that,(32)POS(α,β)(C)⊆POSγ(C),BNDγ(C)⊆BND(α,β)(C),NEG(α,β)(C)⊆NEGγ(C),that is, a ternary classifier move some objects from the positive and negative regions of a binary classifier into a deferment region.Binary and ternary classifiers are different in terms of various types of classification errors and costs. A binary classifier may produce two types of errors, namely, incorrect acceptance and incorrect rejection. A ternary classifier may produce two additional types of errors, namely, deferment of positive and deferment of negative. Tables 3and 4are two confusion matrices resulting from binary and ternary classifiers, respectively. The symbols in the two tables denote the following numbers:TP(True Positive) = The number of correctly classified positive examples.FP(False Positive) = The number of incorrectly classified negative examples.FN(False Negative) = The number of incorrectly classified positive examples.TN(True Negative) = The number of correctly classified negative examples.AP(Accepted Positive) = The number of correctly accepted positive examples.AN(Accepted Negative) = The number of incorrectly accepted negative examples.DP(Deferred Positive) = The number of deferred positive examples.DN(Deferred Negative) = The number of deferred negative examples.RP(Rejected Positive) = The number of incorrectly rejected positive examples.RN(Rejected Negative) = The number of correctly rejected negative examples.The four numbers of a binary classifier have slightly different interpretation as those in a ternary classifier. We use new names for the latter to emphasize their corresponding decisions.Binary and ternary classifiers are also different with respect to errors. The acceptance error is the proportion of accepted examples which are actually not in class C. Let AccE2 and AccE3 denote the acceptance errors of the binary and three-way classifiers, respectively. They are defined by:(33)AccE2=FPTP+FP=|Cc∩POSγ(C)||POSγ(C)|,AccE3=APAP+AN=|Cc∩POS(α,β)(C)||POS(α,β)(C)|.Similarly, the rejection error is the proportion of rejected examples which are actually in class C. The rejection errors of a binary classifier and a three-way classifier are defined respectively by:(34)RejE2=FNFN+TN=|C∩NEGγ(C)||NEGγ(C)|,RejE3=RPRP+RN=|C∩NEG(α,β)(C)||NEG(α,β)(C)|.By the condition β < γ < α, we can express POSγ(C) as POS(α, β)(C) ∪ M, whereM={x∣γ≤Pr(C|[x])<α}. Therefore, the difference between acceptance errors of binary and ternary classifiers is given by:(35)AccE2−AccE3=|Cc∩POSγ(C)||POSγ(C)|−|Cc∩POS(α,β)(C)||POS(α,β)(C)|=|C∩POS(α,β)(C)||POS(α,β)(C)|−|C∩POS(α,β)(C)|+|C∩M||POS(α,β)(C)|+|M|.By the definition of probabilistic regions of binary and ternary classifications, and the condition β < γ < α, we can establish the following facts:|C∩POS(α,β)(C)|=|C∩(⋃{[x]||C∩[x]||[x]|≥α})|=|⋃{C∩[x]||C∩[x]||[x]|≥α}|=∑|C∩[x]||[x]|≥α∣C∩[x]∣≥∑|C∩[x]||[x]|≥αα∣[x]∣=α∣POS(α,β)(C)∣,|C∩M|=|C∩(⋃{[x]|γ≤|C∩[x]||[x]|<α})|=|⋃{C∩[x]|γ≤|C∩[x]||[x]|<α}|=∑γ≤|C∩[x]||[x]|<α∣C∩[x]∣≤∑γ≤|C∩[x]||[x]|<αα∣[x]∣=α∣M∣,where the summation is over all equivalence classes satisfying a certain condition. Thus, Eq. (35) can be rewritten as:(36)|C∩POS(α,β)(C)|·(|POS(α,β)(C)|+|M|)−|POS(α,β)(C)|·(|C∩POS(α,β)(C)|+|C∩M|)|POS(α,β)(C)|·(|POS(α,β)(C)|+|M|)≥α|POS(α,β)(C)|·|M|−α|POS(α,β)(C)|·|M||POS(α,β)(C)|·(|POS(α,β)(C)|+|M|)≥0.Therefore, AccE2 ≥ AccE3. Similarly, it can be shown that RejE2 ≥ RejE3. That is, both acceptance and rejection errors of the three-way classification are lower than the corresponding errors in binary classification. A ternary classifier reduces the acceptance or rejection errors by introducing deferment errors.According to the optimality of Bayesian decision theory, the trade-off made by a ternary classifier, under the condition β < γ < α, leads to lower overall cost of classification. The conditions on the loss function to ensure β < γ < α and the differences between binary and ternary classification with respect to classification costs can be found in Yao (2011).As a note on comparison of ternary and binary classifications, we quote comments from another reviewer of this paper: “ternary and binary classification just reflect different attitudes toward decision making, and they are rather incomparable. Ternary classification is justified if deferment is admissible and low cost. On the other hand, binary classification is preferred if rigid or automatic (non-interactive) classification is required. Therefore, I feel that it is meaningless to compare ternary and binary classification with respect to cost or classification error.” The reviewer correctly pointed out that there are cases when it is meaningless to compare ternary and binary classifications, for example, when “rigid or automatic (non-interactive) classification is required.” However, there are situations where both strategies may be used and it is meaningful for the users to understand the differences between them. In such cases, as we have shown, ternary classification may lead to lower cost or error. The general principle—there does not exist a model that is universally better than another model—applies to ternary and binary classifications. Ternary model only works better under certain conditions or in certain situations. It is necessary to study the conditions under which ternary classification is a better strategy.

@&#CONCLUSIONS@&#
This paper focuses on a framework for studying Bayesian approaches to rough sets. By reviewing existing studies, we emphasize two different classes of Bayesian approaches. Bayesian classification rough sets are intended for making classification decisions based on available information. Bayesian confirmation rough sets are intended for weighting pieces of evidence given by equivalence classes. Both models can be studied with respect to three fundamental issues. For determining thresholds, we have a systematic method for a Bayesian classification rough set model according to Bayesian decision theory. However, determination of thresholds for a Bayesian confirmation rough set model remains to be an open problem. For estimating probability, we present a naive Bayesian method, which is applicable to both classification and confirmation models. For application of three regions, we discuss a three-way decision interpretation of a classification model. The three regions are used to construct a ternary classifier. The same procedure may also used in a confirmation model. However, the physical meaning of three-way decisions is different.Like studies on rough sets in general, some studies on Bayesian approaches to rough sets concentrate on mathematical constructions and formal properties of various notions. In order to fully explore the implication of Bayesian approaches to rough sets, it is important to examine closely semantics differences among different models. Each of the probabilistic models introduces three approximation regions. Although three regions are similar in form, they have different semantics interpretations and therefore are suitable for different applications. Future studies on Bayesian approaches to rough sets need to pay more attention to semantics.Our preference to a separation of two classes of PRS models is from a consideration of sound semantics and another consideration of simplicity. The parameterized model suggested by Greco et al. (2005)2008) provides another promising direction of research. Several challenges faced by the parameterized model are worthy further investigation. One challenge is the complexity introduced by the inter-relations and inter-actions of four thresholds. Another challenge is a sound interpretation of trade-off controlled by four thresholds on conditional probability and a Bayesian confirmation measure. A third challenge is a study of physical semantics of the approximations. Once we have satisfactory solutions to these challenges, we would have a single, more generalized Bayesian PRS model, instead of two classes of models as presented in this paper.