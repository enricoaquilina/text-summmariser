@&#MAIN-TITLE@&#
Component evolution analysis in descriptor graphs for descriptor ranking

@&#HIGHLIGHTS@&#
We show that image/video datasets and descriptor performance can be efficiently represented by random geometric graph models.We show that analysing the phase transition of such graph models can be used for descriptor ranking.We present a ranking function for graph analysis that can be used for automatic feature selection and descriptor evaluation.Although the presented scheme is descriptor-independent, we evaluate and validate the approach on image/video datasets.The goal is to build an evaluation framework where descriptors can be analysed for automatic feature selection.

@&#KEYPHRASES@&#
Descriptor evaluation,Feature extraction,Feature selection,Graph representation,Graph components,

@&#ABSTRACT@&#
This paper presents a method based on graph behaviour analysis for the evaluation of descriptor graphs (applied to image/video datasets) for descriptor performance analysis and ranking. Starting from the Erdős–Rényi model on uniform random graphs, the paper presents results of investigating random geometric graph behaviour in relation with the appearance of the giant component as a basis for ranking descriptors based on their clustering properties. We analyse the phase transition and the evolution of components in such graphs, and based on their behaviour, the corresponding descriptors are compared, ranked, and validated in retrieval tests. The goal is to build an evaluation framework where descriptors can be analysed for automatic feature selection.

@&#INTRODUCTION@&#
Content based retrieval in large video/image datasets is highly dependent on the choice of discriminating features and efficient index structures. Recent approaches involve graph clustering, clique searching, and component analysis methods. Open issues remain how to build the graphs (selection of edges and weights), and how to navigate them efficiently (neighbourhood search). We propose and work towards proving that graph theoretic approaches can be useful in content based retrievals, for descriptor evaluation and automatic feature selection. We build our approach on the investigation of entity difference distributions according to several descriptors and analysing their relations and behaviour during component formulation and the appearance of the so called giant component in the graphs of the descriptors. As we will detail later, as the novelty of our approach, our goal is to exploit the inherent properties of the graph representations to evaluate descriptors based on the behaviour of their graphs during the formulation of the giant component, analysing their discrimination capabilities. The presented method has some connections to graph clustering methods in the sense that the effects of the descriptors on the structure of their graphs is related to their clustering properties.When searching for similar content in video/image datasets, we need to apply feature extractors that gather information about the content and structure of the stored data, and use that information to create a searchable index for the dataset, which in turn will be the basis of searching for similar content. However, there are a lot of different descriptors, and usually it is very hard to select those, that perform well for a given dataset, when using them to produce retrieval results. Our purpose is to help this process by providing a means to evaluate a set of descriptors for a given set of classes and data, and to find a combination of descriptors that perform better. This information can then be used to create more efficient indexes and produce higher precision retrievals.Feature selection in the presence of irrelevant features (noise) is presented in [1], taking into consideration sample data points in 2D for boundary selection and investigating the distribution of feature weights in high dimensions. A method for feature selection [2] is based on approximately 1000 features on real videos, using heuristics for feature retention, using the sort-merge approach for selecting ranked feature groups. A method for sport video feature selection is presented in [3]; Setia and Burkhardt [4] present a method for automatic image annotation based on a feature weighting scheme and machine learning; Guldogan and Gabbouj [5], Li et al. [6] present similar approaches for feature selection based on mutual information and principal component analysis. Zhang et al. [7] presents a query by example approach where histograms of point distances are investigated as a basis to show that with increased dimensions the distance distributions tend to be narrower (poor discrimination), and SIFT feature distribution histograms are used to improve clustering and retrieval.Graphs are a natural way of representing data structures, describing interconnections and internal structures of datasets, visualising relations and distances of elements, and finding subsets, clusters and communities in such structures. Graphs have been widely used for clustering applications, including spectral clustering [8] for graph partitioning, MST (minimum spanning tree) based clustering [9], dense sub-graph mining [10], etc. The uses of graph clustering approaches are various, from generic pattern recognition (e.g., [11,12]), to the recently highly researched community detection approaches in graphs representing social structures [13–16].Contrary to other approaches, we do not use artificial feature weighting or a priori clustering, instead we use real data with multiple features and weigh the built graphs by the points' differences according to features, and investigate the behaviour of the distributions. The goal is to show that this method is a good alternative to previous ones for finding features with higher discriminative properties. In our earlier work [17] we have proposed the use of descriptor graphs for descriptor ranking, and we produced a fitness function for providing such a rank [18]. This work extends these previous results by deeper investigation of the properties of such graph structures, regarding similarity in behaviour and topography, and the use of such intrinsic properties for feature selection.We will start by introducing basic concepts and random geometric graphs (Section 2), followed by the description of the proposed parameters for ranking based on phase transition and component behaviour of descriptor graphs (Section 3), then the presentation of the used datasets and descriptors (Section 4), and finally the presentation of the ranking function and the performed evaluations (Section 5).In this section we overview the properties of two frequently applied random graph models and their component structures. Based on the results corresponding to random graphs, we get a better understanding of the properties of real-world graph structures. Let us start with some definitions of important terminology.Definition 1An undirected graph is aG=(V,E)pair, where V denotes the set of vertices (or nodes) and E denotes the set of edges.E⊆V×Vis a symmetric binary relation on V. The edges represent connections between the vertices of the graph,eij∈Ebeing an edge connecting verticesviandvj.Definition 2The neighbourhood of a vertexv∈VisN(v)={w:(v,w)∈E}. The degreed(v)of a vertex v is the number of its neighbours.Definition 3GraphG′=(V′,E′)is a sub-graph of G ifV′⊆V,E′⊆Eand ifeij∈E′thenvi,vj∈V′.Definition 4IfW:E↦Ris a weight function onG=(V,E), then we say that the graph is weighted and awijweight value corresponds to an edgeeij.Definition 5AG=(V,E)graph is connected, if there is a path between any two vertices. A path is a sequence of vertices in the graph, where neighbouring vertices of the sequence are adjacent in the graph, and a vertex appears only once in the sequence.Definition 6C is a component ofG=(V,E), if C is a sub-graph of G and it is connected. The size of a component is the number of vertices it contains.Definition 7A random geometric graph (RGG) is obtained as follows. We pick n random node position values asX1,X2,...,Xn∈Rd(according to a probability distribution ν onRd, where d is the number of dimensions). We connect two nodesviandvj(i≠j) if their distance‖Xi−Xj‖<rn, the radius of the graph.The theory of random graphs has an important role in discrete mathematics since the early 60's. Besides the theoretically interesting problems, random graphs have proven to be useful in engineering applications as well. Although real-world datasets are usually too complex to mimic each of their properties with synthetic datasets, some important parameters of their structure can be exposed by analysing random graphs. Famous examples are social networks [13,16] and web graph analysis [19,20].The network parameters frequently modelled by random graphs are: the probability of the existence of certain edges of the real graph, the degree constraints, or – in case of weighted graphs –, the weights' distribution. After the model is built, some structural patterns get revealed, such as the number or size of components, cliques, or the occurrence of some special sub-graphs.In our case, random graphs are used to analyse the number and size of components in real graphs. We aim to compare graphs built from test datasets based on a well known phenomenon in random graphs, namely the appearance of the giant component (defined in Section 2.1, Theorem 1). Our test results provide evidence of the existence of a component in these graphs with similar behaviour to the giant component (GC) in random graphs. Besides the properties of the GC in real graphs, we are also interested in the size and number of the components as well.Erdős and Rényi analysed the properties of random graphs with uniformly distributed edges [21]. They considered the evolution of components, while adding randomly selected edges to the graph. The process starts with n vertices and 0 edges, and in each step a randomly selected new edge is added, independently of the already chosen edges. After each step, the size and number of components are studied. During the evolution of the graph, connected components start to appear and, when reaching a critical point, they merge into a so called giant component (GC).The Erdős–Rényi model (ER-model) was originally described by the number of vertices and edges at a given step of the evolution:G(n,e), where n denotes the number of vertices, and e is the number of edges. Recent results connected to this problem are formulated using the number of vertices and the p probability of the existence of an edgeG(n,p). If the edges are selected independently, this formulation gives the same result (as the above(n,e)description), and p is usually described as a function of the number of vertices:p=c/n, where c is a constant. A complete graph with n vertices hasn(n−1)/2edges, that is aG(n,p=1/n)graph (c=1) corresponds to the ER-model withn/2edges.One of the most interesting results of Erdős and Rényi is a theorem [21] that can be formulated as follows:Theorem 1Erdős–RényiThe behaviour of the ER graph model can be divided into three important phases, from the point of view of component sizes (where the size of the largest component is denoted byCmax):1.c<1:Cmax=O(lnn)(the graph contains only small components);c=1:Cmax=O(n2/3);c>1:Cmax=O(n)(the giant component appears), and all other components have sizeO(lnn).The results presented in [21] also deal with the complexity of the components, but now we are interested in their sizes. The important consequence of this theorem is that after a given number of edges, a unique giant component (GC) appears. Below this threshold all components are small – the probability of a component containing a large fraction of the vertices is 0. Above this threshold, the probability of the existence of a GC is 1.This statistical model has a well known application in percolation theory. From the late 50's, the attention was drawn towards cluster size and percolation problems and their applications [22,23]. Let us have an infinite graph of sites (vertices) and bonds (edges), where particles occupy sites, and a site is occupied with a fixed probability p, independently from others. There exists a critical probabilitypcrit, so that ifp<pcrit, all clusters formed by occupied sites will have a finite size, but ifp>pcrit, a cluster with infinite size will appear. It has been proven that the phenomenon described by Erdős and Rényi is the same as this type of so called percolation problem.Although numerous scientific results are connected to the appearance of the GC in the ER-model, its existence was investigated in more complex random graph models as well, but there are still several open questions.Besides the mentioned classical random graph models, several versions have been published for modelling certain properties of complex real networks. In our case, where the graph models images/videos and their distances, the most suitable model is the geometric graph. In this model, the edge weights correspond to pairwise distances of nodes based on a given metric.The random geometric graph model (see Definition 7) offers a solution that mimics real network properties in a synthetic environment. The existence of the giant component (GC) in random geometric graphs has also been examined. A radius threshold is used to select graph edges: edges with a weight lower than the radius are selected [24–26]. The thermodynamic limit, a term from statistical physics, was also used to describe this phenomenon. This limit corresponds to the critical radius of the RGG:rn∼c⋅n−1/d. At this limit, the expected value of the average degree in the graph tends to a constant (c). Above a certain c, a GC is likely to appear. The existence probability and the uniqueness of this GC is the same as in the ER-model. Another important point worth mentioning is, that this model has a strong connection with Poisson processes as well [27,28].An example of a 2-dimensional RGG of 600 vertices is presented in Fig. 1(a). Fig. 1(b) shows the distribution of the edge weights, while Fig. 1(c) illustrates the ratio of the sizes of the second largest to the largest components while adding edges to the graph by their increasing weights (until the GC appears).In this section we present the proposed graph analysis scheme based on investigating the component evolution process and the phase transition in descriptor graphs, and the proposed behaviour parameters that we will use in the ranking/fitness function for automatic descriptor ranking.The previously introduced examples (ER-model, RGG) are both statistical models. They have important roles in studying the behaviour of some network parameters such as the evolution of components in the graph. However, from several points of view, these models are only simplifications of real networks. In this section, we will present some of the issues of analysing real-world datasets.The appearance of the GC in real networks with geometric restrictions on the edge weights is an interesting mathematical topic of its own. The number of vertices and the number of dimensions are relevant parameters to determine the circumstances of the appearance of the GC in an RGG. It is essential to note that the definition of the RGG contains a restriction on the distribution of vertex coordinates, i.e., the coordinates are uniformly distributed in each dimension. This restriction might be acceptable in certain applications such as sensor networks that can be modelled by a low dimensional (2D or 3D) geometric graph, but in most application areas, the positions of the vertices are not that structured.In case of image/video datasets, the vertices are placed in a descriptor space with significantly larger number of dimensions, and we have no a priori knowledge on the distribution of coordinates. As it was mentioned in Section 2.2, even in the case of uniform distribution, the exact place of appearance of the GC is unknown if the number of dimensions is high. There have been very few results published on the number and size of components in case of non-uniform weight distributions. To the best of our knowledge, the existence and the circumstances of the appearance of the GC in arbitrary geometric graphs are still open questions.We analyse descriptor graphs to find out whether the appearance of the GC is traceable, and if it is, whether it is descriptor-dependent. Our objective is not the solution of the above mentioned general theoretical questions, but to study the importance of the component sizes in real world applications in case of arbitrarily structured graphs.The behaviour of the descriptor graphs during the process of GC formulation is an indirect way of analysing the clustering properties of a descriptor. Appearing components are basically clusters of nodes that are close together according to the used metric. How these components evolve (appear and merge), is an indicator of the performance of the descriptor. Additionally, how and when the GC appears (visualised by phase transition and component evolution graphs) is in direct connection with the descriptor's performance. E.g., if the GC appears early at a point where we only have very small or very few components, then the used descriptor is probably not a good choice for describing the dataset (low discriminative properties). Also, we expect that descriptors with similar phase transition and component evolution will perform similarly in a retrieval process.In order to prove our assumptions, besides the place (critical weight) of the phase transition we also investigate the components' evolution process with regard to the applied weight thresholds. Fig. 2shows the behaviour of a descriptor graph in a 3D visualisation, where the axes are the edge weight thresholds (th), the sizes of existing components at a given threshold (comp.sizes), and the number of such sized components at a given threshold (no.comp.). We are looking for locations where there are sudden jumps in component sizes during the increase of the weight threshold (i.e., when smaller components suddenly merge into a large one), as an indicator of the estimated place of the appearance of the GC. This critical weight is a main area of interest in these graphs – such a region is shown with the big black arrow in Fig. 2(a). For the example in Fig. 2(a), Figs. 2(b), (c) show the visual structure of the descriptor graph right before (b) and at (c) the appearance of the GC (visualised with Gephi [29]).Similarly to random geometric graphs, we build the descriptor graphs by using the dataset elements as nodes, and the distance between them – according to a selected descriptor – as edge weights. Thus, we will have a complete (i.e., fully connected) graph for each descriptor. Then, we use these graphs to evaluate the performance of the corresponding descriptor by analysing the components. The main steps of our approach are the following:1.Calculate thed(vi,vj)distances of allvi,vjnode pairs for all descriptorsdk.For each descriptordk,(a)Start building the graph,G(V,E), with vertex setV={vi|i=1...n∈N}and weighted edge setE={ek|ek=(vi,vj),i≠j,vi∈V,vj∈V},|E|⩽n⋅(n−1)/2, with weightsw(ek)=d(vi,vj)by gradually increasing the weight thresholdth∈[0,1]∩Rand including all edges whose weightsw(ek)⩽th.Merge components that have become connected by inserted edges, and iterate steps (a)–(b).During the iterations in steps 2 (a)–(b), monitor the number and size of components (phase transition and component formulation) to find the appearance of the giant component (GC). Calculate parameters that describe this process and can be used to rank the descriptors.Although the place of the appearance of the GC cannot be exactly determined, the tests prove the existence of a single dominant component during the edge addition process. The parameter to estimate this critical threshold will be the ratio of the sizes of the second largest to the largest component. The estimation will be the weight threshold where this ratio decreases below 0.1, and will not exceed that threshold later.Fig. 3shows, as an example, visual snapshots during the component behaviour evaluation of the edge histogram descriptor. These visualisations have been produced by taking the component statistics from certain steps of the process and producing connected groups of vertices. Here, the locations of the vertices have no real meaning, the point is to visualise the number and sizes of components as they appear during the process. Fig. 14 below shows visual excerpts from components during the descriptor graph building process, elements from the same components grouped together.Above, we presented the graph building process. However, in case of larger datasets, we modify the process to gain a less detailed analysis, while keeping the significant steps. Instead of selecting single edges, we add all edges with the same weight in one step, and the critical edge number threshold is replaced by the critical edge weight threshold.Test results of the critical threshold estimation are presented in Fig. 4. The size of the largest component of the graph (normalised with the number of vertices to show a common scale) is tracked during the building process of the descriptor graphs and its evolution is presented versus the increasing weight thresholds in the phase transition graphs of Figs. 4(a) and (c). The largest component grows rapidly within a small weight range in case of all descriptors, which is an expected behaviour based on the general theoretical knowledge regarding the evolution of RGGs and the existence of a GC. Although this parameter is insufficient on its own to distinguish between descriptors – e.g., the evolution of the largest components of some descriptors might be similar –, the functions clearly shows the sudden emergence of a large component (note the steepness of the curves in a short weight range). However, the GC phenomenon also means that this large component becomes unique. To prove that the components of the descriptor graphs meet this requirement, we also observe the changes in the ratio of the sizes of the second largest to the largest components (Figs. 4(b) and (d)). Using both these parameters, most descriptors will show significant differences. For space considerations, a selected number of descriptors for two of the used datasets are displayed in Fig. 4, so as differences in behaviour can be visually observed, showing by example that indeed the phase transition curves of different descriptors show discriminable differences. However, since the presented method is descriptor-independent, the selection of the descriptors does not really matter, only that their differences can be observed and determined.Critical edge weights for some of the descriptor graphs on the CDB7k dataset (for details see Section 4) are shown in Fig. 5(a). As it shows, the critical weights depend on the number of vertices of the graph, but the impact of this parameter depends on the descriptor. Detailed behaviour of the critical weight values of the focus region descriptor over the CDB7k dataset are shown in Fig. 5(b).However, there is a third parameter that should be considered: the number of components near the critical threshold where the GC suppresses the others (e.g., Fig. 6). The edge histogram-based graph near this weight value consists of more components than the one corresponding to the average colour descriptor, which means that before the largest component becomes dominant, it performs better in dividing the vertices. This is exactly the property that can serve as the base for automatic selection of better discriminating descriptors.To conclude the above discussed graph properties, we summarise the interesting parameters of descriptor graphs that we use to produce the automatic descriptor ranking:•wcrit, the estimated critical weight where the GC appears, expecting that performance should be better if the GC appears at a later stage,|C2|/|Cmax|, the ratio of the second largest to the largest component sizes at the appearance of the GC. A high ratio means that the GC was formed from larger components, a lower one means a high number of smaller components existed before the GC. This ratio is a direct controller of the graph building process, stopping when it reaches below a specified threshold (typically10%). Graphs showing the behaviour of this ratio for some descriptors are shown in Figs. 4(b) and (d).nrcomp/n, the number of components (normalised with the number of vertices n) at the appearance of the GC, relating to how the nodes have been encompassed in components.For evaluating the viability of the proposed approach, we use various datasets. One is the publicly available MIRFLICKR25000 dataset22Detailed description can also be found at http://press.liacs.nl/mirflickr/.[30] (denoted by MIRFLICKR25k), which contains 25 000 images gathered from Flickr, along with tags, and is roughly partitioned into 24 categories with lots of overlaps. Another dataset we used is our own video and image dataset (denoted by CDB7k), which contains approximately 7000 video segments (515 min total length) collected from television captures in 13 categories, e.g., sports, nature, cartoons, music, cooking, news, street surveillance, outdoor, indoor (some examples in Fig. 7). The videos were automatically cut into shots and manually labelled into categories. For each shot a representative frame was automatically extracted. Video features are extracted from the shots, while image features from the representative frames of the shots. Another dataset is the publicly available University of Washington (UW) dataset33http://www.cs.washington.edu/research/imagedatabase., which contains 1333 images in 22 categories with annotations. The last dataset is the INRIA Holidays (IH) dataset [31]44http://lear.inrialpes.fr/~jegou/data.php., which contains 1491 images in 500 categories.We extracted all the features for images and video segments for all dataset elements from CDB7k, WU and IH. In the case of the MIRFLICKR25k dataset, we use 8000 elements for descriptor evaluation and ranking, and use the full dataset for evaluating retrieval performance.For evaluating features for general distribution and content differentiation, we selected a set of various descriptors, namely: average colour (custom descriptor that produces average representative colour values over image areas, denoted by d1), curvelets [32] (d4), focus regions [33] (d7), local binary patterns (LBP) [34] (d9), MPEG-7 descriptors [35] – colour layout (d2), colour structure (d3), dominant colour (d5), edge histogram (d6), homogeneous texture (d8), motion activity (d11) –, average motion (custom descriptor gathering average representative motion directions over frame areas, denoted by d12), PHOG – pyramid of histograms of orientation gradients [36] (d10). However, there is no limit on the number of descriptors that could evaluated in our framework. For calculating the difference between images/videos we used Euclidean distance calculations, i.e., for a feature all elements can be displayed along a 1D axis from 0 todmax(D)(maximal difference for a descriptor) and they adhere to the triangle inequality.We discussed the important parameters of the components of the descriptor graphs in Section 3.3. Here, we present the suggested ranking function based on these parameters. The parameters can be weighted depending on the dataset, the number of classes we have, and the level of classification we target. In [18], we introduced a fitness/ranking function that combines these parameters into a formula:(1)F(⋅)=w1⋅wcrit+w2⋅|C2|/|Cmax|+w3⋅nrcomp/n,wherew1,2,3are weights that are manually specified and are constant for a given dataset (we intend to work on creating a process for adaptive weight selection in the future). Further on, we use this formula to produce a descriptor ranking and perform the evaluations, with weights always (for all cases and all datasets) set to 0.7, 0.2, 0.1, respectively, giving higher importance to the critical weight where the GC appears, but also taking into account the other parameters, the importance of which was discussed in Section 3.3.It is important to note, that the calculation of the above function does not depend on the used dataset, or the used descriptors, but only on the structure of the analysed graphs, and can be calculated without an a priori training or labelling process. In our experiments, we only used the class labels in the datasets to evaluate the performances of the retrievals. However, the described descriptor ranking process could be also used to select better performing descriptors not only on a ‘blind’ dataset, but on selected sub-categories as well, which could help in establishing descriptor pools for any particular content (sub-)class.We calculated the F fitness values for the participating descriptors to produce a ranking based on the graph analysis results, taking into consideration the GC appearances in the respective descriptor graphs. Such ranking results are shown in Fig. 8for the used datasets.In [18] we have shown that the produced descriptor ranking based on the above fitness function produces a ranking close to the exhaustive ground truth ranking, while at the same time has important benefits:•it does not require exhaustive evaluation for all categories and all descriptors in the dataset to produce a ranking,it can produce a descriptor ranking for a given dataset while the method itself is independent of the number of categories and descriptors, providing a ranking based on the discriminating properties of the descriptors.We evaluate the proposed framework by running retrievals on the used datasets. As a baseline, we use our parallel multi-tree indexing scheme [37] with and without exploiting the obtained ranking/fitness information, and compare the results. This retrieval uses a scheme in which the search is done for each included feature in parallel and the results are combined in a form of result aggregation process. However, any other similar indexing-retrieval scheme could be used just as well.Overall, retrievals using the produced descriptor ranking can produce results that have similar or higher precision using a reduced number of descriptors, and the results contain lower variation in the number of categories that are irrelevant (not belonging to the query's category). In practise this means that in the case of rank-based retrievals the responses contain more relevant results, essentially decreasing the ‘noise’ of the retrievals (showcased by Figs. 9 and 15).The above properties are shown in Fig. 9 for the CDB7k dataset, where 4 queries are used with a fixed result retrieval number of 50 and 100 (the first 50 and 100 best matches are returned), and using two retrieval approaches: v1, where results are retrieved without the produced descriptor ranking information (results are generated using all available descriptors), and v2, where results are retrieved using the produced ranking (results are generated using the first 7 best performing descriptors). In the case of v2 retrievals, there might not just be an improvement in precision (the results are more relevant), but the remainders of the results show less variation (i.e., results were generated by descriptors with better clustering properties). This is shown by ‘cX’ labels along the columns, representing the labels of classes that make up the irrelevant results. Fig. 15 shows visual results for the two types of retrievals, showing the difference between the responses, where v2 (with rank) was only16%better, and the images in the shown sample all belong to the same class (“sport”), however, the visual consistency of retrievals based on descriptor ranking is much higher.For evaluating the eventual (expected) improvement in retrieval precisions (ratio of relevant results vs. all retrieved), we ran several queries, retrieved the closest 50 data points, and calculated the retrievals' precision. Fig. 10shows results on the used datasets. Here, v1 again refers to retrievals without the use of ranking information (all descriptors are used), and v2 refers to retrievals which use the ranking data, only using the top 7 better performing descriptors in the retrieval process. In general, we can say that ranking information preserves or improves precision, Figs. 10(b), (e), (h), (k) show the differences in precision values. For CDB7k, in52%of the retrievals the precision improved and in22%it remained the same, producing on average increase of10.2%, while in the26%of the cases where the precision was lower, the average decrease was only5.8%. Fig. 10(c) shows average precisions (AP) calculated for the tests in (a) with queries belonging to the same class grouped and averaged. For UW (g)–(i), the precision was better or equal in80%with an average increase of5%.Figs. 10(d)–(f) show the precision values for 56 queries over the MIRFLICKR25k dataset, with one important note: we only used 8000 of the dataset's 25 000 elements to create the descriptor ranking, but used the whole dataset in the retrieval process. Even in this case, the v2 results are better or similar in62.5%of the retrievals. This in turn means, that the presented method is applicable to large datasets, by using a smaller sub-set to create the ranking.Fig. 10 also shows another important aspect and goal of the proposed method: that, when using ranking information, we can achieve similar of better performance in the retrievals, but with a reduced set of descriptors, which in practise translates into a more streamlined and lightweight retrieval scheme.The visual retrieval example (based on the CDB7k dataset) in Figs. 15(a)–(b) show that while the precision of v2 (with rank) was only16%better, the visual consistency of the retrievals based on descriptor ranking is better (the used query is shown as the top-left image in both cases).We also performed tests to see whether descriptors with similar properties behave similarly during a retrieval process (e.g., produce similar precision retrievals). This is an important issue when designing the collection of descriptors to use for a particular dataset, or when designing bag of words (BOW) retrievals, since if we know that some descriptors behave in a similar manner, then we do not have to use more or all of them, but we can select the best performing one. This in turn can make the retrieval process more lightweight. We selected 10 random queries from 5 different classes, and ran retrievals using only one selected descriptor at a time. Fig. 11shows average precision (AP) values of these test runs (averaging 10 retrievals for each descriptor, using the CDB7k dataset as an example), where the first group of descriptors was selected from the middle of the phase transition graph (Fig. 4(a)), and the other group from the left region, showing distinguishable difference between the two groups. This supports the expectation that phase transition properties are in correlation with internal topological parameters of the descriptor graphs, which could be a topic of continued research in the area.In order to show on a proof-of-concept level, that the proposed method could also be usable in other types of retrieval frameworks, we evaluated the performance of ranked descriptors in a bag of visual words type of retrieval test. Based on the IH dataset used earlier, we created a bare-bones retrieval using descriptors d10 (PHOG) and d9 (LBP), which were ranked better and worse, respectively (for the IH dataset, see Fig. 12(a)). We tested the retrieval using 5000 and 10 000 visual words, built separately for the two descriptors. We extracted feature points from the images, and used a fixed32×32region size around each point to extract the descriptors. For comparing the resulting visual word histograms during the retrieval, we used two standard metrics: the earth mover's distance (emd) and theχ2metric (chi2). We ran the same 30 retrievals as for Figs. 10(j)–(l), and we expected that retrievals using the d10 descriptor would perform better than using d9. Results are shown in Figs. 12(b)–(c). Fig. 12(b) shows the average precision (AP) values for the retrievals, using 5000 visual words and the two descriptors (d10, d9), the two metrics (emd, chi2) and their averages (avg). Fig. 12(c) shows results for the same dataset, descriptors and metrics, comparing the results of using visual dictionaries of 5000 (5k) and 10 000 (10k) size. The results also support the expected behaviour: ranking relates to performance.Regarding the computational complexity of the proposed descriptor graph analysis approach, Fig. 13shows normalised (by the number of nodes) computational times for all used datasets. In the worst case, the computation time is proportional ton2(n being the number of vertices); however, this can be reduced by more optimised coding, which was not our primary goal here.

@&#CONCLUSIONS@&#
We proposed a data- and descriptor-independent evaluation framework for descriptor evaluation and feature selection, exploiting descriptor graph behaviour analysis results with regard to the giant component formulation process. Any kind of descriptors can be evaluated this way, by providing their feature extraction algorithm and a metric to compare the features, and the datasets also can be other than visual (images/videos). The goal is to provide a means to select better performing descriptors for a given dataset, reducing the number of necessary descriptors, and producing more relevant results. We have presented relevant parameters and proposed a fitness function to rank descriptors. We experimentally showed, that the observation of the phase transition process is an effective and usable method for descriptor characterisation. We performed evaluations to explore the practical viability of the suggested approach. As a practical continuation, we plan to apply the method to other large datasets, with the ranking step performed on a subset of content classes. We also intend to work on making the weighting of the ranking parameters adaptive, to further improve the descriptor discrimination and ranking results. In its current form, the proposed descriptor ranking method is suitable for automatic feature selection in image and video datasets.