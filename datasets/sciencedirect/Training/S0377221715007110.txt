@&#MAIN-TITLE@&#
New bounding and decomposition approaches for MILP investment problems: Multi-area transmission and generation planning under policy constraints

@&#HIGHLIGHTS@&#
We propose a novel methodology to solve investment-planning problems.The method enhances bounding algorithms and Benders decomposition.Combining both methods is more effective than using them separately.We show an application to investment planning in power systems.

@&#KEYPHRASES@&#
OR in energy,Stochastic programming,Benders decomposition,

@&#ABSTRACT@&#
We propose a novel two-phase bounding and decomposition approach to compute optimal and near-optimal solutions to large-scale mixed-integer investment planning problems that have to consider a large number of operating subproblems, each of which is a convex optimization. Our motivating application is the planning of power transmission and generation in which policy constraints are designed to incentivize high amounts of intermittent generation in electric power systems. The bounding phase exploits Jensen’s inequality to define a lower bound, which we extend to stochastic programs that use expected-value constraints to enforce policy objectives. The decomposition phase, in which the bounds are tightened, improves upon the standard Benders’ algorithm by accelerating the convergence of the bounds. The lower bound is tightened by using a Jensen’s inequality-based approach to introduce an auxiliary lower bound into the Benders master problem. Upper bounds for both phases are computed using a sub-sampling approach executed on a parallel computer system. Numerical results show that only the bounding phase is necessary if loose optimality gaps are acceptable. However, the decomposition phase is required to attain optimality gaps. Use of both phases performs better, in terms of convergence speed, than attempting to solve the problem using just the bounding phase or regular Benders decomposition separately.

@&#INTRODUCTION@&#
The electric power industry is a major applications area for optimization (Hobbs, 1995). This sector comprises over 2 percent of the U.S. economy, and recent restructuring has strengthened incentives to plan and operate power infrastructure efficiently. Increasing amounts of generation from renewable resources makes optimization of short-term operations and long-term planning more challenging, thus promoting the development of new decision-support tools to account for renewable variability and unpredictability. For instance, stochastic unit commitment models that explicitly factor in uncertainty in the availability of supply from wind and solar generators often yield lower dispatch costs when compared to traditional deterministic unit commitment approaches. However, these cost reductions come at the expense of higher computational complexity (Bertsimas, Litvinov, Sun, Zhao, & Zheng, 2013; Papavasiliou & Oren, 2013). Because they consider both investment and operations, investment planning models present even greater computational challenges. First, resource-specific characteristics, such as locational constraints and distance from load centers and the existing transmission grid, require analysis of both transmission and generation investment alternatives on a system-wide basis. Second, failure to capture the variability and spatial correlations among intermittent resources will likely result in suboptimal investment recommendations (Joskow, 2011). In this paper, we develop practical approaches to solve multi-area generation and transmission investment planning problems that account for the aforementioned challenges.Because of computational limitations, as well as uncertainty in long-term forecasts of demand and capacity factors of intermittent resources, investment planning models have traditionally avoided fine-grained representations of short-run production costs (Palmintier & Webster, 2011). To achieve computational tractability, investment planning instead utilized deterministic or probabilistic models for calculating production costs based on load-duration curve approximations (Hobbs, 1995; Kahn, 1995). These models usually approximate the load or net-load duration curves using a small number of categories (e.g., peak, shoulder and off-peak demand), ignore spatial correlations between demand zones and intermittent generation across multiple regions, and do not model time dependencies in operations, thereby ignoring ramping constraints and start-up costs. Early planning models only considered single-area load duration curves based on time-series of historical and forecasted data (Anderson, 1972; Booth, 1972). These were later improved, e.g., through the use of Gram−Charlier series (Caramanis, Tabors, Nochur, & Schweppe, 1982), to account for the effect of non-dispatchable generation technologies, such as wind and solar, on the optimal generation mix. A simple approach to the latter determines operating hours to be simulated via moment matching on demand, wind, solar, and hydro data (van der Weijde & Hobbs, 2012). In this approach, the sample of hours that best approximates the means, standard deviations, and correlations of the data is selected to determine the optimal portfolio of transmission and generation investments. Palmintier and Webster (2011), Shortt, Kiviluoma, and O’Malley (2013), and de Sisternes and Webster (2013) proposed further refinements of the use of load-duration curves in planning models considering unit commitment variables and constraints. Yet these were only applied to generation and not transmission planning. None of these approximation methods, however, provide metrics (e.g., bounds) to quantify the effect of the quality of the approximations on the resulting investment plans and total system costs. Therefore, they can only be deemed as heuristics.Large-scale applications and computational limitations have historically motivated researchers to solve generation and transmission planning models using Benders decomposition (Bloom, 1983; Bloom, Caramanis, & Charny, 1984; Huang & Hobbs, 1994; Pereira, Pinto, Cunha, & Oliveira, 1985; Sherali & Staschus, 1990; Sherali, Staschus, & Huacuz, 1987). Such approaches separate the investment problem (i.e., master problem) from the production cost problems (i.e., subproblems), which can then be solved by independently taking advantage of parallel computer systems. The quality of the investment plans proposed by the master problem is improved by iteratively evaluating their performance against the production cost models, which also provide marginal cost information that is subsequently used in the master problem. Benders decomposition also provides bounds upon the optimal system costs for each candidate investment and its convergence is guaranteed under certain conditions (Geoffrion, 1972). However, these bounds cannot be guaranteed as valid if only a few observations of demand, wind, solar, and hydro data are considered in the subproblems, as is often the case in planning studies. Furthermore, convergence of the algorithm is often slow, which has prevented its widespread utilization among practitioners, although acceleration techniques have been an important subject of research (Magnanti & Wong, 1981; McDaniel & Devine, 1977; Sahinidis & Grossmann, 1991). Finally, consideration of environmental constraints, such as imposition of minimum annual amounts of generation from renewable resources, impedes the parallel solution of the subproblems. These constraints couple the solutions for distinct hours, which then all need to be considered simultaneously in the optimization problem. This imposes a computational restriction on the level of granularity of the market operations representation.Transmission expansion is a particularly challenging aspect of infrastructure planning in power systems that has received significant attention from researchers. Latorre, Cruz, Areiza, and Villegas (2003), Lee, Ng, Zhong, and Wu (2006), and Hemmati, Hooshmand, and Khodabakhshian (2013) provide comprehensive surveys of the most significant advancements. Recent contributions in the area focus on enhancing models to account for uncertainty and optionality (Konstantelos & Strbac, 2014; Moreno, Pudjianto, & Strbac, 2013; Orfanos, Georgilakis, & Hatziargyriou, 2013). New modeling approaches with potentially efficient solution algorithms for large-scale applications have been proposed by Lumbreras and Ramos (2013), Asadamongkol and Eua-arporn (2013), Desta Zahlay, De Cuadra, Olmos, Rivier, and Perez-Arriaga (2013), Ozdemir, Munoz, Ho, and Hobbs (2015). However, most of these algorithms have only been applied to small test-cases or to large instances but with a very small number of dispatch scenarios.In this paper, we develop a computationally-tractable algorithm to generate candidate transmission and generation investment plans, as well as bounds upon the minimum system costs. We propose a two-phase approach based on a bounding algorithm (Hobbs & Ji, 1999) and Benders decomposition, both of which provide bounds on the expected system costs. In Phase 1, a lower bound is computed by solving a low-resolution planning problem using clustered observations of time-dependent demand, wind, solar, and hydro data, based on an extension of Jensen’s inequality for stochastic programs with expectation constraints. Upper bounds are estimated using a sub-sampling method to approximate the operations costs for each candidate investment plan proposed by the lower-bound planning problem. These bounds are progressively tightened by refining partitions of the space of time-dependent load and renewable energy data. Due to the asymptotic properties of our algorithm, however, tight optimality gaps may only be achieved in the limit, requiring very fine partitions of the data that result in computationally expensive lower-bound planning problems. To overcome this difficulty, we propose a second phase (Phase 2) to the bounding approach that uses Benders decomposition with an auxiliary lower bound to close the optimality gap. This is the first solution approach capable of solving multi-area transmission and generation planning problems with expectation constraints, while providing bounds on the optimal total system costs. We apply that algorithm to a realistic large-scale representation of a power system in the U.S. Our approach can be generalized to other stochastic programs with both per-scenario and expectation constraints, such as optimization problems with CVaR constraints in finance (Krokhmal, Palmquist, & Uryasev, 2002).The rest of this paper is organized as follows. In Section 2, we describe an abstract planning model that is formulated as a stochastic mixed-integer linear program with per-scenario and expectation constraints. In Section 3, we extend Jensen’s inequality in order to compute lower bounds for a stochastic problem with expected-value constraints and describe a statistical method to compute upper bounds that takes advantage of parallel computer systems. Section 4 describes our implementation of Benders decomposition, including the introduction of auxiliary lower bounds in the master problem to accelerate convergence. In Section 5, we illustrate the performance of the proposed bounding and decomposition algorithms on a transmission and generation planning study of a 240-bus representation of the Western Electricity Coordinating Council (WECC). The WECC is the largest synchronized power system in the U.S., comprising 14 western states as well as portions of Alberta, British Columbia, and Mexico. Conclusions are presented in Section 6. Proofs of all propositions together with details of derivations are provided in the electronic supplementary material.We focus on investment planning models that can be formulated as linear or mixed integer linear programs.11Modeling unit commitment variables and constraints or AC optimal power flows yields non-linear and non-convex operations models. Their use in long-term investment models has been limited to research applications on small test-cases.Examples of such models include: Caramanis et al. (1982), Bloom (1983), and Sherali and Staschus (1990) for generation expansion planning; Binato, Pereira, and Granville (2001) for transmission expansion planning; and Pereira et al. (1985), Dantzig et al. (1989), van der Weijde and Hobbs (2012), and Munoz, Hobbs, Ho, and Kasina (2014) for composite transmission and generation expansion planning. Other electricity investment planning market simulation models that are commonly used for energy and environmental policy analysis include IPM (ICF, 2013), the Electricity Market Module of NEMS (Gabriel, Kydes, & Whitman, 2001), ReEDS (Short et al., 2011), Haiku (Paul & Burtraw, 2002), and MARKAL (EIA, 2013).22There also exist more sophisticated modeling approaches based upon game theory that consider the strategic interactions between transmission and generation investments (Pozo, Contreras, & Sauma, 2013a; Pozo, Sauma, & Contreras, 2013b). However, the development of solution approaches for such models is beyond the scope of this paper and is left to future work.We now define the main notation used in the paper. Additional parameters and variables will be introduced as needed.ParametersACoefficient matrix associated with investment constraintsRight-hand-side vector associated with investment constraintsVector of marginal generation and curtailment costsRight-hand-side vector associated with expected-value constraintsVector of transmission and generation capital costsFixed recourse matrix associated with expected-value constraintsCoefficient matrix associated with investment variables in operations problem. Also known as a transition matrix. This matrix includes scenario- or time-dependent parameters such as hourly levels of wind, solar, and hydro power productionDiscrete probability space composed of the sample space Ω and the probability measure p(·) over Ω. For planning purposes, this space can, for example, be constructed using 8760 historical observations of hourly demand, wind, solar, and hydro data from a representative year (i.e.,|Ω|=8760). Each event ωiwould then have probability of occurrencep(ωi)=1/8,760,∀ωi∈ ΩRight-hand-side vector of constraint parameters for scenario ωFixed recourse matrixVector of generation and transmission investment variables. Some investment decisions are often modeled as binary (e.g., transmission investments) while others are modeled as continuous (e.g., generation investments)Vector of power generation levels, power flows, phase angles, and demand curtailment variables for each realization of ω in ΩThe basic goal of a planning tool is to provide a recommendation of where and when to invest in new transmission and/or generation infrastructure, given a distribution of forecast operating conditions that we model with the probability space (Ω, p).33The loss of model fidelity in assuming that we can anticipate the full horizon of uncertain load and renewable generation supply will, in general, depend on the penetration level of time-dependent resources, the quality of forecasts actually available to operators, and the practical consequences of those forecasts in terms of, e.g., ex-post sub-optimal generator unit commitment decisions. Although we have not quantified the economic implications of this simplification, a relatively simple approximation to account for this error is to enforce a percentage of operating reserves as a function of renewables available in the system. This approximation is used in the ReEDS model by the National Renewable Laboratory (Short et al., 2011).We formulate the planning problem as the following stochastic mixed-integer linear program:(1)TC((Ω,p))=minxeTx+f(x,(Ω,p))(2)s.t.Ax≤b(3)x=(x1,x2),x1∈{0,1},x2≥0.The function f(x, (Ω, p)) denotes the minimum expected operating costs for a given set of investments x and scenarios described by (Ω, p).44The focus of the algorithms proposed in this paper is on models for economic planning, which aims at finding the most cost-effective combination of generation and transmission investments to meet demand and policy objectives assuming risk neutrality (minimization of expected system cost). Although reliability requirements are core drivers of infrastructure investments–particularly transmission–they need to be evaluated using non-economic tools that focus on the ability to serve load, which differs from the economic-focused tools we consider here (Munoz et al., 2014). Regarding economic risks, there is empirical evidence that suggests that both public and private investors are inherently risk averse. Some alternatives to account for such behavior in planning models include the use of utility functions, a constraint or weight on the conditional value at risk (CVaR), or the use of the Savage minimax regret criterion discussed in van der Weijde and Hobbs (2012). The linearity of the CVaR implementation of Rockafellar and Uryasev (2000) make this metric an attractive alternative to model risk aversion in generation and transmission planning models. A study of the economic implications and algorithmic challenges that result from using a CVaR is, however, beyond the scope of this paper and we leave it as a subject of future research.By adopting a discrete two-stage stochastic programming model, we explicitly assume the availability of a finite, fixed number of operations scenarios. This is not an unrealistic assumption in practice, particularly when operations scenarios are (as in our case) associated with historical observations. However, we consequently ignore issues relating to scenario sample selection and validation. The function TC(Ω, p) denotes the minimum total system cost. The matrix A and vector b define investment constraints such as generation build limits, installed reserved margins per area, and limits on the maximum number of transmission circuits per corridor. The elements in the vector of investment variables x can be defined as discrete (x1, counts of plants or transmission lines at a particular location) or continuous (x2, generation capacity variables, in mega-watts). As in Binato et al. (2001), van der Weijde and Hobbs (2012), Munoz, Sauma, and Hobbs (2013), our application models transmission investments using binary variables and generation capacity using continuous variables. The formulation in (1)–(3) assumes a single investment planning stage, i.e., all investments are “here-and-now” decision variables, while all recourse variables are operations variables. More generally, the solution methods of this paper can be applied to multi-stage planning models, such those described in van der Weijde and Hobbs (2012), Munoz et al. (2013), and Munoz et al. (2014), which can be represented using minor variants of formulation (1)–(3).The objective of the operations problem is to minimize operating costs for a given discrete probability space, denoted (Ω, p). The problem is formulated as a linear program:(4)f(x,(Ω,p))=Miny(ω)Eω[cTy(ω)](5)s.t.Wy(ω)≤r(ω)−T(ω)x∀ω∈Ω(6)Eω[Ky(ω)]≤d(7)y(ω)≥0∀ω∈Ω.In our application, the per-scenario (e.g., hourly) constraints (5) consist of Kirchhoff’s first and second law, maximum generation limits for both conventional and intermittent units, maximum power flow limits, flowgate limits, and ramping constraints. The expectation constraints (6) are used to enforce policy objectives, such as renewable targets or emission limits on a yearly basis (Munoz et al., 2014).One alternative to solving large-scale stochastic programs (Birge & Louveaux, 1997) is to use computationally-tractable approximations that provide lower and upper bounds on the optimal objective function value. Well-known bounds for problems with stochastic right-hand sides include Jensen’s inequality for lower bounds (Jensen, 1906) and the Edmunson−Madansky inequality for upper bounds (Madansky, 1960). These bounds can be progressively tightened by refining the partitioning of the space Ω, until a certain optimality gap is achieved (Birge & Louveaux, 1997; Hobbs & Ji, 1999; Huang, Ziemba, & Ben-Tal, 1977). However, in our case expectation constraints prevent the direct application of Jensen’s inequality, because it is only applicable to separable problems with per-scenario constraints. Computation of upper bounds still involve the solution of large optimization problems, which are sometimes facilitated by the application of decomposition algorithms (Hobbs & Ji, 1999). We now introduce an extension of the Jensen’s-inequality-based lower bound to problems with both per-scenario and expectation constraints (Section 3.1), and describe a sub-sampling method that provides a statistical estimate of the upper bound problem, which we implement on a parallel computer system (Section 3.2).Proposition 1 extends the Jensen’s-inequality-based lower bound to stochastic linear programs with expected value constraints, as is the case in the operations model of Section 2.3. Detailed derivations and proofs are provided in the electronic supplementary material.Proposition 1Given a discrete probability space Ω with measure p and a partitionS1,…,Smof Ω, a sample spaceΨm={ξ1,…,ξm}is defined with measure qm such that the probability of each event ξi in Ψm equals the probability of each subset Si, defined asqm(ξi)=p(Si),∀i∈{1,…,m}. If the vector of right-hand-side parameters r(·) and transition matrix T(·) are computed using the expected value of these parameters over the partitionsS1,…,Smsuch thatr(ξi)=Eω[r(ω)|Si]andT(ξi)=Eω[T(ω)|Si],∀ξi∈ Ψm, then for any vector of investments x, f(x, (Ψm, qm)) ≤ f(x, (Ω, p)).We can interpret this result as follows. If the space Ω is partitioned or clustered into subsets, and if the expected values of these parameters, conditioned on each subset/cluster, are used in the optimization problem and weighted in the objective function in proportion to the cluster sizes, then solving the operations problem f(x, (Ψm, qm)) provides a lower bound on the operations problem f(x, (Ω, p))—which considers the full distribution of time-dependent parameters. If a hierarchical clustering algorithm is used then the bound can be guaranteed to be nondecreasing (i.e.,f(x,(Ψm,qm))≤f(x,(Ψm+1,qm+1)),∀x,∀m∈{1,…,|Ω|−1}) and convergent to f(x, (Ω, p)) (Birge & Louveaux, 1997). Next, Proposition 2 shows how this bound can be used to compute bounds on the optimal total system cost TC(Ω, p) associated with the investment planning problem.Proposition 2Given the conditions described in Proposition 1, TC((Ψm, qm)) is a lower bound on TC((Ω, p)).Further, if a hierarchical clustering algorithm is used, such that f(x, (Ψm, qm))≤f(x,(Ψm+1,qm+1)),∀x,∀m∈{1,…,|Ω|−1},then TC(Ψm, qm) ≤TC(Ψm+1,qm+1),∀m∈{1,…,|Ω|−1}.For any feasible investment plan x,eTx+f(x,(Ω,p))clearly provides an upper bound on the optimal total system cost TC(Ω, p). However, computing f(x, (Ω, p)) could be prohibitive due to the presence of expectation constraints that link all scenarios within the operations problem and which consequently impede a direct parallel implementation on a per-scenario basis. Relaxation of these constraints through Benders or Dantzig−Wolfe decomposition methods can address this difficulty (O’Brien, 2004), but this approach then requires implementation of nested decomposition algorithms within our proposed approach. Furthermore, some investment planning problems require consideration of multi-year time-series, sub-hourly resolution of intermittent data, and scenarios including component failures. These additional features would result in extremely large sample spaces (Ω, p) and, therefore, large operations models that would be difficult to solve even in the absence of expectation constraints.Other approaches that can reduce the computational complexity of large-scale stochastic optimization problems involve the use of samples of uncertain parameters, as opposed to their full distributions. Examples of this approach to compute upper bounds are described in Birge and Louveaux (1997) and Pierre-Louis, Bayraksan, and Morton (2011), and within Benders decomposition in Infanger (1992) and Higle and Sen (1991). The quality of these approximations is progressively improved by increasing the sample size between iterations (Birge & Louveaux, 1997; Pierre-Louis et al., 2011) or by combining information from multiple independent samples through Benders’ cuts (Higle & Sen, 1991; Infanger, 1992). In particular, convergence of the Sample Average Approximation method (SAA), which relies on large-sample results, is guaranteed for stochastic linear programs with per-scenario and expectation constraints (Anitescu & Birge, 2008). However, a major drawback of these approximation methods is the poor quality of the estimation during initial iterations as a consequence of their asymptotic convergence properties. Recent results from Birge (2011) show that in some cases a combination of sub-sample estimates, as in the batch-means method (Law & Carson, 1979; Schmeiser, 1982), can achieve faster convergence and more robust results than when considering a single large sample of equivalent size. In the spirit of Birge (2011), our method to compute estimates of f(x, (Ω, p)) utilizes a sub-sampling approach that is enhanced through stratified sampling, which reduces the estimate variance.Our estimation method relies on using the means of N independent groups, or batches, of M observations each, instead of using a single large sample of equivalent size N × M. We denote a random sample of M observations from the space Ω as ΩM, and define a new probability measure pM(·) such that all observations have the same probability of occurrencepM(ωi)=1/M,∀i∈{1,…,M}. To approximate f(x, (Ω, p)), we draw N independent samples of M observations each, denoted{Ω1M,…,ΩNM},and solve N independent operations problems, denotedf(x,(Ω1M,p1M)),…,f(x,(ΩNM,pNM)). An estimate of f(x, (Ω, p)) is then calculated as:(8)f(x,(Ω,p))≅1N∑j=1Nf(x,(ΩjM,pjM)).Using our sub-sample method to approximate f(x, (Ω, p)), the termeTx+1N∑j=1Nf(x,(ΩjM,pjM))provides an upper bound on TC(Ω, p) for large values of M and N. Convergence of this method as N is increased is assured for stochastic linear programs (Birge, 2011), as it is in our case. However, small systematic biases might arise depending on the structure and stringency of the constraints in the operations problem. Previous research on the traditional batch-means estimator shows that small batch sizes (i.e., small M) can be a source of bias for the sample-mean estimator (i.e., sample size), although an asymptotically normal distribution of errors is guaranteed for large samples and batch counts (Schmeiser, 1982; Steiger & Wilson, 2001). An advantage of this method over other approaches that rely on unique, large-sample results is that problemsf(x,(Ω1M,p1M)),…,f(x,(ΩNM,pNM))can be solved in parallel. Therefore, the extra computational load that results from increasing the sample size M or batch count N to reduce bias, and to ensure tight confidence intervals on the sample mean, can be efficiently distributed among multiple independent processors, instead of being given to a single optimization problem of comparable size (e.g., f(x, (ΩN × M, pN × M)).To further reduce the computational cost of approximating f(x, (Ω, p)) through sub-sample estimations, we propose the utilization of a stratification technique to select samples that would more accurately match the characteristics of Ω and therefore reduce the variance of the sub-samplesf(x,(Ω1M,p1M)),…,f(x,(ΩNM,pNM)). Stratified sampling has been previously used for electricity production cost modeling (Marnay & Strauss, 1991), but has yet to be utilized in the context of the sub-sampling method proposed by Birge (2011). Our stratified sampling algorithm proceeds as follows. For a given predetermined sample size M, the space Ω is partitioned into disjoint subsetsS1,…,SM,with the objective of grouping the events into clusters with similar characteristics (e.g., observations are grouped based on load, wind, solar, and hydro levels included in r(ω) and T(ω)). Since in our application each observation is defined by a spatially disaggregated vector of load, wind, solar, and hydro levels for one representative hour, the clustering algorithm preserves the correlations among time-dependent parameters. For instance, high-load and low-wind hours are grouped separately from low-load and high-wind scenarios. The stratified sampling method selects one representative hour from each of these clusters (e.g., one representative observation from a cluster with high-load and low-wind scenarios and one from the low-load and high-wind group). Therefore, a stratified sample of M observations{ω1,…,ωM}⊂Ωis such that ωi∈ Si,∀i∈{1,…,M}. We weight each observation ωiin the operations problem with probabilitypM(ωi)=p(ωi)/p(ωi|Si)∀i∈{1,…,M}.Bounding methods that rely on clustering algorithms, or sampling, require either progressive refinement of the sample space partition or increasing sample sizes to decrease the optimality gap and improve the accuracy of the upper bound estimate (Birge & Louveaux, 1997; Hobbs & Ji, 1999; Pierre-Louis et al., 2011). To avoid poor initial estimates of the upper bound, we propose selecting and fixing both the sample size M and batch count N prior to initialization of the bounding or decomposition phases of our algorithm. An experimental analysis of the effects of sample sizes and sampling methodologies is given in the electronic supplementary material. Throughout the rest of this paper, we will assume that a computationally efficient method to approximate f(x, (Ω, p)) for any candidate investment plan x is available.The bounding algorithm, or Phase 1 of our methodology, proceeds as follows. For a given sample size M and batch count N, we initialize the iteration counter by settingk=0,and the lower and upper bounds asLB0=−∞andUB0=+∞,respectively. The incumbent solution is denoted x*.1.Setk=k+1,solve the lower-bound planning problem using the partitioned space (Ψk, qk) in (1), instead of the full space (Ω, p). Find a trial investment planxk*,and a lower bound on the optimal total system costs TC(Ψk, qk). IfTC(Ψk,qk)>LBk−1,update the lower bound toLBk=TC(Ψk,qk); otherwise,LBk=LBk−1.Compute the operating costsf(xk*,(Ω,p)). IfeTxk*+f(xk*,(Ω,p))<UBk−1,update the upper boundUBk=eTxk*+f(xk*,(Ω,p))and the incumbent solutionx*=xk*; otherwise,UBk=UBk−1.Compute the optimality gap, defined asGAPk=100percent×(UBk−LBk)/UBk. If GAPkis less than or equal to a pre-determined optimality gap, stop and use x* as the proposed investment plan. Otherwise, go to step 1.For a sequence of partitions defined such that its limit is the full discrete probability space (Ω, p), and a large sample size M and batch count N, the lower (LBk) and upper bounds (UBk) are convergent to TC((Ω, p)) as k → |Ω|. A formal proof of convergence is given in the electronic supplementary material.Empirically, clustering algorithms often exhibit the following behavior: only a few partitions are required to explain a large fraction of the variance observed in the full dataset (the “elbow” phenomenon), but the remaining fraction of variance explained converges asymptotically to 1 as the partitions are refined (Tibshirani, Walther, & Hastie, 2001). A potential implication of this observation for the bounding phase of our algorithm is that moderate optimality gaps might be achieved using only small numbers of representative hours from the sample space (clustered load, wind, solar, and hydro levels), but that tight optimality gaps might be only attained for large values of k (Hobbs & Ji, 1999). This is particularly challenging for planning problems that consider binary decision variables (e.g., transmission investments), because solving the lower-bound planning problems TC(Ψk, qk) using a branch-and-bound type of algorithm for large values of k become increasingly difficult. In the following section, we describe the use of Benders decomposition (Phase 2 of our methodology) to close the residual optimality gap from the bounding phase through the addition of cuts to the lower-bound planning problem.An alternative to the bounding approach described in the previous section is to take advantage of the decomposable structure of the planning problem and solve it iteratively using Benders decomposition (Bloom, 1983). However, the main drawback of this method is its slow convergence speed and the increasing size of the master problem as the algorithm iterates due to the accumulation of cuts. Multiple techniques have been proposed to accelerate the convergence of Benders decomposition when applied to mixed-integer linear optimization problems. For example, speed improvements can be achieved from tighter mixed-integer formulations and the selection of Pareto optimal cuts for subproblems with degenerate solutions (Magnanti & Wong, 1981; Sahinidis & Grossmann, 1991). Other techniques address the computational challenge of solving multiple mixed-integer linear master problems by initially computing cuts from linear (McDaniel & Devine, 1977) and Lagrangian relaxations (Aardal & Larsson, 1990; Cote & Laughton, 1984; Hoang Hai, 1980; van Roy, 1983), as well as from feasible sub-optimal solutions found by prematurely stopping branch-and-bound type algorithms (Geoffrion & Graves, 1980). Trust regions combined with high-quality initial solutions have been proposed to reduce the magnitude of changes in the master problem solution from iteration to iteration, resulting in quicker convergence (Sherali & Staschus, 1990; Sherali et al., 1987). However, approximate solution methods may prevent achievement of the conditions required to guarantee convergence of Benders decomposition to an optimal solution, resulting in either premature convergence to a suboptimal solution or failure to converge (Holmberg, 1994).Phase 2 of our algorithm uses a modification of Benders decomposition master problem, which we augment with a polyhedral lower bound on the optimal operating costs f(x, (Ω, p)), based on the results we introduced in Section 3.1. This approach can be interpreted as a generalization of the stabilization scheme for the stochastic decomposition algorithm (Higle & Sen, 1991) currently implemented in the NEOS Solver (Sen, 2013), which utilizes a partition with a single subset (i.e., the expected-value of all time-dependent parameters) to construct an auxiliary lower bound in the master problem.Our modified Benders decomposition master problem is:(9)minx,θeTx+θ(10)s.t.Ax≤b(11)θ≥f(x*,(Ω,p))+πT(x*)(x−x*)(12)θ≥f(x,(Ψm,qm))(13)x=(x1,x2),x1∈{0,1},x2,θ≥0.Constraint (11) represents the Benders’ cuts, which are computed using the sub-sampling method described in Section 3.2.1. The Lagrange multipliers π(x*) in (11) result from imposingx=x*in the calculation of f(x*, (Ω, p)). Constraint (12) captures the operations problem defined by Eqs. (4)–(7) for the sample space (Ψm, qm). The Benders master problem in electricity capacity expansion typically only considers investment variables (Bloom, 1983). In contrast, our master problem corresponds to a planning problem with an embedded low-resolution operations problem. The fidelity of the operations problem can be improved by increasing the number of clusters k used to approximate the sample space Ω. Fork=0,no constraints on the value of θ are imposed through constraint (12), so that the problem defined by Eqs. (9)–(11) and (13) corresponds to the standard master problem. Fork=|Ω|,all observations are considered and the master problem is equivalent to the original planning problem, which converges in a single iteration.A second improvement on the auxiliary lower bound is the utilization of the objective function value of a relaxed linear programming version of the problem (1)–(3), denoted TCLP(Ψk, qk). For a suitably large value of k, TCLP(Ψk, qk) might provide a tight initial lower bound (LB0) for the Benders’ iterations. This auxiliary bound is introduced to address a potential limitation of bounding algorithms that might find high-quality solutions during initial iterations, but optimality cannot be proven until the difference between the upper and lower bounds is below a certain tolerance.55This is often observed in branch-and-bound type of algorithms, where even if an optimal solution is found within the first iterations, optimality cannot be guaranteed until the algorithm has completed all the nodes.Note that unlike the methods proposed by Infanger (1992) and Higle and Sen (1991), we utilize a unique large sample across all iterations. This means that the quality of the approximation of the recourse function f(x*, (Ω, p)) is predetermined and it does not depend on the number of iterations of the Benders decomposition algorithm. The main advantages of this approach are its simplicity, since it does not require resampling between iterations, and the possibility of obtaining high-quality approximations of the recourse function during early iterations. However, the disadvantage is that we cannot prove that the batch size and count we select are large enough to guarantee convergence of the algorithm to TC((Ω, p)). Much larger batch sizes and batch counts could potentially improve the quality of the solution, and re-sampling between iterations could be implemented to build on the results of Infanger (1992) and Higle and Sen (1991). However, implementation of those variants is beyond the scope of this paper.Finally, as in the L-shaped method for stochastic programs (Birge & Louveaux, 1997), we compute a single cut at each Benders iteration using the expected value of the dual multipliersπ1(x*),…,πN(x*)and the operating costsf(x*,(Ω1M,p1M)),…,f(x*,(ΩNM,pNM))of the N sub-samples. An extension of this method, known as the multi-cut L-shaped algorithm, requires the addition of one cut per scenario (instead of a single “expected” cut) at each Benders iteration (Birge & Louveaux, 1988). However, improved convergence of Benders decomposition due to the multi-cut method is at least partially offset by the increased growth in the size of the master problem. Thus, we leave implementation of this extension to future research.In this section, we describe an application of our two-phase bounding and decomposition algorithms to a large-scale generation and transmission planning problem using a 240-bus representation of the Western Electricity Coordinating Council (WECC) in the U.S. (Munoz et al., 2014). This model consists of 240 existing buses, 448 transmission elements, and 157 aggregated generators. We model intermittent resources using 151 historical profiles of hourly demand, wind, solar, and hydro levels across multiple regions representing operating conditions for a typical year. More details of this model are given in the electronic supplementary material. The planning problem is formulated using 873666The sample is weighted by 8760/8736 in the objective function and expectation constraints of the operations problem. The sample size of 8736 hours results from considering 52 weeks of hourly solar data for a typical year.observations of time-dependent data results in a mixed-integer linear program with 56 million constraints and 31 million variables; 1020 of these variables represent discrete (integer) transmission investment options.77We model transmission investment alternatives as binary decision variables because network infrastructure is “lumpy”, e.g., a corridor is populated or not (Joskow & Tirole, 2005). Modeling generation investments as continuous variables is obviously an approximation. However, there exist relatively small scale economies for new renewable and conventional generation technologies, such that the minimum economic sizes for those new resources is well below the capacity of a single extra high voltage line. This issue is discussed in Munoz et al. (2013).We consider both the mixed-integer linear formulation, suited for real-world planning studies, and its linear relaxation, which resembles high-level models used for policy analysis. In order to obtain feasible operations problems for any candidate investment plan, we allow for load curtailment at a cost of $1000 per megawatt hour (the price ceiling used in most electricity markets in the U.S.). Noncompliance with annual renewable energy targets is penalized at a rate of $500 per megawatt hour.The clustering, bounding, and Benders decomposition algorithms are all implemented using the Pyomo algebraic modeling package (Hart, Laird, Watson, & Woodruff, 2012). All optimization problems are solved with the CPLEX 12.4 solver and parallelized through the Message Passage Interface (MPI) on a 32-core computer system with 2 AMD Optetron processors of 2.2 gigahertz and 112 gigabyte of RAM.We partition the space of time-dependent parameters using the k-means algorithm (MacQueen, 1967), although other partitioning schemes have been used in similar applications (Hobbs & Ji, 1999). The k-means algorithm has been used to model load levels in production cost models (Wogrin, Duenas, Delgadillo, & Reneses, 2014) and for wind generation investment (Baringo & Conejo, 2013), but not to compute lower bounds upon total system cost as we do in this article. K-means is a nonhierarchical clustering algorithm, implying that the total cost associated with the planning problem might decrease rather than increase as the partitions are refined (Birge & Louveaux, 1997). Despite this property, Hobbs and Ji (1999) report that k-means yielded the best clustering efficiency in their application (probabilistic production cost modeling), measured as the fraction of variance captured from the full data set, compared to several other partitioning methods–including hierarchical methods. Our implementation considers up to 500 clusters, which capture 71.1 percent of the variance of the normalized data set of 8736 observations of load, wind, solar, and hydro (see Fig. 1). The point of diminishing returns (i.e., the elbow) is reached at approximately 50 clusters, capturing 46.4 percent of variance. After this point, capturing an extra 10 percent of variance requires partitioning the space of load, wind, solar, and hydro levels using 100 additional clusters.First, we consider a linear relaxation of the original mixed-integer linear problem. This allows us to study the efficiency of our algorithm when applied to linear electricity investment planning and market simulation models such as the ones described in ICF (2013), Gabriel et al. (2001), Short et al. (2011), Paul and Burtraw (2002), and EIA (2013). Fig. 1 shows the optimality gap as a function of the number of clusters, calculated as100%×UB−LBUBfor the linear relaxation. We compute the lower bound (LB) by solving the planning problem using one representative hour from cluster (i.e., average of all observations of load, wind, solar, and hydro parameters within each cluster). The upper bound (UB) equals the sum of the investment costs associated with the lower-bound problem, plus a statistical estimate of the 8736 hour operations problem f(x, (Ω, p)) computed using the sample mean ofN=20sub-samples ofM=200stratified samples of time-dependent parameters. For this test-case, only 33 representative hours are needed to obtain a solution within 10 percent of the global optimum (Fig. 1), but more than 200 clusters are required to reduce the gap further to 5 percent. The “knee” in the optimality gap mirrors the elbow associated with the percentage of variance captured from the 8736 hour dataset, which decreases at a much slower rate after the first 39 clusters. The optimality gap is reduced from 28.9 percent to 8.9 percent using 39 clusters. Increasing the number of partitions to 500 only reduces the gap to 2.8 percent, which is acceptable for high-level planning models associated with policy analysis. The optimality gap for a single cluster, also known as the expected-value problem, provides an upper bound on the Value of the Stochastic Solution (VSS) as defined in Birge and Louveaux (1997). This is a measure of the potential cost savings that could be achieved by considering the full distribution of hourly load levels and capacity factors of wind, solar, and hydro resources across all regions, instead of planning a system using the expected value of these parameters.For the mixed-integer linear case we estimate the optimality gap as100%×UB−(1−ϵ)LBUB,where ϵ corresponds to the MILP optimality gap for the lower bound model. Note that(1−ϵ)LBprovides a lower bound on the optimal system costs for a zero MILP gap (ϵ=0). Therefore,100%×UB−(1−ϵ)LBUBis an upper bound on the optimality gap that could be achieved ifϵ=0. As the MILP gap is increased, the optimality gap increases as a consequence of both the deterioration of the lower bound (the(1−ϵ)factor) and the suboptimality of the investment decisions, reflected as higher operating costs in the upper bound. We observe that 10 clusters are sufficient to achieve optimality gaps of 12.1 percent for a 1 percent MILP gap and that 90 additional clusters only reduce the optimality gap to 7.4 percent. Optimality gaps and bounds as a function of the number of clusters are shown in Figs. B.4 and B.5, respectively, of the electronic supplementary material. For practical implementations, the linear relaxation of the mixed-integer planning problem could be used as a screening tool to assess the minimum number of clusters needed to approximate the operations problem with a pre-specified optimality gap.Solution times for the mixed-integer linear formulations are sensitive to the choice of the MILP gap and are orders of magnitude larger than solution times for the linear relaxation. While the linear relaxation of the 100-cluster problem requires only 5 minutes to solve, the mixed-integer formulations requires approximately 1.3, 6.6, and 6.8 hours to achieve 5 percent, 3 percent, and 1 percent MILP gaps, respectively. The reduction of 5.5 hours in solution time achieved from loosening the MILP gap from 1 percent to 5 percent is, however, contrasted with an increase in the total optimality gap from 7.4 percent to 11.1 percent. Further attempts to solve mixed-integer linear problems with 200 or more clusters and a MILP gap of 1 percent did not yield a solution within the optimality gap after more than 30 hours of computation time, and execution was stopped.Our bounding algorithm can efficiently find investment solutions whose total system costs have a 2.8 percent optimality gap for the linear relaxation and a 7.4 percent gap for the mixed-integer linear case. Achieving tighter optimality gaps, however, requires significant refinements of the clustering scheme due to the slow convergence of the bounding method (Hobbs & Ji, 1999). To overcome this limitation, we use Benders’ cuts to further reduce the optimality gap by iterating successively between the lower-bound investment planning problem (i.e., master problem) and the operations problems (i.e., subproblems).Fig. 2shows the optimality gap for the linear relaxation of Benders decomposition using different number of clusters as auxiliary lower bounds in the master problem. The figure also illustrates the convergence behavior of the traditional Benders decomposition algorithm without auxiliary bounds on the operations costs. The number of clusters for the experiments was selected based on changes in the convergence rate of the optimality gap of the linear relaxation (see Fig. 1). No experiments are considered beyond 33 clusters due to the significant decrease in the convergence rate of the bounding algorithm after that point. Although more clusters could further reduce the number of Benders’ iterations required to achieve tight optimality gaps, their use causes significant increases in the time required to solve the model in each iteration, particularly in the mixed-integer linear case. We leave those experiments as a subject of future research. We use the objective function value TCLP(Ψ150, q150) to initialize the lower bound (LB0), similar to the method proposed by van Roy (1983) to compute initial bounds for Benders decomposition using Lagrangian relaxation.In Fig. 2 we observe that including the expected-value problem (i.e., single partition) in the master problem, as done in Sen (2013), results in shrinkage of the optimality gap from 28.9 percent to 3 percent in 329 iterations after 26.4 hours of computing time (Table 1). In contrast, the regular Benders decomposition implementation in which no such auxiliary constraint is included only yields a 10.0 percent optimality gap after 400 iterations and 30 hours of computing time. Including more clusters results in further reductions in the number of iterations and solution time to achieve tight optimality gaps (Table 1). In this case, using 33 clusters reduces the time to achieve a solution with a 1 percent optimality gap by more than half (to 12.2 hours), compared to including the expected-value solution (1 cluster) in the master problem (more than 32.6 hours).Larger speedups can be achieved for looser optimality gaps by including more clusters in the auxiliary lower bound of Benders decomposition, but the resulting larger master problem eventually caused solution times to become higher than those achieved by increasing the number of clusters using the bounding algorithm in the linear case. Attaining a 3 percent optimality gap for the LP relaxation requires 400 clusters and 30 minutes of computation through the bounding method, but 29 iterations and 2.8 hours using our enhanced Benders’ approach with 33 clusters. Hence, the bounding method may be more efficient and practical than the modified Benders’ algorithm if loose optimality gaps for the linear relaxation are acceptable for planning purposes.Contribution of the auxiliary lower bound toward convergence of the UB and LB:Fig. 3depicts the value of the dual variable on constraint (12) of the modified master problem. The dual provides a measure of the contribution of the auxiliary lower bound to the objective function value of the lower-bound planning problem (f(x, (Ψk, qk))). No cuts are available the first time the master problem is solved in the modified Benders decomposition and the only lower bound upon the operating costs is f(x, (Ψk, qk)). The dual variable is in that case equal to−1. As Benders’ cuts are incorporated into the master problem, the contribution of the auxiliary lower bound f(x, (Ψk, qk)) to the total system cost is progressively reduced, reflected in the smaller magnitude of its dual. When Benders’ cuts alone provide a tighter (i.e., higher) lower bound on the operating costs than f(x, (Ψk, qk)), the value of the dual variable for the auxiliary constraint (12) is equal to zero, as it is the case for the 10- and 33-cluster experiments after 300 and 279 iterations, respectively. Note that the contribution of the lower bound is far beyond what a good initial solution or warm start of the master problem can provide. This behavior can be observed in the 33-cluster experiment, for instance, where constraint (12) is still binding after 278 iterations, at which point the enhanced Benders’ algorithm has attained an optimality gap of 0.67 percent.For the mixed-integer linear case, we follow the approach proposed by McDaniel and Devine (1977) and use cuts computed from the linear relaxation of the mixed-integer master problem, in conjunction with the auxiliary lower bound (12), to warm start the algorithm.For illustration purposes, we only consider further iterations of the enhanced Benders decomposition using the expected-value problem (1 cluster), a 0.5 percent MILP gap, and 400 pre-computed cuts from the linear relaxation. Computing the 400 cuts required 32.5 hours, but results in significant improvements in the optimality gap in the mixed-integer linear problem. Only 11 iterations are needed to attain a 5 percent optimality gap (after 1.9 hours), and letting the algorithm run for 166 more iterations and 39.4 hours only reduces the optimality gap to 4.1 percent. In contrast, the best investment plan found without pre-computed cuts results in a 7 percent optimality gap after 200 iterations and requires 43 hours of computation time.As in the previous experiments with the linear relaxation, we utilize the objective function valueTCLP(Ψ150,q150)=$624.3B to initialize the lower bound (LB0=$624.3B). However, a tighter lower bound can be obtained from solving the linear relaxation close to optimality using the enhanced Benders decomposition. In our experiments, the 33-cluster implementation results in the tightest optimality gap (0.64 percent) and in the highest lower bound ($635.5B) after 400 iterations and 44 hours of computation time (see Fig. C.6 in the electronic supplementary material). If this value is used as an initial lower bound (LB0=$635.5B), the optimality gap after 200 iterations is reduced from 4.1 percent to 2.4 percent, which is arguably sufficient for long-term planning studies.88We have necessarily ignored a number of uncertainties in this model, choosing to focus on specific issues relating to renewables integration. Examples include fuel price uncertainty, political uncertainties (which could, e.g., impact the feasibility of certain transmission corridors), and future demand. Consequently, an overly precise computation given a limited fidelity model is unnecessary in practice. In our experience with real-world planning problems in a variety of domains, decision makers are comfortable with such an approach. Ultimately, outputs of such a model would be just one of many inputs to decision making, meaning that a highly precise solution will not be directly realized in practice.Attaining even tighter optimality gaps requires more iterations of the algorithm, but these come at the expense of longer solution times. Consequently, the auxiliary lower bound, pre-computed cuts, and a tight lower bound can be used to attain tight optimality gaps for large-scale planning problems with integer variables.The lower-bound investment planning problem can provide useful information about total and marginal system costs required to meet forecasted demand and environmental goals, but these results may only be meaningful for large cluster counts. Strictly using the objective function value of the lower-bound planning problem (i.e., total cost) as an indicator of convergence, e.g., as in Heejung and Baldick (2013), could result in premature termination of the algorithm. Changes in the objective function value of the lower-bound planning problem only reflect improvements of the fidelity of the embedded operations problem that utilizes clustered data, but do not necessarily guarantee improvements in the quality of the investment plan when tested against the high-resolution operations problem, which is used to compute upper bounds.99Upper and lower bounds for the linear relaxation are in Fig. B.3 in the electronic supplementary material. The rate of improvement of the lower bound deteriorates rapidly after the first 20 clusters, which could meet the convergence criterion described in Heejung and Baldick (2013), even though the optimality gap is still above 10 percent (Fig. 1).We also observe that the lower-bound problem builds too little transmission capacity relative to near-optimal solutions for small cluster counts, which causes an increase in penalties due to curtailed load and noncompliance fines in the upper bound. This occurs because in the case of small cluster counts, high-impact and low-probability scenarios (i.e., peak load) are not accurately represented by the cluster centroids (i.e., they are grouped with and therefore averaged together with off-peak hours). This can cause underinvestment in transmission or generation infrastructure, of which we observe the former. Modifications to the clustering algorithm could potentially address this issue by weighting (Tseng, 2007) or constraining (Wagstaff, Cardie, Rogers, & Schr, 2001) the clustering algorithm to include peak-load hours, for instance, as individual clusters. Alternatively, the identification of hours that drive transmission and generation investments could also be automated by first solving the planning problem for each hour independently. The resulting vector of total costs could be used to bias an hour-selection algorithm, as done in importance sampling (Infanger, 1992; Papavasiliou & Oren, 2013). However, these are all subjects of future research.The amount of generation investments by technology, on the other hand, remain roughly constant as partitions are refined and more representative hours are introduced into the lower-bound problem.1010Investment costs for transmission and generation represent 3.6 percent and 48.2 percent, respectively, of the total system cost for the 500-cluster experiment ($642.2B) – which yields a 2.8 percent optimality gap. Aggregate generation cost remains relatively constant with respect to the number of clusters. However, transmission costs increase from $16.3B (one cluster, expected-value problem) to $23.4B (500-cluster problem).However, increasing the resolution of the time-dependent parameters shifts the optimal geographical distribution of investments for certain technologies. For instance, the expected-value problem, which considers a single representative hour, underestimates wind capacity by 100 percent and 42 percent in the states of Utah (UT) and Arizona (AZ), respectively, and overestimates investments in Wyoming (WY) by 60 percent with respect to the optimal levels of the 500-cluster problem (see Fig. 4). Although the aggregate capacity of wind investments in these three states becomes stable after only 50 clusters, there is a striking shift of wind investments from Colorado (CO) to Washington (WA) as the number of clusters is increased to 500. This result highlights the importance of using fine-grained representations of variability within investment planning models to capture the true economic value of intermittent electricity generating technologies (Joskow, 2011).In summary, unless large cluster counts are considered, we do not recommend using the lower-bound planning problem to find investment plans without assessing their quality against the full resolution operations problem (upper bound). In the linear case, more than 200 clusters were needed to attain a optimality gap below 5 percent, which is nearly four times the number of clusters needed to achieve the elbow on the fraction of variance explained from the full dataset of load, wind, solar, and hydro levels (see Fig. 1). The elbow criterion, often used to determine the number of clusters in a dataset (Tibshirani et al., 2001), can be used in our case to identify the point when the clustering algorithm becomes inefficient. In this situation, it may be advantageous to switch to Phase 2 (Benders decomposition) of our proposed two-phase approach if tighter optimality gaps are required. However, the elbow itself provides no information regarding the potential quality (i.e., optimality gap) of the investment plan that results from solving the lower-bound planning problem.

@&#CONCLUSIONS@&#
We propose a two-phase algorithm to find investment plans with bounds upon the optimal system costs for large-scale planning models with numerous operating subproblems, with a focus on transmission and generation planning. The bounding phase is an extension of the approaches proposed by Huang et al. (1977) and Hobbs and Ji (1999) for stochastic problems with environmental restrictions that we model with expectation constraints. The decomposition phase is a modification of Benders decomposition that includes a low-resolution operations problem in the master problem as an auxiliary lower bound upon the operating costs. We compute upper bounds for both algorithms using a sub-sample estimation of the true operating costs for a given investment plan implemented in a parallel computer system. From our numerical experiments, we find that the bounding phase can be more efficient than the traditional Benders decomposition to find investment plans within moderate optimality gaps (i.e., 3–6 percent approximately) for both linear and mixed-integer cases. For implementation purposes, the bounding phase is far more practical than Benders decomposition since improving the quality of the investments only requires refining the clustering of the time-dependent data. However, for applications where the bounding method fails to converge sufficiently rapidly, a combination of the bounding algorithm with Benders decomposition, as demonstrated in Phase 2, can be used to attain tight optimality gaps, and is more efficient than using either of these two algorithms separately.Our enhancement of the Benders algorithm is based on a lower bound that can be progressively improved by refining the partitioning of the space of load, wind, solar, and hydro levels, but that requires the planning problem to have all stochasticity limited to the right-hand-side of the constraints so that we can apply Jensens’ inequality. An interesting direction for future research would be to explore the effect of including other valid lower bounds in the master problem of Benders decomposition. This could be done, for example, by relaxing constraints that complicate the solution of the planning model and iterating between loosely constrained (lower bound) and highly constrained (upper bound) problems.Another potential extension of our algorithm is the inclusion of unit commitment variables and constraints in long-term planning models, an area of growing attention among operation researchers (Nweke, Leanez, Drayton, & Kolhe, 2012; Palmintier & Webster, 2011; Shortt et al., 2013). This would require inclusion of binary variables in the operations problems, which would as a result become nonconvex. Our bounding algorithm would still be applicable by relaxing all binary variables to compute lower bounds; however, we would not be able to guarantee convergence of the bounds to the true optimal system costs. Baringo and Conejo (2012) and Kazempour and Conejo (2012) have recently implemented and shown convergence of Benders decomposition including integer variables in the subproblems. Their results are based on Bertsekas and Sandell (1982), who proved that for a certain class of stochastic mixed-integer optimization problems, the duality gap converges to zero as the number of scenarios and integer variables is increased to infinity. A future step in our research is to study the implications of this result for a planning problem with unit commitment variables and to verify convergence of the Benders’ algorithm with nonconvex subproblems.Finally, as we state in Section 5.3, the optimality gaps we obtain are arguably sufficient to answer practical questions. A combined transmission-generation optimization model with a multidecadal time horizon can be useful for both long-range planning and policy analysis. For planning, the method identifies transmission developments that potentially have high economic value as a result of increasing power trade within and between regions. For instance, new lines might link areas whose load and renewable outputs are imperfectly correlated so that when region is short of power it might take more advantage of surpluses elsewhere. If additions between two regions are made for most or all solutions within a gap, this gives confidence that such links are worth investigating by planners with more detailed production costing models. Meanwhile, the model can also be used to investigate policy issues in more detail and with greater accuracy than is now possible using models such as IPM (ICF, 2013) that aggregate generation into large zones while modeling transmission using simplified transportation models that disregard Kirchhoff’s laws. Many environmental and energy policies differ in stringency or type among subregions. Consequently, the geographical detail that our model allows for more realistic representation of local effects, and interactions among the regions. As an example, elsewhere (Perez, Sauma, Munoz, & Hobbs, 2015) we have used this model to examine the impact of diverse state renewable energy requirements in the western US, with a focus on restrictions that states impose on trade in renewable credits. By providing more realistic representation of between-state power flows, more credible estimates have been obtained of the benefits, in terms of reduced costs and pollution, of lessening such trade restrictions. The model can assess the effect of those restrictions on the economic value of power trade and new transmission, as well as the interactive impacts of transmission investment and renewable credit trade limits upon the cost of integrating renewables into a region’s various power markets.