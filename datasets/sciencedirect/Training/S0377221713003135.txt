@&#MAIN-TITLE@&#
General linear formulations of stochastic dominance criteria

@&#HIGHLIGHTS@&#
We develop linear formulations of general Nth order stochastic dominance criteria.Our primal formulations use a piece-wise polynomial representation of utility.Our dual formulations use lower partial moments and co-lower partial moments.An empirical application analyses the passive US stock market portfolio.The passive portfolio seems dominated by actively managed alternatives.

@&#KEYPHRASES@&#
Stochastic dominance,Utility theory,Non-satiation,Risk aversion,Prudence,Temperance,

@&#ABSTRACT@&#
We develop and implement linear formulations of general Nth order stochastic dominance criteria for discrete probability distributions. Our approach is based on a piece-wise polynomial representation of utility and its derivatives and can be implemented by solving a relatively small system of linear inequalities. This approach allows for comparing a given prospect with a discrete set of alternative prospects as well as for comparison with a polyhedral set of linear combinations of prospects. We also derive a linear dual formulation in terms of lower partial moments and co-lower partial moments. An empirical application to historical stock market data suggests that the passive stock market portfolio is highly inefficient relative to actively managed portfolios for all investment horizons and for nearly all investors. The results also illustrate that the mean–variance rule and second-order stochastic dominance rule may not detect market portfolio inefficiency because of non-trivial violations of non-satiation and prudence.

@&#INTRODUCTION@&#
Stochastic dominance (SD), first introduced in Quirk and Saposnik (1962), Hadar and Russell (1969) and Hanoch and Levy (1969), is a useful concept for analyzing risky decision making when only partial information about the decision maker’s risk preferences is available. The concept is used in numerous empirical studies and practical applications, ranging from agriculture and health care to financial management and public policy making; see, for example, the extensive survey in the text book of Levy (2006). A selection of recent studies in OR/MS journals includes Post (2008), Lozano and Gutiérrez (2008), Blavatskyy (2010), Dupačová and Kopa (2012), Lizyayev and Ruszczyński (2012), Lizyayev (2012) and Brown et al. (2012).SD imposes general preference restrictions without assuming a functional form for the decision maker’s utility function. The SD rules of order one to four are particularly interesting, because they impose (in a cumulative way) the standard assumptions of non-satiation, risk-aversion, prudence and temperance, which are necessary conditions for standard risk aversion (Kimball, 1993). This approach is theoretically appealing but not always easy to implement. In some special cases, a closed-form analytical solution exists, as is true, for example, for the textbook case of a pair-wise comparison of two given prospects based on the second-order stochastic dominance (SSD) rule.However, more generally, a closed-form solution does not exist and numerical optimization is required. For example, Meyer’s (1977a,b) stochastic dominance with respect to a function (SDWRF) requires solving an (small and standard) optimal control problem. The rules of convex stochastic dominance (Fishburn, 1974) for comparing more than two prospects simultaneously also require optimization. For example, Bawa et al. (1985) develop Linear Programming tests for convex first-order stochastic dominance (FSD), convex SSD and an approximation for convex third-order stochastic dominance (TSD). Shalit and Yitzhaki (1994), Post (2003), Kuosmanen (2004) and Kopa and Chovanec (2008) develop Linear Programming tests that compare a given prospect using SSD with a polyhedral set of linear combinations of a discrete set of prospects.Unfortunately, a general algorithm is not available. How can we test, for example, whether a given medical treatment is dominated by convex fourth-order stochastic dominance (FOSD) relative to a set of alternative treatments? How can we test whether a given investment portfolio is FOSD efficient relative to a polyhedral set of portfolios formed from a set of base assets? Without an algorithm for these specific cases, we may be forced to use known tests for less discriminating decision criteria. For example, we could use a set of pair-wise FOSD tests to compare the evaluated medical treatment with every alternative treatment. Similarly, we could use pair-wise tests to compare the evaluated investment portfolio with a large number of alternative portfolios, for example, using a grid search or random search over the possibilities set. However, pair-wise comparisons generally are less powerful than convex SD, because a prospect can be non-optimal for all admissible utility functions without being dominated by any alternative prospect. A further possible loss of power stems from using a discrete approximation to a continuous choice set.Section 2 of this study develops linear formulations of general stochastic dominance rules. Our approach is based on a piece-wise polynomial representation of utility and its derivatives. This representation applies generally for higher-order SD rules (Nth order SD), comparing a given prospect with a discrete set of alternative prospects (convex NSD analysis), and comparing a given prospect with a polyhedral set of linear combinations of prospects (NSD efficiency analysis). Our analysis therefore represents a generalization of the lower-order tests of Bawa et al. (1985) and Post (2003). We can also deal with additional preference restrictions such as the bounds on the level of risk aversion of Meyer (1977a,b) and the bounds on utility curvature by Leshno and Levy (2002). The use of piece-wise polynomial functions also generalizes results by Hadar and Seo (1988) and Russell and Seo (1989) on simple representative utility functions for pairwise comparison based on lower-order SD rules.To arrive at a finite optimization problem, we focus on discrete probability distributions. In empirical studies, we usually face discrete sample distributions, and experimental studies generally use prospects with a discrete population distribution. In addition, many continuous distributions can be approximated accurately with a discrete distribution. Our approach can be implemented by solving a relatively small system of linear inequalities. The linear structure seems particularly convenient for the application of statistical re-sampling methods in the spirit of Nelson and Pope (1991) and Barrett and Donald (2003).Our focus is on utility and its derivatives and on restrictions that follow from utility theory. Still, Section 3 also derives linear dual formulations that are formulated in terms of lower partial moments (Bawa, 1975) and co-lower partial moments (Bawa and Lindenberg, 1977) of the probability distribution. We focus on the dominance classification of a given prospect and we do not attempt to identify an alternative prospect that dominates the evaluated prospect. In the case of a discrete choice set, a non-admissible prospect need not be dominated by any alternative prospect. In addition, a prospect that dominates the choice of a given decision maker need not be optimal for that decision maker, and, moreover, the optimum need not dominate the current choice. Finally, the dominance relation between a pair of prospects generally is less robust than the classification of a given prospect. For these reasons, the search for a dominant prospect seems irrelevant for our purposes. Still, the dual formulations are useful for computational efficiency and robustness analysis.Section 4 applies a range of SD tests to historical stock return data to compare the broad stock market portfolio with alternative portfolios formed from a set of risky benchmark stock portfolios and riskless Treasury bills. We analyze horizons ranging from 1month to 10years and consider the decision criteria of SSD, TSD, FOSD, SDWRF, ASSD and mean–variance (M–V) analysis. The analysis is relevant because a large class of capital market equilibrium models predict that the market portfolio is efficient. Another reason for expecting market portfolio efficiency is the popularity of passive mutual funds and exchange traded funds that passively track broad stock market indices.Our empirical analysis shows that the market portfolio is highly and significantly inefficient by the TSD, FOSD, SDWRF and ASSD criteria for every horizon. Few rational risk averters would hold the broad market portfolio in the face of the historical return premiums to active strategies. The appeal of active strategies only increases with the horizon. Our results also illustrate that pair-wise dominance comparisons and the SSD and M–V rules have limited discriminating power and can generate misleading results in relevant applications. The SSD criterion may fail to detect market portfolio inefficiency for short horizons, because it penalizes small-cap stocks for having a relatively high positive systematic skewness, violating prudence. M–V analysis underestimates the level of market portfolio inefficiency for long horizons, because it assigns negative weights to large positive market returns, placing a penalty on outperformance during bull markets. In our application, these phenomena lead to a non-trivial underestimation of the alphas for small-cap stocks.We consider M prospects with risky outcomes x1,…,xM. A prospect is defined here in a general way as an available choice alternative and it could be a given combination of multiple base alternatives, for example, a combination of production methods, financial assets or marketing instruments. Depending on the application, the outcomes may be total wealth, consumption, income, or any variable that can reasonably be assumed to enter as an argument to a utility function that obeys the maintained assumptions. The outcomes are treated as random variables with a discrete, state-dependent, joint probability distribution characterized by R mutually exclusive and exhaustive scenarios with probabilities pr>0, r=1,…,R. We use xi,rfor the outcome of prospect i in scenario r. We collect all possible outcomes across prospects and states in Y={y: y=xi,ri=1,…,M; r=1,…,R}, rank these values in ascending order y1⩽⋯⩽ySand useqi,s=Pr[xi=ys]=∑r:xi,r=yspr.Decision makers’ preferences are described by N-times continuously differentiable, von Neumann–Morgenstern utility functionsu(x):D→R. We use un(x) for the nth order derivative, n=1,…,N, andu0(x)=u(x). To implement stochastic dominance of order N⩾1, we will consider the following set of admissible utility functions:(1)UN={u∈CN:(-1)n-1un(x)⩾0∀x∈D,n=1,…,N}.Thus, first-order dominance assumes non-satiation (u1(x)⩾0, ∀x∈D); second-order dominance assumes also risk aversion (u2(x)⩽0, ∀x∈D); the third-order criterion adds prudence (u3(x)⩾0, ∀x∈D) and fourth-order SD also assumes temperance (u4(x)⩽0, ∀x∈D). In some applications, zero values for the derivatives may not be allowed, for example, in the cases of strict non-satiation (u1(x)>0, ∀ x∈D) and strict risk aversion (u2(x)<0, ∀x∈D). The needed adjustments to our Linear Programming tests are obvious substitutions of weak and strict inequalities. In our experience, these adjustments have a negligible effect in empirical applications. For the sake of brevity, we therefore ignore this issue here.For practical reasons, it is often useful to assume some sort of standardization, such as u1(y1)=1, in order to avoid numerical problems when evaluating utility functions that approximate u1(x)=0 ∀x∈D, or the indifferent decision maker. Since utility analysis is invariant to positive linear transformations, such standardizations are harmless.We distinguish between three types of SD relations: pair-wise dominance relations, discrete convex dominance relations and continuous convex dominance relations, or efficiency relations. These relations differ regarding to the assumed choice possibilities: a single prospect, a discrete set of prospects, or all convex combinations of a discrete set of prospects. Consider first the case of pair-wise comparison between two given prospects:Definition 1Pair-wise ComparisonAn evaluated prospect i∈{1,…,M} is not dominated in terms of Nth order stochastic dominance, N⩾1, by an alternative prospect j∈{1,…,M} if there exists an admissible utility function u∈UNfor which it is preferred to the alternative:(2)∑r=1Rpru(xi,r)⩾∑r=1Rpru(xj,r)⇔∑s=1Su(ys)(qi,s-qj,s)⩾0.This formulation uses a weak inequality and hence does not require a strict preference relation. We can alternatively use strict inequality to require strict dominance. There generally is no robust difference between the two definitions. For example, if we compare prospect x1 with a mean-preserving anti-spread x2, then (2) will apply with equality for a risk neutral decision maker. In this case, there exists a weak preference for some utility functions but no strict preference for any utility function. However, the violation of strict preference is infinitely small and the difference between the two definitions is not robust. The same consideration applies below for convex SD. For the sake of brevity, we use weak inequalities here.If there are M>2 prospects, we could perform (M−1) pair-wise dominance tests for any given prospect. However, a prospect can be non-optimal for all admissible utility functions without being dominated by any individual alternative prospect. All decision makers may agree that the evaluated prospect does not maximize their expected utility even if they do not agree on which specific alternatives achieve a higher expected utility.Definition 2Convex Stochastic DominanceAn evaluated prospect i∈{1,…,M} is admissible in terms of Nth order stochastic dominance, N⩾1, relative to the set of prospects {1,…,M} if there exists an admissible utility function u∈UNfor which it is preferred to every alternative prospect:(3)∑r=1Rpru(xi,r)⩾∑r=1Rpru(xj,r)j=1,…,M⇔∑s=1Su(ys)(qi,s-qj,s)⩾0j=1,…,M.Since admissibility can be violated without a pair-wise dominance relation, this criterion generally is more powerful than pair-wise dominance tests.Despite the adjective ‘convex’, convex SD does not consider convex combinations of the prospects. Rather, the terminology reflects that the convex SD criterion can equivalently be formulated by considering convex combinations of the cumulative distribution function (CDF) of the prospects. By contrast, the analysis of Shalit and Yitzhaki (1994), Post (2003) and Kuosmanen (2004), among others, assumes that convex combinations of the prospects are feasible:(4)X=∑j=1Mλjxj:∑j=1Mλj=1;λj⩾0j=1,…,M.This situation is relevant if, for example, the decision maker can create a mixture of production methods, financial assets or marketing instruments. Diversification is especially relevant for risk averters (N⩾2).The formulation is not restricted to convex combinations but can also be applied in the more general case that general linear combinations of the prospects can be made subject to a set of general linear restrictions. The Minkowski–Weyl Theorem says that any polytope can be represented as the convex hull of its vertices (vertex representation). Therefore, the prospects should be considered more generally as the vertices of a polyhedral choice set.We use x∗∈X for the evaluated combination of prospects. The ordering of the scenarios is inconsequential in our analysis and we are free to label the scenarios by their ranking with respect to the evaluated combination:x1∗⩽⋯⩽xT∗.Definition 3Stochastic Dominance EfficiencyAn evaluated combination of prospects x∗∈X is efficient in terms of Nth order dominance, N⩾2, relative to all feasible combinations x∈X if it is the optimum for some admissible utility function u∈UN:(5)∑r=1Rpruxr∗⩾∑r=1Rpru(xr)∀x∈X⇔∑r=1Rpru′xr∗xr∗-xj,r⩾0j=1,…,M.All three criteria (pairwise dominance, convex dominance, efficiency) seek an admissible utility function or marginal utility function that solves a finite set of inequality conditions. If utility and marginal utility can be expressed as linear functions of a finite set of parameters, then the criteria reduce to solving a set of linear inequalities, a task that can be performed using Linear Programming.Theorem 1Linearization of Utility and its DerivativesFor any utility function u∈UN, N⩾1, and a discrete set of outcomes z1⩽⋯⩽zT, we can represent the levels of utility and its derivatives by piece-wise polynomial functions:(6)u(zt)=∑n=0N-2βn(zt-zT)n+∑k=tTγk(zt-zk)N-1,(7)uq(zt)=∑n=qN-2n!(n-q)!βn(zt-zT)n-q+(N-1)!(N-q-1)!∑k=tTγk(zt-zk)N-q-1,q=1,…,N-1,where(8)βn=un(zT)n!,n=0,1,…,N-2,(9)γk=uN-1zk-1∗-uN-1zk∗(N-1)!,k=2,…,T-1,γT=uN-1(zT-1∗)(N-1)!,for some valueszk∗∈[zk,zk+1],k=1,…,T-1such that(10)(-1)nβn⩽0,n=1,…,N-2,(11a)(-1)N-1γk⩽0,k=1,2,…,T-1,(11b)(-1)N-1γT⩽0forN⩾2.Moreover, for all parameters satisfying(10) and (11) we can construct an admissible utility function u∈UN. (Proof in the Appendix)The theorem uses a piecewise-constant representation to the (N−1)th derivative of the utility function. The lower-order derivatives are obtained by integrating over the higher-order derivatives and take the shape of piecewise higher-order polynomials. The (N−2)th order derivative will be a kinked, piecewise linear function; the (N−3)th order derivative will be a smooth, piecewise quadratic function; and so forth. The piece-wise polynomial representation generalizes results by Hadar and Seo (1988) and Russell and Seo (1989) on simple representative utility functions for pairwise comparison based on lower-order SD rules.In the special case of a constant (N−1)th order derivative, or uN−1(zt)=c for t=1,…,T, we could set γt=βN−1, for t=1,…,T−1, and find a (N−1)th order polynomial utility function(12)u(zt)=∑n=0N-2βn(zt-zT)n+βN-1(zt-zT)N-1.Our approach differs from using this polynomial function in an important way. A polynomial generally does not obey the regularity conditions (restrictions on the signs of the derivatives) over the entire range. In addition, if we restrict the polynomial to obey the regularity conditions, then it generally loses its flexibility. Our approach in effect uses a local (rather than global) polynomial representation for the utility function. Since the NSD criterion restricts only the sign (and not the shape) of the Nth derivative, it allows a piecewise constant representation of the (N−1)th derivative and the piecewise polynomial representation of lower-order derivatives (but not for the global polynomial approximation).The utility levels and marginal utility levels are linear in the (N+T−1) parameters βn, n=0,1,…,N−2, and γt, t=1,…,T. This finding implies that the inequalities (2), (3) and (5) are linear in these parameters, allowing for Linear Programming. Applying our theorem to the admissibility conditions (3), we find:Corollary 1Convex Stochastic DominanceAn evaluated prospect i∈{1,…,M} is admissible in terms of Nth order stochastic dominance, N⩾1, relative to the set of prospects {1,…,M} if there exists a non-zero solution for the following system of inequalities:(13.1)∑s=1S∑n=0N-2βn(ys-yS)n+∑k=sSγk(ys-yk)N-1(qi,s-qj,s)⩾0,j=1,…,M,(13.2)(-1)nβn⩽0,n=1,…,N-2,(13.3a)(-1)N-1γk⩽0,k=1,2,…,S-1,(13.3b)(-1)N-1γS⩽0forN⩾2.An evaluated combination of prospects x∗∈X is efficient in terms of Nth order dominance, N⩾2, relative to all feasible combinations x∈X if and only if there exists a non-zero solution for the following system of inequalities:(14.1)∑r=1R∑n=1N-2nβnxr∗-xR∗n-1+(N-1)∑k=rRγkxr∗-xk∗N-2xr∗-xj,rpr⩾0,j=1,…,M,(14.2)(-1)nβn⩽0,n=1,…,N-2,(14.3)(-1)N-1γk⩽0,k=1,2,…,R.For N=2, these inequalities reduce to those underlying the Post (2003, Thm. 2) SSD test.We can specify LP problems to test the systems of inequalities in (13) and (14). The specific objective function and standardization of the variables would depend on the specific application. Our empirical application in Section 4 will use the following LP tests for convex NSD(N⩾2):(15)θ∗=minβn,γk,θθ,s.t.∑s=1S∑n=0N-2βn(ys-yS)n+∑k=sSγk(ys-yk)N-1(qi,s-qj,s)+θ⩾0,j=1,…,M,(-1)nβn⩽0,n=1,…,N-2,(-1)N-1γk⩽0,k=1,2,…,S,∑s=1S∑n=1N-2nβn(ys-yS)n-1+(N-1)∑k=sSγk(ys-yk)N-2qi,s=1.Parameter θ is an upper bound on the violations of the admissibility conditions. A value of θ∗⩽0 means that the evaluated prospect is admissible; a value of θ∗>0 means that it is non-admissible.1Values of θ∗<0 are not possible if the evaluated prospect is one of the M prospects. In this case, admissibility implies θ∗=0.1The last restriction on average marginal utility is a harmless standardization to avoid the trivial solution of an indifferent decision maker.Similarly, our empirical application will use the following LP test for NSD efficiency:(16)θ∗=minβn,γk,θθ,s.t.∑r=1R∑n=1N-2nβnxr∗-xR∗n-1+(N-1)∑k=rRγkxr∗-xk∗N-2xr∗-xj,rpr+θ⩾0,j=1,…,M,(-1)nβn⩽0,n=1,…,N-2,(-1)N-1γk⩽0,k=1,2,…,R,∑r=1R∑n=1N-2nβnxr∗-xR∗n-1+(N-1)∑k=rRγkxr∗-xk∗N-2pr=1.Parameter θ now is an upper bound on the violations of the first-order conditions. A value of θ∗=0 means that the evaluated combination of prospects is efficient; a value of θ∗>0 means that it is inefficient.The higher-order derivatives (q⩾2) do not enter explicitly in convex NSD tests and NSD efficiency tests (regardless of the order N). Still, the higher-order derivatives may be useful to impose additional structure on the utility function, such as in Meyer (1977a,b) and Leshno and Levy (2002). For example, Meyer (1977a,b) bounds the coefficient of absolute risk aversion a(x)=−u2(x)/u1(x) from below by a given function f(x)⩾0 and from above by another given function g(x)⩾0. We can impose these bounds by means of the following restrictions:(17)u2(zt)+f(zt)u1(zt)⩽0,t=1,…,T,(18)u2(zt)+g(zt)u1(zt)⩾0,t=1,…,T.Since the first-order and second-order derivatives are linear functions of the βnand γkparameters and f(x) is exogenous, the restrictions are linear in the parameters. In a similar way, we could impose restrictions on, for example, the coefficient of relative prudence b(x)=−u3(x)x/u2(x).Similarly, the Almost Second-order Stochastic Dominance (ASSD) rule bounds the relative range of the second-order derivative u2(x) from above by the constant1ε-1,ε∈0,12. Leshno and Levy (2002) present a closed-form solution for pair-wise comparison. The same restriction can be implemented for convex SD and SD efficiency tests by using the following linear restrictions:(19)δ⩽-u2(zt)⩽δ1ε-1,t=1,…,T,(20)δ⩾0.The focus of this study is on utility functions and their derivatives and on restrictions that follow from utility theory. It is well known that SD criteria can also be formulated in terms of lower partial moments or related statistics such as cumulated distribution functions, quantiles and Gini coefficients. This section develops linear dual formulations of our utility-based tests in terms of lower partial moments (Bawa, 1975) and co-lower partial moments (Bawa and Lindenberg, 1977). The dual representation is less economically appealing than the utility representation, but it is often adopted in OR/MS for computational efficiency and robustness analysis. The analysis in this section allows for a direct comparison with and generalization of a range of earlier studies based on lower partial moments or related statistics.We use the following definition for the nth order lower partial moment for prospect i and threshold value w:(21)LPMin(w)=∑r=1Rpr(w-xi,r)n1(xi,r⩽w).For analyzing combinations of prospects, we use the following definition of the nth order co-lower partial moment between a given combination x∈X with weights λ=(λ1⋯λM) and another combination x∗∈X with weights τ=(τ1⋯τM):(22)coLPMτ,λn(w)=∑r=1R∑j=1Mxj,rλjprw-∑j=1Mxj,rτjn1∑j=1Mxj,rτj⩽w.Theorem 2Dual Convex NSD TestAn evaluated prospect i∈{1,…,M} is non-admissible in terms of Nth order stochastic dominance, N⩾1, relative to the set of prospects {1,…,M} if and only if there exists a solution for the following system of inequalities:(23.1)∑j=1MλjLPMjn(yS)⩽LPMin(yS),n=1,…,N-2,(23.2)∑j=1MλjLPMjN-1(yk)⩽LPMiN-1(yk),k=1,2,…,S-1,(23.3)∑j=1MλjLPMjN-1(yS)<LPMiN-1(yS)(23.4)∑j=1Mλj=1,(23.5)λj⩾0,j=1,…,M.Since the nth order LPM equals the n-times cumulative distribution function, this test is a direct generalization of Fishburn (1974, Thm. 2), Bawa et al. (1985, p. 423) and Levy (2006, p. 131).For N=2, we arrive at the linear, slack-variable formulation of convex SSD of Bawa et al. (1985, p. 423), safe some trivial differences between weak and strict inequalities related to our utility functions (1) allowing for zero values for the utility derivatives. Similarly, for N=3, we arrive at the linear, super-convexity approximation to convex TSD of Bawa et al. (1985, Section B) for a given perturbation parameter ε>0, after modifying (23.4) to∑j=1Mλj=1+ε.Finally, for M=2, our test (23) reduces to the pair-wise NSD test presented in Levy (2006, p. 131). Our test however applies more generally for convex NSD, including, for example, convex FOSD for M>2.Theorem 3Dual NSD Efficiency TestAn evaluated combination of prospects x∗∈X with weights τ=(τ1⋯τM) is inefficient in terms of Nth order dominance, N⩾2, relative to all feasible combinations x∈X if and only if there exists a solution for the following system of inequalities:(24.1)coLPMτ,λn∑j=1Mxj,Rτj⩽coLPMτ,τn∑j=1Mxj,Rτj,n=0,…,N-3,(24.2)coLPMτ,λN-2∑j=1Mxj,kτj⩽coLPMτ,τN-2∑j=1Mxj,kτj,k=1,…,R-1,(24.3)coLPMτ,λN-2∑j=1Mxj,Rτj<coLPMτ,τN-2∑j=1Mxj,Rτj,(24.4)∑j=1Mλj=1,(24.5)λj⩾0,j=1,…,M.For N=2, (24.1) disappears and (24.2)–(24.4), coincides with Post (2003, p. 1929) dual SSD test in terms of zero-order co-lower partial moments or ‘ordered mean differences’.We can specify LP problems to test the systems of inequalities in (23) and (24) by analogy to problems (15) and (16). As discussed in the introduction, we are rather skeptical about attempts to identify an alternative prospect that dominates the evaluated prospect. Areas where the dual formulation clearly adds value to the primal formulation are computational efficiency and robustness analysis. Fábián et al. (2011) and Gollmer et al. (2011) suggest algorithmic improvements for stochastic optimization problems with SSD (N=2) constraints based on dual problem formulations. Applying their insights may also reduce the computational burden of our tests for large sample applications, which is particularly relevant when statistical re-sampling methods are used. Similarly, robustness analysis of dominance relationships traditionally focuses on the dual formulation; see, for example, Dentcheva and Ruszczyński (2010), Dupačová and Kopa (2012) or Liu and Xu (2013) for the case of SSD (N=2).We will now use a range of SD tests to analyze the efficiency of the broad stock market portfolio for various investment horizons. Our stock market portfolio is constructed as a value-weighted average of all NYSE, AMEX and NASDAQ stocks. It is compared with a standard set of 10 active benchmark stock portfolios that are formed, and annually rebalanced, based on individual stocks’ market capitalization of equity (or ‘size’), each representing a decile of the cross-section of stocks in a given year. Furthermore, we include the 1-month US Treasury bill as a riskless asset. We use data on monthly value-weighted returns (month-end to month-end) from July 1926 to December 2011 (1026months) obtained from Kenneth French’ data library. The size portfolios are of particular interest because a wealth of empirical research, starting with Banz (1981), suggests that small-cap stocks earn a return premium that defies rational explanation.2We arrive at similar conclusions using benchmark portfolios formed on stocks’ book-to-market equity ratio or prior 12 – 2month returns (momentum) and/or using a sample that starts in July 1963, a popular breakpoint in the empirical asset pricing literature. An exception occurs for 10-year returns to the size portfolios (but not for book-to-market and R12 – 2 portfolios). The recent sample of 10-year returns assigns a relatively large weight to the extraordinary late-1990s large-cap stock market rally, which has the effect of increasing the market beta of large-caps above that of small-caps. Therefore, risk aversion in effect penalizes large caps and the lowest alphas are obtained with a (nearly) risk-neutral pricing kernel for every efficiency criterion.2SD analysis is invariant for positive linear transformations of the returns and does not require a specification of the initial wealth level. Nevertheless, it is useful for the SDWRF criterion to use gross holding period returns (HPRs), which are positive (xmin>0) and proportional to final wealth (rather than the increase in wealth); the relevant restrictions on the ARA coefficient depend on the return definition. Our analysis does not use continuously compounded or log returns, because log returns generally are not proportional to final wealth, and, in addition, do not combine linearly in the cross-section. We do not object to assuming a log-normal distribution for some assets and horizons and we also do not object to logarithmic utility for some investors. SD analysis simply does not require such parametric specifications.Monthly returns are commonplace in the empirical finance literature. However, a 1-month period may not be appropriate as the horizon of the representative investor. We therefore also consider returns for periods of 1 and 10years. As discussed in Benartzi and Thaler (1995), a period of 1year seems most plausible as the relevant evaluation horizon, because most financial reporting takes place on an annual basis (for example, financial statements, tax files and updates of retirement accounts). To represent long-term investors, our analysis also includes 10-year returns. Whereas the benchmark portfolios are annually rebalanced, our analysis fixes the investor’s allocation across the benchmark portfolios during the investment period. For a long-term investor who periodically adjusts her asset allocation and style mix, a dynamic programming model may therefore be more appropriate.Common problems in the analysis of long-term return are a limited number of non-overlapping return intervals and a possible sensitivity to the specification of the starting month and year. We therefore focus on the HPRs for all 1015 sub-periods of 12 sequential months and all 907 sub-periods of 120 sequential months. This approach preserves possible auto-correlation in the monthly data and therefore leads to more realistic long-term HPR scenarios than random simulation. Nevertheless, our conclusions are robust to using non-overlapping long-term HPRs for any starting month and year, and for simulated long-term HPRs based on independent random draws of 12 or 120 monthly returns (with or without replacement).Table 1shows descriptive statistics for the excess returns of the relevant portfolios. We generally recommend to measure systematic risk based on co-lower partial moments rather than co-variance. Still, the descriptives include the classical market beta, because of its familiarity, and because it helps to interpret the M–V results. Not surprisingly, small-cap stocks tend to have a higher standard deviation, market beta and skewness than large-cap stocks. Interestingly, the market portfolio has less skewness than nine of the 10 benchmark stock portfolios. Apparently, broad diversification yields a relatively small reduction in downside risk (relative to the more concentrated size-decile portfolios) at the cost of a relatively large reduction in upside potential. However, the differences become smaller for long horizons. The market skewness increases with the horizon, reflecting the effect of compounding returns. By contrast, the returns to small caps become less skewed and have lower betas for longer horizons, presumably reflecting the effect of long-term mean reversion.In this application, it seems natural to test whether the market portfolio is efficient relative to all convex combinations of the 11 base assets, so as to allow for portfolio diversification (but without short selling). Nevertheless, it is insightful to also apply pair-wise dominance and convex dominance relative to the undiversified base assets. For each of the 11 base assets, we will apply LP test (15) to analyze whether the market portfolio is pair-wise dominated by the base asset (M=2). A test for admissibility applies LP test (15) to compare the market portfolio with the 11 base assets simultaneously (M=11). To test whether the market portfolio is efficient, or not dominated by any convex combination of the base assets, we use LP test (16).All tests use equal weights for the T historical observations (pt=1/T). Since we analyze the market portfolio, the marginal utility function can be interpreted as a pricing kernel and the violations of the first-order conditions as pricing errors, or ‘alphas’. The objective function is the largest positive alpha, because this term represents a deviation from optimality even in case of binding short-sales constraints, making the test more general than one based on, for example, the sum of squared alphas or the mean absolute alpha. After all, a large negative alpha generally offers only limited profit opportunity without short selling. The standardization in (15) and (16) follows a convention in the asset pricing literature to set the average value of the pricing kernel equal to unity.We implement the SSD, TSD and FOSD criteria by setting N=2, 3, 4, respectively, in (15) and (16). We also perform SDWRF tests based on ARA restrictions (17) and (18) with lower bound f(x)=0 and upper bound g(x)=3x−1. This means that the coefficient of relative risk aversion (RRA) r(x)=a(x)x is bounded from above by the value of 3. For the utility of wealth, as opposed to the utility of consumption, there exist compelling arguments for an average RRA value close to one and slightly increasing; see for example, Meyer and Meyer (2005). Since we analyze gross returns, which are proportional to final wealth, a value of r(x)=3 seems relatively high.3For the utility of consumption rather than the utility of wealth, higher RRA values may be required to be consistent with the historical equity premium; see, for example, Mehra and Prescott (1985).3We also implement ASSD tests using restrictions (19) and (20) on utility curvature. Following Levy et al. (2010) and Bali et al. (2009), we use critical value of ε=0.032. Finally, we include tests based on the mean–variance (M–V) criterion, using a linear marginal utility function (or quadratic utility). For the sake of brevity, we will focus on the primal problem solution and results here and omit the dual problem solution.Since we use empirical returns that are generated by an unobserved population distribution, we must account for the effect of sampling error on our test results. Unfortunately, the sampling distribution appears analytically intractable due to the large number of inequality restrictions involved. Fortunately, statistical re-sampling methods can overcome analytical intractability using brute computational force. An early study by Nelson and Pope (1991) demonstrated that SD analysis based on a bootstrapped return distribution is more powerful than analysis based on the original empirical return distribution. More recently, Barrett and Donald (2003) and Linton et al. (2005) derive powerful consistent bootstrap and sub-sampling tests for pair-wise comparisons.Interestingly, the tractable LP structure of (15) and (16) suggests that the computational burden of re-sampling is manageable also for convex SD tests and SD efficiency tests. Under the assumption of identically and independently distributed time-series returns, the empirical return distribution is a consistent estimator of the population return distribution, and bootstrap samples can simply be obtained by random sampling with replacement from the empirical return distribution. To ensure that the population distribution of the bootstrap samples obeys the null hypothesis of market portfolio efficiency, we first re-center the empirical distribution by correcting the original time-series of returns for a given base asset by subtracting the estimated alpha of that base asset. While this adjustment aligns the assets’ means with the null hypothesis, it does not affect the general risk levels and the dependence structure between the assets. We implement this bootstrap method by generating 10,000 pseudo-samples of the same size as the original sample through random draws with replacement from the re-centered original sample, and test market portfolio efficiency in every pseudo-sample. Since the long-term HPRs were constructed from historical sequences of 12 or 120 monthly returns, the method in effect is a block bootstrap applied to random blocks of the same length. Finally, we compute the critical values for the original test-statistics from the percentiles of the bootstrap distribution.Table 2shows the test results for the different decision criteria and investment horizons. For the sake of comparability, we multiply the monthly results by 12 and divide the 10-year results by 10, to arrive at ‘annualized’ results. This method obviously does not account for compounding effects, but it does preserve the relative differences between the portfolios.Testing whether the market portfolio is dominated by one of the base assets in a pair-wise fashion using any of the six decision criteria has limited discriminating power in this study. The market portfolio has a relatively low risk level and is not dominated by any of the more risky size deciles 1–9 using any of the criteria and horizons. At the same time, the market portfolio has a higher average return than the large-cap stock portfolios and the T-bill and is also not dominated by these alternatives. As discussed above, the pair-wise approach ignores that the market portfolio can be non-admissible without being dominated. Different investors may agree that the market portfolio is not optimal, even if they do not agree on which base assets are better than the market portfolio; they may see improvement possibilities in different base assets. The violations of admissibility are however also relatively small and not statistically significant. Presumably, the admissibility test has limited power in this application, because it overlooks the effects of diversification across the base assets. Indeed, the efficiency test detects economically and statistically significant violations of market portfolio efficiency. In the remainder of this section, we focus on these tests results in more detail. Table 3shows the alphas for the individual size decile portfolios and Fig. 1shows the associated pricing kernels.For monthly returns, M–V analysis classifies the market portfolio as significantly inefficiently due to a substantial undervaluation of small-cap stocks, confirming known empirical results. For the first decile portfolio, the alpha is 2.05% per annum, and highly significant. The SSD criterion leads to a large reduction of the alphas by using a step function for the pricing kernel (see Fig. 1); the alpha for small-cap stocks falls to 0.71% per annum. The kernel has large concave (!) segments and it penalizes small-cap stocks for having a relatively high positive skewness. Clearly, this is not consistent with prudence (or skewness preference). The higher-order SD tests do not allow for this pattern and their results cannot be distinguished from the M–V results (a linear kernel) in this case. The SDWRF kernel is convex and places a reward on positive skewness; the SDWRF alpha for small-cap stocks increases to 2.40% per annum. By contrast, the ASSD test does not remedy the problem of skewness aversion and yields similar results as the SSD test. Arguably, the parameter value ε=0.032 is too high for the relatively narrow range of monthly returns.The situation is remarkably different for annual returns. In this case, the M–V kernel becomes negative (!) for gross returns in excess of about 160% (or net return of 60% per annum). While such cases represent only a few percent of the total number of annual observations, they do represent a substantial part of the total return of in the sample period and can have a large effect on the estimated alphas. The M–V alpha for the first decile portfolio is only 0.99% per annum and is not statistically significant. This number seems to underestimate the appeal of small-cap stocks, because M–V analysis penalizes these stocks for having more systematic upside potential than large-cap stocks do.The SD efficiency tests impose non-satiation and avoid negative weights. However, the SSD test again penalizes small-cap stocks for their relatively high skewness. The higher-order tests impose both non-satiation (violated by M–V analysis) and prudence (violated by the SSD test). The TSD kernel is a convex two-piece linear function with a kink at a net market return of about 50% and it generates an alpha of 1.62% per annum for the first decile portfolio. This value still seems to underestimate the appeal of small-cap stocks, because the kernel displays a discontinuous drop in its slope for high-return levels, violating the assumption of temperance. The FOSD efficiency test corrects for this problem and yields a kernel that is consistent with all four assumptions: non-satiation, risk aversion, prudence and temperance. The FOSD alpha for the first decile portfolio is 1.99% per annum, far exceeding the M–V estimate in terms of economic and statistical significance. The SDWRF and ASSD tests yield similar results as the FOSD and TSD tests, respectively.Due to the multiplicative nature of HPRs, the average return and standard deviation of small caps increase faster than those of large caps as the investment horizon increases.4Investment risk increases at a slower rate when using annualized log returns, confirming known results by Fama and French (1988) and Poterba and Summers (1988), among others. This approach would however lead to spurious ‘time-diversification’ effects in the context of our study, as we assume that the investor maximizes the expected utility of her wealth at the end of a 10-year period.4However, the market beta of small-caps actually decreases at long horizons, presumably reflecting long-term mean-reversion. Related to this, the M–V alpha for the small-caps portfolio increases to 4.75% per annum for a 10-year horizon, economically and statistically highly significant. Fig. 1 shows that the M–V pricing kernel takes an alarming shape for this horizon. Specifically, the kernel becomes negative already for gross 10-year returns in excess of about 270% (or net return of 17% per annum) and its values range from −3.95 to 3.31 in this sample. This means that the M–V criterion in effect penalizes small caps for their systematic upside potential and the M–V alpha may still underestimate the true long-term appeal of these stocks.The SD tests yield different results. The SSD kernel assigns extremely large weights (in excess of 250!) to the largest negative market returns. This weighting scheme penalizes small caps for their downside risk and reduces their alphas to economically and statistically less significant levels. Specifically, the average return difference between small caps and large caps during bear markets is smaller than the unconditional average difference, presumably due an overall increase in correlation between stocks during bear markets. The SSD criterion reduces the alphas by assigning almost all weight to the worst market returns. The SSD kernel in this case is almost convex and the TSD and FOSD criteria yield similar results.The assumed preference structure however seems not representative for most risk averters. The SDWRF criterion ignores these preferences and uses a more moderate weighting scheme by requiring the RRA coefficient to take values between 0 and 3. The SDWRF alpha for small caps is as high as 6.07% per annum, substantially higher than the M–V and SSD values. These large differences arise because SDWRF avoids negative weights for the right tail of the market return distribution and large negative weights for the left tail. The 10-year SDWRF results are confirmed by the ASSD test, which places a cap on the relative range of utility curvature. In fact, the ASSD alphas are so large in this case that we may question whether the parameter value of ε=0.032 may be too low for the wide return range of long-term returns.We may formulate stochastic dominance criteria for discrete probability distributions using a piece-wise polynomial representation of utility and its derivatives. This approach applies generally for higher-order SD rules and can also deal with additional preference restrictions such as the SDWRF bounds on the level of risk aversion and the ASSD bounds on utility curvature. The approach allows for comparing a given prospect (or combination of prospects) with a discrete set of prospects but also for comparison with all linear combinations of a set of prospects. The approach can be implemented by solving a relatively small system of linear inequalities by means of Linear Programming. A linear dual formulation uses lower partial moments or co-lower partial moments.Our empirical application suggests that the passive stock market portfolio is highly inefficient relative to actively managed portfolios for all horizons for nearly all investors. It appears impossible to rationalize the market portfolio for any investment horizon without allowing for implausible shapes of the utility function.Pair-wise dominance tests appear too weak to generate plausible results in this study. Despite their very large alphas, small-cap stocks do not dominate the market portfolio, for the simple reason that they are more risky than the market portfolio. A joint test for convex dominance relative to a small caps, large caps and T-bills also has limited power, as it overlooks the benefits of diversification across these market segments. The SSD criterion appears too weak to generate plausible results. For short-term returns, it penalizes small-cap stocks for having a relatively high positive skewness, violating prudence. The other decision criteria avoid this pattern and show that properly accounting for skewness and kurtosis lowers the level of market portfolio efficiency. An investor who looks for short-term downside protection and/or upside potential will find the market portfolio less appealing than a more concentrated position in small-cap stocks (possibly combined with T-bills to achieve the same standard deviation).The results also support the hypothesis that M–V analysis gives a good second-order approximation for any well-be, haved utility function on the typical range of short-term return for diversified portfolios (see, for example, Levy and Markowitz, 1979). The approximation however breaks down for longer investment horizons. In this case, the M–V criterion assigns negative weights to large positive market returns, hence placing a penalty on the systematic upside potential of active investment strategies. As a result, it can underestimate market portfolio inefficiency and the appeal of active strategies for longer investment horizons.Overall, the TSD and FOSD results appear more plausible than the M–V and SSD results. However, for long-term returns, SSD, TSD and FOSD all assign extremely large positive weights to large negative market returns, hence placing a large penalty on the systematic downside risk of active strategies. The emphasis on the left tail seems to reflect the elevated correlation between stocks during bear markets rather than an extreme aversion to tail risk. The SDWRF and ASSD rules can avoid this type of over-fitting by limiting the level of risk aversion.Undoubtedly, parts of our alphas reflect market micro-structure issues regarding liquidity and transactions costs, and the appeal of active strategies diminishes without the professional trading facilities available to specialized investment companies. Notwithstanding these possible effects, we conclude that risk definitions and risk preferences are unlikely explanations for the high average returns of small caps (and similar results are found for value stocks and past winners). The M–V and SSD criteria can place implausible weights on the systematic downside risk and systematic upside potential of active investment strategies. However, our results show that using positive and moderate weights for all scenarios inevitably leads to the conclusion that the market portfolio is not optimal for all horizons and nearly all investors.

@&#CONCLUSIONS@&#
