@&#MAIN-TITLE@&#
Detecting paralinguistic events in audio stream using context in features and probabilistic decisions

@&#HIGHLIGHTS@&#
We present a sequential algorithm for detecting laughters and fillers in speech.The algorithm performs stepwise probability prediction, context inclusion & masking.We test several architectures for each of the above steps.Our models are more sensitive to change in feature carrying higher predictive power.

@&#KEYPHRASES@&#
Paralinguistic event,Laughter,Filler,Probability smoothing,Probability masking,

@&#ABSTRACT@&#
Non-verbal communication involves encoding, transmission and decoding of non-lexical cues and is realized using vocal (e.g. prosody) or visual (e.g. gaze, body language) channels during conversation. These cues perform the function of maintaining conversational flow, expressing emotions, and marking personality and interpersonal attitude. In particular, non-verbal cues in speech such as paralanguage and non-verbal vocal events (e.g. laughters, sighs, cries) are used to nuance meaning and convey emotions, mood and attitude. For instance, laughters are associated with affective expressions while fillers (e.g. um, ah, um) are used to hold floor during a conversation. In this paper we present an automatic non-verbal vocal events detection system focusing on the detect of laughter and fillers. We extend our system presented during Interspeech 2013 Social Signals Sub-challenge (that was the winning entry in the challenge) for frame-wise event detection and test several schemes for incorporating local context during detection. Specifically, we incorporate context at two separate levels in our system: (i) the raw frame-wise features and, (ii) the output decisions. Furthermore, our system processes the output probabilities based on a few heuristic rules in order to reduce erroneous frame-based predictions. Our overall system achieves an Area Under the Receiver Operating Characteristics curve of 95.3% for detecting laughters and 90.4% for fillers on the test set drawn from the data specifications of the Interspeech 2013 Social Signals Sub-challenge. We perform further analysis to understand the interrelation between the features and obtained results. Specifically, we conduct a feature sensitivity analysis and correlate it with each feature's stand alone performance. The observations suggest that the trained system is more sensitive to a feature carrying higher discriminability with implications towards a better system design.

@&#INTRODUCTION@&#
Non-verbal communication involves sending and receiving non-lexical cues amongst people. Modalities for transmitting non-verbal cues include body language, eye gaze and non-verbal vocalizations. Non-verbal communication is hypothesized to represent two-thirds of all communication (Hogan and Stubbs, 2003) and its primary functions include reflecting attitude and emotions (Argyle et al., 1970; Mehrabian and Ferris, 1967; Halberstadt, 1986), assisting dialog process (Bavelas and Chovil, in press; Johannesen, 1971) as well as expressing personality (Isbister and Nass, 2000; Cunningham, 1977). Studies suggest that non-verbal communication is a complex encoding-decoding process (Zuckerman et al., 1975; Lanzetta and Kleck, 1970). Encoding relates to the generation of non-verbal cues, usually in parallel with verbal communication and decoding involves interpretation of these cues (Argyle, 1972; O'sullivan et al., 1994). Studies broadly classify non-verbal communication into two categories, visual and vocal (Streeck and Knapp, 1992; Poyatos, 1992). Visual cues include communication through body gestures, touch and body distance (Ruesch and Kees, 1956) and vocal cues comprise paralanguage (e.g., voice quality, loudness) and non-verbal vocalizations (e.g., laughters, sighs, fillers) (Schuller et al., 2008; Bowers et al., 1993). Both these channels of non-verbal communication have been extensively studied and the literature suggests their relationship to varied phenomena and constructs including language development (Harris et al., 1986), child growth (Mundy et al., 1986; Curcio, 1978), relationship satisfaction (Kahn, 1970; Boland and Follingstad, 1987) and psychotherapy process (Gupta et al., 2014). This extension of non-verbal communications research beyond understanding their primary functions reflects their significance in interaction.Our focus in this work is on non-verbal vocalizations (NVVs) in speech. Previous research links various forms of non-verbal vocalizations such as laughters, sighs and cries to emotion (Goodwin et al., 2009; Gupta et al., 2012), relief (Soltysik and Jelen, 2005; Vlemincx et al., 2010) and evolution (Furlow, 1997). The importance of each of these non-verbal vocalizations is highlighted by the role they play in human expression. Therefore a quantitative understanding of their production and perception can have a significant impact on both behavioral analysis and behavioral technology development. In this paper, we aim to contribute to the analysis of these non-verbal vocalizations by developing a system for detection of non-verbal events in spontaneous speech.Several previous works have proposed detection methods for NVVs. Kennedy and Ellis (2004) demonstrated the efficacy of using window-wise low level descriptors from speech (Cortes and Vapnik, 1995) in detecting laughters in meetings. Truong and Van Leeuwen (2005) investigated perceptual linear prediction (PLP) and acoustic prosodic features for NVV detection using Gaussian mixture models. Várallyay et al. (2004) performed acoustic analysis of infant cries for early detection of hearing disorders. Schuller et al. (2008) presented static and dynamic modeling approach for recognition of non-verbal events such as breathing and laughter in conversational speech. In particular, the Interspeech 2013 Social Signals Sub-challenge (Schuller et al., 2013) led to several investigations (Kaya et al., 2013; Pammi and Chetouani, 2013; Krikke and Truong, 2013; Brueckner and Schulter, 2014; An et al., 2013) on frame-wise detection of two specific non-verbal events: laughters and fillers. Building upon on our efforts (Gupta et al., 2013) on the same challenge dataset (Salamin et al., 2013) (that was the winning entry in the challenge), in this paper we perform further analysis and experiments. Previous works in this research field have primarily focused on local characteristics and our approach investigates the benefits of considering context during the frame-wise prediction. Our methods are inspired from the fact that the non-verbal events occur over longer segments (and hence analysis frames). The temporal characteristics of these events has been investigated in studies such as (Mowrer et al., 1987; Bachorowski et al., 2001; Candea et al., 2005). These studies reveal interesting patterns such as a positive correlation between duration of laughter (Mowrer et al., 1987) and number of intensity peaks and similarity in duration of fillers across languages (Candea et al., 2005). Bachorowski et al. (2001) went further into the details of laughter types (e.g. voiced vs unvoiced) and their relation to laughter durations. More studies on laughter and filler duration and its relation to their acoustic structures can be found in (Vettin and Todt, 2004; Sundaram and Narayanan, 2007; Vasilescu et al., 2005). As statistics (presented later in Table 1) on our database of interest also show that laughters and fillers exist over multiple analysis frames, we hypothesize that information from neighboring frames can be utilized to reduce the uncertainty associated with the current frame.Given that the target events are temporally contiguous, one can use many of the available sequential classifiers for the problem of interest. Potential techniques include Markov models (Rabiner and Juang, 1986), recurrent neural networks (Funahashi and Nakamura, 1993) and linear chain conditional random fields (Lafferty et al., 2001). For instance, Cai et al. (2003) used Hidden Markov Models (HMM) for detection of sound effects like laughter and applause from audio signals. This approach is similar to methods used in continuous speech recognition (Rabiner and Juang, 1986) with a generative model (the HMM), which may not be as optimal as other discriminative methods in event detection problems (Tu, 2005). Brueckner and Schuller (2013) used a hierarchical neural network for detecting audio events. This model initially provides a frame-level prediction using low level features and then another neural network is used to combine predictions from multiple frames to provide a final frame-level prediction. Other studies (Kaya et al., 2013; Piccardi, 2004) have also used similar prediction ad-hoc filtering methods (median filtering, Gaussian smoothing) to incorporate context in similar sequence classification problems. Most of these works have focused on performance driven approaches towards design of the detection systems but fail to provide a thorough model and feature analysis. In this work, we explore and analyze new architectures to incorporate context at the two levels of (i) frame-wise acoustic features and, (ii) frame-wise probabilistic decisions obtained from the features as a continuation of previous effort (Gupta et al., 2013). Through our analysis in this paper, we aim to understand the relation of laughters and fillers to the low level features as well as the temporal characteristics of these events. We focus on aspects such as using contextual features during classification and incorporating context in frame-wise outputs (termed as ‘smoothing’ and ‘masking’ operations). Our final system achieves an area under the receiver operating characteristics (ROC) value of 95.3% for laughters and 90.4% for fillers on a held out test set. We also present an analysis on the role of each feature used during detection and its impact on the final outcome.The paper is organized as follows: Section 2 provides the database description and statistics and Section 3 lists the set of used features. We present our core NVV detection schemes inclusive of smoothing and masking in Section 4. Section 5 provides feature analysis and conclusion is presented in Section 6.We use the SSPNet Vocalization corpus (SVC) (Salamin et al., 2013) for the experiments in this paper. This data was used as the benchmark during the Interspeech challenge and provides a platform for comparison of various algorithmic methods (Kaya et al., 2013; Pammi and Chetouani, 2013; Krikke and Truong, 2013; Brueckner and Schulter, 2014; An et al., 2013) The dataset consists of 2763 audio clips, each 11 seconds long. Each of these clips have at least one filler or laughter event in between 1.5s and 9.5s. These clips are extracted from 60 phone calls involving 120 subjects (57 male, 63 female) containing spontaneous conversation. The pair of participants in each call perform a winter survival task which involves identifying an entry from a predefined list consisting of objects useful in a polar environment. The conversation was recorded on cellular phones (model Nokia N900) at the two ends. There was no overlap in speech as the recordings were made separately for the speakers involved. The audio files are manually annotated (single annotator) for laughter and filler. We list the statistics for laughter and filler events over the entire database in Table 1. For more details on the dataset please refer to (Salamin et al., 2013; Schuller et al., 2013).For modeling and evaluating the frame-wise detection, we use the training, development and testing splits as defined during the Interspeech challenge (Schuller et al., 2013). The speaker information per clip is not available but the training, development and testing sets contain non-overlapping set of speakers (training: speakers 1-70, development: speakers 71-90, testing: speakers 91-120). Non-overlapping set of speakers allows for speaker-independent evaluation. Table 2shows the counts of clips, laughter segments and filler segments in each data split. In the next sections, we list the set of features extracted per audio clip followed by our NVV detection scheme.We use an assembly of prosodic features and mel-frequency cepstral coefficients (MFCCs) extracted at a frame-rate of 10 milliseconds using OpenSMILE (Eyben et al., 1462). This set of features is same as the one used in the 2013 Interspeech challenge (Schuller et al., 2013) and has been previously used in several other classification and detection experiments involving emotion, depression and child behavior characterization (Liu et al., 2006; Pierre-Yves, 2003; Bone et al., in press; Bone et al., 2013; Gupta et al., 2012). Studies (Kennedy and Ellis, 2004; Truong and Van Leeuwen, 2005) have shown the relation of speech prosody and spectral characteristics to similar non-verbal events. Note that this list of features, while large, is by no means exhaustive and new features for laughter and filler detection have been proposed in several other works (An et al., 2013; Wagner et al., 2013). As we focus on the system development aspect of event detection in this paper, we work with this smaller representative set of features provided during the Interspeech challenge. For further improvement and specific feature analysis, the proposed system can definitely be augmented with the new sets of features in future. The features used in this paper are listed in Table 3. We z-normalize these features per file before subsequent system training. The feature means and variances for normalization are calculated over the entire duration of the file.We use the aforementioned feature set to train our frame-wise detection scheme. We test the effect of incorporating context at various stages in detecting an event E∈ {laughter, filler}. As these events usually last over multiple frames, we hypothesize that proximal information may help in detection of these events. Given the rare occurrence of these events, the Interspeech challenge (Schuller et al., 2013) adopted area under the Receiver Operating Characteristics (ROC) curve as the evaluation metric for laughter and filler detection. We use the same metric and explore several prediction architectures to maximize it. We develop a three step sequential algorithm which involves:(i)Predicting event probabilities based on the speech features. We investigate the effect of incorporating contextual features and compare it with a context independent baseline.Incorporating context in probabilistic frame-wise outputs obtained from the previous step.‘Masking’ the smoothed probabilities based on heuristics based rules.Fig. 1provides a block diagram representation of our methods and constituent experiments. We describe each of these steps below.We initiate our system training procedure with a model that uses the speech features and outputs the frame-wise probabilities of an event E. First, we present a model that takes no context into account and serves as a baseline. Next, we augment the feature set by introducing certain contextual features. We next describe the baseline scheme followed by the incorporation of contextual features.In this classification scheme, we obtain the event probability exclusively based on the 17 features extracted per frame, as listed in Table 3. Note that these features represent the acoustic characteristics of only the analysis frame under consideration and contain no information about the feature values from neighboring frames or the feature dynamics. We train models to assign each audio frame as belonging to a target event or as a garbage frame based on the acoustic features. We experiment with several classification architectures and model our class boundaries using (i) a generative Hidden Markov Model (Rabiner and Juang, 1986) (ii) a linear discriminative classifier (used as baseline in (Schuller et al., 2013)) and (iii) a non-linear discriminative classifier to assess the nature of separability of event classes in the feature space. We represent the column vector of features for the nth frame as xnand the corresponding probability obtained for an event E as uE(n). We describe the training methodology for each of the models below.(i) Classification with Hidden Markov Model (HMM): In this scheme, we train an HMM model using the Kaldi toolkit (Povey et al., 2011). We train monophone models using the Viterbi-EM algorithm (Forney, 1973) for each of the laughter, filler and garbage events from the training set. We then decode the development and the test sets using a unigram language model (LM) assuming equal LM weights for the three events (to account for class imbalance in the training set, as majority of frames belong to the garbage class). During decoding, we obtain the state occupancy probabilities of each frame to be belonging to the laughter, filler or garbage HMM. We use these probabilities from each of these HMMs as our laughter, filler and garbage probabilities.(ii) Classification with a linear discriminative classifier: We determine linear class boundaries using the Support Vector Machine (SVM) classifier (this model was also used in the Interspeech challenge (Schuller et al., 2013)). We train a multi-class SVM classifier over the three classes with pair-wise boundaries. We obtain the class probabilities for each frame by fitting logistic regression models to data-point distances from the decision boundaries. In order to prevent class bias due to unbalanced representative data from each class, we downsample the ‘garbage’ frames (frames not belonging to either of the events) by a factor of 20 during training, as suggested in the challenge paper (Schuller et al., 2013). We use a linear kernel and the slack term weight is tuned on the development set. The predicted probabilities are computed using the Hastie and Tibshirani's pairwise coupling method (Hastie and Tibshirani, 1998).(iii) Classification with a non-linear discriminative classifier: Finally, we test a discriminative classifier with non-linear boundaries. We expect better results with this classifier due to a higher degree of freedom in modeling the class boundaries. On comparing results to the previous classifier, we get a sense of deviation of the non-linear class boundaries from the SVM based linear boundaries. We chose a Deep Neural Network (DNN) (Hinton et al., 2012) with sigmoidal activation as our non-linear classifier. DNNs have been used in several pattern recognition tasks such as speech recognition (Seide et al., 2011; Dahl et al., 2012) and have provided state of the art results. We train a DNN with two hidden layers and the output layer consists of three nodes with sigmoidal activation. Each output node emits a probability value for one of the three classes; laughter, filler and garbage. We perform pre-training (Dahl et al., 2012) before determining the DNN weights. The number of hidden layers and number of neurons in each hidden layer was tuned on the development set.Results and discussion: We list the results using the three models in Table 4. The “chance” Area Under the Curve (AUC) is determined based on random assignment of 0 and 1 probability values per frame for each event.We observe that the acoustic features considered carry distinct information about the non-verbal vocal event which distinguish them from the rest of the speech. Decoding the development and test sets using the HMM framework often outputs the garbage label for laughter and filler frames; as a large portion of the training set consists of frames with garbage label. Although, a higher weight to the events in language model leads to a better output for laughter and filler frames, but this comes at the expense of a higher false alarm rate. Between the discriminative models, we obtain better results using a non-linear boundary as compared to linear SVM boundaries given a higher degree of freedom in modeling the class distributions. The gain in the case of detecting laughters is higher as compared to that for fillers. This indicates relatively less deviation of the non-linear boundary from the SVM boundary in case of the fillers. However, in case of laughters the greater performance boost obtained using a DNN suggests that the true boundary may not be well approximated by a hyper-plane. We also observe that our classifiers obtain a higher AUC in case of fillers. This indicates that given the context independent features, fillers are comparatively more distinguishable than laughters. This happens due to inherent differences in the acoustic structure of the two events. Fillers are contiguous sounds, (e.g. um, em, eh) and laughters typically involve bursts of sounds with silence in between. This heterogeneous acoustic property of laughter makes inference more difficult. For instance, assigning a silence frame to belong to a laughter event is difficult in the absence of any context (Fig. 2). We expect better detection after incorporating contextual features as presented in the next section.Non-verbal events such as laughters and fillers occur contiguously over long segments spanning multiple short term analysis frames. Hence the inclusion of neighboring context from surrounding frames may assist prediction as proximal acoustic properties may help resolve conflicts (e.g., in the case of silence frames within laughter). We extend the previous best system, i.e., the DNN classifier and make modifications to include feature context. We test two methods to incorporate context from features from the neighboring frames: (i) by feature concatenation over a window followed by dimensionality reduction, and (ii) appending statistical functionals calculated over a window to the frame-wise features. We discuss each of these below.(i) Window-wise feature concatenation and dimensionality reduction: In order to make a decision for the nth frame, we consider a window extending to Mxframes before and after the nth frame (window length: 2Mx+1). We concatenate features from all the frames over the window, leading to (2Mx+1)×17 features. We determine the outcome for the nth frame based on these values. An increase in the number of features leads to data sparsity in the feature space. Therefore, we perform dimensionality reduction before training our classifier. We use Principal Component Analysis (PCA) (Jolliffe, 2005) to retain the maximum variance after linearly projecting the 17×(2Mx+1) features. We train a DNN to obtain target event probabilities based on these projected features. We again downsample the garbage class before training and tune Mx, the number of principal components used and the DNN parameters on the development set. We show the classification schematic in Fig. 3. fFCindicates the classifier trained with feature context.(ii) Appending window-wise feature statistics: This scheme relies of appending the feature from the current frame with statistical functionals of features over a longer temporal window in its neighborhood. The set of features along with a linear SVM classifier was used as a baseline during the Interspeech challenge (Schuller et al., 2013). In this scheme we incorporate context by appending velocity (Δ) and acceleration (Δ2) values calculated over the 2Mx+1 frame-long window to the features in the current frame. This leads to 17×3 feature values (feature + Δ+Δ2). The practice of incorporating these contextual features (Δ and Δ2) is widely used in applications such as speech recognition (Kingsbury et al., 1998) and language identification (Torres-Carrasquillo et al., 2002). We further calculate means and variances over these 51 features to obtain our final feature set. Further addition of statistical functionals (apart from Δ and Δ2) provides additional temporal characterization and is inspired from various other previous works (Schuller et al., 2010, 2012). We train a DNN on this set of features and also report the challenge baseline results obtained using the SVM classifier for comparison. We tune Mxand the DNN parameters on the development set. Fig. 4outlines the adopted classification scheme.Results and discussion: We list the results using the above two classification architectures in Table 5. We also list the result from the previous best context independent classifier and the Interspeech challenge baseline results (Schuller et al., 2013) for comparison.From the results, we observe that we obtain higher AUC values for both the events, thus validating that context helps in improving prediction. We obtain higher AUCs using the statistical functionals compared to feature concatenation. This may be due to the fact that the dimensionality reduction leads to loss of information. Also the approach with feature statistical functionals retains the features pertaining to the frame at hand. These features are otherwise projected onto a lower dimensional space in the approach involving dimensionality reduction. Given a better performance using the feature statistical functionals, we proceed with this scheme for further system development.In this section, we explored several methodologies to extract information from a set of vocal features. Overall, non-linear boundaries on frame-wise features appended with neighboring frame information provide us the best results. Fig. 5shows the outputs from two separate audio clips, each containing laughter and filler events. From the plots we observe that these files still contain several ‘garbage’ frames with high laughter/filler probabilities. In spite of incorporating contextual features, the estimated probabilities do not evolve smoothly. Therefore, there is potential to further improve our results after including context in the output probabilities. We address this possibility in the next session, where we account for context in the decisions obtained from the current system.We propose methods to improve the previous model by incorporating context to the sequence of frame-wise probabilities (block (b) in Fig. 1). We concatenate the output frame-wise probabilities uE(n) (n=1…N) from the previous classification into a time series UE={uE(1), …, uE(N)}. We perform a “smoothing” operation on UEconsisting of two steps: (i) Linear filtering, and (ii) Probability-encoding. These steps assist us in incorporating neighboring frame context decisions as discussed below.We observe that the outcomes from the above systems tend to be noisy, consisting of sharp rises and falls. We therefore design a low pass FIR filter to reduce the spikes in U, as it is unlikely that these events last only for a few frames. We determine the filtered probabilityvE(n)at the nth frame for an event E as shown in (1). We use a window of length 2Mu+1 centered at n andamuis the filter coefficient applied to frame output at a distance of mufrom the current frame. We determine the filter coefficients using two approaches: (i) a moving average filter and (ii) a FIR filter with coefficients determined using the Minimum Mean-Squared Error (MMSE) criteria. We explain these two approaches and list the results below.(1)vE(n)=∑mu=−MuMuamu×uE(n+mu)(i) Moving Average (MA) filter: A moving average filter assigns equal values to all the coefficients, as shown in (2). For each frame, this scheme provides equal importance to the frame and its neighbors. We tune the window length parameter Muon the development set.(2)amu=12Mu+1,mu=−Mu,…,Mu(ii) Minimum Mean Squared Error (MMSE) based filter: In this filter design scheme, we find the optimal set of filter co-efficients after minimizing the Mean Squared Error (MSE, Eq. (3)) between the desired probability values and probability values obtained after filtering. MSE is a convex function with respect to (w.r.t.) the co-efficients with a global minima. Eachamuis obtained analytically by setting derivative of MSE w.r.t.amuto zero (Eq. (4)). We tune Mufor the MMSE based filter on the development set.(3)MSE=∑n∈Training set(tn−(∑mu=−MuMuamu×uE(n+mu)))2Size of the training set(4)amu=argminamu(MSE)at:∂MSE∂amu=0Results and discussion: We present the results in Table 6. We list the previous best results using contextual features for comparison.We observe that there is a similar increase in the AUCs using the two filtering schemes.We plot the filter coefficients in Fig. 6and the frequency response (FFT based, Eq. (5)) of the filters in Fig. 7.Apurepresents the discrete Fourier transform at the index pu. Although the coefficients for MA and MMSE filters are different, the similarity in performance of the two filters can be explained by the similarity in their frequency response. Both these filter attenuate high frequency components in the time series U. However the MMSE filter has a slightly higher cut-off frequency and admits more high frequency components when compared to the MA filter. Muwas 50 for fillers and 100 for laughters. This suggests that the context lasts longer for laughters as compared to fillers. It may follow from the fact that mean laughter length is greater than mean filler length as is observed in Table 1.(5)Apu=∑mu=−MuMuamuexp−i2πpumu+Mu2Mu+1;pu=0.…,2Mu+1In the next section, we describe the probability encoding scheme.After processing the data through the above scheme, we pass the outputsvE(n)through an autoencoder (Baldi, 2012). The goal here is to capture any non-linear contextual dependency which the linear filters fail to capture. We define a new time seriesVE={vE(1),...,vE(n),...,vE(N)}consisting of the filtered outputs. The auto-encoder fencis a feed-forward neural network trained to reconstruct the target values from VE. We use an autoencoder with a single hidden layer and sigmoidal activation on the output node. This operation reconstructs a window of inputs{vE(n−MV),…,vE(n),…,vE(n+MV)}to produce an output of the same length. The parameter MVand number of neurons in the hidden layer are tuned on the development set. Autoencoder accounts for any non-linear dependence and is a mapping to multiple target output values (unlike linear filtering). We train a neural network encoder on a window of length2Mv+1centered atvE(n)to obtain the predictions on the same window as shown in (6).(6){wE(n−MV),…,wE(n),…,wE(n+MV)}=fenc(vE(n−MV),…,vE(n),…,vE(n+MV))Results and discussion: We list the results for auto-encoding in Table 7. We train the auto-encoder on the outputs of the MMSE filter.Auto-encoding leads to different degrees of improvements for capturing the two events. Also, the performance is inconsistent across the data splits. On the development set, we obtain a greater increase for fillers and the pattern reverses on the test set. This is indicative of some degree of mismatch between the development and test splits. Very high AUC values on the development set indicate performance saturation and data distribution similarity between the development and the training set. A 1% absolute increase in AUC for laughters suggests that these events benefit more from the non-linear encoding as compared to fillers.Overall, we observe that incorporating context from output decisions leads to a greater detection improvement in the case of laughters than fillers. This shows the importance of context, particularly in case of events with heterogeneous spatio-temporal characteristics. Fig. 8shows the smoothed probabilities for the same set of files as shown previously in Fig. 5. We see that even though false alarms still exist for the sample files, we obtain near perfect detection in case of true positives. Also spurious isolated false alarms are largely non-existent. The false alarms are still a concern and mainly arise from similar acoustic properties between certain verbal sounds and the non-verbal events (e.g. sound “um” in umpire is similar to the filler “um”). We address this problem partially in the next section by using a ‘masking’ technique.As the final step, we make use of inherent properties of the probability time series to further improve our results (block (c) in Fig. 1). We develop the masking scheme based on two heuristics: (i) existence of low event probability values for extended period of time implies the absence of any event, and (ii) similarly, contiguous high event probability values implies presence of an event. We implement this strategy by developing binary masks as described below.(i) Zeros-mask design: If there is contiguous existence of probability values below a threshold T0 for at least a set number of K0 frames, we mask all such probabilities by zero.(ii) Ones-mask design: Similarly, if probability values are contiguously over a threshold T1 for at least K1 frames, we mask all such probabilities by one.The overall operation of implementing the zeros and ones masking operation is shown in (7). We tune T0, T1 and K0, K1 on the development set.(7)yE(n)=0if∃nsuch that:wE(n)<T0∀n∈n,n+1,…,n+K01if∃nsuch that:wE(n)>T1∀n∈n,n+1,…,n+K1wE(n)otherwiseResults and discussion: We show the results before and after masking in Table 8.We observe a slight increase in AUC for fillers and none for laughters. In the case of fillers, we obtain T0=0.02 and T1=0.98. The performance for laughters saturated in the previous step and any T0>0 and T1<1 led to a reduction in AUC. These threshold values suggest that the previous step involving smoothing accounted for most of the information during detection, leading to marginal gains when using additional masking. We plot the output probabilities for the chosen sample files in Fig. 9. The only visible impact is for the file containing fillers where event probabilities between frames 20 and 500 are set to zero. We perform an analysis of the results obtained after masking, and each previous operation, in the next section.In the experiments so far, all features are used together to make an event prediction. This makes it difficult to evaluate the contribution of individual features toward the final laughter and filler prediction outcomes. In this section, we perform two sets of experiments to address this issue. The goal of these experiments is to understand the relation between a feature's discriminative power and its impact on the final outcome. In the first experiment, we look at the sensitivity of the final outcome to perturbations in each single feature followed by computing the AUCs using a single feature at a time. Finally, we investigate the outcomes of these two experiments and find a significant correlation between them with further implications towards the system design.In this experiment, we study the effect of each feature on the final probability outcome. During frame-wise prediction, the neurons in the DNN are activated by all feature inputs from an audio frame. However, all the features may not have similar activation patterns. Through the sensitivity analysis, we investigate the DNN activation using one feature at a time. We expect differences in activation patterns from each feature which may correlate with the feature discriminability (investigated in the next section). We perform separate analysis for the prosodic and the spectral features using our baseline DNN model trained on the 17 z-normalized features. We activate the DNN trained in Section 4.1.1(iii) using one feature at a time, while keeping all other features to be 0. We vary the chosen feature one standard deviation (−1 to +1, as it is z-normalized) around the mean (0 for z-normalized feature) and observe the probability outcome from the DNN. We plot the output probabilities for each of the features in Figs. 10and 11. We discuss our results separately for prosodic features and MFCC coefficients below.We show the results for the MFCC features in Fig. 10. We split the results for the 12 MFCCs in two figures for readability. From the figures, we observe that the first few MFCC co-efficients show the most dynamic range in output probability and variation is low in the second half of coefficients. The first MFCC coefficient shows the most dynamic range for both laughters and fillers indicating high sensitivity. For all coefficients except the first, a monotonic change in a value either favors laughters or fillers. The output probability trends are opposite for the two events, wherein a positive slope for laughters corresponds to a negative slope for fillers. This indicates a monotonic increase/decrease in feature values favor one event while reducing the probability of other. Note that all the curves intersect at 0 as this corresponds to a value where the neural network in not activated at all.The plot of assigned outputs versus variation in prosodic features is shown in Fig. 11. In case of the prosodic features, we observe the most variation in the case of log energy and zero-crossing rate. Apart from log energy, we observe the opposite trends in the slope of the curves for fillers and laughters. We observe that the curves corresponding to log energy are not monotonic, indicating a more complicated boundary for this feature. We observe patterns such as the probability of outcome increases with higher intensity (more sharply for laughters than fillers) suggesting a louder voice implies a higher laughter probability. Similar comments can be made by observing the outcome patterns with increase/decrease in a prosodic feature.From the output patterns of the features, we observe that the trained DNN has different activation patterns for each feature. This implies that variation in features have disproportionate impact on the final probability prediction. Moreover, for several features, the output probability trends are opposite for filler and laughter events. This is expected in a discriminative model as increase in probability output for given feature values should translate to lower probability for the other. In the next experiment, we use the feature values from the dataset and predict the event labels.We analyze the performance of each feature on the test set using the same baseline DNN on 17 features (Note that this is unlike previous experiment where outputs were obtained by synthetically varying feature values). In order to predict the event probabilities, we use the values from a single feature, while setting all other features to zero. We show the corresponding AUCs for laughters and fillers in Fig. 12.From the figures, we observe a difference in AUC patterns across the MFCC features for the two events. This shows that each frequency band carries a different amount of discriminative information for each event type. MFCC-2,6,10 perform the best for laughters and MFCC-1,11,12 for fillers. In case of the prosodic features, log energy provides the highest AUCs for both the events. Prosodic features offer a higher degree of discriminability, particularly for filler as the best two prosodic features achieve an AUC greater that 70% by themselves. We speculate that the results from sensitivity analysis and feature performance analysis are related as the DNN model should tune more to better features. In the next section, we investigate the relationship between the two.The relation between activation patterns in neural networks and their performance have been a subject of study in various other tasks such as face and object recognition (Lawrence et al., 1997; Le, 2013). We hypothesize that the DNN model is more sensitive to features which offer a higher discriminatory power. We test our hypothesis by performing regression analysis (Draper et al., 1966) between the dynamic range of outputs as obtained in Section 5.1 and the AUC values obtained in Section 5.2. The dynamic range obtained in sensitivity analysis is used as a proxy for the degree of activation. We fit a linear model (Eq. (8)) with dynamic range as the predictor variable for AUC values. The linear regression analysis helps us understand the general trend in between the two variables (although the relation between the two variables may not be linear). The outcome of linear fitting is shown in Fig. 13. Each of the 17 datapoints in the figure corresponds to a feature, with the x-axis representing the output dynamic range obtained during sensitivity analysis of the feature and y-axis the AUC using that feature only. Table 9shows the statistics on the relation between the two variables. ρ represents the correlation coefficient between the AUC and the dynamic range as computed over the seventeen features.(8)AUC=α0×Dynamic range+α1The F-statistic shows that the regressor is significant at 5% level in predicting the AUC in case of fillers and at 10% level for laughters. This provides evidence of relation between a feature's discriminative power and output sensitivity to the feature. This also shows that the DNN model is more sensitve to more discriminative features. In the future, we can use this observation during DNN training by discarding nodes corresponding to features with low sensitivity. This observation may particularly be useful in case of increased dimensionality of features introduced while including the contextual features (as in Section 4.1.2).

@&#CONCLUSIONS@&#
