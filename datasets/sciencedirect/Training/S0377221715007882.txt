@&#MAIN-TITLE@&#
A new Bayesian approach to multi-response surface optimization integrating loss function with posterior probability

@&#HIGHLIGHTS@&#
Integrating loss function with posterior probability in a single framework.Quality loss and reliability are considered in multi-response optimization.An improved quality loss function is proposed via Bayesian modeling.Using optimized Monte Carlo simulation and hybrid genetic algorithm.

@&#KEYPHRASES@&#
Quality management,Robust optimization,Loss function,Multi-response surface optimization,Bayesian analysis,

@&#ABSTRACT@&#
Multi-response surface (MRS) optimization in quality design often involves some problems such as correlation among multiple responses, robustness measurement of multivariate process, confliction among multiple goals, prediction performance of the process model and the reliability assessment for optimization results. In this paper, a new Bayesian approach is proposed to address the aforementioned multi-response optimization problems. The proposed approach not only measures the reliability of an acceptable optimization result, but also incorporates expected loss (i.e., bias and robustness) into a uniform framework of Bayesian modeling and optimization. The advantages of this approach are illustrated by one example. The results show that the proposed approach can give more reasonable solutions than the existing approaches when both quality loss and the reliability of optimization results are important issues.

@&#INTRODUCTION@&#
Robust design (RD), introduced by Taguchi (1986), has been proven very useful for improving the quality of products or processes at low cost. RD is a useful tool for controlling the variance of products or processes performance while keeping the difference between the output characteristics and the desired targets as small as possible (Nha, Shin, & Jeong, 2013). Although the experimental design and data analysis methodology proposed by Taguchi have been argued frequently, there is great consensus that the philosophy of robust design has been considered as a milestone in the field of quality engineering (Nair et al., 1992). Response surface methodology (RSM) is viewed as a collection of statistical design, empirical modeling methodologies and numerical optimization techniques used to optimize product designs (He, Zhu, & Park, 2012; Myers, Montgomery, Vining, Borror, & Kowalski, 2004). A common problem in robust design is the selection of optimum parameter levels for optimizing multiple responses simultaneously, which is called a multi-response surface (MRS) optimization problem (Myers, Montgomery, & Anderson-Cook, 2009).If there are multiple responses involved, a series of research issues need to be addressed to optimize products or processes performance because they are often in conflict (Lee, Kim, & Köksalan, 2011; Murphy, Tsui, & Allen, 2005; Tansel İç & Yıldırım, 2013). In general, the MRS problem usually consists of three stages (Kim & Lin, 2006): (1) index construction (i.e., constructing an effective index to measure the robustness and correlation among multiple responses), (2) model building (i.e., building a suitable process model to consider the conflict among multiple objectives and the prediction performance of process model), (3) parameter optimization (i.e., selecting an appropriate optimization algorithm to obtain robust optimal solution and assess the reliability of optimization results).In the last three decades, various creative approaches for MRS optimization have been proposed to solve the aforementioned problems in literature. One common approach to MRS optimization has been the use of dimensionality reduction strategy (Ko, Kim, & Jun, 2005). This strategy usually constructs a simplified performance index which can convert a MRS problem into a single objective optimization problem. The simplified performance index has often been defined as a desirability function, a loss function or a reliability function. The desirability function approach, firstly put forth by Harrington (1965), gives a numerical value between zero (i.e., unacceptable quality) and one (i.e., perfect quality) for a quality characteristic of a product or process. This approach is modified to consider the process economics by adjusting the desirability function shapes (Jeong & Kim, 2009) or the relative weights of multiple responses (Derringer, & Suich, 1980). However, these improved desirability approaches do not take into consideration the correlations among multiple responses, the variance–covariance structure of the responses and the variability of the predictions. In fact, ignoring such information may lead to an unrealistic solution if the responses are highly correlated (Chiao & Hamada, 2001; Gomes, Paiva, Costa, Balestrassi, & Paiva, 2013) or have significantly different variance levels (Ko et al., 2005). As stressed by Myers (1999), understanding the variability of predicted responses is a critical issue for practitioners. Recently, several approaches have been proposed to consider the correlation or robustness using the desirability function. Wu (2005) presented an approach to optimize the correlated multiple quality characteristics based on the modified double-exponential desirability function. He, Wang, Oh, and Park (2010) proposed an overall desirability function which makes the balance between robustness and optimization for MRS problem. Goethals and Cho (2012) extended the desirability function by fitting higher-order variance models and covariance models, which are available with model estimation techniques such as ordinary least squares, to account for the variability measures (i.e., robustness and correlation). Although higher-order models can achieve an ideal fitting accuracy, it also easily results in over-fitting problems (Zhou, Ma, Tu, & Feng, 2013). Besides, Vining and Bohn (1998) also pointed out that the process variance typically is rather “noise” system which results in poor fit or prediction results.Another popular approach to assessing the performance of MRS optimization is based on loss function. Taguchi, Elsayed, and Hsiang (1989) introduced a univariate loss function to obtain a parameter setting where the response value is close to the target with a low variance. Pignatiello (1993) proposed a general multi-response loss function by extending Taguchi's univariate loss function, to resolve the correlation problem among multiple responses. Ames, Mattucci, MacDonald, Szonyi, and Hawkins (1997) presented a quadratic quality loss function, which is applied to MRS optimization with experimentally derived polynomials, to find the optimal parameter setting by minimizing the loss function with respect to process inputs. Vining (1998) improved Pignatiello's method to consider the correlation among multiple responses, the process economics and the quality of prediction. Ko et al. (2005) combined the advantages of Pignatiello's and Vining's methods to propose a new loss function, which allows the analyst to consider both robustness and quality of predictions as well as bias in a single loss function framework. The major advantages of the loss function approach are that it incorporates the variance–covariance structure of the responses and the quality of predictions (Ouyang, Ma, & Byun, 2015). As noted by Ko et al. (2005), a major drawback of their proposed loss function approach is that they ignore the robustness to process parameter fluctuations, which is a very common phenomenon in practice.Recently, a posterior predictive approach which measures the reliability of an acceptable optimization result for any set of operating conditions, has received a great deal of attention for its attempt to tackle MRS optimization problems. The Bayesian reliability approach takes into account the correlation structure of the data, the variability of the process distribution, and the uncertainty of the model parameters (Peterson, 2004). As noted by Peterson (2004), ignoring parameter uncertainty in the optimization criterion (desirability function or loss function) can lead to reliability estimates that are too large. Miro-Quesada, Del Castillo, and Peterson (2004) extended the work of Peterson (2000) for MRS optimization to the robust parameter design case, which considers the noise variables in the integration of the predictive density. Furthermore, Peterson, Miro-Quesada, and Del Castillo (2009) refined the work of Peterson (2004) to consider the case having different covariance structure across response types with seemingly unrelated regression models. In addition, Del Castillo, Colosimo, and Alshraideh (2012) extended this earlier approach of Peterson (2004) to the functional response case based on a hierarchical two-stage mixed-effects model. Robinson, Pintar, Anderson-Cook, and Hamada (2012) also extended the work of Miro-Quesada et al. (2004) to develop a new Bayesian robust parameter design approach involving both normal and non-normal responses within a split-plot experiment. The major advantage of the existing posterior predictive approaches is that they provide a more feasible solution by assessing the reliability of a good future response which falls within the specific region. However, existing posterior predictive approaches may pay more attention to the reliability of optimization results rather than the bias and robustness. As pointed out by Kazemzadeh, Bashiri, Atkinson, and Noorossana (2008), the posterior predictive approach can help practitioners to control the responses in their specification regions; however it does not consider the deviation from the targets. As observed in this paper, the optimization results obtained by the posterior predictive approaches having high posterior probability can also yield undesirable solution with respect to the expected loss (i.e., bias and robustness). In such a case, optimization results based on the existing posterior predictive approaches may be misleading.The purpose of this paper is to develop a new Bayesian approach for multi-response optimization, building on quality loss function (Ko et al., 2005) and a posterior predictive approach (Peterson, 2004). The proposed approach allows the analyst to consider both quality loss and reliability of optimization results in a single framework of Bayesian modeling and optimization. Section 2 reviews several existing loss functions which are the basic ingredient of the proposed approach. A detail description of the proposed approach is provided in Section 3. Section 4 illustrates the proposed approach through one example. Some discussion issues are presented in Section 5. Finally, conclusions are made in Section 6.Loss functions provide an aggregate performance measure through incorporating different optimization criteria (i.e., robustness, bias and quality of prediction) into a single objective function. Several loss functions have been proposed in the literature. Pignatiello (1993) extended Taguchi's univariate loss function to a general multi-response loss function:(1)L(Y(x),θ)=(Y(x)−θ)TC(Y(x)−θ)where Y(x) is a p × 1 vector of responses at a parameter setting x, θis the p × 1 vector of the specified target values, and C is a p × p positive definite cost matrix which represents the losses incurred when Y(x) deviates from the targetθ. It can be shown that the expected loss function can be expressed as(2)E[L(Y(x),θ)]=(E[Y(x)]−θ)TC(E[Y(x)]−θ)+trace[C∑Y(x)]where∑Y(x) is a p × p variance–covariance matrix for responses Y at a parameter setting x. The term(E[Y(x)]−θ)TC(E[Y(x)]−θ)in Eq. (2) denotes a squared bias component, which refers to the expected deviation of responses from their targets. Another term trace[C∑Y(x)] in Eq. (2) is a variance component which represents the robustness measured by the variance–covariance matrix∑Y(x) among multiple responses Y at x. As the variance component decreases, the robustness improves. Therefore, Pignatiello's approach is useful when robustness and bias are both significant issues.In addition to robustness and bias, quality of predictions is also an important issue in multi-response optimization. Vining (1998) proposed another loss function by substituting the model predicted valueY^(x)for Y(x) in Eq. (1). The new loss function is given as(3)L(Y^(x),θ)=(Y^(x)−θ)TC(Y^(x)−θ)whereY^(x)is a p × 1 vector for the predicted responsesY^at x. With the definition of loss function given above, the expected loss function is given as(4)E[L(Y^(x),θ)]=(E[Y^(x)]−θ)TC(E[Y^(x)]−θ)+trace[CΣY^(x)]whereΣY^(x)is a p × p variance–covariance matrix forY^(x)at x. The term(E[Y^(x)]−θ)TC(E[Y^(x)]−θ)in Eq. (4) represents the penalty imposed for any predicted value which does not simultaneously achieve all specified target values. The termtrace[CΣY^(x)]represents the penalty due to the quality of the prediction. The fundamental difference between Pignatiello's approach and Vining's approach is that Vining uses the variance–covariance structure of the predicted responses rather than the variance–covariance structure of the responses. In doing so, Pignatiello's approach considers bias and robustness, but it does not take into account the quality of the predictions. However, Vining's approach considers bias and quality of predictions, but not robustness.Ko et al. (2005) proposed an improved loss function by integrating the strengths of Pignatiello's and Vining's approaches. The improved loss function is defined as(5)L(Y˜new(x),θ)=(Y˜new(x)−θ)TC(Y˜new(x)−θ)whereY˜new(x)is a p × 1 vector of responses based on the estimated model for a new observation at x. AssumingY^(x)is given, we may define the estimated model as follows(6)Y˜new(x)=Y^(x)+ɛnew(x)where ɛnew(x) is a random error term. With the definition of Eqs. (5) and (6), the expected loss function is given by(7)E[L(Y˜new(x),θ)]=(E[Y˜new(x)]−θ)TC(E[Y˜new(x)]−θ)+trace[CΣY˜new(x)]=(E[Y^(x)]−θ)TC(E[Y^(x)]−θ)+trace[CΣY^(x)]+trace[CΣY˜(x)]The proof of Eq. (7) can be found in Ko et al. (2005). Eq. (7) consists of three terms in its right hand side. The first term,(E[Y˜new(x)]−θ)TC(E[Y˜new(x)]−θ), represents the penalty due to the deviation from the target, i.e., the bias. The second term,trace[CΣY^(x)], represents the penalty due to the uncertainty in the predicted responses, i.e., the poor quality of predictions. The third term,trace[CΣY˜(x)], represents the penalty due to the variation of responses, i.e., the poor robustness. Three terms in Eq. (7) can be referred to as ELbias, ELqop and ELrob, respectively.Here, letY=(Y1,Y2,…,Yp)′be a multivariate (p × 1) response vector and letx=(x1,x2,…,xk)′be a (k × 1)vector of factor variables. Assuming that there are p responses and q experimental factors in multi-response optimization problems, the multi-response surface model is given as(8)Y=Bz(x)+ewhere B is a p × q matrix of model coefficients and z(x) is a q × 1 vector-valued function of factor variable x. The vector e has a multivariate normal distribution with mean vector 0 and variance–covariance matrixΣ.In the Bayesian approach, the uncertainty in model parameters is directly incorporated into the analysis. In order to account for the uncertainty in model parameters, B andΣ, the multi-response surface model may be analyzed in the framework of Bayesian modeling and optimization (Del Castillo, 2007; Peterson, 2004). Accordingly, some key issues (i.e., correlation among multiple responses, robustness of process model, and reliability of optimization results) of multi-response optimization problems can be addressed simultaneously in a single framework. Under usual non-informative prior for model parameters B andΣ, namely,(9)p(B)∝constantp(Σ)∝|Σ|−(p+1)/2So the joint prior distribution of the parameters B andΣis given as(10)p(B,Σ)∝|Σ|−(p+1)/2Given by the experimental data and factor variables x, the Bayesian posterior predictive density for Y is(11)p(Y˜new(x)|data)∝{1+1ν(Y−B^z(x))′H(Y−B^z(x))}−(p+v)/2whereH=νS−11+z(x)′D−1z(x),S=(Y−ZB^)′(Y−ZB^)D=∑i=1nz(xi)z(xi)′,B^=(X′X)−1X′Y.Here, ν is the degree of freedom (ν=N−p−q+1), and N is the sample size. Y is a N × p matrix formed by N (1 × p) y′iresponse data vectors, and Z is a q × N matrix formed by N (q × 1) z(xi) vectors.Y˜newis a p × 1 vector estimated based on the new observation z(xi).According to the relevant knowledge of multivariate statistical inference (Johnson, 1987), Eq. (3) also can be denoted as a non-central multivariate student t-distribution, namely,(12)Y˜new(x)|data∼tv(B^z(x),H−1)Since the observed responseY˜newin Eq. (12) is a multivariate random variable with t-distribution, it is easy to simulate the response values from this predictive density using Monte Carlo method. As pointed out by Johnson (1987), we can simulate a multivariate random variable with t-distribution by simulation of a multivariate normal random and an independent chi-square random variable. For the specified Eq. (12), implementation procedures of the simulation are conducted as follows (Del Castillo, 2007; Peterson, 2004). Firstly, one can simulate a multivariate normal random variable W with zero mean vector and variance–covariance matrix equals toH−1, namely,W∼N(0,H−1). Then, one can simulate a chi-square random variable U with v degrees of freedom, which is independent of W, namely,U∼χv2. Finally, we letY˜new=WvU+B^z(x). In this way, one can obtain the simulated posterior samplesY˜pnewfor the response vectorY˜newwhich has a multivariate t-distribution with v degrees of freedom.From a robustness and reliability perspective, a natural way to optimize a multi-response process is to integrate the loss function approach (Ko et al., 2005) and the reliability approach (Peterson, 2004) in a unified Bayesian modeling framework. In doing so, the proposed approach can take into account the correlation among multiple responses, the robustness of the process model and the reliability of optimization results simultaneously.On the one hand, the probability that a future multivariate response Y will satisfy some specified quality conditions A for given experimental data, P(Y(x) ∈ A|data), can be computed by a posterior predictive approach. As noted by Peterson (2004), the posterior predictive approach provides a complete way to assess the reliability of an acceptable result for any set of operational conditions from a quality perspective. Therefore, one can calculate the reliability of optimization results for given experimental data using the posterior predictive approach based on Eq. (12)(13)P(Y˜pnew(x)∈A|data)=∫AP(Y˜pnew(x)|data)dY˜pnewSince the calculation of Eq. (13) with numerical integration is an intractable issue, one can approximate the posterior probability P(x) using Monte Carlo method(14)P(x)=P(Y˜pnew(x)∈A|data)≈1Nsim∑s=1NsimI(Y˜pnew(s)(x)∈A)where Nsim is the number of simulation, and I(•) is a 0–1 binary indicator function. If the simulation sampling valueY˜pnew(s)(x)is within the region A, the binary indicator is equal to 1, and vice versa.As pointed out by Peterson (2004), since P(x) is a joint probability, it turns out that the marginal probabilities Pi(x) corresponding to marginal events about some of the response values Yican also be easily computed as a by-product of the Monte Carlo simulation process for P(x). If assumingPi(x)=P(Yi(x)∈Ai|data), where Aiis an interval (possibly one or two sided), then the Pi(x) simultaneously along with P(x) can be calculated.On the other side, the mean vectorE[Y˜pnew(x)]and the variance–covariance matrixΣY˜pnew(x)can be obtained through using the posterior samples. Then, the improved Bayesian loss functionE[L(Y˜pnew(x),θ)]for multiple responses can be expressed as(15)E[L(Y˜pnew(x),θ)]=(E[Y˜pnew(x)]−θ)TC(E[Y˜pnew(x)]−θ)+trace[CΣY˜pnew(x)]Eq. (15) consists of two terms in its right hand side. The first term,(E[Y˜pnew(x)]−θ)TC(E[Y˜pnew(x)]−θ), represents the penalty due to the deviation for the mean of posterior samplesE[Y˜pnew(x)]from the targetθ, which is denoted here as ELp-bias. The second term,trace[CΣY˜pnew(x)], represents the penalty due to the robustness when taking into consideration of the fluctuation of posterior samples, which is denoted here as ELp-rob. Then, the total of expected loss from posterior samples reduces to(16)ELp=ELp−bias+ELp−robwhere ELp denotesE[L(Y˜pnew(x),θ)]. Compared with Ko et al.’s loss function approach, the improved loss function ELp can measure the robustness of process fluctuations by taking into account the uncertainty of model parameter.By integrating the improved Bayesian loss function with the posterior predictive approach within the framework of Bayesian multivariate regression model, a new MRS optimization model is constructed as(17)minE[L(Y˜pnew(x),θ)]s.tP(Y˜pnew(x)∈A|data)≥p0The value p0 in Eq. (17) is the probability that experimenters or customers expect to be satisfied with. If analysts lack prior information for the expected probability p0 in practical applications, they can obtain an effective reference value for p0 by individually optimizing Eq. (14) to maximize the posterior probability. The reference value can provide some information to help analysts to select a suitable expected probability p0.Since Eq. (17) may become highly nonlinear and heavily constrained, conventional optimization methods may find only a local optimum or even fail to find a feasible solution (Ortiz, Simpson, Pignatiello, & Heredia-Langner, 2004). As noted by Jourdan, Basseur, and Talbi (2009), a hybrid optimization approach often performs better than one algorithm alone. In these situations, one alternative is to use a hybrid approach which can effectively incorporate the advantages of global optimization algorithms (e.g., genetic algorithm, or simulated annealing) with local search techniques (e.g., pattern search). Therefore, we utilize a hybrid genetic algorithm with pattern search method to optimize the proposed model (He et al., 2012). In addition, it is also a critical issue to improve the running speed of optimization algorithms in practical applications. In fact, we have to spend plenty of time to obtain the posterior samples using a loop statement in Monte Carlo simulation. In this case, we employ a vectorized version in Matlab codes, which was used by Del Castillo (2007), to avoid the use of loops. These codes are available by contacting the first author.Within the framework of Bayesian modeling and optimization, the steps of the proposed multi-response optimization approach can be summarized as:Step 1: Select the suitable model structure in Eq. (8) via analyzing the experimental data with ordinary least squared (OLS) for each response.Step 2: Simulate the sampling for the given experimental data and obtain the posterior samples of the responses.Step 3: Compute the mean vector and variance–covariance matrix of the predicted responses according to the posterior samples.Step 4: Construct an improved Bayesian loss function via the results of Step 2, and then regard it as the objection function of the proposed approach.Step 5: Establish a posterior prediction function through the results of Step 2 to assess the reliability of predicted responses within the specified region, and then view it as the constrained function of the proposed approach.Step 6: Use the hybrid genetic algorithm with pattern search method to optimize the proposed model and obtain the optimal parameter setting.The purpose of this experiment was to study the surfactant and emulsification variables involved in pseudolatex formation for controlled-release, drug-containing beads (Anderson & Whitcomb, 1998). The controllable factors in this experiment were: “percent of Pluronic F68” (x1), “percent of polyoxyethlene 40 monostearate” (x2), “percent of polyoxyethylene sorbitan fatty acid ester NF” (x3). The two responses were particle size (y1) and glass transition temperature (y2). An extreme vertices design was used to determine the optimal setting of x1, x2 and x3 to minimize as best as possible both y1 and y2. The data for two responses y1 and y2 are collected by experimenters through actual physical experiment. For the detailed experimental process, please refer to Anderson and Whitcomb (1998). The experimental design and observed responses are given in Table 1.Following the study of Anderson and Whitcomb (1998), a highly significant outlier (i.e., run 10) is deleted from the experimental data. The model coefficients and summary for regression models estimated using OLS are given in Table 2.The summary statistics (e.g. RMSE, predicted R2) in Table 2 indicate that the regression models have great predictive ability, which also suggest the models can predict new observations nearly as well as they fit the existing experimental data. The regression results show that the assumption of the same model structure for two responses is reasonable in the mixture experiment. As pointed out by Peterson (2004), the model structure z(x) in Eq. (8) is assumed the same for each response so that one can simulate sampling from the posterior predictive distribution of responses.Here, referring to the results of the contour lines given by Anderson and Whitcomb (1998), we define the specification limits for the responses y1 and y2 are (150, 240) and (8, 20), respectively. The responses y1 and y2 are both smaller-the-better (STB-type) responses, hence appropriate target values for two responses are set byθ={θ1,θ2:θ1=150,θ2=8}. The proposed approach assumes that the cost matrix C can be evaluated subjectively or in some other way not addressed here (Ko et al., 2005). Then, the cost matrix C is set byC=(0.1000.0250.0250.100)Given that there is no any prior information for p0 in Eq. (16), an effective reference value can be obtained by maximizing Eq. (14) with the hybrid genetic method alone. The population size is set by 100, and the pattern search is selected as the hybrid function as well as other parameters are selected as default values in Matlab Optimization Toolbox. Assuming that the number of simulation is 10,000, the posterior predictive probability that the responses satisfy the specific regionA={150≤y1s≤240,8≤y2s≤20}can be calculated using the hybrid genetic method in Eq. (14). The optimization results based on the posterior predictive approach are given in the first row of Table 3. In this case, the optimal setting is (0.25, 0.16, 0.238) and the joint posterior probability P is equal to 0.961. Meanwhile, the expected loss of the corresponding optimal setting based on Eq. (15) is 158.099.On the other hand, the expected loss function is individually optimized by Eq. (15) without consideration of the posterior probability. The optimization results based on expected quality loss function are displayed in the second row of Table 3. In this case, the optimal setting is (0.152, 0.353, 0.053) and the expected loss is equal to 4.372. Meanwhile, the posterior probability of the corresponding optimal setting is as low as 0.351.Based on the optimization results given above, it can be assumed that the expected probability is equal to 0.95 in the proposed method. Given that the same assumption in the optimization process, the proposed model, namely, Eq. (17) is optimized by the hybrid genetic method. The optimization results based on the proposed approach are shown in the last row of Table 3. In this case, the optimal setting is (0.033, 0.259, 0.219) and the expected quality loss is 43.878 as well as the joint posterior probability is 0.951. It is noted that the marginal posterior probability (P2) of response y2 is slightly less than the marginal posterior probability (P1) of response y1 in Table 3. The results show that the posterior probability may be affected by model predictive ability because the prediction R2 of response y2 is less than that of response y1.As pointed out in Section 3.2, the proposed approach can obtain global optimization results using the genetic algorithm with the pattern search method. However, the ridge analysis used by Peterson (2004) usually finds local optimization results. In addition, it is noted that the optimization results are relevant to the assumptions of the cost matrix C and the target valuesθ.The posterior predictive approach does pay more attention on the reliability of optimization results, so the posterior predictive approach in Table 3 only takes into account the posterior probability without consideration of the expected loss in the optimization process. Therefore, the corresponding value for posterior probability is bolded in the second row of Table 3. Similarly, the expected loss is individually considered to find the optimal parameter setting in the loss function approach. Hence, the loss function approach in Table 3 gives an optimal setting at which the expected loss is the best while the posterior probability is often undesirable. Hence, the corresponding value for expected loss is also bolded in the third row of Table 3. Furthermore, since the two criteria (i.e., expected loss and posterior probability) are both incorporated into the proposed optimization strategy, the corresponding values of the two criteria are bolded in the last row of Table 3.Compared with the posterior predictive approach in Table 3, the proposed approach not only greatly reduces the expected quality loss but also yields a very close posterior probability in the mixture experiment. Furthermore, compared with the quality loss function in Table 3, the proposed approach can significantly improve the posterior probability. In summary, the proposed approach clearly gives more reasonable results than the existing approaches when both quality loss and reliability of optimization results are significant issues.How to select a suitable expected probability p0 in the proposed approach is critical. Hence, it is necessary to further discuss and analyze the influence of different expected probabilities on optimization results of the proposed approach. However, the prior information for the expected probability p0 in the illustrative experiment of this paper is limited, then an effective reference value can be obtained based on the upper limit and the lower limit of expected probability p0. The joint posterior probability is equal to 0.961 when individually maximizing the posterior probability function, so the optimization result (0.961) can be viewed as the upper limit of the reference value for the expected probability. On the other hand, the expected loss is equal to 4.732 when separately minimizing the expected loss function. Meanwhile, the posterior probability of the corresponding optimal setting is as low as 0.351, which can also be regarded as the lower limit of the reference value for the expected probability. Therefore, it can be assumed that an interval [0.35, 0.95] for the expected probability p0 is given to illustrate its influence on optimization results in the proposed approach. Assuming that the same gap equals to 0.05 in the given interval [0.35, 0.95], optimization results for different expected probabilities p0 are given in Table 4.Given that the expected probability p0 is equal to 0.35, the optimization results of the proposed approach are very close to the optimization results by separately minimizing the expected loss function, which means that the proposed approach will be an unconstrained optimization problem when the expected probability p0 is less than or equal to the lower limit 0.351. Assuming that the expected probability p0 is equal to 0.98 that is greater than the upper limit 0.961, a feasible solution cannot be found by using the proposed approach in this situation. The optimization results in Table 4 show that the bias component ELp-bias has significant increase but the robustness component ELp-rob only has small variation when the expected probability p0 is gradually increasing. In addition, the relationship between expected loss and posterior probability is displayed in Fig. 1. The result in Fig. 1 indicates that the corresponding expected loss also will be increasing when the reference value p0 becomes larger step by step.Hence, the choice of a suitable expected probability p0 not only depends on the actual demands or relevant experiences from the customers or experimenters, but also involves how to balance quality loss and reliability of optimization results in multi-response optimization.It is a very insightful and constructive suggestion from an anonymous reviewer who inspires us to enhance the implications of the proposed approach in quality management. In fact, if assuming the specific region A to be product specification limits in the proposed approach, the constraint function of the proposed approach can take into account the probabilities of simulated responses which fall within the product specification limits. That is to say, the constraint function of the proposed approach pays more attention to the basic definition of product quality, namely, the collection of features and characteristics of a product that contributes to its ability to meet given requirements. In this situation, the expected probability p0 in Eq. (17) could reflect some information of the product qualification rate required by customers and experimenters. In doing so, the constraint function of the proposed approach can make sure that the probability of falling within the product specification limits can directly reflect the requirements of customers and experimenters.On the other hand, quality is measured by the degree of conformance to predetermined specifications and standards, and deviations from these standards can lead to poor quality and low reliability from the view of the manufacturers. Efforts for quality improvement are aimed at eliminating defects (products or processes that are out of conformance), reducing deviations and improving robustness, and hence overall reductions in production costs. Therefore, it is very significant to regard the expected loss function (i.e., bias and robustness) as an objective function of the proposed approach. In doing so, the proposed approach not only takes into account quality conformance with the posterior probability function, but also considers higher quality requirement such as process bias, robustness and economics through using the expected loss function in a single framework of Bayesian modeling and optimization.A new Bayesian optimization approach is proposed to consider a series of problems involving the correlation, robustness and reliability of optimization results in multi-response optimization process. The major contribution of this paper is the fact that it provides a novel optimization scheme integrating a loss function approach with a posterior predictive approach in a single framework of Bayesian modeling and optimization. The proposed approach not only considers the correlation among multiple responses and the uncertainty of model parameters, but also takes into account quality loss (bias and robustness) and reliability of the optimization results. The illustrative example also shows that the proposed approach can give more reasonable results than the existing approaches when both quality loss and reliability of optimization results are important issues. In addition, the Bayesian optimization approach using Monte Carlo simulation and hybrid genetic algorithm can quickly calculate the quality loss and posterior probability of the optimal parameter setting, which is also applicable to other multi-response optimization problems. It should be noted that the proposed optimization strategy is also applicable to other operational research optimization problems such as stochastic stock problem and supply chain optimization problem.The model forms for different responses are assumed the same in the proposed approach. However, it may be unreasonable to assume the same model forms in some practical cases (Peterson et al., 2009). It is important to note that model structure uncertainty is ignored in the proposed approach (Apley & Kim, 2011). Future research is needed to extend the proposed approach to include the uncertainty of model structure itself. In this aspect, Bayesian model average approach (see Ng, 2010; Rajagopal & Del Castillo, 2005) and seemingly unrelated regression model can be adopted to deal with this uncertainty. Apart from that, the proposed approach is based on the posterior density function for multiple responses which has a closed form. This is to say, the specific formula of posterior density function for responses y can be obtained by means of assuming parameter prior information and sample data information. However, it is unlikely to directly obtain the specific formula of posterior density function in some cases. A challenging work for further search is to simulate the posterior predictive distributions by using Markov Chain Monte Carlo (MCMC) method based on the proposed approach when the posterior density function of multiple responses is non-closed form.

@&#CONCLUSIONS@&#
