@&#MAIN-TITLE@&#
Environmentally robust ASR front-end for deep neural network acoustic models

@&#HIGHLIGHTS@&#
Effects of various front-end schemes are examined using DNN acoustic models.Meeting transcription experiments are conducted using a single distant microphone.Both speaker independent/adaptive configurations are considered.A pipeline is proposed to integrate different classes of front-end schemes.The pipeline is used to analyse the way in which different schemes interact.

@&#KEYPHRASES@&#
Environmental robustness,Deep neural network,Front-end,Meeting transcription,

@&#ABSTRACT@&#
This paper examines the individual and combined impacts of various front-end approaches on the performance of deep neural network (DNN) based speech recognition systems in distant talking situations, where acoustic environmental distortion degrades the recognition performance. Training of a DNN-based acoustic model consists of generation of state alignments followed by learning the network parameters. This paper first shows that the network parameters are more sensitive to the speech quality than the alignments and thus this stage requires improvement. Then, various front-end robustness approaches to addressing this problem are categorised based on functionality. The degree to which each class of approaches impacts the performance of DNN-based acoustic models is examined experimentally. Based on the results, a front-end processing pipeline is proposed for efficiently combining different classes of approaches. Using this front-end, the combined effects of different classes of approaches are further evaluated in a single distant microphone-based meeting transcription task with both speaker independent (SI) and speaker adaptive training (SAT) set-ups. By combining multiple speech enhancement results, multiple types of features, and feature transformation, the front-end shows relative performance gains of 7.24% and 9.83% in the SI and SAT scenarios, respectively, over competitive DNN-based systems using log mel-filter bank features.

@&#INTRODUCTION@&#
Overcoming performance degradation caused by background noise and reverberation has been one of the main challenges facing automatic speech recognition (ASR) for the last two decades. With the recent increased adoption of the ASR technology by industry, it is becoming a pressing problem to make ASR systems robust against environmental distortion. A range of approaches has been proposed to tackle this problem, including moment normalisation (de la Torre et al., 2005; Hilger and Ney, 2006), speech enhancement (Macho et al., 2002), feature enhancement (Stouten, 2006; Yoshioka and Nakatani, 2013), feature transformation (Droppo et al., 2001), and acoustic model adaptation (Kalini et al., 2010; Wang and Gales, 2012; Lu et al., 2013). Most of the existing robustness techniques have been evaluated using conventional acoustic models consisting of Gaussian mixture models (GMMs) and hidden Markov models (HMMs).In the last couple of years, significant progress has been made on acoustic modelling using context-dependent deep neural networks (DNNs), which has significantly changed the nature of acoustic models. A DNN is a multi-layer perceptron (MLP) with many hidden layers. The multiple layers of nonlinear processing allow the acoustic model to learn complex decision boundaries between HMM states. Following great initial success in several different tasks (Dahl et al., 2012; Mohamed et al., 2012a), the DNN-based acoustic model is now becoming an essential component of today's ASR systems. It has been shown that this novel acoustic modelling approach is less sensitive to variations in speech features than the conventional approach based on GMMs and that some of the classical techniques developed for GMM-based systems, such as heteroscedastic linear discriminant analysis (HLDA), have a limited impact on the recognition performance of DNN-based systems under certain conditions (Seide et al., 2011). Therefore, it is important to examine the usefulness of existing environmental robustness techniques in DNN-based systems and to analyse how the effects of the different approaches interact.The DNN-based acoustic models have two widely used configurations (Hinton et al., 2012). In the first configuration, which is called a DNN–HMM hybrid or simply a hybrid, a DNN is utilised to compute the posterior probabilities of context-dependent HMM states based on observed feature vectors (Morgan and Bourlard, 1995; Renals et al., 1994; Dahl et al., 2012). A Viterbi decoding is then performed with these posteriors. The second configuration, which is called an MLP tandem or a tandem for short, uses the DNN to perform a nonlinear discriminative feature transformation, which yields MLP features (Hermansky et al., 2000; Grezl et al., 2007). These features are merged with a standard set of features, such as MFCCs or PLP coefficients, to form a new set of features, which are collectively called TANDEM11In this paper, we capitalise the term ‘tandem’ to refer to a feature vector. When we refer to an acoustic model configuration, we use lower-case letters.features, and then input into a GMM-HMM acoustic model. This type of acoustic models has often been used with a speaker adaptive training (SAT) set-up since various adaptation techniques are available for GMM-HMM acoustic models, including cluster adaptive training (Gales, 2000) and CMLLR (Gales, 1998). More recent, but less established, architectures, such as stacked hybrids (Knill et al., 2013) and convolutional neural networks22A conference paper discussing the use of CNNs in speaker independent meeting transcription was published just before submission of this paper (Renals and Swietojanski, 2014).(CNNs) (Abdel-Hamid et al., 2012), are not considered in this paper.While DNN acoustic models have been successfully applied to noisy speech recognition tasks with both configurations, little has been revealed about which element of the DNN acoustic model is susceptible to environmental distortion and which robustness techniques still matter in the DNN-based systems. The DNN–HMM hybrid approach was first applied to a noisy ASR task by Seltzer et al. (2013). They achieved the best published result on the Aurora 4 data set using a multi-condition hybrid acoustic model trained with the drop-out technique. They also showed that performing speech enhancement on both training and test sets with the method described in Yu et al. (2008) degraded the recognition performance. Geiger et al. (2014) showed that a non-negative matrix factorisation-based enhancement method improved the recognition performance of a heterogeneous acoustic model consisting of GMM-HMMs and a long short-term memory network in the CHiME2 medium vocabulary task. Li and Sim (2013) attempted to exploit a vector Taylor series (VTS) adaptation to improve the noisy digit recognition performance of a hybrid acoustic model. A few papers were also presented at ICASSP 2014 that proposed front-end processing schemes using clean/noisy stereo corpus (Narayanan and Wang, 2014; Li and Sim, 2014; Weninger et al., 2014). On the other hand, a large body of work applied MLP tandem acoustic models, with both shallow and deep configurations, to tasks related to environmental robustness such as meeting and lecture transcriptions based on distant microphones (Hain et al., 2012; Stolcke, 2011; Chang et al., 2013). These previous efforts showed that, while DNN acoustic models greatly outperform conventional GMM-based models in acoustically adverse environments, they still suffer from acoustic degradation. However, since the previous work made little use of existing robustness techniques developed for GMM-based models, the performance gain that can be obtained from the existing techniques when DNNs are being used is unknown. Furthermore, most previous work used small to medium vocabulary tasks with simulated data, such as Aurora 2 and Aurora 4. and considered only additive noise.In this paper, we investigate the individual and combined impact of various front-end approaches for environmental robustness on the performance of DNN-based systems. To this end, after describing the data sets used in this work and our baseline DNN-based acoustic models in Section 2, we examine the environmental robustness of DNN acoustic models trained on corrupted data in Section 3. Our investigation uses the AMI meeting corpus (Carletta et al., 2006) with a single distant microphone set-up, which enables us to evaluate the practical relevance of the techniques being investigated. In Section 4, we classify various front-end processing approaches based on functionality and evaluate each class of approaches individually to reveal its fundamental usefulness in DNN-based systems. Based on this investigation, we propose a front-end processing pipeline in Section 5 to allow different classes of approaches to be incorporated efficiently in a single system. Using this front-end, we evaluate the combined effects of different classes of approaches with both hybrid SI and tandem SAT configurations. Finally, we conclude the paper in Section 6. Although many of the techniques considered in this paper have been described previously, there has been no work that has investigated their individual and combined effects on the DNN acoustic models in practically relevant tasks with both SI and SAT set-ups.Note that stereo corpus-based feature transformation and noise insensitive parameterisation are not covered in this paper. The former learns a mapping from corrupted features to their underlying clean features using a set of clean and noisy feature pairs, where the mapping may be modelled using piece-wise linear functions (Droppo et al., 2001) or neural networks (Narayanan and Wang, 2014; Li and Sim, 2014; Weninger et al., 2014). These methods have been successful for artificially simulated data. However, they assume a stereo corpus to be collectable from target or acoustically similar environments while the way to do this has yet to be established for practical applications.33Du et al. (2014) describes an attempt to overcome this limitation by generating pseudo-clean features.The latter approach seeks acoustic features that are inherently robust against noise. Such features include power normalised cepstral coefficients (Kim and Sterm, 2012) and frequency domain linear prediction (Thomas et al., 2008). To limit the scope of this paper, we focus on conventional features, such as MFCCs and log mel-filter bank outputs.This work used the AMI meeting corpus. The corpus consists of recordings of meetings conducted in English at three different sites. Each meeting has four participants. The meetings are either scenario-based role playing discussions or natural unconstrained conversations. Many of the meeting participants are non-native speakers and hence they may have very different accents and speaking styles. Each meeting was recorded with an eight-channel circular microphone array placed on a table in a meeting room, providing eight-channel synchronised acoustic signals. In this paper, we focus on a single distant microphone (SDM) task in which only the first channel is used.44Our work on a multiple distant microphone task is described in Yoshioka et al. (2014).The large distances between the microphone and the speakers mean that the speech signals are distorted by reverberation and background noise. The background noise is almost stationary and the SNR does not vary significantly over different meetings. As regards reverberation, the three meeting rooms seem to have similar reverberation times. However, since the relative positions of individual speakers in relation to the microphone vary significantly and some speakers appear to be moving within an utterance, different utterances may have different reverberation characteristics. See the corpus website (https://www.idiap.ch/dataset/ami) for further details.We divided the corpus into training, development test, and evaluation test sets in the same way as Breslin et al. (2011). Specifically, we selected a set of eight meetings (ES2009a–d and IS2009a–d) as a development test set and another set of eight meetings (ES2008a–d and IS2008a–d) as an evaluation test set. The remaining portion of the corpus was used for training. The development and evaluation sets each included eight speakers while the training set consisted of utterances from 175 speakers. We excluded overlapping speech segments from both the training and test sets, which left 59h of speech for training, 2.7h for the development test, and 2.6h for the evaluation. Note that these quantities refer to the quantities of audio after segmentation. Speech segments and speaker identities were obtained from manual annotations.55Although accurate speaker diarisation is a challenging task from a single microphone input, our SAT experiments allow us to understand the impact of adaptation on the recognition performance in adverse acoustic environments and the insights obtained from the experiments will be useful for other tasks.In our experiments, we considered both SI and SAT set-ups. With the SI set-up, adaptation was not performed in either the training or test stages. The speaker information was used only for mean and variance normalisation (MVN). Specifically, we performed normalisation processing on a per speaker and per meeting basis. We did this so that the SI and SAT systems used the same input features, which enabled us to accurately evaluate the gains from adaptation techniques on top of speaker and session-level MVN. With the SAT set-up, the acoustic models were trained with a speaker adaptive approach. The test data were swept over multiple times, which means that decoding was performed with off-line processing.In this section, we describe the ways in which we built our baseline SI and SAT systems and show the word error rates (WERs) of these systems. We employed a DNN-HMM hybrid configuration for the SI set-up because it achieved lower WERs than a tandem configuration in our preliminary tests (see Yoshioka et al., 2014 for a comparison of the hybrid and tandem SI performances). On the other hand, we adopted the MLP tandem configuration to build our SAT systems, which allowed us to exploit conventional adaptation techniques including CMLLR and MLLR. The tandem configuration enables SAT to be performed with any input feature type, which matters in the experiments using expanded feature sets described later. These two baseline systems are described in Sections 2.2.1 and 2.2.2, respectively.Fig. 1shows the processing flow of the baseline hybrid SI system. The input features consisted of 24-channel log mel-filter bank outputs, which are called FBANK features. We employed these features because they provided lower WERs than MFCCs, which is consistent with the findings of other studies (Mohamed et al., 2012b; Deng et al., 2013). These input features were concatenated with their delta parameters up to the third order, resulting in a stream of 96-dimensional feature vectors. Then MVN was performed on a speaker-by-speaker, meeting-by-meeting basis. The resultant normalised feature vectors were spliced with neighbouring feature vectors within a context window. This context-extended feature set was fed into a DNN to compute the posterior probabilities of individual context-dependent HMM states. Decoding was performed based on these posteriors with the Viterbi algorithm. In our default settings, the context window consisted of nine consecutive frames (four frames on each side) and the DNN consisted of five hidden layers, each with 1500 units, followed by a softmax output layer. Our previous experiments showed that changing the context window size and the network topology had little impact on the recognition performance for the same data set (Yoshioka et al., 2014).The hybrid SI system was trained according to the standard recipe (Dahl et al., 2012), consisting of two stages. In the first stage, an underlying GMM-HMM system was built using 13 MFCCs (including C0). The initial step of the GMM-HMM system construction was to augment the training MFCC set with their delta coefficients up to the third order, which yielded 52-dimensional feature vectors. This was followed by speaker and meeting-level MVN. These normalised feature vectors were projected onto a 39-dimensional feature space by HLDA. Then, a maximum likelihood GMM-HMM acoustic model was trained to model the HLDA features. The acoustic model consisted of approximately 4000 context-dependent states and 16 Gaussians per state. The model was further refined by MPE training, which yielded the baseline GMM-HMM SI system.In the second stage, a DNN was trained to predict the context-dependent HMM states from context-extended FBANK feature vectors. The second stage began with the forced alignment of the entire training data set to produce frame-level state labels. Then, the DNN was trained to predict these labels from the nine-frame context-extended feature vectors. This DNN training was achieved by layerwise discriminative pre-training (Seide et al., 2011), followed by fine-tuning based on a cross entropy criterion. The DNN trained in the second stage and the HMMs obtained in the first stage constitute the baseline hybrid SI acoustic model. At the test stage, decoding was performed by bigram lattice generation with a 40K-word language model, followed by trigram lattice rescoring and confusion network rescoring. The language model was built from a variety of sources including transcriptions of AMI, ICSI, NIST, and ISL meetings, Callhome, Switchboard, Gigaword, and extra web data (Breslin et al., 2011).Table 1shows the performance of the baseline hybrid SI system and that of the MPE-trained GMM-HMM system. The DNN acoustic model improved the average WER from 55.2% to 43.1%, providing a relative improvement of 21.9%.66The WER of 43.1% is comparable to performance figures reported by other sites on the same corpus with similar acoustic model configurations (Swietojanski et al., 2013), although the partitions of the corpus are slightly different.This gain is consistent with other work and indicates the usefulness of the DNN acoustic models in adverse acoustic conditions. Section 4 investigates various approaches to further improve this competitive baseline system.Fig. 2shows the processing flow of the baseline tandem SAT system, which is based on the bottleneck configuration (Grezl et al., 2007). As in the SI system, input features consisting of 24 FBANK coefficients were appended with their first, second, and third-order delta coefficients and then mean and variance-normalised on a speaker-by-speaker, meeting-by-meeting basis. Then, each of the resultant 96-dimensional feature vectors was extended using a nine-frame context window, yielding a sequence of 864-dimensional feature vectors. The tandem DNNs used in this work consisted of four hidden layers, each with 1500 units, followed by one hidden bottleneck (BN) layer and a softmax output layer, where the BN layer had only 26 units. The linear outputs from the BN layer were further converted by a global semi-tied covariance (STC) transform (Gales, 1999) and concatenated with the corresponding MFCC-derived HLDA features to form 65-dimensional TANDEM feature vectors. The TANDEM features were recognised with a GMM-HMM system adapted with global CMLLR and MLLR mean transforms (Gales, 1998).Our tandem system construction followed the rapid training pipeline described in Park et al. (2011). A DNN with a BN layer was trained in the same way as the DNN of a hybrid SI system. After the DNN was trained, each training feature vector was forwarded through the network and the linear outputs from the BN layer were computed. A global STC transform was optimised for and applied to these BN features. Then each BN feature vector was concatenated with the corresponding HLDA feature vector to generate a 65-dimensional TANDEM feature vector. A GMM-HMM acoustic model was trained with an ML criterion using these TANDEM features. With this ML model, global CMLLR transforms were obtained for each speaker to eliminate speaker-specific variations. Finally, based on these speaker-normalised TANDEM features, MPE training was performed to obtain a discriminative SAT acoustic model.At the test stage, adaptation was performed with supervisions generated by the baseline hybrid SI system. Global CMLLR and MLLR mean transforms were optimised for each speaker at each meeting and used to adapt the test TANDEM features and the TANDEM-feature GMMs, respectively. Decoding was performed by bigram lattice generation, followed by trigram lattice rescoring and confusion network rescoring as with the SI set-up.Table 2shows the WERs of the baseline tandem SAT system and the MPE-SAT GMM-HMM system. The results clearly show the advantage of the DNN tandem acoustic model over the GMM-HMM acoustic model with conventional non-MLP features. The DNN tandem SAT system outperformed the MPE-SAT GMM-HMM system by 8.7% absolute, or 17.6% relative. Comparing Tables 1 and 2, we can see that the tandem SAT system achieved a lower WER than the DNN–HMM hybrid SI system, indicating the benefit of SAT. Note that the GMM-HMM SAT system used adaptation supervisions generated by the MPE-trained GMM-HMM SI system. When we used the supervisions produced by the hybrid SI system, the WERs were improved to 47.3% and 48.7% for the development and evaluation test sets, respectively, although these numbers were still far greater than those of the tandem SAT system.In the following sections, results should not be compared across tables because different tables may use different configurations.First, we conducted a set of experiments to identify the causes of performance limitation. While acoustic features extracted from a distant microphone are certainly degraded by background noise and reverberation, it is unclear how much of the recognition error can be attributed to the acoustic degradation. In addition, while training of a DNN-based acoustic model (in particular, a hybrid model) consists of two stages, i.e., state alignment generation and DNN parameter learning, it is an open question as to which stage is more susceptible to acoustic distortion. This section describes our experimental results related to these questions.To examine the impact of environmental distortion on recognition performance, we used meeting audio recorded with individual headset microphones (IHMs) in addition to the SDM recordings. The AMI corpus contains both distant microphone recordings and those recorded with IHMs that were synchronised with distant microphones. Since the IHM data are little affected by environmental distortion, the contrast between the performance of the SDM and IHM data sets illuminates the performance loss caused by environmental distortion. To enable an accurate evaluation of the performance difference between the SDM and IHM set-ups, the IHM training and test sets did not contain overlapping speech segments.Table 3contrasts the system trained and tested on the IHM data set with the system trained and tested on the SDM data set, both using the hybrid SI configuration. The IHM system was built exactly in the same way as the SDM system. Note that, in this experiment, we used DNNs with 1000×5 hidden units. A significant performance gap can be seen between these two systems, which means that the acoustic degradation caused by the background noise and reverberation constituted a major cause of the recognition errors of the SDM system. Specifically, 39.2% of the word errors made by the SDM system can be attributed to environmental distortion. This clearly indicates that, while acoustic modelling based on DNNs provides significant performance gains, such acoustic models are still liable to be harmed by environmental distortion even when they are trained on corrupted data.A further experiment was performed to investigate whether state alignments or DNN parameters are more susceptible to environmental distortion and thus need improvement. To this end, we developed two hybrid SI systems. One was based on a DNN trained on the SDM data set using the state alignments generated by performing forced alignment on the IHM data set with the IHM MPE system. Thus, this system shared the alignments and the HMMs with the IHM system described above. The other system was trained on the IHM data set while the state alignments and the HMM set were taken from the SDM MPE system.The WERs obtained with these cross-set training set-ups are listed in Table 4. When we used the SDM data as inputs, the average WER was as high as 41.3% even though the alignments and the HMM set were taken from the IHM system. Increasing the number of hidden units to 2000×5 did not change the performance at all. On the other hand, when we used the IHM data as the inputs, the average WER was significantly improved, achieving 28.8%, in spite of the use of the SDM-based state alignments and the SDM-derived HMM set. These results clearly indicate that the DNN parameters are prone to acoustic distortion and needs improvement for environmental robustness.In Section 3, we showed that making DNN parameters insensitive to distortion would result in a higher degree of environmental robustness of a recognition system. In this section, we review various environmental robustness techniques, classify them into distinguishable categories, and evaluate the efficacy of individual classes of approaches in the SDM meeting transcription task with the hybrid SI set-up. Our aim is to examine whether each class of approach can fundamentally improve the performance of DNN-based acoustic models. Since the prerequisite for such environmentally robust ASR is insensitivity to the irrelevant variations that exist in observed speech, some of the approaches considered in this section can also be applied to general ASR tasks.Following the classification of Yu et al. (2013), we can distinguish three categories of robustness approaches based on where the operation takes place in the processing flow of a system.•Front-end processing – this enhances, transforms, or expands a set of features input into a DNN to make them more robust against environmental distortion.Network adaptation – this adjusts DNN parameters to the characteristics of each environment or speaker to make the DNN less sensitive to variations resulting from differences in speakers and environments.Output transformation – this transforms the outputs from the DNN (i.e., context-dependent HMM posteriors or TANDEM features) for each environment or speaker to eliminate irrelevant variations from these DNN outputs.This paper exclusively considers the front-end processing approaches and only explains the other two approaches briefly below.The effectiveness of network adaptation, which directly modifies DNN parameters, has been established mainly in supervised speaker adaptation tasks. A representative network adaptation approach is regularised retraining, which modifies the DNN parameters to improve the classification accuracy for a given adaptation data set while keeping the transformed model from deviating too much from the original model. Such regularisation can be achieved by using an L2 penalty (Liao, 2013) or a Kullback–Leibler divergence penalty (Yu et al., 2013). While this approach has been successful in supervised speaker adaptation tasks, there has been limited evidence of its usefulness in large vocabulary unsupervised adaptation tasks, particularly those with high error rates.For output transformation, the form of the transform depends on the acoustic model configuration. When the acoustic model is based on a tandem configuration, existing feature transformation and GMM adaptation techniques can be employed since the outputs from the DNN are recognised with a conventional GMM-HMM acoustic model. For example, front-end CMLLR (Gales and Flego, 2012) and global CMLLR may be used to perform environment- or speaker-specific TANDEM feature transformation while cluster adaptive training (Gales, 2000), regression-class CMLLR and MLLR may be utilised for GMM adaptation. The effect of SAT using a tandem configuration was confirmed in the previous section. With a hybrid configuration, the state posteriors generated by the DNN may be further converted with a linear transform. Such a transform can be estimated with an output discriminative linear transform (Yao et al., 2012) or similar techniques.The objective of front-end processing is to convert an observed speech signal to a set of DNN input features that are insensitive to environmental distortion while simultaneously containing a sufficient amount of discriminant information. This can be approached in three different ways.•Speech enhancement – this attempts to recover clean features by exploiting models of speech, noise, and the way in which speech and noise interact to form a noisy speech signal. In the classification adopted here, speech enhancement refers to a process exploiting an interaction model of speech and noise and therefore acts before MVN as MVN processing makes it impossible to mathematically formulate the relationship between clean speech and noise. Example methods are spectral subtraction, Wiener filtering, and front-end VTS (FE-VTS).Feature transformation – this class of approaches, namely where the feature transforms are learned from data unlike the speech enhancement approaches, aims at reducing irrelevant variations from the MVN-processed features and possibly enhancing the discriminant information inherent in the features. FMPE (Povey et al., 2005), global CMLLR, and linear input network (LIN) (Neto et al., 1996) can be placed in this category. Note that we classify stereo corpus-based speech enhancement methods, such as stereo piecewise linear compensation for environment (SPLICE) (Droppo et al., 2001), as a feature transformation approach.Feature set expansion – this expands a feature set by including auxiliary features that are obtained in different ways from the primary feature extraction path. The objective is to provide the DNN with complementary information to improve the classification accuracy.Fig. 3shows the processing flow of robust feature extraction, which highlights where the operations of each category of approaches take place. In the rest of this section, we review the approaches of each category and discuss their characteristics. Then, we describe the results of a series of experiments conducted to evaluate the impact of individual classes of approaches. Based on these experimental results, in the next section a front-end processing pipeline is proposed to further investigate the combined effects of different approaches.Features extracted from clean speech signals contain much more discriminant information than those of corrupted speech. Therefore, if clean features can be recovered to some extent, better DNN parameters will be obtained. The aim of speech enhancement is to achieve this by removing background noise and reverberation from observed speech.A characteristic common to all speech enhancement approaches is that they exploit explicit models of the relationship between clean speech, noise, and noisy speech rather than statistically learning such relationships from data. This allows these approaches to be applied on a relatively short time scale basis, for example, on a frame-by-frame, block-by-block, or utterance-by-utterance basis. It is also important to note that most speech enhancement methods efficiently exploit a much longer acoustic context than the DNNs for noise and reverberation estimation. The DNNs cannot necessarily learn environmental characteristics by extending the temporal coverage of a context window as demonstrated in Yoshioka et al. (2014) probably because such a long context window may make training difficult.As shown in the diagram in Fig. 3, the speech enhancement approaches can be further classified according to the quantities that they modify. Below, we discuss the general characteristics of these approaches to clarify the commonalities and differences.The first approach, linear filtering, processes time-domain signals, or almost equivalently, complex-valued short time Fourier transform (STFT) coefficients with a linear time-invariant filter or a filter that slowly changes with time. The enhanced time-domain or STFT-domain speech signals are transformed into FBANK features. This approach requires an array of synchronised microphones to reduce additive noise (Tashev, 2009). By contrast, for reverberation reduction, or dereverberation, there are established families of linear filtering algorithms that can be applied to single microphone signals (Yoshioka et al., 2012). In one such method called weighted prediction error (WPE) minimisation (Yoshioka and Nakatani, 2012), enhanced STFT coefficient ytis computed as(1)yt=xt−∑k=T⊥T⊤gk*xt−k,where xtand(gT⊥,…,gT⊤)denote an observed STFT coefficient and a set of filter coefficients, respectively, with t being a frame index. Note that the frequency bin index is omitted for conciseness. Thanks to the linear convolutional nature of the room impulse responses, which constitute reverberation, there is a set of filter coefficients that can cancel the effect of reverberation (Abed-Meraim et al., 1997). In practice, the filter coefficients need to be estimated based on the observed speech. See Yoshioka et al. (2012) and the references therein for filter estimation methods and a detailed discussion.A distinctive characteristic of linear filtering as opposed to the other speech enhancement approaches is that this approach is unlikely to produce unnatural artefacts or irregular transitions between frames. This is because this approach is based on a time-invariant filter, whereas spectral and feature enhancement approaches process signals on a frame-by-frame basis. Thanks to this property, the features obtained with linear filtering can be directly fed into a standard speech recognition pipeline unlike those generated by the other enhancement approaches as discussed later.The second approach, spectral enhancement, operates after obtaining the magnitudes or squared magnitudes of the observed STFT coefficients. The principle is to estimate the noise components of each short time frame of the observed speech and then remove them from the observations. The removal operation can be performed, for example, by assuming the power spectra of speech and noise to be additive, i.e.,(2)|xt|2=|st|2+|nt|2,where stand ntdenote speech and noise STFT coefficients, respectively. There are a large number of spectral enhancement methods for background noise reduction, including spectral subtraction (Boll, 1979), log-spectral amplitude estimation (Ephraim, 1985), and the two-stage Wiener filtering used in the ETSI advanced front-end (ETSI ES 202 050 Ver. 1.1.5, 2005). There are several spectral enhancement methods that aim at reducing reverberation (Lebart et al., 2001; Kameoka et al., 2009).We found that, when an acoustic model was trained on corrupted data, it was good to use the enhanced speech only for computing static features. The enhanced static features were combined with dynamic features obtained from the original un-enhanced speech. Similar results were reported for GHM-HMMs in Droppo and Acero (2008). Although no clear explanation has been given, we suspect that this is because the frame-by-frame operation performed by spectral enhancement causes unnatural transitions between neighbouring static features. The dynamic features computed from the enhanced static features could thus be less reliable than those computed from the original static features.As an alternative to spectral enhancement, enhancement may be achieved after coding observed signals into features. Typically, the feature enhancement approach also assumes the additivity of speech and noise power spectra (although there are several methods that account for the modelling errors resulting from this additivity assumption (Deng et al., 2004)), which may be formulated in the FBANK domain as(3)xt=st+h+log1+exp(nt−xt−h),wherext,st, andntdenote FBANK feature vectors of corrupted speech, clean speech, and additive noise, respectively, at time t, andhdenotes a static convolutional noise vector. Unlike the speech enhancement approach, most feature enhancement methods exploit a statistical model of clean features, such as GMMs and ergodic HMMs, to effectively compensate for environmental distortion. A variety of methods have been proposed to compensate for the degradation caused by background noise, including a front-end vector Taylor series (VTS) (Moreno et al., 1996; Stouten, 2006) and an unscented transform (Shinohara and Akamine, 2009). There are also a few methods that compensate for the effect of reverberation in the feature domain (Krueger and Haeb-Umbach, 2010; Yoshioka and Nakatani, 2013).The feature enhancement approach is similar to spectral enhancement in the sense that both approaches modify observed speech on a frame-by-frame basis. Therefore, in our experiments, enhancement was performed only on static features, and delta coefficients were computed from original un-enhanced speech. It is sometimes argued that using a clean feature model allows the feature-domain approach to generate features that are less distorted than those obtained with spectral enhancement as demonstrated in Yoshioka and Nakatani (2013). Although this seems to be true for clean training tasks, it has yet to be clarified whether the same argument applies to multi-condition or matched training tasks when a DNN-based acoustic model is used.As shown in the diagram in Fig. 3, the (original or enhanced) static FBANK features are combined with dynamic features and then mean and variance normalised to reduce the speaker-specific variations. The features generated by MVN may be further transformed to reduce the remaining irrelevant variations while enhancing the discriminant information inherent in the features. Feature transformation is usually achieved with a data-driven approach since a simple relationship, such as Eqs. (1) and (2), cannot be formulated between the normalised clean and noisy features. In the following, three forms for transformation are identified and discussed.The first form modifies feature vectors on a frame-by-frame basis, thus applying different modifications to different feature frames. If stereo data consisting of clean and noisy feature pairs can be used, SPLICE (Droppo et al., 2001) may be used, in which the transform takes the following form:(4)yt=xt+∑c=1Cγt,cbcwithxtandytbeing original and transformed feature vectors, respectively. Eachbcis called a correction vector and the set of correction vectors, (b1, …,bC), defines the transform. Which correction vector to use is determined by γt,cand computed as the posterior probability of a C-component GMM of the original feature vectors. A stereo training data set is used to train the correction vector set. When stereo data are unavailable, FMPE may be used as an alternative way of estimating the correction vector set without utilising such a stereo corpus. With FMPE, the correction vector set is trained by interleaved updates of the correction vector set and an acoustic model parameter set, where the respective updates are achieved with MPE and ML criteria (Povey et al., 2005). The mathematical link between SPLICE and FMPE is discussed in Deng et al. (2005).Apart from the frame-by-frame transformation approach, when speaker (or environment) labels are available, global CMLLR feature transformation may be performed to de-emphasise the differences between speakers (or environments).77Although we use the term “speaker adaptation”, adaptation was actually performed at the meeting and speaker level in our experiments, i.e., the fact that some of the speakers were present at multiple meetings was not exploited.With global CMLLR, each feature vector of speaker s is transformed as(5)yt=A(s)xt+b(s),whereA(s) andb(s) define a transform for this speaker. The speaker transform is estimated for each speaker contained in the training and test data sets by using a GMM-HMM acoustic model based on FBANK features. Such an FBANK model can be efficiently constructed from a baseline MFCC-based GMM-HMM acoustic model with single pass retraining (SPR) (Young et al., 2009). Since FBANK features are closely correlated with each other, it is essential to use full covariance matrices or semi-tied covariance (STC) transforms (Gales, 1999). In our experiments, we used a global STC transform. Thus, SPR was configured to produce both FBANK-space GMM parameters and a global STC transform. Therefore, the speaker transforms actually used in our experiments had the following form:(6)yt=A(s)Uxt+b(s),whereUdenotes the global STC transform. We used CMLLR rather than DNN-based discriminative methods, such as LIN. According to Li and Sim (2010), since CMLLR is based on generative principles, it is more effective than LIN for unsupervised adaptation in high error rate tasks.Yet another approach is utterance-based transformation, the concept of which may be implemented by quantised CMLLR (Q-CMLLR) as explained below. The fundamental idea is to cluster the utterances in the training set into disjoint classes in some appropriate way and train a global CMLLR transform for each utterance class. At the test stage, each test utterance is clustered into one of the classes. Then the transform associated with the selected class is applied to the utterance. Unlike the speaker transformation approach described above, utterance-based transformation can be applied to SI scenarios. Although frame-based transformation is also applicable to SI tasks, there is a fundamental difference between the utterance-based and frame-based approaches. While the frame-based approach selects the transform to apply based on a relatively short acoustic context (i.e., the centre frame plus several adjacent frames), the utterance-based approach uses all the feature vectors within an utterance to select the transform, thus utilising a much longer acoustic context than the DNNs. This may be viewed as performing very rapid adaptation with one recognition pass (Yao et al., 2011).As our preliminary test showed little performance gain from using FMPE,88This is also shown by the experimental results presented at http://wissap.iiit.ac.in/proceedings/TSai_L7.pdf.our experiments investigate the other two approaches, namely the utterance-based and speaker-based approaches for SI and SAT scenarios, respectively.With the goal of providing complementary information to help a DNN discriminate between different HMM states, feature set expansion expands the default feature set, consisting of log mel-filter bank outputs and their dynamic parameters, by adding auxiliary features. It is desirable for the auxiliary features to be extracted with little additional computational cost. A simple example is to add static MFCCs to the feature set, which indeed provides meaningful performance gains (Section 4.2.3).It is useful to note that the feature set expansion approach becomes effective only with DNN-based acoustic models. This is because the DNNs are much more insensitive to the increase in input dimensionality than GMMs when a sufficient quantity of training data is available. Furthermore, as the DNNs can deal with correlations between different features, it is possible to include multiple features that have strong correlations. This research direction was initiated by the work described in Plahl et al. (2011), in which the authors combined three different coding systems, each extracting MFCCs, PLP coefficients, or gammatone features, using an MLP with a BN layer. The feature combination improved the performance of their Spanish recognition system. Noise aware training, whereby each observed feature vector is concatenated with a noise feature estimate, may be regarded as expanding the feature set (Seltzer et al., 2013).Feature set expansion is particularly interesting for noisy speech recognition because it offers an alternative way of exploiting speech enhancement and feature transformation outputs. Conventionally, enhanced or transformed features are substituted for the original un-enhanced features as explained earlier. Instead, it is possible to use both the original and enhanced features as inputs into a DNN (Weninger et al., 2014). This allows us to utilise multiple speech enhancement and feature transformation systems that cannot be arranged in tandem. Another motivation for expanding the feature set is to complement features masked by environmental noise. Since some speech features are lost in acoustically adverse environments due to the masking property of noise, using extra features would lead to performance improvement when they are extracted in a way that emphasises parts of the observed speech that are likely to be disregarded in the primary feature extraction path.

@&#CONCLUSIONS@&#
