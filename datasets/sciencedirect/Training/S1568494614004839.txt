@&#MAIN-TITLE@&#
Elitist clonal selection algorithm for optimal choice of free knots in B-spline data fitting

@&#HIGHLIGHTS@&#
In this paper we introduce an adapted elitist clonal selection algorithm for automatic knot adjustment of B-spline curves.Our method determines the number and location of knots automatically in order to obtain an extremely accurate fitting of data.In addition, our method minimizes the number of parameters required for this task.Our approach performs very well and in a fully automatic way even for the cases of underlying functions requiring identical multiple knots, such as functions with discontinuities and cusps.Our experimental results show that our approach outperforms previous approaches in terms of accuracy and flexibility.

@&#KEYPHRASES@&#
Reverse engineering,B-spline curve fitting,Knot adjustment,Artificial immune systems,Clonal selection algorithm,

@&#ABSTRACT@&#
Data fitting with B-splines is a challenging problem in reverse engineering for CAD/CAM, virtual reality, data visualization, and many other fields. It is well-known that the fitting improves greatly if knots are considered as free variables. This leads, however, to a very difficult multimodal and multivariate continuous nonlinear optimization problem, the so-called knot adjustment problem. In this context, the present paper introduces an adapted elitist clonal selection algorithm for automatic knot adjustment of B-spline curves. Given a set of noisy data points, our method determines the number and location of knots automatically in order to obtain an extremely accurate fitting of data. In addition, our method minimizes the number of parameters required for this task. Our approach performs very well and in a fully automatic way even for the cases of underlying functions requiring identical multiple knots, such as functions with discontinuities and cusps. To evaluate its performance, it has been applied to three challenging test functions, and results have been compared with those from other alternative methods based on AIS and genetic algorithms. Our experimental results show that our proposal outperforms previous approaches in terms of accuracy and flexibility. Some other issues such as the parameter tuning, the complexity of the algorithm, and the CPU runtime are also discussed.

@&#INTRODUCTION@&#
Fitting curves to data points is a relevant problem in many applied domains. For instance, it is a key technology in reverse engineering, a field that aims to obtain a digital representation of the curves and surfaces of an already existing real-world object [52,66]. Typically, the geometry of that object is captured through a laser scanner or other digitizing methods, as it happens in the construction of car bodies, ship hulls, airplane fuselage and other free-form objects. Other relevant examples include the shoes industry, archeology (reconstruction of archeological assets), medicine (computer tomography), and many others.As a result of this process, we are provided with a set of digitized/sampled data points that are to be fitted by using a mathematical function in order to recover the shape of the object. Different functions can be used to this purpose. If the shape of the object is simple, it can be faithfully represented by means of an algebraic function. However this approach is not well suited for complex shapes. Classical choices in this case are the free-form piecewise polynomial functions, such as Bézier and B-splines [14,18,19,33,65]. In general, B-splines are the most commonly used approximating functions because they are very flexible, widely available, have powerful mathematical properties and can represent well a large variety of shapes [15,50].In real settings, large sets of noisy data points are commonly obtained, so approximation is preferred over interpolation, usually in the form of a least-squares optimization problem. This problem is far from being trivial because it requires to obtain suitable values for the B-spline parameters. In particular, the choice of knots has considerable effect on the shape of a curve [15,24]. Most current methods consider fixed knots, so that the fitting process reduces to a simple linear optimization problem. However, the resulting model is severely limited and this approach fails to reconstruct models of complicated shapes or non-trivial geometric features (such as discontinuities, cusps, turning points, or self-intersections). This limitation can be overcome if knots are treated as free variables [3,4,37,47]. Unfortunately, the knot adjustment problem (obtaining suitable values for the knots of B-spline curves) is a multivariate, continuous, nonlinear optimization problem [13]. Furthermore, it is also affected by the (potential) existence of many local optima of the least-squares function [37,57]. As a result, this problem also becomes multimodal, i.e., it typically has several (global and/or local) good solutions, making it hard (if not impossible) to determine when the global optimum has been attained.Another critical issue is that B-spline curves exhibiting sharp features (such as discontinuities or cusps) require multiple identical knots in their knot vector. In the absence of information about the underlying function (the most typical case in many practical situations), it is difficult to determine from the cloud of data points whether or not identical knots are actually necessary. A typical example is given by discontinuous functions, where identical knots appear at the discontinuity points as many times as the order of the B-spline curve. Most existing methods for knot adjustment assume the smoothness of the underlying curve and, therefore, they do not accept multiple knots. Consequently, they cannot represent accurately functions exhibiting sharp features. As we will shown in Section 6, our proposal overcomes this limitation at full extent.The knot adjustment problem has been the subject of intensive research for decades (see Section 2 for details). Due to the multimodal nature of this problem, there is a need for powerful global optimization methods for solving it. Recently, researchers in the field are turning their attention towards computational algorithms for optimization inspired in biological systems. Amongst them, Artificial Immune Systems (AIS) is an emerging branch of evolutionary computation aimed at replicating algorithmically the behavior of the human immune system. AIS have very remarkable properties (such as self-identity, uniqueness, adaptability, reinforcement learning, memory, pattern-recognition and machine learning) and have been successfully applied to solving both supervised and unsupervised optimization problems [11].In this paper we apply the AIS paradigm to solve the knot adjustment problem for B-spline curves. The AIS can be understood as a computational methodology based upon metaphors of the biological immune system. As such, there is not only one but several AIS algorithms. In this paper we focus on the clonal selection theory, which explicitly takes into account the learning and affinity maturation processes of the immune response deployed by B-cells. This algorithm is applied to determine the location of knots automatically (i.e., without human intervention) and efficiently. Our scheme yields very accurate results even for curves with discontinuities and/or cusps. Experimental results show that our proposal is very efficient. Furthermore, we are able to recover truly identical multiple knots when needed in a fully automatic way. We also provide a procedure to determine the optimal number of control points automatically. To the best of our knowledge, this is the first AIS-based method providing these remarkable features in the context of data fitting with B-spline curves.The structure of this paper is as follows: firstly, previous methods for knot adjustment with B-spline curves are briefly reported in Section 2. Then, some basic concepts about data fitting with B-spline curves are given in Section 3. Then, we provide a gentle overview about the artificial immune systems and their main features in Section 4. The section also describes the fundamentals of the clonal selection theory, the theoretical basis of the method applied in this paper. The core of the paper is in Section 5, where our proposed method for automatic knot adjustment is reported in detail. Our discussion includes the issues of parameter tuning and computational complexity of the method. Some illustrative examples of its application to three carefully chosen test functions exhibiting challenging features along with some implementation details, the analysis of the CPU runtime, and a comparison of our method with other alternative approaches are reported in Section 6. The paper closes with the main conclusions and our plans for future work.Main contributions of our method are summarized below. All these claims are adequately discussed and justified throughout the paper.(1)First and foremost, we introduce an adapted elitist clonal selection algorithm to solve the knot adjustment problem in B-spline data fitting. Our method computes, in a fully automatic way, the number and location of interior knots in order to obtain an extremely accurate fitting of explicit data by using B-spline curves.An outstanding feature of our method is its generality. Opposed to previous methods, we do not assume any condition on the underlying function of data (such as continuity, differentiability, or the like). Similarly, we do not assume anything about the knot vector; internal knots in our method are truly free variables of the problem. Even the length of the knot vector is never assumed but determined according to the best BIC criterion (see Section 6.2 for further details). Consequently, our approach performs very well even for the cases of underlying functions requiring identical multiple knots, such as functions with discontinuities and cusps. To the best of our knowledge, this is the first AIS-based scheme providing these remarkable features in the context of the knot adjustment problem.In addition, our method minimizes the number of parameters required for this task. In other words, our accurate fitting of data points is not obtained at the expense of an excessive number of parameters for the model. Instead, it provides an adequate trade-off between simplicity and accuracy (see Section 5.2 for details).We also provide a detailed analysis about some relevant issues regarding our approach, such as the parameter tuning, the complexity of the method, and its CPU runtime.Finally, we carry out a comparative analysis of our method with other close metaheuristic techniques for this problem. The analysis include the discrete and continuous versions of genetic algorithms, and an AIS-based method for the discrete case. The comparative experimental results, discussed in Section 6.5, show that our approach is faster and also outperforms these previous schemes in terms of accuracy and flexibility.The issue of knot adjustment for data fitting with splines has attracted the attention of researchers and practitioners for decades. First works can be traced back in the 60s and 70s, when it was shown that the approximation with splines improves significantly if free knots are considered [3,4,37,57]. However, the use of free knots has not been as popular as one might expect from these results. As explained in Ref. [37], this fact can be attributed to the lack of analytic expressions for optimal knot locations, and the (potential) existence of many local optima of the least-squares function [57]. These are clear indicators of the difficulty of getting optimal knots for B-spline data fitting and the need and opportunity of our proposal.The general approach to this problem consists of starting with a certain number of knots and iteratively modify such amount by either knot insertion or knot removal to satisfy a prescribed error bound [13,24,37,44,45,53]. However, both methods require human intervention in order to determine (subjectively at certain extent) some needed parameters. For instance, the methods in [13,24,44,45,67] require a tolerance error or a smoothing factor, whose determination is often based on subjective factors. Similarly, the method in [37] needs a good initial guess of both knot vector length and knot locations, a task that typically demands a high level of expertise from the user. In addition, when initial parameters are not well determined, all these methods tend to be time-consuming for large amount of input data. Other methods yield unnecessary redundant knots [53], so the smooth shape of the curve is lost or degraded. Some approaches that avoid free knots to coalesce have been proposed to overcome this drawback [13,37,47]. Obviously, they cannot be applied to fit data to functions with discontinuities and cusps, where multiple knots are actually needed. The method in [49] computes the knots by using the concept of dominant points. The method requires the computation of different feature points, which are hard to be obtained. The method in [68] performs an optimal control over the knots, which is generally difficult to achieve. Other methods use curvature information extracted from input data and are, therefore, restricted to smooth data points [10,30,42,46,51,56]. And since noise in the curvature is more severe than noise in data themselves, all these methods are strongly affected by the noise intensity. To summarize, these methods can perform reasonably well under human intervention but fail to automatically generate a good knot vector.Other group of techniques formulates this problem as a constrained nonlinear optimization problem solved by numerical procedures. The authors in [55] apply a Levenberg-Marquardt iterative algorithm to solve this problem. Their method is limited by the fact that they do not compute the number of knots; instead, an initial guess about the number of knots is required. As a result, the method is very time-consuming if this initial guess is far away from the optimal value. A similar limitation happens in Ref. [60], where the authors fix the number of knots a priori and compute their location through a generalized Gauss-Newton method. Then, knot removal is applied for further improvement. Unfortunately, the Gauss-Newton method computes only local optima. In addition, the computation of the number of knots and their location are treated as independent sub-problems, even although they are strongly coupled each other. As a consequence, the method generally returns sub-optimal solutions.It has recently been shown that the application of Artificial Intelligence techniques has allowed the researchers and practitioners to achieve remarkable results regarding this data fitting problem. Most of these methods rely on some kind of neural networks [27,29] and its generalization, the functional networks [5,14,22,31–34]. Other approaches are based on the application of metaheuristic techniques, which have been successfully applied to solve difficult continuous optimization problems, such as the knot adjustment problem. However, so far there are only a few papers in the literature to address this problem with metaheuristic techniques. Roughly, they can be classified into two groups: discrete approaches [58,62,69] and continuous approaches [17,70,72]. Methods in the former group convert the original continuous problem into a discrete combinatorial optimization problem to be solved by either genetic algorithms [58,69] or artificial immune systems [62]. As expected, this conversion process introduces large discretization errors, making them both inaccurate and unreliable for real-world problems. The continuous methods avoid the discretization errors but they generally fail to obtain truly identical multiple knots, because the probability of generating the same value through random numbers is almost zero. The method in Ref. [70] uses a real-code genetic algorithm, but cannot deal accurately with features such as discontinuities and cusps. This issue has been addressed in Ref. [17] by using particle swarm optimization. A very recent stochastic optimization approach has been proposed in Ref. [72] where knots are computed through estimation of distribution algorithms (EDAs) using Gaussian mixture distributions and clustering techniques. The method performs well but it is limited to closed curves and no multiple knots are allowed.The most recent works (published during the time this paper was under review) are those in Refs. [20,63,64,71]. The method in Ref. [63] applies a Pareto Envelope-Based Selection Algorithm (PESA) method (a type of genetic algorithm specially aimed at multi-objective optimization) to compute the B-spline curve approximation of given data. Unfortunately, the method is once again based on conversion of the original problem into a combinatorial optimization problem, leading to large discretization errors, even for very simple shapes. Furthermore, the method is severely limited and can only be applied to smooth simple curves. Finally, it is also very time-consuming because the number of knots is not optimized and the method generally yields many more knots than necessary (see Section 6.5 for details). The method in Ref. [64] applies a multi-objective genetic algorithm (MOGA) to compute the knots of B-spline curves approximating a given set of explicit data points. The method is able to fit highly oscillating shapes, but it is strongly limited by two serious drawbacks: on one hand, it is restricted to smooth curves; in fact, the curves are assumed to be of class C2 (implying continuity of up to the second derivatives and therefore, curvature continuity); on the other hand, the method tends to produce more knots than necessary with no noticeable improvement of the fitting error (see Section 6.5 for further details).The methods in Refs. [20,71] also compute the location of knots for B-spline curve approximation of explicit data. The method in Ref. [20] applies a metaheuristic approach known as the firefly algorithm, while that in Ref. [71] is based on a multiresolution basis set that contains B-spline basis functions with different levels of curvature, which are subsequently selected through the Lasso method to derive an optimal subset based on the observed data. The resulting knot vector is then subjected to pruning to yield the final knot vector. Both methods work reasonably well for smooth curves, but cannot be applied to functions exhibiting discontinuities or cusps. On the other hand, the method in Ref. [20] does not compute the optimal number of knots. Therefore, its applicability requires a high level of expertise in order to determine manually a suitable value for this parameter, a very limiting factor in many industrial settings. The method in Ref. [71] allows to compute the number of knots but it is not very efficient, as it tends to obtain superfluous knots, thus increasing unnecessarily the complexity of the model.As we will show later on, the method presented in this work improves existing approaches, so all previous limitations are fully overcome. In particular, our method can yield truly identical knots, so functions with discontinuities and cusps can be accurately reconstructed. Furthermore, it also minimizes the number of knots required for optimal fitting of data.Letξ¯={ξ0=α,ξ1,ξ2,…,ξη−1,ξη=β}be a nondecreasing sequence of real numbers on the interval [α, β] on which the variable ω is valued. Without loss of generality, variable ω can be assumed to take values on the interval [0,1]. Elements ofξjj=0,…,ηare called knots andξ¯is called the knot vector. For each sequence of knots as above, we can construct the l-th B-spline basis functionϕlρ(ω,ξ¯)of order ρ (or equivalently, degree ρ−1) defined by the Cox-de Boor recurrence relations:(1)ϕl1(ω,ξ¯)=1ifξl≤ω<ξl+10otherwise(l=0,…,η−1)ϕlρ(ω,ξ¯)=ω−ξlξl+ρ−1−ξlϕlρ−1(ω,ξ¯)+ξl+ρ−ωξl+ρ−ξl+1ϕl+1ρ−1(ω,ξ¯)(ρ>1,l=0,…,η−ρ)Note that in the second equation of (1) the knots are not only in the numerator but also in the denominator. This means thatϕlρ(ω,ξ¯)is a nonlinear function ofξ¯. Note also that i-th B-spline basis function of order 1,ϕi1(ω,ξ¯), is a piecewise constant function with value 1 on the interval [ξi, ξi+1), called the support ofϕi1(ω,ξ¯), and zero elsewhere. This support can be either an interval or reduce to a point, as knots ξiand ξi+1 must not necessarily be different. If necessary, the convention 0/0=0 in Eq. (1) is applied. The number of times a knot appears in the knot vector is called the multiplicity of the knot and has an important effect on the shape and properties of the associated basis functions (see, for instance, [15,50] for details).The knot vectors can be classified into two groups, namely:•uniform knot vector: each knot appears only once andξi−ξi−1=c∈ℝ+, ∀i=1, …, η.non-uniform knot vector: distance ξi−ξi−1 can be different and/or knots can appear more than once (i.e., multiplicity can be larger than one).These two cases lead to qualitatively different behaviors. In the uniform case, each basis function is similar to the previous one but shifted to the right according to such a distance. Such regularity is lost for non-uniform knot vectors, where more complicated situations can arise. The most common one consists of repeating the end knots as many times as the order while interior knots appear only once (such a knot vector is called non-periodic knot vector). This is achieved by setting: ξ0=ξ1=…=ξρ−1=0, ξη−ρ+1=ξη−ρ+2=…=ξη=1. In such a case, the B-spline curve does interpolate the first and last control points [50]. We remark that in general, a B-spline curve does not interpolate any of its control points; interpolation only occurs when end knots are repeated as many times as the order of the curve. In addition, repeating interior knots has the effect of decreasing the continuity of the curve at such knot, from the initial Cρ−2 for a single knot to Cρ−L−1 for knots of multiplicity L. Obviously, this last case may lead to curves with cusps (for L=ρ−1) and even discontinuities (for L=ρ). This fact will be used for our discussion in Section 6. Because of their popularity in computer design and manufacturing, in this work we will consider the case of non-uniform knot vectors. Note however that our method does not preclude any other kind of knot vectors to be used instead.Let now(ωi,ζi)i=1,…χbe a set of sampled data points over [α, β]×R having the structure:(2)ζi=φ(ωi)+γi(i=1,…,χ)where φ(ω) is the underlying (unknown) function of the data and γiis the measurement error. In this setting, a convenient model function for φ(ω) is given by:(3)ψ(ω)=∑l=0η−ρμlϕlρ(ω,ξ¯)where both the vectorμ¯=(μ0,μ1,…,μη−ρ)Tof coefficients and the vectorξ¯of knots are considered as tuning parameters and (.)Tdenote the transpose of a vector or matrix. Eq. (3) is fitted to the data by Eq. (2) using the least-squares method. The approximating curve ψ(ω) in the least-squares sense is defined by minimizing the sum of squares of the residuals:(4)Θ(ξ¯)=∑i=1χ(ζi−ψ(ωi))2=∑i=1χζi−∑l=0η−ρμlϕlρ(ωi,ξ¯)2.Note that constraints can be incorporated into (4) using Lagrange's multipliers [50]. LetΦ(ξ¯)=ϕlρ(ωi,ξ¯)i=1,…,χ;l=0,…,η−ρdenote the χ×(η−ρ+1) matrix of sampled basis functions and Ξ=(ζ1, …, ζχ)T. If the knots are fixed, Eq. (4) reduces to the straightforward linear least-squares problem:(5)Θ(ξ¯)=minμ¯∈Rη−ρ+1||Ξ−Φ(ξ¯).μ¯||2=(Φ(ξ¯)T.Φ(ξ¯))+.Φ(ξ¯)T.Ξwhere ||.|| is the Euclidean norm, and(Φ(ξ¯)T.Φ(ξ¯))+denotes the Moore-Penrose inverse ofΦ(ξ¯)T.Φ(ξ¯), which is defined by choosingμ¯to minimize||μ¯||2among the solutions ofΦ(ξ¯)T.Φ(ξ¯).μ¯=Φ(ξ¯)T.Ξ.For fixed ρ, if the knots are treated as free variables, we have to solve a nonlinear (because ψ(ω) is a nonlinear function of the knots) least-squares problem given by:(6)Θ*(ξ¯)=minμ¯∈Rη−ρ+1,ξ¯∈[α,β]η+1||Ξ−Φ(ξ¯).μ¯||2.For eachξ¯fixed, Eq. (6) reduces to (5). In fact, it has been shown that the solution of (6) is:(7)minξ¯∈[α,β]η+1||Ξ−Φ(ξ¯).μˆ(ξ¯)||2whereμˆ(ξ¯)is the solution of the linear problem (5). Therefore, we denote the objective function henceforth as:(8)Θˆ(ξ¯)=minξ¯∈[α,β]η+1||Ξ−Φ(ξ¯).μˆ(ξ¯)||2.By Artificial Immune Systems (AIS) we refer to a powerful computational methodology based upon metaphors of the biological immune system of humans and other mammals. Rather than a single technique, AIS comprises a collection of several computational intelligent approaches under the common feature of being based on principles and processes that typically happen at the level of the immune system. Since the appearance of the first approaches in the field, Artificial Immune Systems are gaining increased attention from scientific community because of their ability to solve complex optimization problems in several fields. The reader is kindly referred to the books in [6,11,12] for a gentle introduction to the field. We also recommend the excellent review papers in Refs. [61,28] for a detailed description of the theoretical advances on AIS and the main past, present and future areas of application of AIS, respectively.To date, no individual AIS tried to implement all features of a real immune system. Instead, there are several models in AIS, each focused on the implementation of one or, at most, a few of those features. Relevant examples of AIS models include negative selection [16], artificial immune network [36], dendritic cells [25] and clonal selection [8]. Because of its appealing features regarding the optimization of multimodal functions [9] in this paper we will focus on the clonal selection theory. This is a widely accepted theory used to explain the basic features of an adaptive immune response to an antigenic stimulus. When pathogenic microorganisms, called antigens (and represented by Ag onwards) invade a body, the immune system of the host reacts by activating the production of antigen receptors called antibodies (represented by Ab in this paper). These antibodies are relatively specific to the antigens; the degree of specificity is determined by the affinity of the couple Ag–Ab. The clonal selection theory states that only the Ab's with the highest affinity are selected to proliferate. During this process, the average antibody affinity increases for the given antigen, a process called affinity maturation and caused by a somatic hypermutation (an extremely high rate of somatic mutation altering the specificity of antibodies through small random changes) and selection mechanism occurring during the clonal expansion. Also, the antibodies of low affinity will eventually disappear through a process called apoptosis (or programmed cell death). The interested reader is kindly referred to [6,7] for a gentle introduction to the clonal selection theory from the point of view of computer science.In this section we apply the clonal selection theory to the knot adjustment problem described in Section 3. We begin with a brief discussion about the clonal selection algorithm. Then, we consider an elitist variant and adapt it in order to determine the optimal choice of knots. Finally, we discuss the parameter tuning and computational complexity of our elitist clonal selection algorithm.The clonal selection algorithm (CSA) is an AIS scheme based on the clonal selection theory discussed in previous section. The algorithm, initially introduced by De Castro and Von Zuben in Ref. [8] and formally described in Ref. [9], is based on two basic principles: (1) only the cells recognizing the antigen are selected for growing (mutation and cloning); (2) the affinity of the selected cell to the antigen is increased by affinity maturation process. This algorithm has shown to be very well suited for optimization problems, having been successfully applied to problems such as character recognition and multimodal optimization of functions with very good performance. The reader is referred to [8,9] for a deep analysis of the clonal selection algorithm from a computational point of view and a description of its potential applications. See also [21] for a recent example of its application to the data parameterization for data fitting with Bézier surfaces.Main immune features taken into account in the algorithm are [9]:1maintenance of a specific memory set;selection and cloning of the most stimulated Ab's;death of nonstimulated Ab's;affinity maturation;re-selection of the clones proportionally to their antigenic affinity; andgeneration and maintenance of diversity.Typically, the given problem to be solved is represented through a Ag–Ab codification (binary or real-valued) and a distance measure (called the affinity measure), used to calculate the degree of interaction between these molecules. Affinity between an antibody and an antigen, represented by Af(Ab,Ag), can be estimated by distance measure between two arrays (or vectors) by using different methods. If the vectors representing antigens and antibodies are real-valued vectors, then Manhattan or Euclidian distance measures can be used; if they are represented by binary symbols, then the Hamming distance is usually applied (see [6] for more details).Fig. 1shows the flowchart of the clonal selection algorithm as originally proposed for pattern recognition purposes [9]. The algorithm considers two repertoires (populations): a set of antigens Ag{M} and a set of antibodies Ab{N}. Similar to [9], cardinality is indicated by the subindexes within brackets for clarity. The set of antibodies Ab{N} is further divided into two subsets: memory Ab repertoire, Ab{m}, and remaining Ab repertoire, Ab{r}, such that m+r=N. The algorithm also keeps track of two other sets: the set Ab{n} of the n Ab's with the highest affinities to a given Ag, and the set Ab{d} of the d new Ab's that will replace the low-affinity Ab's from Ab{r}. The algorithm can be summarized as follows:1Random choice of an antigen Agj. It is presented to all antibodies of Ab{N}.Compute the vector affinity f=(f1, f2, …, fN) where fi=Af(Abi, Agj).Select the n highest affinity components of f to generate Ab{n}.Elements of Ab{n} will be cloned adaptively. The number of clones is proportional to the affinity: the higher the affinity, the higher the number of clones. Such amount is given by***:Nc=∑h=1nround(Λ.N/h)where Nc represents the number of clones, Λ is a positive number that plays the role of a multiplying factor, N is the total number of Ab's and round(.) is the operator that rounds its argument toward the closest integer.The clones in the set resulting from the previous step are subjected to somatic hypermutation. The affinity maturation rate is inversely proportional to the antigenic affinity: the higher the affinity, the smaller the maturation rate.Compute the vector affinity of Agjwith respect to the new matured clones.From this set of matured clones, select the one with the highest affinity to be candidate to enter into the set Ab{m}. If Af(Abk, Agj)>Af(Abl, Agj) for a given Abl∈Ab{n}, then Abkwill replace Abl.Replace the d Ab's with lowest affinity in Ab{r} by new, randomly generated individuals in Ab{d}, inserted into Ab{r} in order to preserve the diversity of population.Each execution of the previous steps for Agj, j=1, …, M, is called a generation. The algorithm is repeated for a certain number of generations, Ngen, a parameter that depends on the specific problem under analysis.The algorithm described above is only suitable for supervised problems, for which an explicit Ag{M} population is available for recognition. To overcome this limitation, authors in [9] proposed a modified version of CSA for multimodal optimization problems. Main changes in their modified version are:•There is no need to maintain a separate subset of memory Ab{m}, since no specific Agjhas to be recognized. Instead, the whole population of antibodies will compose the memory set.The affinity measure function corresponds to the evaluation of the least-squares function, so that each Ab represents a potential solution of the problem.Several Ab's with high affinity are selected in step 7 of the algorithm, rather than just the best one.All Ab's in the population can be selected for cloning in step 3, so no need to maintain set Ab{n}.In that case, the affinity proportionate cloning is no longer necessary. All antibodies can be cloned at the same rate (i.e., the number of clones generated for each antibody will be the same).In the knot adjustment problem we seek for the (unknown) knot vector to fit the data points better in the least-squares sense. This is clearly an unsupervised problem, so we consider the modified version of CSA for multimodal optimization. However, in order to apply that algorithm to our problem, some additional issues must be addressed, as described in next paragraphs.Representation scheme for antibodies: Firstly***, we need an adequate representation scheme for antibodies. In our case, they are encoded as real-valued vectors of length κ=η−2ρ+1, representing the internal knotsξ¯⊙={ξρ,ξρ+1,…,ξη−ρ}, which must not be necessarily all different. In fact, the case of multiple identical knots is fully supported by our method without the need of further pre-/post-processing. All antibodies are initialized with uniformly distributed random values on the interval [0, 1]κ. Components of each antibody are then sorted in increasing order to reflect the non-decreasing structure of knot vectors.Somatic hypermutation: Although*** our problem is intrinsically multimodal, our goal is to get the optimal choice for the knot vector rather than a collection of multiple local optima. Consequently, we still keep the set Ab{n} so that only the best antibodies will be cloned. The number of clones varies with the affinity, according to the following rule:(9)Nc=∑j=1nround[(υj+1).Λ.N],where υ1=5, υ2=3, υ3=1, and υj=0, for j≥4 and Λ is a multiplying factor. Thus, the antibodies with higher affinity will have more clones, which at turn will then undergo mutation as explained in next paragraph.Mutation mechanism: In ***this work we apply a single-point, inductive uniform mutation operator. It introduces random perturbations on one randomly chosen component ξiof the antibody, i∈{1, …, η−2ρ+1}. This component is mutated according to the rule: ξi→ξi−Δ/2(σ−1/2), where Δ=min{ξi/2, 1−ξi} and σ is a random value from the distribution U(0, 1). In other words, the attribute ξiis perturbed by an additive uniform random number bounded in the neighborhood of ξi, while all other attributes{ξj}j=1,…,η−2ρ+1,j≠iremain unaltered. Finally, the knots are rearranged so that the antibody still maintains the knot vector ordered pattern.Elitism: Our algorithm is enriched by applying elitism, so that we allow some of the better Ab's from the current generation to carry over to the next, unaltered. The number of Ab's transferred to the next generation without further transformation is determined by a parameter Ne. This feature provides faster convergence rates with respect to the non-elitist variant and improves the memory capacity of the approach.Affinity measure: As a key ingredient of the method, we need a mechanism to compute the affinity. In our case, the goal is to minimize the least-squares error function given by Eq. (4). But since this error function does not take into account the number of data points, we also compute the RMSE (root mean square error) function, given by:(10)RMSE=∑i=1χ(ζi−∑l=0η−ρμlϕlρ(ωi,ξ¯))2χThis function is still simplistic however, as it only computes the error without further consideration about the complexity of the model. This means that the best error might be obtained at the expense of a large number of variables, possibly leading to overfitting. A classical solution to this problem is to perform cross-validation on the data set, but we found this process to be very time-consuming, since multiple rounds and different partitions of data could be required in order to reduce variability. To overcome this limitation, we compute two additional affinity functions: AIC and BIC. Both are penalized information-theoretical criteria increasingly used in recent years to address model selection problems. The Akaike Information Criterion (AIC) was developed to select the model that minimizes the negative likelihood penalized by the number of parameters of the model [1,2]. In our case, it is given by:(11)AIC=χLn(Θ(ξ¯))+2(2η−3ρ+2)where χ is the size of sampled points and Ln(.) means the natural logarithm (with base e). AIC is specifically aimed at finding the best approximating model to the true data. To this purpose, it is comprised of two terms: the first one accounts for the fidelity of the model function while the second one is a penalty term introduced for the sake of simplicity of such a model function (in fact, the second term reflects the number of free parameters of the model). Their interplay allows us to reach an adequate trade-off between fidelity and simplicity. The Bayesian Information Criterion (BIC) was developed as an estimate of the Bayes factor for two competing models [59] and is defined as:(12)BIC=χLn(Θ(ξ¯))+(Ln(χ))(2η−3ρ+2)A simple visual comparison between Eqs. (11) and (12) show that both criteria are quite similar. Superficially, BIC differs from AIC only in the second term which now depends on sample size, χ. In general, BIC applies a larger penalty than AIC, thus other factors being equal it tends to select simpler models than AIC (i.e., models with fewer parameters). When applied to our specific problem, both methods provide very similar results for data with a smooth underlying function. Differences arise, however, for functions exhibiting discontinuities and/or cusps. In particular, AIC tends to yield unnecessary redundant knots and, therefore, BIC becomes more adequate for such non-smooth models.A great advantage of using AIC and BIC is that they avoid the use of subjective parameters such as error bounds or smoothing factors. In fact, they play the role of smoothing factors but in an automatic (and hence, human-independent) way. Furthermore, they provide a very simple and straightforward procedure to determine the best model: the smaller their value, the better fitness. Owing to these reasons, they will be used in Section 6 to select the best model for our examples and for comparative purposes of our results and those of other alternative approaches.In order to apply our adapted elitist CSA method to the problem, we need to specify two kinds of parameters, related to the approximating B-spline curve and the own method, respectively. For the former, the only input we need is: (1) the order of the approximating B-spline curve, ρ, and (2) the number of internal knots, κ, which are freely chosen by the user with the only constraint that η≥2ρ−1. Choice of ρ is important because lower-order polynomials give little flexibility in controlling the shape of the curve while higher-order polynomials can introduce unwanted wiggles and require more computation. A classical choice is to consider fourth-order B-spline curves and so do we in this paper. Then, we show our results as a function of κ. Note, however, that our method does not depend on the value of ρ.A more critical issue is the determination of suitable values for the parameters of our adapted CSA method. Although some guidelines are given in the literature to tackle this issue, such a selection is problem-dependent and, therefore, it remains empirical to a large extent. In this paper we also follow this empirical approach. To this purpose, we have analyzed experimentally the effects of the variation of parameter values both on the performance of the method and on the quality of the solution. From them, we have derived the set of parameter values reported in Table 1. The table shows four different items (in columns): the symbol of the parameter, its meaning, its selected value for this problem, and the range of parameter values we tested. In general, the selected values are optimized in the sense of providing a very accurate solution to our problem while minimizing the computational cost to reach it. The parameters analyzed in our study are:•population size, N: many theoretical results have shown that a certain population size is required in order to promote exploration of the search space in evolutionary algorithms. In the case of AIS, some previous works have suggested a minimum value of 100 antibodies for better efficiency. On the other hand, larger values of the number of antibodies increase the exploration capacity but also the number of function evaluations (and hence, the runtime). In this paper, we tested our method for population sizes ranging from 100 to 5000 antibodies with step-size 100, and obtained similar results in all cases. Since larger populations imply longer computation times without any error improvement, a population of 100 antibodies is recommended in this work.parameter d: this number accounts for the amount of low-affinity antibodies to be replaced during the CSA workflow, with the aim of increasing the exploration ability of the method. Its choice is troublesome, since too low (high) values degrade the exploration (exploitation) capabilities of the method, respectively. In this paper, we tested different values for this parameter ranging from 10 to 500 with step-size 10. Our computer simulations show that the fitting error improves from 10 to 20, but no significant improvement is reached for higher values. Therefore, we set this value to d=20.parameter n: it represents the number of antibodies to be cloned. These clones will be subsequently subjected to somatic hypermutation according to the antigenic affinity, as explained in Section 5.1. This number is usually a percentage of the population size, typically ranging from 10% to 30%, so we tested values for this parameter from 10 to 30 with step-size 10. Since in our computer simulations we obtained similar fitting errors for all cases but larger values of n imply longer runtimes and more storage and capacity requirements, we set this value to n=10.Λ: it is a multiplying factor appearing in Eq. (9) in combination with the population size. Therefore, its value can be fixed during the computation process of the algorithm. In this paper, its value is set to Λ=0.1.parameter Nc: it represents the number of clones generated at each new generation. According to Eq. (9), it is determined by the values of previous parameters N, n, and Λ. By doing so, the value of this parameter ranges from 190 to 1950 clones. We noticed, however, that the results of our method do not change for increasing values of Nc over this entire range, but complexity increases. Therefore, it is advisable to take the lowest value Nc=190.parameter Ne: it represents the number of antibodies selected for elitism. Consequently, it determines how many good antibodies are selected for next generation unaltered, thus increasing the memory of the process. In this paper, we tested different values for this parameter ranging from 10 to 200 with step-size 10, but we did not notice any significant difference in our results, so we opted to keep this parameter to the minimum value Ne=10 in order to save computational time.number of iterations,Niter: this is a critical parameter, because we need to ensure we reach convergence without wasting unnecessary resources (i.e., iterations) in the process. Once again, the optimal value was determined empirically; the parameter was varied in the range 100–2000 iterations with step-size 100. Our simulation results show that, for our choice of the other parameters, 100 iterations are enough to reach convergence. This is the value used in this paper.After the selection of those parameters, our algorithm is executed for the selected number of iterations. The antibody with the best (i.e., minimum) affinity value is selected as the best solution to the problem.Regarding the complexity of the method, it is very similar to that of the original CSA method, discussed in [9]. The main computational load is due to the affinity calculation, that requires 4κ+2 basis function evaluations for each antibody, therefore O(N.κ) overall, selection and re-selection of the highest-affinity antibodies (requiring O(N) and O(Nc) in the worst cases, respectively) and hypermutation, requiring O(Nc.κ) in our case. Therefore, the total computational time is of order O((N+Nc).κ).Our method has been applied to several test functions. To keep the paper at manageable size, we restrict our discussion to three examples exhibiting very challenging features, such as a continuous step-like function, a function discontinuous at a point, and a function with a cusp, respectively. These examples have been carefully chosen to reflect the diversity of situations to which our algorithm can be applied. Indeed, at the first level of abstraction, any given function can be (from the most to the least restrictive case): differentiable, continuous but non-differentiable, or non-continuous. In addition, these test functions have already been used in previous works in the field [69,70]. Because of these reasons, they represent a very good benchmark to evaluate the performance of our approach as well as for comparative purposes.The first example is a smooth continuous function with a region of a very steep upward slope in the neighborhood of ω=0.4, where it increases sharply as if following a step function:(13)φ1(ω)=901+e−100(ω−0.4)Second example consists of a function with a strong discontinuity at ω=0.6, with a quite different behavior at both sides of that point:(14)φ2(ω)=10.01+(ω−0.3)2ω<0.610.015+(ω−0.65)2ω≥0.6Last function has a cusp at ω=0.5 and an almost symmetric shape at both ends of such a point:(15)φ3(ω)=100e|10ω−5|+(10ω−5)5500Each function is evaluated at uniformly distributed values of ω on the interval [0, 1] to generate a collection of 201 data points. To check the robustness against the noise of our approach, these data points have been perturbed by an additive random noise ϵ that follows the normal distribution N(0, 1) of mean 0 and variance 1.

@&#CONCLUSIONS@&#
This paper presents an adapted elitist clonal selection algorithm to solve the knot adjustment problem for B-spline curves. Given a set of noisy data points, our method determines the number and location of knots automatically and accurately in order to obtain an optimal fitting of data. Our approach performs very well even for the cases of underlying functions requiring identical multiple knots, such as functions with discontinuities and cusps, in a fully automatic way. By using the AIC and BIC error functions, we also provide a simple yet efficient procedure to determine the optimal number of internal knots. In other words, our method minimizes the number of parameters required for optimal fitting of data, thus alleviating user's requirements in terms of memory storage and computational time. To evaluate the performance of our approach, it has been applied to three challenging test functions, and results have been compared with those from other alternative methods based on AIS and genetic algorithms. Our experimental results show that our proposal outperforms previous approaches in terms of accuracy and flexibility. Some other issues such as the parameter tuning, the complexity of the algorithm, and its CPU runtime are also discussed in this paper.Potential applications of our method to real-world problems include the design of turbine blades [26,38,54] and aircraft wings [41,43]. In the former field, the design parameters used for the blades as well as the hub and shroud surfaces construction correspond to 2D sections, usually described in terms of B-spline curves [38]. Similarly, in [26] the blade surface is represented by a smoothly varying family of B-spline curves. An example of the application of non-uniform rational B-spline curves for numerically-controlled machining of turbine blades can be found in Ref. [54]. Authors in Ref. [41] show that non-uniform rational B-spline curves can be effectively used to optimize the aerodynamic design process of aircraft wings. To this goal, a given airfoil section is approximated by B-spline curves to provide an initial guess of the aerodynamic design optimization. Also in [43] control points and weights of rational B-splines are used as design parameters.Other interesting industrial applications include animation approximation [48] (where the aim is to obtain a compact representation of animation sequences through B-splines), medical imaging [35] (to represent the displacement vector field for deformable image registration) and tool-path generation for computer-numerically-controlled (CNC) machining, where B-spline curves are widely used for tool-path interpolation and approximation of CNC tools for pocket machining and other purposes [23,39,40,54].Main limitations of our method are the computation times and the parameter tuning. As shown in Section 6.4 a typical execution takes a few seconds (even tens of seconds, depending on the complexity of the problem) to be obtained. Even though this makes our method faster than other alternative approaches, it still prevents it to be applied to real-time settings. Furthermore, it is very difficult to determine a priori the computation time for any given example. On the other hand, as usual when dealing with metaheuristic techniques, parameter tuning is a problem-dependent issue, meaning that different problems might require a totally different set of values for the parameters of the method in order to perform properly.Future work includes the extension of this method to rational B-spline curves, where the existence of additional parameters (weights) can modify our procedure of selection of optimal parameters. The application of our approach to interesting real-world problems such as those described above is also part of our future work.