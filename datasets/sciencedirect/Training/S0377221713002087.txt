@&#MAIN-TITLE@&#
Nonsmooth nonconvex optimization approach to clusterwise linear regression problems

@&#HIGHLIGHTS@&#
We develop an incremental algorithm to solve clusterwise linear regression problems.The algorithm gradually computes clusters and linear regression functions.Two special procedures to construct initial solutions are proposed.The algorithm finds global or near global minimizers of the overall fit function.

@&#KEYPHRASES@&#
Clusterwise linear regression,Incremental algorithm,Späth algorithm,

@&#ABSTRACT@&#
Clusterwise regression consists of finding a number of regression functions each approximating a subset of the data. In this paper, a new approach for solving the clusterwise linear regression problems is proposed based on a nonsmooth nonconvex formulation. We present an algorithm for minimizing this nonsmooth nonconvex function. This algorithm incrementally divides the whole data set into groups which can be easily approximated by one linear regression function. A special procedure is introduced to generate a good starting point for solving global optimization problems at each iteration of the incremental algorithm. Such an approach allows one to find global or near global solution to the problem when the data sets are sufficiently dense. The algorithm is compared with the multistart Späth algorithm on several publicly available data sets for regression analysis.

@&#INTRODUCTION@&#
Unsupervised classification, or clustering, is an important task in data mining, which consists in finding subsets of similar points in a data set, in order to find patterns in the data. Regression analysis consists in fitting a function (often linear) to the data to discover how one or more variables vary as a function of another.The aim of clusterwise regression is to combine both of these techniques, to discover trends within data, when more than one trend is likely to exist. Clusterwise regression has applications for instance in market segmentation, where it allows one to gather information on customer behaviors for several unknown groups of customers [1,7]. It is also applied to investigate the stock-exchange data [20] and the so-called benefit segmentation [28]. The presence of nonlinear relationships, heterogeneous subjects, or time series in these applications necessitate the use of two or more regression functions to best summarize the underlying structure of the data.The simplest case in the clusterwise regression is the use of two or more linear regression functions to investigate the structure of the data. Such an approach is called clusterwise linear regression and it is widely used and studied better than other approaches. This problem can be formulated as an optimization problem. Mixed integer nonlinear programming formulations can be found in [9,12]. Such problems may have a very large number of variables even for moderately large data sets. Therefore exact global optimization of such problems is very challenging and out of reach of existing algorithms [9]. The most popular approaches to clusterwise linear regression are generalizations of classical clustering algorithms such as k-means [23,24] or EM [14]. In [8] the authors base their approach on the variable neighborhood search.In the paper [20] the clusterwise linear regression is studied when the set of predictor variables forms an L2-continuous stochastic process. For each cluster the estimators of the regression coefficients are given by partial least square regression. The number of clusters is treated as unknown. The paper [15] extends the so-called TCLUST methodology to perform robust clusterwise linear regression. In this paper, a feasible algorithm for the practical implementation is also proposed.The paper [12] presents a conditional mixture, maximum likelihood methodology for performing clusterwise linear regression. This methodology simultaneously estimates separate regression functions and membership in K>0 clusters or groups. The conditional mixture, maximum likelihood methodology is introduced together with the EM algorithm utilized for parameter estimation.Existing clusterwise linear regression algorithms suffer from the same drawbacks as their clustering counterparts: they are very sensitive to the choice of an initial solution and they may lead to sub-optimal solutions [31]. Furthermore, most of these algorithms assume the number of clusters to be known a priori. Most of algorithms try to separate data into subsets of observations and use one regression function for each subset.There have been several attempts to simultaneously find all regression functions to approximate a data set and to estimate the number of subsets. The paper [13] presents a methodology which simultaneously clusters observations into a preset number of groups and estimates the corresponding regression functions’ coefficients, all to optimize a common objective function. Then a simulated annealing-based methodology is described to accommodate overlapping or nonoverlapping clustering. In the paper [16], the authors show that the estimation of the clusterwise regression model is equivalent to solving a nonlinear mixed integer programming model.An information-based criterion for determining the number of clusters in the clusterwise regression problem is proposed in [22]. It is shown that, under a probabilistically structured population, the proposed criterion selects the true number of regression hyperplanes with probability one among all class-growing sequences of classifications, when the number of observations from the population increases to infinity. The paper [21] studies the problem of estimating the number of clusters in the context of logistic regression clustering. The classification likelihood approach is employed to tackle this problem. A model-selection based criterion for selecting the number of logistic curves is proposed and its asymptotic property is also considered.In this paper, a new approach for solving the clusterwise linear regression problems is proposed based on a nonsmooth nonconvex formulation. This approach starts with one regression function and summarizes the underlying structure of the data by dynamically adding one hyperplane at each iteration. A special procedure is introduced to generate good starting points for solving global optimization problems at each iteration of the incremental algorithm. Such an approach allows one to find global or near global solution to the problem when a data set is sufficiently dense.Several incremental algorithms have been proposed to solve the sum of squares clustering problems. The global k-means algorithm and its variations [2,17] are based on constructing the clusters incrementally, starting from finding the center for the whole data set and then adding a cluster at a time and refining the new set of clusters by applying k-means. In this paper we propose to apply a similar scheme in order to solve the clusterwise linear regression problem. Instead of classical centers, we propose to use affine functions as representatives of clusters.The proposed algorithm is numerically tested on twenty small and seven medium size and large publicly available data sets for regression analysis. We also compare it with the multi-start Späth algorithm for the clusterwise linear regression. Additionally, we study the efficiency of the proposed algorithm depending on the number of points, features and clusters using randomly generated data sets.The structure of the paper is as follows. In Section 2 the clusterwise linear regression problem is introduced. We briefly explain the Späth algorithm for solving the clusterwise linear regression problem in Section 3 and describe the incremental algorithm in Section 4. Computation of initial solutions is discussed in Section 5. Section 6 contains computational results and Section 7 concludes the paper.In this section we will present the nonsmooth nonconvex optimization formulation of the clusterwise linear regression problem. Given a data setA={(ai,bi)∈Rn×R:i=1,…,m}, the aim of the clusterwise linear regression is to find simultaneously an optimal partition of data in k clusters and regression coefficients {xj,yj}, j=1,…,k within clusters in order to minimize the overall fit. Let Aj, j=1,…,k be clusters such thatAj≠∅,Aj⋂At=∅,j,t=1,…,k,t≠jandA=⋃j=1kAj.Let {xj,yj} be linear regression coefficients computed using only data points from the cluster Aj, j=1,…,k. Then for the given data point (ai,bi) and coefficients {xj,yj} the regression error h(xj,yj,ai,bi) is:h(xj,yj,ai,bi)=〈xj,ai〉+yj-bip. Here p>0. We associate a data point with the cluster whose regression error at this point is smallest. Then the overall fit function is:(1)fk(x,y)=∑i=1mminj=1,…,kh(xj,yj,ai,bi),wherex=(x1,…,xk)∈Rk×nandy∈Rk. Thus the k-clusterwise linear regression problem is formulated as follows:(2)minimizefk(x,y)subjecttox∈Rk×n,y∈Rk.In general, the objective function fkin this problem is nonsmooth nonconvex. One can consider any positive values of p to define regression errors. If p=1 then the function fkis piecewise linear and if p=2 then it is piecewise quadratic. Moreover, if k=1 then in both cases the objective function is convex and if k>1 it becomes nonconvex. One can consider different values for p, however, in this paper we consider the case when p=2.It should be noted that the number of clusters k is not always known a priori. Therefore the number k should be specified before solving Problem (2).In this section we recall the algorithm from [24] for solving Problem (2) which is based on the well known k-means algorithm. In this paper the algorithm was described for p=2, however, in our description below we will present it for p⩾1 in general.Algorithm 1Späth algorithmStep 1: (Initialization) Select mutually disjoint clusters A1,⋯,Aksuch that⋃j=1kAj=A.Step 2: For j=1,⋯,k, solve the following linear regression problem:(3)minimizeφ(xj,yj)=∑(a,b)∈Ajh(xj,yj,a,b)subjecttoxj∈Rn,yj∈R.and obtain regression coefficients (xj,yj), j=1,…,k.Step 3: For j=1,⋯,k, recompute the cluster Ajsuch that a point (a,b)∈A belongs to Ajifh(xj,yj,a,b)=minl=1,…,kh(xl,yl,a,b)and go to Step 2 until no more data points change their clusters.The main and most time consuming step in Algorithm 1 is Step 2 where one solves the linear regression problems to find regression coefficients. This is a convex optimization problem when p⩾1. It is a quadratic programming problem when p=2. More specifically, it is a classical least squares regression problem. If p=1 or p=∞ then the objective function φ is convex piecewise linear and nonsmooth. Methods of nonsmooth optimization can be applied to minimize it (for example, finite convergent algorithm from [6]). For p=1 or p=∞, Problem (3) also can be reformulated as a linear programming problem and linear programming techniques can be applied to solve it.Algorithm 1 converges to local minimizers of Problem (2) whereas only global or near global solutions provide meaningful clusters. As the number of clusters k and the number of data points m increase, the number of local solutions to this problem increases drastically. The quality of the solution obtained by the algorithm depends on the initial set of clusters. For moderate number of clusters in small data sets a multi-restarting strategy can be applied, however, the success of this strategy deteriorates as the number of clusters or the size of the data set increase.In this paper we propose to use an incremental approach to solve Problem (2). This approach iteratively adds one linear function at a time and uses the current clusters to construct a good starting cluster distribution for the next iteration. Incremental algorithms are increasingly popular in data mining and in particular to solve clustering problems [2,3,5,17]. For example, the modified global k-means algorithm has been shown to be very efficient for solving clustering problems (see, [2,4]).The global optimization Problem (2) may have a large number of solutions among which only global or near global ones are of interest. However, conventional global optimization techniques cannot be directly applied to solve this problem due to its size, while efficient local methods can only reach local solutions whose quality depends on starting points. Therefore it is crucial to develop a procedure for finding those good starting points. In this section we propose to incorporate local methods within an incremental approach for solving Problem (2) based on the idea that the solution to the (k−1)-clusterwise linear regression problem provides a good basis to construct a starting point for solving the k-clusterwise linear regression one.At the first iteration of the incremental algorithm we consider only one cluster in which case Problem (2) reduces to finding one linear regression function for the whole data set. This problem is quadratic convex and its global minimizers are easy to find. Next we describe the kth iteration of the incremental algorithm where k>1. At this iteration the solution (x1,y1,⋯,xk−1,yk−1) to the (k−1)-clusterwise linear regression problem (2) and the corresponding valuefk-1∗=fk-1(x1,y1,⋯,xk-1,yk-1)of the overall fit function (1) are known. Denote the regression error of the data point (a,b) at the (k−1)th iteration byrk-1ab=minj=1…k-1h(xj,yj,a,b).Then we introduce the kth auxiliary clusterwise linear regression function as follows:(4)f¯k(u,v)=∑(a,b)∈Aminrk-1ab,h(u,v,a,b),u∈Rn,v∈R.Clearly,f¯k(u,v)=fk(x1,y1,⋯,xk-1,yk-1,u,v). Furthermore(5)max(u,v)∈Rn+1f¯k(u,v)=∑(a,b)∈Ark-1ab=minx∈R(k-1)n,y∈Rk-1fk-1(x,y).This justifies the use of the auxiliary functions to find a linear regression function which improves clustering results. Specifically, in order to solve Problem (2) we propose a special procedure to construct a starting point(u1,v1,⋯,uk-1,vk-1,uk,vk),uj∈Rn,vj∈R,j=1,…,k. Here we set uj=xj, vj=yj, j=1,…,k−1 and additional linear regression coefficients {uk,vk} are found by solving the following problem:(6)minimizef¯k(u,v)subjecttou∈Rn,v∈R.Problem (6) is called the kth auxiliary clusterwise regression problem. This problem has n+1 variables and unlike Problem (2) the number of variables does not depend on the number of linear regression functions.We now can describe the main algorithm for solving the clusterwise linear regression problem (2). Assume the number of linear regression functions k is known.Algorithm 2Incremental algorithm for solving Problem (2).Step 1: (Initialization) Compute the linear regression function(x1,y1)∈Rn×Rof the whole set A. Set l=1.Step 2: (Computation of the next linear regression function). Set l=l+1. Let (x1,y1,⋯,xl−1,yl−1) be the solution to the (l−1)-clusterwise regression problem. Find a solution(u¯,v¯)∈Rn×Rto the lth auxiliary clusterwise linear regression problem (6).Step 3: (Refinement of all linear regression functions) Select(x1,y1,⋯,xl-1,yl-1,u¯,v¯)as a new initial solution, compute their respective clusters and apply Algorithm 1 to solve the l-clusterwise linear regression problem.Step 4: (Stopping criterion) If l=k, then stop. Otherwise go to Step 2.Steps 2 and 3 are the most important steps in Algorithm 2. In Step 2 one solves Problem (6) to find initial solutions for the lth linear regression function. This problem is nonconvex and therefore it may have a large number of local solutions. Again success of local methods for solving this problem depends on initial solutions. It is crucial to develop a special algorithm to generate good initial solutions if one applies local methods to solve Problem (6). In the next section we design one such algorithm. In Step 3 we apply the Späth algorithm starting from the solution found in Step 2 to solve Problem (2) for k=l.In this section we design an algorithm for solving Problem (6).We denote a hyperplane by a pair (u,v) whereu∈Rnandv∈R. Consider the following set of hyperplanes:Ck=(u,v)∈Rn+1:h(u,v,a,b)>rk-1ab∀(a,b)∈A.The set Ckcontains all hyperplanes which do not attract any point from the set A. It is clear that over this set the functionf¯kis constant and reaches its global maximum value (5). Therefore any hyperplane from the set Ckis a stationary point for the functionf¯k. This means that if we choose a starting point in this set then most local methods will be unable to escape it and will not decrease the value of both the auxiliary and overall fit functions. Therefore it is crucial to select starting points in the complementary setC¯kof the closure of Ck:C¯k=(u,v)∈Rn+1:∃(a,b)∈Asuchthath(u,v,a,b)<rk-1ab.Clearly,C¯kis the set of hyperplanes which attract at least one data point from the set A. Any hyperplane from this set will decrease the value of the auxiliary clusterwise linear regression function. Our aim is to find hyperplanes which provide significant decrease of the value of this function. Next we describe how such hyperplanes can be found.Since Problem (6) is a global optimization problem the success of a local method depends on initial solutions. Therefore it is important to develop a scheme generating “most promising” initial solutions. There is a trade-off between computational effort and the quality of a solution obtained by a local method and this trade-off can be balanced by the number of initial solutions used. Our approach consists in generating a large number of candidate initial solutions and only keep those providing a sufficient decrease of the overall fit function. We propose a general scheme to generate initial solutions and this scheme contains some parameters whose values determine the number of initial solutions.Assume that the solution (x1,y1,⋯,xk−1,yk−1) to the (k−1)th clusterwise linear regression problem is known and let (a,b)∈A be any data point which does not lie on any hyperplanes (x1,y1),⋯,(xk−1,yk−1). Assume that this point belongs to the cluster determined by the linear regression coefficients {xj,yj} where j∈{1,…,k−1}. We define another hyperplane (xab,yab) parallel to the hyperplane (xj,yj) passing through the point (a,b). In this case xab=xjand yab=b−〈xj,a〉. It is clear that(xab,yab)∈C¯k. The valuef̃k-1of the fit function fk−1 over the set A with hyperplanes (x1,y1,⋯,xk−1,yk−1) is:f̃k-1=∑(a,b)∈Ark-1aband the valuef̃kof the fit function fkover A with hyperplanes (x1,y1,⋯,xk−1,yk−1) and (xab,yab) is:f̃k=f¯k(xab,yab)=∑(c,d)∈Amin{rk-1cd,h(xab,yab,c,d)}.The difference between these two values is:d(xab,yab)=f̃k-1-f̃k=∑(c,d)∈Amax0,rk-1cd-h(xab,yab,c,d).d(xab,yab)>0 for any data point (a,b)∈A. Let γ1∈[0,1] be a given number. Define(7)d¯1=max{d(xab,yab):(a,b)∈A}and the following set(8)A¯1={(a,b)∈A:d(xab,yab)⩾γ1d¯1}.This set contains all the solutions providing decrease above a thresholdγ1d¯1. For γ1=0 the setA¯1=Aand for γ1=1 the setA¯1contains only data points providing largest decreased¯1of the lth clusterwise linear regression function.For each(a,b)∈A¯1we compute the set Babas follows:(9)Bab=(c,d)∈A:h(xab,yab,c,d)<rk-1cd.The set Babcontains all points from the set A attracted by the clusterwise linear regression function (xab,yab). We compute(x¯ab,y¯ab)as a linear regression function approximating the set Bab. This additional step to update the clusterwise linear regression function (xab,yab) allows one to improve an initial solution determined by the cluster Bab. Moreover, we use only data points from the setA¯1since it is expected that other data points will not provide a sufficient decrease of the overall fit function. The use of only “most promising” points allows one to decrease the number of linear regression problems to be solved.Now we can define the following set of hyperplanes:(10)A¯2=(u,v):u∈Rn,v∈Rand∃(a,b)∈A¯1s.t.u=x¯ab,v=y¯ab.Next we compute the valuefˆk(u,v)of the overall fit function fkover A with hyperplanes (x1,y1,⋯,xk−1,yk−1) and(u,v)=(x¯ab,y¯ab):fˆk(u,v)=f¯k(x¯ab,y¯ab)=∑(c,d)∈Aminrk-1cd,h(x¯ab,y¯ab,c,d)and the following number:(11)fˆk,min=minfˆk(u,v):(u,v)∈A¯2.It is clear thatfˆk,min⩾0. Let γ2∈[1,∞) be a given number. Define the following set(12)A¯3={(u,v)∈A¯2:fˆ(u,v)⩽γ2fˆk,min}.All hyperplanes from the setA¯3are considered as an initial solution to solve Problem (6). We define the numberγ2fˆk,minas a threshold and if the value of the clusterwise (or the auxiliary clusterwise) linear regression function at(u,v)∈A¯2is greater than this threshold this hyperplane is not considered as a “promising” to be an initial solution to minimize the auxiliary clusterwise linear regression function, since the value of this function at this initial solution is significantly larger than its best value. If γ2=1 then hyperplanes fromA¯2with the lowest value of the auxiliary clusterwise linear regression function are chosen and if γ2 is sufficiently large thenA¯3=A¯2.Thus, an algorithm for finding good initial solutions for solving Problem (6) can be summarized as follows:Algorithm 3An algorithm for finding initial solutions to solve Problem (6).Step 1: (Initialization) Select the numbers γ1∈(0,1) and γ2∈[1,∞).Step 2: Compute the numberd¯1using (7) and the setA¯1using (8).Step 3: For each(a,b)∈A¯1compute the set Babusing (9), update the clusterwise regression functions (xab,yab) and compute the setA¯2applying (10).Step 4: Compute the numberfˆk,minusing (11) and the setA¯3using (12). Any hyperplane(u,v)∈A¯3is an initial solution to solve Problem (6).Next we describe an algorithm for solving the auxiliary clustering problem (6). For a given hyperplane (u,v) we define the following sets:(13)B(u,v)=(a,b)∈A:h(u,v,a,b)<rk-1ab,B¯(u,v)=(a,b)∈A:h(u,v,a,b)=rk-1ab.The setB(u,v)contains all points from the set A which are attracted by the linear regression function (u,v) andB¯(u,v)contains all points from A which are attracted by at least two regression functions including (u,v). It is obvious thatB(u,v)≠∅for all(u,v)∈A¯3.Algorithm 4An algorithm for solving Problem (6).Step 1: (Initialization) Select the initial linear regression function(u0,v0)∈C¯k, the setB(u0,v0)and set l=0.Step 2: Solve the following linear regression problem:(14)minimizeφ(u,v)=∑(a,b)∈B(ul,vl)h(u,v,a,b)subjecttou∈Rn,v∈Rand obtain regression coefficients(ũl,ṽl).Step 3: Compute the setB(ũl,ṽl).Step 4: (Stopping criterion) IfB(ũl,ṽl)=B(ul,vl), then set(u¯,v¯)=(ul,vl)and stop.Step 5: Otherwise setul+1=ũl,vl+1=ṽl,B(ul+1,vl+1)=B(ũl,ṽl),l=l+1and go to Step 2.Some explanation on Algorithm 4 follows. In Step 1 we initialize the linear regression function and the setB(u,v). We choose the initial solution (u0,v0) outside of subregions where the functionf¯kis constant. This will allow us to apply any local search method for solving Problem (6). Such an approach guarantees a strict improvement of the overall fit function when adding a linear regression function. In Steps 2–5 we apply local search Algorithm 1 to solve Problem (6). However here we fix the first k−1 linear regression functions and update only the kth one at each iteration.Proposition 1Algorithm 4terminates in a finite number of iterations and if the setB¯(u¯,v¯)=∅then the point(u¯,v¯)is a local minimizer of the functionf¯k.Since Algorithm 4 is a descent algorithm that is the value of the objective functionf¯kis strictly decreased at each iteration and the number of possible combinations of data pointsB(u,v)is finite Algorithm 4 terminates in a finite number of iterations.Now assume that the setB¯(u¯,v¯)=∅. Then(15)f¯k(u¯,v¯)=∑(a,b)∈B(u¯,v¯)h(u¯,v¯,a,b)+∑(a,b)∈A⧹B(u¯,v¯)rk-1ab.It is clear that(u¯,v¯)is a global minimizer of the convex function(16)Φ(u,v)=∑(a,b)∈B(u¯,v¯)h(u,v,a,b)that isΦ(u¯,v¯)⩽Φ(u,v)for all(u,v)∈Rn+1.LetBε(u¯,v¯)={(u,v)∈Rn+1:‖(u,v)-(u¯,v¯)‖<ε}. There exists ε>0 such thath(u,v,a,b)<rk-1ab∀(a,b)∈B(u¯,v¯)and(u,v)∈Bε(u¯,v¯),h(u,v,a,b)>rk-1ab∀(a,b)∈A⧹B(u¯,v¯)and(u,v)∈Bε(u¯,v¯).Then for any(u,v)∈Bε(u¯,v¯)we havef¯k(u,v)=∑(a,b)∈B(u¯,v¯)h(u,v,a,b)+∑(a,b)∈A⧹B(u¯,v¯)rk-1ab=Φ(u,v)+∑(a,b)∈A⧹B(u¯,v¯)rk-1ab⩾Φ(u¯,v¯)+∑(a,b)∈A⧹B(u¯,v¯)rk-1ab=f¯k(u¯,v¯).Thusf¯k(u,v)⩾f¯k(u¯,v¯)for all(u,v)∈Bε(u¯,v¯).□We use the solution(u¯,v¯)found by Algorithm 4 as the initial solution to the kth clusterwise linear regression function in Step 3 of Algorithm 2. Any(u,v)∈C¯kcan be considered as an initial solution to solve Problem (6) since such initial solutions guarantee the decrease of the clusterwise linear regression function. However, we are interested in initial solutions which guarantee a significant decrease of the clusterwise linear regression function. Moreover, since Problem (6) is a global optimization problem and we apply local search Algorithm 1 it is important to use good initial solutions for finding global or near global solutions to Problem (6). We use solutions from the setA¯3for this purpose.Let γ3∈[1,∞) be a given number. Take any(u,v)∈A¯3and apply Algorithm 4 starting from this solution. As a result we get a regression function(u¯,v¯)which is the local minimizer of the auxiliary clusterwise linear regression functionf¯k. We denote byA¯4the set of all solutions obtained by Algorithm 4 starting from some(u,v)∈A¯3. Next we definef¯k,min=minf¯k(u¯,v¯):(u¯,v¯)∈A¯4and the following setA¯5=(u¯,v¯)∈A¯4:f¯k(u¯,v¯)⩽γ3f¯k,min.We use hyperplanes from the setA¯5as initial solutions to the lth clusterwise linear regression function in Step 3 of Algorithm 2 to solve Problem (2). This means that as a result we get a set of solutions to this problem and then we can choose the best one among them. Notice that if γ3=1 then only best local minimizers of the functionf¯kare chosen. If γ3 is sufficiently large then all local minimizers from the setA¯4of this function are chosen andA¯5=A¯4. Summarizing we can modify Algorithm 2 as follows:Algorithm 5Modified incremental algorithm for solving Problem (2).Step 1: (Initialization) Compute the linear regression function(x1,y1)∈Rn×Rof the whole set A. Set l=1.Step 2: (Computation of the next linear regression function). Set l=l+1. Let (x1,y1,⋯,xl−1,yl−1) be the solution to the (l−1)-clusterwise regression problem. Apply Algorithm 4 starting from each(u,v)∈A¯3to find a set of solutionsA¯5to the lth auxiliary clusterwise linear regression problem (6).Step 3: (Refinement of all linear regression functions) For each(u¯,v¯)∈A¯5select(x1,y1,⋯,xl-1,yl-1,u¯,v¯)as an initial solution, compute their respective clusters, apply Algorithm 1 to solve the l-clusterwise linear regression problem and compute a setA¯6of solutions(x¯1,y¯1,⋯,x¯l,y¯l)to the l-clusterwise linear regression problem.Step 4: (Computation of the solution) Choose any(xˆ1,yˆ1,⋯,xˆl,yˆl)=argmin{fl(x¯1,y¯1,⋯,x¯l,y¯l):(x¯1,y¯1,⋯,x¯l,y¯l)}as a solution to the l-clusterwise linear regression problem. Setxj=xˆj,yj=yˆj,j=1,…,l.Step 5: (Stopping criterion) If l=k, then stop. Otherwise go to Step 2.Algorithm 5 terminates at local minimizers of the overall fit function fk. Furthermore, the incremental nature of this algorithm and the use of many starting solutions ensure that the final solution is either global or near global minimizer of the function fkin many data sets as will be demonstrated in our numerical experiments.In order to implement Algorithm 5 one should select the values of three parameters: γ1, γ2 and γ3. The small values of γ1∈[0,1] will allow to include the most of data points in the setA¯1and therefore will lead to the solution of large number of regression problems. This can become very time consuming in large data sets. Therefore for small data sets (with the number of data points m⩽ 200) we choose γ1=0.3; for medium size data sets (with 200<m⩽1000) γ1=0.5 and for large data sets (with m>1000) γ1=0.95–0.99. Parameters γ2, γ3∈[1,∞) should be chosen so that not to remove any “promising” initial solution to globally minimize the functionf¯kusing Algorithm 4 and the function fkusing Algorithm 5. In numerical experiments we select γ2=γ3=10 for all data sets although these values can be decreased for some data sets.In order to solve linear regression problems in Algorithms 1, 4 and 5 we applied the Quasi-Newton method with the BFGS update (see, for example, [19]).Both Algorithms 1 and 5 were implemented in Lahey Fortran 95. Numerical experiments were carried out on a PC with Processor Intel (R) Core (TM) i5–3470S CPU 2.90gigahertz.In this section we present results of numerical experiments by applying the proposed algorithm to some real and random regression data sets. First we present some illustrative examples using three small data sets. Then results of numerical experiments on data sets with known solutions will be demonstrated and finally, we present results on large data sets. We also compare the proposed algorithm with the multi-start Späth algorithm using numerical results.Here we present the performance of the proposed algorithm and the importance of the choice of initial solutions using three small (with one independent and one dependent variables) data sets, namely, 2 lines, Votes and CEO Salaries data sets (see, [9,18,27] for details).We demonstrate the importance of the choice of initial solutions for the Späth algorithm using 2 lines data set. This data set contains 100 points. The initial solution was generated randomly and then we applied the Späth algorithm to find two lines approximating this data set. The result is presented in Fig. 1. This result show that the Späth algorithm is sensitive to the choice of initial solutions even in small data sets and depending on it the algorithm can end up at a solution which is significantly different from the global one.Next we demonstrate that the proposed algorithm is able to find global solutions to the clusterwise linear regression problem. For this purpose we applied it to Votes and CEO Salaries data sets. Votes data set contains 50 and CEO Salaraies data set contains 59 points. The value of global solutions for two and three linear regression functions in Votes data set and two linear regression functions in CEO Salaries data set were reported in [9]. All these values were reached by the proposed algorithm. In all cases the algorithm finished almost instantaneously, however it should be noted that unlike the method from [9] the proposed algorithm is not guaranteed to find the global minimum. In Fig. 2we illustrate the solutions for two and three regression functions in Votes data set and in Fig. 3we illustrate the solutions for two and four regression functions in CEO salaries data set.In this subsection we present results with small real as well as random data sets from [9]. Description of these data sets and values of global solutions with two and three linear regression functions using them can be found in [9]. We also include results by the multi-start (MS) Späth algorithm for comparison. To get comparable results we apply Späth algorithm starting from random initial solutions during the same amount of CPU time or number of linear regression problems solved required by the incremental algorithm to reach its solution. This means in small data sets the number of initial solutions in the MS Späth algorithm is small (50) and in large data sets this number is also large (1000–2000).In order to present results we use the following notation in all tables in this section:•k is the number of clusters (or linear regression functions);foptis the best known value of the overall fit function fk;E is the error in %;N is the overall number of linear regression problems solved for the computation of the corresponding number of linear regression functions;t is the CPU time (in seconds).The error E is computed as(17)E=(f¯-fopt)fopt+1·100,wheref¯is the value of the overall fit function obtained by an algorithm. E=0 implies that an algorithm finds the best solution.Results for small real data sets are given in Tables 1 and 2. Table 1 contains results with two clusters and Table 2 contains results with three clusters. We include in these tables only the number of linear regression problems solved since both algorithms reach the solution instantaneously.Results from Table 1 demonstrate that the proposed algorithm is able to find global or near global solutions in sufficiently dense data sets. However, this algorithm fails when data sets are sparse or have outliers. For example, data sets Check Off (with 56 points, 4 independent and 1 dependent variables), Crime Rates (with 47 points, 5 independent and 1 dependent variables) and Temperatures (with 56 points, 2 independent and 1 dependent variables) are among such data sets and solutions obtained by the proposed algorithm in this data sets are considerably different from global ones. Overall, the performance of the proposed and MS Späth algorithms in these data sets are similar.We used some of small real data sets to compute three clusters since only for these data sets values at global solutions are known. Results from Table 2 demonstrate that in most data sets both algorithms fail to find global solutions. This is due to the fact these data sets are not dense or they have outliers. These results again confirm that the proposed algorithm is not always efficient for solving clusterwise linear regression problems in such data sets.In order to investigate the performance of the proposed algorithm depending on the number of data points we applied it to randomly generated data sets varying the number of points from 20 to 5000. Data sets with 20–100 points are taken from [9]. Results are given in Table 3, One can see from this table that the proposed algorithm fails to locate global solutions in small data sets (20⩽m⩽30) however its ability to find global or near global solutions increases as the number of data points increases (m⩾ 35). As the number of data points increases the performance of both the proposed and multi-start Späth algorithms becomes very similar. One can also see that the number of solved linear regression problems increases almost linearly as the number of data points increases.Fig. 4shows the dependence of the number of solved linear regression problems on the number of data points for the proposed algorithm. One can see that after 1000 data points this dependence become linear.Table 4presents results on the performance of the proposed algorithm depending on the number clusters. Here we used the random data set with 5000 data points. These results demonstrate that the proposed algorithm is efficient to find a good approximation to global solutions of the clusterwise linear regression problem as the number of clusters increases. Moreover, the CPU time depends only linearly on the number of clusters.Dependence of the performance of the proposed algorithm on the number attributes is presented in Table 5. Here we used the random data set with m=50 from [9]. These results confirms the fact that the proposed algorithm is not efficient when a data set is not dense or has outliers. We can see that as the number of attributes increases the data becomes more sparse and the accuracy of the proposed algorithm is worsening. However, the increase of the number of attributes does not lead to any significant increase of the computational effort required: the number of solved linear regression problems are not increasing with the number of attributes. We do not include the CPU time because it is almost 0 for all data sets. The MS Späth algorithm is more accurate than the proposed algorithm in these data sets.Table 6presents results with the data sets of the synthetic observations generated from two lines with increasing perturbations from the normal distribution. Data sets contain 100 points with a perturbation from 0 to 1.2 standard deviations and we computed two clusters in each of them. We can see that both the proposed and MS Späth algorithms find global solutions in all data sets. Moreover, the computational effort required does not strongly depend on perturbation level. These results confirm that the proposed algorithm is not sensitive to the level of a perturbation in a data set.In this subsection we present results with real-world data sets having between 103 and 5875 data points. Seven test data sets for regression analysis available from [26] are used in calculations. We chose data sets with only real and integer attributes or removed categorical attributes before applying algorithms. No further preprocessing was applied. A brief description of the data sets are given in Table 7. Their full description can be found in [26] and in corresponding references shown in Table 7. Some data sets (Concrete slump test and Parkinsons telemonitoring) contain more than one output attributes. In this case we consider these attributes separately.Results of numerical experiments are presented in Tables 8 and 9. These results allow us to draw the following conclusions:1.The Späth algorithm starting from a randomly generated initial solution requires a large number of iterations to converge to a local solution. In comparison the algorithm for constructing initial solutions using incremental approach allows one to reduce the number of iterations required by the Späth algorithm. This can be seen from Table 9. Although the proposed algorithm solves more linear regression problems it requires significantly less CPU time that the MS Späth algorithm.One can see that the multi-start Späth algorithm is accurate for solving the clusterwise linear regression problems when the number of linear regression functions is small that is 2 or 3. When the number of linear regression functions is larger the incremental algorithm produce significantly more accurate solutions. One way to improve the accuracy of the multi-start Späth algorithm is to increase the number of random initial solutions however the running time will quickly become excessive. This demonstrate that the multi-start Späth algorithm is not an alternative to the proposed algorithm for solving the clusterwise linear regression problems even in relatively large data sets.Comparing results for small and large data sets one can see that the proposed algorithm produce more accurate results even for a small number of clusters in large data sets than in small ones. Moreover, the CPU time required by the proposed algorithm is reasonable.Figs. 5–7provide a graphical representation of the results by the proposed algorithm in Concrete slump test data set (second output), Housing data set and Parkinson’s telemonitoring data set (first output). They present the evolution of the number of linear regression problems, solved by the proposed algorithm, with the number of clusters. These figures clearly demonstrate that the number of solved linear regression problems increases almost linearly with the number of clusters. The proposed algorithm involves the solution of linear regression problems and distribution of points among clusters and the former one takes significant part of computational effort. These results show that the complexity of the algorithm depends linearly on the number of clusters. The evolution of the CPU time, required by the algorithm, with the number of clusters is very similar to that of the number of linear regression problems solved.In this subsection the generalization performance of the proposed algorithm is tested by splitting some data sets into training and test sets. This is done using the 10-fold cross-validation. More specifically, we randomly split the whole data sets into two parts. One part contains 90% of all points and is used as a training set. Another part contains 10% of all points and is used for testing. This is repeated ten times. We use three real-world data sets for this purpose: Housing, Red wine quality and Parkinson’s telemonitoring (with the first output) data sets. Housing data set is among small data sets, Red wine quality data set is among medium size and Parkinson’s telemonitoring data set the largest data set used in this paper. Clusters and linear functions approximating them are computed using training sets and they were tested using test sets. Results are reported in Table 10where we include the average error and also the standard deviation over 10 runs for both training and test sets. Note that we compute the error by dividing the value of the overall fit function to the number of instances in both training and test sets. In this table k stands for the number of linear functions used to approximate data.Results for the Housing data set presented in Table 10 demonstrate that the difference between average errors for training and test sets is quite large and also the standard deviations for the test set is much larger than those for the training set. This is due to the fact that data points in this data set are not dense and therefore the algorithm is not efficient for such data sets. We can conclude that the generalization performance of the proposed algorithm for such data sets is not always good. On the contrary, results for the Red wine quality data set demonstrate that the generalization performance is good for data sets where the algorithm is able to find either global or near global solutions. One can see that for this data set both the average errors and standard deviations for training and test sets are quite similar. The values of the standard deviation is too small demonstrating that the algorithm shows the good performance at all steps of the cross-validation. A similar picture can be observed for the Parkinson’s telemonitoring data set. However, the values of standard deviation for the test set is larger than those for the training set meaning that the algorithm does not show the good performance at all steps of the cross-validation. Results presented demonstrate the generalization performance of the algorithm depends on how efficient it is in finding global or near global solutions to clusterwise linear regression problem in a data set which in its turn depends on density of data points. Moreover, the algorithm demonstrate a good generalization performance in large data sets.

@&#CONCLUSIONS@&#
In this paper we developed an incremental algorithm for solving the clusterwise linear regression problem. This algorithm gradually finds clusters and linear regression functions within these clusters and minimizes the overall fit function. We proposed the algorithm to construct initial solutions at each iteration of the incremental algorithm using results obtained at the previous iteration. This algorithm allows one to find significantly more accurate solutions considerably faster than the multistart Späth algorithm in large data sets. Using small real and random data sets with known solutions we demonstrate that the proposed algorithm is able to find the global or near global solutions if the data sets sufficiently dense. However, the proposed algorithm fails to find global solutions if a data set is not dense or contains outliers. We also tested the proposed algorithm on seven publicly available regression data sets. The results presented demonstrate that the proposed algorithm is efficient to solve the clusterwise linear regression problem in large data sets.The Fortran source code of the proposed algorithm is available by request from the authors.