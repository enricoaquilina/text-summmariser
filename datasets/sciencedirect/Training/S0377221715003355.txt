@&#MAIN-TITLE@&#
Elicitation of multiattribute value functions through high dimensional model representations: Monotonicity and interactions

@&#HIGHLIGHTS@&#
We provide an assumption free method for eliciting multiattribute value functions.The methodology is grounded in the high dimensional model representation theory.The approach is tested via an experiment in which subjects evaluate mobile phone packages.The method allows a practical assessment of interactions and monotonicity.

@&#KEYPHRASES@&#
Multiattribute value theory,High dimensional model representations,Value function elicitation,Decision analysis,

@&#ABSTRACT@&#
This work addresses the early phases of the elicitation of multiattribute value functions proposing a practical method for assessing interactions and monotonicity. We exploit the link between multiattribute value functions and the theory of high dimensional model representations. The resulting elicitation method does not state any a-priori assumption on an individual’s preference structure. We test the approach via an experiment in a riskless context in which subjects are asked to evaluate mobile phone packages that differ on three attributes.

@&#INTRODUCTION@&#
In several circumstances, decision-makers (DMs) face decision problems with value trade-offs (Keeney & Raiffa, 1993). DMs might be selecting among transportation means based on attributes such as price, time, and comfort (Siskos & Yannacopoulos, 1985). In consumer research, several studies have shown that the DM’s willingness to pay depends on the alternative attributes of the product under scrutiny (Bettman, 1979). In purchasing a car for instance, consumers are asked to trade-off price vs safety and environmental protection (Bettman, Luce, & Payne, 1998). These problems fall within the realm of multiattribute utility theory (MAUT) (Smith & von Winterfeldt, 2004; Wallenius, Dyer, Fishburn, Steuer, Zionts, & Deb, 2008). Wallenius et al. (2008) underline the significant growth in applications of MAUT, by stating that the business world has become more competitive and less predictable, accentuating the importance of effective decision making and the use of decision support tools (Wallenius et al., 2008, p. 1340). MAUT focuses on one of the main steps of the decision analysis process: the definition and quantification of the DM’s values and objectives.If the selection is among alternatives with uncertain prospects, following Keeney and Raiffa (1993), the resulting preference function is called a utility function. Conversely, under certainty, one refers to a value function. Several researchers have addressed the conditions under which a value function may exist and special forms of value functions can be used (Dyer & Sarin, 1979; Fishburn, 1970; Gorman, 1968; Keeney & Raiffa, 1993; Krantz, Luce, Suppes, & Tversky, 1971). One of the key concepts in studying the form of value functions is preferential independence (preference separability in Gorman, 1968). To ease the appraisal, additive value functions are often assumed. Utility additive (UTA) methods use linear programming (LP) to construct preference model representing the DM’s value system (Figueira, Greco, & Slowiński, 2009; Greco, Mousseau, & Slowiński, 2008; 2010; Jacquet-Lagreze & Siskos, 1982).Deviations from additivity, however, are often encountered in practice. Methods for capturing more general preference structures have then been introduced. Recently, Kadziński, Greco, and Slowiński (2012) proposed a method which can be seen as a new interactive UTA-like procedure. The approach in Kadziński et al. (2012) is based on an additive model but it aims at selecting a single representing value function while accounting for the robustness concern. Other authors have been dealing with interactions and dependencies in a more direct way. Sounderpandian (1991) considers the situation when preferential independence is absent and studies the value functions which are applicable to such cases. Angilella, Greco, Lamantia, and Matarazzo (2004) and Angilella, Greco, and Matarazzo (2010) model preferences with interactions among criteria via Choquet integral (Grabisch, 1996). Angilella, Corrente, Greco, and Slowiński (2014) extend the Multi-criteria Satisfaction Analysis method to handle positive and negative synergies between pairs of attributes. Greco, Mousseau, and Slowiński (2014) propose a new method called UTAGMS-INT, which handles general interactions among criteria and provides an interesting alternative to the Choquet integral.Our goal is to provide a method for assessing the strength of interactions in a value function, without stating any a-priori assumption on its functional form. The method is grounded in the high dimensional model representations (HMDR) theory (also known as functional ANOVA), a multivariate integral decomposition method that has its roots in the findings of works such as Efron and Stein (1981), Rabitz and Alış (1999), Li, Wang, and Rabitz (2002), Sobol’ (2003). We exploit the facts that(i)each integrable multiattribute value function can be expanded as a sum of terms related to individual preferences for attributes, plus terms that quantify interactions among pairs, triplets of attributes, etc.,the terms in the expansion retain several of the properties of the original function (Beccacece & Borgonovo, 2011), such as monotonicity.The methodology consists of two steps: 1) experimental elicitation of subjects’ value functions over predetermined values of the attributes suggested by the adopted metamodeling method, and 2) reconstruction of the multiattribute value function through interpolation, guided by the HDMR principles.11The literature on this subject is vast, and offers us families of methods such as smoothing spline ANOVA (Friedman, 1991; Lin & Zang, 2006), polynomial chaos expansion, sparse grid interpolation (Buzzard, 2012; Sudret, 2008), cut-HDMR (Li, Wang, Rabitz, Wang, & Jaffé, 2002).In this respect, the methodology does not foresee to specify a functional shape or a predetermined set of properties for the value function (e.g., assuming that the value function is additive and/or monotonic, etc.) and then control for deviations. Conversely, it consists of first interpolating the value function based on the experimental responses, and then of analyzing the resulting shape and properties, to determine whether assumptions such as additivity and monotonicity hold.We apply the methodology in a laboratory experiment. Subjects are asked to price mobile phone packages that differ on three attributes: messages, minutes, and presence or absence of a special number (this is a number for which an unlimited number of minutes is allowed). We determine the value subjects assign to each package by a bisection procedure. This allows us to create a dataset on which the numerical interpolation procedure can be applied. The calculation of the first and total sensitivity indices is then used to determine relevance of interaction effects. Determination of the first order HDMR functions produces insights about monotonicity.The remainder of the paper is organized as follows. Section 2 provides a literature review. Section 3 introduces the theory and implements it. Section 4 is devoted to the experiment, while Section 5 shows the results. Section 6 concludes with suggestions for further research.A seminal review about research activity in MAUT up to the early 1990s is offered by Dyer, Fishburn, Steuer, Wallenius, and Zionts (1992). Dyer et al. (1992), aside describing state of the art, identify areas of significant future research in multiple criteria decision making (MCDM) and MAUT. Following up sixteen years later, Wallenius et al. (2008) determine that more than 5000 publications in MCDM or MAUT have been published between 1992 and 2007. This number indicates both the importance and the intensity of the research activity in the field.In this section, we limit our review to articles more closely related to the topic of the present work, in particular, works on the elicitation of value functions.Value functions play a role in several complex decision making problems, as witnessed by a considerable amount of formal theoretical developments (see the fundamental monograph of Keeney and Raiffa (1993)). Debreu (1954) proves that preferential independence among attributes is a sufficient condition for the existence of an additive value function. Additivity is a fundamental property and plays a central role in one of the most widely employed elicitation procedures, namely the UTA method (Jacquet-Lagreze & Siskos, 1982).As reported in Kadziński et al. (2012), Roy (1985) distinguishes multicriteria decision problems into four types: choice, ranking, sorting and description. To these problems Greco et al. (2008; 2010), Kadziński et al. (2012), and Kadziński, Ciomek, and Slowiński (2015) have successfully applied the UTA method. Besides additivity, the UTA method assumes piecewise linear and monotonic marginal value functions. This assumption is relaxed in the UTAGMS method (Greco et al., 2008). UTAGMS finds the largest set of additive value functions compatible with the preference information provided by the DM. We have two possible outcomes: the UTAGMS method either does not find a solution or it finds potentially multiple solutions. Alternative reasons can lead to the no-solution case. Among these, the fact that the DM’s preferences do not match the additive model. Conversely, when the preferences of the DM fit the additive model, then UTAGMS might find more than one compatible value function. A more recent preference disaggregation method, called RUTA (Kadziński, Greco, & Slowiński, 2013), which can be considered as a generalization of UTA, accounts for another type of indirect preference information in form of rank-related requirements. Rank related requirements or pairwise comparisons are a kind of preference information admitted by the family of robust ordinal regression methods (Corrente, Greco, Kadziński, & Slowiński, 2014; Greco, Slowiński, Figueira, & Mousseau, 2010b). These methods implement a preference construction paradigm which can be seen as a mutual learning of the DM and the preference model (Corrente, Greco, Kadziński, & Slowiński, 2013). On the experimental side, Korhonen, Silvennoinen, Wallenius, and Oorni (2012) investigate whether subjects are consistent with a linear value function while making binary choices.In some applications, if deviations from additivity do not lead to big biases, then they can be ignored (Dolan, 2000). However, it is often the case that separability is violated and interactions among attributes become relevant (Farquhar, 1977; Payne, Laughhunn, & Crum, 1984), so that interactions are too large to be ignored (Wakker, Jansen, & Stiggelbout, 2004, p. 219). The presence of interactions complicates the assessment. Kadziński et al. (2012) while assuming an additive form for the value function, introduce a new interactive UTA-like procedure to select a single value function representing the entire set of compatible value functions. The general procedure consists of four stages, which allow the DM to compare different alternatives on a relative basis and to specify the type of the marginal value functions (for greater details on the stages, we refer to Kadziński et al. (2012)). Besides additivity, a property widely used in the multiattribute elicitation is monotonicity. For instance Kadziński et al. (2012) assume a marginal monotone value function.Several papers have proposed theoretical developments in case of absence of preferential independence (Gorman, 1968; Sounderpandian, 1991). Interaction among criteria has been modelled by means of Choquet integrals following the seminal work of Grabisch (1996). Grabisch adopts a Choquet’s representation(1)C(v)=∫vdμ(A),where μ is a fuzzy measure. The representation in Eq. (1) captures interactions through its reliance on the non-additive measure, which attributes a weight of importance to every subset of criteria thus revealing the DM’s preference structure. Different types of interactions ranging from redundancy to synergy can be captured by Choquet integrals identifying the appropriate fuzzy measure. Thus, in addition to the usual weights on criteria taken separately, weights on any combination of criteria are also defined (Grabisch, 1996, p. 5). Within the Choquet integral framework, Angilella et al. (2004, 2010) and model interaction among criteria using non-additive ordinal regression and non-additive robust ordinal regression respectively.Although the riskless context is the main scope of this paper, we cannot omit a brief excursus on recent advances in preference elicitation under risk. In the risky context, Wakker et al. (2004) propose a methodology for determining levels of attributes that are as little as possible affected by interactions, so as to retain the advantages of additivity. Abbas and Howard (2005) propose a class of multiattribute utility functions called attribute dominance utility functions and show that they can be constructed using products of normalized marginal-conditional utility assessments or using single-attribute utility assessments and a copula structure (Abbas, 2009, p. 1368). To relax some conditions imposed in Abbas and Howard (2005) (such as the n-increasing condition which corresponds to multivariate risk-seeking (Richard, 1975)), Abbas (2009) introduces a more general copula structure, called multiattribute utility copula. As to monotonicity, also in the risky context, it stems naturally from the non-satiation principle, both in the single and multiattribute cases (Ingersoll, 1987; Tsetlin & Winkler, 2009) and is a widely adopted assumption (Abbas, 2009).The methodology we propose aims at assessing a DM’s value function without requiring any a-priori assumption. With respect to UTA, UTA-like and RUTA methods, our method does not assume a predetermined shape for the multiattribute value function. With respect to the Choquet integral approach, the methodology adopts an additive measure, and then, if interactions exist, reveals them assigning a non-additive shape to the value function v. Also, the traditional assumption on monotonicity is not stated.In this section, we lay out the mathematical aspects of the method. We represent a multiattribute value function asv:In⊆Rn→R,where I denotes the unit interval [0, 1], and Inthe n-dimensional unitary hypercube. Our sole assumption is that v is square integrable.One traditional approach is to use a multivariate Taylor expansion around a reference value to approximate the dependence of output on the attributes. Instead, we propose the use of an integral-based expansion, the HDMR of Rabitz and Alış (1999) (Li et al., 2002; Sobol’, 2003).We refer to Wang (2006) for a thorough account. We follow the notation of these works and consider a group of r attributes, letting z = {i1, i2, …, ir}⊂{1, 2, …, n} denote the corresponding indices. The complementary set (i.e., the set of indices not in z) is denoted by ∼ z, and is formally equivalent to {1, …, n}∖z. For such z,xz=(xi1,xi2,⋯,xir)denotes a group of r attributes, xz∈ Ir. Under the assumption that v is measurable, with x ∈ In, it can be proven (Sobol’, 2003) that v can be written as(2)v(x)=v0+∑z≠⌀vz(xz),wherev0=∫Inv(x)dx,∑z≠⌀denotes the sum over all nonempty subsets of indices, and the functions vz(xz) are determined recursively by(3)vz(xz)=∫In−r(v(xz,x∼z)−∑t⊂zvt(xt))dx∼z.As shown in Sobol’ (2003), the functions vz(xz) in Eq. (3) are orthogonal with respect to the L2 inner product using Lebesgue measure. The representation in Eq. (2) expands v(x) in such a way that the first order terms (functions depending on only one variable) represent how much of the deviation from the mean of the preferences can be ascribed to each attribute separately. The higher order terms represent the additional portion that emerges from interactions. In HDMR theory, one considers also the quantityD=∫In(v(x)−v0)2dx. Using orthogonality of the vz, it follows that(4)D=∑z≠⌀Dz,whereDz=∫Ir[vz(xz)]2dxz.In Eq. (4), D is the integral of the square deviations of v from the constant term v0. Thus, D is decomposed into 2n− 1 terms which are in one-to-one correspondence with the nonconstant terms of the integral decomposition of v(x). As such, Eq. (2) is not representing or responding to uncertainty, but, instead, it is obtained through a series of nested integration. We are replacing a Taylor expansion based on differentiation through an integral expansion. If one were to substitute the Lebesgue measure with any product measure (including Lebesgue) representative of the DM’s belief about x, then D would be the variance of v, i.e., the second moment of the distribution of v. However, this interpretation goes beyond the mechanical decomposition originally conceived in Sobol’ (1993), and, in fact, the decomposition fails if correlations are present.The consideration of Eqs. (2)–(4) is informative in many respects. In this work, two are of particular interest: (a) the quantification of the strength of interactions and (b) monotonicity. Concerning the strength of interactions, the quantity(5)ηz=DzDrepresents the contribution of the term Dzin the decomposition of D and is called the interaction effect of order r, where r is the cardinality of z. Of particular relevance are the so-called main and total effects, i.e.,(6)ηi1=D{i}DandηiT=∑z:z∋iDzD.In the formula forηiTthe sum in the numerator includes all terms in the decomposition of v containing index i. HenceηiTis the fraction of D associated not only with xialone but also with its interactions with all the remaining factor groups. The difference(7)ηiI=ηiT−ηi1=(∑z:z∋iDz)−D{i}Drepresents the strength of interactions associated with xi. Now, we recall that an additive multivariate mapping v is of the formv=∑i=1nhi(xi),where hi(xi) are univariate functions. If no interaction effects are present,ηi1=ηiTand, consequently,ηiI=0for all i = 1, 2, …, n. Conversely,ηiI≃0implies that a tolerable numerical error is committed truncating the expansion of v(x) after the sole first order terms (Rabitz & Alış, 1999). If v(x) is an elicited value function, estimation ofηiIthen allows us to appraise how far we are from additivity. That is, if the difference betweenηiTandηi1is non-negligible, then interactions emerge and we face non-additive preferences. However, if the difference is negligible, then we are close to additivity and the elicitation can be carried out with one of the methods that exploit this property.Knowledge of Eq. (3) can be utilized for assessing monotonicity as follows. Consider the univariate function of xiobtained by averaging v(x) over all attributes but xi(8)ti(xi)=∫In−1v(xi,x∼{i})dx∼{i}.Then it is readily seen that(9)vi(xi)=ti(xi)−v0.Thus, vi(xi) reproduces up to a constant term the average behavior of v as a function of xi. In this respect, it is worth noting that vi(xi) retain the monotonicity of v. In fact, it can be proven that if v(x) is monotonic, then all first order terms vi(xi) retain the monotonicity of v(x), independently of whether it is additive or not (see Theorem 2 in Beccacece and Borgonovo (2011)). Furthermore, in the case v(x) is additive, one obtains an if and only if condition. That is, if v is additive and monotonic, then all first order terms of its HDMR expansion are monotonic and the converse. Moreover, we recall that v is additive if it can be written as(10)v(x)=∑i=1nti(xi)−(n−1)v0.The additivity of v and Eq. (9) imply that the first order terms vi(xi) capture the dependence of v as a function of xiexactly. If v is not additive, this result can be used as follows. If any of the first order terms is non-monotonic, then we are ensured that v is non-monotonic.In the example below we apply the integral decomposition to a two-attribute multilinear function (see Appendix A for the corresponding calculations). The purpose of the example is to clarify how interactions are quantified by an HDMR expansion.Example 1Consider the following multilinear function,v:I2→R(11)v(x1,x2)=k1x1+k2x2+k1,2x1x2.Its integral decomposition results in the following terms(12)v0=k1x¯1+k2x¯2+k1,2x¯1x¯2,wherex¯i=∫01xidxi,i = 1, 2. The first and second order terms, are, respectively(13)v1(x1)=(x1−x¯1)(k1+k1,2x¯2)(14)v2(x2)=(x2−x¯2)(k2+k1,2x¯1)(15)v1,2(x1,x2)=k1,2(x1−x¯1)(x2−x¯2).The main effects are(16)η11=(k1+k1,2x¯2)2σ12Dandη21=(k2+k1,2x¯1)2σ22D,whereσi2=∫01(xi−x¯i)2dxiandD=∫01∫01(v(x1,x2)−v0)2dx1dx2. The total order effects are(17)η1T=(k1+k1,2x¯2)2σ12+k1,22σ12σ22Dandη2T=(k2+k1,2x¯1)2σ22+k1,22σ12σ22D,which leads to the interaction effects(18)η1I=η2I=k1,22σ12σ22D.Eq. (15) confirms that the presence of k1, 2 is instrumental to trigger interactions between the two attributes: if k1, 2 = 0, the function is additive and we have no interactions. Conversely, suppose k1, 2 ≠ 0. Then, we have a non-null interaction term v1, 2(x1, x2) in Eq. (15). However, note that v1, 2(x1, x2) is not simply equal to k1, 2x1x2. In particular, it equals the difference between k1, 2x1x2 and the termk1,2(x¯1x2+x¯2x1−x¯1x¯2),that is(19)v1,2(x1,x2)=k1,2(x1x2−x¯1x2−x¯2x1+x¯1x¯2).In fact, the strength of the interactions is quantified as a residual value over the individual effects [see Eq. (3)].Concerning monotonicity, Eqs. (13) and (14) suggest that v(x1, x2) depends linearly on x1 and x2, on average. The “on average” can be omitted if k1, 2 = 0. In fact, v1(x1) = k1x1 + const in the same way as v(x2) = k2x2 + const if k1, 2 = 0. Thus, the closer the function is to additivity, the better the average behavior approximates the exact behavior.The key to exploit the above-mentioned theoretical results is to approximate the mapping v: x↦y from given data using one of the so-called surrogate model or metamodel methods that have been developed recently in the applied statistical and mathematical literature. Metamodeling methods have developed greatly in association with the study of the behavior of scientific models produced through computer codes — we refer to the special issues of Bayarri, Berger, and Steinberg (2009) and Ratto, Castelletti, and Pagano (2012).The governing principle can be summarized intuitively as follows. The unknown input–output mapping is assumed to be square-integrable. Therefore, the mapping can be written in the form of Eq. (2), where a suitable truncation order applies. For illustrative purpose, let us stop the expansion at order 2. Then the governing equation is(20)v(x)≃v0+∑i=1nvi(xi)+∑i<jvi,j(xi,xj).In case of a polynomial surrogate model, each of the terms in the integral expansion is then, in turn, written through polynomials in the form(21)vi(xi)≈∑r=1hαriϕr(xi)vi,j(xi,xj)≈∑p=1h′∑q=1h′′βp,qi,jϕp(xi)ϕq(xj),where the ϕr( · ) functions are elements of an orthonormal basis, andαri,βp,qi,jare corresponding coefficients to be determined numerically. In general, the specific choice of polynomials, ϕr, and the maximum degree of these polynomials, both in a single-variable and in combinations of variables, has a great impact on the ability of the resulting expansion to approximate a given function. A great deal of work has been done recently on the approximation properties of such expansions (Barthelmann, Novak, & Ritter, 2000; Bungartz & Griebel, 2004) as well as on their use for calculating sensitivity coefficients (Buzzard, 2012; Crestaux, Le Maître, & Martinez, 2009). To describe the construction of a surrogate model more completely, let[X^Y^]denote a dataset, whereX^is an N × n matrix containing the realizations of the attributes andY^an N × 1 vector with the corresponding values of Y. Depending on the number of degrees of freedom of the surrogate model vsthe number of samples, the coefficientsαri,βp,qi,jare computed for best fitting the generated inputs (X^) to the corresponding model output (Y^). Then the functions vi(xi), vi, j(xi, xj) can be estimated numerically using Eq. (21) to obtain estimatesv^i(xi),v^i,j(xi,xj).Many methods for approximating the functions viand vi, jare possible. For instance, polynomial chaos expansion is discussed in Sudret (2008). An alternative method, based on orthogonal polynomials, which builds over the cut-HDMR expansion is discussed into Rabitz and Alış (1999), Li et al. (2002) and Ziehn and Tomlin (2009). Smoothing spline ANOVA methods are widely discussed in the statistical literature (Lin & Zang, 2006). Ratto and Pagano (2010) uses an approach based on the ACOSSO non-parametric regression method. All these methods use given data of the inputs (X^) and the output (Y^) to approximate v(x).The procedure can be summarized with the implementation steps presented in Table 1. We observe that the methodology applies also if the datasetX^Y^is given. This is the case if the dataset is obtained through, for instance, an online survey. IfX^Y^is not given, one needs to assess preferences through experiments, in which case both the choice ofX^and the method for obtainingY^must be considered carefully. This case shall be examined in this work in detail.In this work, we follow the steps in Table 1 using the formulation in Eq. (21) to guide the selection of sampling points (described further below). The sample pointsX^are chosen to be a nearly D-optimal set of points for polynomial regression, with the maximum degree 4 in each variable separately and interaction terms up to degree 2. This polynomial basis is commonly used for sparse grid interpolation, which is known to have good approximation properties (Barthelmann et al., 2000). As shown in Fedorov and Hackl (1997) for a given linear regression problem, a D-optimal set of sampling points allows for estimation of both the coefficients in the regression problem and the variance in these coefficients, assuming a Gaussian distribution of the sampled values. The use of a fixed polynomial basis in our setting allows us to construct a set of D-optimal points for this basis. There is a strong interaction between the choice of basis and the choice of sample points. If fewer data points are available, then fewer terms in the basis expansion can be estimated reliably. The criteria of D-optimality provides a condition that determines for a given basis which data points are sufficient for reliable estimation. Conversely, it can be used to determine a reasonable basis given constraints on available data. We then use cubic spline interpolation based on the Matlab routine “griddata” to approximate the functions viand vi, jfor purposes of calculating sensitivity coefficients. Once the samplesX^and the corresponding experimental valuesY^are known, we approximate v using cubic splines and then use a quasi Monte Carlo method to evaluate the integrals in Eqs. (3) and (4) to obtain the sensitivity coefficients.The HDMR theory and the associated tools are designed to measure the strength of interactions. Because HDMR is based on an additive measure, it offers a natural way for quantifying interactions in the value function. Once the relevant interactions are identified, one can simplify (ex-post) the functional form excluding those interaction terms which are not relevant. That is, one can exclude terms from the HDMR expansion in Eq. (2). After this step, one obtains a shape which includes the minimum number of interaction terms. Also, several methods assume monotonic marginal value functions. Our approach, conversely, does not assume monotonicity, but rather allows the analyst to test for it. In fact, the theory insures us that if this assumption is satisfied, the first order terms of the integral expansion will reveal this feature. This information can then be used to substantiate further analysis (or simplifications in the elicitation process) in which a monotonicity assumption is assumed.Subjects were 65 students from Bocconi University, coming from various academic backgrounds (mainly Economics and Business Administration). They were paid a show-up fee of 10EUR. In addition, each subject had a 10 percent chance to be selected to play out one of his choices for real. Once the experiment was over, subjects drew a ticket from a nontransparent bag containing 10 tickets, one of which was a winning ticket. If the subject had a non-winning ticket, he received 10EUR and the experiment was over. If he had a winning ticket, he received 10EUR and he had to choose a number in a given range, whose upper bound was determined by the subject’s number of questions. We then showed the subject the choice question corresponding to the selected number and the relative stored answer he provided. The answer could be either a mobile phone package or a sure amount of money. In case the package was chosen, the student was provided the corresponding monetary amount accounted for through interpolation.The experiment was computer-based, run in Italian with 30 minute sessions of 8 subjects (on average). Two experimenters were present in each session. Instructions were read aloud and contained a warm up question. At the end of the instructions the subject provided her/his age, gender, undergraduate/master degree.Subjects had to select between pairs of alternatives. Alternative A was composed of a mobile phone package, lasting one month, with three attributes: messages, minutes, presence (or absence) of a special number (a number for which an unlimited number of minutes is allowed). Alternative A was composed in such a way that subjects were familiar with the kind of questions that have been asked, given that the rationale is that the students should be actively and seriously involved in the decision problem (Scheubrein and Zionts, 2006, p. 20). Alternative B was a sure amount of money. Subjects had to choose between A and B, by clicking on their preferred option. They had to confirm their choice. If they confirmed their choice, then the next question was displayed; if not, the choice was displayed anew. The confirmation question had the purpose of reducing the impact of response errors. For each package, we determined the value that would make the subject indifferent between A and B through a series of choices that zoomed into subjects’ preferences. The iteration procedure to determine the indifference value is explained in detail in Appendix B. All choices were presented via a computer interface, an example of which is depicted in Fig. 1.Table 2reports 26 points for which we want to determine the indifference values yifor i = 1, …, 26. The 26 points were chosen to form an approximately D-optimal set of points for polynomial and/or spline regression over 2 variables. At the end of the instruction, at the beginning of the experiment, subjects had to answer two training questions. These questions only served to monitor subjects complete understanding of the task to be performed. There is always a trade-off between experimental accuracy and precision in the elicited function. Many papers, among the most recent ones (Attema, Werner, & Brouwer, 2013), find that matching tasks worsen in having more inconsistency than the choice task. The decision of being rigorous on the experimental side (using the choice task) forces us to have a longer experiment. To respect the reasonable amount a subject should spend on an experiment, the number of points should be less. However, 26 points are reasonable for the elicitation of such value function. Indeed, in the case of a package without a special number, 13 sample points allow for approximation up to the fourth order in each variable individually and up to the second order in the variables jointly. We then use the same points but with a special number.It is well established in the experimental literature that subjects might deliver answers which are inconsistent with one another (Korhonen et al., 2012). Thus, a proper system for monitoring consistency is necessary. Best practices recommend that such system is implemented in the experiment setup as a tool in support of internal validity of measurements. We monitored subjects’ consistency using two tests. In the first test, subjects were presented again with the third choice of 10 randomly chosen iterations. The third choice was repeated because the value of B in the third choice is generally close but not equal to the elicited indifference value, hence response errors are more likely (Bleichrodt, Cillo, & Diecidue, 2010). In the second test, subjects had to repeat the entire elicitations of y10, y12, y24, y26. These repetitions served for the purpose of giving an insight of the error made in the elicited indifference values.Consistency checks are often also used to detect and hence, eliminate, subjects who are not approaching the experiment seriously. The consistency rate in the repetition of the third choice of a given iteration is in general high and around 70 percent (Stott, 2006). In the repetition of the full procedure, an absolute difference between the repeated point and the original one which is twice higher than the standard deviation reflects an inconsistent behavior (Baillon, Bleichrodt, & Cillo, 2014). Violations of monotonicity are not rare, but a significant deviation from the average rate observed in the literature (Birnbaum & Thompson, 1996) can be interpreted as a subject’s confusion in the task.Once we started the elicitation of the indifference value of one package, we finished the entire bisection procedure. However, the consistency questions above described were interspersed all over so that subjects could not understand the bisection procedure.In total, 65 subjects were examined. 7 subjects were eliminated by the software because of inconsistency in executing the task. Indeed, if a subject clicked on the same option more than 5 times, the experiment ended automatically to avoid retaining subjects that had not understood the task.For subjects 11, 32 and 40 the consistency rates for the third choice of the 10 randomly chosen iterations are 40 percent, 50 percent, and 50 percent, respectively. These rates are very low compared to the above-mentioned criterion. Adopting the criterion of the maximum acceptable deviation of the repeated point from the original one, we observe that subject 15 fails in 3 repetitions for points y12, y24, and y26, while subject 24 fails in 2 repetitions for points y24 and y26. Subject 1 has a violation of monotonicity rate of 53.8 percent (rate extremely high compared to the ones observed in the literature (Birnbaum & Thompson, 1996)) in the 13 pairwise comparisons of a package with the special number and the same package without the special number, as in Table 2. Hence, by excluding these six subjects, we are left with 52 subjects. Of these 52 subjects, the repeated third choice of 10 randomly selected iterations and the repetition of 4 entire elicitations are not distant from the original answers, displaying consistency. Precisely, the consistency rate for the third choice of the 10 randomly iterations for 52 subjects is 91.3 percent, above the consistency rates in the literature (Stott, 2006). The entirely repeated elicitations of y10, y12, y24, y26 produce a paired t-test with p-values of 0.22, 0.6, 0.52, 0.23, respectively.We start analyzing the aggregate data for a mean and median subject. As postulated by best practices in experimental works, the crude experimental dataset has been analyzed to identify subject responses characterized by too large deviations. First, those violations that are larger than 2 times the average standard deviation (8.6) of the four differences between the original and the repeated measurements. This occurs in 3.2 percent of the cases (4.4 percent in the 30 tests and 2.1 percent in the 13 tests). This figure is lower than what is commonly observed in expertimental results (for a review see Birnbaum (2008)). The excluded points are the ones deviating more from the median values for the 52 subjects. The resulting aggregate mean, median and standard deviation values for the 52 subjects are reported in Table 2. These values are the input to the HDMR interpolation.Fig. 2displays the results for mean data. The graphs in the first row show the conditional preference functions v(x1, x2|x3 = 0) and v(x1, x2|x3 = 1) as they result from interpolation, where x1 = Minutes, x2 = Messages, x3 = Special Number. Graphs (a) and (b) display the response surface obtained without and with special number, respectively. The graph with special number presents some weak violation of monotonicity, as it will be discussed next. Graph (c) displays the response surface obtained without special number and the one with special number.Graphs (d) and (e) display the main effect functions. These functions show a monotone behavior.Graph (f) displays the estimates of the main effects for messages and minutes in the absence of the special number. We haveη^11=0.723andη^21=0.184. Thus, the attribute with the highest individual effect is minutes. Graph (g) displays the corresponding total order sensitivity indices,η^1Tandη^2T. We have:η^1T=0.817andη^2T=0.277,respectively. This result reveals that preferences are almost additive. In fact, the interaction effect is estimated atη^1I=η^2I=0.093.Graph (h) displays the main effects in the presence of the special number. Their estimates areη^11=0.646andη^21=0.206. Graph (i) represents the total order sensitivity indices:η^1T=0.794 andη^2T=0.354. However, we register a higher value of the interaction effect, which, conditional on the presence of a special number, increases toη^1I=η^2I=0.148. Thus, in the presence of a special number, preferences deviate further from additivity.Fig. 3displays the results obtained utilizing median data. Graphs (a) and (b) display the response surface with and without the special number, respectively, while graph (c) overlaps the two response surfaces.Graphs (d) and (e) display the Sobol’ main effect functions without and with the special number. Graph (f) displays the estimates of the main effects for messages and minutes in the absence of the special number. We haveη^11=0.581andη^21=0.286. Graph (g) displays the corresponding total order sensitivity indices,η^1T=0.715andη^2T=0.419,respectively. This result confirms minutes as the most important attribute. The estimated interaction effect isη^1I=η^2I=0.134,which is non-negligible.Graph (h) displays the main effects in the presence of a special number. Minutes are still more important than messages, but not as much as before. Indeed, the estimates areη^11=0.502andη^21=0.32. This result is confirmed by the values of the total order sensitivity indices in graph (i):η^1T=0.68andη^2T=0.5. We also note that preferences become less additive: the interaction effect increasesη^1I=η^2I=0.177.At an aggregate level, violations of monotonicity are more pronounced for median data than for mean data. Analyzing the data, we observe that the presence of a special number seems to make preferences less stable. Of the 13 pairwise comparisons between alternatives which differ only on the presence or absence of the special number, mean and median data always attribute a higher value to the alternative with the special number. On average, the presence of the special number is valued 4EUR more, both for mean and median data. Of the 30 pairwise comparisons between alternatives which differ only on one attribute, either minutes or messages, mean and median data seem to resist to violation of monotonicity, with few exceptions. In general the problem seems to appear with packages (800, 500, 0) and (800, 800, 0) for mean data and with packages (800, 500, 1) and (800, 800, 1) for median data. The increase of messages from 500 to 800 does not seem to be evaluated by subjects, independently of the presence or absence of the special number. A similar problem appears for median values of (1200, 800, 1) and (1500, 800, 1), and for median values of (1200, 800, 0) and (1500, 800, 0): the alternatives have the same value.At an individual level, we observe that the interaction terms both in presence and absence of the special number become significant (p = 0). Moreover, we cannot reject the null hypothesis of their difference being equal (two-sample t test p = 0.89). Individual interaction terms with and without the special number are extremely high. The mean value of the interaction term without the special number is 0.59 (median value 0.6), while the mean value of the interaction term with the special number is 0.59 (median value 0.67).At an individual level, violations of monotonicity are more frequent. Some violations of monotonicity are more common than others. Table 3shows the alternatives for which more than 30 percent of subjects violate monotonicity.As we can see from Table 3, violations of monotonicity occur more frequently when subjects are comparing alternatives that differ either in minutes or messages, then when they are comparing alternatives that differ by the presence or not of the special number. Four subjects violate only one of the 11 tests in Table 3, seven subjects violate two tests, 11 subjects violate three of them, 13 subjects violate four, nine subjects violate five, six subjects violate six, one subject has seven and another one eight of such violations. The interaction terms for subjects with a low rate of violation of monotonicity are still very high. There is no significant relationship between the rate of violation of monotonicity and the value of the interaction term.Zooming further into Table 3, we can see that several of the above tests compare packages with high levels of minutes. We perform an analysis that allows us to state if some of these violations are statistically more significant than other ones. Namely, we perform a paired t-test for each of the above comparisons, to see if we can reject the null hypothesis that the values are equal. For most of them we cannot reject the null hypothesis. This might suggest that subjects were indifferent between the compared packages, and that an apparent violation of monotonicity might be due to error. There are though some exceptions. For some comparisons we can reject the null hypothesis that the values are equal. It seems that subjects, when facing a high level of minutes (500 seems to be perceived already as such), ignore the presence of the special number ((800, 1200, 0) vs (800, 1200, 1)), or a higher level of messages ((800, 200, 0) vs (800, 500, 0) and (1500, 200, 0) vs (1500, 500, 0)), or a higher level of minutes ((500; 800; 0) vs (800; 800; 0)), to the point that better packages are evaluated significantly less (p < 0.02). Most of these pairwise comparisons also display a high level of messages, which can create similar problems. As before mentioned, the repeated elicitations provide values which do not differ from the original ones. Hence, also a comparison with the values obtained from the repeated elicitations would not provide a different answer. It is also true that if subjects evaluate wrongly a package this has impact on the other comparisons. Unfortunately, we are not able to control for this.

@&#CONCLUSIONS@&#
We have presented an elicitation method for multiattribute value functions. The method rests on the HDMR theory and has the advantage of not stating any assumption on the DM’s preferences structure. The interpolation of the value function exploits recent advances in the field of surrogate modelling which allows one to study whether preferences are additive and/or monotonic. The method does not pose restrictions on the type of data, which can be the result of surveys (internet surveys) or experiments (as discussed here). It is readily replicable and offers new reading glasses to investigate experimental results. It can be used as an initial screening exercise to detect consumers preferences. In fact, getting to know whether certain structural properties of multiattribute preferences hold in the initial phases of the elicitation process might simplify the overall assessment procedure providing guidance on the methods to use in a subsequent phase of the elicitation.As common in several multiattribute value function elicitation methods, also with our method one is exposed to the “curse of dimensionality” problem. With the current method, as we explained, one has a way to understand the accuracy of the estimation given the planned number of points. However, it is clear that when the number of attributes becomes high (say, greater than ten), one would need to elicit preferences at a high number of points, running the risk of long experimental times and, therefore, of worsening the accuracy of the elicited answers.To test the method, we analyzed data produced by experiments involving preference elicitation for mobile phone packages — a problem close to the sensitivity of the subjects involved in the experiment, namely, students. The mobile phone package contains three attributes. Application of the method has revealed (aside the overall consistency) that, at an aggregate level, little violations of monotonicity are registered and the presence of a special number increases the relevance of interactions and causes the value function to deviate from additivity. When data are analyzed at an individual level, however, results reveal highly non-additive value functions. Also, violations of monotonicity are present for high levels of minutes and messages.The nature of the experiment was descriptive and not a prescriptive one. However, the elicitation method can also be used in an experimental setting aimed at providing feedback to the subjects. In this context, after expressing their preferences in the first stage of the experiment, the subjects would be confronted with their answers and would be offered the possibility of re-expressing their preferences after this feedback. From the viewpoint of experimental techniques, because this type of experiment involves learning, it requires a dedicated experimental structure, different from the one used in this work. The method proposed here, however, would fit in this type of context as well. Indeed, exploring the differences between multivariate preferences elicited before and after a feedback is an innovative subject and part of future research. Also, the extension of the method to a risky context is part of future research by the authors.