@&#MAIN-TITLE@&#
Comparison study of orthonormal representations of functional data in classification

@&#HIGHLIGHTS@&#
Explain why a functional sample can be seen as a point in corresponding Euclidean space.Answer how to select an orthonormal basis for a given functional data type.Discover orthogonal representation is better than non-orthogonal representation as per classification performance.

@&#KEYPHRASES@&#
Functional data,Orthonormal representation,Orthonormal basis,Classification,

@&#ABSTRACT@&#
Functional data type, which is an important data type, is widely prevalent in many fields such as economics, biology, finance, and meteorology. Its underlying process is often seen as a continuous curve. The classification process for functional data is a basic data mining task. The common method is a two-stage learning process: first, by means of basis functions, the functional data series is converted into multivariate data; second, a machine learning algorithm is employed for performing the classification task based on the new representation. The problem is that a majority of learning algorithms are based on Euclidean distance, whereas the distance between functional samples is L2 distance. In this context, there are three very interesting problems. (1) Is seeing a functional sample as a point in the corresponding Euclidean space feasible? (2) How to select an orthonormal basis for a given functional data type? (3) Which one is better, orthogonal representation or non-orthogonal representation, under finite basis functions for the same number of basis? These issues are the main motivation of this study. For the first problem, theoretical studies show that seeing a functional sample as a point in the corresponding Euclidean space is feasible under the orthonormal representation. For the second problem, through experimental analysis, we find that Fourier basis is suitable for representing stable functions(especially, periodic functions), wavelet basis is good at differentiating functions with local differences, and data driven functional principal component basis could be the first preference especially when one does not have any prior knowledge on functional data types. For the third problem, experimental results show that orthogonal representation is better than non-orthogonal representation from the viewpoint of classification performance. These results have important significance for studying functional data classification.

@&#INTRODUCTION@&#
Recent years have witnessed considerable improvements in data acquisition technology and data storage abilities. As a result, it has become imperative to classify individual systems in various research fields based on one or more data series. The underlying process of every data series is an unknown function (continuous curve), called functional data. The classification process for functional data is typically the same as that for their underlying generation functions.At present, for the classification of functional data, there are two types of commonly used methods. One involves constructing functional classifiers, such as a functional support vector machine (SVM) by means of kernel techniques [3,33,48] and functional logistic regression [5,18,24,43,47,49], and the other is a two-stage classification method [28]. For the second method, in the first stage, usually, functional samples are represented in a finite dimensional functional subspace by means of basis functions; thus, functional data with infinite dimension becomes multivariate data, which consists of coefficients before the basis functions. In the second stage, a classical learning algorithm for finite dimensional data is used. The reason is that the high dimensionality of data series renders many data mining methods ineffective and fragile [8]. This obstacle is sometimes referred to as the “curse of dimensionality” [14]. In most data series mining problems, there is a need for dimensionality reduction and forming new data series representations [27]. It is required that the new representation preserves sufficient information for solving data series mining problems correctly. Once the basis is chosen, the optimal value for the number of basis functions can be derived from the data [48].Representing data series in the transformed domain is a common dimensionality reduction approach. Some of the popular transformation techniques are Fourier transform [15,33,53] and wavelet transform [11,16,32,37]. Functional principal component analysis(FPCA) [10,21,29,39,43,46,54–56] is a popular technique that uses statistical methods. Other methods include B-spline functions [1,3,35,59], Mercer kernel transforms [36,38], radial basis functions [4,5,26], etc.In fact, the representation of functional data is essentially a kind of approximation of itself. In the process of machine learning of functional data, a kind of structured representation using basis functions is used to transform functional data into multivariate data, and then, the distances between functional samples are converted into the Euclidean distances between the corresponding multivariate data. However, the representability of using the corresponding multivariate data to represent functional data, and the rationality of using the distance between the corresponding two multivariate data to replace the distance between two functional samples have not been studied in detail. Therefore, the relationship of different spaces is first introduced, and then the orthonormal representation theory is employed to explain the representability and rationality.Theoretically, under orthonormal basis, for any two different functional samples, the distance between them can be approximated based on the distance between their low-dimensional representations, which is isomorphic to the corresponding Euclidean distance. At this time, choosing an appropriate orthonormal basis is still a problem. Therefore, three kinds of common orthonormal basis and their differences are considered. The three kinds of orthonormal basis are normal Fourier basis, wavelet basis, and functional principal component basis, the eigenequation of FPCA is derived by means of variational theory.It is well known that non-orthogonal representation can also represent a functional data series as certain multivariate data. Therefore, it is important to verify if orthogonal basis has a stronger representation ability than non-orthogonal basis for functional data under the same number of basis functions from the viewpoint of classification performance.In order to verify the representation ability of the above orthonormal basis in classification, the extracted features(the coefficient vector, which consists of coefficients before the basis functions) of the functional data will be used in classification model construction. It has been pointed out in the literature [17] that support vector machine(SVM) and random forest are two preferred classification methods, and thus, LibSVM [12] and RandomForest [9,44] are first used to classify the functional data for three kinds of orthonormal representations. As other choices, logistic regression [29,40], K-nearest neighbor [30,31], and artificial neuron network [34,41] will also be used as classifiers for discriminating functional samples. Based on these classifiers, we shall also compare the classification performance of orthogonal representation with that of non-orthogonal representation.The main objective of this paper is to explain the rationality behind converting functional samples into corresponding multivariate data that are to be used for training a classifier. At the same time, from the point of view of experiments, we shall explain that among the three basis candidates, Fourier basis is suitable for representing stable signals(especially, periodic functions), wavelet representation can yield better results than Fourier representation for non-stationary signals, and orthonormal basis obtained through functional principal components offers good representation ability for some functional data with complex trend characteristics. Functional principal component analysis (FPCA), in particular, can be the first choice when people do not have any prior knowledge. Furthermore, we also demonstrate that orthogonal basis is indeed better than non-orthogonal basis from the viewpoint of classification performance.The remainder of this paper is organized as follows. Some basic concepts of functional data and some approximation theory under orthonormal representation are presented in Section 2. Section 3 describes three kinds of common orthonormal representations for functional data, and in particular, the eigenequation for functional principal component is derived using the variational principle. Section 4 introduces several classification methods including LibSVM, RandomForest, logistic regression, K-nearest neighbor, and artificial neuron network. Furthermore, four classification performance indexes such as the precision, the recall, F1 score, and the accuracy are introduced in detail. Section 5 provides numerical studies for feature extraction and classification methods for functional data. In this section, we analyze the classification performance of three different kinds orthonormal basis, point out which kind of orthonormal basis is appropriate to represent what type of functional data, and answer whether orthogonal representation is better than non-orthogonal representation for classifying functional data for the same number of finite basis functions. Section 6 concludes the paper with some remarks and discussions.Advances in data collection and storage have led to an increased presence of functional data, whose graphical representations are curves, images, or shapes [51]. The observation form of the functional data is also a two-dimensional table, which is shown in Table 1, in which Xi(t) (abbreviated as Xi), t ∈ I,i=1,2,…,Nis an underlying continuous and smooth function, and Xi∈ L2(I), where L2(I) is the space of the square-integrable functions defined on the compact set I,X:I→R,(∫IX2(t)dt)1/2 < ∞,Ris the real number space. At the same time, L2(I) is a separable Hilbert space with the inner product<X,Y>=∫IX(t)Y(t)dtand the norm∥X∥2=(∫IX2(t)dt)1/2. Xi(tj) denotes the observed value for Xi(t) at a discrete point tjfor the ith functional sample.To understand the L2(I) space, the relationship among different spaces is first introduced. It is well known that the introduction of the distance is for the purpose of studying the convergence. People, therefore, defined the metric space. In the metric space, the distance between any two elements can be computed. If the concept of completion (any Cauchy sequence is a convergent sequence [58]) is introduced in the metric space, the space will become a complete metric space.However, the metric space only has a topological structure, which restricts its application area. If a linear operation is introduced to the metric space, a linear normal space [58] can be obtained and the algebraic operation between elements can be carried out. In this case, the distance is transformed into the norm, which combines the metric and the linear operations perfectly. In other words, the linear normal space not only keeps its topological structure but also maintains its algebraic structure. The Banach space [2,52], especially, is a complete linear normal space, in which any element can be approached by a linear combination of basis vectors.On the other hand, the inner product space [50,58] is also a linear normal space, in which the norm is induced by the inner product (i.e.,∥X∥2=<X,X>1/2). Different from an ordinary linear normal space, in the inner product space, people can define the angle so as to further discuss the orthogonality. In particular, the Hilbert space[2] is a complete inner space, in which people can discuss both approximation and angle. In other words, any element in the Hilbert space can be infinitely approached by a linear combination of orthogonal basis.The relationship of different spaces is clearly shown in Fig. 1.Through Fig. 1, for a functional object, the complete metric space can be used to judge whether the object can be approached or not; the Banach space answers the problem of how to approach it (i.e., what is used to approximate it); and the Hilbert space indicates that it can be approached by a linear combination of a family of orthogonal basis. Functional data belong to the space of the square-integrable functions defined on the compact set I, just L2(I).Owing to L2(I) being a Hilbert space, in this subsection, we discuss several important properties of the system of normalized orthogonal functions in L2(I).Lemma 1[58]Let {φi} be a system of normalized orthogonal functions in L2(I), X ∈ L2(I). For a given k, suppose(1)X(k)=∑i=1kaiφi,whereai(i=1,2,…,k)is a real number, then∥X−X(k)∥2achieves its minimum value ifai=<X,φi>(i=1,2,…,k).[58]Let {φi} be a complete system of orthogonal functions in L2(I), X ∈ L2(I). Givenai=<X,φi>(i=1,2,…),one has that(2)limk→∞∥X(k)−X∥2=0.When {φi} is a complete system of normalized orthogonal functions, as per Lemma 1, one knows that X(k) is the optimal approximation of X in the k-dimensional subspace H0 of L2(I), where H0 is spanned by{φ1,φ2,…,φk}. Moreover, Lemma 2 shows that the approximation performance will improve as k increases.[57]LetXbe a Hilbert space. If {φi} is a complete system of normalized orthogonal functions ofX,thenXhas the corresponding Parseval equivalent formulation, i.e.,∀X∈X,(3)∥X∥22=∑a∈A|<X,φi>|2,where A is an index set.Since L2(I) is a Hilbert space, if {φi} is the complete system of normalized orthogonal functions in L2(I), for ∀X ∈ L2(I), takeai=<X,φi>(i=1,2,…),we have(4)∥X∥22=∑i=1∞ai2.Through the above lemmas, one can draw the following conclusion.Let {φi} be a complete system of normalized orthogonal functions in L2(I), ∀X, Y ∈ L2(I), andai=<X,φi>(i=1,2,…),bi=<Y,φi>(i=1,2,…). GivenX(k)=∑i=1kaiφi,Y(k)=∑i=1kbiφi, we have that(1)∥X(k)−Y(k)∥22=∑i=1k(ai−bi)2,∥X−Y∥22=limk→∞∥X(k)−Y(k)∥22.(1)It is evident that∥X(k)−Y(k)∥22=∥∑i=1kaiφi−∑i=1kbiφi∥22=∥∑i=1k(ai−bi)φi∥22=∑i=1k(ai−bi)2.By Lemma 3 and argument (1) of this theorem,∥X−Y∥22=∑i=1∞|<X−Y,φi>|2=∑i=1∞|<X,φi>−<Y,φi>|2=∑i=1∞(ai−bi)2=limk→∞∑i=1k(ai−bi)2=limk→∞∥X(k)−Y(k)∥22.This completes the proof.□Theorem 1 shows that the distance between two elements in L2(I) can be approximated by the corresponding distance between their low-dimensional representations in a subspace of L2(I). In fact, in a complete system of normalized orthogonal functions, the distance between two elements in the low-dimensional subspace equals the Euclidean distance between their coefficient vectors.For a classification problem, based on the observationXi(tj),i=1,2,…,N,j=1,2,…,p,one first finds an approximationXi(k)(t)of Xi(t),i=1,2,…,Nin a given subspace of L2(I). Based onXi(k)(t)=∑j=1kaijφj(t),we know that (ai1,ai2,…,aik) can be used to represent Xi,i=1,2,…,N. In this case, many classification algorithms can be directly applied to the objects characterized by the new features(ai1,ai2,…,aik),i=1,2,…,N.It is well known that the Fourier series can provide a basis expansion. Let X ∈ L2(I) and T be the measure of I, then X can be represented by the following orthonormal basis,1T,2Tsin(2πTt),2Tcos(2πTt),…,2Tsin(2kπTt),2Tcos(2kπTt),⋯Moreover, the fast Fourier transform(FFT) provides a strategy to determine the coefficients extremely efficiently if p (the observation number of X) is a power of 2, and the arguments are equally spaced [46]. A point worth noting is that: a Fourier series is well suited for representing stable functions (especially, periodic functional instances), while it is inappropriate for those functions with strong local features or discontinuous features.Functional principal component basis is also a kind of orthonormal basis. However, it differs from Fourier basis in that people cannot write out its explicit expression formula, which is often depicted by some trend characteristics. Its basic idea originated from Ramsay’s work [45]. In order to clearly express the process of acquiring functional principal components, in this subsection, multivariate PCA is introduced and the eigenequation for functional PCA is derived.We first introduce and discuss the method of multivariate PCA.For p-dimensional multivariate data X1, X2, ⋅⋅⋅, XN, letX=(X1′X2′⋮XN′)=(x11x12⋯x1px21x22⋯x2p⋮⋮⋱⋮xN1xN2⋯xNp)be a standardized matrix. Multivariate PCA can be used to find a linear transformation matrix Ak × p(k ≤ p) with a low-dimensional subspace, in which sample variance can be maximized on each dimension. Let ξjbe the jth column of A′,fij=ξj′Xirepresents the score of the ith sample on the jth dimension, and ξjis the jth principal component vector. The overall information mean (the mean squares of the scores) of all samples on the jth dimension is represented as1N∑i=1Nfij2,j=1,…,k. In fact,(5)1N∑i=1Nfij2=1N(f1jf2j⋯fNj)(f1jf2j⋮fNj)=1Nξj′(x11x12⋯x1px21x22⋯x2p⋮⋮⋱⋮xN1xN2⋯xNp)′(x11x12⋯x1px21x22⋯x2p⋮⋮⋱⋮xN1xN2⋯xNp)ξj=1NξjX′Xξj.LetV=1NX′X,where V is a sample covariance matrix. Hence, the jth principal component vector ξjhas(6)maxξj′Vξj.Furthermore, in order to guarantee the uniqueness of solutions, the constraint conditionξj′ξj=1needs to be considered. Based on this consideration, the above optimization problem becomes the following conditional extreme value problem:(7)F(ξ)=ξ′Vξ−λ(ξ′ξ−1).Taking the derivative with respect to ξ, the following equation is obtained:(8)F′(ξ)=2Vξ−2λξ=0,i.e., each principal component vector should satisfy the following eigen equation:(9)Vξ=λξ.In practice, we only select the k eigenvectorsξ1,ξ2,…,ξkwith the top k eigenvaluesλ1,λ2,…,λk,which constitute an orthonormal basis of a k-dimensional subspace. In this case,A′=(ξ1,ξ2,…,ξk). A popular method for choosing the parameter k is the scree plot, which is a graphical method [23]. To apply it, one plots the successive eigenvalues λjagainst j. The method recommends determining the j for which the decrease of the eigenvalues appears to level off. This point is used as the selected value of k.Many statistical applications today involve data that do not fit into classical univariate or multivariate frameworks; for example, growth curves, spectral curves, and time-dependent gene expression profiles [19]. These functional objects can be regarded as the samples in the space of square-integrable functions L2(I), where I is a compact set. In order to improve the performance of machine learning and speed up machine learning algorithms, functional PCA can be used to extract the important discriminant features of functional data. In essence, for any X(t) ∈ L2(I), the aim of functional PCA is to find an optimal approximation X(k)(t) in a low-dimensional functional subspace of L2(I), whereX(k)(t)=∑i=1kaiξi(t). The low-dimensional functional subspace H0 is spanned by{ξ1(t),ξ2(t),…,ξk(t)}. In addition, different from Fourier basis, wavelet basis, spline basis, etc., the functional principal component basis is driven by data.In this part, we mainly focus on the problem of acquiring functional principal components. Given some centralized functional objects X1(t), X2(t),…,XN(t), the objective of functional PCA is to find a functional subspace H0 so that the information of the functional data is maximized on each eigen dimension, whereξ1(t),ξ2(t),…,ξk(t)are eigen functions.fij=∫Tξj(t)Xi(t)dtrepresents the score of the ith sample on the jth dimension and ξj(t) is the jth principal component function. Similar to multivariate PCA, the overall information mean of the samples on the jth eigen dimension can be also represented as(10)1N∑i=1Nfij2=1N∑i=1N[∫Tξj(t)Xi(t)dt]2,j=1,…,k.In order to guarantee the uniqueness of the solutions, the constraint condition∫Tξj(t)2dt=1needs to be considered. The process of mining the principal components basically becomes the process of finding ξ′s that maximize:(11)F(ξ)=1N∑i=1N[∫Tξ(t)Xi(t)dt]2−λ[∫Tξ(t)2dt−1].Given∀ϵ∈R,∀η ∈ L2(I),Ris the real number space, based on the formula (11), one can obtain the following variational equation:(12)F(ξ+ϵη)=1N∑i=1N[∫T(ξ(t)+ϵη(t))Xi(t)dt]2−λ[∫T(ξ(t)+ϵη(t))2dt−1].Especially, whenϵ=0,variational Eq. (12)becomes formula (11).Letξ=ξ(t)maximize formula (11). For formula (12), taking the derivative with respect to ϵ, one has that(13)dFdϵ|ϵ=0=0,in detail,(14)dFdϵ|ϵ=0=1N∑i=1N2∫Tξ(t)Xi(t)dt∫Tη(s)Xi(s)ds−2λ∫Tξ(s)η(s)ds=2∫Tη(s)[∫Tυ(s,t)ξ(t)dt−λξ(s)]ds,whereυ(s,t)=1N∑i=1NXi(s)Xi(t).Combining (13), (14) with the arbitrariness of η(s), we have the following eigen equation(15)∫Tυ(s,t)ξ(t)dt=λξ(s).Thus, ξ′s maximizing formula (11) are solutions of Eq. (15). The left side of (15) is an integral transformVof the eigen function ξ defined by(16)Vξ=∫Tυ(s,t)ξ(t)dt.The integral transform is named as the covariance operator V. Therefore, we may also express the eigen Eq. (15)directly as (9). For the computational methods of functional principal components, see [46].In practice, we only select the corresponding k eigenfunctionsξ1,ξ2,…,ξkof the top k eigenvaluesλ1,λ2,…,λk. In this case,ξ1,ξ2,…,ξkconstitute an orthonormal basis in a k-dimensional functional subspace.(fi1,fi2,…,fik)can be used to represent Xi(t),i=1,2,…,N,t ∈ I.Remark 4To avoid the influence of different variable units, multivariate data usually need to be standardized. However, univariate functional data are not influenced by the units. For FPCA, functional data only need to be centralized. Of course, in order to better capture critical features, longitudinal transformation for functional data can be first carried out before principal component analysis. The longitudinal transformations include logarithmic transform, first order difference transform, second order difference transform, and so on.In the literature [20], it is mentioned that one common approach of functional data analysis is to project the functional samples onto a finite dimensional subspace of L2(I) and to use the basis coefficients in a learning algorithm. It is well known that functional data with dramatically local changes appear in many fields including economics, medical science, engineering, etc. Unlike Fourier basis and splines, wavelet transform can easily capture local properties of a functional signal. In general, wavelet basis is constructed using multiresolution analysis. For any primary resolution level j0 ≥ 0, the collection{ϕj0k,k=0,1,…,2j0−1;ψjk,j≥j0,k=0,1,…,2j−1}constitutes an orthonormal basis of L2(I), and ϕjk(resp.ψjk) is obtained by translations [13] and dilatations of a compactly supported function ϕ(resp. ψ), which is called as a ‘father’ wavelet(resp. a ‘mother’ wavelet), where(17)ϕjk(t)=∑l∈Z2j/2ϕ(2j(t−l)−k)and(18)ψjk(t)=∑l∈Z2j/2ψ(2j(t−l)−k).The idea underlying the wavelet approach is that a broad class of functions can be arbitrarily well approximated by a wavelet series [7]; i.e., for any function X(t) ∈ L2(I),(19)X(t)=∑k=02j0−1<X,ϕj0k>ϕj0k(t)+∑j=j0∞∑k=02j−1<X,ψj,k>ψj,k(t).The coefficient<X,ϕj0k>and < X, ψj, k> are called the scaling and wavelet coefficients of X(t), respectively.The first term in Eq. (19) is the smooth approximation of X(t) at level j0, and the second term is the detail part of the wavelet representation. We assume that each functional curve X is observed on a fine sampling gridt1,…,tp. Note that a wavelet decomposition of X can also be given in a form similar to that in (19). Forj0=0,we have(20)X(tl)=<X,ϕ00>ϕ00(tl)+∑j=0J−1∑k=02j−1<X,ψj,k>ψj,k(t),where J ≔ log2(N) is the maximal number of wavelet levels and < X, ϕ00 > and < X, ψj, k> are, respectively, the scale and wavelet coefficients of the discretized curve X at position k for resolution level j. These empirical coefficients can be efficiently computed using the discrete wavelet transform described in the literature [42].There are many types of wavelet transforms in the literature. In this paper, we shall adopt the Daubechies wavelet, which is an orthogonal basis with a compact support.In this subsection, we analyze the computational complexity of each of the above three orthonormal representations. In fact, for the representation of functional data, it is critical to find the basis coefficients. Suppose that a functional dataset has N functional samples and each functional sample has p observation points. For Fourier representation, the FFT makes it possible to find all coefficients extremely efficiently, and its time complexity isO(Nplogp). For wavelet representation, discrete wavelet transform(DWT) provides p coefficients closely related to the wavelet coefficients of each functional curve inO(p)operations [46]. As a consequence, given a functional dataset, the time complexity based on wavelet representation isO(Np). For a functional PCA representation, it is important to compute the covariance function in Eq. (15), therefore, its time complexity isO(Np2). According to the above analysis, one can see that the time complexity of wavelet representation is the lowest, that of functional PCA is the highest, and that of Fourier representation is in the middle. Of course, in practice, the characteristic of the data itself should be first considered for the choice of basis functions.In this subsection, we employ the average temperature data of Shanghai in 2012 to show its intuitionistic characteristic after orthonormal representation by using the above three orthonormal basis (see Fig. 2). In Fig. 2, the blue curve represents the raw average temperature curve, the black curve is based on Fourier representation, the red curve is based on wavelet representation, and the green curve is based on FPCA representation. From Fig. 2, one can see that wavelet basis can cope well with rapid changes in behavior, functional PCA can better fit the raw curve, and Fourier basis is smooth and stable.After the orthonormal representation of functional data, each functional sample becomes a point in Euclidean space. Using the Weka platform, we choose five kinds of classification methods including LibSVM, RandomForest, logistic regression, K-nearest neighbor(KNN), and artificial neuron network(ANN) to classify the functional data.LibSVM uses the library LibSVM and calls from Weka for classification with Gaussian kernel, based on gamma=1 and tolerance=0.001.RandomForest implements a forest of RandomTree base classifiers with 100 trees, using⌊log(#inputs+1)⌋inputs and unlimited depth trees.Logistic learns a multinomial logistic regression model with a ridge estimator, using ridge in the log-likelihoodR=10−8.ANN is a multilayer perceptron network with sigmoid hidden neurons, learning rate 0.3, momentum 0.2, 500 training epochs, and#hidden neurons equal (#inputs and#classes)/2.KNN is a K-nearest neighbor classifier, which tunes K by using cross-validation with linear neighbor search and Euclidean distance.To test the representation ability of different basis in the classification of functional data, four classification performance measures are employed including recall(Rec), precision(Pre), F1 score, and accuracy (Acc).Joachims [25] proposed the precision and the recall of a decision rule ψ in binary classification problems. Now, we generalize them to multi-class classification problems. For multi-class classification problems with K classes, given a decision rule ψ, the recall Rec(ψ)iof the ith class is defined to be the probability that an example X with labely=iis classified correctly, i.e.,ψ(X)=i:(21)Rec(ψ)i=P(ψ(X)=i|y=i)=P(ψ(X)=i,y=i)∑j=1KP(ψ(X)=j,y=i).Similarly, the precision Pre(ψ)iof the ith class is defined to be the probability that an example X classified asψ(X)=idoes indeed have the same label, i.e.,y=i:(22)Pre(ψ)i=P(y=i|ψ(X)=i)=P(ψ(X)=i,y=i)∑j=1KP(ψ(X)=i,y=j).The F1 score of the ith class is used to reconcile the precision withthe recall, which is formally defined as follows:(23)F1(ψ)i=2Pre(ψ)iRec(ψ)iPre(ψ)i+Rec(ψ)i.Based on the recall, the precision, and F1 score of the ith class, we can obtain the weighted recall (Rec), the weighted precision(Pre), and the weighted F1 score(F1), which are called the recall, the precision, and F1 score of the decision rule ψ, respectively. Their definitions are as follows:(24)Rec(ψ)=∑i=1KP(y=i)Rec(ψ)i,(25)Pre(ψ)=∑i=1KP(y=i)Pre(ψ)i,and(26)F1(ψ)=∑i=1KP(y=i)F1(ψ)i,respectively.Furthermore, the accuracy Acc(ψ) of a decision rule ψ is defined as follows:(27)Acc(ψ)=1N∑l=1NI(ψ(Xl)=yl),whereI(ψ(Xl)=yl)={1,ψ(Xl)=yl,0,ψ(Xl)≠yl.Tecator data is available at http://lib.stat.cmu.edu/datasets/tecator. The dataset(see Fig. 3) consists of 215 nearinfrared absorbance spectra of meat samples, recorded on a Tecator Infratec Food Analyzer. Each observation consists of a 100-channel absorbance spectrum in the wavelength range of 850–1050 nm. The goal here is to predict whether the fat percentage is greater than 20% from the spectra. Among the 215 samples, 138 have fat percentage less than 20%.Face data, ECG data, and ItalyPower data are taken from the UCR Time Series Classification and Clustering website.11http://www.cs.ucr.edu/~eamonn/time_series_data/The Face dataset (see the figure on the left in Fig. 4) consists of 112 curves sampled from 4 groups at 350 instants of time. The ECG dataset(see the figure on the right in Fig. 4) consists of 200 electrocardiogram from 2 groups of patients sampled at 96 time instants. The ItalyPower dataset (see Fig. 5) consists of 1029 curves sampled from 2 groups at 24 time instants.Sdata dataset is a simulated dataset. In this dataset, we make a simulated classification example with three known underlying generation mechanism:g2(t)=cos(1.5πt),g2(t)=sin(1.5πt),g3(t)=sin(πt)t ∈ [0, 1]. Fig. 6presents underlying generation curves and three sample curves of each class, where each class consists of 200 curves.Phoneme data was formed by selecting five phonemes for classification based on digitized speech from the TIMIT database. The dataset consists of 4509 speech frames with “aa” (695), “ao”(1022), “dcl”(757), “iy” (1163), and “sh” (872). The phonemes are transcribed as follows: “sh” as in “she”, “dcl” as in “dark”, “iy” as the vowel in “she”, “aa” as the vowel in “dark”, and “ao” as the first vowel in “water”. From each speech frame, a log-periodogram of length 256 was computed. The data, which is available at http://www-stat.stanford.edu/~tibs/ElemStatLearn/, was used in the paper on penalized discriminant analysis (PDA) by Hastie et al. [22]. Fig. 7shows five examples for each of the five classes.From Figs. 3 and 4, Tecator dataset, Face dataset, and ECG dataset have local significant feature differences. From Figs. 5–7, ItalyPower dataset, Sdata dataset, and Phoneme dataset have global feature differences.For machine learning of functional data, one needs to first find the critical features of functional data based on their low-dimensional orthonormal representations. That is, letφ1,φ2,…,φkbe the orthonormal basis in certain k-dimensional subspace, then X can be approximated byX(k)=<X,φ1>φ1+<X,φ2>φ2+⋯+<X,φk>φk.In addition, k-dimensional coefficient vector(<X,φ1>,,<X,φ2>,…,<X,φk>)can be used to represent functional sample X. Secondly, a learning algorithm like SVM is performed with the ‘reduced’, low-dimensional data [6]. All of the following classification experiments are carried out according to the 10-fold cross-validation criterion.In most applications, it is important to determine a value of k such that the actual data can be replaced by the approximation∑i=1k<X,φi>φi. For example, in the case of functional principal component basis, a subjective decision for the choice of k can be made from a scree plot [28], which shows percentages of variation of functional samples. As for the Fourier basis and wavelet basis, k can be selected based on the total mean-squared error [45], which makes a trade-off between bias and sampling variance. Ramsay [45] pointed out that people find it difficult to attempt to fix model dimensionality since there is no one gold standard method for the variable selection problem. In this paper, we mainly focus on the classification performance difference of three orthonormal representations with the same number of basis and the same classifier. Therefore, according to the characteristic of functional data set, different number of basis is utilized to represent the functional data set. The comparisons of their classification performance on the six data sets are shown in Tables 2–7. In every table, the value of k indicates the number of basis functions.From Tables 2–4, it can be seen that theclassification performance of wavelet representation is statistically better than that of Fourier representation. It shows that wavelet basis may be better at capturing local characteristics. This results from the fact that the wavelet expansion of a function X yields a multiresolution analysis and thus wavelets provide a systematic sequence of degrees of locality (see formula (18)). From Tables 5–7, it is evident that the classification performance of Fourier representation is statistically better than that of wavelet representation, and is more robust with respect to number of basis. It shows that Fourier basis is appropriate for representing periodic functions and stable signals. In fact, Fourier expansion is a linear combination of sine functions and cosine functions, and is generally uniformly smooth. From these tables, we argue that functional PCA may be a better representation method especially when one does not have any prior knowledge for the characteristics of functional data. For complex functional data such as Face dataset, ECG dataset, and Phoneme dataset, functional PCA exhibits much better performance than others since it is data-driven. Besides, we can also see that the classification performance is very relevant to the number of orthonormal basis. However, it should be noted that too many basis functions may cause an over-fitting problem for the classifier. Through the experiments, it can be also seen that LibSVM, RandomForest, and artificial neuron network(ANN) show much better classification performance.In this subsection, we compare the classification performance of classifiers induced by orthogonal representation and non-orthogonal representation. Now, Fourier basis{1,sin(t),cos(t),…,sin(kt),cos(kt),…}is used to represent functional samples. If the measure T of I is not 2π, then the above basis is not orthogonal basis. The classification performance for non-orthogonal representation (just Non-or) can be seen from Tables 2–7.First, we can see that the classification performance of classifiers induced by orthogonal representation is statistically better than that induced by non-orthogonal representation on every data set. Second, since Fourier basis is appropriate to represent functional data with periodic characteristic, the difference between two kinds of representations is not too large for the Sdata data set. However, the classification performance of classifiers induced by orthogonal representation is still better than that of non-orthogonal representation. Third, for functional data with local difference characteristics including Tecator dataset and Face dataset, there are obvious difference between the classifiers induced by orthogonal representation and those induced by non-orthogonal representation for classification performance.

@&#CONCLUSIONS@&#
The main motivations of this study were to answer three important problems: (1)Why can a functional sample be seen as a point in the corresponding Euclidean space after certain orthonormal representation? (2)How to select orthonormal basis for a given functional data type? (3) For orthogonal representation and non-orthogonal representation, which one is better under finite basis functions with the same number of basis?For the first problem, in this paper, we have given a theorem for illustrating the reasonability of representing a functional sample as a point in the corresponding Euclidean space, which is isomorphic to the low-dimensional representation space for the functional sample. In this case, the distance between two functional samples becomes the Euclidean distance between two points in the classification process, and thus, based on this representation, some machine learning algorithms can be utilized to classify a functional data set.For the second problem, we have performed a series of comparison analysis in-between Fourier basis, functional principal component basis and wavelet basis. Experiment results show that the selection of orthonormal basis may depend on the characteristics of functional data themselves, which is helpful for obtaining a classifier of functional data with better generalization ability. Fourier basis may be suitable for stable functional data (especially periodic data), wavelet basis may be appropriate for functional data with local characteristic, and data driven functional principal component basis could be the preferred choice when prior information about the characteristics of functional data is not known. In particular, the eigenequation of FPCA is obtained by means of variational theory.For the third problem, experimental results have also shown that orthogonal representation may be much better than non-orthogonal representation from the viewpoint of classification performance, because orthogonal representation may include more information under finite basis functions with same number of basis.To summarize, the research results would be very helpful for guiding researchers to reasonably use orthonormal representation methods for machine learning of functional data.