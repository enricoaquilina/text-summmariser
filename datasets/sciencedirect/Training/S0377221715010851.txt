@&#MAIN-TITLE@&#
Order acceptance and scheduling problems in two-machine flow shops: New mixed integer programming formulations

@&#HIGHLIGHTS@&#
We consider the order acceptance and scheduling problem in two machine flow shops.Two new mixed integer linear programming formulations are presented for the problem.Several techniques are developed to improve each of these formulations.Larger instances of the problem are solved within a reasonable amount of time.

@&#KEYPHRASES@&#
Order acceptance,Scheduling,Mixed integer programming,Preprocessing,Valid inequalities,

@&#ABSTRACT@&#
We present two new mixed integer programming formulations for the order acceptance and scheduling problem in two machine flow shops. Solving this optimization problem is challenging because two types of decisions must be made simultaneously: which orders to be accepted for processing and how to schedule them. To speed up the solution procedure, we present several techniques such as preprocessing and valid inequalities. An extensive computational study, using different instances, demonstrates the efficacy of the new formulations in comparison to some previous ones found in the relevant literature.

@&#INTRODUCTION@&#
Many manufacturing companies use Make-To-Order (MTO) production systems. In MTO systems, planning for the manufacture of a product will begin only when a customer order is received. The main advantage of these systems is that they give rise to low finished goods inventories. However, these systems have a significant disadvantage in that the lead time for the fulfillment of orders may result in significant financial loss for companies because of the loss of business due to production limitations. As a consequence, to remain competitive, companies employing these systems must decrease their order delivery times. This can be achieved by employing an accurate production plan that determines which orders should be accepted and how they should be scheduled. The solution to the Order Acceptance and Scheduling Problem (OASP) is an important step in the development of such a plan.OASPs have been studied extensively over the past 20 years and a number of different versions of these problems exist. We refer interested readers to the literature survey by Slotnick (2011) for details. Versions of OASPs in which the objective functions maximize the total net revenue, i.e. the difference between sum of revenues and total weighted tardiness or lateness, have been studied by many authors in a single-machine environment. Slotnick and Morton (1996) are believed to be the first researchers who addressed this problem under the assumption of static arrivals, meaning that all jobs are assumed to be available at time zero. They proposed two heuristic algorithms and a Branch and Bound (B&B) technique to solve the problem in this case.Later, Ghosh (1997) proved that an OASP with lateness penalties is NP-hard. He also presented two pseudo-polynomial time dynamic programming algorithms, and a polynomial-time approximation scheme in order to solve the problem. Slotnick and Morton (2007) considered tardiness related penalties instead of lateness penalties. They developed a B&B algorithm and a number of heuristics to solve this problem exactly with at most 10 jobs in about 6000 seconds on average. As far as we know, the largest instances of OASP with tardiness related penalties in a single-machine environment were solved by Nobibon and Leus (2011). They proposed two Mixed Integer Linear Programming (MILP) formulations and could solve instances of the problem with at most 50 jobs to optimality within two hours using the IBM ILOG CPLEX Optimizer (see http://www-01.ibm.com/software/info/ilog). In order to compute high quality solutions for large size instances of the problem, Rom and Slotnick (2009) developed a genetic algorithm. They showed that while their proposed approach is slower than the available heuristics appearing in the relevant literature, it generates solutions of higher quality.Oğuz, Salman, and Yalçın (2010) added more assumptions to the OASP with tardiness related penalties in a single machine environment. They considered release dates for each job and sequence dependent setup times. They gave a MILP formulation of the problem and could solve instances of the problem with at most 15 jobs to optimality. To compute high quality solutions for larger size instances of the problem, they also developed three heuristics. Later, Cesaret, Oğuz, and Salman (2012) developed a tabu search algorithm for this problem. They showed that their proposed algorithm is faster and can provide solutions with higher quality when compared with previous heuristics. Lin and Ying (2013) introduced a new artificial bee colony based algorithm to solve this problem. Their experimental results indicated that their proposed heuristic is competitive with the algorithm by Cesaret et al. (2012).There are also a few studies about the OASP with tardiness related penalties in an m-machine permutation flow shop environment. For example, Xiao, Zhang, Zhao, and Kaku (2012) developed a partial optimization based simulated annealing algorithm to solve instances of the problem. Later, Lin and Ying (2015) presented a multi-initiator simulated annealing algorithm, and showed that the new heuristic outperforms the algorithm by Xiao et al. (2012). Recently, Lei and Guo (2015) addressed the biobjective version of the problem where the objectives are minimization of the makespan and maximization of the total net revenue. To solve instances of the problem, they employed a parallel neighborhood search algorithm and compared it with a tabu search and a variable neighborhood search algorithm.In this paper, we consider the OASP in a 2-Machine Flow shop environment (OASP-2MF) which is recently addressed by Wang, Xie, and Cheng (2013a) and Wang, Xie, and Cheng (2013b). In Wang et al. (2013b), the authors tried to generalize the work of Slotnick and Morton (2007). They introduced two MILP formulations which could solve instances of the problem with up to 13 jobs within a one hour time limit using CPLEX. In addition, they proposed a B&B algorithm which simultaneously took into account job selection and scheduling and benefited from the use of some dominance rules in the pruning procedure. They showed that their purpose-built solver can solve larger sized instances of the problem with up to 20 jobs within an hour. In Wang et al. (2013a), the authors developed a modified artificial bee colony algorithm to compute good solutions for even larger instances of the problem.The main contribution of our research is the development of two new MILP formulations for the OASP-2MF. In addition, to speed up the solution procedure, we present several enhancements (cuts and preprocessing techniques) which can reduce the size of the problem significantly and make the formulations stronger. Our new formulations have the following three desirable characteristics:•The number of variables and constraints in these formulations is quadratically bounded by the number of jobs. Some previous researchers have developed time-indexed formulations for single-machine or 2-machine flow shop versions of the OASP, but the size of these formulations can increase dramatically if processing times are large.They outperform previous formulations even before applying the enhancements.CPLEX can solve instances of the problem that are 5 times larger than those solved by the purpose-built solver which is developed in Wang et al. (2013b).We compare our new formulations after applying the enhancements and show that one of them performs far better than the other. Using our best formulation, CPLEX can achieve an optimality gap of less than 2 percent, on average, within 1800 seconds, even for instances of OASP-2MF with as many as 100 jobs.The rest of the paper is organized as follows. In Section 2, we review some preliminary notation and results. In Section 3, we introduce two new MILP formulations for OASP-2MF. In Section 4, we discuss enhancements to make the formulations stronger. In Section 5, we report the results of a comprehensive computational study. Finally, in Section 6, we give some concluding remarks.In an OASP-2MF two decisions must be made at the same time: which orders to be accepted for processing and how to schedule them. We assume that the set of orders (jobs) is known in advance. Due to the flow shop structure of the problem, each job can be processed on machine 2 at some time after its processing on machine 1 has been completed.We denote the set of jobs byN={1,2,…,n}. The revenue and processing times of each job i ∈ N on machines 1 and 2 are denoted byui∈Z+,pi1∈Z+andpi2∈Z+(where we useZ+to denote the set of positive integers), respectively. We sometimes sort the processing times on each machine from small to large. We usep[i]1andp[i]2to denote the processing times in the ith position of the sorted lists, for machines 1 and 2, respectively. We denote the completion time of job i ∈ N by Ci. We assume that each job i ∈ N has a due date, denoted bydi∈Z+,and that there is a delay penalty, denoted bywi∈Z+,for each unit of the completion time which exceeds di, i.e.,Ci−di(note that due dates are positive integers). Also, there is no reward or penalty for early delivery. We sometimes refer to the delay time of the job i ∈ N as its tardiness, and denote it by Ti, i.e.,Ti=max{0,Ci−di}. The net revenue, i.e., the difference between the revenue and the delay cost, of each job i ∈ N is defined byπi:=ui−wi·Ti. Furthermore, we assume that the goal is to maximize the total net revenue in the OASP-2MF.Observe that, in an optimal solution, if job i ∈ N is accepted, then πi≥ 0. Moreover,πi=uiif job i ∈ N is accepted and fulfilled before its due date, and πi< uiif job i ∈ N is accepted and fulfilled after its due date. We sometimes refer toui−πi(or equivalently wi· Ti) as the tardiness penalty for job i ∈ N, if it is accepted. The following propositions provide the basis for the development of the different MILP formulations and enhancements in this paper. Note that Propositions 1 and 3 are straight forward to prove, and they are known results in the literature of the classical 2-machine flow shop problem (see for instance Baker, 1974; Kim, 1993). Therefore, we have omitted their proofs. Moreover, it should be mentioned that Wang et al. (2013b) used Proposition 1 to validate their MILP formulations.Proposition 1For accepted jobs, there is an optimal schedule in which each job is processed on both machines in the same sequence.In an optimal schedule,CiU:=uiwi+diis an upper bound for the completion time of an accepted job i ∈ N.Suppose that the assertion is not true. Therefore, there must exist a job i ∈ N in an optimal schedule whose completion time Ciis strictly larger thanCiU. The net revenue of job i isπi=ui−wi·max{0,(Ci−di)}.BecauseCiU<Ci,πi<ui−wi·max{0,(CiU−di)}=ui−wi·max{0,(uiwi+di−di)}=0.This contradicts our assumption that the schedule is optimal since we can improve the total net revenue by simply rejecting job i.□Note that Proposition 2 is independent of machine environment and processing restrictions.Proposition 3Let S be the list of accepted jobs for a schedule and suppose that the jobs will be processed in the same order as they appear in the list. Let Jk∈ S be the job which is allocated to position k in the list S where 1 ≤ k ≤ |S| and |S| is the number of elements of S. ThenCJkL:=max{∑1≤q≤kpJq1+pJk2,∑1≤q≤kpJq2+pJ11}is a lower bound for the completion time of job Jk.In this section, we describe two new MILP formulations for the OASP-2MF. We then compare them with two Previous Formulations (PF1 and PF2) proposed by Wang et al. (2013b) in terms of size complexity and the number of disjunctive constraints (meaning those with a big M parameter). The validity of both of these formulations is confirmed by Proposition 1.New Formulation 1 (NF1): In order to describe the model, we first define some sets of decision variables. For each job i ∈ N, we use a binary decision variable to indicate whether the job is accepted or rejected,yi:={1Ifjobiisaccepted,0Otherwise.For each pair of jobs (i, j) ∈ N2 with i ≠ j, we define a binary decision variable to indicate which one is to proceed earlier,zij:={1Ifjobsiandjareaccepted,andjobjisprocessedafterjobi,0Otherwise.We also use continuous decision variables Ciand Tito represent the completion time and tardiness, respectively, of each job i ∈ N. Using these decision variables, our first formulation of the OASP-2MF can be expressed as follows:(1)max∑i∈N(ui·yi−wi·Ti)(2)subjecttozij+zji≤yi∀i,j∈Nwithi≠j(3)zij+zji≥yi+yj−1∀i,j∈Nwithi<j(4)Ci+pj2+(zij−1)·M≤Cj∀i,j∈Nwithi≠j(5)∑i∈N∖{j}pi1·zij+(pj1+pj2)·yj≤Cj∀j∈N(6)Ti≥Ci−di∀i∈N(7)yi∈{0,1}∀i∈N(8)zij∈{0,1}∀i,j∈Nwithi≠j(9)Ci,Ti≥0∀i∈N.The objective function maximizes the total net revenue. Constraint (2) guarantees that each job i ∈ N is accepted if it is processed before or after another job j ∈ N\{i}. Constraint (3) ensures that if jobs i, j ∈ N are accepted, then either job i is processed after job j or vice versa. Constraint (4) states that if job i ∈ N precedes job j ∈ N\{i}, then the completion time of job j should be no shorter than the sum of the completion time of job i and the processing time of job j on machine 2. In these constraints, M is a sufficiently large positive number. Constraint (5) implies that if job j ∈ N is accepted, then its completion time is no shorter than the sum of the processing times of the jobs which should be processed before job j on machine 1 plus the processing times of job j on machines 1 and 2. Constraint (6) captures the tardiness of each job i ∈ N. Finally, constraints (7)–(9) specify the domains of each decision variable. Observe that in an optimal solution, if a job i ∈ N is rejected thenTi=0,and so Ci≤ di.Before explaining the second formulation, we make some comments about constraint (4). First, it is not difficult to see that constraint (4) implies thatzij+zji≤1for all i, j ∈ N with i ≠ j. This is because, for i ≠ j, if jobs i, j ∈ N are accepted, then either Ci< Cjor Cj< Ci(note thatpj2∈Z+). This implies that the formulation is valid even without constraint (2). However, we include constraint (2) because they can be interpreted as a simple valid inequality for the model and our computational results show that this can improve the run time of CPLEX. Secondly, we can safely setM=∑i=1n(pi1+pi2). Later, we show that it might be possible to find an even smaller positive value for M.Finally, constraint (4) plays an important role in enforcing a transitive relationship between the decision variables zij. Let a, b, c ∈ N be three jobs such that a ≠ b ≠ c. It is easy to see that ifzab=1andzbc=1,then zacmust also be equal to one. To enforce such a relationship between binary decision variables, researchers usually add some inequalities to their models. For instance, Dyer and Wolsey (1990); Nemhauser and Savelsbergh (1992); Unlu and Mason (2010), and Reisi-Nafchi and Moslehi (2015) added the constraintzab+zbc+zca≤2to their models while some other researchers, such as Chudak and Hochbaum (1999), added the constraintzab≤zac+zcbto their models in order to ensure transitivity of their decision variables. Proposition 4 shows that we do not need to add any of the above logical constraints to NF1 because constraints (3) and (4) imply the existence of a transitive relationship between our decision variables.Proposition 4Constraints(3)and(4)imply the existence of a transitive relationship between the decision variables zij where i, j ∈ N and i ≠ j.Let a, b, c ∈ N be three accepted jobs such that a ≠ b ≠ c. Suppose thatzab=1andzbc=1. We prove zacmust be exactly one.It is easy to see from constraint (4) that sincepj2∈Z+(that is,pj2≥1) andzab=zbc=1,we must haveCa+1≤CbandCb+1≤Cc. Consequently,Ca+2≤Cc. Since a and c are accepted jobs, constraints (3) and (4), or equivalently constraints (2) and (3), imply that exactly one of zacand zcamust be equal to one.Suppose that zcais equal to one. Constraint (4) implies thatCc+1≤Cawhich gives a contradiction. Therefore, zacmust be equal to one.□New Formulation 2 (NF2): In order to describe the model, we first define some new sets of decision variables. LetQ:={1,2,…,n}be the set of positions (in the sequencing list) to which jobs can be assigned. For each job i ∈ N and for each position k ∈ Q, we define a binary decision variable xikwhich indicates whether the job is assigned to that position, that is,xik:={1Ifjobiisacceptedandassignedtopositionk,0Otherwise.For each position k ∈ Q, we define a continuous decision variableCk′which stores the completion time of the job allocated to position k. Using these decision variables, our second formulation of the OASP-2MF can be expressed as follows:(1)max∑i∈N(ui·yi−wi·Ti)(10)subjectto∑k∈Qxik=yi∀i∈N(11)∑i∈Nxik≤1∀k∈Q(12)Ck+1′≥Ck′+∑i∈Npi2·xi,k+1∀k∈Q∖{n}(13)Ck′≥∑i∈N∑k′∈Q,k′≤kpi1·xik′+∑i∈Npi2·xik∀k∈Q(14)Ti≥Ck′−di+M·(xik−1)∀i∈Nandk∈Q(15)yi∈{0,1}∀i∈N(16)xik∈{0,1}∀i∈Nandk∈Q(17)Ck′≥0∀k∈Q(18)Ti≥0∀i∈N.The objective function in this formulation also maximizes the total net revenue. Constraint (10) ensures that each accepted job is allocated to exactly one position (evidently, the rejected jobs will not be allocated to any position). Constraint (11) states that each position cannot be assigned to more than one job. Constraint (12) guarantees that the job in the (k+1)th position cannot be completed in a time any shorter than the sum of the time it takes to complete the job in position k and the processing time for the (k+1)th job on machine 2. Constraint (13) implies that the job allocated to position k cannot be completed in a shorter time than the sum of the processing times on machine 1 of the jobs in the earlier positions added to the processing times for the job in position k on machines 1 and 2. Constraint (14) captures the tardiness of each job i ∈ N. Note that we can safely setM=∑i=1n(pi1+pi2)in these constraints. Finally, constraints (15)–(18) define the domains of each decision variable. Observe that in an optimal solution, if a job i ∈ N is rejected thenTi=0. Also, for position k ∈ Q,Ck′≥Ck′′for allk′∈{1,…,k−1}.It should be mentioned that in NF2, constraint (10) can be written as ∑k ∈ Qxik≤ 1 and hence the binary (auxiliary) variable yifor i ∈ N can be removed from the model (we can substitute ∑k ∈ Qxikfor yiin the objective function). However, we have observed that the presence of these variables increases the efficiency of our algorithm during our computational experiments and so we have chosen to include these variables in the expression of our model. One reason for this increased efficiency could be the new branching strategy which is defined by introducing the new (auxiliary) variables yifor i ∈ N that influence the value of ∑k ∈ Qxikin the B&B search tree.In NF2, empty positions are allowed to appear between two consecutive jobs. This means that the formulation is symmetric. An integer linear program is symmetric if its variables can be permuted without changing the structure of the problem (Margot, 2010). In Section 4.3 we break the symmetry of NF2 by using a set of valid inequalities.Next, we compare NF1 and NF2 with PF1 and PF2 in terms of size complexity. Interested readers may refer to Appendix to see PF1 and PF2 in detail. Note that PF2 is a time-indexed formulation, and so the number of binary variables and constraints of this formulation is highly dependent on the processing times. As a consequence, to evaluate the size complexity of PF2, we should know the number of breakpoints in time defined for each job i ∈ N, because that indicates how many time-indexed variables exist in the model. In PF2, for each job i ∈ N, the number of breakpoints in time for job i ∈ N isΓ−pi1−pi2+1,where the parameter Γ is an upper bound for the number of breakpoints. Suppose that the processing times are generated randomly and independently from the discrete uniform distribution on the interval [1, L] whereL∈Z+. IfN¯:={i∈N:pi1≥pi2}be the set of jobs whose processing times on machine 1 are not less than their processing times on machine 2, then Γ is defined byΓ=∑i∈N¯pi1+∑i∈N∖N¯pi2+L.Proposition 5Ifpi1,pi2∈Z+for i ∈ N are generated randomly and independently from the discrete uniform distribution on [1, L], then the expected value E[Γ] of Γ is given byE[Γ]=n(4L−1)(L+1)6L+L.IfXi:=max(pi1,pi2),then we certainly haveΓ=∑i∈NXi+L. Consequently, because the processing times are generated randomly and independently,E[Γ]=E[∑i∈NXi+L]=nE[X]+L.LetFXi(x)be the cumulative distribution function of Xifor each i ∈ N, where x ∈ [1, L] is an integer. ThenFXi(x)=Pr(Xi≤x)=Pr(pi1≤x)·Pr(pi2≤x)=(xL)2.So,Pr(Xi=x)=FXi(x)−FXi(x−1)=2x−1L2.Therefore,E[X]=E[Xi]=∑x=1Lx·Pr(Xi=x)=(4L−1)(L+1)6Land our result follows.□Table 1summarizes the Number of Binary Variables (NBVs), the Number of Continuous Variables (NCVs), the Number of Constraints (NCs) and the Number of Disjunctive Constraints (NDCs) required by each model to formulate an OASP-2MF with n jobs.As can be seen from Table 1, the NBVs and NDCs of the PF1, NF1 and NF2 are bounded by O(n2). However, the NBVs and NDCs of PF2 are bounded by O(nΓ) and O(n2Γ), respectively. Moreover, for NF1 and NF2, the NCs is bounded by O(n2). By contrast, the NCs is bounded by O(n3) and O(n2Γ) for the PF1 and PF2, respectively. The NCVs is linearly bounded in the number of jobs for all formulations.Tables 2–4give numerical values which show the differences between the formulations for different problem sizes when processing times are generated randomly from the intervals [1, 10] and [1, 100]. Note that the value of Γ is problem dependent and we cannot compute it without knowing the values of the processing times. However, Proposition 5 allows the expected value of Γ to be computed if the processing times are generated randomly from a discrete uniform distribution. So we have used E[Γ] instead of Γ in the tables. It is evident from the tables that the numbers of variables and constraints for NF1 and NF2 are significantly smaller than the numbers of variables and constraints for PF1 and, especially, PF2. Note that PF2 is a time-indexed formulation.In this section, we provide some techniques which can possibly make NF1 and NF2 stronger and reduce their size complexity dramatically. Some of these techniques can be applied to only one of the formulations, but the others can be used with both formulations.LP relaxation of a MILP formulation with disjunctive sets may be improved significantly by moderating its big-M parameters. In NF1 and NF2, constraints (4) and (14) contain big-M parameters. As was discussed previously, one simple way to moderate the big-M parameters for these formulations is to setM=∑i=1n(pi1+pi2)in both formulations. However, this is not the smallest trivial value that can be assigned to M. Proposition 2 allows us to compute smaller values for M. In NF1, we can useM^ij:=⌊CiU⌋+pj2where i, j ∈ N and i ≠ j to replace M in constraint (4) and obtain the new constraints:Ci+pj2+(zij−1)·M^ij≤Cj∀i,j∈Nandi≠j.It is easy to see thatM^ijis a suitable value for M because ifzij=0,the inequalityCi−⌊CiU⌋≤Cjholds. Note that we use⌊CiU⌋instead ofCiUbecause we have assumed that all parameters in the problem are positive integers. So, the completion time of each accepted job should also be a positive integer. In NF2, we can substituteMˇi:=maxj∈N⌊CjU⌋−diwhere i ∈ N for M in constraint (14). This yields the new constraints:Ti≥Ck′−di+Mˇi·(xik−1)∀i∈Nandk∈Q.It is easy to see thatMˇiis a suitable value to replace M because ifxik=0,the inequalityCk′−maxj∈N⌊CjU⌋≤Tiholds.To illustrate the differences between M,M^ijandMˇi,we compare their expected values, i.e., respectivelyE(M^ij),E(Mˇi),and E(M), in an example with n jobs. We assume that due dates are generated randomly from a uniform distribution on the interval [2L, nL], but that all the other parameters are generated uniformly from the interval [1, L]. It is not hard to see thatE(uiwi)≈ln(L),soE(M^ij)≈L2(n+3)+ln(L). Moreover, becausemaxj∈N⌊CjU⌋≤L(n+1),we see thatE(Mˇi)≤12nL. However,E(M)=n(L+1)which is much larger than both of the calculated values forE(M^ij)andE(Mˇi).Knapsack constraints have long been studied in operations research (see, for instance, Atamtürk & Savelsbergh, 2005). If modern commercial MIP solvers identify these inequalities in the model, they can possibly make the formulation stronger by generating some additional valid inequalities such as cover cuts (see, for instance, Avella, Boccia, & Vasilyev, 2012; Kaparis & Letchford, 2010). To exploit this ability of commercial solvers, we propose to add the following two knapsack constraints to NF1:(19)∑i∈N∖{j}pi1·zij≤(⌊CjU⌋−pj1−pj2)·yj∀j∈N.(20)∑i∈N∖{j}pi2·zij≤(⌊CjU⌋−p[1]1−pj2)·yj∀j∈N.Note that the validity of these knapsack constraints for NF1 is ensured by Propositions 2 and 3. In other words, we may deduce from Propositions 2 and 3 that ifyj=1where j ∈ N, thenmax(∑i∈N∖{j}pi1·zij+pj1+pj2,∑i∈N∖{j}pi2·zij+p[1]1+pj2)≤⌊CjU⌋.This inequality is valid because the left hand side of each inequality gives a lower bound for the completion time.To see how cover cuts can be generated by commercial solvers, consider constraint (19) and job j ∈ N. A set Kj⊆N\{j} is called a cover if∑i∈Kjpi1>(⌊CjU⌋−pj1−pj2).For any cover Kj, the cover cut∑i∈Kjzij≤|Kj|−1is a valid inequality for NF1 and can be added to the formulation.Proposition 2 also allows us to add some valid bounding inequalities for the completion time and tardiness of each job i ∈ N to the formulation of NF1:(21)Ti≤⌊CiU⌋−di∀i∈N(22)Ci≤⌊CiU⌋∀i∈N.Adding bounding inequalities is useful because they allow commercial MIP solvers to eliminate some of the variables in their preprocessing phase. For instance, in inequality (21), we may place bounds on the tardiness values for each job. As a consequence, if⌊CiU⌋=di,commercial solvers can set Tiequal to zero and eliminate this variable from the mathematical formulation.We first discuss some valid inequalities which were introduced by Nobibon and Leus (2011) to solve the OASP in a single-machine environment, namely(23)∑i=1nxi,k+1≤∑i=1nxik∀k∈Q∖{n}.These inequalities are useful because they break the symmetry in the formulation by removing any empty positions that appear between two consecutive jobs. In other words, these constraints ensure that positionk+1is empty if position k is empty. Interested readers may refer to Sherali and Smith (2001) for the application of such hierarchical constraints in breaking the symmetry.Valid inequality (23) can be implemented differently by introducing binary auxiliary variables vkfor all k ∈ Q and adding the following constraints:(24)∑i=1nxik=vk∀k∈Q(25)vk+1≤vk∀k∈Q∖{n}.As will be shown in Section 5, this implementation results in a significant improvement in the runtime for two main reasons. First, the number of nonzero elements in the coefficient matrix of the constraints is sharply reduced by2n2−5n+2. This is because constraint (25) has simpler representation in comparison with constraint (23). Moreover, the addition of constraint (24) to the formulation makes constraint (11) redundant and allows their removal. Second, the introduction of the auxiliary variables vkfor all k ∈ Q provides a new type of branching strategy which influences the value of∑i=1nxikin the B&B search tree. More precisely, by introducing an auxiliary variable vk, we can branch on the position k, and decide whether it is empty or not. It is evident from constraints (24) and (25) that branching on position k can be effective since ifvk=0thenxik′=0for eachi∈{1,…,n}andk′∈{k,…,n}.Note that introducing auxiliary variables is quite common, both in the contexts of constraint programming and of integer programming, because they can guide branching schemes. However, as we have shown, they can also be useful in decreasing the number of nonzero elements in the matrix of coefficients.Another type of valid inequalities which can be added to NF2 is bounding inequalities such as(26)Ti≤⌊CiU⌋−di∀i∈N(27)Ck′≤∑i∈N⌊CiU⌋·xik+CmaxU·(1−vk)∀k∈QwhereCmaxU:=maxi∈N⌊CiU⌋. These inequalities are similar to the ones we added to NF1. Evidently, for a position k, inequality (27) might be active only if a job is assigned to position k, and it implies that the completion time of the job allocated to position k cannot exceed its upper bound. Note that we can also add similar knapsack inequalities to the ones proposed for NF1 to NF2 as follows:∑i∈N∑q∈Q,q≤kpi1·xiq+∑i∈Npi2·xik≤∑i∈N⌊CiU⌋·xik+CmaxU·(1−vk)∀k∈Q∑i∈N∑q∈Q,q≤kpi2·xiq+∑i∈Npi1·xi1≤∑i∈N⌊CiU⌋·xik+CmaxU·(1−vk)∀k∈Q.However, these knapsack inequalities do not improve the runtime of NF2 when used in conjunction with constraint (27). This is mainly because constraint (27) is stronger than the knapsack inequalities. Both the knapsack constraints and constraint (27) have the same right hand sides, but the left hand sides of the knapsack inequalities give a lower bound forCk′(see Proposition 3).Note that constraint (27) imposes a relationship between the binary variables that are used in the knapsack inequalities (see the right hand sides) for NF2. This is not the case for NF1, and so there is no relationship between the knapsack inequalities for NF1 and constraint (22). Thus, knapsack inequalities can improve efficiency for NF1, even in the presence of the bounding constraint (22).We will now describe two preprocessing techniques which can be applied to NF2. We also describe how these techniques can be used in practice. The development of these techniques was based on the fact that we are able to compute a lower bound, denoted byCiqL,for the completion time of each job i ∈ N if it is located in position q ∈ Q. Consequently, if the obtained lower boundCiqL≥CiU,we can safely setxiq=0. Since we have assumed that all parameters for the problem are positive integers, we can use⌈CiqL⌉≥CiUinstead ofCiqL≥CiUas a condition to setxiq=0. In other words, to reduce the size of the problem, if⌈CiqL⌉≥CiU,then the variable xiqshould not be generated for NF2. Note that in general, we cannot use⌈CiqL⌉≥⌊CiU⌋instead of⌈CiqL⌉≥CiUas a condition to setxiq=0.Suppose thatCiU∈Z. If⌈CiqL⌉=CiU,then we can safely setxiq=0. If job i is completed at timeCiU,then its net revenue is zero, i.e.,πi=0. Therefore, settingxiq=0does not make the objective value worse. However, ifCiU∈R∖Zand⌈CiqL⌉=⌊CiU⌋,we cannot setxiq=0. This is because if the job is completed at time⌊CiU⌋,then its net revenue is strictly positive, πi> 0. Therefore, settingxiq=0may cut off global optimal solutions of the OASP-2MF.Next, we introduce some notation to facilitate the presentation and discussion of our preprocessing techniques. LetAi:={q∈Q:⌈CiqL⌉≥CiU}be the set of impermissible positions for job i ∈ N. Similarly, we denote the set of impermissible jobs for position q ∈ Q byBq:={i∈N:⌈CiqL⌉≥CiU}. Evidently, before beginning our preprocessing procedure, these sets are empty. However, in each iteration of the preprocessing procedure, we update them. Observe that the elements of B1 cannot be assigned to any position. As a consequence, they should be rejected straight away.The First Preprocessing Technique (PRE1): By Proposition 3, regardless of which job is allocated to position q ∈ Q, we are able to compute a lower bound, denoted byCqL,for its completion time. This can be done simply by computingCqL=max{∑k=1qp[k]1+p[q]2,∑k=1qp[k]2+p[1]1}.Consequently, we can setCiqL=CqLfor all i ∈ N and update the elements of sets Aiand Bq.It is easy to see that a better lower bound may be computed if we computeCiqLdirectly instead ofCqL. This can be done simply by fixing i ∈ N to position q ∈ Q. Let{p˜[1]1,…,p˜[n−1]1}:={p[1]1,…,p[n]1}∖{pi1}and{p˜[1]2,…,p˜[n−1]2}:={p[1]2,…,p[n]2}∖{pi2}be the sets of sorted preprocessing times (from small to large) when job i ∈ N is eliminated. The valueCiqLcan be computed as follows:CiqL=pi2+max{∑k=1q−1p˜[k]1+pi1,∑k=1q−1p˜[k]2+p[1]1}.Because computingCiqLprovides us with a better lower bound thanCqL,we use it in this paper. Note that the main advantage of using processing technique PRE1 is its speed because its implementation does not require any significant computational effort. We later show in Section 5 that PRE1 works well and its quality is just a little bit worse than the preprocessing technique that is introduced next.The Second Preprocessing Technique (PRE2): Another way of computingCqL,regardless of which job is allocated to position q ∈ Q is to modify NF2. LetHq:={1,…,q}be the set of positions in Q, up to and including q ∈ Q. To computeCqL,we assume that for all k ∈ Hq\{q},CkLhas been previously computed and that the sets Aiand Bkhave been updated at each step. The lower boundCqLcan be obtained by solving the following optimization problem:(28)CqL=minCq′(29)subjectto∑i∈N∖Bkxik=1∀k∈Hq(30)∑k∈Hq∖Aixik≤1∀i∈N∖B1(31)Ck+1′≥Ck′+∑i∈N∖Bk+1pi2·xi,k+1∀k∈Hq∖{q}(32)Ck′≥∑i∈N∖B1∑k′∈Hq∖Ai,k′≤kpi1·xik′+∑i∈N∖Bkpi2·xik∀k∈Hq(33)Ck′≤∑i∈N∖Bk⌊CiU⌋·xik∀k∈Hq(34)xik∈{0,1}∀i∈N∖B1andk∈Hq(35)Ck′≥0∀k∈Hq.The objective is to minimizeCq′,the completion time of the job which should be assigned to position q ∈ Q. Constraint (29) ensures that all positions are filled. Constraint (30) guarantees that each job is allocated to at most one position. Constraints (31) and (32) are equivalent to constraints (12) and (13) in NF2, respectively. Constraint (33) ensures that the completion time of the job allocated to a position is not more than its upper bound. Finally, constraints (34) and (35) determine the domains of the variables.Note that the presented optimization problem minimizes the make span of all allocated jobs in Hq. Therefore, the objective value gives a lower bound on the completion time of the job that has been allocated to position q ∈ Q.As was discussed for PRE1, after computingCqL,we can setCiqL=CqLfor all i ∈ N and update the elements of sets Aiand Bq. It is easy to see that, if the optimization problem is infeasible forCqL,then we can setBk={1,…,n}for allk∈{q,q+1,…,n}and add the elements of the set{q,q+1,…,n}to Aifor all i ∈ N.Observe that the number of optimization problems that need to be solved is at most n. Unfortunately, this is too time consuming because each of these problems is, itself, a MILP. Consequently, to speed up the procedure, we solve the LP-relaxation of these optimization problems in this paper.Note that, in a similar way to the one discussed for PRE1, we can computeCiqLdirectly rather thanCqL. This can be done simply by adding the constraintxiq=1to the optimization problem. However, in this situation, the number of optimization problems that would then need to be solved is bounded above by n2. Unfortunately, this is not a practical approach because it is too time consuming even if we solve only the LP-relaxations.An example: In order to illustrate how the preprocessing techniques work, we consider a simple example which was introduced by Wang et al. (2013b). The parameters for the problem are given in Table 5. The upper bound on the completion time of each job is given in the last row of the table.The output of algorithms PRE1 and PRE2 for the given example is shown in Table 6where column EV( percent) gives the percentage of eliminated xikvariables, i.e.,EV=∑i∈N|Ai|n2×100. As can be seen from the table, both preprocessing techniques have roughly the same effect. Technique PRE1 could remove 44 percent and PRE2 could omit 40 percent of variables. The difference between PRE1 and PRE2 is only because of Position 2. Here PRE1 could recognize that Job 3 was unable to be allocated to it, but PRE2 could not.Note that becauseCiqLis directly computed in PRE1, rather than computingCqLas in PRE2, PRE1 could eliminate more variables in this particular example. However, as we will show in our later computational experiments, PRE2 performs better for larger size instances because it considers processing times on both machines simultaneously (in the optimization problem) to compute a lower bound.To evaluate the performance of the proposed formulations and improvement techniques, we conducted an extensive computational study. We used the C++ programming language to code the formulations and preprocessing techniques and used CPLEX 12.4 to solve the formulations. All the experiments were run on a computer with a single-core 2.5 gigahertz Intel processor and 4.0 gigabyte RAM.We generated 7 classes of instances denoted by CLn (where n is the number of jobs) including CL10, CL20, CL40, CL60, CL80, CL100 and CL150. The largest class was only used to compare the performance of the preprocessing techniques for NF2. Each class contained 9 subclasses and each subclass had 10 randomly generated instances. Subclasses were defined in such a way that they contained instances with different characteristics. Each subclass was characterized by parameters τ (average tardiness factor) and R (relative range factor) which controlled the due dates. Each of these parameters could take values from the set {0.3, 0.6, 0.9}. Processing times and delay penalties were generated from a discrete uniform distribution on the interval [1, 10]. Moreover, revenues were drawn from a log-normal distribution with mean 0 and standard deviation 1 (all numbers were rounded up to the smallest following integer). The due date for job i ∈ N was generated randomly from a discrete uniform distribution on the interval[max{P(1−τ−R/2),pi1+pi2},max{P(1−τ+R/2),pi1+pi2}].whereP=∑i=1npi1+p[1]2. As is mentioned by Nobibon and Leus (2011) and Wang et al. (2013b), generating due dates from this interval guarantees that no jobs are consistently tardy.To show the effectiveness of the proposed enhancements, we added them to the formulations in stages. We considered three different levels of improvements for NF1:•NF1I: The original formulation of NF1;NF1II: NF1 when big-M coefficients were moderated;NF1III: NF1IIwith all valid inequalities incorporated.Six different levels of improvements were considered for NF2:•NF2I: The original formulation (NF2).NF2II: NF2 with valid inequality (23).NF2III: NF2 with valid inequalities (24) and (25).NF2IV: NF2IIIwhen big-M coefficients were moderated.NF2V: NF2IVwith preprocessing and valid inequality (26).NF2VI: NF2Vwith valid inequality (27).To quantify the performance of each level of the improved formulation, we report the average computational time in seconds (Avg.Time). Note that we imposed a time limit of 1800 seconds for CPLEX. For cases in which some instances could not be solved to optimality within the time limit, the average optimality gap (Avg.Gap) is reported. Where suitable, the average objective value of LP-relaxation (Avg.LP Obj) and the average number of searched nodes (Avg.#Node) are also reported. To demonstrate the performance of the preprocessing techniques, we report the average percentage of eliminated xikvariables (Avg.EV) as well as the average computational time in seconds.We now compare NF1Iand NF2Iwith PF1 and PF2 to show that the new formulations perform better than the previous ones even without using any enhancements. To do the comparison, we only use class CL10. This is because PF1 and PF2 perform poorly on larger classes. For instance, we generated some instances with 15 jobs, but PF1 and PF2 were unable to solve most of them within the time limit. Table 7reports the average solution times and LP-relaxation values of all formulations, to enable their comparison. Using the data in this table, we make the following comments and observations.(1)Instances withτ=0.3are harder to solve than others for PF1, PF2 and NF1I. One reason for this is that when τ is smaller, fewer jobs are expected to be rejected.NF1Ihas the best LP-relaxation values. Evidently, the LP-relaxation value of NF1Igets better as τ increases. Note that PF2 is a time-indexed formulation. Among the different formulations of scheduling problems presented in the literature, time-indexed formulations are well known for their high quality LP-relaxation objective values (see, for instance, van den Akker, Hurkens, & Savelsbergh, 2000). However, PF2 is around 16.1 percent weaker than NF1Ion average. This is mainly because PF2 contains more disjunctive constraints (see Section 3).There is a significant difference in solution times between the new and the previous formulations. Of course, this is not surprising since the number of constraints and binary variables associated with the new formulations is far smaller. Because NF1Iis stronger than the other formulations, it could solve all instances more quickly, taking less than one second on average in this experiment. It can be seen that, on average, NF1Iis around 7, 70 and 128 times faster than NF2I, PF1, and PF2, respectively.Note that we only tested the formulations with problem instances in which the processing times of the jobs were drawn from [1,10]. However, it is worth mentioning that by increasing the processing times, time-indexed formulation performance is expected to get even worse. The reason for this is that the numbers of variables and constraints in the time-indexed formulation are dependent on the processing times. However, this does not have a considerable effect on the performance of the other formulations. Note that because the due dates were generated in terms of processing times, the parameter characteristics did not change dramatically for the other formulations when the processing times increased. This phenomenon was also observed by Wang et al. (2013b). They mentioned that PF1 is robust with respect to the distribution range of the processing times, but that is not the case for PF2 (which is a time-indexed formulation), meaning that it can only solve instances with fewer jobs as processing times increase. They reported that when processing times are generated from the interval [1,100], PF2 can only solve problem instances with up to six jobs within one hour.We now compare NF1I, NF1II, and NF1IIIon class CL20 to show the importance of the proposed enhancements. The results are reported in Table 8. Based on the data in this table, we make some observations and comments.(1)All formulations display similar behavior for different values of τ and R. In other words, instances with larger values of τ are more easily solved by NF1. Note that τ affects the mean of the due dates, but R mainly affects their variance. So, when τ increases, it is expected that more orders will be rejected. Consequently, the NF1 formulation shows better performance for those instances. Evidently, whenτ=0.3,the instances increase in ease of solution as R increases. However, the opposite occurs whenτ=0.9. Following this pattern, the instances with a medium value of R (e.g.R=0.6) are easier to be solved whenτ=0.6.There is no significant difference between the runtimes of NF1Iand NF1IIon average. So, moderating big-M values is not very effective for NF1.NF1IIIoutperforms NF1Iand NF1IIin terms of runtime and the average gap in the time limit. It should be mentioned that 15 out of the 90 instances could not be solved to optimality by NF1Iand NF1II. However, using valid inequalities reduced the number of unsolved problems from 15 to 11.Before comparing different variants of NF2, we discuss the performance of preprocessing methods PRE1 and PRE2 on the CL20, CL80 and CL150 instances. The experimental results for these cases are reported in Table 9. From the table, we make some observations and comments.(1)In general, for both techniques, the average number of eliminated variables is larger when τ is larger. That is because when τ increases, the time window of the due dates shifts forward. As a result, more orders will be rejected.In general, for both techniques, whenτ=0.3,the average EV becomes larger as R increases. However, whenτ=0.9,it is the other way around.The average computational time of PRE2 is smaller for larger τ-values. This is again a consequence of the fact that more orders will be rejected when τ increases. So, a smaller number of optimization problems will need to be solved for PRE2. Note that it is not surprising that the average computational time of PRE2 is increased for larger classes of instances because more optimization problems need to be solved.As the number of jobs increases, PRE2 performs better than PRE1 in terms of eliminated variables. In class CL20, the average EVs for both techniques is 47.4 percent. However, for class CL150, PRE2 is around 4 percent better than PRE1 on average. Of course this is costly in terms of efficiency and, on average, we need to spend around 270 seconds of valuable computational time eliminating variables.For the remainder of this paper, we only use PRE2 in our computational experiments because it eliminates more variables on average. However, PRE1 is faster, so, it may be useful in heuristic or meta-heuristic approaches.We first compare NF2I, NF2II, NF2IIIand NF2IVon CL20 instances. The results are given in Table 10. We do not show the values of the average optimality gap for NF2II, NF2IIIand NF2IVsince they could solve all the instances to optimality. Based on the data in the table, we make the following observations and comments.(1)The average optimality gap and runtime of NF2Iincrease as parameter τ increases. This is completely the opposite of what we observed for NF1. So, unlike the NF1, larger τ-values make problem instances harder to solve using NF2.The main issue with NF2Iis symmetry. Consequently, many nodes in the B&B tree needed to be investigated by CPLEX. The result was that 38 out of 90 instances could not be solved to optimality by NF2Iwithin the time limit. However, after breaking symmetry in NF2IIby adding constraint (23) to NF2, the number of investigated nodes was decreased by about 96.5 percent. This resulted in a significant improvement in the runtime so that we were able to solve all instances to optimality in less than one minute, on average.The number of nodes which are solved by NF2IIIis around 60 percent more than the number solved by NF2II. However, its runtime is around half of the runtime of NF2Ion average. As mentioned previously, when we include constraints (24) and (25) in place of constraint (23), constraint (11) should be removed from the NF2. Moreover, constraint (25) have much simpler structure in comparison with constraint (23). Consequently, the number nonzero elements in the matrix of coefficients decreases sharply. So, it is not surprising that, on average, NF2IIexplores 369 nodes per second, but NF2IIIexplores 1080 nodes per second;By contrast to our observation for NF1, moderating big-M values has a significant impact on NF2. The number of explored nodes and runtime are decreased by about 87.4 percent and 68.2 percent, on average, respectively, when NF2IVis implemented rather than NF2III.Before showing that NF2 can be improved even further, we now compare NF1 and NF2. From the data in Tables 8 and 10, it is evident that if we compare the basic models of NF1 and NF2, i.e., NF1Iand NF2I, then NF1 outperforms NF2. On average, NF1Isolves all instances in around 385.4 seconds (with an average gap of 1.295 percent), but NF2Isolves them in about 973.6 seconds (with an average gap of 11.201 percent). However, the situation is completely different for the improved version of these formulations, i.e., NF1IIIand NF2IV. On average, NF1IIIsolves all instances in about 346.0 seconds (with an average gap of 1.090 percent), but NF2IVsolves them in around 9.8 seconds (with an average gap of 0.000 percent). As a result, the difference between NF2IVand NF1IIIis significant enough to say that the improved NF2 is a better formulation.Next we show that NF2 can be improved even further by using preprocessing and adding valid bounding inequalities. Table 11shows the data for the implementation of NF2IV, NF2Vand NF2VIon CL40. NF2IVcould solve all instances of class CL20 in less than 10 seconds on average. However, when we doubled the number of jobs, its runtime increased by a factor of more than 62, on average, and it could not solve all instances to optimality within the time limit. Results for NF2Vshowed that preprocessing can reduce the runtime by a factor of 6 on average. It can also decrease the average optimality gap from 0.949 percent to 0.099 percent.Note that, as we discussed previously, preprocessing of commercial solvers can eliminate some of the variables if we add valid inequality (26). Therefore, the role of these valid inequalities is the same as for PRE2, meaning that they are developed for eliminating variables. That is why we add PRE2 and valid inequality (26) at the same time to NF2IVto obtain NF2Vfor the model. The results can be improved even further by using NF2VI. Evidently, when compared with NF2V, the runtime is improved by around 26.9 percent and the optimality gap is decreased from 0.099 percent to 0.056 percent, on average, by using NF2VI.In Table 12, we report the results of the implementation of the best formulation , i.e., NF2VI, on different classes of instances. It can be seen that instances of small size are solved easily without any difficulty, and larger instances are solved with a reasonable optimality gap within a half-hour time limit by using this model. On average, instances of class CL100 are solved with a 1.971 percent optimality gap within the time limit. As is evident from the data in the table, the subclass withτ=0.6andR=0.9contains the most difficult instances which have been solved. Whenτ=0.6andR=0.9,none of the instances of classes CL80 and CL100 could be solved to optimality within the time limit. However, the average optimality gap never exceeded 5.2 percent even in these instances.It is also interesting to compare the results of NF2VIand the B&B proposed by Wang et al. (2013b). Wang et al. (2013b) showed that their proposed B&B algorithm can solve instances with 20 jobs in 253.5 seconds, on average. However, CPLEX can use NF2VIto solve such instances easily in no more than 1.3 seconds with a computer that has similar characteristics to the one used by Wang et al. (2013b). As a result, we can almost improve the runtime by a factor of 200 for instances with n=20.

@&#CONCLUSIONS@&#
