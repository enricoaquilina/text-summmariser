@&#MAIN-TITLE@&#
Approach for developing coordinated rubrics to convey quality criteria in MCAD training

@&#HIGHLIGHTS@&#
Six main quality dimensions in CAD models for inexperienced CAD trainees.Approach to convey quality strategies to CAD trainees through rubrics.Assertions Map that adapts the rubrics to CAD trainee progress.Lessons learned on rubrics (separate, accurate timing, support with lecture notes).

@&#KEYPHRASES@&#
CAD model,Quality,Design intent,Rubric,

@&#ABSTRACT@&#
This paper describes an approach to convey quality-oriented strategies to CAD trainees by embedding quality criteria into rubrics so as to force CAD trainees to understand them early in their instruction. To this end, the paper analyzes how CAD quality criteria can be organized around quality dimensions and embedded into rubrics in order to enforce quality modeling during the CAD training of novice product designers. Hence, the ambit of this study is training with history-based parametric feature-based MCAD systems, although the general conclusions are believed to be adaptable to other CAD training scenarios. Furthermore, it introduces an approach based on progressive refinement, which results in an assertions map that indicates quality dimensions vs. sequence of tasks for CAD models. The map illustrates how the expand–contract strategy adapts the rubrics to CAD trainee progress and assists them in comprehending the different dimensions of the rubrics. The paper also highlights lessons learned on the suitability of using separate rubrics for different tasks, the need of accurately timing the expand–contract process, and on the convenience of supporting rubrics with suitable teaching, which must convey good practices and evaluation tools through rubrics. The experiments describe the different lessons learned and illustrate the suggested process for replicating our approach for further developing rubrics adapted to other scenarios.

@&#INTRODUCTION@&#
Currently, New Product Development (NPD) processes in leading companies are completely based on digital product representation, from 3D definition to digital manufacturing. Their implementation of collaborative/concurrent engineering is entirely centered on Product Lifecycle Management (PLM) systems that provide support for the exploitation of 3D CAD models along the NPD process. Standards relating to digital product definition data practices such as ASME Y14.41-2012 and ISO 16792:2006 are reinforcing the central role of 3D CAD models in this context.The growing importance of 3D CAD models as the central core of the NPD process has been accompanied with the corresponding development of the CAD data quality field. Standards, such as the “Strategic Automotive product data Standards Industry Group” (SASIG) Product Data Quality (PDQ) Guideline V2.1 (ISO/PAS 26183:2006) and VDA 4955/4.1, provide basic quality criteria for “product data” that is defined as any and all product data required from product conception to manufacturing. Product data includes not only computer aided design (CAD) data, but also data from computer-aided manufacturing (CAM), computer-aided engineering (CAE), and product data management (PDM), among others—and CAD-education should address all phases of the life cycle  [1]. However, current versions of these standards are oriented to provide mathematical and topological quality criteria for CAD data.Poor data quality management represents a serious hurdle to obtain all promised benefits of history-based parametric feature-based mechanical CAD systems (MCAD), provoking avoidance, mitigation, and delay costs  [2]. These concerns are especially critical regarding CAD model reuse issues. Partial redesign of an existing product is one of the typical approaches in the NPD process  [3]. Here, high quality CAD models play an important role, as reported by the Aberdeen Group in 2007  [4]. Companies showed a 30% reduction in design times for new products that were closely associated with preexisting products, making an 80% reduction in design times for products heavily dependent on reuse. However, reusing CAD models is not without problems, as the same report  [4] indicates the following obstacles to CAD model reuse:•Model modification requires expert CAD knowledge.Models are inflexible and fail after changes.Only original designer can change models successfully.These problems are not new. In 1998, Anderl and Mendgen  [5], in researching the creation of real life complex CAD models said, “If it is difficult to create a model then it is even more difficult to reuse it for variation of modification purpose”. Horwood and Kulkarni  [6] indicate that defective data can be attributed to factors such as improper modeling practices, lack of communication of methods, nonconformance to generic methodologies, neglecting the enforcement of quality standards, time pressures to complete the work, too many data translations, and lack of training and oversight.In another study, Bodein et al.  [7] identified as the main aspect axes for an efficient CAD strategy:•To reduce design time in all design phases (conceptual, preliminary or detailed),To reuse existing CAD models and geometry,To accelerate the automation of routine design tasks based on knowledge-based engineering (KBE) applications,To enhance collaboration between designers, andTo improve the general quality of CAD models.Previous research and our own experience suggest a proactive approach to embed quality concepts from the initial stages in CAD training in an explicit way. This concept is aligned with the idea by Bhavnani et al.  [8] that strategic knowledge holds the key to efficient CAD usage and that this knowledge must be explicitly taught. These authors refer to strategic knowledge as that knowledge related to identify alternate working procedures and how to choose between them in order to make the most efficient use of the CAD tool.We have successfully tested rubrics as a valid tool for disclosing and conveying those quality criteria that can be efficiently transmitted by way of good practices in a basic 3D CAD course for fresh designers/engineers  [9]. Additionally, we have reported  [10] that only specific rubrics are useful and varying levels of detail are required at different phases in the training period. Hence, in this paper, we study the organization of quality criteria in rubrics.We describe our approach to develop quality-oriented rubrics based on a panel of experts that progressively refine quality criteria that can be disclosed and conveyed through rubrics, and determine when and how they can be transmitted to the CAD trainees. Instead of constructing a single rubric, the approach intends to produce what we call Assertions Maps. These maps are visual representations which indicate which assertions are necessary to include in the different rubrics used to convey quality in CAD documents throughout the training period. The assertions maps display the expand–contract process, where the assertions are very detailed the first time they are introduced, and are later recursively abstracted to force CAD trainees to comprehend general quality criteria, as suggested in  [10].Hence, in this paper, we search for a suitable organization of CAD quality criteria into assertions maps, aimed at embedding them into a set of rubrics used to enforce quality during the CAD training period of novice product designers. To validate this approach, we conducted three pilot experiments. Hence, the paper also describes lessons learned about the suitability of using separate rubrics for different tasks, the need of accurately timing the expand–contract process, and on the convenience of supporting rubrics with suitable teaching, which must convey good practices and evaluation tools through rubrics.Rubrics are a common authentic assessments tool to describe student achievement  [11,12]. A rubric is a scoring tool that lists the criteria for a piece of work and articulates gradations of quality for each criterion. Rubrics can increase student performance by making explicit teachers’ expectations and by showing students how to meet these expectations. Rubrics are also useful to help students become more thoughtful judges of the quality of their own and others’ work. True assessment emphasizes the application and use of knowledge to solve complex tasks that involve contextualized problems. Here, rubrics are an invaluable tool in order to help students to understand the criteria for judgment from the beginning  [13]. All these arguments lead us to choose analytical rubrics (rubrics that decompose a work into its components that are judged/scored separately first and then combined to produce the evaluation) to both communicate and evaluate CAD quality criteria.Using rubrics to convey quality is surprisingly straightforward: quality dimensions are conveyed as rubric competences, and competences are measured through evidences, which are expressed as “assertions”.Not all quality concepts can be introduced simultaneously, however. So, the proposed approach uses assertions maps that are visual representations of sets of rubrics. Our method to determine these assertions maps is simple but efficient: a panel of experts recirculate the proposal, which is recursively refined. In other words, instead of directly testing the rubrics with students, a team of teachers/researchers iteratively polished them. The approach consists of six stages.First stage:1.Quality concepts are organized into dimensions.The team agrees on a course syllabus and the quality concepts that are embedded in it.The syllabus is organized around tasks structured in series.Assertions maps are obtained to convey the expand–contract process.One team member develops a particular task and its associated rubric.Another team member solves the task and satisfies the rubric.Both members discuss the weak and strong points of the rubric as a means to convey quality criteria and produce an improved version.One member of the team develops tests to quantitatively validate those assertions that are difficult to validate directly.Another team member uses the tests to satisfy the rubric.Both members discuss the weak and strong points of the tests and improve them.While repeating stages two and three in order to accomplish all tasks and their associated rubrics, the rubrics are compiled in the assertions map and their suitability for the expand–contract criterion is checked.Post-graduate students are asked to comment on the clarity and usability of rubrics.Collected comments are used by the experts to produce an improved version.The rubrics are finally tested with undergraduate students.A simple example illustrates the process: drawing profiles is a mandatory task for all sweep-based 3D CAD applications. Robustness is a profile’s quality that is clearly related to its constraints. Therefore, drawing a fully constrained profile is a pertinent, quality-oriented task. The rubric used to assess the task should include an asseveration such as, “The profile is fully constrained”. This asseveration should first appear as a separate entry at the beginning of the assertions map and it should be later grouped with similar asseverations (such as, “The profile does not contain duplicated lines” and “The profile does not contain segmented lines”). The aggregated asseveration should be, “Profile is robust”. Finally, all assertions should be implicitly embedded in the general dimension of model consistency.This approach is similar to the “learning to see” methodology proposed by Bhavnani et al.  [8]. But, instead of simply focusing on developing the teaching of strategies, our approach guarantees that CAD trainees are forced to note the importance of good practices, as they are explicitly asked to check whether they have accomplished them. Furthermore, we also extend the concept of “learning to do” as we develop simple tests to check the quality concept under evaluation. As an example, we detect whether the profile is suitably constrained by asking the student to edit one of its dimensions and check whether the result exactly matches a given template.Sections  3 and 4 of this paper describe the background and development of the first stage of the proposed process. Examples that illustrate the fifth and sixth stages are detailed in Section  6 of this paper, where the experiments describe the different lessons learned and illustrate the suggested process for replicating our approach for further developing rubrics adapted to other scenarios.The second, third, and fourth stages are not further explained here, as they are primarily based on applications of team member expertise. They must be solved using what Rossignac defined as education-driven research (EDR), which develop “specific tools and solutions in a specialized domain, so as to make them easy to understand and internalize”  [14].CAD education was identified by Piegl as one of the ten challenges in computer-aided design  [15]. He suggested that the internet will facilitate the re-use of CAD components, especially with the interconnection of computers and access to data bases. We foresee that models must enforce re-usability (and, in general, quality) before becoming publicly available. He also predicted the advent of on-line CAD education. We anticipate that self-evaluation tools will be required, and suggest using rubrics toward that end.According to the literature, at least three main topics must be considered in CAD education: (1) industrial view, (2) teaching of fundamental tools and knowledge, and (3) teaching of more advanced and specific topics  [16]. In this paper, we are interested in the industrial view, which includes balancing the mathematical foundations of CAD, its relation to computer science, design methodologies, and system evaluation  [17]. In this work, we are concerned with one particular design methodology: feature-based modeling.The continuing evolution in the training and educational needs of users of such CAD-systems was already highlighted by Field, who distinguished three groups of users: the majority, the expert, and the super-user  [18]. Our particular approach focuses on using rubrics to shift users from the majority group to experts, in the sense that they become competent to edit and repair CAD models and transfer information to and from different secondary views (Finite Element meshes, etc.). As expressed by Dankwort et al., CAD-education is not restricted to only teaching solid or surface modeling, but includes learning about the complete development process under the aspect of Computer Aided Product Creation  [1].Hamade and Artail distinguish four types of students (Activist, Pragmatic, Theorist, and Reflector), and describe tests to correctly place trainees in groups, arguing that this methodology increases the efficiency and cost savings of the learning process  [19,20].We also agree with Amadori et al.  [21], who state that if users are taught to use efficient strategies in the context of tasks, they will be able to recognize opportunities to use them in new scenarios. We finally note the importance of forcing CAD trainees to comprehend quality criteria early in their instruction in order to prevent the Einstellung effect  [22].To sum up, between the second and fourth stages of our approach, experts should remember that CAD education should take into account all aspects of Computer Aided Product Creation, distinguish the four types of students, and pivot around different tasks.After a detailed analysis of standards on product data quality and working procedures in the industry related to CAD data exchange agreements, three “levels of quality” can be identified  [23]. The morphological quality level is related to the geometrical and topological correctness of the CAD model, and currently it is the focus of PDQ standards such as SASIG PDQ Guideline V2.1 and VDA 4955. The syntactic quality level is linked to the proper use of modeling conventions such as naming rules for features, datum, part, assembly, drawings and layouts; layer structure and function and part/assembly parameters and attributes. The syntactic level is especially focused on the organizational aspects of the CAD model. Finally there is a third level associated to the semantic/pragmatic quality that takes into account the CAD model capability for reuse and modification. CAD users have a great variety of modeling procedures for shaping their designs. However, experience shows that certain procedures provide better solutions than others.Knowledge linked to the semantic/pragmatic quality level corresponds with the strategic knowledge, defined by Bhavnani et al.  [8] as knowledge for identifying alternate working procedures and selecting the one that most efficiently uses the CAD tool. Also, it can be identified with the procedural knowledge, as opposed to declarative knowledge by Rynne and Gaughran  [24], Johnson and Diwakaran  [25,26], and Mandorli, and Otto  [22].Making this knowledge well documented and easily accessible is very important. Many large companies have developed their own private “modeling guidelines”, where the “best practices” are recorded for improving CAD model quality. Some authors have proposed modeling methodologies to improve overall quality in CAD models. Rynne and Gaughran  [24] termed it “cognitive modeling”. The most recent contributions are the “resilient modeling strategy” by Gebhard  [27], which advocates replacing best-practices manuals with checklists and the “explicit reference modeling methodology” that proposes a modeling methodology for complex parts based on explicit references  [28]. Other modeling methodologies such as “horizontal modeling” are protected by patents  [29].A data quality dimension   [30] is defined as a set of data quality attributes that represent a single aspect or construct of data quality. This paper intends to establish the dimensions of the CAD quality space that can be introduced while training novice CAD users. This scope is limited, as this work does not intend to encompass all aspects of quality in CAD models. The selected dimensions are inspired by the properties of representation schemes proposed by Requicha  [31], where formal properties are: domain, validity, completeness, and uniqueness, while informal properties are: conciseness, ease of creation, and efficacy in the context of applications.The domain of representable entities is a prior limit of each CAD application, which excludes certain categories of shapes, but does not influence the quality of those designs that fall within the range of the application. Similarly, the ease of model creation relies on the friendliness of the user interface, which cannot be readily improved by an experienced user. Both properties should be considered only if users were allowed to select between different CAD applications in order to solve each particular task.Validity is simple, but important while training novices. Solid models are electronic documents that must be correctly saved for later use. A common mistake when beginning to use CAD software is to forget that an opened file must not be manipulated by the Operating System. Attempting to save files while they are still in exclusive use by an application usually produces invalid documents. Furthermore, mistakes made while modeling result in model trees with errors that produce non-usable files.Models are complete if they include all the product aspects that are relevant for design purposes (that is, replicate shape and size of the object). Completeness seems an obvious requirement if fail to remember that it also implies quality, as incorrect modeling flow usually produces a non-complete model.Uniqueness of the internal representation is a simple way for assessing the equality of objects. However, modern parametric feature-based systems prioritize easiness of use, allowing different construction routes to achieve the desired geometry. This easiness of use compromises quality, since as noted previously when the idea of “semantic/pragmatic” level of quality was introduced, different paths to produce the same geometry respond differently to CAD model changes or modification. In this context, consistency expresses in a more exact way than uniqueness, the quality dimension linked to the goal of allowing safe exploration of alternative solutions while maintaining valid geometries.We state that we are not interested in the efficiency of the modeling process itself (i.e. how fast or how easy the modeling task is for the designer). Instead, we are interested in the quality of the high level information conveyed through the resulting model (i.e. how well does it represent the object, how easy it is to alter, or how much design intent it conveys).Hence, we want to determine whether the actions of the CAD user (what tasks user performs during the modeling process), result in documents that convey design intent. Two similar definitions that explain what we understand here by design intent are: (a) CAD models capability of being modified and yet still being able to perform the same functions, and (b) Expected behavior of a CAD model when it is modified. Therefore, we first try to measure whether the modeling process is effective in conveying the right information about a model’s function. Additionally, we are interested in measuring whether the modeling process conveys the information in a correct manner: if the modeling process allows conveying more design intent than other modeling processes (i.e. is efficacious in conveying design intent), and if the modeling process requires less effort than others to convey the same design intent (is efficient).So, we develop a classification with the following dimensions of quality in CAD models:1.Models are valid if they can be opened by suitable applications and do not contain errors or warnings.Models are complete if they include all the product aspects that are relevant for design purposes.Consistent models should not crash as a result of editing tasks and design exploration should be easy.Concise models do not include irrelevant or repetitive information or procedures.Effective CAD models convey design intent.We can confirm and improve this classification by reviewing recent literature. Bhavnani et al.  [8] place a premium on efficiency. Rynne and Gaughran  [24] defined a set of attributes which cover the dimensions of efficiency, robustness and proper design intent.According to Diwakaran and Johnson, the ability to create and easily alter designs is one of the key proposed benefits of CAD programs, but models cannot be easily altered by other designers and engineers if these models “are difficult to understand or do not capture design intent well”  [26]. In fact, McKenney reported many years ago that many engineering analysts are spending half their time repairing poorly constructed CAD models before analysis can begin  [32]. Hence, the four main dimensions of a quality CAD model should be: ease of creation (speed), ease of alteration, ease of understanding and effective capture of design intent.However, as stated before, we understand that speed is not a quality goal. In fact, Hamade et al.  [33] concluded that if production time is an overriding criterion, then using small numbers of complex, more time-efficient features is the best choice. However, speed contradicts ease of alteration. Diwakaran and Johnson concluded that relative feature complexity (greater average number of segments per feature) was found to decrease original modeling time, but also found to decrease design reuse (through lower feature retention without change)  [26].In our view, those properties encompass capabilities belonging to different dimensions. For instance, strategies to increase feature reuse during alteration are: simpler features, the use of reference geometry, and the correct feature sequence improve the perception of the model during alteration.We note that ease of alteration is similar, but not equal, to consistency. In the latter, the emphasis is on the number of changes (the more changes that are allowed the better), while in the former, the emphasis is on the ease of common changes (simpler changes are preferred).We also note that those dimensions are not independent on each other. For instance, reducing to a minimum the amount of information (i.e. conciseness) is sometimes contradictory with easing the understanding of such information, as understanding usually improves with moderate redundancy.Such contradictory requirements become even more apparent if we consider that, according to Amadori et al.  [21], models should be flexible and robust. The wider the range of product configurations, arrangements and sizes the model can cover, the more flexible it is. The fewer the amount of errors or instability issues that changes to the geometrical model may provoke, the more robust the model. In other words, a CAD model is robust if it can be modified without failure. Also, they found that complexity is also related to flexibility and robustness, since robustness and flexibility values tend to worsen when the complexity of models increases.Assuming that CAD models are documents shared by different stakeholders during the design process, communication is also important. In order to facilitate communication, the document must follow conventions and must be clear and comprehensible (aimed at being understood at first glance)  [9].The Resilient Modeling Strategy  [27] considers that models must be editable-robust, obvious, and reusable. This strategy hypothesizes that parent–child relationships are critical in order to allow robust editing, but are sensitive to the feature sequence. The strategy argues that in order to get robust models, modeling operations should be sequenced exactly in the following way:1.Reference operations (datums).Construction operations (skeletons, or scaffolds).Core operations (additive features).Detail operations (subtractive features).Modify operations (replication operations).Quarantine operations (cosmetic operations).We agree with this sequence, but suggest training CAD users to work with true design or manufacturing features instead of simple form features. Hence, we emphasize subordinating the priority additive-before-subtractive to the more important priority of design and manufacturing features before simple form features.Finally, we understand design intent as the way to describe a model’s anticipated behavior once it undergoes alteration  [34]. While many authors have comparable definitions of design intent, they each rely on different methods in order to communicate this information to others. We do not believe that the parametric modeling software can accurately record this data. Instead, methods need to be developed so that this information can be documented and design justifications understood. Still, some simple actions can be done with current CAD applications. To develop obvious models, this approach allows for communicating design intent by renaming features and reordering the model tree so it reads like a recipe for the model.To sum up, we reformulate the dimensions of quality including some sub-dimensions, as follows:1.Valid.1.1Retrievable (can be found and can be opened).Usable (is error-free and compatible with the application).Complete.2.1Replicates shape of the object.Replicates size of the object.Consistent.3.1Robust (changes do not produce unexpected failures).Flexible (allows many changes).Concise.4.1Non-repetitive (does not contain repetitive information or operations).Non-fragmented (does not contain fragmented information or operations).High semantic (uses high-level modeling operations when available).Simple.5.1Clear.Easy to understand (an observer can easily explain the model).Maximizes compatibility with other CAD formats (“saving as” produces files retrievable and usable by other applications).Follows conventions (interpretation is non-ambiguous).Captures design intent.6.1Effective (conveys design intent).Efficacious (conveys more design intent than other modeling processes).Efficient: it is worth modeling this way to convey design intent (the same design intent could not be conveyed in a simpler way).It has been stated that rubrics must adapt to the task and tasks must be arranged in the right sequence  [9]. As a first step to cope with different tasks, a subdivision between modeling, assembling and drawing extraction was considered. We maintain this subdivision. Also, we have organized the tutorials into series, where a series is a set of exercises aimed at training one specific skill. Exercises belonging to every series are internally ordered by increasing level of difficulty, from simple repetitions of the basic skill being trained, to using this skill in novel scenarios. The external sequence of the different series is also important, because the needs and capabilities of CAD trainees evolve along the training period. To adapt to the evolution of CAD trainees, an expand–contract strategy was proposed. Hence, we will develop a sequence of tasks and sub-tasks. Quality concepts are organized around main dimensions, which in turn, are evolved into sets of criteria expressed by way of assertions. Both sequence of tasks and dimensions of quality criteria constitute the two axes of our assertions maps.The 30-hour course is aimed at teaching 3D CAD modeling fundamentals to mechanical and industrial engineers. CAD trainees had been exposed to a previous course named “Graphic Expression”, where they learned the fundamentals of descriptive geometry and standard representation of engineering drawings. They were also instructed in the use of those fundamentals to produce both hand drawn sketches and 2D CAD engineering drawings. The CAD software used in the experiment was SolidWorks®.After analyzing alternatives to configure the course syllabus  [35–39], the team proposed the following series to group the main CAD modeling skills or tasks:1.Drawing profiles.Models obtained by simple extrusion or revolution of profiles.Complex models by combination of different extrusions and revolutions.Models where the reference geometry (“datums”) is used to build the structure (“scaffold”) of the model (which typically includes oblique elements).Models including curved profiles and sweeps (spherical caps, donuts, springs, etc.).Models including replication operations and features.Models of standard parts (Screws, bolts, etc.).Detailed information on the purpose and contents of every task can be found in  [40].Assertions maps are conceived as visual representations of the rubrics where the different assertions of each rubric are displayed along one axis, while the evolution of each assertion throughout the syllabus is displayed along the other axis. The assertions maps are intended to display the expand–contract process.To quantify their criteria, Amadori et al.  [21] apply complex measurements of the design space, which are clearly not applicable while training novice designers.Rynne and Gaughran  [24] argued that creating robust sketch geometry is the most critical user issue in capturing design intent. They also defined a set of “attributes” of a CAD model, which are similar to our assertions:•Correct sketch plane selection for base feature sketch.Optimum model origin.Correct base feature.Correct part orientation.Appropriate use of symmetry planes.Simple sketch geometry.Correct sketch relations.Fully defined sketch geometry.Correct feature sequence.Parent–child feature relations.Correct feature terminations.Correct feature duplication.Correct part design intent.Part accommodates planned and unforeseen design modification without feature failure.However, these attributes are not formulated to be easily answered in the frame of a rubric.Johnson and Diwakaran  [26,41] elaborated those attributes, adding detailed descriptions and metrics, most of them binary (true/false).Working in parallel in CAD Modeling Strategies (CMS), Allsop  [39] proposed a distinction between what she called ‘Feature-based’, ‘Overarching’ and ‘Detailed’ approaches. She used Feature-based instead of the already coined cognitive modeling, whereas overarching and detailed include some procedural modeling strategies: datum-based modeling (horizontal and skeleton strategies), use of replicating operations (duplicate and symmetry strategies) and actively encouraging the user to consider alternate approaches.Departing from these attributes and the assertions included in the appendix of  [9], and applying the approach described in previous section, we acquired the assertions shown in Table 1. These assertions constitute one axis of our assertions map.We note that the CAD models assertions table is disaggregated to distinguish between robustness and flexibility of profiles and the model tree. We do not evaluate robustness and flexibility of modeling operations themselves, which, when used in their basic functionality, guarantee those properties. Obviously, more advanced users should evaluate them, as they usually push some operations to their limits.Assertions related to Dimension 6 were most difficult to determine, because they imply a certain degree of expertise. To this end, we considered negative knowledge, as described by Mandorli and Otto  [22]. They define it as knowing what not to do (i.e. “knowing how to avoid grave errors and approaches which are inefficient in certain situations”).The goal was to define assertions that describe actions that induce situations best avoided. So, we reformulated as assertions those feature deficiencies described by Mandorli and Otto  [22] which were developed as anchor concepts to evaluate CAD models in respect to missing/poor design intent.One important conclusion derived from Table 1 is that some dimensions appear to be opposed to each other. For instance, simple models result from simple and compatible operations (i.e. a negative cylindrical extrusion to produce a drilled hole) while design intent results from complex and sophisticated operations (a drilling feature). The ability to understand these trade-offs between such opposing dimensions and the ability to choose the proper procedure differentiates between novice and expert CAD users. A novice tends to interpret that simple cylindrical holes are better than drills while the expert realizes that conveying design intent is more important, and then subordinates simplicity by choosing the simplest drill available.We will use assessment strategies to introduce such subtle differences.Our assertions maps are similar to the time-line for the semester-long “Strategic Use of CAD” course by Bhavnani et al.  [8]. The main difference being that the assertions maps also convey the expand–contract process.A different rubric is defined for each series. While the six main dimensions are always maintained, the level of detail varies according to the expand–contract criterion:•The first time a new concept was evaluated, it was described by one or more detailed rubric items.In subsequent rubrics, the concept was recursively abstracted.In the end, every main dimension was directly evaluated, and rubric items were only left as a subsidiary way to allow clarifying the score when the interviewed student felt it was necessary.Results are shown in Table 2. A black background indicates that answering the assertion is mandatory. A gray background specifies that only the higher level assertion is mandatory, while the detailed assertions that complement it are optional. Finally, “X” means that the assessment cannot continue if this assertion fails. It is a “no-pass” flag.As shown in Table 2, while solving the first task, CAD trainees are taught about missing electronic documents and making mistakes that result in error messages in the model tree. Then, they are evaluated on Dimension 1, by way of four detailed assertions. At the end of the second task, they are evaluated again. But this time, some assertions are contracted (i.e. M1.1 is initially expanded into M1.1a and M1.1b2, and is later contracted back to M1.1). At the end of the third task, validity is globally evaluated; although, to assist them to fix the concept, the detailed assertions are also provided. At the end of the fourth task, only the global dimension of validity is evaluated. While assessing further tasks, validity is not given a numeric score. On the contrary, lack of validity is used as a no-passing criterion.Consistency is progressively introduced during Sessions 1–4, and is compacted starting from the fifth session. Conciseness is progressively introduced during Sessions 3–6, and is condensed starting from the seventh session.Simplicity and capture of design intent require more expertise and are more abstract concepts; hence, they require more time to be comprehended. Sessions 9–12 (which are primarily concerned with introducing drawing extraction and assemblies) are used to reinforce such dimensions. Our assumption in justifying more than seven sessions in the sequence is that since models interact with assemblies and drawings, it is not until CAD trainees attempt to produce assemblies and drawings that they realize that some subtle mistakes or bad practices are embedded in their models.We divide the six quality dimensions into three groups: (a) nearly dichotomous, (b) require internal arrangement, (c) require external trade-offs. In principle, validity and completeness are dichotomous dimensions, as they either are or are not accomplished, so they could be easily controlled through a checklist (as proposed by Gebhard  [27]). Replacing dichotomous checklists by five-point Likert scales during training periods helps teachers to stimulate the progress of their students, by introducing intermediate scoring values to visualize their progress. Once the dimensions have been understood, they are simplified as purely dichotomous.Consistency belongs to the second category, as it clearly requires an internal compromise between robustness and flexibility. It must be this way, because designers require models simultaneously robust and flexible in order to explore new solutions. An excessively robust design prevents creativity, while a too loose design behaves erratically, even when undergoing simple changes. The ability to understand and solve these trade-offs is a main skill in engineering design. Rubrics make them visible and we shall argue that explicit procedures conveyed through suitable teaching provide students with strategies to cope with.Conciseness and simplicity belong to the third category. Conciseness may be seen as a dichotomous dimension if isolated, but using the higher level modeling operations easily compromises simplicity. So, both dimensions are linked through what we can call external trade-offs. Obviously, the sixth dimension also belongs to the third category, as it interacts the same way with Dimensions 4 and 5. These mutual and contradictory interdependencies suggest longer maturing times for those three dimensions in our expand–contract map.We must highlight the fact that the individual assertions included in Table 1 cannot be understood as categorical goals, but are only simplifications. For instance, it is quite obvious to expert designers that completeness depends on future use. However, the detailed assertions shown in M2 guide novice students to produce full models (including their details). It is only when we use assertion M6.3a (Task 9 in Table 2), that we start distinguishing between “core, detail, replication and cosmetic operations”. In this stage of the training period, we suggest that detail and cosmetic operations must be introduced, explaining their advantages (i.e. they allow for more realistic models) and disadvantages (they incur too much calculation time without noticeably improving the accuracy of CAE analysis). In other words, if (1) the primary view includes a full model and (2) its model tree is ordered from core to detail, then it is always easy to obtain simplified secondary views without a loss of design intent. However, beginning students cannot cope with such a complex strategy, so they must be guided to discover it gradually, which is the intention of an assertions map.In a similar way, it is well known that under-constraining is a good strategy in several cases. Learning to under-constrain is difficult as it requires an appreciation of the process of balancing robustness and flexibility, which is why this concept evolves along the training period. Initially, novice students are taught to always constrain their models, but only after they have consolidated the habit of constraining their models can they be safely informed about the exceptions. Firstly, because novice students must know the rule before they are informed about exceptions. Secondly, since understanding the potential benefits of under-constrained models makes them also conscious of the potential dangers implied in a careless use of such a strategy. This awareness is what the expand–contract strategy tries to provide: the capability of first putting the focus on simple and isolated criteria while progressively introducing more sophisticated mutual relations and agreements between opposite conditions. For instance, the first time students are exposed to the third dimension of the rubric, they are asked to evaluate three simple and supposedly independent criteria in order to measure the robustness of profiles: M3.1a, M3.1b and M3.1c. In parallel, they are asked to evaluate the flexibility of profiles through two simple criteria: M3.2a and M3.2b. Later, they are asked to evaluate the robustness of profiles as a single item (M3.1), which requires agreement between all three previous criteria. Flexibility of profiles is condensed in a similar fashion. In other words: initially, fully constraining the profiles is mandatory, while in the second stage, fully constraining the profiles only helps to ensure that changes do not produce unexpected failures—where “unexpected failures” is a qualitative concept requiring agreement between robustness and flexibility. In the third and final stage, students are asked to acquire “consistent” models, which result from balancing robustness and flexibility, at both the levels of profiles and model trees.Some lessons learned in  [10] guided us to develop the assertion map detailed in Table 2, but some others were found to be undeveloped when guiding decisions about the design and implementation process of quality oriented CAD rubrics. Hence, we performed three pilot experiments to gain knowledge on those topics. Three main aspects are described next: if rubrics may be aggregated, what is the right speed for the expand–contract process, and what is the best method to convey procedures and tools that help in measuring these different assertions. The experiments are explained in detail to illustrate the suggested process for replicating our approach for further developing rubrics adapted to other scenarios.One important lesson learned in  [10] is that particular and specific rubrics are the only valid choice to share and convey quality criteria. Obviously, the assertions map assumes an incremental process in which assertions are introduced sequentially during the training period of novice users. However, nothing was concluded about whether rubrics must be separated or may be interspersed in the same form during the actual evaluation processes. We have conducted a new experiment aimed at determining whether each task requires a separate rubric. We asked our students to solve and evaluate three tasks: (A) obtain the solid model of the casing with flaps depicted in Fig. 1; (B) assemble previously modeled parts of a shut-off valve (Fig. 2), and (C) obtain the assembly drawing of the valve, including part numbers and bill of materials.The form that contains all the assertions used to evaluate the three tasks is shown in Fig. 3. It differs from Table 1 because the experiment was completed previous to solidifying the final set of assertions detailed in the table. Besides, we split it into two forms: one for Task A and a second rubric with assertions corresponding to Tasks B and C interspersed. We also asked the students to evaluate whether they felt that each rubric was helping them to understand and solve the tasks, according to the five-point Likert scale. Their average answer was 0.68 (quantified in the range [Strongly disagree = 0, Strongly agree = 1]), which means the value is closer to agree than to neither agree nor disagree. This moderate optimistic reply contradicts the fact that they found the rubrics difficult to use, since of the 16 interviewed students, five of them (31%) returned incomplete rubrics. The difference between the first form (Task A) and the second form (Tasks B and C) was moderate and better for the second (0.63–0.73). So, we conclude that this information does not help to conclude whether separate forms are better than interspersed ones. We also compared discrepancies between self-evaluation and teacher evaluation for Task A (Table 3) and for Tasks B and C (Table 4).In viewing the tables we conclude that there are no significant differences. Hence, the pilot experiment does not allow us to make any strong hypothesis. We cannot conclude whether separate are better than interspersed forms. The only clues we have in favor of independent forms are the high ratio of students who failed in returning the rubrics (31%) and the subjective evaluation of the teachers that reported that marking interspersed forms was more time consuming and prone to error. However, this is still an open problem, and a full size experiment including a control group is clearly required to validate or reject this hypothesis.The goal in this experiment was determining whether the expand–contract process was necessary. The question posed was: do students understand dimensions at first sight, or do they need time and intermediate stages to get used to the main dimensions?We developed rubrics following the assertions map and undergraduate students were “exposed” to them. Exposed means that they were informed about their existence, were told where to find them (in the virtual classroom site, which is a customized Moodle website), and were generically taught during the practical classes about the meaning and importance of these rubrics. However, they were not required to fill out any of them before the experiment, neither were they specifically taught how to interpret their assertions or measure their degree of accomplishment.In parallel, we taught the same subject to first year Master’s students. This group of post-graduate students was primarily in mechanical, design, and other engineering disciplines. They followed the same program as the undergraduate students  [40]. In principle, course content should be repetitive them, but since they possess different backgrounds, homogenization was required. Furthermore, we varied the teaching strategy by reducing theoretical introductions, emphasizing more advanced aspects, and applying them to solve more complex problems (that is, the last examples in each series).Both groups were required to solve the same exam problem. Fig. 4shows a plughole created as a cut view. Its scale may be fixed though a single given dimension and the bill of materials was also listed. Fig. 5represents the detail drawing of Part 3, which is constructed of brass (Item 3). Students were given complementary information: The upper body of the plughole includes 8 holes to allow water flow; the lower plughole body has 4 union arms between the central anchoring element and the outer funnel, and all threads are ISO metric. In the assembly, the separation between Parts 1 and 3 is due to the (undefined) space occupied by kitchen sink: Parts 1, 2 and 4 are assembled from the upper side of the sink hole; the other parts are assembled to the drain pipe, located below the sink hole (and not identified with a part number in Fig. 4). Solid models had been obtained for all the non-standard parts but 3, and were provided during the exam. Students were asked to: (A) model the lower plughole body (detail Number 3), as detailed in Fig. 5; (B) create the drawing of Part 1 (the drawing was to be obtained by extracting it from the solid model given by the teacher), and (C) create the assembly. Students could freely use the models provided during the exam (Parts 1, 2, 5, 6, and drain pipe), together with the required models of the standard parts obtained from the library (Solidworks Toolbox®).In order to familiarize students with rubrics, Task D explicitly asked them to self-evaluate their performance, and a reward was given to those matching the teacher evaluation.We examined 40 students, and received 30 valid rubrics. Twelve of them (40%) always marked only the main dimensions. Four students (13%) always marked the main dimensions and periodically (when they thought it was opportune) also marked some auxiliary assertions. Eight students (27%) marked all the assertions in all three rubrics while four students (13%) only marked detailed assertions. Finally, two students did not follow any consistent answering pattern.The main result is that direct “immersion” into contracted rubrics does not work. Neither the undergraduate nor the Master’s students obtained good results. Only 53% of the students completed the form in the condensed way, while the remaining students used the expanded version of the form. It is quite clear that some “simple” assertions are well understood by most of the students, but other assertions still require training in order to be understood. Hence, the expand–contract process is clearly beneficial, but it should be evaluated in a full-scale experiment aimed at determining the precise contraction speed for each assertion.The experiment provided other valuable information and these results are presented in Tables 5 and 6. In both tables, the three tasks are tabulated separately. In addition, each task has been labeled to indicate whether main assertions were marked (Yes or Void), and whether auxiliary assertions were marked (Yes, Sometimes, or Void).In addition, we can qualitatively perceive two main facts:1.Master’s students performed at a higher level. Their evaluations are more accurate than those of the undergraduate students. This performance difference is not due to their background in rubrics, as both groups of students have the same background. The only clear difference is the background in 3D CAD, as many Master’s students had been previously exposed to some training in CAD 3D (although not necessarily with SolidWorks).Students appear to understand the rubric used to evaluate Task A, and (to some extent) Task C, but fail in understanding the rubric aimed at evaluating Task B. All of the students had been exposed to rubrics aimed at evaluating 3D models, but none had previous experience with rubrics aimed at evaluating drawings and assemblies. Hence, previous experience with rubrics seems to make a perceptible difference.We also perceive that Dimensions 5 and 6 of Task B are scarcely understood, and, in general, Dimensions 5 and 6 are poorly understood.Apart from the tabulated results that originate from the students, there is also the instructors’ assessment. Instructors do not feel comfortable marking Task B: Dimensions 5 and 6 are confusing (a clear criterion to mark the exams is missing). To overcome this difficulty, teachers verified whether their global perception matched the average qualification for each task. In other words, they adjusted the local qualifications to match a global perception. It is also clearly perceived that some students appear to achieve a passing score by following an “average criterion”: these students assign the identical average evaluation to all the dimensions. Presumably, this average has been qualitatively obtained beforehand. Students should be interviewed to validate or reject this perception.Some students attempt to mark average values for dimensions after first marking their auxiliary assertions, but are unsuccessful because the discrete form forces round offs (does not allow for continuous average values). Forms based on computer documents could assist to automatically calculate the continuous average value, which would prevent incoherencies between the evaluation of main dimensions and their subordinated assertions. It would also help the students to understand the impact of every evaluation in the total.Finally, some students partially mark the assertions and other times mark the global value for the dimension. In such cases, if both marks are inconsistent, it is difficult to determine the intended average value for the dimension. Again, an electronic form that automatically calculates the average for each dimension and the final mark would be helpful.The first group of 40 undergraduate students passed a subsequent test. At the end of the training period, they were asked to model and self-evaluate the six non-standard parts belonging to the same assembly from  [35] (shown in Fig. 6).Five out of the forty students returned non-valid rubrics (incomplete or containing mistakes). The differences between the self-evaluation and the teacher evaluation for the remaining 35 students are tabulated in Table 7. In analyzing the average differences (last row in the table), we can easily conclude that the first dimension is well understood in all cases, but understanding the remaining dimensions clearly depends on the complexity of the part. For simple parts (i.e. the spacing washer), all but the last dimension have been understood, while complex parts still require additional training before contracting the dimensions. We particularly note that an apparently simple part like the axle caused important modeling differences because most students did not use patterns and symmetry to ensure that both slots were equal and symmetrically located. Some students also failed in modeling chamfers as separate operations.The second conclusion of the experiment relates to cases where non-valid or non-complete models were delivered by the students (cells highlighted in yellow). Following the assertions map, those two dimensions had been labeled as no-pass criteria. Students who delivered non-valid or non-complete parts (4 out of 40, i.e. 10%), were also unable to detect their mistakes and to suitably mark the corresponding rubrics.The assertions map depicted in Table 2 is useful for the CAD trainer in order to prepare a sequence of tasks aimed at maximizing quality comprehension, but we hypothesize that it is also important to instruct the CAD trainees in quality concepts from the beginning of their instruction. Hence, the assertions map and the derived rubrics were initially made publicly available for the CAD trainees from the beginning of their training. Therefore, they knew that comprehending those quality criteria was mandatory in order to pass the exams. However, as a result of previous tests, we realized that making this rubric information available was not sufficient for the students to understand and utilize the concepts embedded in these rubrics. Our next step was explicitly explaining the meaning of every rubric before requiring the students to use them.Our third experiment was aimed at determining which specific teaching and lecture notes, if any, are helpful for the students to understand rubric assertions. We hypothesized that we had to convey both suitable strategies and evaluation tools. Currently, we only consider tools already built in CAD applications, and use them to develop metrics aimed at helping to understand and mark those rubric assertions that are difficult to quantify and evaluate. We suggest that the students use them primarily as assessment tools (regardless of its intended main use).To this end, we elaborated suitable explanations for each assertion. For instance, to illustrate assertion M3.2 (profiles flexibility) we argued that profiles are flexible if: (1) Profile constraints are weak enough to allow local changes, and (2) Profile constraints are strong enough to prevent local changes from causing undesired changes or errors. But, both goals are contradictory, so an agreement is required. For instance, in order to convert a quadrilateral into a square, different strategies are equally valid: (1) Make two opposite lines horizontal and the other two vertical, and make two consecutive lines equal (Fig. 7left), or (2) Make three out of the four consecutive lines perpendicular, and make two consecutive lines equal (Fig. 7 right). Both sets constrain the shape, but not the position. However, it is important to note that the two sets of constraints are not equivalent: the first set adds an extra constraint between the shape and the reference system and prevents rotation, while the second set allows rotation. The difference between weak and strong constraint sets can be further emphasized by noting that making opposite sides parallel to each other for the first approach is: (1) insufficient on its own (too weak), or (2) redundant, if previous constraints are already present (too strong).To summarize, we explain that profiles are flexible if shape, size, position and orientation are fixed independently (Fig. 8).As can be deduced from the above example, our current teaching and lecture notes on rubrics are a combination of check-lists, good practices and evaluation tools, all of them related through assertions and sequenced according to the assertions map. However, we note that differences that exist with previous approaches are important. Instead of replacing best-practices manuals with checklists, as advocated by Gebhard  [27], we use best practices to explain why items in the checklist are important. Instead of introducing best-practices and/or checklists after the training period, we insert them along the training period by way of rubrics.But we believe that the most important difference is that checklists are only valid for the most basic dimensions of quality: validity and completeness. The other dimensions always imply trade-offs between opposite criteria, such as the quite obvious incongruity between robustness and flexibility in regard to consistency, or the less obvious inconsistency between conciseness and simplicity. These types of trade-offs are at the core of the design process so students should be taught about them as soon as possible and they should be given clear criteria in order to find reasonable solutions. For instance, as said above, a profile is robust if it is fully constrained and is flexible if shape, size, position and orientation are fixed independently. The profile is consistent if both properties are simultaneously achieved.Finally, we advocate the use of tools already available in most CAD applications as evaluation tools. A trivial example would be “pushing” lines or vertices to try to distort an unconstrained square (by way of editing tools) as an easy way to discover which constraints are missing (Fig. 9). This approach is much more intuitive and direct than trying to count degrees of freedom. However, we also note that a new tool aimed at highlighting unconstrained degrees of freedom or unnecessary mutual dependencies could be beneficial for novice users. So, there is room for new tools aimed at training novice product designers in quality-oriented strategies in CAD.According to our pilot experiment, students exposed to such explanations understand the quality concepts embedded in the rubrics, especially as they obtain self-evaluations which are beneficial and much more similar to teacher evaluations than non-exposed students (see Table 7). The experiment is described as follows: after being taught about the meaning of the first rubric in the assertions map, and after a two-hour session in the CAD laboratory where they solved very simple profiles (such as a quadrilateral given the lengths of four sides and one angle), the students were required to solve the profile displayed in Fig. 10, where dimension L (73.30) had to be obtained as a derived value and was used by the students to check the integrity of their profiles. The exercise solution was provided to all students, but they were discouraged from reading it unless it was needed. After completing the exercise, they were asked to self-evaluate their job using the rubric displayed in Fig. 11. Then, the teacher evaluated their performance using identical rubrics and both evaluations were compared.20 students solved the exercise and 18 returned the form, with two forms being incomplete and one student solving the exercise with a “future version” of the CAD application. These five were discarded and the remaining 15 valid self-evaluations were compared against the teacher evaluations (Table 8).The main result observed is that the students clearly understand the rubric, as differences between teacher evaluation and self-evaluation are negligible. Absolute values of the teacher evaluations are also relevant, as they illustrate that students obtained good marks because they had clear understanding of what validity and consistency of profiles mean (Table 9).Differences between self-evaluation and teacher evaluation reveal two important issues. First, differences in Assertion 1.1b were due to students not realizing that an open sketch is vulnerable: it may be inadvertently modified while re-opening the document. As a result of the pilot experiment, suitable explicit explanations on the importance of existing profiles were added to the corresponding lecture notes. Second, although the differences between teacher and student evaluations were small, and the teacher evaluation was good in general, the trade-off between Assertions 3.2a and 3.2b was not fully understood by all students. More time is required to resolve such trade-offs.Finally, we note that Subject 19 was assessed by the teacher with the highest mark in all the assertions. Hence, the discrepancy in Assertion 1.1a seems to be due to misunderstanding the assertion, as he replied as “strongly disagree”, while he delivered a perfectly valid model in a valid file. We can guess that the implicit double negative in the sentence caused confusion. We should attempt to prevent such complex assertions in future rubrics.Results of the pilot experiment suggest that our current teaching and its associated lecture notes on rubrics are working reasonably well and we intend to develop them in detail. They include checklists, as much as explanations on typical mistakes, but they are centered around good practices and evaluation tools, all of them related around rubric-assertions and sequenced according to the assertions map. The most important innovations are: the main role played by explanations of how to cope with trade-offs between contradictory criteria and the explanation of how existing tools may be used to assist in evaluating assertions.

@&#CONCLUSIONS@&#
We intend to convey quality-oriented strategies to CAD trainees, so we embed quality criteria into our rubrics in order to force CAD trainees to comprehend them early in their instruction and prevent the Einstellung effect.But producing and using such rubrics is not trivial. First, we identified those aspects of quality in CAD models that are appropriate to introduce to large populations of inexperienced CAD trainees, and grouped them around six main dimensions (Table 1). Then, we elaborated an expand–contract strategy that adapts the rubrics to CAD trainee progress (Table 2). As a result, an assertions map was obtained. This map shows how the expand–contract strategy adapts the rubrics to the progress of the CAD trainees and helps them to understand the different dimensions of the rubrics.The paper also included several pilot experiments aimed at illustrating the suggested process for replicating our approach for further developing rubrics adapted to other scenarios. The results of these pilot experiments were useful to (a) discourage the use of interspersed rubrics and suggest that each task must be evaluated by a separate rubric (although the results were not fully conclusive); (b) suggest that the expand–contract process is beneficial to comprehend the quality concepts embedded in the rubrics (i.e. learning to work with generic rubrics is not enough, nor is simply being exposed to quality rubrics); and (c) realize that the students do not understand and utilize the concepts embedded in the rubrics unless the meaning of every assertion is explicitly explained and they are forced to use them and compare their self-evaluations against the teacher evaluations. We also determined that while some “simple” assertions are well understood by most of the students after initial exposure, others still require further training in order for them to be comprehended. Hence, the expand–contract process should be evaluated in a full-scale experiment aimed at finding the right contraction speed for each assertion. We also realized that students need explicit procedures and metrics to assist them in evaluating their performance. Our last experiment was aimed at determining whether explicit explanations are helpful in conveying suitable CAD modeling strategies. The results suggest using evaluation tools built in CAD applications to develop metrics aimed at helping to understand and mark those assertions of rubrics that are difficult to quantify and evaluate. Our current teaching and lecture notes on rubrics appear to work reasonably well and should be developed in detail. They are a mix of checklists, good practices, and evaluation tools, all of them related to assertions and sequenced according to the assertions map. They pay particular attention to the trade-offs required to obtain valid designs.Our future work includes testing model assertions maps and their corresponding rubrics and lecture notes, while also developing similar assertions maps and documents for assemblies and drawing extraction. We do not foresee any reason that may prevent this approach from working in undergraduate or even secondary schools, so we suggest testing it to discover which peculiarities, if any, require adaptions of this approach.