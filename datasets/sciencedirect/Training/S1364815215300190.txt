@&#MAIN-TITLE@&#
An open source framework for many-objective robust decision making

@&#HIGHLIGHTS@&#
Many-objective robust decision making (MORDM) is an emerging approach for eliciting robust strategies under deep uncertainty.This study provides an open source software implementation of MORDM, called OpenMORDM.OpenMORDM aims to provide accessible visualization and analytic techniques to the environmental modeling community.

@&#KEYPHRASES@&#
Robust decision making,Deep uncertainty,Threshold,Scenario discovery,Risk assessment,Robustness,

@&#ABSTRACT@&#
This study introduces a new open source software framework to support bottom-up environmental systems planning under deep uncertainty with a focus on many-objective robust decision making (MORDM), called OpenMORDM. OpenMORDM contains two complementary components: (1) a software application programming interface (API) for connecting planning models to computational exploration tools for many-objective optimization and sensitivity-based discovery of critical deeply uncertain factors; and (2) a web-based visualization toolkit for exploring high-dimensional datasets to better understand system trade-offs, vulnerabilities, and dependencies. We demonstrate the OpenMORDM framework on a challenging environmental management test case termed the “lake problem”. The lake problem has been used extensively in the prior environmental decision science literature and, in this study, captures the challenges posed by conflicting economic and environmental objectives, a water quality “tipping point” beyond which the lake may become irreversibly polluted, and multiple deeply uncertain factors that may undermine the robustness of pollution management policies. The OpenMORDM software framework enables decision makers to identify policy-relevant scenarios, quantify the trade-offs between alternative strategies in different scenarios, flexibly explore alternative definitions of robustness, and identify key system factors that should be monitored as triggers for future actions or additional planning. The web-based OpenMORDM visualization toolkit allows decision makers to easily share and visualize their datasets, with the option for analysts to extend the framework with customized scripts in the R programming language. OpenMORDM provides a platform for constructive decision support, allowing analysts and decision makers to interactively discover promising alternatives and potential vulnerabilities while balancing conflicting objectives.•Name of Software: OpenMORDMDescription: OpenMORDM is an open-source R library for multiobjective robust decision making (MORDM). It includes support for loading datasets from a number of sources including CSV, XLS, XLSX, databases, and R matrices and data frames; visualizing the data sets using various 2D and 3D plots; performing scenario discovery and trade-off analysis; and computing uncertainty/robustness metrics. OpenMORDM also includes a web-based data exploration and visualization toolkit.Developer: D. Hadka (dmh309@psu.edu) with contributions by P. Reed and K. Keller.Funding Source: Development was partially supported by the National Science Foundation through the Network for Sustainable Climate Risk Management (SCRiM) under NSF cooperative agreement GEO-1240507 as well as the Penn State Center for Climate Risk Management.Source Language: RSupported Systems: Unix, Linux, Windows, MacLicense: GNU General Public License, Version 3Availability: http://github.com/dhadka/OpenMORDM

@&#INTRODUCTION@&#
A critical component of environmental planning and management is the search for robust solutions capable of withstanding deviations from our best projections of the future. This challenge is amplified by the presence of deep uncertainty, where the suite of all possible future events as well as their associated probability distributions are themselves uncertain (e.g., future climatological and hydroeconomic factors [Knight (1921), Lempert (2002), Olson et al. (2012)]). These challenges have led to several “bottom-up” decision support frameworks [Nazemi and Wheater (2014), Weaver et al. (2013)], which move beyond trying to predict the most probable future(s) to discover which states of the world (SOWs) may lead to high consequence system vulnerabilities. This step helps with the task of evaluating the likelihoods of the discovered system vulnerabilities as it can help to focus the analysis on a subset of plausible future scenarios (e.g., Lempert et al. (2012)). Bottom-up or robustness-based approaches include Decision Scaling [Brown (2010), Brown et al. (2012)], Information-Gap (Info-Gap) [Ben-Haim (2004)], Robust Decision Making (RDM) [Lempert (2002), Lempert et al. (2013, 2006), Groves and Lempert (2007), Lempert and Collins (2007)], and Many-Objective Robust Decision Making (MORDM) [Kasprzyk et al. (2013)]. As highlighted by Herman et al. (2015), these bottom-up frameworks can be generalized into four steps: identifying decision alternatives, sampling states of the world, specifying robustness measures, and performing scenario discovery to identify the most important uncertainties. The final step, scenario discovery, is commonly used to find policy-relevant controls by determining the ranges of each uncertainty leading to system failure [Lempert et al. (2006)]. Herman et al. (2015) note that while these methods are often defined at a conceptual level, specific implementations share a number of potentially interchangeable concepts which should be compared to understand consequences for decision support. This work addresses the need for software and visualization tools to flexibly support the quantitative components of these “bottom-up” environmental systems planning frameworks, which share the goal of identifying robust solutions. The following paragraphs introduce the conceptual frameworks for decision support under deep uncertainty, while the quantitative methods implemented in this work are described in Section II.Robust Decision Making (RDM), like other “bottom-up” approaches, seeks to distinguish robust solutions which provide satisfactory performance across many plausible SOWs [Lempert (2002), Lempert et al. (2013), Groves and Lempert (2007), Lempert and Collins (2007)]. Given a pre-specified set of alternatives to analyze, RDM subjects each to an ensemble of SOWs that are treated as exploratory samples over plausible ranges of uncertain factors [Bryant and Lempert (2010), Groves and Lempert (2007), Lempert et al. (2006, 2012)]. The goal — as is generally the case in Decision Scaling and Info-Gap — is to identify future scenarios that may cause the system to fail. RDM studies often adopt a “satisficing” approach [Simon (1959)], in which solutions must satisfy performance requirements across many plausible futures rather than provide optimal performance in a single future. Using a satisficing approach, robustness can be quantified with the domain criterion [Schneller and Sphicas (1983), Starr (1962)] which aims to maximize the volume of the uncertain factor space in which performance requirements are satisfied [Lempert and Collins (2007)]. Additionally, RDM analyses typically employ the Patient Rule Induction Method (PRIM) [Friedman and Fisher (1999)] to perform a high-dimensional sensitivity analysis for scenario discovery in order to identify the ranges of uncertain factors most likely to cause system failure [Bryant and Lempert (2010), Groves and Lempert (2007), Lempert et al. (2006, 2008)]. RDM builds upon exploratory modeling [Bankes (1993), Kwakkel and Pruyt (2013)] by providing a systematic approach to identifying vulnerabilities.Info-Gap analysis aims to quantify the maximum allowable deviation of deeply uncertain system factors that can be tolerated while still satisfying performance requirements [Ben-Haim (2004), Hipel and Ben-Haim (1999), Hall et al. (2012)]. Uncertain factors are sampled radially outward from a baseline (expected) future state of the world until a failure condition is reached; the distance from the baseline at which this occurs is termed α, or the “uncertainty horizon” [Hall et al. (2012), Korteling et al. (2013)]. Note that in this definition, there is no mention of probability distributions for the uncertainties. Rather, α defines the distance in the space of deeply uncertain factors between the baseline (expected) state of the world and the nearest state of the world in which the model predicts system failure. It assumes that a larger value of α implies the system is more resilient to perturbations in the deeply uncertain parameters. However, α fails to identify which specific uncertain factors, or combinations of factors, predict system failure. Recent examples of Info-Gap applications in water resources planning problems include Hine and Hall (2010) and Matrosov et al. (2013).Decision Scaling, like other “bottom-up” approaches, inverts the decision making process. Rather than focusing on predictive distributions (derived, for example, by downscaling of Atmospheric-Ocean General Circulation Model (AOGCM) projections), Decision Scaling first aims to identify thresholds likely to trigger consequential system risks. The approach is a three-step process of (1) identifying key concerns and decision thresholds, (2) modeling the response to changing environmental conditions, and (3) estimating the relative probability of the critical environmental thresholds being crossed [Brown et al. (2012)]. Decision Scaling studies typically focus on uncertain climate factors, though recent work extends the approach to include hydroeconomic factors [Ghile et al. (2014), Lownsbery (2014)]. Decision Scaling's most significant difference from the other decision support frameworks is its assumption that the likelihoods associated with changes in temperature and precipitation can be inferred as subjective probabilities. The subjective probabilities are developed via expert evaluations of how SOWs attained from statistical weather generators relate to AOGCM projections [Brown et al. (2012)]. Decision Scaling has been most widely used as a discrete choice framework for choosing between pre-specified design alternatives [e.g., Moody and Brown (2013)] or as a vulnerability analysis to characterize the risks of existing systems [Ghile et al. (2014), Turner et al. (2014)].In the Decision Scaling and Info-Gap frameworks, it is common to analyze a relatively small set of discrete decision alternatives that are pre-specified by stakeholders. This reflects a high degree of knowledge about system behavior under uncertainty, and may cause an analysis to be vulnerable to a significant status quo bias [Brill et al. (1990)]. Furthermore, pre-specified alternatives may overlook important trade-offs between conflicting objectives that reflect decision relevant performance requirements or tensions between stakeholders [Herman et al. (2014)]. RDM analyses can also suffer from these issues if the practitioner explores only a fixed set of alternatives. To overcome these challenges, Kasprzyk et al. (2013) propose Many-Objective Robust Decision Making (MORDM), in which alternatives are discovered via many-objective optimization in the projected future state of the world. MORDM supports constructive learning to improve decisions for complex, ill-defined environmental planning and management problems. This follows the framework of Many-Objective Visual Analytics (MOVA) [Woodruff et al. (2013)], a foundation for constructive decision aiding [Tsoukias (2008), Roy (1999)] in which problem framing is performed interactively with stakeholder feedback. MORDM begins with problem formulation, where the computational model, performance objectives, and well-characterized uncertainties are elicited from stakeholders. Additionally, key decisions or alternative hypotheses of system performance are articulated. The problem formulation is then subjected to many-objective search to generate alternative designs (or solutions) to the problem. With many objectives, there is typically no single optimal design. Instead, there exists a collection of Pareto optimal designs that can be approximated by computational search, where improvements in one performance objective require a degradation in one or more other objectives [Pareto (1896)]. This “generate first, choose later” [Cohon and Marks (1975), Maass et al. (1962)] a posteriori elicitation of preference is well established in water resources planning. The process of understanding performance tradeoffs is typically supported by interactive visualization software [e.g., Kollat and Reed (2007)].In the MORDM approach, robustness is assessed a posteriori for each Pareto-approximate solution (i.e., the solutions which are found to be non-dominated) using a four step process. (1) First, performance requirements and system constraints are elicited from stakeholders in the context of available performance trade-offs. (2) The deeply uncertain factors in the model must be identified and sampled based on an appropriate design of experiments developed in consultation with stakeholders. This step widens the envelope of deeply uncertain SOWs that will be used to evaluate the robustness of decision alternatives. (3) The vulnerabilities, trade-offs, and dependencies of the system are evaluated by subjecting Pareto-approximate alternatives (under well-characterized uncertainty) to the deeply uncertain SOWs sampled in the prior step. These SOWs represent hypothetical but plausible conditions that may be encountered in the uncertain future. (4) Finally, the solutions are annotated with one or more measures of robustness and explored using interactive visualization tools. Because the framework is inherently multi-objective, the robustness goals of multiple stakeholders can be considered, which may conflict [Herman et al. (2014)]. MORDM is able to quantify the trade-offs between robustness and the various performance objectives, explore the dependencies between uncertainties and system performance, and identify the vulnerable states of the system. MORDM provides an opportunity to flexibly bridge and advance bottom-up methodologies.The choice of decision support framework under deep uncertainty (e.g., Decision Scaling, Info-Gap, RDM, MORDM) depends on the knowledge and preferences of the decision makers, the intrinsic predictability of the system under study, the quality of available computational models, and the types of answers sought by the decision makers (e.g., identifying key parameters impacting robustness versus quantifying resilience to small perturbations). Herman et al. (2015) highlight the importance of consistently choosing methodological options that support a posteriori decision support (e.g., that generate alternatives prior to expressing preferences [Cohon and Marks (1975)], and globally explore future SOWs to discover sensitive uncertainties rather than assuming them in advance of analysis [Bryant and Lempert (2010)]). Herman et al. (2015) also note that currently popular decision support frameworks focused on robustness to uncertainty share many interchangeable ideas, and need not be considered separate.MORDM is a comprehensive conceptual framework that includes optimization, scenario discovery, and interactive visualization, and is a viable target for an open source software package to unify many of the ideas in bottom-up decision support frameworks. Moreover, given the growing interest among stakeholders in decision support frameworks to address deep uncertainty, the R basis of OpenMORDM is highly complementary to other open source tools such as the Python-based EMA Workbench [Kwakkel (2015)] for expanding access to large communities of practice. There is a strong opportunity for the R and Python open source programming communities to collaborate to expand the scope and quality of tools available for exploratory modeling, sensitivity analysis, tradeoff analysis, and assessments of robustness.While developing a consistent application programming interface (API) for MORDM, we considered the availability of existing open source tools. For optimization, we considered metaheuristics designed for solving multi-objective, non-linear, mixed discrete and continuous, disjoint, multimodal problems commonly found in complex planning or design applications such as water resources management [Coello Coello et al. (2007), Nicklow et al. (2010)]. Metaheuristics are available as individual software libraries available from the original authors, such as AMALGAM [Vrugt and Robinson (2007)] or the Borg MOEA [Hadka and Reed (2013)], or contained within comprehensive software frameworks for multiobjective optimization, such as Paradiseo [Cahon et al. (2004)], JMetal [Durillo et al. (2010)], or the MOEA Framework [Hadka (2014)]. For scenario discovery, two commonly-used algorithms are the Patient Rule Induction Method (PRIM) and Classification and Regression Trees (CART). PRIM and CART are introduced in detail in section 2.1. There are two open-source implementations of PRIM written in the R programming language, including the original non-interactive prim package [Duong (2014)] and the interactive sdtoolkit package [Bryant (2014)]. sdtoolkit interactively allows the user to select the desired coverage and density of the generated PRIM hyperboxes. CART is available in the rpart package within R. Finally, there exist several toolkits to support interactive visualization. For example, Kollat and Reed (2007) developed the Visually Interactive Decision-making and Design using Evolutionary Multi-objective Optimization (VIDEO) software. VIDEO encompasses the optimization and interactive visualization aspect, but lacks scenario discovery and robustness quantification.This study contributes OpenMORDM, an open-source implementation of MORDM in the R programming language that encompasses all three aspects: optimization, scenario discovery, and interactive visualization. R is selected as the host language due to its prevalent use in environmental modeling, the ability to leverage a powerful functional scripting language to support extensibility, and the availability of prebuilt analytical and statistical packages. We will demonstrate the decision support capabilities of OpenMORDM using a conceptually simple but sufficiently challenging hypothetical environmental management problem called the “lake problem” [Carpenter et al. (1999), Singh et al. (2015)]. Using OpenMORDM, we will demonstrate how to generate near-optimal alternative designs in the best available projection of the future SOW, sample deeply-uncertain SOWs, explore competing hypotheses for alternative definitions of robustness, and identify key deeply uncertain factors and scenarios that should inform ex-post monitoring of system performance. While OpenMORDM focuses on the application of MORDM, the software is not restricted in scope to MORDM. Rather, it is viewed as a decision support library empowered by a collection of complementary analysis tools. The ultimate goal is addressing the relevant questions and concerns of the decision maker, and allowing the decision maker to explore the impacts and significance of alternative management actions and conceptions of robustness.The remainder of this paper is organized as follows. Section 2 discusses the software architecture of OpenMORDM. Section 3 introduces the lake problem. Section 4 demonstrates applying MORDM on the lake problem using OpenMORDM. Particular attention is given to the software API that facilitates MORDM and supports user-customizable visualizations and analyses. Section 5 concludes this paper by providing recommendations for further enhancements to OpenMORDM.OpenMORDM is an open-source implementation of MORDM in the R programming language. Readers interested in in-depth conceptual details on MORDM should refer to Kasprzyk et al. (2013) and Herman et al. (2015). The remainder of this section is organized as follows. Section 2.1 discusses the overall MORDM workflow and the software architecture of OpenMORDM for supporting this workflow. Section 2.2 discusses the robustness measures built into OpenMORDM for quantifying the impacts of deep uncertainty on a model.OpenMORDM is developed as an R package. As such, it provides a collection of data structures and functions (the API) within R for performing all of the functionality required by MORDM. Fig. 1shows the overall software architecture of OpenMORDM. The API can be accessed through an interactive R session or through a web-based visualization toolkit accessible from any WebGL-enabled web browser. The web-based visualization toolkit, powered by Shiny [RStudio, Inc. (2014)], provides a standardized, interactive interface for visualizing, exploring and analyzing many-objective datasets. Furthermore, being a web-based technology, datasets can be publicly or privately shared with colleagues by instantiating a Shiny web server. See the documentation for the explore command for details on starting the web visualizations.OpenMORDM supports a variety of data access mechanisms for generating or loading datasets. If the computational model is available as an R function or a standalone executable, OpenMORDM can evaluate designs directly against the model. Any R function of the formis supported. If the model is a standalone executable that resides on the host computer, then OpenMORDM can invoke the executable using the standardized external problem interface defined by the MOEA Framework [see Chapter 5.2 in Hadka (2014)]. This external problem interface uses the standard input/output channels to pass the input variables to the executable and read the resulting objectives and constraints. Alternatively, one could use the Model-R Coupler to interface with standalone executables [Wu et al. (2014)].OpenMORDM can access pre-computed datasets stored in a database or structured input files. Any database with a corresponding R driver is supported (e.g., RPostgreSQL for accessing PostgreSQL databases [Conway et al. (2013)]). To remain agnostic of the database and table structure, OpenMORDM requires the decision maker to write the appropriate SQL queries (or other lookup mechanisms for non-SQL databases) for accessing the data. OpenMORDM also supports loading datasets stored in comma-separated value (CSV) files and some proprietary spreadsheet documents (XLS or XLSX).OpenMORDM defines functions for each stage of the MORDM process. New problem formulations are defined using the define.problem command. This assumes a compatible R function or standalone executable exists as described above. In addition to providing the name of the R function or path to the executable, the user also specifies the lower and upper bounds for each model input and the preference direction for performance objectives (i.e., whether an objective is minimized or maximized).Alternative designs are generated with the borg.optimize command. The borg.optimize command implements the Borg Multiobjective Evolutionary Algorithm (Borg MOEA) for many-objective search [Hadka and Reed (2013)]. The Borg MOEA has an established history of effectively solving many-objective problems from the water resources and broader engineering domains [Hadka et al. (2012), Hadka and Reed (2012), Woodruff et al. (2012), Reed et al. (2013), D'Ervau (2013), Giuliani et al. (2014), Hadka and Reed (2015), Singh et al. (2015)]. Note, however, that any many-objective search algorithm can be supported assuming it generates a dataset in the appropriate file format. OpenMORDM can load datasets from CSV files with mordm.read.csv, Excel files with mordm.read.xls, or load native R matrices and data frames with mordm.read.matrix.After generating alternative designs, the next step in MORDM is to define and compute the robustness measures. OpenMORDM provides the compute.robustness function for this step. Typically, deep uncertainty requires defining multiple problem formulations for each deeply uncertain parameterization. One or more problem formulations created by the define.problem function can be passed as arguments. Several robustness calculations are provided by default, as described in Section 2.2. Otherwise, custom functions for computing robustness can be provided. This customization is necessary for satisficing-based robustness, where the decision maker defines required levels of performance and/or constraints specific to each application.The result of the uncertainty function is a vector or matrix (in the case where multiple robustness measures are calculated). The dataset annotated with robustness values is visualized using mordm.plot to generate a 3D scatter plot of the alternative designs. As demonstrated in this study, points are color-coded by the robustness value. Nevertheless, robustness can be plotted on any axis if desired. Other 2D and 3D visualizations can be generated using functionality built into OpenMORDM, such as mordm.plot.parallel for generating a parallel coordinates plot synchronized with the 3D scatter plot, or plotting capabilities provided by other R packages.For more control over the uncertainty calculations, it is often useful to split the deep uncertainty factor sampling from the robustness metric calculation. The two underlying R functions, mordm.sample.uncertainties and mordm.evaluate.uncertainties can be invoked separately. This is particularly useful for performing scenario discovery and vulnerability analysis, where the sampled uncertainty factors are required inputs.MORDM also emphasizes the identification of key vulnerabilities and dependencies caused by deep uncertainties. For example, one could define vulnerable scenarios as those in which a solution fails to meet one or more of the stakeholders' performance requirements. OpenMORDM provides two “association rule learning” algorithms for identifying key uncertainties and their associated ranges that lead to vulnerabilities. The Patient Rule Induction Method (PRIM) attempts to find regions in uncertainty space with an above-average number of uncertainty parameterizations leading to vulnerabilities [Friedman and Fisher (1999)]. It achieves this goal by iteratively “peeling” away subregions to maximize the density of vulnerable solutions in the remaining volume, represented as a hyperbox. An alternative algorithm called Classification and Regression Trees (CART) recursively subdivides the uncertainty space into partitions exhibiting less and more frequent vulnerabilities [Breiman et al. (1984)]. The partitions are chosen to minimize Type 1 classification error (i.e., minimize the false-positive rate). Lempert et al. (2008) compared PRIM and CART when developing their RDM framework, concluding that neither approach was distinctly superior to the other. PRIM requires user interaction to select the appropriate peeling thresholds and stopping criteria, but produces easily-interpretable hyperboxes (i.e., factor mapping that identifies ranges of uncertainties leading to vulnerabilities). CART, on the other hand, requires no user interaction but may partition space into many disjoint sets. We do note that CART has the theoretical advantage of minimizing Type 1 classification errors [Breiman et al. (1984)]. Regardless, OpenMORDM supports both PRIM and CART via the analyze.prim and analyze.cart methods. For PRIM, OpenMORDM supports both versions from the prim [Duong (2014)] and sdtoolkit [Bryant (2014)] packages. Moreover, since the R community provides an extensive library of statistical methods, other association rule learning methods can be considered.Sensitivity analysis is another technique to identify the vulnerabilities and dependencies within the model(s) used to support decisions. Unlike PRIM and CART, sensitivity analysis will not identify the specific ranges that lead to vulnerabilities, but it will rank order the uncertain factors by their influence on metrics of focus (i.e., factor prioritization [Saltelli et al. (2008)]). The R sensitivity package provides numerous sensitivity analysis routines, including ANOVA, Morris' elementary effects, and Sobol' sensitivity indices [Pujol et al. (2014)]. The compute.sensitivity function in OpenMORDM provides a standardized interface to these sensitivity routines. Additionally, we include an implementation of the delta-moment algorithm by Plischke et al. (2013) within OpenMORDM. The delta-moment method is notable due to its ability to compute sensitivities from “given data”. Whereas traditional sensitivity analysis routines (e.g., Sobol' sensitivity analysis) require specific sampling techniques, the delta-moment method computes sensitivities from arbitrary datasets.OpenMORDM contains many additional functions not mentioned here for brevity. Documentation for these functions is provided in the OpenMORDM package. One of the advantages of using a functional language like R as the implementation language is the ability to embed hooks for extensibility. For example, users can define custom robustness functions as an argument to the compute.robustness method. In this manner, custom extensions can be developed to enhance OpenMORDM for any particular application.The decision support frameworks described in Section I have proposed a number of measures to quantify robustness under deep uncertainty. OpenMORDM implements several of these to allow users to compare and select an appropriate measure for the system under consideration. The elicited definition of thresholds for robustness is a critical step in MORDM and drives the remainder of the analysis. The appropriate thresholds, based either on system constraints and/or required levels of performance, may differ across stakeholders and risk attitudes. Following Lempert and Collins (2007), Herman et al. (2015) distinguish two major classes of robustness measures in the literature: regret and satisficing. Generally speaking, regret-based robustness seeks to minimize deviations in system performance caused by deep uncertainties compared to a preferred or ideal design (i.e., minimizing expected loss), whereas satisficing-based robustness seeks to maximize the SOWs where system requirements as defined by stakeholders are met (see below for a more detailed and technical discussion). OpenMORDM implements four robustness measures. The decision support frameworks reviewed in Section 1 are defined in general terms to allow any of the following measures to be used. Herman et al. (2015) demonstrate the importance of selecting an appropriate metric regardless of the overarching framework under consideration.The two regret-based robustness measures, Regret Type I and Regret Type II, measure the deviation in performance in deeply uncertain SOWs. Both metrics use the 90th percentile objective value in the uncertainty ensemble so capture the worst-case behavior of a solution while limiting the susceptibility to outliers. Regret Type I focuses on the impact of incorrect assumptions about the future SOW by computing the change in system performance across all sampled SOW [Kasprzyk et al. (2013)]. For a given design x,(1)RegretTypeI=maxi{quantiles∈S(|fi(x;s)−fi(x;s¯)fi(x;s¯)|,0.9)},where fi(x;s) is the value of the ith objective in SOW s,s¯is the baseline SOW under well-characterized uncertainties, andquantiles∈S(·,0.9)computes the 90th percentile value across all SOWs s∈S. Minimizing Regret Type I differentiates designs with minimal fluctuation from the baseline performance across all sampled SOWs. By contrast, Regret Type II focuses on the impact caused by incorrectly selecting a design alternative for the future SOW [Savage (1951)]. We compute the degradation in system performance between the selected design and the “best” design within each sampled SOW:(2)RegretTypeII=maxi{quantiles∈S(|fi(x;s)−supy∈Pfi(y;s)fi(x;s)|,0.9)},where P is the set of alternative designs generated by the optimization step andsupy∈Pf(y;s)identifies the “best” design in P within SOW s. Here, the best design is computed as the ideal point. In multi-objective space, the ideal point marks the best value achieved in each objective. Note that the ideal point may not correspond to a valid design; instead, it serves as an upper bound on the best attainable performance in each objective. Note that both equations (1) and (2) assume all objective values are being maximized. If the objective is being minimized, then equations (1) and (2) should compute the minimum objective value.The satisficing-based robustness measures, Satisficing Type I and Satisficing Type II, both determine the feasibility of a design with respect to some performance criteria defined by the stakeholder. The criteria are encoded with a satisficing indicator function, IS(…), evaluating to 1 when all conditions are satisfied and 0 otherwise. For Satisficing Type I, feasibility is measured by the fraction of SOWs that satisfy the criteria:(3)SatisficingTypeI=1|S|∑s∈SIS(f(x;s)).Satisficing Type I is similar to the domain criterion introduced by Starr (1962). Rather than computing the volume of the uncertain factor space (i.e., the volume of sampled SOWs) satisfying the criteria, which cannot be computed precisely when using a discretized Latin hypercube sampling, Satisficing Type I instead computes the fraction of SOWs satisfying the criteria. Alternatively, Satisficing Type II measures feasibility as the maximum allowable uncertainty as a distance from the expected future SOW while still satisfying the criteria:(4)SatisficingTypeII=Minimize‖s-s¯‖suchthatIS(f(x;s))=0overalls∈S.Note that Satisficing Type II is similar in nature to the approach used in Info-Gap (Ben-Haim, 2004). A larger value for Satisficing Type II indicates larger perturbations in the uncertainty factors (i.e., larger error between the expected and actual future SOW) is tolerable. To summarize, Regret Type I represents the deviation from baseline performance; Regret Type II represents the deviation from the “best” solution in each scenario; Satisficing Type I represents the volume or fraction of successful scenarios out of the total; and Satisficing Type II represents the minimum distance to a failure scenario.In this paper, we demonstrate the OpenMORDM package on an environmental management test problem termed the “lake problem.” The lake problem is a hypothetical and highly stylized decision problem where a town's inhabitants must determine the amount of allowable pollution that can be emitted into a nearby lake for a given planning horizon [Carpenter et al. (1999)]. As shown in Fig. 2, pollution enters the lake from two sources: anthropogenic pollution resulting from industrial and agricultural activities from the town, and uncontrolled natural inflows from the environment. In this exercise, we will represent the alternative values or objectives of the town's inhabitants while seeking an appropriate pollution control strategy to avoid irreversibly polluting the lake. The temporal dynamics of the pollution in the lake is approximated by(5)Xt+1=Xt+at+Xtq1+Xtq−bXt+ε,where Xtis the amount of phosphorus in the lake at time t, at∈[0, 0.1] is the allowed anthropogenic pollution at time t (i.e., the pollution control strategy), b measures the lake's phosphorus removal rate (due to sedimentation, outflow, and biological sequestration), q defines the lake's recycling rate (primarily through sediments), and ε represents uncontrollable natural inflows of pollution modeled as a log-normal distribution with a given mean, μ, and standard deviation, σ [Singh et al. (2015)]. Here, the time t is expressed in years and we consider a planning horizon of 100 years. For certain combinations of b and q, there exists a critical phosphorus threshold, Xcrit, above which the lake will experience irreversible eutrophication since the recycling rate exceeds the removal rate even in the absence of anthropogenic input. For more details on these parameters and their interpretation, see Carpenter et al. (1999) and Singh et al. (2015).For the purposes of this study, we use the four-objective formulation of the lake problem developed by Ward et al. (2015). The four objectives are minimizing phosphorus in the lake, maximizing economic benefit, maximizing inertia, and maximizing reliability. The first objective, minimizing phosphorus in the lake, computes the average phosphorus level over all time periods and SOWs:(6)fphosphorus=1|S||T|∑s∈S∑t∈TXt,s,where S is the set of all sampled SOWs, T is the set of time steps, Xt,sis the level of pollution in the lake at time t in SOW s. Note that fphosphorusis correlated with but different from the decision variables atin Equation (5). The decision variables control the allowed anthropogenic pollution released into the lake, such that if all other factors are kept equal, an increase in allowed anthropogenic pollution raises the average phosphorus level in the lake. However, fphosphorusaccounts for phosphorus removal due to various natural mechanisms, recycling, and other natural sources of pollution.Allowing increased anthropogenic pollution yields economic benefit for the town's inhabitants due to reduced control costs. The second objective approximates this economic benefit as:(7)feconomic=1|S|∑s∈S∑t∈T(αat−βXt,s2)δt,where α is the economic benefit realized per allowing a unit of anthropogenic pollution, β is a reduction in benefit resulting from polluting the lake (eutrophic cost), and δ is a fixed discount rate.The third objective, inertia, measures the fraction of time steps where the year-to-year change in allowable pollution, at−at−1, remains below a defined threshold, τ:(8)finertia=1(|T|−1)∑t=1|T|I(at−at−1>τ).Since we are developing a pollution control strategy to place limits on anthropogenic pollution emitted by the town, ideally the year-to-year change in allowable pollution are below a certain threshold. Drastic changes year-to-year burdens policy makers as they must revise policies every year and as installed infrastructure may have to be changed before the end of the planned lifetime. An inertia value of 1.0 is ideal, indicating no major policy changes exceeding the threshold τ are required. In this study, τ is set for illustrative purpose to 20%.Lastly, the fourth objective, reliability, measures the fraction of years in which the pollution level in the lake is below the critical phosphorus threshold of the lake, Xcrit:(9)freliability=1|S||T|∑s∈S∑t∈TI(Xt,s<Xcrit),where Xcritis the critical phosphorus threshold for the lake and I(…) is an indicator function returning 1 if the conditions are met and 0 otherwise. Exceeding Xcritresults in irreversible eutrophication of the lake. Therefore, a value of freliabilitynear 1.0 is preferred, indicating the lake remains in an oligotrophic state during all time periods and across all SOWs. This formulation also specifies a hard constraint of freliability>0.85.A prior study of the lake problem (Singh et al., 2015) first considers the case where the parameters b, q, μ, σ and δ are assumed to be well-known. These well-characterized, baseline parameters are shown in Table 1. When a system is not well-characterized and model parameters not known to a degree of certainty, actionable decisions may be based on faulty assumptions. Under more realistic assumptions, the lake properties (b and q) and the natural pollution inflows (μ and σ) might be deeply uncertain. Additionally, the appropriate discount rate applied to the long-term future economic consumption is also an “unresolvable uncertainty” [Weitzman (1998), Newell and Pizer (2003)]. Singh et al. (2015) later considered deep uncertainty in their analysis, but limited the experiment to deeply uncertain natural pollution inflows (μ and σ). In this study, we extend this analysis and consider all five parameters under deep uncertainty (b, q, μ, σ and δ).We stress that this problem is intentionally kept simple to both demonstrate OpenMORDM while also serving as a reproducible tutorial for interested readers. The intent is to demonstrate the analytic and visual tools provided by OpenMORDM on a conceptually simple problem, where the factors driving decision making are known a priori for verification and validation of these techniques and where users may have some initial intuition about the trade-offs. Even with its conceptually simple formulation, prior studies have demonstrated that the lake problem exhibits non-linear dynamics illustrative of real-world applications, which pose significant challenges to modern decision support frameworks [Singh et al. (2015), Ward et al. (2015)].This section guides the reader through an example application of OpenMORDM to analyze the lake problem. This analysis is designed to demonstrate the use and capabilities of OpenMORDM. This section is styled as a tutorial, where the reader can follow these examples using the publicly-available OpenMORDM software. The supplemental material includes instructions for downloading and installing this software as well as example data files and scripts containing the results generated below. Readers replicating these results should note that these analyses are stochastic and observed results may differ slightly from the figures presented in this paper subject to their individual implementation choices.We begin by loading the OpenMORDM and lhs (Latin hypercube sampling) packages:Next, we construct the problem formulation for the lake problem under well-characterized uncertainty. Here, the lake problem is compiled into a standalone executable, lake.exe, configured with 100 decision variables, 4 objectives, and 1 constraint as detailed in Section 3:This example adopts nomenclature for a Windows system, but Linux and Mac systems are also supported. Specifically, executable filenames on Mac and Linux typically do not include the “.exe” extension. Furthermore, Linux and Mac commands typically need “./” prepended to the command to run an executable. Additional information provided to OpenMORDM includes the decision variable bounds (each having a range of [0, 0.1]), the name of the four performance objectives, and whether an objective is maximized or minimized. By not specifying any arguments to the executable, we are using the default baseline parameters shown in Table 1. These default baseline parameters reflect the expected future SOW under well-characterized uncertainty.Next, we generate the alternative designs for the lake problem under well-characterized uncertainty using the Borg MOEA [Hadka and Reed (2013)]. In the command below, we allow the optimization algorithm 100,000 function evaluations to find the Pareto approximate solutions:Note that we are finding Pareto approximate solutions to discover the trade-offs that exist for a baseline of the best available estimate of the future SOW. The MORDM framework recommends establishing a baseline optimization to the projected future, because optimizing to a mixture of deeply uncertain future SOWs will implicitly sacrifice attainable system performance (i.e., Pareto efficiency) to maintain feasibility across the breadth of future possibilities in a manner that will be hidden from stakeholders. Establishing a baseline enables the effects of the deep uncertainties and alternative definitions of robustness to be explored explicitly to determine if the problem needs to be re-formulated and additional uncertainties included in the multi-objective optimization step. For the lake problem example, we illustrate this process by optimizing to a simulated best available estimate of the future SOW (i.e., assuming that natural phosphorus inputs are the only well-characterized uncertainty). Subsequently, alternative robustness measures are used to explore the potential impacts of a broader suite of deep uncertainties in the system.The resulting baseline Pareto approximate set can be displayed in a three-dimensional (3D) scatter plot and a parallel coordinates plot to view the trade-offs between performance objectives. We also define a color palette for the graphical display of reliability that is arguably more intuitive than the default.The result of these commands is shown in Fig. 3. In the 3D scatter plot displayed in Fig. 3(a), the three spatial axes display the phosphorus in the lake, economic benefit, and reliability and the glyphs (the spherical markers displayed in the scatter plot) are color-coded to represent the range of reliabilities, where green indicates 100% reliability and red indicates 86% reliability. Each point in the scatter plot represents a pollution control strategy (a set of decision variables at) that are approximately Pareto-optimal. Recall that since the problem formulation defines the constraint freliability>0.85, no solutions with lower reliability are shown in the plot. This constraint threshold is built into the model (i.e., lake.exe). Each glyph in the plot represents the alternative lake pollution control strategies that compose the trade-offs facing decision makers. The parallel coordinates plot presented in Fig. 3(b) is an alternate visual representation of high-dimensional datasets, where each line represents an alternative lake pollution control strategy. The location where the line intersects each vertical axis designates the relative objective value. Note that the ideal point and ideal axis values are indicated by a star in both figures.The negotiated selection of solution(s) for further analysis or implementation requires diverse stakeholders to agree upon the levels of compromise between the strategies. The 3D scatter plot and parallel coordinates plot are useful for visualizing these trade-offs. Stakeholders can observe how performance objectives are affected, either positively or negatively, by their decisions. For instance, we observe in Fig. 3 that phosphorus in the lake is positively correlated with economic benefit. A stakeholder pursuing increased economic activity must accept that the pollution in the lake increases proportionally. The results show that phosphorus in the lake is inversely correlated with reliability. If a stakeholder is risk averse (i.e., they desire high reliability), then tighter pollution control strategies are required. Consequently, such restrictive pollution control strategies also dampen economic activity. This form of analysis has two advantages: (1) by exploring the effects of trade-offs, the stakeholders learn the dependencies between their decisions and system performance; and (2) by comparing and contrasting alternative designs, the stakeholders discover the cost-benefit compromise offered by various strategies and avoid myopic decisions.There are several ways to explore these solutions in detail, ultimately identifying one or several designs of interest. If the stakeholder articulates preferences, one could use a weighting scheme to identify the most-preferred designs. If instead there are specific thresholds in performance required by the stakeholder, one can use “brushing” to eliminate all solutions that fail to meet those thresholds [Kollat and Reed (2007)]. OpenMORDM supports both methods. One easy way to access these controls is through the web-based visualization toolkit, which provides slider widgets for specifying the preferences or brushing limits (see the documentation for the explore command). Alternatively, preferences can be specified through R commands:The result of these commands is shown in Fig. 4. Solutions are now color-coded by the preference weighting with green (in the web version) indicating the most preferred designs. In this example, the stakeholder prefers maximizing economic benefit and reliability (both with a weight of 0.325) with the other two performance objectives having less significance. Based on these weights, the preferred design can be identified and displayed in the plot as the enlarged glyph. It should be noted that this weighted preference is imposed a posteriori after visualizing the full set of Pareto-approximate alternatives; methods such as Multi-Criteria Decision Making (MCDM) [Triantaphyllou (2000)] requiring a priori preference weighting would produce only a single solution, not permitting an interactive decision process with evolving stakeholder preferences and increase the risk of myopic analysis.In many cases, it is also desirable to see the decision variables (i.e., control policies) present in select designs. The underlying data for each design can be extracted with the following command:or specific decision variables can be selected with:In this example, OpenMORDM assigns default variable names of the form “Var1”, “Var2”, and so on, but user-defined names can also be assigned in the define.problem method. This data, which is stored as an R matrix, can subsequently be plotted by standard R functions.This simple first pass of the analysis assumes that the uncertainties are well characterized. In contrast, the uncertainties surrounding real-world environmental systems are often poorly characterized. The value of certain model inputs may be unknown or only known to lie within a range of values. Or the probability distributions representing the uncertainty of key parameters may be deeply uncertain. For example, testing the assumptions about the extreme values of the distribution of the natural inputs of phosphorus into the lake requires long observations that may not be available. As another example, consider the task of projecting the discount rate on a century time scale (e.g., Newell and Pizer (2003)). In both cases, adopting a probability density function is a deeply uncertain choice.As discussed in Section 2, MORDM is a decision support framework for identifying the vulnerabilities, dependencies, and trade-offs in systems with deep uncertainties. Table 2shows the uncertain parameters now modeled under deep uncertainty. Since we are deeply uncertain of the values for these parameters, we select a range of plausible values for each uncertainty from which we sample using a Latin hypercube strategy. Below, we create 1000 randomly-generated Latin hypercube samples.The output from randomLHS is a 1000×5 matrix (1000 rows and 5 columns) with values on the range [0, 1]. These values need to be scaled to the sample range for each uncertain parameter shown in Table 2. The next line, invoking t(apply (…)), applies the correct scaling to each row in the matrix. The result of apply (…) is transposed, so we use the transpose operator, t, to convert the matrix back into the appropriate form. For convenience, we also assign the column names to factors, which will appear as labels in any plots generated by OpenMORDM.Next, for each uncertainty parameterization, we define a new problem formulation by passing the parameters to the executable as command-line arguments.After these two commands are invoked, models stores a list of problem formulations, each of which models one uncertainty parameterization from factors.Finally, we subject the designs discovered previously under well-characterized uncertainty to the deeply-uncertain parameterizations. The performance of designs across SOWs can be used to compute robustness measures defined in Section 2. These regret-based and satisficing-based measures emphasize different goals (e.g., to minimize deviation from performance in the expected future, or to maximize the number of plausible futures in which some criteria are met). The robustness measure may change the design recommendation, so this choice must align with stakeholder preferences [Herman et al. (2015)]. All four of these robustness measures are provided by OpenMORDM. For the satisficing measures, we define the satisficing function with two criteria: (1) satisfy the reliability constraint such that the sum of constraint values is 0; and (2) preserve a minimum economic benefit of 0.1.Once the calculations to evaluate the selected robustness measures are complete, we can then update the 3D scatter plot to color-code each glyph with the robustness measure:Fig. 5shows the result of these commands. Each panel in the figure, labeled (a)-(d), depicts the glyphs colored by the respective robustness measure. Glyphs colored green indicate relatively strong robustness while red glyphs indicate poor robustness. Fig. 5 illustrates several key arguments, as discussed below.First, the two regret-based measures in panels (a) and (b) identify opposite regions with strong and poor robustness. Solutions with lower phosphorus in the lake have poor robustness under Regret Type I but have superior robustness under Regret Type II. Recall that Regret Type I measures the performance deviation from the baseline SOW whereas Regret Type II is the deviation from the best performance within the current SOW. Regret Type I is a weaker indicator of robustness because we expect solutions with more restrictive pollution control strategies (i.e., less phosphorus in the lake) to be more robust. It is instead identifying the solutions with more “room to fail”. Solutions with high levels of phosphorus in the lake in the baseline SOW are already struggling, resulting in less impact from the addition of deep uncertainties. Regret Type II, which measures the performance deviation from the best performance within the current SOW, identifies that solutions with more restrictive pollution control strategies perform better across all SOWs.Second, Regret Type II in panel (b) and Satisficing Type I in panel (c) show similar regions with strong robustness. The left-most region corresponding to more restrictive pollution control strategies offers superior robustness. This result is interesting since Regret Type II and Satisficing Type I measure different criteria. Regret Type II is based on the deviation in performance, whereas Satisficing Type I measures the fraction of SOWs that satisfy the user's criteria. Observing similarities between these two robustness measures is consistent with the hypothesis that restrictive pollution control strategies are more robust to deep uncertainties.Third, we observe that the results in panels (a)-(c) all show that the ideal design identified earlier in Fig. 4 exhibits inferior robustness. Had we assumed well-characterized uncertainties, then the town would potentially implement a risky pollution control strategy that has a higher chance of polluting the lake. By conducting a broader experiment and exploring the deeply uncertain states, we can suggest solutions that are resilient to deeply uncertain futures. Beyond this simple illustrative example, MORDM is a constructive learning process where it would be expected that concerns raised in the robustness analysis may require the generation and exploration of new multi-objective problem formulations with new decisions, changes in the modeled uncertainties, and potentially modified objectives or constraints [Kasprzyk et al. (2012), Kasprzyk et al. (2013)].One additional advantage of this analysis is the ability to quantify changes in robustness with performance trade-offs. For example, there is a trade-off between phosphorus in the lake and economic benefit. Figures (a)-(c) allows the stakeholders to assess degradation to any objectives sacrificed to achieve stronger robustness. Robustness Type I in panel (a) suggests severe restrictions on economic activity are required. Robustness Type II and Satisficing Type II in panels (b) and (c), respectively, are more relaxed, allowing higher phosphorus levels. As demonstrated above, the choice of robustness measure can impact the decision making process. OpenMORDM provides tools to experiment with different robustness criteria. Understanding how different robustness criteria interact can help to gain a deeper understanding of the ramifications of a decision analysis.In Fig. 5, the Satisficing Type II result in panel (d) shows all designs exhibit poor robustness (i.e., all solutions colored red). Recall that Satisficing Type II measures the maximum allowable uncertainty as a distance from the expected future SOW while still satisfying the criteria (see equation (4) in section 2.2). In this case, the SOW causing failure of all solutions is b=0.32, q=2.04, μ = 0.017, σ = 0.0016, and δ = 0.98. This SOW unveils a severe vulnerability causing all solutions to pollute the lake. This severe vulnerability was not detected by the other robustness measures considered in this study since their averaging tends to hide outliers. Again, this demonstrates the need to consider multiple robustness measures in an analysis. Such a result is an indication that further investigation of the system vulnerabilities is required.To narrow the scope of the vulnerability analysis, we select a single solution for further analysis. Nevertheless, it is common in practice to explore several alternative designs at this stage. Clicking on a point using the middle mouse button within the 3D scatter plot of OpenMORDM prints the id (row number) of the selected point. On computers lacking a middle mouse button, run the command mordm.identify (button = ?) replacing ? with 1 to enable clicking with the left button or 2 for the right mouse button. In this example, we have selected the point in Fig. 5 as indicated by the enlarged glyph. This point was selected based on the strong robustness performance shown in panels (a)-(c) of Fig. 5. For our particular dataset, the selected point is #274:The poor robustness indicated by Satisficing Type II suggest there exists key vulnerabilities leading to failures — the user-specified criteria is not satisfied under all future SOWs — but detailed analysis using MORDM can reveal the factors causing these vulnerabilities. Two methods for exploring these vulnerabilities are PRIM and CART, as introduced in Section 2. PRIM is the method often used in RDM and MORDM (see, for example, Lempert et al. (2008), Kasprzyk et al. (2013), and Herman et al. (2014)). Both PRIM and CART support a posteriori scenario discovery [Lempert et al. (2008)] to identify the combinations of deeply uncertain factors to which a system is vulnerable. Here we use the ease of switching operators within the OpenMORDM toolbox to compare and contrast the efficacy of PRIM and CART in extracting salient vulnerabilities for the lake problem.Both PRIM and CART succeed in identifying the key drivers of vulnerabilities. Fig. 6shows the vulnerability results using PRIM resulting from the following command:In this example, we are defining vulnerable states as those with low reliability. Recall that reliability measures the fraction of years that the phosphorus level in the lake remains below the critical threshold, Xcrit. The argument threshold.type = −1 specifies that the PRIM algorithm identifies ranges of the deeply uncertain parameters leading to below average reliability (by default the threshold is the average response value, a specific threshold can be defined using the threshold = t argument). Each of the vertical bars in Fig. 6 corresponds to one of the uncertainty parameterizations in Table 2. The red overlaid boxes represent the ranges of each parameter that lead to vulnerabilities. If all uncertainty parameters fall within the red boxes, then there is a greater chance that the lake will fail to satisfy the stakeholders' criteria. Fig. 6 illustrates the bounds for each deeply uncertain factor that are of concern. For this example, low values of b and q and moderate to large values of μ are identified is critical factors. Note that the red boxes covering the entire range of σ and δ in Fig. 6 indicates that these two factors are not significant with respect to vulnerabilities — any value of σ or δ will lead to vulnerabilities as long as the uncertainties fall within the critical factor's bounds (b, q, and μ).PRIM enables the user to influence the generated boxes by specifying the desired coverage, the percentage of all vulnerable states contained within the box, and density, the fraction of states within the box that are vulnerable. In the example shown in Fig. 6, the coverage is 76% and density is 33%. The prim package, used in this study, is not interactive but can be controlled using optional arguments. In this study, we kept the default arguments. Alternatively, the sdtoolkit PRIM implementation allows the user to interactively see the tradeoff between coverage and density and select a PRIM box with the desired attributes. Due to this tension between coverage and density, the user must often sacrifice density to gain better coverage, or reduce coverage to improve density. This increases the burden of using PRIM since the user must choose the appropriate tradeoff between coverage and density.Similarly, Fig. 7quantifies how CART identifies key drivers of vulnerabilities. Fig. 7 is generated by the following command.In this example, we partition solutions into two classes: Polluted and Unpolluted. A solution is Unpolluted if the lake has 100% reliability, meaning the phosphorus level remains below the critical threshold, Xcrit, in all SOWs. Otherwise, the lake is classified as Polluted, meaning the lake becomes eutrophic in at least one SOW. As shown in Fig. 7, CART produces a decision tree where every node is a recursive partition of the uncertainty space. Below each node is a rule, such as b<0.26, that further divides the partition. Each partition is color-coded either red or blue (in the web version) if the partition identifies polluted (vulnerable) or unpolluted states, respectively. CART recursively partitions the space to minimize Type 1 errors. The depth of each uncertainty parameter in the tree (and note that an uncertainty can appear multiple times) is informative of the significance of each uncertainty in classifying vulnerabilities. Uncertainties nearer to the root offer more predictive power in identifying vulnerabilities.Both PRIM and CART are consistent in the key vulnerabilities identified. From Figs. 6 and 7, we learn that b and q are the most critical uncertainties. In order to avoid vulnerable states, small values of b and q should be avoided. Since b and q are uncontrollable properties of the lake in our problem formulation, these observations suggest ex post monitoring recommendations for the town's inhabitants. The optimal pollution control strategies devised in this exercise are satisfactory for most conditions, except for when b and q are small. μ, the mean of natural pollution inflows, has a less significant effect. σ and δ have no significant effect in this case, indicated in PRIM by a hyperbox spanning the entire range of uncertainties and in CART by the parameter not appearing in any partitioning rule.Since CART generates a decision tree, we can identify partitions that provide a desired misclassification rate and derive specific ex post monitoring recommendations. For example, following the entire right-branch of the tree, if the town's inhabitants can monitor the system to ensure that b ≥0.26 and q ≥2.2, then only 8% of the remaining SOWs would be vulnerable to failure. Further restrictions can reduce the failure rate. Ensuring b ≥0.33 and q ≥3.0 results in a 2% failure rate. If the system properties violate these monitoring recommendations, then further MORDM analysis and problem reformulations may be necessary to discover satisfactory designs. Furthermore, identifying the key drivers of vulnerabilities advises stakeholders where further analysis may be beneficial (i.e., conducting experiments to provide better estimates of b and q). Such monitoring recommendations are similar to the “adaptation tipping points” of dynamic adaptive policy pathways [Haasnoot et al. (2013), Kwadijk et al. (2010)], triggering a shift to alternative policies better suited for the realized SOW, a concept also developed by the Adaptive Robust Design approach [Hamarat et al. (2013)].Our analysis illustrates one limitation of PRIM. Recall from Fig. 5 that the Satisficing Type II measure indicated the existence of at least one uncertainty parameterization near the expected future state that caused all designs to fail. On further inspection of the data, there was a single parameterization causing all designs to fail with b=0.32 and q=2.04. This parameterization is correctly classified as ‘Polluted’ by CART, as shown in Fig. 7. However, PRIM fails to identify this uncertainty parameterization as vulnerable — it falls outside the hyperbox indicated in Fig. 6. For this particular application, CART has demonstrated stronger predictive power than PRIM, possibly due to CART's ability to minimize Type 1 classification error (false positives) by creating disjoint partitions of the data. On applications with more uncertain factors, this could lead to complex partitions that are difficult to interpret. PRIM aids interpretability by identifying a single contiguous region based on the desired coverage and density. Given these observations, it seems advisable to consider both PRIM and CART for the scenario discovery to assess whether their use is appropriate. These results expand on the analysis of Lempert et al. (2008) to highlight why the methodological flexibility facilitated by OpenMORDM improves the power, transparency, and value of bottom-up decision support frameworks under deep uncertainty.

@&#CONCLUSIONS@&#
