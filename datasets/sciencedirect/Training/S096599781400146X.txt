@&#MAIN-TITLE@&#
GPU-based acceleration of computations in elasticity problems solving by parametric integral equations system

@&#HIGHLIGHTS@&#
A novel (parallel) approach of numerical implementation of PIES (named GPU-accelerated PIES) is proposed.Processing power of GPU is used to accelerate computations.The accuracy of the solutions obtained using GPU-accelerated PIES was examined.Computational time of GPU-accelerated PIES was examined.The effectiveness of the serial version of PIES program and GPU-accelerated PIES was compared.

@&#KEYPHRASES@&#
Parametric integral equations system,Elasticity problems,Graphics Processing Unit,GPGPU,CUDA,Multithreaded computing,

@&#ABSTRACT@&#
Application of techniques for modelling of boundary value problems implies two conflicting requirements: obtaining high accuracy of the results and speed of the solution. Accurate results can be obtained only by using appropriate models and algorithms. In the previous papers the authors applied the parametric integral equations system (PIES) in modelling and solving boundary value problems. The first requirement was satisfied – the results were obtained with very high accuracy. This paper fulfils the second requirement by novel approach to accelerate PIES. Graphics cards (GPUs) programming for numerical calculations in general purpose applications (GPGPU) using NVIDIA CUDA is used for this purpose. The speed of calculations increased up to 80 times whereas high accuracy of the solutions was maintained. Examples included in this paper concern solving elasticity problems which are modelled by three-dimensional Navier–Lamé equations.

@&#INTRODUCTION@&#
For many years the authors of this paper have being worked on development and application of parametric integral equations system (PIES) to solve boundary value problems. PIES has already been used to solve problems modelled by 2D and 3D partial differential equations, such as: Laplace’s [1,2], Helmholtz [3] or Navier–Lamé [4–6]. The remarkable advantage of PIES, compared with traditional boundary integral equation (BIE), is direct inclusion in its mathematical formalism a shape of boundary of a considered problem [7]. The shape of boundary is generally defined using particular functions. For this purpose, the curves (eg. Bézier, B-spline, etc.) or surface patches (Coons, Bézier, and others) widely used in computer graphics, were applied in PIES. PIES is an analytical modification of traditional BIE. The above mentioned curves and surface patches are applied in modelling the shape of the boundary, instead of the contour integral as in the case of BIE. Therefore in practice, a small number of control points is required to define a shape of the boundary. This way of modelling is definitely much easier in comparison with the other methods: boundary element method (BEM) [8] or finite element method (FEM) [9]. Moreover, the accuracy of solutions can be efficiently improved without interference in modelling of a shape of boundary.The former studies focused on high accuracy and efficiency of the results obtained using PIES, which were compared to the other algorithms (FEM or BEM) as well as analytical methods. These studies confirmed the high effectiveness and accuracy of PIES in solving 2D [1,4] and 3D [2,5,6] boundary value problems. However, the problem of increasing computing time of complex numerical algorithms forced the authors’ attention, as well. In general, the greater number of input data, the longer an algorithm works. It was noticed in case of some more complex problems solved by PIES (eg. modelled by Navier–Lamé equations or increasing the dimensionality of the problem from 2D to 3D). PIES uses the strategy of global numerical integration on the entire surface patches. This integration does not require the splitting of patches on smaller elements, however it is necessary to use a large number of weight coefficients in cubatures in order to obtain high accuracy of solutions. The number of these coefficients has a significant influence on the duration of the implementation of PIES. Acceleration of the algorithm can be achieved in the following ways: by optimisation of the existing code as well as use of more advanced computers, multiprocessor machines, clusters or GPUs.Recently, many researchers in various fields have increased their attention on the implementation of graphics cards (GPUs) for numerical calculations in general-purpose applications (GPGPU). Due to great possibility of improving computing performance, scientists have already investigated application of the mentioned technology in problems in the following areas: applied mathematics [10–12], physics [13], biology and medicine [14,15], seismic research [16,17], peridynamics [18], and others. Numerical modelling and solving boundary problems were accelerated using the modern techniques of graphic cards programming adapted to BEM [19,20], FEM [21,22], finite difference method (FDM) [23] or meshless methods [24,25], as well. The interests of scientists are related to the modern architecture of graphics cards (multiprocessor and multithreaded), very fast floating-point arithmetic units, use of high-speed memory and, first of all, ease of programming. In November 2006, NVIDIA has produced the first series of graphics cards that employ a new parallel computing platform and programming model called Compute Unified Device Architecture (CUDA) [26]. It was a break-through in programming and feasibility of general-purpose applications executed on GPU. CUDA C programming language is an extension to the C/C++ and definitely simplifies writing of non-graphical programs compared with graphics-oriented languages such as Cg, GLSL or HLSL.The architecture of graphics processing units (GPUs) significantly differs from central processing units (CPUs). GPU is composed of multiple floating point units (FPU) and arithmetic and logic units (ALUs). It is connected with the nature of performed calculations – the same operations are executed in parallel on large amounts of data. Using a lot of pixels, texels or vertices is typical in graphics applications, therefore GPUs are rated as SIMT (single instruction, multiple thread).CUDA-enabled NVIDIA GPUs are multithreaded, parallel, many-core processors. They are composed of a set of multithreaded streaming multiprocessors (SMs). Each multiprocessor contains a number of general-purpose streaming processors cores (SPs), multithreaded instruction unit and on-chip shared memory. Also, it comprises a number of 32-bit registers, a texture cache and a read-only constant cache. SPs can compute one arithmetic multiply or add operation per clock cycle. In the case of multiply-and-add operation it is equivalent to two operations per clock cycle. The execution of other instructions require greater number of clock cycles (eg. square root is computed per 10 clock cycles).GPU is directly connected to a read-write off-chip device memory (DRAM). It can store a large amount of data according to the type of applied hardware (hundreds megabytes to few gigabytes). CPU exchanges data with GPU via host and device memory. The main disadvantage of device memory is its latency. SM requires about 400–600 clock cycles for access DRAM, whilst SP takes only 4 clock cycle for accessing to registers or shared memory. More detailed studies are presented in [27].GPU works in close connection with CPU – it operates as an additional processor attached to CPU. GPU performs operations assigned to it by the application that must start running on CPU. Only a part of original program, which requires paralellization, should be recoded. Serial part of CUDA application code runs on host (CPU) and parallel part on CUDA device (GPU). The set of all functions performed on host is called host program, while functions performed on device are called kernels. Host program is responsible for initiation and transfer of data to/from device memory. During execution of a program the host calls kernels, as well. Data flow in CUDA is divided into four basic steps: 1. initiation of the program (host), 2. copying data from host to device, 3. calculations on GPU, 4. copying data from device to host.In this paper, the authors decided to examine the application of modern parallel computing solutions to increase the efficiency of calculations in the numerical solution of PIES. Numerical calculations were performed for three dimensional Navier–Lamé equations using CUDA-enabled NVIDIA GPU.PIES for three-dimensional Navier–Lamé equations was obtained as the result of analytical modification of BIE. Detailed studies of the methodology of modification for two-dimensional problems modelled by various differential equations are presented in [1,4]. Generalization of the mentioned methodology to three-dimensional problems with smooth boundary results in the following formula of PIES [5,7]:(1)0.5ul(v1,w1)=∑j=1n∫v¯j-1v¯j∫w¯j-1w¯jU‾lj∗(v1,w1,v,w)pj(v,w)-P‾lj∗(v1,w1,v,w)uj(v,w)Jj(v,w)dvdw,wherev¯l-1⩽v1⩽v¯l,w¯l-1⩽w1⩽w¯l,v¯j-1⩽v⩽v¯j,w¯j-1⩽w⩽w¯j,{l,j}=1,2,…,n,n– is the number of parametric patches that create the domain boundary in 3D, whilst functionJj(v,w)is the Jacobian.IntegrandsU‾lj∗(v1,w1,v,w)andP‾lj∗(v1,w1,v,w)in (1) are presented in the following matrix form [5,7]:(2)U‾lj∗(v1,w1,v,w)=116π(1-ν)μηU11U12U13U21U22U23U31U32U33,μ=E2(1+ν),(3)P‾lj∗(v1,w1,v,w)=18π(1-ν)η2P11P12P13P21P22P23P31P32P33.The individual elements in the matrix (2) in an explicit form are presented as follows:U11=3-4ν+η12η2,U12=η1η2η2,U13=η1η3η2,U21=η2η1η2,U22=3-4ν+η22η2,U23=η2η3η2,U31=η3η1η2,U32=η3η2η2,U33=3-4ν+η32η2,while in the matrix (3):P11=1-2ν+3η12η2∂η∂n,P12=3η1η2η2∂η∂n-1-2νη1n2-η2n1η,P13=3η1η3η2∂η∂n-1-2νη1n3-η3n1η,P21=3η2η1η2∂η∂n-1-2νη2n1-η1n2η,P22=1-2ν+3η22η2∂η∂n,P23=3η2η3η2∂η∂n-1-2νη2n3-η3n2η,P31=3η3η1η2∂η∂n-1-2νη3n1-η1n3η,P32=3η3η2η2∂η∂n-1-2νη3n2-η2n3η,P33=1-2ν+3η32η2∂η∂n,whereinn1≡n1(v,w),n2≡n2(v,w),n3≡n3(v,w)are the components ofnj– the normal vector to the surface j. Parametersνand E are material constants: Poisson’s ratio and Young modulus respectively.Integrands (2) and (3) include in their mathematical formalism the shape of a closed boundary surface. It is created using appropriate relationships between patches({l,j}=1,2,…,n), which are defined in Cartesian coordinates system as follows:(4)η1=Pj(1)(v,w)-Pl(1)(v1,w1),η2=Pj(2)(v,w)-Pl(2)(v1,w1),η3=Pj(3)(v,w)-Pl(3)(v1,w1),η=η12+η22+η32,∂η∂n=η1ηn1+η2ηn2+η3ηn3,wherePj(1),Pj(2),Pj(3)are the scalar components of the vector surfacePj(v,w)=Pj(1)(v,w),Pj(2)(v,w),Pj(3)(v,w)Twhich depends onv,w. This notation is also valid for the surface marked by l with parametersv1,w1, i.e. forj=land parametersv=v1andw=w1.In 3D problems modelled using PIES, vector functionsPj(v,w)have the form of parametric surface patches widely applied in computer graphics. The main advantage of the presented approach, compared with BIE, is an analytical inclusion of the boundary directly in PIES. It results in a lack of discretization. In traditional BIE boundary is not included in its mathematical formalism and it is generally defined by boundary (contour) integral. Therefore, discretization of boundary into elements is required, as is the case of BEM. The advantages of including the boundary directly in mathematical equations (PIES) were widely presented in two dimensional [1,4] and three dimensional [2,5] problems.The application of PIES for solving 2D and 3D problems allows to eliminate discretization of boundary, as well as boundary functions. In the previous studies, the boundary functions took a form of approximating series with Chebyshev polynomials as base functions [5]. Unlike the problems modelled by Laplace’s equation, the ones described by the Navier–Lamé equations require boundary functions in a vector form. Approximating series represent scalar components of vectors of displacementsuj(v,w)and stressespj(v,w)for each surface patch j and they are presented as follows:(5)uj(v,w)=∑p=0N-1∑r=0M-1uj(pr)Lj(p)(v)Lj(r)(w),(6)pj(v,w)=∑p=0N-1∑r=0M-1pj(pr)Lj(p)(v)Lj(r)(w),whereuj(pr),pj(pr)are unknown coefficients, whereas the base functionsLj(p)(v),Lj(r)(w)have the form of Lagrange polynomials:(7)Lj(p)(v)=(v-v0)(v-v1)…(v-vi-1)(v-vi+1)…(v-vN-1)(vj-v0)(vj-v1)…(vj-vi-1)(vj-vi+1)…(vj-vN-1),Lj(r)(w)=(w-w0)(w-w1)…(w-wi-1)(w-wi+1)…(w-wM-1)(wj-w0)(wj-w1)…(wj-wi-1)(wj-wi+1)…(wj-wM-1),where N is the number of terms in approximating series (5) and (6) in direction of coordinate axis v, while M is the number of terms in approximating series (5) and (6) in direction of coordinate axis w on the surface patch j.After substituting (5) and (6) to (1) the following expression is obtained:(8)0.5ul(v1,w1)∑p=0N-1∑r=0M-1Ll(p)(v1)Ll(r)(w1)=∑j=1n∑p=0N-1∑r=0M-1pj(pr)∫v¯j-1v¯j∫w¯j-1w¯jU‾lj∗(v1,w1,v,w)-uj(pr)∫v¯j-1v¯j∫w¯j-1w¯jP‾lj∗(v1,w1,v,w)Lj(p)(v)Lj(r)(w)Jj(v,w)dvdwwherel=1,2,…,n.Similar to the previous researches [2,5,7], the pseudospectral method [28] was applied to numerical solving of PIES. An algebraic version of PIES is obtained by application of collocation points(n¯j=M·N)in the domain of individual patches defining the boundary to (8). It can be presented in the matrix form:(9)[H]{u}=[G]{p}.Submatriceshljandgljof matrices H and G are calculated using the following expressions:(10)[hlj]=0.5δlj∑p=0N-1∑r=0M-1Ll(p)v1(t)Ll(r)w1(t)+∑p=0N-1∑r=0M-1∫v¯j-1v¯j∫w¯j-1w¯jP‾lj∗v1(t),w1(t),v,wLj(p)(v)Lj(r)(w)Jj(v,w)dvdw(11)[glj]=∑p=0N-1∑r=0M-1∫v¯j-1v¯j∫w¯j-1w¯jU‾lj∗v1(t),w1(t),v,wLj(p)(v)Lj(r)(w)Jj(v,w)dvdwwhere{l,j}=1,2,…,n,nis the number of surface patches,v1(t),w1(t)are coordinates of t-th collocation point,t=1,2,…,K,K=∑j=1nn¯jis the number of collocation points andv¯j-1<v1(t)<v¯j,w¯j-1<w1(t)<w¯j.The integrandsU‾lj∗(v1(t),w1(t),v,w)andP‾lj∗(v1(t),w1(t),v,w)are given respectively as (2) and (3).Finally, an algebraic version of PIES (9) is transformed into system of algebraic equationsAx=b. To solve the system, Gaussian elimination with partial pivoting and iterative refinement is used. After solving the system solutions on the boundary are obtained.Algorithm 1Creating and solving PIESRequire:n – the number of parametric patches that creates boundaryN – the number of terms in approximating series (5) and (6) in direction of coordinate axis vM – the number of terms in approximating series (5) and (6) in direction of coordinate axis wK – the number of collocation pointsGv– the number of cubature nodes in direction of coordinate axis vGw– the number of cubature nodes in direction of coordinate axis w1: for1⩽l⩽ndo2:for1⩽j⩽ndo3://start of calculation[glj],[hlj]from Eqs. (10) and (11)4:for0⩽p⩽N-1do5:for0⩽r⩽M-1do6://start of integration from Eqs. (10) and (11)7:for1⩽kv⩽Gvdo8:for1⩽kw⩽Gwdo9:compute:Pj(1)(v(kv),w(kw)),Pj(2)(v(kv),w(kw)),Pj(3)(v(kv),w(kw))Jj(v(kv),w(kw)),Lj(p)(v(kv)),Lj(r)(w(kw))10:for1⩽t⩽Kdo11:compute:Pl(1)(v1(t),w1(t)),Pl(2)(v1(t),w1(t)),Pl(3)(v1(t),w1(t))η1,η2,η3,η,U11,…,U33,P11,…,P33U‾lj∗(v1(t),w1(t),v(kv),w(kw)),P‾lj∗(v1(t),w1(t),v(kv),w(kw))12:endfor13:endfor14:endfor15:endfor16:endfor17:submatrices[glj]and[hlj]found18:insert[glj]into G and[hlj]into H19:endfor20: endfor21: transformHu=GpintoAx=b22: solveAx=b23: solutions on the boundary{x}foundThe way of creating and solving serial version of PIES is shown in Algorithm 1. Generally, the algorithm is proceed as follows. Integrals from (10) and (11) are calculated using Gauss–Legendre cubature (lines between 7 and 14). In order to find integrands (2) and (3), the variables in lines 9 and 11 are calculated for each cubature node (in line 11 also for each collocation point). After calculation of sums in (10) and (11) (for loops in lines 4 and 5) submatrices[glj]and[hlj]are found and inserted into G and H. All computations are performed repeatedly between respective patches l and j (for loops in lines 1 and 2). Next step is transformation of PIES (9) into a system of algebraic equations (line 21) and solving the system (line 22). Finally, solutions on the boundary are obtained (line 23).The algorithm is not well-suited to GPU implementation in a direct form. The most important problem is connected with loading the needed data for each variables (in lines 9 and 11). In case of GPU implementation it will be connected with necessity of continuous data exchange between host and device (global) memory. In this case the global memory accesses are not coalesced, that is the memory accesses are not a single transaction. Also, a part of computations is repeatedly executed in some iterations.Expressions (10) and (11) indicate, that the number of elements in matricesHandGis strictly connected with the number of patches defining the shape of the boundary and the number of terms in approximating series (5) and (6). The elements of matrices are obtained as a result of numerical integration, therefore an efficient process of integration is very important.All integrals in PIES are numerically calculated using the same Gauss–Legendre cubature with different values of integrands. It perfectly fits the SIMT paradigm – the same instructions during the calculation of the cubature can be processed concurrently on a large amount of data. Hence, the calculation of the integrands in expressions (10) and (11) is made for the all collocation points on the boundary and it may be performed concurrently. The integration has become definitely the most time-consuming operation, therefore in this study we decided to accelerate the calculation of integrals using GPU.After solving PIES, only solutions on boundary are obtained. They are represented by approximating series (5) or (6). In order to obtain solutions in the domain, analytical modification of integral identity from BIE is required. New integral identity is formulated in the same way as in two dimensional problems [1,4]. The identity uses the solutions of PIES and it takes the following form [5,7]:(12)u(x)=∑j=1n∫v¯j-1v¯j∫w¯j-1w¯jU‾^j(x,v,w)pj(v,w)-P‾^j(x,v,w)uj(v,w)Jj(v,w)dvdwIntegrands in identity (12) are presented in the following form:(13)U‾^j∗(x,v,w)=116π(1-ν)μrU^11U^12U^13U^21U^22U^23U^31U^32U^33,μ=E2(1+ν),(14)P‾^j∗(x,v,w)=18π(1-ν)r2P^11P^12P^13P^21P^22P^23P^31P^32P^33.The individual elements in the matrix (13) in the explicit form are presented as follows:U^11=3-4ν+r12r2,U^12=r1r2r2,U^13=r1r3r2,U^21=r2r1r2,U^22=3-4ν+r22r2,U^23=r2r3r2,U^31=r3r1r2,U^32=r3r2r2,U^33=3-4ν+r32r2,while in the matrix (14):P^11=1-2ν+3r12r2∂r∂n,P^12=3r1r2r2∂r∂n-1-2νr1n2-r2n1r,P^13=3r1r3r2∂r∂n-1-2νr1n3-r3n1r,P^21=3r2r1r2∂r∂n-1-2νr2n1-r1n2r,P^22=1-2ν+3r22r2∂r∂n,P^23=3r2r3r2∂r∂n-1-2νr2n3-r3n2r,P^31=3r3r1r2∂r∂n-1-2νr3n1-r1n3r,P^32=3r3r2r2∂r∂n-1-2νr3n2-r2n3r,P^33=1-2ν+3r32r2∂r∂n,wherer1=Pj(1)(v,w)-x1,r2=Pj(2)(v,w)-x2,r3=Pj(3)(v,w)-x3,r=r12+r22+r32,∂r∂n=r1rn1+r2rn2+r3rn3.Integrands (13) and (14) in identity (12) are very similar to functions (2) and (3). The main difference is that functions (13) and (14) additionally require coordinates of pointsx≡{x1,x2,x3}in the domain where solutions are searched.The way of finding solutions in the domain in serial version of PIES is shown in Algorithm 2. The algorithm is proceed as follows. Integrals from (12) are calculated using Gauss–Legendre cubature (lines between 4 and 11). In order to find integrands (13) and (14) (line 8), variables in lines 6 and 8 are calculated for each cubature node (in line 8 also for each collocation point). After calculation of sum in (12) (for loop in line 2) solution in particular searching point in domain is found. All computations are performed repeatedly for all searching points (for loop in line 1). Finally, solutions in the domain are obtained (line 16).Similar problems and advantages to ones found during creating and solving serial PIES (Algorithm 1) occur here. The calculation of the integrands in expressions (13) and (14) might be performed concurrently, as well.Algorithm 2Finding solutions in the domainRequire:n – the number of parametric patches that creates boundaryN – the number of points, where solutions are searchedK – the number of collocation pointsGv– the number of cubature nodes in direction of coordinate axis vGw– the number of cubature nodes in direction of coordinate axis wx– solutions on the boundary1: for1⩽l⩽Ndo2:for1⩽j⩽ndo3:for1⩽kv⩽Gvdo4:for1⩽kw⩽Gwdo5:compute:Pj(1)(v(kv),w(kw)),Pj(2)(v(kv),w(kw)),Pj(3)(v(kv),w(kw)),Jj(v(kv),w(kw)),Lj(p)(vkv)),Lj(r)(wkw))r1,r2,r3,r,U^11,…,U^33,P^11,…,P^336:for1⩽t⩽Kdo7:U‾^j∗(x,v(kv),w(kw)),P‾^j∗(x,v(kv),w(kw))8:endfor9:endfor10:endfor11:endfor12:resultulin l-th solution point is found13:insertulinu14: endfor15: solutionsuin all points in the domain are foundThe numerical implementation of PIES in CUDA C language was named as GPU-accelerated PIES. As mentioned previously, the serial version of PIES algorithm is not well-suited to GPU implementation in direct form. To address the challenges, we propose to make set-up stage performed directly in GPU (shown in Algorithm 3).Algorithm 3Set-up stage of GPU-accelerated PIESRequire:send to GPU memory:vc,wc– coordinates of all control points (matrix form)v1,w1– coordinates of all collocation points (matrix form)n – the number of parametric patches that creates boundaryN – the number of terms in approximating series (5) and (6) in direction of coordinate axis vM – the number of terms in approximating series (5) and (6) in direction of coordinate axis wK – the number of collocation pointsGv– the number of cubature nodes in direction of coordinate axis vGw– the number of cubature nodes in direction of coordinate axis w1: for1⩽j⩽ndo2: computePj≪<blks,thrs≫>(vc,wc,n,Gv,Gw,K,Pj(1),Pj(2),Pj(3))3: computeJj≪<blks,thrs≫>(vc,wc,n,Gv,Gw,K,Jj)4: computeLj≪<blks,thrs≫>(vc,wc,n,Gv,Gw,N,M,Lj(p),Lj(r))5: compute_n≪<blks,thrs≫>(vc,wc,n,Gv,Gw,n1,n2,n3,n)6: endfor7: for1⩽l⩽Kdo8: computePl≪<blks,thrs≫>(v1,w1,n,Gv,Gw,K,Pl(1),Pl(2),Pl(3))9: endforIn set-up stage appropriate input data are sent to GPU. Next, all kernels compute matrices of particular variables (marked in bold). These matrices are calculated once. Later, they are directly used in creating and solving GPU-accelerated PIES (Algorithm 4) and during finding solutions in the domain (Algorithm 5). Elements of each matrix are calculated concurrently. Variables blks and thrs mean respectively the number of blocks and threads. The meaning of a thread on GPU is not the same as on CPU. GPU thread is a basic element of the data to be processed. Threads should be running in groups of at least 32 (known as warp) for the best performance. The total number of threads is numbered in thousands. A block in CUDA contains 64–512 threads. Blocks allow for direct manipulation of a number of threads, which makes programming of GPU easier.The way of creating and solving GPU-accelerated version of PIES is shown in Algorithm 4.Algorithm 4Creating and solving GPU-accelerated PIESRequire:n – the number of parametric patches that creates boundaryK – the number of collocation pointsGv– the number of cubature nodes in direction of coordinate axis vGw– the number of cubature nodes in direction of coordinate axis wPj(1),Pj(2),Pj(3),JjPl(1),Pl(2),Pl(3)n1,n2,n3,nLj(p),Lj(r)1: for1⩽l⩽ndo2:for1⩽j⩽ndo3: compute_η≪<blks,thrs≫>(Pj(1),Pj(2),Pj(3),Pl(1),Pl(2),Pl(3),Gv,Gw,K,η1,η2,η3,η)4: compute_glj≪<blks,thrs≫>(η1,η2,η3,η,n1,n2,n3,n,Lj(p),Lj(r),Gv,Gw,K,glj)5: compute_hlj≪<blks,thrs≫>(η1,η2,η3,η,n1,n2,n3,n,Lj(p),Lj(r),Gv,Gw,K,hlj)6: insert[glj]into G and[hlj]into H7:endfor8:endfor9:copy G and H to CPU memory10: //serial part of algorithm11: transformHu=GpintoAx=b12: solveAx=b13: solutions on the boundary{x}are foundThe algorithm is proceed as follows. The kernel in line 3 computes concurrently all relationships between appropriate patches (4). The next two kernels (lines 4 and 5) compute submatrices[glj]and[hlj]. All the computations are performed repeatedly between respective patches l and j (for loops in lines 1 and 2). The next part of the algorithm is executed on CPU, due to weak vulnerability to parallelization of Gaussian elimination. MatricesGandHare copied to CPU memory. Transformation of PIES (9) intoAx=band solving the system are proceed in the same way as in the serial version of PIES. Finally, solutions on the boundary{x}are obtained.To find solutions in the domain vectorxis sent to GPU memory and Algorithm 5 is executed.Algorithm 5Finding solution in the domain using GPU-accelerated PIESRequire:n – the number of parametric patches that creates boundaryN – the number of points, where solutions are searchedK – the number of collocation pointsGv– the number of cubature nodes in direction of coordinate axis vGw– the number of cubature nodes in direction of coordinate axis wPj(1),Pj(2),Pj(3),Jjx– solutions on the boundaryp(1),p(2),p(3)- coordinates of points, where solutions are searchedn1,n2,n3,nLj(p),Lj(r)1: for1⩽l⩽Ndo2:for1⩽j⩽ndo3: compute_r⋘blks,thrs⋙Pj(1),Pj(2),Pj(3),p(1),p(2),p(3),Gv,Gw,K,r1,r2,r3,r4: compute_u⋘blks,thrs⋙r1,r2,r3,r,n1,n2,n3,n,Lj(p),Lj(r),x,Gv,Gw,K,ul5:endfor6: insertulinto u7: endfor8: copy u to CPU memory9: all solutionsuin the domain are foundThe kernel in line 3 computes concurrently all relationships between appropriate patch and all points where solutions are searched. The next kernel (line 4) computes integrals from (12). The kernels are performed repeatedly for all the patches (for loop in line 2) and solutionulin l-th point in a domain is obtained. All computations are performed repeatedly for l points where solutions are searched (for loop in line 1). Finally, obtained solutions in the domainuare copied to CPU memory and written to a file.NVIDIA GeForce GTX 460 works at 1.55GHz with 1GB 256-bit GDDR5 memory (7 streamline multiprocessors each composed by 48 CUDA cores, peak performance 1045.6 Gflops) and two Intel Xeon E5507 (4 cores, 4 threads, 2.26GHz, 4MB cache memory, peak performance 36.26 GFlops) were used during tests. It uses double-precision floating-points operations. Serial version of PIES program was compiled using g++ 4.4.3 with standard settings, whilst GPU-accelerated PIES by nvcc V0.2.1221 CUDA 5.0 release with standard settings, as well. Numerical tests were carried out on 64-bit Ubuntu Linux operation system (kernel 2.6.37).From efficiency viewpoint, Gauss–Legendre cubature with 32 or 64 weight coefficients in each direction of surface patch was applied in the presented examples. It is connected with warp size.The testing example concerns the problem described by Navier–Lamé equations. The shape of the boundary is shown in Fig. 1a. The considered element is firmly fixed at the bottom and subjected to uniform normal loadp=1MPa acting along the upper part of the left side (Fig. 1c). The values of material constants selected for the calculation are Young’s modulusE=1MPa and Poisson’s ratioν=0.3. Comparative tests were carried out for the problems with different numbers of collocation points, but the same number of collocation points in domain of each patch. We considered the possibility of using 16, 25 or 36 collocation points on each patch in the tests. It was considered influence of the number of cubature weight coefficients on the results accuracy as well as computational time.The results obtained by GPU-accelerated PIES were compared with the ones from the serial version of PIES program, in order to verify accuracy. Double-precision floating-point operations were applied in both versions of PIES programs.The shape of the boundary is modelled by 7 rectangular bicubic Bézier surfaces (curved parts of the boundary) and 6 flat rectangular bilinear Coons surfaces (see Fig. 1b). Complete declaration of the boundary defined in PIES by 13 surfaces requires 112 corner and control points. Some of them are shown in Fig. 1b. On each surface patch we have defined the same number of collocation points (from 16 to 36) and finally have solved the system of 624–1404 algebraic equations.Relative error norms of displacements for each coordinate between solutions obtained by GPU-accelerated PIES and the serial one were computed, in order to verify accuracy of the results. This is connected with the lack of analytical solutions of the problem. Relative error normL2was computed using the following formula:(15)‖e‖L2=1K∑k=1Kuk-u¯k2·100%,k=1,2,3.whereukis the components of displacements obtained by GPU-accelerated PIES,u¯kthe components of displacements obtained by serial version of PIES, and K is the number of numerical solutions obtained in the domain.Table 1presents the obtained results. The tests were carried out for 16, 25 or 36 collocation points in the domain of each surface patch and 32 or 64 weight coefficients of Gauss–Legendre cubature in each direction of patch. The number of solutions obtained in the domain isK=22.The results demonstrate that GPU-accelerated PIES gives results almost identical to the serial one. Application of CUDA does not provide numerical errors which have significant influence on the accuracy of the obtained results despite the differences in hardware decoding of floating-points operations between GPU and CPU. However, it can be noticed that relative error norm grows when the number of collocation points increase. In the presented example, greater number of collocation points should be considered in order to obtain greater stability of results. However, it would significantly extend the computational time, especially in case of serial version of PIES. As relative error norms of solutions are very small, the authors resigned to inserting that kind of results.It is proved that in this example the growing number of weight coefficients from 32×32 to 64×64 does not significantly influence the accuracy of the obtained solutions. Reliability and accuracy of the results obtained by the serial version of PIES program have been verified in the previous researches [2,3,5,7].The results presented in Fig. 2concern comparison between computing time of GPU-accelerated PIES and serial version of PIES program. The tests were carried out for different numbers of collocation points, keeping the same number on each surface patch (16, 25 or 36). Additionally, different numbers (32×32 or 64×64) of Gauss–Legendre cubature weight coefficients were considered.The results in Fig. 3show, that GPU-accelerated PIES is about 30–80 times faster than the serial one. It is also noted that double number of cubature coefficients causes a quadruple increase in computational time of the serial version of PIES program. In case of GPU-accelerated PIES increasing in the duration was much smaller. It was proved that in case of greater number of weight coefficients (64×64) the speedup of calculations was significantly greater.The testing example concerns the shape of the boundary shown in Fig. 4a. The problem was modelled by Navier–Lamé equations. Boundary conditions are analytical functions, which are possessed from an exact solution of the following equations:(16)u1=2x1+x2+x32,u2=x1+2x2+x32,u3=x1+x2+2x32,The values of material constants selected for the calculation are Young’s modulusE=1MPa and Poisson’s ratioν=0.25. Similar to the former example, comparative tests were conducted for the same number of collocation points in domain of each patch. We considered possibility of using 16, 25 or 36 collocation points on each patch in the tests.The shape of the boundary is modelled by 24 rectangular and 6 triangle bicubic Bezier surfaces (curved parts of the boundary) and 6 flat rectangular bilinear Coons surfaces (Fig. 4b). The complete declaration of the boundary defined in PIES by 36 surfaces requires 236 corner and control points. Some of them are shown in Fig. 4b. On each surface we have defined the same number of collocation points (from 16 to 36) and finally have solved the system of 1728–3888 algebraic equations.Similar to the former example, relative error norm (15) were computed in order to verify the accuracy of the results. However, in this example:ukis the components of displacements obtained by GPU-accelerated PIES or serial version of PIES program, whileu¯kis the components of displacements obtained by solving analytical functions (16). The comparison of accuracy of the results obtained inK=24points spaced along lines:x=0.0,-20.0⩽y⩽-14.0,z=1.0andx=0.0,y=-20.0,1.5⩽z⩽6.5(shown in Fig. 4a) is presented in Table 2. The tests were performed for 16, 25 or 36 collocation points in the domain of each patch and 32 or 64 weight coefficients of Gauss–Legendre cubature in each direction of surface patch.Similar to the former example, the results prove that GPU-accelerated PIES gives results almost identical to the serial ones. There is not significant influence of CUDA application on the accuracy of the obtained results. It appears that the number of weight coefficients influences the accuracy of the obtained solution. It was proved that in this example the growing number of weight coefficients from 32×32 to 64×64 does not significantly influence the accuracy of the obtained solutions.The results presented in Fig. 5concern comparison between computing time of GPU-accelerated PIES and serial version of PIES program. Tests were performed for different numbers of collocation points, however the same number on each surface patch (16, 25 or 36). Additionally, different numbers (32×32 or 64×64) of Gauss–Legendre cubature weight coefficients were considered.Similar to the former example, we noted a significant speedup of GPU-accelerated PIES about 10–50 times (see in Fig. 6). It is also noted that double the number of cubature coefficients causes a quadruple increase in computation time of the serial version of PIES program. In case of GPU-accelerated PIES increasing in the duration was much smaller.

@&#CONCLUSIONS@&#
The paper presents possibility of acceleration of numerical calculations in solving three dimensional boundary problems modelled by Navier–Lamé equations. It has been achieved by implementing PIES program using multithreaded, parallel computing platform CUDA.The numerical examples show significant reduction of execution time of GPU-accelerated PIES. The speedup in comparison to single-threaded, serial version of PIES program was about 10–80. It depends on the size of the equations system generated during solving PIES and the number of weight coefficients in the applied cubature. We noted almost no difference between accuracy of solutions obtained by GPU-accelerate PIES and the serial one, although computations performed using GPU were significantly faster.We can note that the accuracy of solutions depends on the number of cubature weight coefficients based on the second example, where solutions of Navier–Lamé problem obtained using PIES were compared to analytical ones. Growing number of coefficients results in higher accuracy of solutions. However, these calculations require more time. Therefore, the use of GPU-accelerated PIES is absolutely appropriate.This paper is one of the first attempts to use CUDA in 3D boundary value problems solved by PIES. The presented technique of acceleration of calculations could be extended to the problems modelled by various equations and solving using PIES. However, the most important is the acceleration of calculations for the problems with more complex shape of the boundary.