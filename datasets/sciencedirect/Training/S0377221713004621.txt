@&#MAIN-TITLE@&#
A reduction dynamic programming algorithm for the bi-objective integer knapsack problem

@&#HIGHLIGHTS@&#
Introduce size reduction and upper bound reduction based on the core concept.Construct the mixed network consisting of items with different upper bounds.Develop an efficient dynamic programming algorithm based on the mixed network.Conduct the time complexity analysis for the reduction procedures.Conduct the time and space analysis for the developed algorithm.

@&#KEYPHRASES@&#
Multi-objective programming,Integer knapsack problem,Dynamic programming,Dominance relation,Core concept,State reduction,

@&#ABSTRACT@&#
This paper presents a backward state reduction dynamic programming algorithm for generating the exact Pareto frontier for the bi-objective integer knapsack problem. The algorithm is developed addressing a reduced problem built after applying variable fixing techniques based on the core concept. First, an approximate core is obtained by eliminating dominated items. Second, the items included in the approximate core are subject to the reduction of the upper bounds by applying a set of weighted-sum functions associated with the efficient extreme solutions of the linear relaxation of the multi-objective integer knapsack problem. Third, the items are classified according to the values of their upper bounds; items with zero upper bounds can be eliminated. Finally, the remaining items are used to form a mixed network with different upper bounds. The numerical results obtained from different types of bi-objective instances show the effectiveness of the mixed network and associated dynamic programming algorithm.

@&#INTRODUCTION@&#
The multi-objective knapsack problem (MOKP) is a natural extension of the classic knapsack problem [1,2]. In most real-world situations, decisions are often made in the presence of multiple and conflicting objectives. MOKPs can be encountered in a wide range of applications such as capital budgeting [3–5], selection of different investment projects [6–8], relocation issues arising in nature conservation [9], or pollution issues for remediation planning [10].This paper deals with the multi-objective integer knapsack problem (MOIKP). Consider a knapsack capacity, W, and a set of n items where each item i(i=1,…,n) is associated with a weight wiand r profit values,cik, one per objective k (k=1,…,r); each item i has a number of copies (only bounded by W) available. The problem consists of determining the number of copies for each item such that the overall weight does not exceed W and the total profits are maximized in a Pareto sense. The MOIKP can also be stated as a multi-objective integer linear programming model as follows:(MOIKP)vmax∑i=1nci1xi,…,∑i=1ncirxis.t.∑i=1nwixi⩽W,xj⩾0,integer,i=1,…,n,where xiare decision variables indicating the number of copies for the ith item placed in the knapsack. It is assumed that W, wi(⩽W) andcik(i=1,…,n,k=1,…,r)are positive integers.In a multi-objective optimization context, the solution process consists of finding the Pareto frontier (PF), which contains a number of non-dominated outcomes in the objective space and the corresponding set of efficient solutions in the decision space. The computational effort for identifying the PF increases as the number of objectives increases [11] because the problem of determining whether a given objective vector (outcome) is dominated or not isNP-hard [12] even though the underlying single objective optimization problem can be solved in pseudo-polynomial time. For a given number of objectives, a multi-objective optimization problem with 0–1 decision variables is much easier to handle than the problem with general integer variables due to the combinatorial nature of the problem. Therefore, algorithms for solving the 0–1 MOKP are relatively abundant when compared with algorithms designed for the MOIKP.The 0–1 MOKP as well as the MOIKP fall into the category of multi-objective integer problems. Thus, the resolution of the MOIKP can benefit from the algorithmic developments of both the 0–1 MOKP and the multi-objective integer problem. The literature in the field includes exact and approximate approaches. The former are targeted to find the exact PF while the latter aim at approximating the PF. Exact approaches include dynamic programming (DP) algorithms [11,13–18], branch-and-bound (BB) algorithms [19–22], and other exact methods as the following ones: an exact algorithm that exploits the development for multi-objective linear programs [23], a greedy optimal algorithm [24] for a special class of multi-dimensional knapsack problems with binary weights, an exact algorithm partitioning the search space [25], and algorithms [26–30] resorting to the scalarization techniques [31] such as weighted-sum method and ε-constraint method as well as a general approach for preserving the lexicographic order of efficient solutions in the decision space [32].Approximate approaches include polynomial approximation schemes [33], tailored heuristics [34–36], and metaheuristics [37–44]. The interested readers can also be referred to several comprehensive surveys [45–47] for both exact and approximate approaches.Research on multi-objective integer problems dates back to more than three decades ago when [17] proposed both the basic and the hybrid DP approach for such problems. The algorithm was developed for the MOIKP and then extended to the general multi-objective integer problem. In [48] conceptually DP network approaches for different variants of MOKP, including MOIKP, were proposed. In [11] an implementation of the generic labeling algorithm for the MOIKP and several network models were considered.The current research is mainly motivated by the two recent developments of the 0–1 MOKP. The first idea is related to the development of the core concept for the 0–1 bi-objective knapsack problem [49]. The core concept gives the direction for problem reduction. Different from the single objective case, the core of the MOKP is not determined by a single function but associated with a family of weighted-sum functions [49]. Using this, the core for the 0–1 bi-objective multi-dimensional knapsack problem [22,35] was estimated by solving the linear relaxation of the original problem using the weighted-sum method, where each weighted-sum function is associated with an efficient extreme solution (EES) (non-dominated solution).Each EES can be used to estimate a core where the subset of the variables can be fixed to 0 or 1 outside the core.The second idea is related to a new DP approach [16,50] resorting to the backward state reduction technique. The new DP algorithm is extremely efficient for conflicting instances where the profit objectives are negatively correlated. It applied a backward state reduction DP space (BRDS) generated by discarding systematically some states on the basis of the DP space for the basic DP (BDP) procedure. The DP space consists of all the states and related transitions between states in the DP process.This paper focuses on finding the exact PF of the bi-objective integer knapsack problem (BOIKP) by applying a backward state reduction technique in conjunction with problem reduction techniques based on the core concept. The contributions of the paper are summarized as follows. First, an approximate core is obtained by eliminating dominated items. Second, the items included in the approximate core are further subject to the reduction of the upper bounds by applying a family of weighted-sum functions associated with the EES of the linear relaxation of MOIKP(MOLKP). Third, the items can be classified into two categories according to the values of the upper bounds (UB): one category incudes the items with UB⩾2 and the other with UB = 1 or 0. The UB corresponds to the maximal number of copies of an item in the knapsack for the non-dominated outcomes.The items with UB=0 can be eliminated. Third, the remaining items form a mixed network with two types of upper bounds. Then a backward state reduction technique is applied and the related DP algorithm is developed. Finally, the time complexity analysis is provided for the reduction procedures and the time and space analysis is provided for the algorithm based on the mixed network.Note that solution techniques are independent of problem reduction. Any solution technique can be applied to solve the problem after reduction. However, some solution techniques have a better reaction to reduction in terms of performance improvement than others.The upper boundreduction is motivated by the fact that a simpler network structure can be applied if the number of copies for the items cannot exceed 1 in any non-dominated outcome.Here it is worth mentioning that the upper bound reduction requires a priori knowledge of the correlation relation between the weight and profit coefficients as well as the correlation relation between two profit objectives. Ref. [51] mentioned that the correlation relation between any two coefficients can be determined by a measure called correlation coefficient μ, where −1⩽μ⩽1. Ideally, two coefficients are positively (negatively) correlated if μ>(<)0 and uncorrelated if μ=0. In practice, two coefficients can be treated as uncorrelated if 0⩽∣μ∣<0.3, positively correlated if 0.8⩽μ⩽1 and negatively correlated if −1⩽μ⩽−0.8. It means that it is not difficult to obtain the correlation between any two coefficients before a further processing is conducted.The paper is organized as follows. Section 2 introduces the reduction techniques based on the core concept. Section 3 generates a mixed network with two types of upper bounds by applying a backward state reduction technique and develops the related backward state reduction DP algorithm. Section 4 reports the numerical results for the bi-objective instances. A comparison with the best DP algorithm among the four algorithmsbased on labeling methods (LDP) [11] is also presented in this section.The core for the integer knapsack problem is defined asC={1,2,…,n¯≡max{i:xi>0}}[1], assuming items are arranged according to the non-increasing order of the profit-to-weight ratio (efficiency measure). An approximate core can be obtained by eliminating dominated items, i. e., those items that are filtered by the application of the classic binary dominance relations. This is called the first reduction. For the single objective case, many dominance relations can be defined [52] and then embedded in the solution process to prune states. In this paper only the simple and the multiple dominance relations are extended to the multi-objective context. However, it is difficult to reduce the knapsack capacity for the multi-objective case because the periodicity theorem [52] used to reduce the capacity for the single objective case is hard to extend. Here the second reduction is designed to achieve the reduction of the upper bounds of the items (UB) by applying a family of weighted-sum functions associated with the EES of the MOLKP in such a way that the items can be classified according to the UB. The following notation is introduced.biProfit objective balance measure of item i,cik,k=1,…,rkth profit objective of item i,ci(λ)Profit of item i for the weighted-sum function associated with λ,d(λ)Objective value of the feasible solution for the weighted-sum function associated with λ,dk, k=1, 2kth profit objective of the original BOIKP corresponding to d(λ),ei(λ)Profit-to-weight ratio (efficiency measure) of item i for the weighted-sum function associated with λ,FiControl parameter of item i in Algorithms 2 and 3,G(λ)Set of three items with the top three efficiencies for the weighted-sum function associated with λ,G(λ,i), i=1, 2, 3ith element in G(λ),L(λ)Set of items excluding those in G(λ),R(λ)Set of items excluding G(λ,1),Uia priori upper bound of item i (⌊W/wi⌋),ui,a(λ)Upper bound of the weighted-sum function associated with λ when a copies of item i are placed in the knapsack,ui,ak,k=1,2Upper bound of the kth profit objective for the original BOIKP corresponding to ui,a(λ),Via posteriori upper bound of item i (maximum copies of item i in the knapsack for all the non-dominated outcomes),ΛSet of λs, which are used to form the weighted-sum functions for determining the EES of the linear BOIKP.Algorithm 1Procedure for the size reductionInput: {w,ck(k=1,…,r)} and nOutput:{w¯,c¯k(k=1,…,r)}andn21: Sort the items i (i=1,…,n).2:pk≔c1k,k=1,…,r.3: K≔{1}.4:l≔1;w¯l≔w1,c¯lk≔c1k,k=1,…,r.5: for (i=2 to n) do6:if (∃k such thatcik>pk) then7:pk≔cikfor all k such thatcik>pk.8:K≔K∪{i};l≔l+1;w¯l≔wi,c¯lk≔cik,k=1,…,r.9:else10:k¯≔argmin{pk-cik,k=1,…,r}.11: Check item i against j∈K withcik¯⩽cjk¯.12:if ((SDR) is not satisfied) then13:K≔K∪{i};l≔l+1;w¯l≔wi,c¯lk≔cik,k=1,…,r.14:end if15:end if16: end for17: if (l>1 and⌊w¯l/w¯1⌋>1) then18:j≔1.19:while (j⩽l−1)20:D≔∅.21:i≔j+1.22:while (i⩽l) do23:if (⌊w¯i/w¯j⌋>1and(MDR) is satisfied) then24:D≔D∪{i}.25:end if26:i≔i+1.27:end while28: Remove item i∈D and l≔l−∣D∣ if D is not empty.29:j≔j+1.30:end while31:end if32: n2≔l.Definition 1Simple dominance relationGiven two items i and j of any instance of the MOIKP, if(SDR)wi⩾wjandcik⩽cjk,k=1,…,rthen item i is dominated.Given any instance of the MOIKP and an item i, if there exists an item j with qij=⌊wi/wj⌋ such that(MDR)qijcjk⩾cik,k=1,…,rthen item i is dominated.Note that the above definitions are the direct extension of the related definitions of the single objective case (r=1) [1,52] to accommodate more than one objective (r>1). It can be seen that no restriction is imposed on the relative quantity of wiand wjin Definition 2. It means that (MDR) can be used to define the dominance relation for any two items. If qij=0, i. e., wi<wj, (MDR) means that the item with smaller weight cannot be dominated by that with larger weight. If qij=1, i. e., (2wj>) wi⩾wj, (MDR) coincides with (SDR). When these two dominance relations are applied together, (SDR) is applied first to eliminate the possible dominated item i with wi⩾wj. Then (MDR) is used to conduct further elimination to remove the possible dominated item i with ⌊wi/wj⌋>1 (refer to conditions at Steps 17 and 23 of Algorithm 1).Algorithm 1 presents the reduction procedure based on the above twodominance relations and n2 is the problem size after dominated items are eliminated. In the algorithm,{w,ck(k=1,…,r)}≔w≔(w1,…,wn),ck≔c1k,…,cnk,k=1,…,r,{w¯,c¯k(k=1,…,r)}≔w¯≔(w¯1,…,w¯n2),c¯k≔c¯1k,…,c¯n2k,k=1,…,r,n2⩽n,{w¯,c¯k(k=1,…,r)}⊆{w,ck(k=1,…,r)}.At Step 1, the items are sorted according to a non-decreasing order of weight coefficients (wi); when wi=wi+1, thenci1⩾ci+11; when wi=wi+1 andci1=ci+11,…,cik=ci+1k,k<r, thencik+1⩾ci+1k+1. At Step 2, pkis the largest value of criterion k up to now. At Step 3, K is the index set for non-dominated items. At Step 10,k¯is the criterion index where the corresponding criterion difference from its largest criterion value is the smallest. At Step 20, D is the index set for the dominated items.The item orders at Step 1 of Algorithm 1 guarantee that the items selected into set K cannot be dominated by later items. The reason why the largest criterion values pkof all k are recorded is that the item cannot be dominated if one of its criterion values is larger than the largest value of the corresponding criterion for allthe non-dominated items recordedup to now (refer to Steps 6–8 of the algorithm). The reason whyk¯is found at Step 10 is to reduce the number of items to check heuristically at Step 11 when (SDR) is applied. (SDR) is applied fromSteps 12 to 14 to obtainthe non-dominated items directlywhile (MDR) is appliedfrom Steps 17 to 31 to obtain the non-dominated items in a different manner, i. e., the dominated items were recorded first (from Steps 23 to 25) and then eliminated later (Step 28).Lemma 3The time complexity ofAlgorithm 1isO((r+1)n2), where r is number of objective criteria and n is number of items.If the operation time of an integer is τ, then the operation time of a vector(wi,ci1,…,cir)is (r+1)τ. At Step 1, the operation before the first ‘;’ is a major one. The time complexity for the best sorting algorithm for n (r+1) dimensional vectors isO((r+1)nlog((r+1)n))because it is known that the best sorting algorithm for n integers isO(nlogn)). The operations of applying (SDR) are the loop from Steps 5 to 16. The major comparison operations occur at Step 11 and at most 1+… + (n−1)=n(n−1)/2 comparisons are made. The operations of applying (MDR) consist of two loops: the outer loop from Steps 19 to 30 and the inner loop from Steps 22 to 27. The number of comparisons made is (l−1)+…+1=l(l−1)/2, where l⩽n. Thus the total number of comparisons are bounded byO(n2) and the time complexity of Algorithm 1 isO((r+1)n2).□The second reduction should be applied on the basis of the first reduction if n2⩾3. It implies that the second reduction may work inefficiently if the dominated items are not eliminated. It also means that the item orders of the first reduction are preserved. However,for the sake of simplicity, in this reduction, the notation for the reduced problem is the same as the original problem (MOIKP) even though the new notation for the reduced problem is introduced in Algorithm 1. It aims at separating the items with UB =1(0) from the items with UB⩾2. The reduction is achieved by applying a family of weighted-sum functions associated with the EES of the MOLKP sequentially. Note that the first reduction can be applied easily to the problem with r⩾2. However, for the second reduction, only the BOIKP is considered because it is related to the core concept defined for the bi-objective knapsack problem [49].Each EES is associated with a weighted-sum function, which is a single objective optimization problem formulated by a vector of weights. Here the weight vector associated with the EES is used to formulate the weighted-sum functions of the original problem. As for the BOIKP, the weighted-sum function with weight vector (λ, 1−λ), denoted as z(x,λ).(1)z(x,λ)=λz1(x)+(1-λ)z2(x),0⩽λ⩽1,wherezk(x)=∑i=1ncikxi,k=1,2. Profit ci(λ) and efficiency ei(λ) of item i associated with λ are defined as follows.(2)ci(λ)=λci1+(1-λ)ci2=ci2+ci1-ci2λ,0⩽λ⩽1.(3)ei(λ)=λci1+(1-λ)ci2wi=ci2wi+ci1-ci2wiλ,0⩽λ⩽1.Then the λ associated with the EES can be obtained by a similar procedure as in [49], starting with λ=0 and ending with λ⩽1. For a given λ, items are ordered according to a non-increasing order of ei(λ). Then items are sequentially selected into the knapsack based on the greedy principle, i. e. the number of copies of items are selected according to its theoretical upper bound ⌊W/wj⌋, as many as possible, until item θ, whereW∼/wθ<⌊W/wθ⌋andW∼=W-∑i=1θ-1wi. Item θ is called the break item. For the BOIKP, the item with the second largest ei(λ) is always the break item. The next λ is determined according to the break item θ of the current λ, λcu. Lethi=ci2/wi,gi=ci1-ci2/wi,i=1,…,n2, then(4)λ=minhi-hθgb-gi,gθ≠gi,1⩾hi-hθgθ-gi>λcu,i=1,…,n2,i≠θThe set of weighted-sum functions associated with the set of λs determined by (4) plus λ=0 is used to estimate UB.Algorithm 2Procedure for the upper bound reductionInput:N≔{1,…,n2}Output:N∞≔{i∣i∈N, Vi=Ui}, N1≔{i∣i∈N, Vi=1} and N0≔{i∣i∈N, Vi=0}1: Λ: set of λs.2: CalculateU,b.3: Set Fi≔0, i∈N (control parameter).4: Set Vi≔0.5: while (Λ≠∅) do6:λ≔Λ(1).7: Obtainc(λ),e(λ), G(λ), R(λ) and L(λ).8:V≔Algorithm 3 (b,λ,e(λ),c(λ),V,U,F, G(λ),R(λ)).9:Λ≔Λ−{λ}.10: end whileProcedure for the upper bound reductionInput: b, λ,e(λ),c(λ),V,U,F, G(λ) and R(λ)Output: V1: VG(λ,1)≔UG(λ,1).2: Calculate d(λ) based on (6) and obtain the corresponding d1 and d2.3: while (R(λ)≠∅) do4:i≔R(λ,1).5:if (Vi<Ui) then6:t≔Vi, flag≔1.7:k≔t+1 if Fi=0 and k≔t otherwise.8:while (flag=1 andk<3) do9:Calculate ui,k(λ) based on (7) and obtain the correspondingui,k1andui,k2.10:if⌊ui,k1⌋⩽d1and⌊ui,k2⌋⩽d2then11:Vi≔k−1, flag≔0.12:else13:Fi≔1.14:if (Certain conditions are satisfied) then15:k≔k+1.16:else17:Vi≔k, flag≔0.18:end if19:end if20:end while21:Vi≔t if Vi<t.22:if (k⩾3 orVi=2) then23:Vi≔Ui.24:end if25:end if26:R(λ)≔R(λ)−{i}.27: end whileAlgorithm 2 presents the upper bound reduction procedure based on a family of weighted-sum functions as just mentioned. In the algorithm, Λ refers to the set of the corresponding λs associated with the weighted-sum functions, arranged in an increasing order. Λ(1) denotes the first element of Λ. Uiand Videnote the a priori and the a posteriori upper bound of item i respectively. Ui=⌊W/wi⌋, which can be interpreted as ∞. Virefers to the maximum copies of item i in the knapsack for all the non-dominated outcomes. Algorithm 2 estimates Vifirst (Step 8) and then classifies the items according to Vi(see Output of the algorithm, where the items in N0 can be discarded). The Viis updated from one weighted-sum function to another (refer to Step 8), whereVis both an output and an input parameter of Algorithm 3.Since the purpose of estimating Viis to separate the items with UB =1 (0) from the remaining items, Vi←Uionce Vi>1 in a weighted-sum function (refer to Steps 22–24 of Algorithm 3). In the subsequent weighted-sum functions Viremains to be Uibecause of the condition at Step 5 of Algorithm 3, which specifies that only the items with Vi<Uiare subject to the upper bound checking.In addition, the role of parameterFneeds to be explained.Fis a control parameter used to controlV(refer to Step 7 of Algorithm 3). It is initialized as 0 at Step 3 of Algorithm 2 and set to 1 at Step 13 of Algorithm 3. Once it is set to 1, it cannot be reset to 0. Thus if Fi=1, it is expected that Vican be changed on the basis of the value of the previous weighted-sum function. Otherwise, Vi=Vi+1 and Viis subject to reduction according to the later reduction principles (MGRP) (Steps 10 and 11 of Algorithm 3.) The purpose of designingFis straightforward. To set UB=1 (0) of item i finally, it requires that Vi=1 (0) for all weighted-sum functions.As for upper bound reductions, note that underestimating the upper bound is not allowed while overestimating the upper bound sacrifices the solution efficiency. When in the presence of conflicting objectives, an auxiliary measure to help separation is defined as follows.(5)bi=minci1,ci2ci1-ci2,where biis called profit objective balance measure of item i. It reflects the gap (difference) between two profit objectives. The larger the gap, the less balance between two profit objectives, the smaller the measure.In the following, the discussion focuses on how to estimateVor UB for a given weighted-sum function. There are three issues. The first issue is how the reduction principles for the single objective case are extended. The second issue is how the conditions at Step 14 of Algorithm 3 are designed. The final issue is how the related parameters for the conditions are set.For a given weighted-sum function, the UB can be estimated based on the general reduction principle of the integer knapsack problem. Ref. [53] discussed the reduction principle for the bounded knapsack problem where the bounds of the items can be tightened. Both the 0–1 knapsack problem and the integer knapsack problem are a special case of the bounded knapsack problem. For the sake of simplicity, assume that 0⩽xi⩽Ui(⩾1), xi=a (integer or integer part of a fraction solutionfor the linear knapsack problem), d is the feasible solution and ui,ais the upper bound of the solution assuming xi=a for the knapsack problem. Then the general reduction principle can be stated as follows:(SGRP)when0<a<Ui,xi⩽aifui,a+1⩽dwhen0<a<Ui,xi⩾aifui,a-1⩽dwhena=0,xi=aifui,a+1⩽dwhena=Ui,xi=aifui,a-1⩽dAs for the 0–1 knapsack problem, Ui=1, the reduction principle corresponds to the last two formula for Eq. (SGRP). Ref. [54] discussed the relation between the core and the optimal solution of the linear knapsack problem and [55] developed the reduction procedure for fixing the variable taking 1 (0) for the linear knapsack problem to 1 (0) for the knapsack problem based on the last two formula of Eq. (SGRP).For the integer knapsack problem, the above reduction principle can be used to tighten the upper bounds of the items. Three items with the top three efficiencies are sufficient to estimate a good upper bound according to [1].In the following, the reduction procedure is described for a given weighted-sum function with weight λ. For each weighted-sum function, the items are arranged into two sets G(λ) and L(λ), N=G(λ)∪L(λ). Set G(λ) includes threeitems with the top three efficiencies in a non-increasing order of the efficiency measures (3), and L(λ) the remaining items (not ordered). Let G(λ,i) denote the ith element in set G(λ). All the notation related to Eq. (SGRP) is adapted (e. g., d→d(λ)). Let R(λ)=N−{G(λ,1)} and m(λ)=W modwG(λ,1). The item with the top efficiency is not subject to reduction, i. e., VG(λ,1)=UG(λ,1). The feasible solution d(λ) is calculated as follows.(6)d(λ)=UG(λ,1)cG(λ,1)(λ)+max{⌊m(λ)/wj⌋cj(λ)|j∈R(λ),wj⩽m(λ)}Note that cj(λ) is not integer. The upper bound ui,a(λ), i∈R(λ) is calculated according to [1] by adaptation.ui,a(λ)=max{U0,U¯1}W∼=W-wiaW¯=W∼modwG(λ,1)(7)W′=W¯modwG(λ,2)z″=aci(λ)z′=⌊W∼/wG(λ,1)⌋cG(λ,1)(λ)+⌊W¯/wG(λ,2)⌋cG(λ,2)(λ)U0=z″+z′+W′eG(λ,3)(λ)U¯1=z″+z′+W′+⌈(wG(λ,2)-W′)/wG(λ,1)⌉wG(λ,1)eG(λ,2)(λ)-⌈(wG(λ,2)-W′)/wG(λ,1)⌉cG(λ,1)(λ)The corresponding decision variable values are known after d(λ) and ui,a(λ) are determined. Then the feasible solution and upper bound for the original problem can be computed, refer to Eq. (1). Let d(λ)=λ d1+ (1−λ)d2 andui,a(λ)=λui,a1+(1-λ)ui,a2. The⌊ui,a1⌋and⌊ui,a2⌋can be used as the upper bounds of the first and second profit objective considering the integral requirements of the original problem. Then the bi-objective version of the reduction principle (SGRP) can be stated as follows.(MGRP)when0<a<Ui,xi⩽aif⌊ui,a+11⌋⩽d1and⌊ui,a+12⌋⩽d2when0<a<Ui,xi⩾aif⌊ui,a-11⌋⩽d1and⌊ui,a-12⌋⩽d2whena=0,xi=aif⌊ui,a+11⌋⩽d1and⌊ui,a+12⌋⩽d2whena=Ui,xi=aif⌊ui,a-11⌋⩽d1and⌊ui,a-12⌋⩽d2Now Algorithm 3 gives the procedure for the upper bound reduction according to the reduction principles (MGRP) as well as some conditions used for characterizing the efficiency of itemsfor a given weighted-sum function. In the algorithm, item i∈R(λ) is checked sequentially (Steps 3 and 4) and k is the current upper bound of item i. In the meanwhile, k is also used to control the loop at Step 8 to guarantee that the items with Vi=1(0) can be singled out. The upper bounds for a small fraction of items can be obtained directly according to the reduction formula (MGRP) (see Steps 10 and 11). The upper bounds for a large portion of items need to be determined indirectly according to Steps 13–18. Consequently, the effectiveness of the procedure is related to how to design the conditions at Step 14. These conditions are used to capture the characteristics of efficient items (see conditions (a)-(g) below). If the conditions are satisfied, then the upper bound of the item can potentiallyincrease (See Step 15) because more copies of the item can be selected into the knapsack for the non-dominated outcomes. Otherwise, the upper bound of the item remains unchanged (See Step 17). The numerical experiments proposed for randomly generated instances showed that correlated (weight and profit coefficients are positively correlated) conflicting instances (two profit objectives are negatively correlated) need additional sophisticated conditions on the basis of the conditions for the other instances. For a given instance, the correlation between weight and profit coefficients as well as the correlation between two profit objectives can be determined by a measure called correlation coefficient as discussed by [51] and reviewed in the introduction section.Therefore, it is not difficult to single out correlated conflicting instances before the reduction procedure starts. The following conditions are designed for our procedure.(a)i=G(λ,3)andeG(λ,2)(λ)−eG(λ,3)(λ)⩽ε1andVG(λ,2)=UG(λ,2).i=G(λ,2)andeG(λ,2)(λ)−eG(λ,3)(λ)⩽ε1and VG(λ,3)=UG(λ,3).i∈G(λ)−{G(λ,1)},ui,k(λ)>d(λ)andbi∈[ε2, ε3].(wi=w1or|wi-w1|⩽β1andwn2/wi⩾β2)andui,k(λ)>d(λ).i∈L(λ), ui,k(λ)>d(λ)andeG(λ,1)(λ)−ei(λ)⩽ε4.i∈L(λ), ui,k(λ)>d(λ)andeG(λ,1)(λ)−eG(λ,2)(λ)⩽ε5andVG(λ,2)=UG(λ,2)andeG(λ,1)(λ)−ei(λ)⩽ε6.i∈L(λ), ui,k(λ)>d(λ)andbi∈[ε2,ε3]andeG(λ,2)(λ)−ei(λ)⩽ε7andVG(λ,2)=UG(λ,2).where ε1,…, ε7,β1 and β2 are positive values, which need to be determined empirically. In addition, multiple sets of conditions can be designed according to different combinations of ε5 and ε6 of condition (f). For correlated conflicting instances, one of the above conditions needsto be satisfied while for the other instances, one of the first four conditions needsto be met.It means that the parameters (ε1, ε2, ε3, β1 and β2) related to conditions (a)-(d) are not sensitive to the considered types of instances while the parameters (ε4,…, ε7) are specific to correlated conflicting instances. Next, the guidelines for choosing parameters are given.Conditions (a)-(g) capture the efficient items from different angles according to common sense. The efficient items can be chosen from the items with the second and third largest efficiency (conditions (a)-(c)) if these two efficiencies are close to each other (conditions (a) and (b)) and if these two items have moderate profit balance measure bi(condition (c)) as well as the items with weight coefficients close to the smallest weight w1 and the range of weight coefficients are moderately large (condition (d)). For correlated conflicting instances, the items other than those with the top three efficiencies can be candidates for efficient items. It requires that the efficiency measures of items are not far away from those of the top two efficiencies and the difference between the top two efficiency measures is small (conditions (e)-(g)).However, the specific values of the parameters need to be validated according to the instances in the numerical experiments. For the experiments conducted in the current study, the parameters ε1…,ε7, β1 and β2 are chosen as follows: ε1=10−4, ε2=0.1,ε3=1.8, ε4=0.08, ε7=0.03,β1=3 and β2=4. Two sets of conditions are derived according to condition (f): one chooses ε5=10−4 and ε6=0.125, the other ε5=0.015 and ε6=0.09.The selection of the parameters should guarantee that Viis not underestimated and the average performance for the upper bound reduction is acceptable. However, the algorithm may overestimate the Viof certain items i for some instances. For example, Vi=0 for the non-dominated outcomes of the instance, but the Vimay be estimated as 1 or even larger than 1 according to Algorithms 2 and 3 for the specific selection of parameters. This is related to the fact that it is not easy to characterize the efficiency of the item in the multi-objective context.Lemma 4The time complexity ofAlgorithm 2isOn23, where n2is number of items after the problem size reduction ofAlgorithm 1.Algorithm 2 addresses a set of weighted-sum functions associated with λ∈Λ. The time complexity of the algorithm is determined by the loop from Step 5 to 10. Within the loop, the major operations occur at Step 8, where Algorithm 3 is called. The time complexity of Algorithm 3 isO(n2) because at most n2−1 items are checked at each weighted-sum function associated with λ. The λ is determined according to the break item for the previous weighted-sum function based on (4). For the BOIKP, the item with the second largest efficiency is always the break item. It means that two items with top efficiencies are sufficient to determine the EES. There are at most n2(n2−1) sequences with two items, i. e., ∣Λ∣⩽n2(n2−1). Thus, the time complexity of Algorithm 2 isOn23. □Algorithm 4Procedure for generating necessary nodes of the BRDSInput: w, WandnOutput:Nj(j=1,…,n)1: Nn≔∅, Γ≔∅.2: for (i=0 to ⌊W/wn⌋) do3:k≔W−iwn; Nn≔Nn∪{k}.4: end for5: for (j=n−1 to 2) do6:if (wj⩾wj+1andwjmod wj+1=0or card (Nj+1)=W+1) then7:Nj≔Nj+1.8:else9:Γ≔Nj+1, Nj≔Nj+1.10:while (Γ≠∅) do11:a≔Γ(1), k≔a−wj.12:if (k⩾0 andk∉Nj) then13:Nj≔Nj∪{k}.14:if (k−wj⩾0) then15:Γ≔Γ∪{k}.16:end if17:end if18:Γ≔Γ−{a}.19:end while20:end if21: end for22: N1≔N2.DP [56] is an optimization approach that decomposes a complex problem into a sequence of simpler subproblems. It can be considered as a recursive process, which interprets an optimization problem as a multi-stage decision process. Associated with each stage of the optimization problem are the states of the process. The state is a way to describe a solution of the subproblem, which contains enough information to make future decisions without the need for checking how the process reached the current state. Finally, a recursion optimization procedure is set up to describe the transition from state to state so that the solution of the problem can be obtained by solving multi-stage subproblems sequentially.The basic sequential DP (BDP) procedure is one possible way to decompose the problem within the DP framework. The BDP procedure for multi-objective optimization [16] directly extended the classical DP procedure for the single objective case [2] according to Bellman optimization principle [56]. Refs. [11,48] also discussed other ways to decompose the problem. Similar to the 0–1 MOKP [16], a backward state reduction DP space (BRDS) of the MOIKP is also generated on the basis of the BDP process. The DP space consists of all the states and related transitions between the states. The BDP procedure and the subsequent backward state reduction DP (RDP) algorithm for the MOIKP are presented based on network optimization terminology. Throughout the paper, the following bi-objective example is used to illustrate the network models and related results.vmax(8x1+3x2+2x3+4x4+5x5,3x1+10x2+5x3+11x4+15x5)s.t.2x1+3x2+2x3+4x4+5x5⩽10,xj⩾0,integer,j=1,…,5.LetG=(N,A,c)be a directed connected network, whereNis the set of nodes andA⊆N×Nis the set of arcs; the arc from node i to node j is denoted by (i,j) and the values associated with the arc (i,j) are represented by an r dimensional vector c(i,j)=(c1(i,j),…,cr(i,j)). A state corresponds to a node, the transition from state to state corresponds to a directed arc.Algorithm 5Procedure for generating nodes for the mixed networkInput: w, W, n and n3Output:Nj(j=1,…,n)1: Nn≔{W}.2: for (j=n−1 to n3) do3:Γ≔Nj+1, Nj≔Nj+1.4:while (Γ≠∅) do5:a≔Γ(1), k≔a−wj+1.6:if (k⩾0 andk∉Nj) then7:Nj≔Nj∪{k}.8:end if9:Γ≔Γ−{a}.10:end while11: end for12: j=n3, Γ≔Nj.13: Repeat Steps 10–19 of Algorithm 4.14: for (j=n3−1 to 2) do15:Repeat Steps 6–20 of Algorithm 4.16: end for17: N1≔N2.RDP procedure based on the mixed networkInput: w, W, n, n3 andck(k=1,…,r)Output:S(nW)1: Nα(α=1,…,n)≔Algorithm 5(w,W,n,n3).2: Assign the node values of N1 based on Eq. (8).3: for (α=2 to n3) do4:Γ≔Nα.5:while (Γ≠∅) do6:t≔Γ(1).7:if (t<wα) then8:One incoming arc ((α−1)t,αt) is identified,9:S(αt)≔S((α−1)t).10:else11:Two incoming arcs ((α−1)t,αt) and (α(t-wα),αt) are identified,12:S(αt)≔NDS((α-1)t)∪cα1,…,cαr⊕S(α(t-wα)).13:end if14:Γ≔Γ−{t}.15:end while16: end for17: for (α=n3+1 to n) do18:Γ≔Nα.19:while (Γ≠∅) do20:t≔Γ(1).21:if (t<wα) then22:One incoming arc ((α−1)t,αt) is identified,23:S(αt)≔S((α−1)t).24:else25:Two incoming arcs ((α−1)t,αt) and ((α-1)(t-wα),αt) are identified.26:S(αt)≔NDS((α-1)t)∪cα1,…,cαr⊕S((α-1)(t-wα)).27:end if28:Γ≔Γ−{t}.29:end while30: end forThe BDP procedure consists of n stages, namely, α=1,…, n. Each stage α corresponds to a single variable whose value is determined. Each stage consists of W+1 nodes, from 0 to W. Let αtdenote the node at the tth position at stage α (α=1,…, n, t=0,…, W). In the BDP procedure, t (node capacity) represents the knapsack capacity of the subproblem at node αtand the total weight of items for the solution at node αtis not larger than t.The arc values related to the r profit objectives are given by (cα1,…,cαr) between the nodes at the same stage α and by (0,…,0) from the nodes between two consecutive stages α−1 and α. Let S(αt) denote the set of non-dominated outcomes at node αt. Then the recursive equations in the DP procedure can be represented as follows.At the first stage α=1, the initial values of the nodes are assigned.(8)S(1t)={(0,…,0)}0⩽t<w1{(⌊t/w1⌋c11,…,⌊t/w1⌋c1r)}w1⩽t⩽W.At the subsequent stages α=2,…, n, the recursive equations are stated as follows.(9)S(αt)=S((α-1)t)0⩽t<wαNDS((α-1)t)∪(cα1,…,cαr)⊕S(α(t-wα))wα⩽t⩽W.where the ‘ND’ operator in Eq. (9) calculates all the non-dominated outcomes for the operating set and the ‘⊕’ operator means that the addition operation is applied over all the elements of the setS(α(t-wα)), i. e.,(cα1,…,cαr)is added to each element of the setS(α(t-wα)). The parametersW,wα,cαk,andn(α=1,…,n,k=1,…,r)are the same as those used in the problem formulation (MOIKP). S(nW) collects all the non-dominated outcomes for the problem.The above recursive equations are the results of extending the Bellman principle [56] for the IKP to the MOIKP context. The 0–1 MOKP has the similar recursive equations, wherec11,…,c1rreplaces⌊t/w1⌋c11,…,⌊t/wr⌋c1rin Eq. (8) andS((α-1)(t-wα))replacesS(α(t-wα))in Eq. (9). This replacement gives the 0–1 MOKP a simpler network structure (refer to the difference between the 0–1 MOKP and the MOIKP in Figs. 1 and 2as well as the discussion in the next subsection).In the figures, all of nodes in the BDP space are illustrated, where the nodes in grey are the necessary nodes to determine the non-dominated outcomes of the problem (refer to the next subsection). The values beside or above the arcs are the arc values. All the horizontal arcs have the same values and all the arcs connecting to the nodes of the same stage or the nodes of two consecutive stages have the same values. The values beside the nodes of Stage 1 are the initial values of the nodes determined by (8).Note that the recursive Eqs. (8) and (9) can obtain the non-dominated outcomes constituting any number of copies for item j=1,…, n. If the number of copies of some item in any non-dominated outcomes cannot exceed 1, then the recursive equations for the 0–1 MOKP can be used to obtain the solution for the corresponding item. This is the foundation for the upper bound reduction technique discussed in Section 2.2.The procedure for generating a BRDS for the MOIKP is presented before the mixed network with items of different upper bounds is discussed.The BDP procedure calculates n(W+1) nodes in the DP process and obtains the PF for any capacity t=0,…, W at the last stage n. If the problem with capacity W is solved, only the necessary nodes related to node nWat the last stage n need to be considered. These nodes can be identified sequentially by using a backtracking procedure. The necessary nodes and related arcs between the nodes form the BRDS.Based on the recursive Eq. (9), at any stage α=2,…,n, two nodesα(t-wα)and (α−1)tare directly connected to αtif wα⩽t⩽W. Only the latter is connected to αtif 0⩽t<wα. Algorithm 4 gives the procedure for generating the necessary nodes (node positions) of the BRDS, where Γ(1) denotes the first element of set Γ and Nj(j=1,…,n) the node positions at stage j. Once the node positions are known, the related arcs between the nodes can be identified according to the recursive Eq. (9). The arc values are assigned according to the discussion in Section 3.1. Fig. 1 illustrates the BRDS forthe MOIKP. The BRDS consists of necessary nodes (in grey) and related arcs including arc values. The nodes at the same stage are connected like a chain.Here a brief comment is given for the BRDS of the MOIKP. Similar to the BRDS of the 0–1 MOKP, the number of nodes assumes a non-increasing pattern. However, the number of nodes may increase faster from stage α−1 to α if the conditions at Step 6 of Algorithm 4 are not satisfied. Several items with different weight coefficientscan make the number of nodes at a stage close to W. It means that the memory requirements for the BRDS can be large even though n is not large.In terms of the number of objective vectors of a node, it depends on the number of copies of items included in the knapsack. In general, if the number of copies is large, the number of objective vectors has a tendency to increase faster from the node of the lowest position to the node of the highest position at the same stage. However, not all of the items contribute to the increase of the number of objective vectors due to the existence of the dominated items. Thus, it is more difficult to characterize the relation between the non-dominated outcomes of the problem with respect to the problem size. Similarly, it is also difficult to characterize the change of the number of objective vectors from stage to stage. Consequently,the memory requirements for storing the objective vectors during the DP process is difficult to estimate purely based on the problem size n.However, the conditions for the effectiveness of the RDP algorithm remain unchanged, i. e., the memory requirements for storing the BRDS is smaller than those for storing the objective vectors during the DP process and the time for generating the BRDS is smaller than that for node calculation. The first condition is a bottleneck.Finally, a brief contrast of the BRDS is given for the MOIKP and for the 0–1 MOKP. The BRDS assumes a chain pattern (the nodes at the same stage are connected like a chain) for the MOIKP while a tree pattern (the nodes at the same stage are not connected) for the 0–1 MOKP. At the last stage, the number of nodes for the MOIKP depends on the value of wn, which can be any number from 2 to W+1, while always 1 for the 0–1 MOKP regardless of wn. This implies that item orders affect the node pattern of the MOIKP more significantly than that of the 0–1 MOKP. These differences imply that structure of the BRDS for the MOIKP is more complicated than that for the 0–1 MOKP.As mentioned in Section 3.1, if the items with UB =1 can be separated from the items with UB >1, the recursive equations of the 0–1 MOKP can be used to obtain the number of copies for the items with UB =1. For generating amixed network, the items with UB >1 and with UB =1 are grouped separately with the former coming first. Assume that n3 items have UB >1 among n items. The mixed network is the combination of the BRDS of the MOIKP and the BRDS of the 0–1 MOKP. The procedure for generating the BRDS for the 0–1 MOKP can be referred to [16,50]. Algorithm 5 presents the procedure for generating the nodes of the mixed network. Note thatthe nodes at stage n3 (transitional stage) are generated following the BRDS of the 0–1 MOKP first (Steps 4–9) and then using these nodes as basis the remaining nodes are generated following the BRDS of the MOIKP (Steps 12–13). Finally the nodes for the remaining stages (from stage n3−1 to 1) are generated following the BRDS of the MOIKP(Steps 14–17). This means that the overall structure of the mixed network is simpler than that of the BRDS for the MOIKP. However, arcs at the transitional stage assume a mixed pattern.Fig. 2 illustrates the mixed network with UB =1 for the last three items. The nodes in grey are necessary nodes. At stage 2, the arcs between the nodes assume a mixed pattern, including the non-zero arcs with values (3,10) between the nodes at the same stage 2 (for the MOIKP) as well as the non-zero arcs with values (2,5) between the nodes from stage 2 to 3 (for the 0–1 MOKP). Compared with Fig. 1, the node and arc patterns for the last three stages are simpler, which helps to reduce the overall computational effort in the DP process. For the illustration example, the number of nodes for the first two stages coincide. In general, the number of nodes related to the items with UB >1 for the mixed network should not be larger than that for the BRDS for the MOIKP if the items are arranged in the same order.Note that the node generation procedures for both the BRDS (Algorithm 4) and the mixed network (Algorithm 5) are only related to the weight and the knapsack capacity of the instance. It means that the solution approach presented in this paper can apply to the MOIKP with more than two profit objectives. Algorithm 6 presents the RDP procedure based on the mixed network. Assume that the nodes at each stage arranged in an increasing order of node positions.Table 1illustrates the results of the BDP procedure as well as the PF of the problem. In each stage, the non-dominated outcomes of the nodes are given. The numbers in the capacity column are node capacities, which coincide with the node positions as mentioned in Section 3.1.The PF consists of six non-dominated outcomes from the results of the node with capacity 10 at stage 5. The column PF gives the efficient solutions (the decision variable values) corresponding to the non-dominated outcomes. For example, (5,0,0,0,0) means that x1=5, x2=x3=x4=x5=0.Two efficient solutions correspond to outcome (16,28). It can be seen that the number of copies of items 3, 4 and 5 in any non-dominated outcome cannot exceed 1. The results generated by Algorithm 5 based on the mixed network illustrated in Fig. 2 are the same as those of the BDP procedure.Finally, the time and space analysis of Algorithm 6 is presented. The similar approaches [50] for analyzing the performance of the BRDS based algorithms for the 0–1 MOKP are adopted. The mixed network presented in Algorithm 5 must be generated first and stored in memory, then Algorithm 6 is applied according to the generated mixed network. Therefore, the memory requirements of Algorithm 6 is the sum of the memory requirements for storing the mixed network and for storing one stage of objective vectors during the DP process. Similarly, the solution time is the sum of the time for generating the mixed network and the time for computing objective vectors in the DP process. The following notation is introduced.MαMaximum number of objective vectors of a node at stage α (a function of ND),M¯αAverage number of objective vectors of a node at stage α (a function of ND),MMaximum number of objective vectors of a node for the DP process, M=max{Mα,α=1,…,n},NαSet of nodes at stage α (∣Nα∣ is a function of W),NDNumber of non-dominated objective vectors for the problem,TMIXGeneration time for the mixed network (Algorithm 5),TproComputational time for the DP process (Algorithm 6),TBDPSolution time for the BDP algorithm,TRDPSolution time for the RDP algorithm,πMIXMemory requirements for storing the mixed network,π1Memory requirements for storing one stage of objective vectors during the DP process,πBDPMemory requirements for the BDP algorithm,πRDPMemory requirements for the RDP algorithm,τ¯MIXAverage time for obtaining a node position for the mixed network by Algorithm 5,τ¯DPAverage time for obtaining an objective vector during the DP process.The memory requirements for an integer are used as a basic unit, then the memory requirements for an r dimensional integer vector are r times the basic unit. The node profile for the mixed network assumes a non-increasing pattern, i. e., ∣N1∣⩾…⩾∣Nn∣, where ∣N1∣⩽ (W+1), according to Algorithm 5, as illustrated in Fig. 2. Thus πMIX=∣N1∣n.(10)πRDP=πMIX+π1=|N1|n+rmaxα(|Nα|Mα)Mαhas tendency to increase as α increases,π1is determined by some middle stage where ∣Nα∣ and Mαare moderately large.(11)πBDP=r(W+1)MAccording to [16], for the RDP algorithm of the 0–1 MOKP to be effective, it requires that πRDP<πBDP. Finally, it requires that ND⋙ρn/r, where ρ>0. This result can be extended to the MOIKP. However, it is not easy to characterize the relations between ND and n for the MOIKP as mentioned in Section 3.2.The solution time for the RDP algorithm is given below.(12)TRDP=TMIX+Tpro=τ¯MIX∑α|Nα|+τ¯DP∑α(|Nα|M¯α)Usually, it requires that Tpro>TMIX to apply the RDP algorithm. In the multi-objective optimization context, this condition is easy to satisfy becauseτ¯DP⩾rτ¯MIXandM¯α>1for the non-trivial case. Thus, TRDP is mainly determined by Tpro. The reason why the RDP algorithm is more efficient than the BDP algorithm (TBDP=τ¯DP∑α((W+1)M¯α))is that it reduces the number of nodes with large Mαdue to its non-increasing node pattern.To test the effectiveness of the two problem reduction techniques, the mixed network consisting of the items with the two types of upper bounds and the backward state reduction technique, both backward and forward state reduction algorithms (RDP and LDP) were applied to solve three variants of problems, including the original problem (o), the problem after the first reduction (r1) and after the second reduction (r2) discussed in Section 2. The overall performance of the LDP algorithm was among the best of DP algorithms based on labeling techniques [11]. Both RDP and LDP algorithms were implemented in C++ in the Microsoft visual studio 2003 environment. All the experiments were carried out on a 2.49GHz Pentium PC with 2.9GB RAM under Windows XP operating systems. In this study, only randomly generated bi-objective instances were considered.In the computational experiments, the processing time of the two reduction procedures discussed in Section 2 is ignored in the recorded solution time (refer to Table 3). From theoretical viewpoints, the reduction procedures consist of simple operations and the number of operations is finite according to Lemmas 3 and 4. The time complexity of the reduction procedures is a polynomial function of the problem size n and has nothing to do with the knapsack capacity W (potentially ∞) and ND (can be large). However the solution time of the DP based algorithms including RDP algorithm is related to ∣Nα∣ (a function of W) andMα(M¯α)(a function of ND) as discussed at the end of Section 3.3. Therefore, the solution time of the instance should be much larger than the processing time of the reduction procedure for a non-trivial instance (n2>1). For a trivial instance (n2=1), the solution of the instance can be trivially obtained as⌊W/w1⌋c11,⌊W/w1⌋c12with no effort. Then the processing time of the reduction procedures is much larger than the solution time of the instance. If the trivial instances account for a larger portion of total instances after reduction, then the processing time of the reduction procedures may account for a relatively large portion of the average solution time. However, in most cases, the processing time of the reduction procedures can be ignored.To justify the theoretical analysis, the processing time of the two reduction procedures was recorded separately in the experiment. To obtain the processing time accurately, the reduction procedures were run repeatedly a sufficient number of times (e. g., 10,000) and total (non-zero) running time was recorded, then the actual processing time is the recorded running time divided by the number of repetitions. To facilitate the comparison, the small solution time of the instances was also obtained in a similar way except the solution time for the trivial instances, which was treated as zero permanently.The numerical experiments were associated with the following four types of instances, similar to those for the 0–1 bi-objective knapsack instances [14,16].(A)Uncorrelated instances:cj1∈R[1,U],cj2∈R[1,U],wj∈R[1,U],j=1,…,n;Unconflicting instances, where two profit objectives are positively correlated:cj1∈R[1+0.1U,U],cj2∈Rcj1-0.1U,cj1+0.1U,wj∈R[1,U],j=1,…,n;Uncorrelated conflicting instances, where two profit objectives are negatively correlated:cj1∈R[1,U],cj2∈Rmax1,0.9U-cj1,minU,1.1U-cj1,wj∈R[1,U],j=1,…,n;Correlated conflicting instances, where two profit objectives are negatively correlated and weight coefficients are positively correlated with profit objectives:cj1∈R[1,U],cj2∈Rmax1,0.9U-cj1,minU,1.1U-cj1,wj∈Rmax1,cj1+cj2-0.2U,cj1+cj2+0.2U,j=1,…,n.Here u∈R[v1,v2], is a uniform random number in [v1,v2]. Set U=1000, andW=12∑j=1nwjfor all the instances. We considered 30 instances for each problem size n (the number of items in the knapsack) and each instance type. Average results were obtained on the 30 instances.The following notation is used in the tables and figures.nNumber of items in the knapsack,W¯Average knapsack capacity on the 30 instances,n¯r1,nˆr1Average and maximum number of items for 30 instances after the first reduction,n¯r2,nˆr2Average and maximum number of items with upper bounds larger than 1 for 30 instances after the second reduction,t¯r1Average processing time for the first reduction,t¯r12Average processing time for the first and second reduction together,pRr1Percentage thatt¯r1accounts for the solution time of the RDP algorithm for the reduced problem r1,pRr12Percentage thatt¯r12accounts for the solution time of the RDP algorithm for the reduced problem r2,rG3, rE3, rE1Ratio of instances (among 30) with number of items larger than 3, equal to 3 and 1 after the first reduction,rdE1Ratio of instances with number of non-dominated outcomes (ND) equal to 1 for the 30 instances,ND¯Average ND on the 30 instances,CVCoefficient of variation (ratio of the standard deviation and average of ND),RMBAverage ratio of memory for storing the BRDS and total memory requirements for the 30 instances,RDA, RDMAverage and maximum ratio of the maximum number of objective vectors of a state during the RDP process and ND for the 30 instances,N/AThe results are not available because some of instances cannot be solved due to memory restriction,AVectAverage number of objective vectors actually calculated during the RDP process for the 30 instances,o, r1, r2The original problem, the reduced problem after the first and second reduction.In the experiments, the items were arranged according to the non-decreasing order of the weight. For the reduced problem (r2), the items with upper bounds larger than 1 and equal to 1 are divided into two groups, then items are arranged according to the non-decreasing order of the weight in each group.First, Table 2gives the average results related to two problem reduction techniques. The first reduction can result in big size reduction if weight and profit coefficients are uncorrelated (types A, B and C) while it is difficult to reduce the problem size for correlated conflicting instances (type D) (refer to the results of columnsn¯r1andnˆr1). Then¯r1andnˆr1represent the average and maximum problem size, respectively after the first reduction. It seems that the size of the reduced problem (r1) does not vary much with the problem size n for instances of types A, B and C while it is roughly proportional to n for instances of type D. The larger rE1, the larger portion of instances that can be solved trivially with only one ND solution. The value (rE3+G3) indicates the ratio of the instances that are subject to the second reduction with n2⩾3 (n2 is the size of the problem after the first reduction) as discussed in the beginning of Section 2.2, which should be hard instances.After the second reduction, the average and maximum problem sizes are indicated byn¯r2andnˆr2, respectively.It can also be seen that the processing time for the first and second reduction together(t¯r12)approximates that for the first reduction(t¯r1)for instances of type B while much larger for instances of type D. The results for types A and C instances fall between the results of instances for types B and D. It means that the second reduction is much more time consuming for instances of type D than for instances of types A, B and C. There are three reasons for this. First, for instances of types A, B and C, only a smaller fraction (rG3+rE3) (refer to Table 2) of instances with n2⩾ 3 are subject to the second reduction. Second, the size of the instances subject to the second reduction is smaller for instances of types A, B and C. Third, the second reduction procedure is much simpler for instances of types A, B and C since it is easier to capture the efficient items according to the discussion in Section 2.2.Whent¯r1andt¯r12are compared against the corresponding solution time for reduced problem r1 and r2 recorded in Table 3for the best algorithm (RDP algorithm), the percentage that the processing time accounts for the solution time (columns pRr1 and pRr12 in Table 2) is reasonably small for all instances except small size instances for types A, C and D (size 10) as well as some size (size 20 and 100) for type B.Next, Table 3 gives the average results of RDP and LDP algorithms for three variants of problems. The memory requirements of the RDP algorithm are the sum of memory requirements for storing the nodes for the BRDS (mixed network) and for storing one stage of objective vectors during the DP process. The memory requirements of the LDP algorithm are those for storing two stages of objective vectors during the DP process.Two evaluations are conducted. The first evaluation is concerned with the relative performance of RDP and LDP algorithms. The second evaluation is related to the effect of problem reduction on the RDP algorithm. GAP (%)=100(Ts−Tb)/Tbis used to do evaluation. For GAP comparison, the results of small size instances are excluded (e. g., size 10 for types A, C and D and size 20 for type B) because the related performance measure is too small to be accurate. When the first evaluation is conducted, Tsand Tbare the performance measures (either solution time or memory requirements) of the subject (RDP) and the benchmark (LDP) algorithms, respectively. When the second evaluation is conducted, Tsis the performance measures of the RDP algorithm for solving the original problem or the reduced problem (r2) and Tbthe performance measures for solving the reduced problem (r1).Now, the comparison is made between the RDP and the LDP algorithm. In terms of solution time, the RDP algorithm performs much better than the LDP algorithm for all types and all three variants of problems considered in the experiment, especially for two reduction variants (r1 and r2) of all types and conflicting instances (types C and D) of all the three variants, the GAP is close to −100%. In terms of memory requirements, the situation varies. As for the original problem, on the average the RDP algorithm requires more space for instances of types A (12%) and B (49%) and slightly more space for instances of type C (0.19%) while less space for instances of type D (11%). It seems that the RDP algorithm has a tendency to improve the memory requirements for instances of types A, B and C when the problem size increases. It can be seen that the RDP algorithm can handle larger size instances of the original problem for these types of instances. Regarding the reduced problems r1 and r2, on the average the RDP algorithm requires less memory for all the four types of instances, ranging from 12% to 70%. This means that the RDP algorithm is more efficient than the LDP algorithm for solving the BOIKP even though its memory requirements are a little larger for some types of instances. According to [16], the RDP algorithm is effective if the memory space for storing the BRDS is less than that for storing the objective vectors during the DP process. Based on the column RBM of Table 4, the RDP algorithm should be effective for all types of problems except the original problem of type B. The problem reduction can further improve the effectiveness of the RDP algorithm.Note that problem reduction can also improve the solution time and the memory requirements for the LDP algorithm. However, it seems that RDP algorithm responds to the reduction much better, especially for the solution time. In the following we focus on the effect of the reduction on the RDP algorithm.As compared with the first reduction, on the average the second reduction shows time improvement in percentage by 48, 69, 56 and 11, memory improvement in percentage by 26, 71, 23 and 1, for instances of types A, B, C and D while the solution time of the original problem is about 47, 70, 38 and 0.4 times larger and memory requirements are about 260%, 150%, 60% and 5% larger for instances of types A, B, C and D.Based on above analysis, the two reductions can achieve consistent solution time and memory improvement regardless of solution techniques (either RDP or LDP algorithms). However, the RDP algorithm reacts to the reduction much better as mentioned above. More importantly, the reduction helps to reduce the memory requirements for storing the BRDS (mixed network) (refer to the column RBM of Table 4), which has direct effect on the effectiveness of the RDP algorithm. In addition, the second reduction helps to reduce the peak memory requirements for instances of type B (refer to the column RDA and RDM of Table 4). This improves the effectiveness of the RDP algorithm for handling the unconflicting instances.Finally, Fig. 3shows the relation between AVect and solution time for the RDP algorithm. The instance size is 40 for types A, C and D and 80 for type B. The vertical axis indicates the solution time. R2∈[0,1], is a fitness indicator. Generally, if R2>0.8, the liner model can be used to describe the relation between the quantities indicated by the vertical and horizontal axis. The larger the R2, the stronger the linear relation. For the reduced variant r2 and the original problem, R2>0.8 for all the four types of instances. For the reduced variant r1, R2<0.8 for instances of types A and B. The possible reason for this may originate from the item order, which can cause the time deviation of the node calculation.

@&#CONCLUSIONS@&#
