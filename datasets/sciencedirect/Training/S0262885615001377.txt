@&#MAIN-TITLE@&#
What is a good evaluation protocol for text localization systems? Concerns, arguments, comparisons and solutions

@&#HIGHLIGHTS@&#
We propose a two-level annotation evaluation protocol for text detection algorithms.Algorithms with different granularity outputs are equitably compared.All matching strategies between the ground truth and the detections are handled.Quantity and quality scores are given to describe a detector's behavior.The protocol can manage any irregular text representation.

@&#KEYPHRASES@&#
Evaluation protocol,Text detection,

@&#ABSTRACT@&#
A trustworthy protocol is essential to evaluate a text detection algorithm in order to, first measure its efficiency and adjust its parameters and, second to compare its performances with those of other algorithms. However, current protocols do not give precise enough evaluations because they use coarse evaluation metrics, and deal with inconsistent matchings between the output of detection algorithms and the ground truth, both often limited to rectangular shapes. In this paper, we propose a new evaluation protocol, named EvaLTex, that solves some of the current problems associated with classical metrics and matching strategies. Our system deals with different kinds of annotations and detection shapes. It also considers different kinds of granularity between detections and ground truth objects and hence provides more realistic and accurate evaluation measures. We use this protocol to evaluate text detection algorithms and highlight some key examples that show that the provided scores are more relevant than those of currently used evaluation protocols.

@&#INTRODUCTION@&#
Text detection is an important task in image processing, and many algorithms have been proposed since the last two decades [1]. Hence, text detection systems require a reliable evaluation scheme that provides a ground truth (GT) as precise as possible and a protocol that can evaluate the precision and the accuracy of a text detector with regard to this GT. A solid evaluation protocol should also be able to fairly compare different algorithms. A text detection algorithm can be evaluated differently depending on its output, that can be either boxes surrounding the detected texts, or masks of detected texts after their binarization. One can also directly evaluate the output of an O.C.R.: in such case, the detection algorithm integrates a recognition module and provides as output the text transcription, which is then compared to the true text.While the output provided by the O.C.R. seems to be the ultimate way to evaluate text detection algorithms, the computed scores do not always correctly reflect the detection accuracy: the transcription can fail because of the distortions of the detected text or its fonts. Furthermore, text transcription is not always necessary, especially in applications for which only the text detection is needed (such as text enhancement or license plate blurring). The evaluation of a text mask is a difficult task as well, mainly because it requires the true binarization of the text, that can vary depending on the text properties (stroke thickness for example). Here again, the evaluation does not focus on the detection results but evaluates both the detection and the binarization (in practice, this binarization is also not necessarily needed).The simplest and most common way to evaluate a text detection algorithm is then to compare its detection bounding boxes to those that have been manually annotated (i.e. from the GT). This is the common strategy used in most text detection challenges (ImageEval, ICDAR) to evaluate and compare algorithms. However, we have noticed that these evaluation protocols are not reliable. This is due, both to the metrics used for the evaluation, and to the GT annotations [2,3], that can lead to irrelevant evaluation and comparison of text detection algorithms.An annotation is sometimes subjective, and therefore it can be difficult to choose how text should be annotated [2]. It is yet possible to construct a dataset only composed of images in which there is no ambiguity for the annotation. However, there is still the problem of tilted or curved texts for which a bounding rectangular box is not appropriate because it can contain a lot of non-text areas. It is then important to define rules for labeling and defining the granularity, i.e. the minimal text entity to include into a bounding box. Different levels of granularity can be defined for the GT annotation, depending on the text to detect: the line, word and character levels. The line level is not well suited for tilted text. The character level provides a tedious annotation and promotes connected component approaches. The best granularity level seems to be the word level, even if it is still not the best choice for multi-oriented text.Choosing good metrics to compare detections that do not correctly match the GT objects is also a complex task. Most of the metrics can not efficiently deal with the difference of granularity levels between the GT and the detections. For example, if the GT is at word level and the detection at line level, the score will be most of the time over-penalized. Moreover, as pointed by Wolf and Jolion in [4], a single metric cannot truly describe the complex behavior of a localization algorithm, namely separating the quantity nature (“how many GT boxes were detected”) from the quality aspect (“how well the GT boxes were detected”) of a detection. Although these issues were addressed in the literature (see Section 2), the proposed solutions are still not satisfactory.Because of all these limitations, researchers do not have any robust tool to get a representative evaluation of their algorithm and a fair comparison with other algorithms. For example, the authors in [5] claim that their scores are too low because the ICDAR2013 protocol does not correctly evaluate line level detections. Hence, some other works that provide detections at line level [6,7] have proposed to change the GT annotation of ICDAR2005 dataset from word to line level to be less penalized. However, this does not permit a correct comparison with other scores obtained using the same database with the word level annotation. Sun et al. [8] manually split their line level detections in order to use the ICDAR2013 protocol and compare their results. Manual splitting is also a problem because it makes the comparison irrelevant with other detectors integrating an automatic splitting step (or ever no splitting). Du et al. [9] have also split their line level detections into words, however, no detail about the splitting procedure is given. Due to the lack of a fair evaluation protocol, many works [10,11] evaluate their algorithm by using others protocols. However, this gives an inconsistent comparison to other algorithms.Only few interest has been given to the evaluation protocol of text detection algorithms. Some works [12,13,14] do not mention at all what protocols are used for the evaluation, while others [15,16,17,18,19,20,21,22] limited their explanations to “standard recall, precision and F-Score” without any further details concerning their computation or matching strategies. DetEval is probably the most frequently used evaluation protocol. Its framework is tunable and hence its configuration should always be specified when used. However, only few works [23,24] specify the used parameters, while many do not mention them [25,26,27,28,29,30]. All these examples prove a need of revising the current evaluation protocols.In this article, we propose a new evaluation protocol providing many advantages compared to the most common used, listed below.•It can handle different detection granularities. For that, we propose a two-level rectangular GT annotation, which allows an equitable comparison between algorithms having different granularity outputs.It provides a clear identification of the matching strategy between a GT object and a detection (one-to-one, one-to-many, many-to-one and many-to-many cases) and adapts the two quality metrics (coverage and accuracy) to each type of matching.It computes both quantity and quality recall and precision scores to give a full comprehension of a detector's behavior.It can be easily adapted to manage any irregular text representation, such as polygonal, elliptic or free-form ones.This article is organized as follows. Section 2 first gives a short survey of the existing metrics and evaluation protocols for text detection algorithm evaluation and comparison. Section 3 presents our evaluation procedure called EvaLTex. We first define our two-level annotation that permits to deal with different detector's output granularities (Section 3.1). Then we detail our matching procedures to avoid over or under penalizations while matching detections and ground truth objects (Section 3.2). We also propose a generalization of our protocol to evaluate a set of images and derive quality and quantity scores for the detection (Section 3.3). Finally, we show how EvaLTex can also manage free form annotations (Section 3.4). Section 4 is dedicated to the validation of our evaluation framework in the context of text detection and its comparison to other evaluation protocols. In particular, we show that the currently used evaluation protocols can not efficiently manage many detection scenarios and that our method provides more logical scores. Finally, concluding remarks and perspectives are given in Section 5.In the past decades, various datasets and performance measures have been proposed for text localization tasks and some of them are listed in Table 1.The performance measures that have been mainly used to compare and evaluate text detectors are the recall scores (i.e. number of correctly detected texts divided by the total number of GT objects) and precision scores (i.e. number of correctly detected texts divided by the total number of detected texts). If an algorithm detects too many text regions, its precision rate will decrease, while if it detects too few texts, its recall rate will decrease. Performing a fair evaluation requires to determine if a detection is correct and to correctly match it with its corresponding GT object (particularly if their granularity is different).Most of the current evaluation protocols consider a detection as correct if the overlap area between its region and the corresponding GT object is sufficiently large [46,44,47,48,4]. This gives a binary evaluation, whether this minimum overlap constraint is satisfied or not. Hence, if we compare the two detections of Fig. 1, one that partially covers a text (without satisfying the overlap constraint) and one that entirely misses it, both will unfairly get the same score. Despite its irrelevant scoring, this approach was however used during the latest ICDAR competitions [32,31,49,42].Another challenge concerns the matching of detections and GT objects, particularly if their granularity is different. The matching consists in establishing the links between the detection and the GT objects. Four types of matchings can be considered, as it can be seen in Fig. 2: (a) one-to-one: one detection matches exactly one GT object; (b) one-to-many: multiple detections match one GT object; (c) many-to-one: one detection matches multiple GT objects; (d) many-to-many: mix of cases (b) and (c).Ma et al. [50] proposed a word level evaluation, where GT objects are clustered with respect to a proximity criterion. Hence, single-row and multi-row merges are equally allowed, but the precision is still penalized when such cases occur. Also, if a GT text box is detected several times, only the maximum overlap area is considered, which is a severe penalization for algorithms that can correctly detect a text through two (or more) detections. The evaluation framework in [46] uses a multi-level text annotation (pixel, atom, word and line) but can not handle word and line level texts represented with bounding boxes. In the evaluation protocol introduced by Hua et al. [33], the GT annotation is done at the line level and then penalizes methods whose detection outputs are at word or character level. The method proposed by Nascimento and Marques [48] can evaluate the percentage of different types of matchings, including splits and merges by generating multiple GT interpretations. However, no global measurements are proposed, which makes the comparison between different algorithms difficult to interpret. This problem also occurs in the protocol presented by Mariano et al. [47], who proposed a set of 7 evaluation metrics but not a global one. The VACE metric described in [51] consists in an overall performance measurement, Frame Detection Accuracy (FDA), between all GT objects and detections. Nevertheless, this metric does not provide a clear separation between recall and precision. The CLEAR metrics, also proposed in [51], compute the accuracy and the precision of a text detector separately based on the true coverage area between the GT and the detections but the authors do not explain the evaluation of different matching scenarios. Anthimopoulos et al. suggested in [52] an evaluation method based on the number of detected characters estimated as their width/height ratio. The MSRA-TD500 [43] framework is able to handle oriented texts. The protocol considers a detection as correct if the angle between it and its corresponding GT object as well as their overlap ratio satisfy two thresholds. If multiple detections match the same text line, they are considered as false positives. The evaluation protocol associated to CUTE80 dataset consists in establishing the minimum intersection area between the GT and the detection polygon points of a curved text line. However, all matching types are treated equally. In [41], the authors proposed an evaluation protocol that can deal with inclined text line but only computes a precision value. In [53,54,55,56] the authors proposed an evaluation framework at block level that does not penalize partial text lines.Wolf and Jolion [4] proposed a more complex text detection evaluation scheme, named DetEval, based on performance graphs, which considers the precision and recall rates as quality scores. It can manage the one-to-many and many-to-one cases, but uses parameter functions to penalize both cases. The ZoneMap metric [57] is a generalization of [58] and [4] that computes different error rates based on the overlapping areas in tables.The ICDAR [44] Robust Reading Competition is considered as the main reference for text detection and localization algorithm comparisons. The evaluation method used during this competition is based on the algorithm proposed in [4], assumed to be the most efficient one. But this protocol, as it is used during the ICDAR competition, faces many problems. First, the overlapping area ratio constraint misclassifies many GT text boxes during the matching protocol. This results in low scores, even when the detected boxes substantially overlap the GT ones. Second, the scattering scenarios are poorly treated. Finally, the annotation is done at the word level which frequently assigns high penalties to text line detections, as seen in the results published in [44].In the next section, we propose a new evaluation procedure, which solves most of the previously mentioned problems.Annotating the GT for text detection is not an obvious task and relies on the target application as well as on the rules chosen for this annotation. We indeed have to decide the minimum text size a detector should be able to deal with, or also if a word such as “COCA-COLA” should be annotated as a single GT object or as two separate ones. While some of these issues remain debatable, others, such as the granularity difference between the GT and detections can be easily overcome, as it will be shown in Sections 3.2.3 and 3.2.4. Many evaluation protocols that do not deal with different granularities can sometimes severely penalize one algorithm but not another, while these should be scored equally. A solution is to deal with multiple GT annotation levels.In our approach, we introduce a two-level annotation. Each GT object is first annotated at the world level using a single rectangular box. Then, GT word boxes are manually grouped into regions. Here, we consider word text boxes as part of a same region if they are horizontally (resp. vertically) aligned and have similar heights (resp. widths), but different region groupings could also be considered as long as the text area within one region is larger than the non-text area (see Fig. 3for examples of incorrect grouping). A region is therefore a rectangular box containing several text objects annotated at word level. Fig. 4shows an example of the proposed two-level annotation of the GT.We have chosen this two-level annotation for two main reasons. First, we do not want to reject a detection when its area covers more than one GT word text box (case of many-to-one detections) if these ones belong to the same GT region. In such case, the precision score will then not be penalized. We also want to provide a comparable and equivalent evaluation of algorithms whose outputs are similar, but at different granularity levels (i.e. word and line-level). The proposed solution is detailed in Section 3.2.4.Let G = (G1,G2,…,Gm) be the set of GT text boxes (tags) and D = (D1,D2,…,Dn) the set of the detections, with m (resp. n) the number of objects in G (resp. in D). Then, for each Gimatched to detection Dj, we define Covias their coverage area and Accias the detection accuracy by:(1)Covi=Area(Gi⋂Dj)Area(Gi)(2)Acci=Area(Gi⋂Dj)Area(Dj).We also assign a valuematchGi(resp.matchDj) to each Gi(resp. Dj), defined by:(3)matchGi=1if∃Dj∣Area(Gi⋂Dj)>00otherwise(4)matchDj=1if∃Gi∣Area(Gi⋂Dj)>0.0otherwiseBefore identifying the type of a match (in our case we consider the 4 possible types, see Section 2.2), a filtering procedure is first used to determine, when a detection covers several GT objects, to which of them it can be matched. Fig. 5a illustrates a case of two overlapping GT objects (in dashed green) because of the tilted text. The word “inside” should be matched to the blue detection, while “intel” should not. Hence, when a many-to-one match occurs, we first determine if a detection D intersects more than one GT object. If so, we match detection D to GT object G and not to G′ (we can generalize this reasoning to as many GT objects as needed) if the following area constraint is satisfied:(5)Area(G′⋂D)−Area(G⋂G′)≤t⋅Area(G′),where t is a threshold that regulates the overlap area rate. In our experiments, t was set to 0.1, to filter GT objects with a small overlap area. By increasing t, we could reject valid GT objects that are part of a many-to-one matching. Fig. 5b illustrates the case of text inclusion: the GT object for word “JAVA” includes other GT objects. In this case, we should match the detection to words “JAVA”, “graphic” and “2”, while the other words should have been detected separately in order to be correctly matched.During the next stage, coverage and accuracy scores are computed differently for each GT object depending on the matching strategy, as described in the following sections.The detection is evaluated using the coverage (Eq. (1)) and accuracy (Eq. (2)) scores. Assuming that we never get a perfect match, we use a margin error meto extend or reduce the area of GT object Gi, computed as follows:(6)me=tm⋅Area(Gi)height(Gi)ifheight(Gi)≥width(Gi)tm⋅Area(Gi)width(Gi)otherwisewhere tmis a parameter that controls the thickness of the margin error.Let[xGi,yGi,wGi,hGi]define the GT text box Gi, withxGiandyGiits left upper corner coordinates, andwGiandhGiits width and height respectively. Let Geiand Gribe the extended and the reduced text boxes (see example in Fig. 6) of Gi, with:(7)Gei:[xGi−me,yGi−me,wGi+2⋅me,hGi+2⋅me](8)Gri:[xGi+me,yGi+me,wGi−2⋅me,hGi−2⋅me].For any one-to-one match between a detected box Djand a GT box Gi, the accuracy is computed by considering the enlarged GT text box Gei:(9)Acci=Area(Gei⋂Dj)Area(Dj),while the coverage is computed using the reduced GT text box Gri:(10)Covi=Area(Gri⋂Dj)Area(Gri).Consequently, the higher tmthe higher the coverage and accuracy values. Hence, it is not recommended to give a very high value to this parameter as it might degrade the detection evaluation. In our experiments, tmis set to 0.1, a reasonable value that allows small imprecisions for detections.The one-to-many case is illustrated in Fig. 7where the word “Yarmouth” is matched to 2 different detection boxes.This scenario implies a fragmentation level given by the number of detections (si) associated to one GT object Gi. We use the fragmentation to penalize the coverage of Giin the following manner:(11)Covi=Coviu⋅FiwhereCoviurepresents the union of all intersection areas between Griand all detections Dj, j ∈ [1,si], normalized by the area of Gri, defined as:(12)Coviu=⋃j=1siArea(Gri⋂Dj)Area(Gri);Firepresents the fragmentation index suggested by Mariano et al. [47]:(13)Fi=11+ln(si).Similarly, the corresponding accuracy Giis defined as the union of all intersection areas between Geiand detections Dj, j ∈ [1,si], normalized by the total of detection areas:(14)Acci=⋃j=1siArea(Gei⋂Dj)⋃j=1siArea(Dj).The many-to-one case implies that several GT objects correspond to one single detection. This case is illustrated in Fig. 8.Our protocol considers a many-to-one match as several one-to-one cases. Hence, the coverage for each GT objects Gi, i ∈ [1,mj], with mjthe merge level of the detection box Dj, is:(15)Covi=Area(Gri⋂Dj)Area(Gri).While the coverage only considers the amount of valid matched GT objects, the detection accuracy takes into account the quantity of non-textual areas (areas outside the GT) that have been detected. Consequently, if a detection matches several GT objects, the non-textual area coming from the inter-object spacing contributes to the penalization of the accuracy score. Then, a fair comparison between a word level detection and, for example, a line level detection is not possible. Hence, many one-to-one detections would always outweigh one many-to-one detection. However, in some cases, word level and sentence level detections should be treated equally. Our two-level GT annotation solves this problem and provides a better comparison between different detection granularities. We then assume that the area of a text region does not contain any non-textual area and now consider the spacing area between GT objects belonging to a same region as valid text areas.Our protocol computes both the coverage and accuracy for each GT object, while traditional approaches assign the coverage to GT objects and accuracy to detections. To compute the accuracy for each Gi, we first match it to a detection. Therefore, the detection area is split to be matched to its corresponding mjGT objects. Fig. 8c shows a many-to-one case, with three GT boxes and one detection whose area is larger than the text one. We defineTextAreaDjas the union of all GT text areas covered by the detection box, andnonTextAreaDjthe rest of the detection area, i.e.:(16)TextAreaDj=Area(⋃i=1mj(Gei⋂Dj))(17)nonTextAreaDj=Area(Dj)−TextAreaDj.The accuracy associated to each matched Giis:(18)Acci=Area(Gei⋂Dj)Area(Dj,i),where Area(Dj,i) is the detection area covering each extended box Gei, defined as:(19)Area(Dj,i)=Area(Gei)TextAreaDj⋅nonTextAreaDj.We now define a text region Reg, as the box bounding a set of GT objects (see the yellow boxes in Fig. 8b and d). Then, we redefine theTextAreaDjas the union of all text regions Regkwithin the detection box:(20)TextAreaDj=Area(⋃k=1rj(Regk⋂Dj)),where rjrepresents the number of GT regions covered by Dj.The many-to-many occurs when the same GT objects are involved simultaneously in a one-to-many and a many-to-one match. This is illustrated in Fig. 9. There is a many-to-one match because a detection (in blue) includes the word “HEALTHY” and a part of the word “COLCHESTER”. The one-to-many match is due to the word “COLCHESTER” that is covered by 2 detections.In our approach, the many-to-many match is treated as a one-to-many followed by a many-to-one match. Therefore, the coverage and accuracy are computed using the equations defined for the many-to-one case (Section 3.2.4) and for the one-to-many case (Section 3.2.3). For example, the word “HEALTHY” is part of a many-to-one scenario: its coverage is computed using the Eq. (15) and its accuracy using the Eq. (18). The coverage and accuracy of word “COLCHESTER”, involved in a one-to-many match, are computed using Eqs. (11) and (14) respectively.The scores presented in the previous sections evaluate the quality nature of a detection: how well an individual GT text box has been detected and the precision of each valid detection. However, when dealing with a whole dataset (i.e. a set of images), it is also necessary to evaluate the quantity nature of the detections, namely how many GT objects or false positives were detected on the whole database. The distinction between the quantity and quality aspects of a detection is useful for a better comprehension of the detection results. As pointed in [4], “a recall value equal to 50% can mean that either 50% of the GT text boxes were matched at a 100% rate or that 100% of the GT boxes were detected at a 50% rate. Similarly, a 50% precision result can mean that the total GT area covered by the detection boxes represents 50% of the total detection areas, but it can also mean that only 50% of the detection boxes correctly cover the GT, while the other 50% are false positives”. Consequently, the quality values measure the matching area rates between the GT and detection boxes, whereas the quantity values represent the amount of valid matchings between the GT and detections as well as the amount of false positive.Then we compute, for the whole set of images, both quality and quantity overall recall and precision scores and combine them to get two global scores. Let tp be the number of true positives (i.e. of matched objects in G) and fp the number of false positives (i.e. of objects in D that have no correspondence in G), i.e.:(21)tp=∑i=1m(matchGi=1),(22)fp=∑j=1n(matchDj=0).Here m and n are respectively the number of GT objects and detections over the whole dataset.For a many-to-one case (Section 3.2.4), we split the detection into several areas, each one covering a GT object. For the one-to-many case (Section 3.2.3), we merge the corresponding detection areas and compute a single precision score (Eq. (14)).Then, we define two metrics, that measure how many GT objects were detected (quantity recall Rquant) and how many detections have a correspondence in the GT (quantity precision Pquant), given by:(23)Rquant=tpm,(24)Pquant=tptp+fp,Moreover, we compute a quality recall, Rqual, measuring the overlap area for all valid matchings between the detections and the GT objects and a quality precision, Pqual, estimating the total detection accuracy, defined as:(25)Rqual=∑i=1mCovitp(26)Pqual=∑i=1mAccitp.Since a good set of metrics should reflect both the quantity and the quality nature of a detection, we propose the two following global recall and precision scores:(27)RG=∑i=1mCovim,(28)PG=∑i=1mAccitp+fp.The quality components of these global metrics are given by the numerator values∑i=1mCoviand∑i=1mAcci(sum of all GT object qualities Coviand Acci). The quantity nature is given by the mean of the coverage quality components of the m GT objects, and the accuracy quality components over the total number of detections tp + fp. Indeed, RGand PGare equivalent to the product of the quality and the quantity components:(29)RG=∑i=1mCovim=Rquant⋅Rqual,(30)PG=∑i=1mAccitp+fp=Pquant⋅Pqual,The F-Score FGis used to measure the overall performance of a detection algorithm and is defined as the harmonic mean of the global recall and precision values:(31)FG=2⋅RG⋅PGRG+PG.Fig. 10shows an example for a set of four images and their corresponding GT and detections. The evaluation for each image and for the whole set using our proposed metrics is summarized in Table 2.A rectangular representation of texts can generate errors during the matching process, in cases of inclined, curved of circular texts, as it can be seen in Fig. 11. Although we proposed a procedure in Section 3.2.1 to discard “unlikely” matched GT objects, this cannot ensure that all matchings will be correct (see Section 4.2). For texts that are neither horizontal, nor vertical, typically texts that are encountered in urban scenes, a representation using a free-form mask is more adapted.In this section we show how to extend our EvaLTex protocol (matching strategies and performance metrics) to any irregular text representations (also called masks), such as polygonal, elliptic or even free-form shapes. Using a mask representation implies the following changes:•text objects are represented by irregular masks;the extension and reduction of GT object regions (Eqs. (7) and (8)) are computed using dilation and erosion morphological operations on text masks;we consider only one level of annotation (word or region) but still manage different granularities.In this section, we show the efficiency of our proposed method when using a rectangular representation (see Section 4.1) or a mask representation (see Section 4.2).We evaluate the detection results on the Challenge 2 dataset used during the ICDAR 2013 Robust Reading competition [44]. This dataset contains 233 images of natural scene texts and an associated GT annotated at the word level. We use the same word level annotation, but also our region labels (Section 3.1), to introduce another level of granularity.To illustrate the advantages of EvaLTex we compare it to three commonly used evaluation protocols in the text detection field (ICDAR2003, ICDAR2013 and DetEval). A detailed comparison of matching strategies and detection scores is given in Section 4.1.1, and the interest of using our two-level annotation in Section 4.1.2.We compare our evaluation protocol with the one used during the ICDAR 2013 Robust reading competition [44] for two reasons: (i) it is up-to-date and represents what is commonly done and admitted in text detection evaluation, and (ii) all results are publicly available, making the comparison easy.The ICDAR2013 protocol relies on the evaluation framework introduced in [4]. It uses the proposed area precision and recall thresholds, which are set to 0.8 and 0.4 respectively, and which control the matching between GT objects and detections. Moreover, a lower weight is assigned to one-to-many matches, since the output is at the word level, while text-line level detections (many-to-one matches) are supposed not to be penalized [44]. However, we will show that this is not always true, and that many scores are erroneous.Next, we give some scores provided by our matching algorithm (Fig. 12), on the detector TextDetection [2] that participated to ICDAR 2013 challenges. We compare our matching method with the one of ICDAR2013. The corresponding scores are given Table 3.Fig. 12a and b illustrates a one-to-one case for which the recall and precision scores are over-estimated by ICDAR metrics. First, although the detection missed the first letter of the word “AUSTRALIA”, the recall rate is set to 1 (Fig. 12a). Similarly, even if the area of the detection box for the word “moto” is considerably larger than the GT object, its precision rate is 1. The ICDAR2013 approach scores a GT object with a binary recall (1 or 0), depending on whether the overlapping area between GT and detection respects or not a threshold. However, in many cases, this does not provide a fair comparison between algorithms. For example, if an algorithm detects the whole word “AUSTRALIA”, it will get the same score as the detection shown in Fig. 12a. Conversely, our metrics give a more precise and realistic evaluation because they take into account the real overlap match area, and then provide a better comparison between algorithms.As shown in Fig. 12c and d, ICDAR2013 metrics can consider the one-to-many match in different ways. In Fig. 12d, the word “POSTPAK” is detected by two boxes, both considered as correct. In Fig. 12c we have a similar scenario for the word “Yarmouth”, but here none of the two detected boxes is considered as valid because in both cases the overlap area is too small. Moreover, the two detections are counted as false positives, which unfairly penalizes the final scores. Firstly, it decreases significantly the precision rate because the two detected boxes are erroneously counted as false positives, and secondly, it decreases the recall rate by not matching the two detected boxes to the GT. On the contrary, our method correctly recognizes the one-to-many cases and matches the two detected boxes in both cases, but punishes the fragmented detection by penalizing the recall score, as seen in Section 3.2.Fig. 12e and f shows a problem of inconsistency during the many-to-one matching. In Fig. 12e, the detection is at a line level. Only the second and last lines are correctly matched, while the other detected text lines are associated to the GT text box having the largest area within that line (“unauthorized” in the first line, “Permit” in the third one and “operation” in the fourth one). The unmatched GT text objects (“No”, “to”, “work”, “system”, “in”) are considered as false positives. ICDAR2013 metrics then over punish the many-to-one cases by frequently considering them as one-to-one. On the contrary, our protocol correctly matches all text lines and leads to a recall equal to 1. We have a similar problem when detections cover multiple text lines (Fig. 12f). The word “Roland” is matched by ICDAR2013 protocol, while the two other words are discarded. Hence, their recall is penalized, while their precision is not. Our method considers all words as detected, hence the recall rate is set to 1. Nevertheless, we assign a low precision rate, due to the presence of the logo in the left part of the detected box.Finally, the many-to-many case is illustrated in Fig. 12g and h. The word “COLCHESTER” in Fig. 12g corresponds to both a many-to-one and a one-to-many match. Nevertheless, the ICDAR2013 matching protocol rejects it and matches only the word “HEALTHY”, whereas our algorithm validates both text boxes, but penalizes the recall due to its split detection. If we look at the second line in Fig. 12h we observe that the word “family” is covered by two detections (one-to-many). Both detections involve a many-to-one case, the first one corresponding to the words “Lifelines“ and “family”, while the second one to the words “family“ and “Support”. The ICDAR2013 matching algorithm considers GT text boxes as matched those containing the words “Lifelines“ and “Support”, and classifies the word “Family” as missed. This leads again to an unfair comparison: if another localization algorithm would have completely missed the word “Family”, then, both algorithms would have got the same scores, although the first one detected only 87% of the area of the “Family” GT text box.DetEval [59] is a tool, based on the method of Wolf and Jolion [4], that is the core evaluation protocol used during the ICDAR 2011 and 2013 Robust reading competitions.The system's object matching criteria can be configurable through eight parameters: six of them representing the minimum recall and precision overlap area between detections and GT objects for one-to-one, one-to-many and many-to-one cases, one parameter to add a border verification or not, and a threshold on the center distance between two matched boxes. We first evaluate the text detection results using a “relaxed” version of DetEval by disabling the minimum area coverage constraints:•the recall and precision area thresholds are set to 0;the center difference threshold is set to 1.We have chosen this parametrization because it is in spirit the closer to our evaluation protocol, which is more permissive.Next, we will describe the behavior of the “relaxed” DetEval protocol when dealing with one-to-one and many-to-one cases. Fig. 13shows some examples of partial one-to-one detections which got maximum recall scores, as seen in Table 4. However, the many-to-one detections from Fig. 13, although they match entirely all GT text objects, are penalized, as seen in Table 4. Moreover, both recall and precision values are penalized and set to value 0.8 independently of the number of matched GT text boxes. Our method correctly penalizes the recall of one-to-one detections, and does not penalize the many-to-one cases. Hence, even when using the most permissive configuration of DetEval, our method is able to give better results in the evaluation of detections.DetEval also integrates a set of new metrics to characterize both the quality and the quantity natures of a detector's output. Recall and precision are computed over a range of 20 different area threshold values to produce two curves. Then, two overall metrics are derived by computing their area under the curve (AUC). While these metrics solve the problem of partial matchings of the “relaxed” and default DetEval, the precision tends to even out the recall values when dealing with one-to-one cases (see Table 4), failing to differentiate the two characteristics of a detection. We have a similar problem for the many-to-one matches (see Table 4): the small difference between the recall and precision values does not give a clear distinction between the number of GT boxes that were detected and how well these detections were covering them.The ICDAR2003 protocol, also used during the ICDAR 2005 competition, is still widely used for evaluating text localization methods [60,61,62, 63,64,65]. The advantage of this method is that the recall for partial one-to-one matches is scored accordingly to the true ratio between the intersection and the GT surface. On the other hand, the precision is not computed with respect to the detection surface and, similarly to DetEval protocol, seems to always be equal to the recall rate, as it can be seen in Table 4.Another main drawback is due to the choice of the best match approach used to solve the many-to-one cases, illustrated in Fig. 13, whose corresponding scores are given in Table 4.As previously said, one-to-one detections are treated differently from one protocol to another one. We propose a simple experiment which consists in gradually decreasing the quality (coverage area) of a one-to-one detection (see Fig. 14a) and analyze the recall and precision evolution depending on this.Fig. 14b shows the evolution of recall and precision scores given by the default and “relaxed” configuration of DetEval when dealing with partial one-to-one matchings. For the “relaxed” DetEval recall and precision values are constant and equal 1, because of area thresholds set to 0. On the other hand, for the default DetEval, there is an irregular decreasing of recall and precision values. This reflects the binary evaluation of one-to-one matchings, which depends on the recall and precision area parameters and which cannot correctly differentiate total and partial detections.Fig. 14c illustrates the behavior of DetEval AUC metrics. One can observe that we have similar plots for recall and precision. However, since the intersection area always remains within the boundaries of the GT, the metrics should have a different behavior. Our method (Fig. 14d), evaluates the precision to a fix value of 1, which is a logical because the detection correctly fits the GT object. On the contrary, the recall score decreases linearly with the progressive shrinkage of the detection box.We now consider the detection results of ten participants at the ICDAR 2013 Robust Reading competition (Challenge 2) [44]. Our goal is to compare scores given by different evaluation protocols on the same dataset as in this competition. Table 5gives the scores obtained for the ten participants with the ICDAR 2003 protocol [31], DetEval [44], DetEval AUC and EvaLTex. The results of these methods are publicly available on the competition website page [44].Among the four protocols, the DetEval AUC metrics seem to be the strictest. But, even if the ICDAR evaluation protocol is less penalizing than the AUC metrics, in the case of the Inkam participant, it gives lower scores. This is because a high number of partial one-to-one detections are rejected by ICDAR, due to its covering area constraint. On the contrary, AUC metrics, by varying this constraint, can better handle partial matchings. An interesting point is the similarity of the ranking produced by AUC metrics and EvaLTex, despite of their high score variations.While DetEval recall scores are 13% higher compared to ICDAR scores (see the Text Detection participant), their precision score is similar. This high recall difference can be explained by the large number of GT objects involved in many-to-one matchings, that are rejected by the ICDAR protocol but not by DetEval. Furthermore, EvaLTex recall scores tend to be higher than those of ICDAR and DetEval protocols, because the former fairly validates more partial one-to-one matchings.Moreover, EvaLTex relaxes the unfair precision penalties applied by the other two methods. On the contrary, it penalizes algorithms that produce detection areas significantly larger than the GT objects (Fig. 15), as in the case of TextSpotter participant, which gets a precision score 10% lower compared to DetEval score. On the other hand, it gives higher precision scores to algorithms I2R_ NUS_ FAR, I2R_ NUS and Inkam for which a high number of partial one-to-one detections were mismatched by DetEval and ICDAR protocols. To finish, we note that ICDAR and DetEval rankings are relatively similar, while EvaLTex proposes substantial changes in it.Fig. 16shows the impact of the GT annotation on the precision and recall scores (computed in Section 3.3). One can observe that the precision value increases proportionally to the surface of the text region. This is logical because the more GT objects a region contains, the smaller the non-textual area becomes and therefore the less the precision is penalized.Table 6shows the interest of using the two-level annotation on some key examples in Fig. 17. Most of the detections correspond to many-to-one matchings. Here, the region labeling is done at line level. As it can be seen, by enabling this region annotation (and then having a two-level annotation), we get higher precision scores. On the contrary, recall scores are not changed.In this subsection we compare the evaluation given by EvaLTex to detection cases depending on the type of annotation (rectangles or masks). Fig. 18shows most of the problems that may be encountered when using a rectangular annotation: GT rectangles that contain more non-textual areas (Fig. 18b, c, d, e, g), intersection of boxes in the GT (Fig. 18d, f, g), and inclusions of GT boxes (Fig. 18f).The corresponding evaluation scores are presented in Table 7. One can observe that, when using a rectangular representation, the matching is disturbed by the text objects that intersect in the GT. Namely, text objects such as “ALBACORE” in Fig. 18d, are matched two times: with their corresponding detection and with detections targeting objects that intersect them in the GT. Hence, the coverage scores of such GT objects are penalized by the fragmentation parameter invoked during the one-to-many matching, which can furthermore impact the global recall score. This can however be avoided for similar cases, such as “GRAS” and “ANISETTE” (Fig. 18f), by using the filtering procedure described in Section 3.2.1.Recall values of Fig. 18e show another example of difference when using these two representations in the case of a tilted and perspective deformed text (“ALAINAFFLELOU”), only partially matched. The coverage ratio computed with rectangles is smaller than the one computed with masks and consequently leads to a significant recall difference.Another difference between these two annotation types comes from the precision variations that are higher when dealing with many-to-one detections. Such situations can be seen in Fig. 18b and c that illustrate many-to-one detections covering curved text strings (“KEMA-KEUR”, “3G0.75” and “VDE” in Fig. 18b, respectively GT objects “Enjoy” and “yours” in Fig. 18c). For Fig. 18b, the precision values vary from 0.48, when using the rectangular representation to 0.81, in the case of mask annotation. Similarly, the precision scores for the two text representations in Fig. 18c range from 0.73 to 0.98. Once again, the rectangle representation shows its limitation and that it can significantly penalize the evaluation of a detector.

@&#CONCLUSIONS@&#
Today, no accurate protocol allows a reliable evaluation and comparison of text detection algorithm outputs. The few existing protocols used in several challenges have many problems. We claim that it is of crucial importance for the researchers to evaluate and compare their detection algorithms using a strong and reliable protocol so that they can better evaluate the pros and cons of their algorithms and improve them.That is why, in this paper, we have introduced a novel approach to evaluate and compare text localization algorithms, that overcomes some of the existing drawbacks of current evaluation systems. We can cite several systems: the best match approach, which assigns a many-to-one detection to only one GT object and rejects other valid matched GT text boxes; the minimum overlap area constraint which assigns to a detection box a GT object if and only if their intersection area is large enough; the use of inappropriate GT annotations, or also the lack of distinction between recall and precision scores when dealing with partial detections. Even if it would seem reasonable for many object detection purposes, for text detection these constraints are rather restrictive and usually lead to severe penalties. The purpose of our evaluation is not to provide detections with higher scores, but more precise ones that reflect more accurately the reality.The novelty consists in the definition of a set of new rules and the re-interpretation of standard metrics at object level, coverage and accuracy, to improve the evaluation quality. When using a rectangular text representation we introduce a new GT granularity level, the region tag, to relax the precision penalizations and to allow an evaluation of word and line-level outputs. Moreover, our evaluation protocol identifies and treats independently the one-to-one, one-to-many, many-to-one and many-to-many matches. The protocol penalizes the recall in cases of fragmented detections, and penalizes the precision if the detections are not accurate enough. Furthermore, we proposed quality and quantity performance measures that can capture the whole complexity of a detection. Global recall and precision metrics are then obtained by combining the quality and quantity values. Finally, we proved that our evaluation framework can handle different GT annotation and detection representations, such as polygonal, elliptical or free-form shapes. Consequently, our procedure provides a more realistic and representative evaluation comparison between different text detection algorithms. Notice that our evaluation protocol can be seen as a first step of an truthful end-to-end evaluation protocol because, in order to compare the text transcriptions with the GT, a reliable matching strategy is required. Without a matching procedure, robust to granularity differences, systems would be under evaluated, and hence many transcriptions would not be compared to the correct GT objects.Further work will consist in adapting the two-level option used for rectangle GT annotation to mask representation. This task requires a precise annotation algorithm and the definition of a procedure that permits to link and group GT objects into mask regions. An additional work will focus on evaluating the results of text detection algorithms on more challenging datasets, such as the Street View Text, iTowns and MSRA-TD500.