@&#MAIN-TITLE@&#
Measurement of preferences with self-explicated approaches: A classification and merge of trade-off- and non-trade-off-based evaluation types

@&#HIGHLIGHTS@&#
We provide a conceptual framework for classifying self-explicated approaches.We propose PASEA, which merges trade-off- and non-trade-off-based evaluation types.In 2 studies, we compare PASEA to 6 recently published self-explicated approaches.PASEA overcomes insufficient discrimination and discourages simplification.PASEA also improves predictive validity compared with other popular approaches.

@&#KEYPHRASES@&#
Preference measurement,Self-explicated approaches,Marketing research,

@&#ABSTRACT@&#
Self-explicated approaches are popular preference measurement approaches for products with many attributes. This article classifies previous self-explicated approaches according to their evaluation types, i.e. trade-off- versus non-trade-off-based, and outlines their advantages and disadvantages. In addition, it proposes a new method, the presorted adaptive self-explicated approach that is based on Netzer and Srinivasan’s (2011) adaptive self-explicated approach and that combines trade-off- and non-trade-off-based evaluation types. Two empirical studies compare this new method with the most popular existing self-explicated approaches, including the adaptive self-explicated approach and paired comparison preference measurement. The new method overcomes the insufficient discrimination between importance weights, as usually found in non-trade-off-based evaluation types; discourages respondents’ simplification strategies, as are frequently encountered in trade-off evaluation types; is easy to implement; and yields high predictive validity compared with other popular self-explicated approaches.

@&#INTRODUCTION@&#
Preference measurement approaches are an effective instrument for companies that seek to understand consumers’ needs, as frequently used in new product development, positioning, pricing, market segmentation, and advertising processes (e.g., Natter & Feurstein, 2002; Schlereth & Skiera, 2012; Schlereth, Stepanchuk, & Skiera, 2010). Yet preference measurement continues to face challenges related to high numbers of product attributes as the complexity of the product increases (Netzer & Srinivasan, 2011).Self-explicated approaches (SEAs) refer to a class of popular preference measurement approaches that are compositional in nature (e.g., Scholz, Meissner, & Decker, 2010). Respondents directly evaluate the desirability of each attribute level, as well as the importance of the attributes, and the combination of both evaluations determines the products’ utilities. In contrast, decompositional approaches, such as conjoint analysis, derive consumers’ preferences or willingness to pay for attribute levels indirectly from their evaluations of products as a whole (Eggers & Sattler, 2009; Gensler, Hinz, Skiera, & Theysohn, 2012; Schlereth, Eckert, & Skiera, 2012). The latter approach is taxing with more attributes (e.g., Karniouchina, Moore, Van der Rhee, & Verma, 2009), resulting in higher interview expenses and cognitive burdens for respondents (Scholl, Manthey, Helm, & Steiner, 2005). Thus, SEAs rather than decompositional approaches currently are more popular for determining consumers’ utilities for complex products (Scholz et al., 2010).Various SEAs adopt alternative evaluation types to determine the desirability of attribute levels (stage 1) and attribute importance weights (stage 2). The most basic distinction involves whether the evaluation asks consumers to trade off levels or attribute importance weights (e.g., ranking tasks, paired comparisons) or not (e.g., rating tasks). We refer to the former evaluation types as trade-off-based, whereas the latter are non-trade-off-based evaluation types.In stage 1, both evaluation types seem to work equally well (Green, Krieger, & Agarwal, 1993; Pullman, Dodson, & Moore, 1999), but recent research increasingly recommends trade-off-based evaluations in stage 2, noting that different respondents use non-trade-off scales differently (i.e., means and variances are individual rather than task specific; Green et al., 1993; Pullman et al., 1999). Although this problem also can arise in stage 1, the heterogeneity in scale interpretation seems even more pronounced in the cognitively more challenging stage 2, which compares improvements of different attributes. Another argument against non-trade-off-based evaluation types is that they rarely provide sufficient discrimination among attributes. Respondents tend to give many attributes too much importance weight, even when they are not relevant for decision making (King, Hill, & Orme, 2004). This unwanted property becomes even more manifest with an increasing number of attributes (Pullman et al., 1999).In response to these concerns, various evaluation type combinations have been proposed, varying in both their degree of implementation complexity and their ability to recover underlying preferences. Marketing researchers are challenged to choose the combination that suits their research problem best, but they generally lack support for this decision, because no extensive empirical comparisons of different SEAs exist, and relations among the different approaches are not transparent. This study therefore structures and classifies existing SEAs and compares the most popular methods empirically, including recently published methods such as adaptive self-explicated approaches (ASEA, Netzer & Srinivasan, 2011) and the incomplete ratio preference networks used in pairwise comparison-based preference measurement (PCPM; Scholz et al., 2010).Noting the exclusive focus on trade-off- versus non-trade-off-based evaluation types in stage 2 of the SEAs in prior research, we also propose and empirically test a new method that combines the two approaches, to overcome the associated drawbacks of each evaluation type. Specifically, we propose that an initial non-trade-off-based task helps presort the alternatives and thus simplifies the subsequent trade-off-based task, which then provides discriminatory evaluations. Because consumers are familiar with the presentation of information in clusters (as in search engines, price comparison websites, or shelf arrangements), the cognitive burden of the trade-off-based task subsequent to presorting should be much smaller than that associated with using the trade-off-based task alone. Our proposed method, the presorted adaptive self-explicated approach (PASEA), presents an attractive alternative to commonly used preference measurement methods by stimulating respondents to provide more discriminatory evaluations while still keeping the task manageable for them.That is, we give marketing researchers a means to understand the relationships among different SEAs, which is of particular importance considering the vast and constantly increasing diversity of approaches proposed in extant literature. It can help analysts appreciate the applicability and performance of both traditional and more sophisticated methods. We also propose a new SEA to overcome problems associated with the use of either trade-off- or non-trade-off-based evaluation types for determining attribute importance weights.The remainder of this article is organized as follows: Section ‘Overview of self-explicated approaches’ provides an overview and classification of SEAs. In Section ‘Development of a new method’ we discuss the combination of trade-off- and non-trade-off-based evaluation types in stage 2 of SEAs. After we describe the setup of the empirical study in Section ‘Empirical study design’, we present the results in Section ‘Empirical study results’. We conclude with a summary in Section ‘Summary’.The foundation for SEAs is the linear additive multiattribute utility model. The utility of a product is the sum of its partworths, defined as the product of the desirability of the attribute levels and the associated attribute importance weights (Srinivasan, 1988):(1)up,h=∑j∈J∑mj∈MjPWmj,h·xp,mj(p∈P,h∈H)and(2)PWmj,h=wj,h·bmj,h(j∈J,mj∈Mj,h∈H),whereup,h: utility of product p for respondent h;PWmj,h: partworth of level m of attribute j for respondent h;xp,mj: binary variable that shows if level m of attribute j is part of product p;wj,h: importance weight of attribute j for respondent h;bmj,h: evaluation of level m of attribute j for respondent h;P: index set of products;J: index set of attributes;Mj: index set of levels of attribute j; andH: index set of respondents.Accordingly, an evaluation of both attribute importance weights and attribute levels is needed to determine utility partworths. This evaluation typically involves up to three stages, though a two-stage process is the most common. Fig. 1depicts a framework that classifies existing SEAs according to the number of stages and the evaluation type used in each stage.As Fig. 1 shows, in some studies, respondents can choose totally unacceptable attribute levels in stage 0 (e.g., Srinivasan & Park, 1997), which then are not presented in the subsequent stages. The advantage of this stage is the reduction of the number of attribute levels that need to be evaluated in the subsequent stage 1.In stage 1, a respondent evaluates the desirability of the levels within each attribute. To elicit respondents’ evaluations, researchers use non-trade-off-based evaluation types, such as ratings, or trade-off-based evaluation types, such as rankings or paired comparisons. The ranking method assumes equal differences between the ranks, so the evaluation of the desirability of an attribute level corresponds to the inverted rank (Pullman et al., 1999). In paired comparisons, respondents compare all possible pairs of attribute levels. Scholz et al. (2010) illustrate how the number of total paired comparisons thus can be kept to a minimum by using incomplete ratio preference networks, in which only a subset of all possible attribute level pairs are compared on a nine-point scale.The rating method allows for more flexibility in the evaluation. Respondents rate the levels separately or in comparison with either the most attractive or the most and least attractive levels (e.g., Eggers & Sattler, 2009; Green & Krieger, 1990). Researchers typically rescale the evaluations of the attribute levels so that the most preferred level receives an evaluation of 11 and the least preferred is 1 (e.g., Netzer & Srinivasan, 2011). The intermediate levels of each attribute are scaled accordingly.In stage 2, a respondent evaluates the importance of each attribute. To avoid different interpretations by respondents, the attribute importance should be defined as the improvement from the least to the most preferred level of an attribute (Srinivasan, 1988). For the evaluation of attribute importance, researchers also have implemented several methods, which again can be classified as trade-off-based or non-trade-off-based evaluation types. The latter rely on ratings and the use of dollar metrics. In such rating tasks, respondents rate the importance of an attribute either separately (e.g., Pullman et al., 1999; Pöyhönen & Hämäläinen, 2001) or relative to the most important attribute (e.g., Pöyhönen & Hämäläinen, 2001; Srinivasan & Park, 1997). Less popular is the use of the dollar metric (Leigh, MacKay, & Summers, 1984), for which the dollar value the respondent is willing to pay for the improvement from the least to the most preferred level of an attribute represents that attribute’s importance weight.The constant-sum and ranking evaluation types are among the more popular trade-off-based evaluation types for stage 2. In the constant-sum method (e.g., Louviere & Islam, 2008), respondents divide a fixed number of points (usually 100) across attributes, and the score each attribute receives represents its importance weight. Rankings provide another means to evaluate the importance of an attribute, such that the importance weight is equal to the inverted rank, assuming an equal difference between ranks. Other trade-off-based evaluation types in stage 2 include those based on choice decisions: For example, Orme (2003) uses paired comparisons of all attributes, and Louviere and Islam (2008) employ a similar maximum difference scaling method (also called best-worst scaling). The comparison consists of four to six attributes per choice set, and respondents indicate which attributes are most and least important to them. For both evaluation types, researchers use multiple comparisons to estimate importance weights, according to a multinomial logit model.Chrzan and Golovashkina (2006) propose Q-Sort as another trade-off-based evaluation type for stage 2. It imposes a quasi-normal distribution of importance weights and an unbounded rating scale for evaluations. In a Q-Sort task, respondents classify the attributes into importance categories, the number of attributes per category is given, and the difference between categories is equal. For example, with ten attributes to evaluate, the respondent chooses five importance categories (first [fifth] category=most [least] important) and classifies them at a ratio of 1:2:4:2:1 into the five classes. The unbounded rating scale has no predefined number of scale points. Respondents rate an attribute with as many i values for important and u values for unimportant as they feel are appropriate and thus have greater freedom in their evaluations.More recent trade-off-based evaluation types include the methods proposed by Netzer and Srinivasan (2011) and Scholz et al. (2010). Netzer and Srinivasan’s ASEA uses a combination of two evaluation types to determine attribute importance: Respondents first rank the attributes according to their importance, followed by several (adaptively chosen) constant-sum paired comparisons of selected pairs of attributes to improve the evaluation of appropriate distances between ranks. Scholz et al.’s PCPM instead extends the use of paired comparisons of attribute importance weights by using the incomplete ratio preference networks already discussed in stage 1.In summary, SEAs consist of up to three stages, and researchers can choose different evaluation types for the attribute levels (stage 1) and the attribute importance evaluation (stage 2). Because researchers can choose the evaluation types in stages 1 and 2 independently, many combinations of these types, and thus many different SEAs, exist.For several reasons, two-stage SEAs became the standard. The SEAs that only considered attribute levels assumed equal importance weights and thus provided unrealistic results, and SEAs relying solely on stage 2 ignored the attribute levels and range of each attribute. Because the importance weight of an attribute changes with its range, ignoring the range would lead to biased importance weights (von Nitzsch & Weber, 1993). In general, researchers recommend avoiding stage 0 (e.g., Sawtooth-Software, 2007a), because self-reports of the total unacceptability of attribute levels are unreliable (Yee, Dahan, Hauser, & Orlin, 2007), particularly when respondents are too quick to mark attribute levels as unacceptable (e.g., Green, Abba, & Bansal, 1988).In two-stage SEAs, stage 1 usually provides reliable results with good predictive power, irrespective of the particular method used, because of the relative simplicity of attribute level comparisons (e.g., Green et al., 1993; Pullman et al., 1999). Respondents can just compare levels of the same attribute, which they use to perform on a daily basis. In contrast, stage 2 results frequently are unsatisfactory, with inadequate importance weights that reduce their validity (e.g., Green et al., 1993; Pullman et al., 1999). The most common stage 2 evaluation types, rating and constant-sum methods, produce no significant differences in prediction ability (Pullman et al., 1999), though both types have known weaknesses that reflect the general disadvantages of non- and trade-off-based evaluation types, respectively. Even though non-trade-off-based evaluation types are cognitively less challenging, respondents often rate all attributes as (equally) important (Krosnick & Alwin, 1988), which leads to insufficient discrimination and thus limited usefulness for marketers. Respondents use the mean and variance of rating scales differently, which makes comparisons across them challenging too (Chrzan & Golovashkina, 2006).Results from trade-off-based evaluation types can be easily compared across respondents; due to their very nature, they also provide discrimination between evaluations. However, these methods become very taxing if the evaluation includes more than ten attributes. For example, Sawtooth-Software (2007b) reports that the ranking task in stage 2 is already challenging for respondents if the number of attributes exceeds 7, which lowers the accuracy and consistency of choice decisions, because respondents try to simplify the task and switch to simple decision heuristics (e.g., Bettman, Johnson, & Payne, 1990). Long lists in particular tend to have low test–retest reliability, which goes against the main advantage of SEAs, namely, their ability to determine attribute importance weights for a large number of attributes.The combination of several trade-off-based evaluation types in stage 2, as proposed in Netzer and Srinivasan’s (2011) ASEA, promises to alleviate the high task complexity for many attributes while still outperforming rating tasks (a non-trade-off-based evaluation type) and the constant-sum method (trade-off-based evaluation type; Srinivasan & Wyner, 2009) in stage 2 with respect to their predictive ability. However, the merits of such a combination have not been compared against other competing SEAs. Similarly, the use of incomplete ratio preference networks for paired comparisons, as proposed by Scholz et al. (2010), may reduce the cognitive burden for respondents in trade-off-based tasks in stage 2 of SEAs, but again, with the exception of a German research article that studied theme parks and found ASEA to be on par with PCPM (Meissner, Decker, & Adam, 2011), no empirical comparisons exist to contrast these methods.In summary, the two-stage process is standard for SEAs, and it is commonly acknowledged that the determination of importance weights (stage 2) requires careful consideration. Although researchers have used several evaluation types to support importance determination, an extensive, empirical comparison of two-stage SEAs that differ in their importance determination is missing. Existing comparisons of attribute importance measurement methods ignore attribute levels and the range of attributes (e.g., Chrzan & Golovashkina, 2006). In our empirical study, we therefore compare the most popular two-stage SEAs that evaluate attribute importance in different ways, including more recently published methods such as ASEA (Netzer & Srinivasan, 2011) and paired comparisons using incomplete ratio preference networks (Scholz et al., 2010). We also test these methods against our proposed combination of trade-off- and non-trade-off-based evaluation types in stage 2, as outlined next.We propose a new method, PASEA (presorted adaptive self-explicated approach), that uses sequential, adaptive evaluation methods to determine attribute importance weights in stage 2. This method alleviates the complexity associated with a high number of attributes in trade-off-based evaluation types while also avoiding the poor discriminatory power of non-trade-off-based evaluation types. We follow a suggestion by Netzer and Srinivasan (2011) who proposed, but did not implement similar modifications on the ranking task to improve its quality particularly in the context of large number of attributes. Although we focus on self-explicated approaches, the underlying idea of our proposed method applies in various settings that use trade-off- and non-trade-off-based evaluation types.In stage 1, the ASEA asks respondents to evaluate all levels of each attribute on an 11-point rating scale. To ensure discrimination of all levels, respondents choose the most and least preferred levels, to which values of 11 and 1 are assigned automatically. Then, they rate all remaining levels. Stage 2 of the ASEA consists of two steps. First, they see a list of randomly sorted attributes and rank them. The importance weights are then calculated proportional to the respective rank. Second, to remove the assumption of equal differences between ranks, respondents divide 100 points, multiple times, across several paired attributes. This step is adaptive, but its mechanism for selecting pairs of attributes assumes that the ranking is correct. This assumption also applies in the estimation stage, when importance weights for attributes that have not been evaluated by the respondent must be inferred (see Netzer & Srinivasan, 2011 for details).The extrapolation in ASEA works correctly only if respondents’ paired comparisons are consistent with the ranking obtained from the first step of stage 2. This consistency is rather unlikely however, because ranking tasks already tax respondents in studies with far fewer attributes than those usually found in SEAs (e.g., Dellaert, Brazell, & Louviere, 1999). To increase the consistency between the paired comparisons and ranking, we therefore propose PASEA, which simplifies the cognitively challenging trade-off-based ranking task by combining it with a preceding non-trade-off-based rating task. That is, instead of immediately asking respondents to perform the ranking in stage 2, we ask them first to rate the improvement from least to most preferred level of this particular attribute on a seven-point scale. These ratings serve to presort the attributes that should ease the subsequent ranking and paired comparison tasks. We compare PASEA with ASEA in Figs. 2 and 3.The motivation for this small, important adjustment is that a presorted initial ranking reduces the effort and complexity of the ranking task, because it reduces the number of rank changes and thus the number of (pairwise) position changes a respondent must make to transform the initial ranking into the final ranking (a respondent only has to adjust ranks for attributes rated equally). Fig. 4illustrates the advantage of a presorted ranking: Computer science literature reports that the effort to sort a random list of n attributes increases quadratically with an increasing number of attributes (Knuth, 1998). Presorting the list of attributes, as illustrated in Fig. 4, splits the ranking problem into smaller subproblems, requiring far less effort to rank, both in each subproblem and for the sum over all subproblems.To quantify the potential effort saving, Eq. (3) shows the average (av) and maximum (worst) number of rank changes (RCH) to the order of n attributes (Knuth, 1998).(3)RChav[worst](n)=14·n·(n-1)12·n·(n-1)∀n>10else.The average and maximum number of rank changes (Eq. (4)) depend on the number of points on the rating scale (e.g., five- or seven-point rating scale) and the distribution of the respondents’ ratings of the attributes across the rating scale. The number of rank changes obviously decreases with greater heterogeneity in the ratings for the attribute importance weights, such as when respondents exhibit more discrimination in their ratings. Therefore,(4)RChav[worst]∗(n)=∑c∈CRChav[worst](nc),wherenc: number of attributes rated by respondents with point c on the rating scale; andC: index set of scale points.A numerical example quantifies how much a sorted initial ranking reduces the number of rank changes. We consider a respondent who faces n=14 attributes and rates the importance of each attribute on a seven-point rating scale (C∈{1,2,3,4,5,6,7}). In case 1, the respondent uniformly distributes all 14 attributes across the seven-point scale, meaning that two attributes are rated as 1, two attributes as 2, and so on (i.e., 2:2:2:2:2:2:2). In case 2, a respondent uses a (quasi-) normal distribution across the scale (0:1:3:6:3:1:0), which means for example that one attribute has a rating of 2, three attributes have a rating of 3, and so on.Eq. (3) indicates, for a random list of attributes, that on average 45.5 (i.e., 1/4·14·(14–1)) and in the worst case 91 (i.e., 1/2·14·(14–1)) rank changes are necessary to transform the initial into the final ranking. The reduction in the number of rank changes for a presorted list of attributes depends on heterogeneity among attributes. If the attributes are uniformly distributed, Eq. (4) shows that the number of rank changes drops to 3.5 (i.e., 0.5+0.5+0.5+0.5+0.5+0.5+0.5) in the average and 7 (i.e., 7·1) in the worst case. Thus, sorting the initial ranking reduces the number of rank changes by 92.3% (=1−3.5/45.5). If the attributes are normally distributed, 10.5 (i.e., 0+0+1.5+7.5+1.5+0+0) rank changes are necessary in the average case and 21 (i.e., 0+0+3+15+3+0+0) in the worst case, which corresponds to a reduction of 76.9% (=1−10.5/45.5).These results suggest that more heterogeneous evaluations of attributes decrease the number of rank changes more strongly. The reduction in the number of rank changes for presorted lists of attributes is always higher than the number of attributes. Thus, the reduction in the number of rank changes should compensate for the additional ratings required for presorting. The advantages of presorting become even larger for more attributes, simply because the effort for a rating task rises linearly, whereas the effort for a ranking task rises quadratically with the number of attributes.The expected number of rank changes a respondent needs to perform to sort any given list into a preferred ordering also can be used as a benchmark for the effort the respondent puts into the survey. If the number of rank changes observed for the respondent is considerably smaller than the expected number of rank changes, this respondent likely is engaging in simplification strategies rather than fully evaluating the list. We therefore report both the expected and the observed numbers of rank changes in the empirical study to provide a proxy for survey engagement based on actual behavior.The aim of the empirical study is to compare seven SEAs, including the most popular, the recently published ASEA by Netzer and Srinivasan (2011), PCPM by Scholz et al. (2010), and PASEA. Using rating tasks in stage 1 for all SEAs, we can compare the convergent validity and predictive accuracy for different evaluation types to determine attribute importance weights in stage 2.Considering the scope of application of SEAs, we focus our empirical comparison on high-involvement products characterized by high complexity and many attributes. In cooperation with a leading telecommunication service provider, we conducted an initial survey about Triple Play—a bundled offer of high-speed Internet access, telephone, and television services. Through intensive discussions with the management of the telecommunication service provider and a comprehensive pretest study, we identified 13 important attributes and up to seven levels, as summarized in Table 1. We recruited 189 students and asked them to participate in a controlled lab environment at the university.To test the robustness of our results, we conducted a second survey about vacation packages. Since Scholz et al. (2010) published an SEA of paired comparisons using incomplete ratio preference networks (i.e., PCPM) after our first survey, we also implemented and included this method in our second survey. This survey accounts for additional aspects of SEAs, not covered in the first survey, as we discuss in more detail subsequently. This time, we used an online panel provider with 482 respondents. Thus, the second survey serves to validate our findings, not only in a different product category, but also using respondents who are representative of the population and in a non-lab, non-students-only environment.For the second survey, we used the same attributes and attribute levels as Scholz et al. (2010) in their survey 1 (see also Table 1). The advantage of using their setup is that it allowed us to be confident that the attributes and levels were selected with meticulous care, as these authors outlined. Unlike survey 1, the exclusion of price in survey 2 prevents possible correlations between attributes and levels, which may harm the elicitation of accurate preferences.Fig. 5illustrates the questionnaire design for both studies. We split the respondents randomly into different subsamples, on which we tested the performance of the various SEAs. Each questionnaire consisted of five parts.Part 1: product description: In both product categories, we provided respondents with a detailed description of the attributes and attribute levels used. For survey 1, we started with a video that illustrates the benefits of Triple Play and its different attribute levels. The video was professionally produced by the cooperating telecommunication service provider. For survey 2, we used the brief textual and pictorial descriptions also used by Scholz et al. (2010) to illustrate the attributes and attribute levels of the vacation packages.4We thank Reinhold Decker and Martin Meissner for their generous provision of their survey and the insights into their data analysis.4We excluded respondents who did not have any purchase experience and who were not intending to book a vacation package within the next 12months.Part 2: preference measurement task: Because we were mainly interested in the performance of different evaluation types in stage 2, we used the same rating task for all methods in stage 1 (i.e., for the evaluation of attribute levels). Respondents were randomly assigned to one of three subsamples in survey 1 and to one of four subsamples in survey 2. Subsample 1 (N1=63, N2=112) faced the classical SEA that uses the constant-sum method (constant-sum SEA [CSSEA]); subsample 2 (N1=61, N2=122) completed Netzer and Srinivasan (2011) ASEA; subsample 3 (N1=65, N2=127) completed our proposed PASEA; and subsample 4 (N1=n.a., N2=121), which only existed in survey 2, completed the incomplete ratio preference networks (PCPM) by Scholz et al. (2010). We also obtained three SEAs by using the results from the rating task only and the combined rating and ranking tasks only of PASEA (without further adjustments by pairwise comparisons) and the results from the ranking task only of ASEA (also without further adjustments by pairwise comparisons) as measures for importance weights. Thus, we compared six SEAs in total in survey 1 and seven SEAs in survey 2.The rating tasks in stage 1 were performed on an 11-point rating scale: Respondents first identified the most and least preferred attribute levels, which received the maximum and minimal ratings, then rated the remaining attribute levels relative to the most and least preferred attribute levels (e.g., Eggers & Sattler, 2009).For CSSEA, respondents had to divide 100 points among all presented attributes in stage 2. For ASEA, we presented a random list of attributes. For PASEA, we used a seven-point rating scale to rate all attributes (i.e., improvement from least to most preferred level of an attribute; see Srinivasan, 1988).The rating served to presort the attributes. Equally rated attributes thereby appeared in the order reverse that of their appearance in the desirability evaluation (see Table 1). In survey 1, 23 constant-sum paired comparisons of attributes (i.e., dividing 100 points across a pair of attributes) were used in both ASEA and PASEA. These comparisons were adaptive and based on the individual importance ranking. According to the functionality of the adaptive procedure, 23 comparisons equal the maximum number of comparisons in studies with 13 attributes. In survey 2, we only showed 9 paired comparisons and obtained the importance weights of the remaining attributes by applying the approximation proposed by Netzer and Srinivasan (2011).For PCPM, we implemented the paired comparisons using incomplete ratio preference networks only in stage 2 and used the same evaluation type in stage 1 as in ASEA, PASEA, and CSSEA.5We thank Martin Meissner for this suggestion.5We used a two-cycle design (see the Web Appendix of Scholz et al., 2010), such that each respondent had to make 20 paired comparisons. In contrast, Scholz et al. (2010) used pairwise comparisons for both stages. However, with our modification, we can focus on differences in the predictive performance or cognitive challenges exclusively due to stage 2. Even though this deviation from the original PCPM requires respondents to engage in two different mental tasks, it is less monotonic and thus more compelling for respondents.Part 3: survey complexity and effort: In this part of the survey, we assessed survey complexity, which we measured using Bettman, John, and Scott (1986) seven-item differential scale. We also recorded the time respondents needed to complete the survey as a measure of the effort a respondent expended on the task.Part 4: validation tasks: We chose two tasks to assess the validity of individuals’ responses in the previous parts. We first confronted respondents with four full-profile choice situations, each consisting of three alternatives. For each choice set, we avoided more than four identical attribute levels across two alternatives and any identical attribute levels across the three alternatives. For survey 2, we used the same choice sets that appeared in Scholz et al. (2010) vacation package survey.Then for the second validation task, we automatically estimated the importance weights of the respondents after they had completed the SEA. We plotted their importance weights and asked them to rate, on a seven-point rating scale, how well these weights matched their real importance weights. Such a rating task might be biased by the way the different methods are presented in stage 2 (e.g., the constant-sum evaluations are more similar to the graph than the paired comparisons and are thus likely to be more favorably evaluated), so survey 2 also included another task, in which respondents identified their own importance weights among four graphs. For illustration purposes, this task is included in the Appendix.Part 5: demographics: In the final part of the surveys, we asked respondents for their demographic information to ascertain whether they differed substantially across subsamples. In both surveys, the information included their gender, age, education, and income; we also gathered additional information on occupational status and household size in survey 2.We assessed the comparability of the different subsamples using the demographic information collected in part 5 of the surveys. The t-tests did not indicate any significant differences (p>.10) between the subsamples within each survey.Here we report the attribute importance weights as determined by the different methods in the two product categories. High correlations among attribute importance weights demonstrate a high convergent validity. Table 2reports the mean attribute importance weights (normalized to sum to 100 for each method) as determined by seven methods: CSSEA, Ranking, ASEA, Rating, Rating&Ranking, PASEA, and PCPM.Table 2 reveals that in the Triple Play category, the attribute importance weights determined by the different methods are highly similar. Except for Rating, the correlations are all above .75, and all other methods consistently conclude that price, access speed, and telephone tariff are the three most important attributes. The different methods exhibit small differences between the three least important attributes, but all methods converge on the program information and provider as among the least important attributes.In the vacation packages category, the convergent validity of the different methods is considerably lower, such that the correlations between CSSEA and the other methods in particular is very low; that for Ranking and Rating&Ranking is even negative. Correlations between the most recently discussed methods in literature, ASEA and PCPM, and with our proposed method PASEA are very high (>.65). These methods consistently determine the most and least important attributes in this category.Table 2 also reveals that Ranking, Rating, and Rating&Ranking provide very little discrimination between attribute importance weights, such that Rating exhibits the smallest range between the most and least important attributes. Thus, our results confirm previous findings (e.g., Krosnick & Alwin, 1988; Pullman et al., 1999) that non-trade-off-based methods do not provide sufficient discrimination between importance weights; they also suggest that trade-off-based Ranking enables only marginally more distinction. In both categories, Ranking, Rating, and Rating&Ranking also lead to a lower average dispersion of attribute importance weights across respondents: The mean standard deviations across attributes are only about half the size of the heterogeneity detected by CSSEA, ASEA, and PASEA. Thus, Ranking, Rating, and Rating&Ranking fail to discriminate not only across attributes but also across respondents. The amount of heterogeneity detected by PCPM is also considerably smaller than with CSSEA, ASEA, and PASEA, likely because CSSEA, ASEA, and PASEA all use 100-point scales in their paired comparisons, whereas PCPM evaluations are restricted to a 9-point scale. Additional research should investigate this issue further.In summary, our proposed PASEA exhibits high convergent validity compared with the most prominent (CSSEA) and most recently proposed (ASEA and PCPM) methods. At the same time, it overcomes two issues commonly associated with non-trade-off-based evaluation types, namely, the failure to discriminate across attributes (reflected by small ranges) and the failure to discriminate across respondents (reflected by small standard deviations).Table 3provides insights into the ability of each method to predict respondents’ choices in four holdout tasks, each consisting of three alternatives. In both categories, our proposed PASEA outperforms all other methods, as measured by first-choice hit rates. This outperformance is significant at p<.1 for the comparison with ASEA and at p<.01 for all other comparisons in the Triple Play category and for CSSEA (p<.1), Ranking (p<.05), and PCPM (p<.1) in the vacation packages category.In addition to the hit rates discussed, Table 4reports the perceived quality of importance weights, for which respondents rated how well the importance weights determined by the different methods matched their true importance weights. The importance weights evaluated thus were based on the fullest possible information extracted from the respondent; that is, a respondent in the ASEA subsample viewed the importance weights based on their ASEA results, not on their Ranking results (as was the case for PASEA respondents). Again, PASEA outperforms CSSEA and ASEA in the Triple Play category (survey 1), though this outperformance is only significant in the latter comparison (p<.05). For the vacation package category (survey 2), PCPM is the method for which respondents reported the highest similarity between their determined and real importance weights. However, none of the four methods performs significantly better or worse on this criterion in this category.Table 4 also provides the hit rate for the identification of the own importance weights among the set of four alternative importance weights in survey 2. This additional measure of individual-level fit was motivated by the similarities between the SEAs (particularly CSSEA) and the presentation of the importance weights Rating, which might have biased the results. This time, PASEA and PCPM perform best, with no significant differences. Both methods perform significantly better than CSSEA and ASEA. Even though respondents directly enter the importance weights in the constant-sum task in CSSEA, they fail to recognize them a few minutes later (worst hit rates, 51%). Respondents also struggle with ASEA, perhaps due to the inconsistencies in the ranking task versus the paired comparisons in stage 2.An important characteristic of any SEA is respondents’ perception of its complexity. High task complexity may lower accuracy and decision consistency (e.g., Schlereth, Skiera, & Wolk, 2011), because respondents try to simplify the task and switch to simple decision heuristics (e.g., Bettman et al., 1990). We measured survey complexity using Bettman et al. (1986) seven-item differential scale (with a higher rating corresponding to lower complexity). Before the comparison, we calculated Cronbach’s alpha (variance extracted) values for each version. All values are larger than .70 (50%), so the complexity scale appears adequate to establish the individual indicators and constructs (Cortina, 1993). To measure survey effort, we used the time a respondent spends on the task (Netzer & Srinivasan, 2011; Scholz et al., 2010).In both categories, the CSSEA outperforms all other SEAs in terms of survey complexity (see Table 5), and the difference is significant at p<.01 for both ASEA and PASEA but insignificant for PCPM in survey 2. With respect to survey duration, CSSEA also beats the more recently proposed methods, including PASEA: The time spent on the importance evaluation task is significantly lower for CSSEA than for ASEA and PASEA (both p<.01 for Triple Play; p<.1 and p<.01, respectively, for vacation packages), but again, no significant differences were recorded for CSSEA versus PCPM. All methods stayed well below the 20-min mark, which is the maximum amount of time people generally can attend to one task (Dukette & Cornish, 2009), so it is unlikely that the effort associated with either method is overwhelming for respondents.Thus, CSSEA and PCPM are perceived as the easiest SEAs, and respondents need the least amount of time to complete them. This superiority over ASEA and PASEA likely arises because CSSEA and PCPM both require respondents to engage in only one mental process (dividing 100 points and doing pairwise comparisons, respectively), whereas ASEA and PASEA require two and three different mental processes, respectively (ranking and pairwise comparisons for ASEA and rating, ranking, and pairwise comparisons for PASEA). Also, even though the presorting in PASEA theoretically should ease the task, compared with the effort required for ASEA, the latter is perceived as less complex and took less time in both categories (though none of these differences is significant).Both perceived complexity and survey duration seem to suggest that CSSEA and PCPM are easier to answer, whereas ASEA and PASEA are more difficult. However, the validity of the former measures is questionable, and neither is necessarily correlated with task engagement. As we show subsequently, respondents in ASEA also may have been substantially less engaged than respondents in PASEA. For example, a shorter time to complete the survey may imply that the task requires less effort to complete and is thus more likely to provide reliable results, or it might just mean that respondents rush through the task without serious effort, leading to less reliable results. As long as we do not directly measure respondents’ brain activity, these measures can only be viewed as indicative and should be treated with caution. For the comparison between ASEA and PASEA, we discuss an additional measure of engagement, as outlined in Section ‘Numerical example to illustrate reduction of task complexity’. Because this measure is based on respondents’ behavior, we believe it offers a more reliable assessment than just survey duration or perceived complexity. In a similar spirit, we derive a proxy for engagement in CSSEA, based on respondents’ behavior.Because self-assessments of survey complexity or survey duration may be poor measures of task engagement, we consider alternative measures of engagement based on respondents’ behavior for CSSEA, ASEA, and PASEA; we leave it to future research to construct a valid engagement measure for PCPM. As we discussed in Section ‘Numerical example to illustrate reduction of task complexity’, the measure of engagement we propose for ASEA and PASEA is based on the number of rank changes a respondent makes to sort the initial ranking into his or her preferred version. Table 6shows for both surveys and both SEAs the expected number of changes a respondent should make, on average and in the worst case, when the initial order of the attributes is completely opposite the preferred one. The number of changes for both the average and worst case are lower for PASEA than for ASEA, as a result of the initial presorting by rating in the former. For an engaged respondent who tries to sort the list of attributes into a preferred order, we expect to observe a number of changes that lies within these respective ranges.The first two columns in Table 6 imply that ASEA respondents theoretically should make more rank changes than PASEA respondents. In both studies, the observed number of rank changes for PASEA lies within the expected range, whereas the observed number of rank changes for ASEA is substantially below the lower bound. The low number of changes is indicative of simplification behavior; respondents are most likely concentrating on the most and least important attributes. We can test this proposition by comparing the correlations between the final attribute importance weights and the attribute ranks. A low correlation suggests that respondents used the paired comparisons in stage 2 to correct for their errors in the ranking. In both studies, this correlation was significantly higher for PASEA than for ASEA. We therefore conclude that respondents did not put as much effort into ASEA as into PASEA.For CSSEA, the measurement of task engagement is less straightforward, but previous research indicates that respondents have problems distributing values that sum to 100 (Sawtooth-Software, 2007c), so they simplify the task by allocating points only to a subset of (more important) attributes. We find indications of such simplification behavior in both studies: 26% and 25% of all importance evaluations are 0 in survey 1 and survey 2, respectively. Similarly, 40% of the respondents in survey 1% and 36% of the respondents in survey 2 allocate 0 points to more than three attributes.

@&#CONCLUSIONS@&#
