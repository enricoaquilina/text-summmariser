@&#MAIN-TITLE@&#
A mobile robot based system for fully automated thermal 3D mapping

@&#HIGHLIGHTS@&#
We propose a fully autonomous system for 3D thermal modeling of buildings.A robot finds the positions for data acquisition using 3D sensor placement planning.Data from a laser scanner, a thermal camera, and a photo camera are automatically joined into one full model.Post-processing prepares the data for inspection in a viewer and points out interesting parts in the environment to experts.

@&#KEYPHRASES@&#
3D thermal mapping,Autonomous mobile robots,Calibration,Next-best-view-planning,Visualization,

@&#ABSTRACT@&#
It is hard to imagine living in a building without electricity and a heating or cooling system these days. Factories and data centers are equally dependent on a continuous functioning of these systems. As beneficial as this development is for our daily life, the consequences of a failure are critical. Malfunctioning power supplies or temperature regulation systems can cause the close-down of an entire factory or data center. Heat and air conditioning losses in buildings lead to a large waste of the limited energy resources and pollute the environment unnecessarily. To detect these flaws as quickly as possible and to prevent the negative consequences constant monitoring of power lines and heat sources is necessary. To this end, we propose a fully automatic system that creates 3D thermal models of indoor environments. The proposed system consists of a mobile platform that is equipped with a 3D laser scanner, an RGB camera and a thermal camera. A novel 3D exploration algorithm ensures efficient data collection that covers the entire scene. The data from all sensors collected at different positions is joined into one common reference frame using calibration and scan matching. In the post-processing step a model is built and points of interest are automatically detected. A viewer is presented that aids experts in analyzing the heat flow and localizing and identifying heat leaks. Results are shown that demonstrate the functionality of the system.

@&#INTRODUCTION@&#
Imagine a technology that automatically creates a full 3D thermal model of an environment and detect temperature peaks in it (cf. Fig. 1). Such a system would be a big step in monitoring and inspection of existing buildings and technical assets as well as in achieving energy efficiency in building construction. For example, data centers and factories rely on correct functioning of their infrastructure. Damaged pipes and cables or other parts endanger their functionality, cause pauses in the work flow and may lead to harmful fires. In many cases leaks and overheating could be detected in their early stages by use of thermography thus preventing further damage. Building construction has undergone major changes in recent years. The importance of energy efficiency has attracted notice. To meet the Passivhaus, the Zero-energy building, or even the Energy-plus building standard modern building design makes use of all available heat sources including electrical equipment or even the body heat from people and animals inside the building. While this leads to changes in the way buildings are designed it also poses the question how existing buildings can be modified to meet these standards and to eliminate heat and air conditioning losses. According to the Action Plan for Energy Efficiency [13] of the European Commission the largest and cost-effective energy savings potential lies in residential (≈27%) and commercial (≈30%) buildings. The system proposed in this article is meant to aid in reaching these savings.The current state of the art for analyzing temperature related issues is thermal imaging. Fouad and Richter present a guideline for thermography in the building industry in Germany [20]. To detect thermal bridges of exterior building walls outdoor thermography is commonly used. Thermal bridges lead to a loss of energy and can cause humidity and mold growth. Only a few images are necessary to capture the entire building but at the expenses of the resolution. To detect flaws in the construction a difference of 15K is necessary between indoor and outdoor temperature to come to significant conclusions. It is desired that the weather conditions remain stable over a longer period of time, making the morning hours in the winter months ideal.Keeping stable conditions is easier to achieve for indoor thermography. The analysis of back-ventilated walls and roofs is only possible from indoors. Thermal bridges at exterior walls and interior walls connecting heated and unheated rooms, pillars that interrupt the thermal insulation of a building, air leaks at windows and doors and the moisture penetration at basement walls are the common applications for indoor thermography, that focus on energy efficiency in existing buildings. Other applications aim at documenting and examining the run of heating pipes, detecting blocked pipes and construction units in a building to eliminate flaws, to make room for improvements and, in some cases, to ensure safety. For new buildings or the energetic restoration of existing buildings it has also become common to perform thermography before, during and after the construction phase for quality management.Monitoring and analysis of the data for a large building is tedious. For indoor thermography the room should be prepared at least six hours before the inspection to achieve best results. A constant temperature is desired for this period. Furniture has to be moved away from the walls to allow their inspection. The difference between indoor and outdoor temperature has to be at least 10°C. During the inspection each room is examined with the thermal camera. For each picture the inspector has to note the exact position and orientation from where the picture was taken [19]. After the inspection the images have to be analyzed taking into account the room temperature, the humidity, the material of the wall and the angle from which objects are seen. For some applications, e.g., inspection during construction or renovation, it is also necessary that the changes are documented over time, thus asking for comparability between independently acquired datasets [20].Thermal images document the precise temperatures without any spatial dimensions. To identify a heat source and measure its extent it is necessary that the expert analyzes the scene on-site which is time-consuming and in cases even dangerous. For applications that require repeated thermography over time, comparison of the 2D data is only possible to some extent. Successfully modifying a building with respect to thermal issues involves extensive planning. This planning would greatly be improved by the existence of a geometrically correct thermal model. We propose a robotic system that creates a full 3D model of the environment with color and thermal information (cf. Fig. 1) enabling an expert to fully analyze the recorded scene offline on a computer. Furthermore, regions of interest, e.g., regions with temperature peaks or drastic changes in temperature, are automatically detected and pointed out to the user, increasing the efficiency of the analysis. The spatial accuracy of the model enables one to measure the extent of the area in question and its immediate localization in the building.Our system creates a 3D thermal model of the area autonomously. The full 3D model makes it easy to identify the location of each picture as it is shown within its surroundings and its position is known. This is especially important since indoor photos capture only a small part of the scene. Automatic registration, as known from robotics, enables the merging of two models acquired at different times. According to Fouad and Richter [20] thermography can only be an auxiliary device. The analysis of the scene is only possible through expertise and experience. By automatically pointing out regions of interest in the data our system helps to find damages quickly during the analysis.This article presents our approach to fully autonomous 3D thermal modeling of buildings. We combine thermal imaging with the technology of terrestrial laser scanning. The system has to fulfill three main tasks. First, the data needs to be acquired and registered, i.e., all data from different sensors and positions needs to be put into one common reference frame. Second, the positions where sensor data is collected need to be planned and driven to autonomously. Third, the data has to be analyzed and presented to a user. Experiments evaluate the performance of the system and results show exemplarily a complete 3D model of an office environment collected by the system. We extend our initial work [8] by several components. The system was completed with a photo camera that allows better identification of objects in the resulting model. Errors in the calibration procedure were reduced with a novel method and the automatic detection of interesting temperature distributions was implemented in the post-processing step. The main contribution is the extension of the exploration strategy to consider the full 3D structure of the environment. Instead of exploring solely based on the floor plan of the building, the system tries to find positions from where the entire 3D space can be seen, reducing the amount of occlusions in the captured data.To access the energy efficiency of houses thermal cameras are commonly used. These cameras measure temperatures precisely, but return only 2D images of the environment and therefore the loss of energy can only be roughly quantified. Images are projections to 2D. From a sequence of images it is in principle possible to perform a 3D reconstruction. These approaches are called bundle adjustment or structure from motion (SFM). Bundle adjustment uses image features to calculate 3D positions of them. SFM adds automatic data association and therefore solves the simultaneous localization and mapping (SLAM) problem, i.e., the problem of recovering the 3D structure of the environment and the sensor poses (position and orientation). Since reliable solutions to image based 3D reconstruction from thermal images alone have not been presented yet, a second sensor has to be used to create the 3D model. Instead of using a co-calibrated system of a thermal camera and an RGB camera we use the emerging technology of terrestrial laser scanning.Laser scanning methods are well established in the surveying community and in robotics. Terrestrial 3D laser scanning systems yield precise 3D point clouds. Scanning from different poses enables one to digitize a complete indoor environment and to resolve occlusions. Registration algorithms from the geodesy and robotics community are available to automatically align scan views from different poses.Related work in inspection robotics includes human detection with thermal cameras using temperature signatures [31]. Högner and Stilla present a modified van as surveying vehicle for acquiring thermal images in urban environments [24]. However, the focus is on outdoor environments and image-based techniques like SFM. Prakash et al. present stereo imaging using thermal cameras, but focus on small scale applications [44]. Iwaszczuk et al. suggest an approach to map terrestrial and airborne infrared images onto existing building models [25]. The model is textured by extracting polygonal parts from the image and mapping those onto the model using standardized masked correlation. Only little work has been done to combine 3D scanners and thermal cameras. Carbelles et al. present a methodology to exhaustively record data related to a World Heritage Monument using terrestrial laser scanning, close range photogrammetry and thermal imagery [10]. They use four different sensors for data acquisition: a reflectorless total station, a terrestrial laser range scanning sensor, a digital photo camera and a thermal camera. With a total of eight natural control points a total station they relate the geometry between different sensors. Pelagottia et al. present a first automatic approach for multispectral texture mapping [43]. Their method is based on the extraction of a depth map in the form of an image from the model geometry, whose pixels establish exact correspondences with the vertices of the 3D model. The registration with the chosen texture is performed based on the maximization of mutual information.3D environment mapping using 3D scanners on mobile robots are subject to research [51,36]. Building thermal 3D models of environments has received some attention recently. Ham and Golparvar-Fard model and evaluate thermal models and the energy performance of buildings [23]. For this purpose they co-calibrate a thermal camera with an RGB camera. The color images are used to create a 3D model using SFM. SFM approaches are prone to failure in regions with few features or repeating structures, the density of the resulting model is low and the scale is unknown. Vidas et al. [56,48] focus on co-calibrating a thermal camera and a Microsoft Kinect. They developed a hand-held system that creates a 3D model of the environment based on registering the Kinect data. This approach yields a dense model but is limited to the accuracy of the Kinect camera and requires a human operator. Laser scanning has the advantage that the resulting dense model has a high geometric accuracy and is not as sensitive to repeating structures and feature-less areas as SFM approaches are. Gonzáles-Aguilera et al. [21] combine the technology of laser scanners and thermal cameras to create models of building exteriors. They extract features from both the thermal images and the projections of the point cloud from the laser scanner and match these to register the data. In indoor environments this approach is prone to errors as only small parts will be visible due to the small opening angles of thermal cameras. In combination with the low resolution of typical thermal cameras images tend to have too few features for a reliable registration. To the best of our knowledge, a fully autonomous system for modeling using 3D scanning and thermal imaging has not been done yet.Sensor placement planning is needed for the goal directed acquisition of 3D data. The task of a sensor placement planning algorithm is to find a set of sensor configurations needed for obtaining a detailed environment model. Since a typical 3D laser scan takes 3–5min for one position, depending on the resolution, it is desirable to minimize the number of scanning positions. This leads to an optimization problem similar to the Art Gallery Problem (AGP) (where to place guards such that the entire gallery is guarded). The AGP problem is NP hard and is usually solved by heuristics that perform well in practice [22]. These methods are categorized as model-based sensor placement planning (a priori model of the environment is known) and non-model-based methods. The latter are applied for exploration tasks in which the robotic system has to navigate autonomously in an unknown environment and build its own model. The planner must determine the next-best-view (NBV) based on the information collected from previous scans. Most exploration strategies push the robot onto the border between explored and unexplored regions [17,54]. The majority of exploration algorithms is not reliable when applied under real conditions due to the sensitivity to uncertainty of measurements, localization, and map building. A small divergence in localization at the pre-computed NBV point can lead to many unnecessary movements. Moorehead et al. include the uncertainty of the robot pose into the exploration strategies [34]. Recently, numerous sensor placement planning algorithms have been developed for the reconstruction of 3D environment models. Most methods take 3D scans based on a 2D exploration strategy [51]. For creating a full 3D thermal model of a building it is essential to consider the 3D geometry of the environment to ensure that all parts of the building are mapped. Blaer and Allen propose a 3D NBV method which plans additional viewing locations based on a voxel-based occupancy procedure for detecting holes in the model [6]. Low and Lastra present a full non-model-based 3D NBV method based on an exhaustive hierarchical 3D view metric evaluation [29]. However, the computational complexity is still the major challenge in designing practical 3D NBV solutions.The process of generating a surface from a set of 3D points is called surface reconstruction. The last challenging task in building a 3D thermal model of an indoor environment is the reconstruction of the 3D mesh with the temperature scalar field mapped onto it. Methods to reconstruct the 3D surface from a point cloud can be divided into two distinct groups, namely face-based reconstruction (triangulation) and iso-surface reconstruction (meshing). The most common triangulation methods use are the Voronoi [2] and 3D Delaunay [3] algorithms. Su and Scot Drysdale present several implementations of the latter algorithm analyzing the performance [50]. These algorithms require closed and organized geometry, i.e., that the model has to be fully connected and without major outliers in the point cloud. The algorithms tend to close the structure of the model, which is good for the digitization of cultural heritage sites as presented in [12,1] but not sufficient for environment reconstruction in terms of time and memory efficiency as well as overall output quality. Iso-surface reconstruction, unlike the triangulation which reconstructs part-by-part, generates iso-surfaces, meaning that the model is generated by connecting the points with constant values, i.e., distance, pressure, temperature, etc. The most popular algorithm for iso-surface reconstruction is the Marching Cubes Algorithm (MCA) developed in [28]. This algorithm starts with dividing the input space into cubes. After selecting a random cube and its eight neighbors, the algorithm determines the polygonal part of the iso-surface passing through the cube.There are also modifications to the standard MCA presented in [11,35] that deal with “ripples” caused by topological inconsistencies of the model (or point cloud acquisition). Since the space is divided equally, it is suitable for the purpose of 3D thermal model reconstruction of indoor environments, as it allows for thermal inspection of the space segments. In order to create iso-surfaces realistically, it is important to calculate precise iso-values. This is typically done by ball pivoting [4], Poisson surface reconstruction [27] and butterfly subdivision of surfaces [33]. These methods require very high resolution point clouds resulting in large data sets (millions of points) and are generally very slow.The setup for simultaneous acquisition of 3D laser scan data and thermal images is the robot Irma3D (see Fig. 2). Irma3D is built of a Volksbot RT-3 chassis. Its main sensor is a Riegl VZ-400 laser scanner from terrestrial laser scanning. A thermal camera is mounted on top of the scanner. The optris PI160 thermal camera has an image resolution of160×120pixels and a thermal resolution of 0.1°C in a spectral range of 7.5–13μm. It acquires images at a frame rate of 120Hz and with an accuracy of 2°C with a field of view of approximately 40°×64°. For acquisition of color data a Logitech QuickCam Pro 9000 webcam is used featuring a video resolution of1600×1200pixels. The laser scanner acquires data with a field of view of360°×100°. To achieve the full horizontal field of view the scanner head rotates around the vertical scanner axis when acquiring the data. We take advantage of this feature when acquiring image data. Since the cameras are mounted on top of the scanner, they are also rotated. We acquire 10 images per camera during one scanning process to cover the full360°. To avoid blurring and the problems that come from the necessity of synchronization we refrain from taking the images while scanning. Instead we perform a full 360° rotation for scanning and rotate back with stops at the image positions. A further advantage of this strategy is that the cameras can be connected with regular USB cables because the cable is unwound after each rotation. For obstacle avoidance a SICK LMS100 2D laser scanner is installed at the front of the robot.After acquiring the 3D data it has to be merged with the image information. This processing consists of five steps that will be explained in this section.Each sensor perceives the world in its own local coordinate system. To join the perceived information we need the specific parameters of these coordinate systems. Each camera has unique parameters that define how a point(X,Y,Z)in world coordinates is projected onto the image plane. These parameters are calculated through a process known as geometric camera calibration. Given the focal length(fx,fy)of the camera and the camera center(cx,cy)image coordinates(x,y)are calculated as:(1)xy1=fx0cx0fycy001X/ZY/Z1.Given the radial distortion coefficientsk1,k2,k3and the tangential distortion coefficientsp1,p2andr=x2+y2the corrected image points(xc,yc)are calculated as(2)xcyc=x(1+k1r2+k2r4+k3r6)+2p1y+p2(r2+2x2)y(1+k1r2+k2r4+k3r6)+p1(r2+2y2)+2p2xTo determine the parameters of optical cameras chessboard patterns are commonly used because the corners are reliably detectable in the images. A number of images showing a chessboard pattern with known number and size of squares are recorded. In each image the internal corners of the pattern are detected. The known distances between the corner points in world coordinates allow to formulate Eqs. (1) and (2) as a non-linear least squares problem and to solve for the calibration parameters [9].For low resolution thermal cameras a chessboard pattern is error-prone even after heating it with an infrared lamp. For pixels that cover the edge of the squares the temperature is averaged over the black and white parts thus blurring the edges. Luhmann et al. [30] have explored the calibration procedure using different types of thermal cameras. Generally an object with a unique pattern of distinct targets is used which eases labeling and increases accuracy of the calibration process. The points are actively or passively heated. In case of passive heating different materials cause the pattern to show up. Luhmann et al. developed a pattern consisting of targets of self-adhesive foil on an aluminum plate. While the targets emit radiation based to their own temperature the reflective metal surface reflects the cold temperature of space thus leading to a strong contrast. Unfortunately this concept is not applicable for the co-calibration of the thermal camera and a laser scanner as it is very difficult to position the board in a way that the sky is reflected without occlusions and the board is completely visible in the laser scan. Instead we suggest a pattern with clearly defined heat sources such as small light bulbs as it shows up nicely in thermal images.Fig. 2 shows our pattern in the background. It is composed of 30 tiny 12V lamps, each with a glass-bulb diameter of 4mm. The overall size of the board is 500mm (width)×570mm (height). Identifying the heat sources in the image enables us to perform intrinsic calibration in the same way as for optical cameras. The approach is similar to the approach used by Ham and Golparvar-Fard [23]. The main difference comes from the ability of the optris PI160 thermal camera to output the raw temperature information for each pixel rather than providing a color coded image only. While the color settings have to be carefully tuned to allow even for manual detection of the light bulbs, the raw temperature information can easily be used to detect the calibration pattern automatically in the data. A thresholding procedure is applied to create a binary image showing regions of high temperature. A further thresholding step discards effectively all regions that are too big or too small. If the remaining number of regions is equal to the number of light bulbs in the pattern the regions are sorted according to the pattern to allow for easy determination of correspondences. To calculate the exact center of the features, the mean is calculated by weighing all the pixels in the region by their temperature values.After calculating the internal parameters of the cameras we need to align the camera images with the scanner coordinate system, i.e., extrinsic calibration. The three rotation and three translation parameters are known as the extrinsic camera parameters and define the geometric relation between camera and laser scanner. Once all the points are in the camera coordinate system the projection to the image can be defined up to a factor s using Eq. (3)[9]:(3)sxy1=fx0cx0fycy001r11r12r13t1r21r22r23t2r31r32r33t3XYZ1Suppose there are n images of the calibration pattern and m planar points on the pattern considering the distortions as independent and identically distributed noise, then the maximum likelihood estimate of the transformation between the scanner and camera coordinate system is obtained by minimizing the reprojection error(4)∑i=1n∑j=1m||pij-pˆ(A,D,Ri,ti,Pj)||2whereRiis the rotation matrix andtithe translation vector of the ith image.Ais the intrinsic matrix andDcontains the distortion parameters as calculated in the intrinsic camera calibration.pˆ(A,D,Ri,ti,Pj)defines the projection of pointPjin image i according to Eq. (3) and (2), andpijdescribes the pixel coordinates of the point in the image. This approach assumes that we have a number of points that are identifiable in both the laser scan and the image. For this purpose we attach the calibration pattern onto a board. For the optical camera this is a printed chessboard pattern and for the thermal camera light bulbs are arranged in a regular grid pattern. The calibration patterns are depicted in the background of Fig. 2. The positions of the points in these patterns are known. Algorithm 1 detects the points in a laser scan.Algorithm 1Calibration pattern detection in a laser scan.Require: point cloud, specification of calibration pattern1: discard points outside the area of the expected board2: find the most prominent plane using RANSAC (RANdom SAmple Consensus) [18]3: project a generated plane model into the center of the detected plane4: use ICP (Iterative Closest Point) algorithm [5] to fit the plane model to the data points5: if each point from the plane model has a corresponding point in the point cloud then6:return position of the light bulbs according to ICP result7: endifTo facilitate the detection of the calibration board in the point cloud data and to enable the easy positioning at different locations, the board is mounted on a tripod. This way the board hangs almost freely in the air. After removing the floor, the ceiling and most objects behind the board with a simple thresholding technique, the board becomes the most prominent plane in the data and can be detected using the RANSAC algorithm [18]. A plane model is generated by subsampling points on a plane with the dimensions of the calibration board. This model is transformed towards the center of the detected plane facing the same direction as the plane. To fit the model perfectly to the data the ICP algorithm [5] is used, thus giving the exact pose (position and orientation) of the calibration board. Since the positions of the light bulbs or chessboard corners on the board are known, their exact positions in 3D space can be calculated from the pose of the board.During the data acquisition phase laser scans and images are acquired simultaneously. After determining the relations between scanner and cameras in the calibration step this relation is used directly to assign temperature and color values to the point cloud.Due to the different fields of view, the sensors see different parts of the world. An area that is visible for one sensor might be occluded for the other sensor. When mapping the thermal information to the point cloud this causes wrong correspondences and therefore faulty assigned values. This impact is fortified by the low resolution of the thermal camera. With only 120 by 160 pixels per image each pixel corresponds to many 3D points seen by the laser scanner leading to errors at edges. Consequently small calibration inaccuracies have a large impact on the results. To solve this problem initially a fast procedure was implemented. All points that were projected onto one pixel and its neighboring pixels were clustered depending on their distance to the scanner. Assuming that most points fall onto the correct pixel a heuristic based on distance to the 3D scanner and size of the cluster determines which points are considered and enhanced with thermal information [7]. The method works sufficiently for the low resolution of the thermal camera but fails for higher resolution cameras. Therefore we now perform a ray tracing procedure that checks whether a point in the point cloud can be seen by the camera. We connect the pointPand the camera positionCwith a straight linePCand select all points with a distance less than a threshold t toPC, i.e., all pointsOifor which(5)|P-Oi|2-|(P-Oi)·(P-C)|2|P-C|2<t2holds true. If any pointOilies betweenPandC,Pis not visible from the camera and is therefore discarded. The threshold t accounts for small inaccuracies in the calibration and the low resolution of the camera simultaneously. To speed up the checking procedure the points are organized in a kD-tree data structure. With a quick check those voxels are immediately discarded that are not traversed by the ray and therefore all the points within are ignored.Laser scans acquired at different positions are registered into one common coordinate system using 6D SLAM from 3DTK – The 3D Toolkit[38]. It calculates a high presicision estimate of the scanner pose with 6 degrees of freedom. The scan registration algorithm implemented in 3DTK is described in detail in [37].The sensor placement planning module enables the full autonomous exploration in order to acquire a dense model of the environment. An algorithm based on the 2D horizontal plane in the point cloud at a specific height can be used efficiently for many tasks. However, this is not sufficient for creating a complete 3D model. Consider an empty room without obstacles. In a 2D scenario a robot with a horizontal field of view of 360° will finish after one scanning position because it perceives all the walls of the room from any position within the room. Constraints in the vertical field of view of the scanner, that cause missing data at the floor and the ceiling or parts of the walls outside of the considered height level, will simply be ignored. This is even more eminent in complex shaped rooms or when obstacles, like chairs, tables, wardrobes, etc., are present in the room and occlude other objects at varying height. Consequently, if the aim is to have a complete 3D model, a method that searches for unexplored areas in 3D is needed.Implementing and applying a complete 3D sensor placement planning algorithm in a large indoor environment needs significant memory space for storing the occupancy information of the whole environment. One would also need to store the exploration status of each part of the environment. All together this stretches the memory demands a lot. Additionally, the computational effort needed to process all the stored information will be high and increases the exploration time. The approach proposed in this paper combines 2D and 3D planning to enable tracking of the three dimensional information of the environment with lower computation and memory demands. The block scheme of the overall sensor placement planning algorithm is depicted in Fig. 3. The main idea relies on a typical indoor environment structure, composed of enclosed spaces like rooms, halls, corridors and so on. Starting with exploration based on only 2D measurements and following the next best view (NBV) positions obtained from 2D planning, the robot detects an enclosed space, i.e. a room, and models it. At that moment the procedure of searching for the NBV position switches from 2D to 3D NBV planning, which takes into account the whole 3D environment information captured by the 3D laser scanner. The NBV planning algorithm based on 3D information explores only the detected room as a small unit of the large environment, thus needing to store only all the 3D information of this small part. Therefore, a combination of 2D and 3D information based exploration keeps the computational and memory requirements low, because only a small part of 3D model is used while exploring. When the detected room is explored, the 3D NBV planning algorithm terminates and exploration continues again with the 2D NBV planning until a new unexplored enclosed space is detected and the algorithm switches back to 3D NBV planning. The algorithm terminates when the 2D NBV algorithm detects that the whole environment is explored, i.e. when there is not any so-called jump edge left in the memory. A jump edge is an edge that separates explored and unexplored regions of the environment. The next sections describes the three main modules of the algorithm: 2D NBV planning, room detection and 3D NBV planning.The 2D NBV planning algorithm is based on our previous approach presented in [14]. The algorithm does not require any information about the environment beforehand. Initiated with a blank map it starts to explore the environment based on the first scan.The inputs are range values uniformly distributed on the360°field of view. The ranges are extracted from the 3D point cloud so that all range data lies in the plane parallel to the floor plane. To ensure that the robot does not hit any obstacles, a slice covering the entire height of the robot is used to create the map, i.e., all values that are between approximately 30 and 70cm above the ground are used to create a 2D floor plan. The relatively large distance to the floor was chosen to account for small inaccuracies in the leveling of the robot.Assuming that the environment model is initially unknown it is incrementally built after each scan. The model is hierarchical with three abstraction levels. At the lowest level the grid map is used to store the static and dynamic obstacle information needed for path planning and obstacle avoidance. The next abstraction level contains the polygonal representation of the environment which stores environment edges, such as walls and other obstacles, which have been extracted from the range data and jump edges – edges that separate explored and unexplored regions of the environment. The most abstract level contains scanning position candidates which are considered for finding the NBV scanning position, i.e., the next goal position for the path planning module. We assume a setup where the robot localization problem is solved. The GMAPPING module under ROS [52] is used in our experiments. While exploring, the robot has to navigate between scanning positions in an unknown environment. We use a motion planning algorithm based on the D∗ algorithm and the Dynamic Window obstacle avoidance algorithm described in [47].The 2D NBV planning algorithm is composed of three consecutive steps, which are executed at each scanning position: (1) vectorization – extracting lines from range data, (2) creation of the exploration polygon EP – building the most recent model of the environment, and (3) selection of the NBV sensor position – choosing the next goal for the path planning module. These three steps are explained in the following.The main goal of the vectorization step is to obtain line segments from the input range data extracted from the 3D scan using the least squares method. First, the Progressive Probabilistic Hough Transform (PPHT) from the OpenCV library [26] is applied to the range data to calculate an initial estimation of the line segments, which are then used to group all range data around the calculated line segments according to their distanceΔρkfrom the lines. Second, more precise line parameters are calculated by a least squares line fitting algorithm.A polygonal representation of the environment is used for selecting the NBV position, for creating the gridmap, and for path planning. The calculated line segments from the vectorization step form the measurement polygon as follows. The ending points of adjacent detected lines are connected with jump edges and define the polygonPiat the ith scanning positionpi. The result is a polygonPithat is composed of real line segments and artificial edges, i.e., jump edges, between them. However, some jump edges from the new scan might fall into the already explored area from previous scans. To discard those jump edges we use the union of the new polygonPi(from the last scan) and the old exploration polygonEPi-1(from previous scans) as a representation of the currently explored area and we discard jump edges within the union of the polygons (from GPC library [55]. In each stepi,EPiis updated asEPi=EPi-1⋃Pi. The union of two polygons keeps only those edges from both polygons that are most distant from the point of view of the robot, and thus ensures that new jump edges are not created within previously explored regions. Jump edges that are longer than the preset valueΔrare considered for the selection of the next scanning position. The minimal length of the jump edgeΔris chosen in accordance with the robot dimensions and ensures that small jump edges are discarded, i.e., those the robot cannot pass through. If the non-empty extended polygonEPicontains no reachable jump edge, it is considered as the reliable polygonal description and the exploration process stops.We use a simple heuristic criterion for selecting the NBV position similar to [17]. By taking a scan directly in front of the jump edge it is easy to imagine that we will gain a larger amount of new information than by scanning further away from the jump edge inside of the explored area. Therefore, one candidate scanning position is assigned to each jump edge. It is an obstacle free position near the mid point of the jump edge at distance d from the jump edge. d is chosen to be equal to the dimensions of the robot to ensure safety in case the jump edge is close to an obstacle that has not yet been detected. Additionally, d must be larger than the minimal sensor range. The next sensor position is chosen by maximization of a criterion that estimates the amount of unexplored regions seen from each potential position.Fig. 4shows two candidate positionsp1andp2with jump edges denoted by red lines. The current scanning position is at R. The measure of the size of the unexplored region that is possibly seen from the k-th candidate position is calculated from the angles in the triangles that are defined by the k-th candidate position and all jump edges. To maximize the information gain all jump edges have to be considered that are visible from the candidate position. In Fig. 4 both jump edges are visible fromp1. Considering also the lengthdjof the shortest path from the path planning module between the current robot position R and the j-th candidate position the selection criterion is as follows:(6)Ij=k11dj+k2∑i=1Nαij.N is the number of candidate positions andαijis the angle in the triangle defined by the j-th candidate position and the i-th jump edge. Two parametersk1andk2are used as weighting parameters of angle and distance estimations, respectively. Numerous experiments in simulation and with the real robot showed good performance withk1set to the maximal range distance andk2set to1/Nwhich averages over all angles.The crucial step of the proposed sensor placement planning algorithm is the room detection algorithm, which is used to switch between the 2D and 3D NBV planning algorithms. After each scan taken at the position chosen by the 2D NBV planning algorithm, the room detection algorithm searches for the enclosed space (room) based on the current information captured from previous scans. If the room is detected, the 2D NBV algorithm is paused and the 3D NBV algorithm starts.In the 2D exploration phase a 2D cut of the environment is used that represents the area that is traversable by the robot. For room detection such a slice close to the ground is not suitable because objects occlude the room boundaries and windows and doors interrupt them. The main idea for room detection is grounded on the detection of a closed space in the 2D polygonal line map of the environment obtained by vectorization of range data at the most suitable height level above the floor. The most suitable height level for room detection is an obstacle free 2D plane at the height where the room boarders and walls are easily detected, i.e., a plane close to the ceiling. We choose it manually before the exploration starts. From the current 2D line map (taken at a suitable height level) of the workspace the room detection algorithm tries to detect a room. Let A be the lines corresponding to real environment edges obtained from the last scan. The algorithm searches for a room starting from the first line within the set A and tries to close the loop through other lines in the entire map. The process proceeds by finding the line closest to the current line in each step. When the loop is closed, i.e., when the nearest line is the starting line again, the room is detected. The nearest line search refers to the nearest ending point of a line in the vicinity of the current line. The vicinity area around the current line is defined with the radius parameter equal to the expected doorway width. With that, we ignore holes caused by doorways and windows inside the room. If there is no room detected starting from the first line in set A, the detection process starts from the second line in A. The order of the lines in A is not relevant. In case no closed polygon can be found with any line from set A as starting line, the room could not be detected in that step and exploration continues with the 2D NBV algorithm.The room detection algorithm provides the room parameters including the boundary area and the coordinates of the room. When the room is detected at least one scan inside the room is available since the robot has already entered the detected room and taken at least one scan inside which has been used for the room detection. From the available scans inside the room the initial 3D model of the room is built and the exploration continues by using the 3D model based NBV planning algorithm. The main idea of the 3D NBV planning algorithm is described hereafter.To obtain NBV positions the 3D model of the detected room needs to hold information of the explored and unexplored area inside the room. We use a voxel based 3D model where each voxel has one of the following labels: occupied if the volume within the voxel is occupied, unseen if the occupied status of the voxel is unknown or empty if the voxel is empty with no obstacles inside. The dimensions of the room define the number of voxels that should be used to cover the whole area of the room, i.e., memory allocation for the detected room. The shape of the enclosed space can be arbitrary. A cuboid 3D voxel model that encloses the entire detected room is chosen. The voxel model has to be large enough to enclose the largest room expected in the environment. Although there could be voxels in the cuboid model which are not part of the room, they are not considered during NBV position planning.Once the room is detected, its voxel based 3D map is initialized with all voxels set to unseen, since we do not have any information on the environment. The stored scans that were taken inside the detected room are used to update its initial voxel based 3D model. This is performed by using the ray tracing algorithm which traces a ray from the position where the scan was taken to each data point in the scan. Each voxel that is crossed by the ray is marked as empty and the voxel containing the data point is marked as occupied. The potential NBV position candidates are all voxels at the height of the laser scanner with the status empty. Position candidates that are not reachable for the robot are then removed from the list. The aim is to choose the candidate scanning position from where the most unseen voxels could be seen. For each candidate scanning position we count the number of unseen voxels. A ray is traced from the candidate scanning position to each unseen voxel and, if all crossed voxels are empty, the counter is incremented. Since considering every unseen voxel is unnecessarily time consuming only unseen voxels with at least one empty neighbor are taken into account [6]. In that way the number of voxels that need to be tested is decreased and voxels outside the room boundaries are not considered. The approach is similar to the jump edges in 2D in the way that unseen voxels that are taken into account actually corresponds to jump planes which divide explored and unexplored regions. Constraints in the field of view and range properties that are limits introduced by the sensors are considered by checking the range and the angle between the candidate scanning position and the unseen voxels. If the constraints are not satisfied, the unseen voxel is not counted. After finding a location that maximizes the number of unseen voxels, i.e., the NBV position, the robot drives to it, takes the 3D scan, and the whole procedure is repeated. The algorithm stops when the number of unseen voxels that can be seen from the best candidate position is below some predefined thresholdVminand the room is considered explored.Experiments were carried out in a research building (Fig. 5) at Jacobs University Bremen, Germany with the robot Irma3D (Fig. 2). The aim was to build a complete 3D model of the environment based on 3D scans with thermal information attached. Each scan took 3min and 15s. The laser scanner is constrained by a vertical field of view of 100° and the thermal camera with a vertical opening angle of 60°, respectively. Since the laser scanner field of view exceeds that of the thermal camera, the constraints of the algorithm are set to consider the field of view of the camera. The voxel volume used in the experiment was set to 0.008m3 (0.2m×0.2m×0.2m). For creating the 2D map a slice of the 3D scan was taken at a height between approximately 30cm and 70cm above the ground to make sure the robot hits no obstacles at any height. The room detection height level was set to 2.5m to avoid difficulties with windows and doors. Figs. 6–8present the results of an experiment illustrating the behavior of the proposed sensor placement planning algorithm.Fig. 6 shows the floor plan of the explored environment which consists of three rooms and a corridor. The scanning positions chosen by the proposed sensor placement scanning algorithm are ordered from the start position (position 1) to the end position (position 13) and marked with the robot footprints. Green footprints refer to scanning positions generated by the 2D algorithms and red footprints by the 3D NBV algorithm, respectively. Some photos illustrating the environment are given in Fig. 5. The photos are excerpts from the data that the robot Irma3D acquired during the experiment. The robot started from position 1 in room 1 and took the initial scan. The first scan was sufficient for the room detection algorithm to detect room 1. When the room was detected, the jump edges inside room 1 were discarded, leaving only the jump edges detected outside of room 1 for continuing with the 2D NBV planning algorithm after exploring room 1. The initial 3D voxel model of the room was built and the 3D NBV planning algorithm was switched on. Then the 3D NBV planning algorithm chose position 2, took the scan there and updated the 3D model of room 1. Since the number of unseen voxels was larger than the threshold (Vmin= 15 voxels) the 3D NBV algorithm continued with the exploration of room 1 and chose scanning position 3. After the third scan, the 3D model of room 1 was of the desired accuracy and the exploration was continued with the 2D NBV algorithm, which moved the robot to position 4 in the corridor, i.e. in front of one of the two available jump edges (see Fig. 7a). The 2D NBV algorithm chose two additional scanning positions in the corridor (positions 5 and 6). The reason for the jump edge at position 5 is a range constraint of 8m on the laser data in 2D causing part of the corridor not to be seen in the 2D laser data from position 4. Further away from the robot the points are very sparse making line extraction difficult. After the corridor, the robot moved to position 7 at the entrance of room 3, i.e. to the edge with the lowest value of the criterion (Fig. 7b) without exploring the corridor with the 3D NBV algorithm. The corridor was not considered as a room since it was closed on both sides with glass doors as can be seen in Fig. 5(c). At the room detection height level of 2.5m the glass doors were not visible in the laser scan.Due to the lack of clutter in the corridor three scanning positions were sufficient to satisfy the criteria of the 2D NBV algorithm. After scan 8 room 2 was detected and fully explored with only one additional position from the 3D NBV algorithm. Afterwards, 2D NBV planning chose position 10 based on the polygon shown in Fig. 7c and position 11 and 12 before room 3 was detected. Finally, position 13 was chosen by the 3D NBV algorithm to achieve the required accuracy of the room model. The exploration of the environment finished here as there were no jump edges in the memory (Fig. 7d), which means that the whole environment was explored and modeled.Fig. 8 represents the 3D voxel models of room 1 after the first (8a) and the third (8b) scan. The occupied voxels are colored red, potential position voxels (PP) are colored blue, while unseen voxels are colored green. In the figure we also have white voxels, which are unseen voxels that can be seen from at least one PP position in the workspace. They are a subset of the unseen voxels and are treated as such in the exploration algorithm. They only serve to show that some of the unseen voxels could never be seen from any position. The specific cone in the figure is a consequence of the sensor field of view constraints. The other unseen (green) voxels are situated under the tables, chairs and other obstacles inside the room representing unexplored area. As can be seen in the model and in Table 1, the number of unseen voxels that can be seen from the best position is zero after the last scan inside room 1.To evaluate the benefits of the 3D exploration strategy, we conducted two new experiments in room 1, one using only 2D exploration and one with both 2D and 3D exploration. We placed the robot at the same starting position in both experiments. The results are seen in Figs. 9 and 10. Fig. 9 shows the final exploration polygon with the positions where the scans were taken. Rectangular marks show positions chosen only by 2D NBV and circles refer to 3D NBV based positions. In the 2D mode the robot finished after two scanning positions. As can be seen in the panorama image, large parts of the ceiling, the floor and the walls are not captured with the thermal camera. The unexplored area is drastically reduced when 3D exploration is employed as well. From Table 2it becomes clear that 703 unseen (UB) voxels could still be seen from the NBV position after the 2D exploration.The complete model of the environment can be inspected in the viewer from 3DTK enhanced with either reflectance values or thermal data (see Fig. 1). The color scale is adjustable for a good view of the temperature distribution in the current data. In all images depicted here, blue corresponds to cold temperatures while dark red corresponds to warm temperature values. Switching between the different views enables the user to detect sources of wasted energy and to locate them clearly in the 3D view. The Riegl VZ-400 laser scanner has an accuracy of 5mm [32]. In previous work it was shown that the scan matching algorithm used here leads to a positional error of less than 4cm when joining several 3D laser scans even in large outdoor environments [37]. However, humans are used to virtual models consisting of filled structures without holes, i.e., surfaces or meshes. The process of generating surfaces from a set of points is called surface reconstruction. We present a method to reconstruct a 3D model with added thermal field information and to retrieve information from the finished model. The automatic detection of heat sources in the model to point out possible leaks is of special interest.The Marching Cubes Algorithm (MCA) calculates meshes based on iso-surfaces from point clouds that are organized in voxels. It calculates triangles that separate each voxel into the parts that lie within and outside of the object that is to be reconstructed. To generate the iso-surfaces more quickly, a probabilistic approximation method is used [46]. Every point in the input point cloud is assigned a Gaussian distribution. The final iso-surface is calculated by applying an aggregation operator, i.e., minimum, maximum, or sum. Changing the width of the Gaussian bell reduces the influence on the neighboring points/voxels. This method improves the results if normals in every point are provided.The Gaussian estimation of iso-values and the subsequent MCA reconstruction do not deal with scalar temperature values acquired by the thermal camera. These values have to be mapped onto the reconstructed surface using an appropriate color scale.The surface is reconstructed from the point cloud. The temperature values are assigned to the points. Mapping the temperature color distribution onto the reconstructed surface is difficult because not every point on the surface corresponds directly to one point in the original point cloud and therefore cannot be assigned a temperature/color value directly. To overcome this problem, the points from the model, i.e., the vertices of the triangle mesh are organized in a spatial data structure that enables fast range and nearest neighbor (NN) searches. For each point in the point cloud the closest point in the mesh is found and assigned the scalar temperature value. R-trees have proven to be an efficient data structure for solving the scalar field mapping problem [39], but the original kD-tree based solution produces the model with better quality and scalar field mapping precision. An R-tree organizes spatial objects by defining bounding boxes that contain all objects indexed by the child nodes. In the case of scalar field mapping the bounding box of leaf nodes contains preferably one or more points that are vertices of a polygonal face. The size of these bounding boxes influences the precision of the NN search result and thus the overall quality of the scalar field mapping as shown in [39].One of the major drawbacks of the presented reconstruction and scalar field mapping is the precision. The precision of the reconstructed model is mainly affected by the density of the point cloud [41]. In our case, the average density of the point cloud is greatly affected by outliers that increase the size of the bounding box of the model. These outliers are often noise caused by reflections from windows or simply objects that are seen through the window and are not of interest for the model.This corresponds to a typical point cloud segmentation problem, where the point cloud is to be segmented into several smaller parts. The task given here differs from other segmentation problems as we can assume that valid points and invalid points will be somewhat disconnected. Thus, a connectivity criterium based on K-means clustering effectively eliminates points that are problematic for the reconstruction of the 3D model without the need for time and resource consuming segmentation algorithms. A choice ofK=4has shown to give good results in the experiments. In general, it is not easy to conclude the perfect number of clusters from the data. Standard techniques for choosing the initial number of clusters are much slower and tend to choose a number much higher than necessary, thus removing many of the valid points [41].Heat sources are identified by analyzing the scalar field values contained in the 3D model of an indoor environment based on a high local increase of temperature. In buildings, sources of heat are radiators with pipes, air conditioners, incandescent lights and other electrical equipment emitting high amounts of infra-red radiation. We observe from Fig. 11that computer monitors and light bulbs are major sources of heat. The detection can be considered as a typical image processing task, but in our case a full 3D model (or at least 3D point cloud) makes the task more complicated. In [42] we analyze scalar value thresholding, K-means clustering and Fuzzy c-means. The scalar value thresholding method from [49] was modified so that all vertices in the mesh, with a temperature value succeeding a threshold, are marked as potential heat sources. K-means and Fuzzy c-means cluster the points from the model based on their temperature values. In [42] we show that K-means shows the most promising results, with fuzzy c-means having somewhat worse performance, but with almost the same quality.Fig. 12shows the reconstruction of one part of the scanned indoor environment, a scan taken in room 1. The model contains close to 11,926,000 points for a volume of less than 200m3. Compared to the data sets presented in [4,27,33], this data set is rather small and the expected precision is lower. However, it is clear that geometry is preserved, although the precision when it comes to reconstructing furniture, computer monitors, and people is not sufficient. To increase precision, a larger space subdivision is needed. This comes at the expense of higher computing power and memory requirements to store the model [40]. Time performance dependence on spatial subdivision for the part of the dataset is given in Table 3.To evaluate the obtained model visual perception of an expert is necessary [40]. The methodology presented in Section 6.3 attempts to minimize the local error. We managed to increase the point cloud density by the factor of≈50. In the end this decreases the reconstruction error to a few centimeters compared to the original 10–20cm [41]. In Fig. 13the benefit of removing problematic points is clearly shown. The reconstruction algorithm takes a point cloud with much higher density as an input and produces a model with higher precision.Finally, we present the results of the detection of heat sources in the discussed environment. There are several sources of heat in this model and they have a color ranging from orange to red. These points have the surface temperature from roughly 35°C to 60°C. In Fig. 14we present the input model and the output of the detection algorithm which labels the potential heat sources.

@&#CONCLUSIONS@&#
This article presents a complete system for autonomous 3D exploration and thermal mapping of an indoor environment. The data is autonomously collected, thermal and color information is automatically mapped onto the 3D data and the environment is reconstructed into a thermal model. To perceive the entire environment we propose a model based 3D sensor placement approach that uses a voxel representation and room extraction to drastically decrease the computational requirements while still reaching high accuracy. A viewer is presented to analyze the reconstructed model afterwards and to inspect the automatically detected points of interest in the data.The work presented here is fundamental research and does not claim to offer a ready to use product. The system is a first step towards a tool that targets at improving the effectiveness of indoor thermography. The main goal, to design a system that autonomously collects the data necessary to create a model and perform the thermal analysis of a building with this model, is achieved. The system is designed to point out possible problems in the data to an experts who interprets them with his experience and expertise. Hotspots as they appear in heat sources and power lines were chosen as an example here. The general idea works the same for cold spots. The system is more adequate for indoor office buildings or for very cluttered environments, in which the room extraction algorithm will find a closed loop in the part of the environment that is not a real room.However, setting a predefined maximal cuboid size supports the exploration of larger rooms. If the currently collected data reaches the predefined maximal cuboid size, the algorithm will switch to the 3D phase using a cuboid part of the environment that does not exceed the allocated memory. In our future work, we plan to extend the algorithm for such cases by considering possibilities of finding virtual rooms inside a large room.The majority of work that still has to be done lies in the post processing, the model building and the detection of thermal flaws. As the interpretation of thermal data underlies the strong influence of the material properties and the external conditions such as room temperature and humidity, it is very hard to make decisions automatically. A system, such as this, aims at pointing out points of interest to the expert. In this paper, a method is presented that aims at pointing out hot spots, as they appear in heat sources and power lines. The evaluation of the effectiveness of this approach for real thermal issues is still subject to further research. For the detection of cold spots, such as thermal bridges, it is generally possible to use the same method with a lower boundary. However, it is to be expected that the temperature differences are much lower and therefore harder to detect. Machine learning approaches could help in detecting and classifying these points of interest. Some preliminary work on automated Building Information Modeling (BIM) from laser scans by detecting doors and windows have been presented [57,53,16]. We have extended this work by labeling windows in 3D thermal data [15]. Future work will build upon this work and further investigate the use of machine learning for interpreting thermal 3D models.We plan to further improve the precision of the 3D model by analyzing other factors that influence low point cloud density. A more effective method for removing invalid points using associations based on connectivity might lead to better results. In this work we focused on creating the 3D thermal model. The point cloud viewer offers the possibility to switch between thermal and color mode. A good method has to be found to make the reconstructed model available with both modalities as well. Possibilities are to switch between the two models, to create a model with one modality and add the option to show the corresponding image of the other modality at a certain spot, or to overlay the color model with the thermal information, thus modifying the color space of regions with temperature peaks and keeping the true color for the other areas.