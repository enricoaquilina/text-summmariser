@&#MAIN-TITLE@&#
A computational study for bilevel quadratic programs using semidefinite relaxations

@&#HIGHLIGHTS@&#
We consider bilevel quadratic problems with binary variables in the leader and convex quadratic follower problems.We derive equivalent Mixed Integer Linear Programming formulations. Thus, we compute optimal solutions and upper bounds.We transform the bilevel problems into binary quadratic programs and derive semidefinite relaxations.The particular case where the follower problem is formulated as a linear program is also considered.The SDP bounds are significantly tight. Finally, they are obtained at low computational cost.

@&#KEYPHRASES@&#
(I) Conic programming and interior point methods,Bilevel programming,Semidefinite programming,Mixed integer linear programming,

@&#ABSTRACT@&#
In this paper, we deal with bilevel quadratic programming problems with binary decision variables in the leader problem and convex quadratic programs in the follower problem. For this purpose, we transform the bilevel problems into equivalent quadratic single level formulations by replacing the follower problem with the equivalent Karush Kuhn Tucker (KKT) conditions. Then, we use the single level formulations to obtain mixed integer linear programming (MILP) models and semidefinite programming (SDP) relaxations. Thus, we compute optimal solutions and upper bounds using linear programming (LP) and SDP relaxations. Our numerical results indicate that the SDP relaxations are considerably tighter than the LP ones. Consequently, the SDP relaxations allow finding tight feasible solutions for the problem. Especially, when the number of variables in the leader problem is larger than in the follower problem. Moreover, they are solved at a significantly lower computational cost for large scale instances.

@&#INTRODUCTION@&#
The bilevel programming problem (BPP hereafter) is a two level hierarchical optimization problem. The first (or upper) level problem is referred to as the leader problem while the second (or lower) level is referred to as the follower problem. Formally, a BPP can be written as followsmax{x∈X,y}F(x,y)s.t.G(x,y)≤0y∈argmax{y∈Y}f(x,y)s.t.g(x,y)≤0wherex∈X⊂Rn1,y∈Y⊂Rn2,F:Rn1×Rn2→Randf:Rn1×Rn2→Rare the decision variables and the objective functions of the leader and the follower problems, respectively. Analogously, the functionsG:Rn1×Rn2→Rm1andg:Rn1×Rn2→Rm2denote upper and lower level constraints respectively. In a BPP, the main goal is to find an optimal point such that the leader and the follower maximize (or minimize) their respective objective functions F(x, y) and f(x, y) subject to the linking constraints G(x, y) and g(x, y). Applications concerning BPP include agriculture, economic systems, finance, engineering, banking, transportation, network design, management and planning among many others. For different domains of applications see for instance Dempe (2003); Floudas and Pardalos (2001); Gang, Roger, and Mo (2014); Migdalas, Pardalos, and Varbrand (1997).Bilevel linear programs have been more considered in the literature during the last decades than quadratic bilevel programs. Several approaches were proposed to solve these problems amongst all genetic based algorithms (Calvete, Galé, & Mateo, 2008) and penalty methods (Aboussoror & Mansouri, 2005; Ankhili & Mansouri, 2009). Bilevel quadratic programming problems, we denote by QQBPs, are a special case of BPPs where the leader and follower problems are quadratic optimization problems. Our approach in this paper mainly consists in expressing a QQBP with binary decision variables in the leader problem and convex quadratic (or linear) programs in the follower problem as an equivalent quadratic single level optimization problem with complementarity constraints. This is achieved by replacing the follower problem with the equivalent Karush Kuhn Tucker (KKT) conditions. Subsequently, we use the single level problems to obtain equivalent mixed integer linear programming (MILP) formulations, and also to derive semidefinite programming (SDP) relaxations. This is achieved by handling the quadratic complementarity slackness conditions of the follower problems with the linearization approach proposed by Audet, Hansen, Jaumard, and Savard (1997) and the quadratic terms with the Fortet linearization method (Fortet, 1960). This allows computing optimal (or feasible) solutions and upper bounds using linear programming (LP) and SDP relaxations. As far as we know, computational studies for this particular class of QQBPs using semidefinite programming approaches have not been investigated so far in the literature. In this paper, we mainly focus on the quality of these bounds rather than finding the optimal solution of the problem. For this purpose, we derive LP and SDP relaxations directly from the equivalent quadratic single level formulations.Semidefinite programming consists in optimizing a linear objective function subject to an affine space intersected with the conic space spanned by positive semidefinite matrices. SDP appears as a generalization of linear programming while replacing the vector of variables with a symmetric matrix and the nonnegativity constraints with a positive semidefinite constraint. We use SDP due to its proven efficiency when finding tight bounds for combinatorial optimization problems (Grotschel, Lovasz, & Schrijver, 1988; Helmberg, 2000; Lovasz & Schrijver, 1991). It is particularly effective when applied to binary quadratic constrained problems (BQCPs) (Adasme & Lisser, 2014; Adasme, Lisser, & Soto, 2011; Goemans & Williamson, 1995; Helmberg, Rendl, & Weismantel, 2000). Besides, SDP relaxations can be solved to optimality using interior point algorithms (Borchers, 1999; Helmberg, 2000). There has been a branch of recent work on general Quadratic Programming (QP) problems. We refer the reader to the works Amaral, Bomze, and Júdice (2014); Burer (2009); Engau, Anjos, and Vannelli (2012); Ghaddar, Anjos, and Liers (2011); Júdice (2012); Luo, Ma, So, Ye, and Zhang (2010); Mitchell, Pang, and Yu (2012); Saxena, Bonami, and Lee (2010, 2011); Scholtes (2004) for a general perspective. For instance, Burer (2009) models any nonconvex quadratic program having a mix of binary and continuous variables as an LP over the dual of the cone of copositive matrices while Engau et al. (2012) investigate cutting planes and interior-point methods for solving semidefinite relaxations of binary quadratic optimization problems. Cutting plane methods as well as SDP relaxations of mathematical programs with complementarity constraints are also investigated by Mitchell et al. (2012). Similarly, Saxena et al. (2010, 2011) address the problem of generating strong convex relaxations of mixed integer quadratically constrained programming (MIQCP) problems by using techniques from disjunctive programming, and projection techniques of the lift and project methodology to build low dimensional relaxations that capture the strength of extended formulations. Finally, Luo et al. (2010) survey several SDP relaxations of quadratic optimization problems. For a more general and deeper comprehension on SDP relaxations, we refer the reader to Anjos, Lasserre, Anjos, and Lasserre (2012). Less recent works but more specialized on QQBPs can be found in Calvete and Galé (1998); 2004); Júdice and Faustino (1994); Thirwani and Arora (1998); Vicente, Savard, and Judice (1994); Wang, Wang, and Rodriguez (1994). Algorithmic approaches are also presented in Thirwani and Arora (1998); Vicente et al. (1994) and (Wang et al., 1994). In Vicente et al. (1994), the authors propose two descent methods for QQBPs while in Thirwani and Arora (1998) they solve the problem using linearization techniques, Gomory cuts and the dual simplex method. These cuts as well as disjunctive cuts were initially developed for linear bilevel programs in Audet, Haddad, and Savard (2007a); Audet, Savard, and Zghal (2007b).Concerning the complexity, BPPs are strongly NP-hard even for the simplest case in which all the involved functions are affine. See for instance Ben-Ayed and Blair (1990); Blair (1992); Deng, Wang, and Wang (1995); Hansen, Jaumard, and Savard (1992); Scholtes (2004) or (Migdalas et al., 1997) for more details on complexity issues. For a more general understanding on bilevel programming, the reader is referred to the books by Dempe (2002); Migdalas et al. (1997).This paper is organized as follows. In Section 2, we present the QQBP problem and the equivalent quadratic single level formulation. In Section 3, we derive the mixed integer linear programming model while in Section 4 we derive the SDP relaxation. In Section 5, we present two algorithmic procedures to obtain feasible solutions for QQBP. The first one computes a nearest feasible solution using the optimal solutions of the LP and SDP relaxations, while the second one computes feasible solutions from scratch with a simple reduced variable neighborhood search (VNS for short) metaheuristic procedure (Hansen & Mladenovic, 2001; Hansen, Mladenovic, & Perez-Brito, 2001; Mladenovic & Hansen, 1997). Subsequently, in Section 6 we present numerical results for the LP and SDP relaxations, and for the two algorithmic procedures when compared to the optimal solution of the problem (or best solution found with CPLEX). These numerical results are simulated for density levels in the input data ranging from 50 to 100%. Finally, in Section 7 we give the main conclusions of the paper.In this section, we present the QQBP under study and derive an equivalent quadratic single level formulation for this problem.We consider the following (0–1) bilevel quadratic programming problem(1)QQBP:max{x,y}xTQx+rTx+sTy(2)s.t.Ax+By≤b(3)x∈{0,1}n1(4)y∈argmax{y}{12yTFy+gTy+hTx}(5)s.t.Cx+Dy≤f(6)y∈[0,1]n2wherex∈{0,1}n1andy∈[0,1]n2are the decision variables. The number of constraints in (2) and (5) are denoted by m1 and m2, respectively. The matrices and vectors are defined as follows:Q∈Mn1×n1(R),A∈Mm1×n1(R),B∈Mm1×n2(R),F∈Mn2×n2(R),C∈Mm2×n1(R),D∈Mm2×n2(R),r∈Rn1,s∈Rn2,b∈Rm1,g∈Rn2,h∈Rn1andf∈Rm2,respectively. In QQBP, (1)–(3) correspond to the leader problem whereas constraints (4)–(6) represent the follower problem. Notice that the leader problem is formulated as a binary quadratic problem while the follower is a convex quadratic problem subject to linear constraints. This implies that the follower input matrix F must be symmetric and negative semidefinite.We transform the aforementioned QQBP into an equivalent quadratic single level optimization problem. We perform this by replacing the follower problem with its equivalent KKT conditions. Thus, we write the Lagrangian function of the follower problem as(7)L(y,λ,μ,θ)=12yTFy+gTy+hTx+λT(f−Cx−Dy)+μT(en2−y)+θTywhere λ, μ and θ are Lagrangian multipliers for the dualized constraints in the follower problem. The vectoren2denotes the vector of all ones of dimension n2. Notice that the third term hTx in (7) is constant since the follower optimizes on variable y. Then, it can be discarded. Stationary conditions can be stated as follows∂L∂y=Fy+g−DTλ+θ−μ=0The complementarity conditions imply(8)λT(f−Cx−Dy)+μT(en2−y)+θTy=0λ≥0,μ≥0,andθ≥0This yields the following equivalent quadratic single level problemSQ1:max{x,y,λ,θ,μ}xTQx+rTx+sTys.t.Ax+By≤bx∈{0,1}n1Cx+Dy≤fy∈[0,1]n2Fy+g−DTλ+θ−μ=0λT(f−Cx−Dy)+μT(en2−y)+θTy=0λ≥0,μ≥0,θ≥0Notice that the complementarity constraint (8) is written by means of an unique aggregated constraint. This is possible since each term in (8) is non-negative. We note that SQ1 is formulated as a MIQCP which is generally non-convex problem and thus hard to solve. In the next section, we derive an equivalent MILP formulation for SQ1. Next, in Section 4, we formulate an SDP relaxation for SQ1 as well. For the LP relaxations, notice that the constraint qualification condition is always satisfied. While for the SDP relaxation, typically a Slater constraint qualification is invoked. As in our SDP relaxations, it is straightforward to find a feasible non-singular positive semidefinite matrix, then the Slater constraint qualification always holds. In addition, in all our models we consider full rank matrices in order to warranty that the interior of the feasibility sets are non-empty. Finally, we mention that in our numerical results, all the instances of the SDP relaxations are solved with zero duality gap.In order to derive a MILP model for SQ1, we apply the Fortet linearization method to deal with the binary quadratic terms in the objective function of the leader problem (Fortet, 1960). On the other hand, we use a linearization splitting scheme (Audet et al., 1997) to handle the complementarity constraints of the follower problem. Deriving a MILP model allows computing optimal solutions for SQ1 as well as LP upper bounds. There are several linearization methods which allow to obtain a linear formulation from a binary quadratic optimization problem (Adams, Forrester, & Glover, 2004; Glover, 1975). However, the simplest and probably the most natural way to do it is the classic Fortet linearization method (Fortet, 1960). In order to apply Fortet method to SQ1, we define the binary variableX=xxT. This allows to replace each binary product xixjby an additional binary variable Xij. In addition, the linear constraintsxi+xj−1≤Xij≤xiand Xij≤ xj∀i, j are added to the problem. On the other hand, to deal with the complementarity conditions in (8), we apply the splitting scheme proposed by Audet et al. (1997). This method consists in replacing each nonnegative product term in (8) by two linear constraints using a binary variable. We denote the Frobenius inner product between two n × n symmetric matrices``Z""and``W""by〈Z,W〉=∑i=1n∑j=1nZijWij. Thus, an equivalent MILP formulation for SQ1 can be written as(9)MILP1:max{x,y,λ,θ,μ,X,ν1,ν2,ν3}〈Q,X〉+rTx+sTy(10)s.t.Ax+By≤b(11)Xij≤xi,1≤i,j≤n1(12)Xij≤xj,1≤i,j≤n1(13)Xij≥xi+xj−1,1≤i,j≤n1(14)x∈{0,1}n1,Xij∈{0,1}n1×n1(15)Cx+Dy≤f(16)y∈[0,1]n2(17)λ≥0,μ≥0,θ≥0(18)Fy+g−DTλ+θ−μ=0(19)f−Cx−Dy+ν1L≤L(20)λ≤ν1L,ν1∈{0,1}m2(21)(en2−y)+ν2L≤L(22)μ≤ν2L,ν2∈{0,1}n2(23)θ+ν3L≤L(24)y≤ν3L,ν3∈{0,1}n2where L is a large positive number andλ∈Rm2,μ∈Rn2andθ∈Rn2. In MILP1, constraints (11)–(13) are the Fortet linearization constraints (Fortet, 1960) while constraints (19)–(24) are due to the splitting scheme we use to handle the complementarity constraints (Audet et al., 1997). For example, (19)–(20) handle the following constraint(f−Cx−Dy)Tλ=0by means of the binary variablesνi1∈{0,1},i=1,…,m2. Whenνi1=1,we havefi−Ci,•x−Di,•y≤0and0≤λi≤Lwhich means λiis relaxed and the expression(fi−Ci,•x−Di,•y)must be equal to zero. The latter is implied by the primal constraintCx+Dy≤fof the follower problem. On the opposite, ifνi1=0for a particulari=1,…,m2,this impliesfi−Ci,•x−Di,•y≤Landλi=0In the next section, we derive an SDP relaxation from SQ1 to come up with upper bounds for SQ1. We denote hereafter by LP1 the LP relaxation of MILP1. Notice that the number of variables in LP1 is of the order ofO(n12+n2+m2)while the number of constraints is of the order ofO(n12+m1+m2+n2).In this section we derive an SDP relaxation for SQ1. For this purpose, letSndenote the set of symmetric n × n matrices, and letSn+={Z∈Sn|Z⪰0}denote the set of positive semidefinite n × n symmetric matrices (Helmberg et al., 2000). Next, consider the following rank-1 positive semidefinite matrix(25)Z=(x;y;λ;μ;θ)(x;y;λ;μ;θ)Tunless the column vector(x;y;λ;μ;θ)=0. Notice that this vector is composed of the decision variables in SQ1. Also notice that Eq. (25) is equivalent to the following two constraints(26)Z−(x;y;λ;μ;θ)(x;y;λ;μ;θ)T⪰0(27)Z−(x;y;λ;μ;θ)(x;y;λ;μ;θ)T⪯0In particular, the Schur complement theorem implies that the convex constraint (26) can be equivalently written as[Z(x;y;λ;μ;θ)(x;y;λ;μ;θ)T1]⪰0Consequently, dropping the non-convex constraint (27) as well as the rank-1 condition on matrix Z, we can derive an SDP relaxation for SQ1 as follows(28)SDP1:max{x,y,λ,θ,μ,Z}〈Q¯,Z〉+rTx+sTys.t.Ax+By≤bZii=xi,i=1,…,n1(29)Cx+Dy≤fFy+g−DTλ+θ−μ=0(30)λTf−〈C¯,Z〉−〈M¯,Z〉−〈N¯,Z〉+〈W¯,Z〉+μTen2=0(31)y∈[0,1]n2(32)[Z(x;y;λ;μ;θ)(x;y;λ;μ;θ)T1]⪰0λ≥0,μ≥0,θ≥0where{Q¯,C¯,M¯,N¯,W¯}are input(n1+3n2+m2)(n1+3n2+m2)symmetric matrices. All these matrices are sparse matrices defined as follows.Q¯ij=Qij,∀i,j∈{1,…,n1},C¯i,n1+n2+j=C¯n1+n2+j,i=Cj,i2,∀i∈{1,…,n1},j∈{1,…,m2},M¯n1+2n2+m2+i,n1+i=M¯n1+i,n1+2n2+m2+i=12,∀i∈{1,…,n2},N¯n1+n2+m2+i,n1+i=N¯n1+i,n1+n2+m2+i=12,∀i∈{1,…,n2},W¯n1+n2+i,n1+j=W¯n1+j,n1+n2+i=Dij2,∀i∈{1,…,m2},j∈{1,…,n2}. In SDP1, constraint (28) is a relaxation constraint for the conditionxi2=xi,∀i∈{1,…,n1}. Notice that the number of variables in SDP1 is of the order ofO(n12+n22+m22+n1n2+n1m2+n2m2)while the number of constraints is of the order ofO(n1+n2+m1+m2). Finally, notice that constraint (30) is equivalent to constraint (8). We observe that this constraint can be disaggregated into three equivalent constraints, one for each term in (8) being equal to zero. We mention that we performed this disaggregation process in our SDP relaxations. However, our numerical results show that the SDP bounds remain unchanged and that the CPU times are nearly the same. Therefore, we omit presenting these numerical results in the paper.In this paper, we compute feasible solutions for QQBP in two ways. First, we use a reduced VNS procedure and then, we solve an auxiliary optimization problem to obtain a nearest feasible solution. For the latter method, we use the optimal fractional solutions obtained with the LP and SDP relaxations.VNS is a metaheuristic approach that uses the idea of neighborhood change during the descent toward a local optima. We define only one neighborhood structure as Ngh(x) for MILP1 as the set of neighbor solutions x′ in MILP1 at a Hamming distance “H” from x. We proposea reduced VNS procedure (Hansen & Mladenovic, 2001; Hansen et al., 2001) in order to compute feasible solutions for MILP1. VNS approach mainly consists in solving iteratively the problemMILP1(x¯)=max{y,λ,θ,μ,ν1,ν2,ν3}{sTy:(10),(15)−(24)}for different fixed binary vectorsx¯. There are2n1feasible assignments for vectorx¯in MILP1. The VNS procedure we propose is depicted in Algorithm 5.1. It receives an instance of problem MILP1 as input and returns a feasible or an unfeasible solution. Our VNS proceeds as follows. First, it initializes the required parameters and generates randomly an initial binary vectorx¯and save it in xb. Next, it checks whether MILP1(x¯)is feasible. If so, it computes the objective function value of MILP1 denoted by “Opt”. This is performed while usingx¯and the optimal solution of MILP1(x¯). If a new current solution is better than the best found so far, thenH←1,the new solution is saved and the process continues. Otherwise, we keep the previous solution xb. Independently of the feasibility of MILP1(x¯),the algorithm performs a variable neighborhood search process by randomly switching {0, 1} values inHentries of vectorx¯. Initially,H←1and it is increased by one unit when there is no improvement after new “η” solutions have been evaluated. Notice that the value ofHis increased untilH=n1,otherwiseH←1. This gives the possibility of exploring in a loop manner from local to wider zones of the feasible space. The whole process is repeated while the CPU time variable “Time” is less or equal than “maxTime”.Finally, we mention that VNS Algorithm 5.1 is only intended to find feasible solutions for QQBP and not optimal or near optimal solutions.We also compute feasible solutions for QQBP by solving an auxiliary optimization problem. For this purpose, we use the optimal fractional solution xfobtained with LP1 (or SDP1) as an input and solve the following optimization problem(33)min{x,x+,x−,X,y,λ,μ,θ,ν1,ν2,ν3}∑i=1n1(xi++xi−)(34)s.t.(10)−(24)xi+−xi−+xif=xi,i=1,…,n1(35)xi+≥0,xi−≥0,i=1,…,n1where constraints (10)–(24) correspond to the feasible set of MILP1. Constraints (34) are level constraints leading to the binary condition for each variable xi,∀i∈{1,…,n1}in MILP1. This is accomplished by using the nonnegative level variables defined in (35). These level variables are minimized in the objective function (33). Finally, we use the output solution of the auxiliary optimization problem to compute a nearest feasible solution according to the objective function (9) in MILP1.In this section, we perform computational experiments in order to compute lower and upper bounds as well as optimal (or feasible) solutions for QQBP. Additionally, we consider the case where QQBP has an LP as follower problem, i.e.,F=0in (4). We denote it hereafter by QLBP. The numerical results for QLBP are similar to the ones presented for QQBP, thus we omit presenting them in the paper explicitly. However, all the data we use to draw the curves in Figs. 1–4 and all the numerical results for QLBP as well, with additional information such as gaps, branch and bound nodes, minimum, maximum and average values can be found in tabular form in Adasme and Lisser (2015). In this appendix, we further report more instances with different dimensions varying m1, m2, n1 and n2.We implement a Matlab (R2012a) program using CPLEX 12.6 to solve the MILP and LP models, CSDP 6.1.0 (Borchers, 1999) to solve the SDP relaxations, and for VNS Algorithm 5.1 and the nearest feasible procedure. The numerical experiments have been carried out on an Intel(R) 64 bits core(TM) with 3.4 gigahertz and 8 gigabyte of RAM. Both CPLEX and CSDP solvers are used with default options. We generate the input data for QQBP and QLBP as follows. Each entry in the matrices {A, B, C, D, Q0, F0} and in the vectors {r, s, g, h} is randomly and uniformly distributed in the interval[−5,5]. To ensure that the matrix Q is symmetric we setQ=Q0+Q0T2. While the matrix F is set toF=−110F0F0T⪯0. The latter ensures that it is symmetric and negative semidefinite. The right hand side vectors b and f are generated as followsbi=12(∑j=1n1Aij+∑j=1n2Bij),∀i=1,…,m1andfi=12(∑j=1n1Cij+∑j=1n2Dij),∀i=1,…,m2All the input matrices and vectors are generated with density levels of 50% and 100%, respectively. This allows to observe how sparsity affects our numerical results. We put zeros randomly in 50% of the entries in Q while preserving its symmetry. While for the matrix F, we put zeros randomly in 50% of the entries in F0 in order to preserve the negative semidefiniteness property of F. We set the maximum available time in VNS Algorithm 5.1 tomaxTime=100seconds and the value ofη=5. This is reasonable CPU time as we noticed in our numerical experiments that CPLEX can solve MILP1(x¯)and MILP2(x¯)in less than 2 seconds for small and medium size instances of the problem, i.e., for n1, n2 ≤ 100 and m1, m2 ≤ 20. We recall that Algorithm 5.1 is only intended to find feasible solutions. On the other hand, we set the CPU time to solve the auxiliary optimization problems to 5 minutes in the nearest feasible procedure. Finally, we arbitrarily set the maximum CPU time to solve MILP1 to twelve hours in Figs. 1 and 2. In our numerical results, we generate all the instances randomly and consider only feasible instances. The instance dimensions we solve in Figs. 1–4 are shown in Table 1.In this table, we consider carefully selected size instances where the condition n1 ≥ n2 holds since these instances are significantly hard to solve with CPLEX. Notice that we repeat some instance dimensions in Figs. 1 and 2. We perform this because some of them can be solved in short CPU time with CPLEX while others cannot be solved in 12 hours.In order to solve MILP1, LP1 and MILP1(x¯)within each iteration of VNS Algorithm 5.1, and the auxiliary problems in the nearest feasible procedure with CPLEX, we set the parameterL=1012. We mention that we evaluated the effect of varying L fromL=106toL=1024in our MILP and LP formulations. It turns out that varying L has no effect on the optimal solutions of our models. In order to ensure that our choice for L is correct, we observe that variables λ, θ and μ in MILP1 are dominant when determining the minimum value of L. This can be verified since constraints (21) and (24) imply that L ≥ 1. Similarly, constraints (15) and (19) imply that0≤f−Cx−Dy≤Lwhich is still not a very large number. In fact, the largest value forf−Cx−Dyoccurs when each entry in matrices C and D is equal to−5. This would imply a minimum value ofL=5(n1+n2)2. Consequently, constraints (17), (18), (20), (22) and (23) might lead to large values of L. The same arguments are valid for the rest of MILP and LP formulations. Thus, we solved directly small, medium and large size instances of our MILP and LP models. We found that the maximum values among the entries of the dual variables λ, θ and μ are in the order of magnitude of 103.In Figs. 1 and 2, we present numerical results for QQBP using one sample for the input data whereas in Figs. 3 and 4, we present average numerical results using 10 samples of the input data for large scale instances. We do neither present average results for the instances in Figs. 1 and 2 nor consider more samples for the instances in Figs. 3 and 4 since the CPU times become highly prohibitive. In Figs. 1 and 3, we use a density level of 100% whereas in Figs. 2 and 4 we use 50%. Figs. 1–2 and 3–4 are each composed by 5 and 4 graphs respectively. Therefore, Figure I.J means the Jth graph of Figure I. In Figs. 1–4, we show similar information in the horizontal and vertical axes. More precisely, x-axes present the instance number whilst y-axes present LP upper bounds (LP1), MILP feasible solutions (MILP1), VNS solutions (VNS), LP based feasible solutions (F(LP1)), SDP upper bounds (SDP1) and SDP based feasible solutions (F(SDP1)). In particular, we present optimal solutions for the MILP models when the CPU times are lower than twelve hours in Figs. 1 and 2. Notice that in graph 1.2 we present the same information as graph 1.1 while removing the bounds of LP1 and F(LP1). This allows to show clearly the remaining curves of graph 1.1. Notice that we perform this process for graphs 1.3, 1.4 and 2.3, 2.4 in Figs. 1 and 2 respectively where we present the CPU time in seconds for the MILP and LP models, for VNS Algorithm 5.1 and for SDP relaxations. In graphs 1.4 and 2.4, we remove the MILP curves. Finally, in graphs 1.5 and 2.5 we show the number of LP iterations used by CPLEX. We do not report the number of iterations required by CSDP solver as it is not affected by the sparsity levels of the input matrices. In contrast, the number of iterations for the LP relaxation varies significantly with the input matrices sparsity levels. In general, the number of iterations for the SDP relaxations goes from 40 to at most 150 iterations. In Figs. 3 and 4, graphs 3.2 and 4.2 present the same information as graphs 3.1 and 4.1 respectively while removing the bounds of LP1. Next, in graphs 3.3 and 4.3 we present the CPU time in seconds for the LP and SDP models whereas in graphs 3.4 and 4.4, we report the number of LP iterations used by CPLEX.From our numerical results in the appendix, we note that for 7 instances out of 224, the quality of the best feasible solutions found with CPLEX in one hour is lower than those obtained with the nearest feasible procedure in less than 10 seconds. Moreover, for 2 of these instances, CPLEX cannot find better solutions than the nearest procedure in twelve hours. All these feasible solutions are found using SDP solutions when the condition n1 ≫ n2 holds.In particular, we observe that the CPU times required by CPLEX to solve MILP1 model increase significantly when using sparse data and when the instance dimensions increase. Regarding the feasible solutions obtained in Figs. 1 and 2 with the nearest feasible procedure, we obtain average gaps of 5.34% and 5.92% when using dense and sparse data, respectively. These gaps are obtained with the SDP relaxations and are calculated byMILP1−F(SDP1)MILP1*100. On the opposite, the average gaps obtained with the nearest feasible procedure using the LP solutions, i.e.MILP1−F(LP1)MILP1*100are between 98% and 100% for dense and sparse data, respectively.Regarding the VNS algorithm, we observe that it cannot find feasible solutions when n1 ≫ n2. In particular, VNS provides better solutions than the nearest feasible procedure for 80 instances out of 224 instances while the nearest feasible procedure finds better solutions for 144 instances. However, only 9 feasible solutions out of 144 are found using LP solutions. Whereas the remainder is found using SDP solutions. Finally, we observe that most of these 144 instances are solved when the number of binary variables in the leader problem is greater or equal than the number of variables in the follower problem, i.e., when n1 ≥ n2 independently of the density levels. In general, we observe that the CPU time required by LP relaxations is larger when the data is sparse than for dense data. The latter is illustrated by the number of iterations required by CPLEX to solve the LP relaxations. We do not report the CPU time used by CPLEX to solve the auxiliary problems in the nearest feasible procedure. They are less than 2 seconds for the whole instances in Figs. 1 and 2. We observe that CPLEX cannot solve 9 instances to optimality within twelve hours. This is also illustrated by the number of branch and bound nodes required by CPLEX which is considerably high in this case. Another observation we perceive in Figs. 1 and 2 is that the time used by CSDP is significantly low when n2 is small. This can be explained by the size of the matrix Z in SDP1 which grows faster with n2 compared to n1. Furthermore, we notice that the time required by CSDP to solve the SDP relaxation of QLBP is significantly lower than for SDP1. Finally, we note that many of the feasible solutions found are obtained at a significantly less computational effort than CPLEX needs to solve the MILP models. Concerning the upper bounds obtained with the SDP relaxations, 206 instances provide tighter bounds when compared to the optimal solutions or best solutions found by CPLEX for the MILP problems. In particular, we observe that SDP bounds quality is significantly better when n1 ≥ n2 whatever the density level is.In Figs. 3 and 4, we observe that the CPU time required by CPLEX to solve the LP relaxations is significantly larger for sparse data than for dense data. In contrast, CSDP can solve all the instances with less computational effort. The CPU time required by CSDP to solve SDP1 is in average 236.23 seconds compared to 486.28 seconds required by CPLEX to solve LP1 using sparse data. Notice that for 7 instances, the nearest feasible procedure cannot find feasible solutions using the LP or SDP fractional solutions in Figs. 3 and 4. We do not report numerical results for VNS in these figures since solving MILP1(x¯)for eachx¯within VNS becomes rapidly prohibitive. On the opposite, within the nearest feasible procedure we solve the auxiliary problem only once.Concerning the upper bounds, we observe that the SDP bounds are considerably tighter than the LP ones. Consequently, whatever the density level, the quality of the feasible solutions obtained with the nearest feasible procedure using SDP fractional solutions are remarkably better than those obtained using LP based solutions. In particular, we notice that the gapsSDP1−F(SDP1)SDP1*100are significantly smaller thanLP1−F(LP1)LP1*100for all the instances. Finally, we observe in Figs. 3 and 4, that the number of iterations required by CPLEX to solve the LP relaxations is higher for 8 instances out of 14 when dense data is used. However, the average CPU times are larger for the sparse instances.In Fig. 5, we plot the average values for LP and SDP relaxations, and for the feasible solutions obtained with the nearest feasible procedure for QQBP. These numerical results are obtained while varying the density levels from 50% to 100%. For this purpose, we randomly generate 50 medium size samples of the input data wheren1=80,n2=10andm1=m2=5for each density level. We mainly observe in Fig. 5 that the curve for the LP relaxation grows significantly more than the SDP one. Regarding the obtained feasible solutions, the curve of F(SDP1) is very tight with respect to the curve of SDP1. On the opposite, the curves found using LP solutions are considerably far from the LP bounds. Finally, in Fig. 6we plot CPU times in seconds for QQBP using dense data for n1 ranging from 100 up to 350 andn2=m1=m2=5; and for n2 ranging from 20 up to 200 andn1=m1=m2=5. We observe that the CPU time required by the SDP relaxation depends on n2 while the LP relaxation one depends on n1. This is due to the size of the matrix Z for SDP1 and to the number of linearization constraints for LP1.

@&#CONCLUSIONS@&#
In this paper, we compute upper bounds, optimal and feasible solutions for bilevel quadratic programming problems with binary decision variables in the leader and convex quadratic (or linear) programs in the follower problem. This is accomplished by transforming the bilevel quadratic problems into equivalent single level optimization problems. Thus, we replace the follower problem with the equivalent Karush Kuhn Tucker conditions. Then, we use the single level formulations to obtain mixed integer linear programming models, semidefinite and linear programming relaxations. Our numerical results indicate that the SDP relaxations are considerably tighter than the LP ones. Consequently, the SDP relaxations provide tight feasible solutions for small, medium and large size instances of the problem. In particular, when the number of variables in the leader problem is larger than in the follower problem. Moreover, they are solved at a significantly small computational effort for large scale instances.