@&#MAIN-TITLE@&#
Challenges in computational studies of enzyme structure, function and dynamics

@&#HIGHLIGHTS@&#
General introduction to the field of computational enzymology.Historical description of the field with emphasis on the work of the 2013 Chemistry Nobel Laureates.Discussion of the current state-of-the-art in the field and contemporary challenges.Overview of what computational enzymology has taught us about enzyme structure, dynamics and function.

@&#KEYPHRASES@&#
Computational enzymology,QM/MM,Free energy simulations,Reaction coordinates,Conformational sampling,

@&#ABSTRACT@&#
In this review we give an overview of the field of Computational enzymology. We start by describing the birth of the field, with emphasis on the work of the 2013 chemistry Nobel Laureates. We then present key features of the state-of-the-art in the field, showing what theory, accompanied by experiments, has taught us so far about enzymes. We also briefly describe computational methods, such as quantum mechanics-molecular mechanics approaches, reaction coordinate treatment, and free energy simulation approaches. We finalize by discussing open questions and challenges.

@&#INTRODUCTION@&#
Enzymes are the guardians of life, reducing the timescales of the chemical reactions so crucial to biology from millions of years to fractions of seconds [1]. Therefore, to understand how enzymes work is to understand how life works, at the most fundamental, molecular level. In addition to their biological roles, there is also great interest in understanding how to use enzymes as artificial catalysts for a range of processes, from chemical synthesis to the generation of novel biofuels [2]. The history of contemporary enzymology is a long and distinguished one, which, despite its seeming experimental dominance, is nevertheless inseparable from theoretical contributions. The earliest work in enzymology dates back to the mid-to-late 1800s, with studies of fermentation (in fact, the word enzyme literally translates to “in yeast” [3]), followed by Emil Fischer's “lock-and-key” theory of enzyme–ligand binding in 1894 [4]. However, in the absence of any structural data, it was hard to infer what kind of molecules enzymes even are, let alone how they actually work. Here, the development of transition state theory (TST) by Eyring and Polanyi [5] proved crucial, as it paved the way for Linus Pauling's brave (for the time) assertion that the tremendous catalytic power of enzymes comes from the highly specific tight binding of the transition states (TS) during the chemical reaction [6]. This observation is particularly impressive in light of the extremely limited information available about enzymes at the time. A major turning point in enzymology came in 1965, with the publication of the 2Å resolution crystal structure of hen egg white lysozyme by Phillips and coworkers [7]. This small 14.3kDa enzyme (mature form, Fig. 1) was the third protein and the first enzyme to have its structure solved [7], and as such, finally allowed an enzyme structure to be examined at atomic resolution, giving birth to structure-function studies of enzymes and allowing us to think of enzyme mechanism in structural terms. Increasing computational power and methodological developments at the time [8–13] meant that it was becoming possible to perform computer simulations on molecular structures. In subsequent years, in silico protein folding by minimization with normal mode jumps [14], and the earliest molecular dynamics (MD) simulation on a biological system [15], demonstrated that proteins are not in fact static structures, but rather dynamic entities. With time such protein dynamics have been accepted to be critical for their function. These major experimental and theoretical advances created a huge paradigm shift in the field, and marked the beginning of what is now contemporary enzymology.Since these early days, computational enzymology has become an invaluable tool for studying enzyme activity. However, despite the many advances in this field, and the corresponding experimental progress [16–19], we still do not have a complete understanding of how enzymes really work. In the present perspective, we begin with a historical overview, showing the birth of computational enzymology, with an emphasis on the work that provided the basis for awarding the 2013 Chemistry Nobel prize to Karplus, Levitt and Warshel [20–22]. Following this, we provide a concise summary of the current state-of-the-art in the field, showing what experiment and, in particular, theory, has taught us so far about enzymes. We also discuss some of the most popular theoretical techniques to study these biological machines. We finalize by highlighting and discussing open questions and challenges that we believe still need addressing.Choosing an exact event for the birth of a field is a difficult task. As discussed in the Introduction, a good “time-zero” for Computational Biochemistry could go back as far as the development of the first computer code for molecular mechanics (MM) methods by Allinger et al. [10,11], which was based on previous work on intermolecular potentials. Subsequently, Némethy and Scheraga developed simplified versions of these potentials for use in statistical mechanics simulations [9]. Concurrently, MD simulations (which date back to the early 1950s [23,24]), were gradually increasing in popularity at this point with the advent of the first supercomputers. However, it was still limited to small system sizes pertinent to problems in chemical and theoretical physics, such as the first simulations of liquids, a molten salt and a small organic molecule [25–29]. The first step enabling the leap from chemical physics to computational biology came in the late 1960s, when Lifson and Warshel developed the first consistent force field (CFF) [13]. This was combined with the fact that the Weizmann Institute had recently acquired what was then a powerful machine, aptly named the Golem (after the automaton from Jewish folklore) [30], which allowed them to perform comparatively computationally demanding calculations. While the first of these still, as would be expected, focused on small molecules [12], in 1969, Levitt and Lifson used CFF to perform the first energy minimization of simple protein structures (myoglobin and lysozyme) [31]. At the time, Karplus, who was an established theoretical chemist in areas ranging from NMR to reaction dynamics, joined the Lifson group at the Weizmann Institute for a six-month leave and the first contact between the Nobel Prize trio was made [32]. In 1970, Warshel and Bromberg then published a QM(VB)+MM study of the oxidation of 4a,4b-dihydrophenanthrene oxidation [33] which is the first published QM+MM simulation of a chemical process and laid the foundation for subsequent QM/MM calculations of biological systems. Note that the key difference between a QM+MM and a QM/MM simulation is that the latter includes treatment of the coupling between the QM and MM regions, whereas the former does not, as also highlighted in [34].At this stage, further progress was to some extent limited not just by computational power, but also by the lack of available structural data. The Protein Data Bank was only established in 1971 [35], at which point it only contained 7 protein structures. Another limitation was that traditional MM approaches, such as the ones used in the study of myoglobin and lysozyme, do not describe electrons explicitly, and cannot be used to describe bond making-and-breaking processes, making their usefulness for understanding chemical reactivity limited. However, numerous QM models were available at the time that could treat electronic structure properties. Thus, the ability to do separate QM and MM calculations existed in these early days, although a QM treatment was only possible for small molecules [33]. Still, there was a tremendous gap that needed to be bridged before it became possible to move to enzymes.In 1971, Karplus, together with Barry Honig, were working on the retinal chromophore, and employed a hybrid Hamiltonian relying on a Hückel one-electron term for the π-electrons, and a pairwise non-bonded energy function for the sigma bond framework [36]. Subsequently, Warshel joined the Karplus group as a post-doc, to continue the work on retinal, and in 1972, Warshel and Karplus published a hybrid method that combined more expensive QM methods with the cheaper MM methods [37]. The first iteration was restricted to planar systems, in which π-electrons were described by quantum mechanics, whereas the remaining system was treated classically, with the σ-electrons being incorporated into the nuclei. Further improvements were presented in the classic Warshel and Levitt 1976 paper [38], where these authors described what may be considered to be the first realization of current QM/MM approaches. Importantly, for the first time the crucial effect of the environment (treated at the MM level) on the QM region was included. During these very same years, the Nobel Prize trio engaged in groundbreaking work on protein simulations [14,15,39]. Gelin and Karplus used empirical potentials to study the side-chain dynamics in proteins using a precursor to the well-renowned CHARMM program [40], and, with McCammon, they studied the dynamics in a folded bovine pancreatic trypsin inhibitor protein [15]. Incidentally, this infant version of CHARMM relied heavily on the prior work of the Lifson and Scheraga groups [41], and on Warshel's presence in the Karplus group at the time [32]. Thus, the transition to biology was complete with the first computer simulations of actual biologically relevant problems.Since these early days of computational biology, tremendous progress has been made over the years. In 1982, Berendsen and coworkers performed the first MD simulation of a membrane [42], followed by a protein in an aqueous crystal [43]. Subsequent work in the Karplus group saw the development of simulated annealing MD simulation approaches for X-ray protein structure refinement [44] and NMR structure determination [45]. At about the same time, the first MD simulation of DNA in solution was performed [46], followed by the first MD simulations of proteins in explicit water, showing the crucial role the solvent plays in stabilizing proteins [47–49]. Later, Warshel developed the empirical valence bond approach (EVB) [50], which is an empirical QM/MM approach based on valence bond theory. EVB has been demonstrated to be an extremely powerful tool for calculating the free energies of enzymatic reactions, as well as provides further insight into enzyme catalysis, as seen not just from several works by Warshel and collaborators [51–54], but also from some of the elegant work by Hammes-Schiffer and coworkers and many others [55–57]. Jorgensen brought further progress to the field with the development and application of the all atom OPLS-AA force field (FF) [58,59], TIPnP water molecules and free energy methods, and field with the implementation of electrostatic embedding and an interface between a FF and a semiempirical program [60]. Further QM/MM specific contributions were made by Mo and Gao [61], Morokuma and coworkers [62], Singh and Kollman [63], as well as Lin and Truhlar [64]. Additionally, these works laid the foundation for the development of the nowadays well-known computer simulation packages such as CHARMM [65,66], Amber [67], GROMACS [68], BOSS [69], NAMD [70], among others, which made these developments available for use by the general scientific community. In the interests of space, not all the excellent contributions can be outlined here, but for more detail we refer the reader to reviews by Senn and Thiel [71], and van der Kamp and Mulholland [72].Together, these developments changed the overall shape of enzymology, demonstrating that not all experiments have to take place in the wet lab. We note that the Nobel trio were standing on the shoulders of giants – indeed, it is impossible to imagine the blossoming of multiscale modeling without the monumental contributions of Kohn and Pople to computational chemistry, for which they received the 1998 Nobel Prize in chemistry [19]. To help provide a chronological perspective for the reader, a crude timeline of some of these key events is shown in Fig. 2.As outlined above, the past century has seen substantial advances, both in computational techniques and in our understanding of how enzymes really work. Indeed, the field of QM/MM has matured significantly, but novel approaches are continuously being developed to address new challenges to our understanding of enzyme reactivity and dynamics. Due to space limitations we cannot address all of these in this Perspective. Therefore, we are limiting ourselves to those closest to our own areas of research. The purpose of this section is to cover current state-of-the-art QM/MM approaches, including simulation techniques adopted in conjunction with QM/MM methods, and highlight some important challenges. Additionally, we would like to provide some technical warnings for newcomers to this field, as well as, hopefully, to highlight a number of less considered problems in order to inspire potential future research efforts.Studying an enzyme at atomic resolution is a computationally demanding task. In this case, the complexity of the system is such that it is currently not possible to resort to pure QM methods to treat the entire system, and the phenomena under study cannot be accurately represented by MM methods (since MM methods are unable to describe the changes in electronic structure in the reacting part). Consequently, contemporary computational studies often use hybrid QM/MM approaches to elucidate the link between enzyme structure and function [71–73]. We note that recent work reporting minimization of small proteins at a pure QM level using code specially designed for graphical processing units, is a promising direction [74]. The QM/MM approach originally pioneered by Karplus, Levitt, and Warshel in the 1970s [38], has seen impressive advances over the past four decades in line with its increasing popularity. The overall concept behind this approach is deceptively simple: divide and conquer by treating the most important part of the system (the inner layer) using a high(er) level electronic structure method, and the rest of the system with a less expensive molecular mechanical method, which reduces computational cost and allows inclusion of the complete protein environment (the outer layer). Such methods may, in turn, be sub-divided into additive and subtractive schemes [75]. In the case of an additive scheme, the effective QM/MM Hamiltonian of the full system can be defined by:(1)HˆEff=HˆQM+HˆMM+HˆQM/MMHere, the energy of the full system is described by adding the energy obtained from the QM calculation in the inner layer with an MM calculation in the outer layer. Furthermore, an explicit coupling term is added that describes the interaction between both layers. The coupling term is crucial [20], and includes the bonded, electrostatic and van der Waals interactions between the atoms in both regions.(2)HˆQM/MM=−∑i,nqn|Ri−rn|+∑N,nZNqn|Ri−rn|+∑N,nANn(RN−rn)12−BNn(RN−rn)6Here the first term accounts for the interaction between the partial charges of the MM atoms (n), qn, and the electrons of the QM region, i. The second term accounts for the interaction between the MM partial charges and the nuclei, N, of the QM region, while the final term accounts for the van der Waals interactions between the QM and MM atoms. This latter term is needed to account for non-electrostatic interactions between QM and MM atoms. The terms of Eq. (2) are added to the Fock (ab initio or semiempirical methods) or Kohn-Sham (density functional theory (DFT) methods) matrix, and the wavefunction is polarized via the first term.In the ab initio and semiempirical formalisms, the total energy may be obtained by solving the Schrödinger equation in the field of the MM partial charges(3)HˆEffψ=ETotψand the total energy is then defined as(4)ETot=ψ|HˆQM|ψ+ψHˆQM/MMψ+EMMSimilar expressions arise for Kohn-Sham versions of DFT.One of the major challenges inherent to all QM/MM approaches is the high computational cost associated with the QM part of the calculation. This is particularly important if one is interested in QM/MM free energies where extensive conformational sampling is required in order to obtain converged results. Ideally, one would employ high-level ab initio or DFT methods for the QM region. However, this is often not practical due to the high computational cost. Alternatives to circumvent this cost have been suggested, as is the case of the Car-Parrinello method, which has been used by several groups to study enzymes [76–78]. One may also employ a semiempirical Hamiltonian to describe the QM region, and there have been an increasing number of recent studies utilizing such approaches in studies of enzymatic reactivity [79,80]. However, standard semiempirical methods are often not accurate enough to give quantitative insight and it is necessary to perform reparameterization of the canonical parameter set to fit a specific target reaction [81]. Such specific reaction parameter approaches have been applied with great success for numerous enzymes [82–88]. Semiempirical versions of DFT present another possible accurate, yet fast, description of the QM region [89]. Additionally, some workers try to resolve this problem by performing the computationally expensive conformational sampling using the semiempirical approach, and then performing a perturbation to a higher level potential using single point calculations [90,91], although the success of this approach has been quite variable, depending on how different the two potentials actually are.In spite of the usefulness of re-parameterized semiempirical methods, the QM calculation often remains the bottleneck for typical QM/MM calculations.This may be overcome in the EVB approach [50,51,92–94], which has its history in the construction of potential energy surfaces (e.g. LEPS [95]). This method elegantly transforms a force field based description of individual electronic ground state (GS) species into a quantum chemical framework using valence bond theory, allowing for an empirically-based QM/MM description of chemical reactivity. Specifically, this approach describes the reaction through a combination of diabatic states that correspond to classical valence bond (VB) structures, [51,92–94], the energies of which can be described by ɛiand ɛj(Fig. 3). The potential energy of each diabatic state is represented by classical MM force fields, which take the form:(5)εi=αgasi+Uintrai(R,Q)+USsi(R,Q,r,q)+USs(r,q)Here, the first term is the gas-phase energy of the reacting fragments in diabatic state i (ɛi) at infinite separation.RandQrepresent the coordinates and charges of the reacting atoms in the relevant diabatic state, andrandqrepresent the coordinates and charges of the surrounding protein and solvent.Uintraiand USsrepresent the potentials of the inner (intramolecular) and outer (solute-solute) layers, andUSsirepresents the coupling between both layers (solute-solvent). The values obtained through Eq. (5) provide the diagonal elements of an n×n EVB Hamiltonian, which in the simple case of a two-state reaction takes the form shown in Eq. (6):(6)HˆEVB=HiiHijHijHjj=εiHijHijεjThe corresponding off-diagonal (Hij) elements that describe the quantum mechanical coupling between the two states are related to the GS adiabatic energy, Eg, by:(7)Hij=(εi−Eg)(εj−Eg)Typically, this coupling is obtained using a simple exponential form as a function of the distance between reacting atoms, although more complex functional forms have also been used [96–98]. It should also be noted that the Hijelements have been rigorously verified to be phase-independent [51,92–94]. From Eq. (7), it can be seen that Egcan be obtained by simply diagonalizing the EVB Hamiltonian. Once the potential energy for each diabatic state has been obtained, the system can be adiabatically driven from one state to another to obtain the activation free energy, Δg‡, vide infra. Therefore, the different diabatic states provide an approximation for the basis functions used in MO-based QM calculations, and, like other semiempirical QM approaches, the EVB approach merely replaces the integrals related to every Hamiltonian matrix element with empirical functions. A core feature of the EVB philosophy is the rigorous calibration of a reference state (normally the background reaction in aqueous solution) to QM or experimental data, and then moving to a different environment such as an enzyme without the need for subsequent parameter adjustment, allowing one to quantify catalytic effects as well as pinpointing their exact origin. When this is combined with the fact that the EVB is an additive empirical QM/MM approach, it means that it is on the one hand fast enough to perform extensive conformational sampling of entire proteins, while at the same time describing both the bond-making and breaking process and the solute–solvent coupling in a consistent, physically meaningful fashion.In passing, we note that non-empirical flavors of QM/MM VB approaches have been developed separately by Gao, Truhlar and Shurki [61,99–102] among others, as well as hybrid approaches such as the simple valence bond method (SVB) [103]. In this latter approach an additional term, which is a function of a pre-determined reaction coordinate, is added to the QM/MM potential energy in order to correct the energy of the high level layer (e.g. a semi-empirical approach). Another example of a valence bond approach is the MCMM of Truhlar et al. This method is similar to the EVB approach, mainly differing in the fact that the resonance integral (off-diagonal elements of the Hamiltonian matrix) are obtained by Shepard interpolation of quadratic expansions around a set of points where electronic structure data are available. A description of this method and the application to the study of two organic reactions: an OH radical reacting with propane at the primary carbon and an OH radical reacting with camphor can be found in [104].Clearly, an ultimate long-term goal would be to be able to describe the entire system of interest with only QM, although we are still far from a stage where the computational cost of doing so would be trivial. Current development of fast DFT codes combined with graphical processing units is bringing this goal closer [74]. Another approach is that used for instance by ONIOM, which allows a multi-layered description of the system. This entails using different levels of theory in the different layers, for example, using a correlated wavefunction method in the inner (high-level) layer, and a simpler DFT or wavefunction-based method for the remaining system. A less computationally expensive approach when dealing with biomolecular systems, such as enzymes, is to use a semiempirical (“cheap”) QM approach or a density functional tight-binding (DFTB) [105] description of the outer layer. An example of a recent enzyme study using a combined DFT/semiempirical approach can be found in ref. [106]. Linear-scaling DFT approaches, such as the ONETEP method, have also been used in studying enzymes [107]. Yet another alternative is to move to constrained/frozen DFT approaches (CDFT/FDFT) [108–111]. In these methods the entire system is treated by DFT, but the electron densities of the atoms in the outer layer of the calculation are frozen (or constrained), thereby reducing the overall computational cost. The coupling between the inner and outer layers can then be evaluated using a non-additive kinetic energy functional (as discussed in e.g.[108–111]), allowing one to couple the two subsystems by means of orbital-free first-principles effective “embedding” potentials. CDFT/FDFT approaches have been successfully applied to a number of biochemical systems, for example studying ligand-to-metal charge transfer in metalloenzymes [112–114] and enzyme-catalyzed proton transfer [115]. For further details on recent developments in such orbital-free embedding approaches we refer the reader to e.g. refs. [116–118].When discussing common current approaches to study enzyme reaction mechanisms, we would like to point the readers to the recent growing interest in QM cluster models of enzyme active sites (for reviews, see e.g.[119,120]). Briefly, in cluster models, such as that illustrated in Fig. 4, the full enzyme system is truncated to a smaller subset of key residues that are deemed important for the catalytic mechanism. Such approaches have gained in popularity in recent years due to increasing computational power, allowing routine DFT calculations of systems comprising several hundred atoms [119]. Excellent advances in our understanding of enzyme mechanism have been made by the use of QM cluster calculations [121], in particular the seminal work of Siegbahn on Photosystem II [122,123] and the work of Shaik on heme-related systems [124]. A major advantage of such approaches is their simplicity, substantially reducing the complexity of handling the full system computationally. Furthermore, it allows the treatment of complex electronic states, which might be too expensive with QM/MM approaches. However, this simplicity comes at the cost of a full-treatment of long-range interactions and conformational entropy, so depending on the nature of the problems one wants to address, one may want to move to more complete (albeit more challenging) models of the system [125,126]. Nevertheless, if treated with care, such models can provide very useful insight into the intrinsic reactivity of the relevant enzymes (see e.g.[127–129]). Nonetheless, there is no “correct” way of choosing the size of the cluster, as even distant charged residues can have significant electrostatic contribution to the total energy and can therefore also influence the energetics of reactions, in particular reactions involving charge migration [84,130–132].Clearly, this Section is by no means exhaustive, and only touches on a small subset of popular current approaches. Ultimately, however, the reliability of all QM, QM/MM and QM/QM studies of enzyme mechanism will be limited by a wide range of issues. The first of these is the level of theory with which the QM region is described (usually DFT, semi-empirical, or EVB QM approach). The size of the high-level region is also crucial, as differently defined QM regions can give very different results (see for example [133,134]). The force field employed to describe the MM region is also very important, as this choice can greatly influence the accuracy of the results. Additionally, the polarization of the QM region via the MM partial charges must usually be fine-tuned on a per-system basis via the QM vdW parameters [135]. Furthermore, it might be necessary to employ a polarizable FF to account for the electronic response of the MM region to changes in the system [136–138]. Having reliably described both QM and MM regions, and their mutual polarization, there is also the question of how to best describe the covalent boundary between the two. This may be done by simple link-atom approaches [60], boundary atom approaches [139,140], localized orbitals [141] and generalized hybrid orbitals [142,143], all of which will affect the outcome of the calculations. Additionally, artifacts may arise if the boundary between the high and low level regions is too close to the reacting atoms.From this section, it can be seen that there are ultimately a plethora of different approaches available for studying enzymatic activity using quantum chemical techniques, and, ultimately, the final decision should be based on the complexity of the system of interest.A crucial issue when embarking on an in silico enzyme study is the choice of free energy simulation approach and reaction coordinate. Many options exist, and the results may vary depending on the choices made and the system at hand. Typically, the goal is to obtain the free energy profile for the reaction as a function of some reaction, or progress, coordinate, x, i.e. Δg(x). In order to obtain this, the simulation methods almost universally employ some form of biasing potential. This is needed in order to allow the sampling of rare events within a reasonable timeframe.The reaction coordinate should allow the simulation to explore as much as possible of the configuration space relevant to the chemical change. When studying reactions in the gas-phase or using a simplified (implicit) solvent model, a geometric reaction coordinate is often adequate, provided that the reaction can be simplified as a function of one or two changing degrees of freedom in a meaningful way. Now, even in aqueous solution, this is not always the case and a lot of important mechanistic information can be lost by such a compression of the multidimensional landscape onto a limited description of the reaction coordinate [93,94]. However, the situation is aggravated when dealing with explicit solvent representations or complex biological systems such as enzymes, where it is important to not only sample over all possible solute conformations, but also to capture the response of the environment to changes in the conformation of the solute.The simplest possible definition of a reaction coordinate is a geometric one. Such coordinates may be a simple bond-distance, angle, or dihedral angle, or the widely used anti-symmetric stretch coordinate [144]. Geometric coordinates are perhaps the most commonly used reaction coordinates in molecular simulations of chemical reactions [145], but require substantial prior insight into how the reaction is likely to proceed in the case of multidimensional systems, and arguably focus the search space too much as a result of mechanistic presumptions. An additional geometric coordinate may be defined as a bond order coordinate, which gained particular popularity in the experimental community due to seminal work by More O’Ferrall [146] and Jencks [147]. Using geometric metrics one may also define the reaction progress in terms of rehybridization, which provides additional insight [148].A fundamentally different approach to the reaction coordinate definition is to use the energy gap between two diabatic states. Here, x is defined as the energy gap between the reactant and product diabatic states, x=ɛ1−ɛ2, and this can then be used to describe the progress of the system from reactant to product. The energy gap reaction coordinate is a generalized reaction coordinate that does not require a pre-defined geometric path, but rather, it uses a superposition of the reactant and product potentials to drive the reaction [93,94]. The main advantages of the energy gap as a reaction coordinate are that it captures the main physics of the environmental response to the changes in solute geometry and charge distributions, while simultaneously allowing for accelerated convergence in free energy calculations in the condensed phase such as biological systems (see also discussion in [93,94]). The energy gap reaction coordinate has been successfully used in numerous studies of enzyme-catalyzed reactions, and it has been demonstrated to be a robust and effective reaction coordinate [52–54,149]. Note also that the energy gap coordinate is not limited to the EVB framework, but rather is a “universal” reaction coordinate that can be used for any system with a quantum chemical Hamiltonian [150]. Similarly to the energy gap coordinate, one may define an electrostatic potential (EP) difference coordinate, which describes the change in the EP difference between selected sites in the system [151]. For instance, one may define the difference between the EP at the donor–acceptor sites in hydrogen transfer reactions. Another category of reaction coordinates is collective variable coordinate, where one attempts to identify a set of coordinates, which describes the reaction progress. Usually, this is a set of geometric coordinates, which is found to change in concert during the reaction.The choice of reaction coordinate is often a result of the QM treatment selected. With ab initio, semiempirical, or DFT methods, geometric reaction coordinates are often a natural choice, whereas in VB or EVB approaches the energy gap coordinate is a good choice. The EP difference potential has the advantage of being applicable to ab initio, semiempirical, or DFT methods, in conjunction with a geometric reaction coordinate, which drives the reaction between the reactant and product states. Choosing a poor reaction coordinate will result in increased recrossing at the dividing surface. That is, trajectories that reach the TS from the reactant state (RS) will recross and return to the reactant side. This may be corrected for quantitatively by computing a recrossing coefficient [152]. This may be done by rotation of the dividing surface or by running activated dynamics simulations. For further information on reaction coordinates, we refer the interested reader to an excellent recent review by Clementi and co-workers [144].In addition to choosing an appropriate reaction coordinate, it is crucial to consider carefully the sampling method employed. It might initially seem tempting to simply identify a minimum potential energy path, which connects the reactant and product states via a TS. This is typically done in gas-phase reactions as well as in the solution phase with an implicit solvent description. However, this might be rather dangerous in explicit solvent simulations, as the energy landscape for such systems is quite complex, and one can easily get trapped in local minima [153]. The problem might become acute in enzyme simulations, and different starting structures can potentially give rather different results. We do note that in certain systems energy minimization approaches can be successful, presumably as a result of a good starting structure and a fairly rigid enzyme framework [71].One of the most commonly used sampling methods is umbrella sampling (US) [154]. The free energy difference along the reaction coordinate is defined as:(8)Δg(x)=−RTlnρ(x)+Cwhere R the gas constant, T is the temperature, ρ(x) is the probability density along the reaction coordinate, and C is an undetermined constant. In this approach, one applies an umbrella potential, which ideally equals the Uumb(x)=−Δg(x). Since this function is not known a priori (i.e. the goal is to find Δg(x)), an iterative approach is usually adopted, wherein the umbrella potential is refined in subsequent free energy simulations (so-called “adaptive umbrella sampling” [155–158]). Additionally, the reaction coordinate is usually divided into windows, which allows focused sampling in designated regions along the progress coordinate. To keep the system in the simulation windows, a restraining potential is applied, usually in the form of a harmonic potential(Ures(x)=k(x−x′)2). The combined biasing potential is thus composed of the umbrella and restraining potentials. The effect of the bias is removed using statistical methods such as the weighted histogram analysis method [159].Additional standard simulation techniques may be employed, such as free energy perturbation (FEP) and thermodynamic integration (TI). In the FEP approach one calculates free energy differences along the reaction coordinate using Zwanzig's perturbation formula [160]:(9)Δgx′→x″=−RTlnexp{−βΔETotx′→x″}x′In Eq. (9), the brackets…represent an ensemble average, β=(kBT)−1, and kB is Boltzmann's constant. Various schemes may be devised to do this efficiently, to reduce the number of QM calculations [161]. Analogously, one may define the free energy using TI, where the mean force acting on the reaction coordinate is integrated:(10)Δgx′→x″=∫x′x″dxdg(x)dx=∫x′x″dx∂H(x)∂xxPractically, in TI simulations, constrained dynamics may be run at discrete values of x and one samples the average force at each location, and the average forces are then integrated numerically over the reaction coordinate. If one instead integrates over time, one obtains slow-growth or steered MD, with the following expression:(11)Δgx′→x″=∫t′t″dt∂H(x)∂x∂x∂tIf the dynamics is driven by a restraint potential, the method is dubbed slow-growth, while if a guiding potential is used, it is termed steered MD.Warshel developed an approach, which combines FEP and US with the EVB Hamiltonian and the energy gap coordinate [162]. Initially, the potential energy for each diabatic state is obtained, and thereafter the system can be adiabatically driven from one state to another to obtain the activation free energy, Δg‡. This is done using a mapping potential that is written as a linear combination of reactant and product potentials:(12)εm=(1−λm)εi+λmεjwhere λmdescribes the weighting between the two states and is changed incrementally from 0 to 1 during the mapping procedure. The free energy, ΔGm, for changing the system from state i to j can be obtained using the standard FEP/US methods described above [162], and the corresponding free energy surface along the reaction coordinate, x, can be obtained from the free energy functionals of the individual diabatic states:(13)Δgi(x′)=ΔGm−β−1lnδ(x−x′)exp[−β(εi(x)−εm(x))]εmhere, Δgi(x′) corresponds to the free energy functional of the diabatic state i at position x′ along the reaction coordinate x. Provided that the perturbations to Δgi(x′) are sufficiently gradual, they can be patched together to give the full free energy surface. The GS free energy surface, Δgi(x′), can be evaluated in a similar way, using the GS energy instead of the diabatic energy (see Ref. [93,94,162] for discussion and Fig. 3 for a schematic representation). Despite its apparent simplicity, when the EVB approach is combined with the energy-gap reaction coordinate and FEP/US, it is an extremely powerful tool for describing chemical reactivity in both chemical and biological systems (see e.g.Fig. 5) [162].A powerful alternative to the standard simulation techniques described above, is the metadynamics approach of Laio and Parinello [163]. This method, which follows from the idea of using a reference potential approach, as well as the local elevation [164] and conformational flooding approaches [165], is an elegant way to focus conformational sampling by introducing the concept of memory to a simulations. Specifically, in this approach, repulsive markers are placed in a coarse timeline that is spanned by a small number of chemically relevant collective variables (which essentially define the “reaction coordinate” for the system). These markers are placed on top of the underlying free energy landscape for a system, thus discouraging the simulation from revisiting points that have already been visited in the conformational space. Therefore, the system can escape over the lowest energy TS as soon as the growing biasing potential and the underlying free energy well exactly counter-balance each other, allowing one to sample a much larger portion of the free energy landscape. In recent years, metadynamics has become a vastly popular tool, and has been successfully employed in the study of enzymatic reaction mechanisms [166–173], predominantly using DFT potentials. Additionally, recent advances in metadynamics allow estimation of the correct dynamic properties of barrier-crossings [174,175]. Despite its elegance, however, there are still two major challenges with the metadynamics approach. The first is the appropriate choice of collective variables (CVs): while there is a lot of flexibility in choosing these CVs, they nevertheless introduce bias into the simulation, and therefore the correct choice of CVs is both crucial and nontrivial. More of an issue, however, is the exceedingly high cost of repeated QM calculations that are necessary to obtain converged results in ab initio metadynamics calculations. For example, in even the simple case of metadynamics simulations of the nucleophilic attack of Cl− on MeCl in vacuum, 106 calls to the QM program were required in order to obtain convergent results [150].An alternative to the above-mentioned methods, entails the family of “string” approaches, which have recently gained in popularity [176–178]. Once again, these approaches try to minimize bias in the system by assembling a set of isocommitor surfaces between reactant and product states, and then trying to obtain the minimum energy path between these two states by trying to find the curve with the maximal flux of transition current through these isosurfaces (see [144]). This is done while creating a string of conformations that connect state A to state B, and trying to move this string toward the minimum energy path connecting these two states. In principle, a limitation of this approach is that it is best suited for smooth energy surfaces, however, it has been successfully applied to a range of problems including, for example [148,179–182].All of the above mentioned methods assume equilibrium behavior. This is in line with the basic presumptions of TST. However, if the chemical step is fast this assumption may break down, and non-equilibrium behavior becomes important. One way to introduce non-equilibrium behavior is by employing the Jarzynski equality [183,184], which connects between the irreversible work, W, for arbitrary switching times, t, and the equilibrium free energy:(14)exp−Δgx→x′kBT=exp−Wtx→x′kBTxwhere we move from a point x at equilibrium to a non-equilibrium point x′ in time t. The average is performed over the equilibrium ensemble…xat point x, and one performs a series of non-equilibrium pulling simulations to point x′. The practical implementation of the Jarzynski equality in actual simulations is termed fast growth MD. Thus, it seems that careful comparison between such non-equilibrium fast-growth methods and standard equilibrium methods could be of great interest [161].Additional concerns in enzyme simulations are the biases introduced in the simulations in order to cross barriers and the use of a pre-defined reaction coordinate. A method that alleviates both of these concerns is the transition path sampling (TPS) approach of Chandler and co-workers [185]. This method is a rare event sampling approach that focuses primarily on sampling the transition between two given states for a process, the reaction basins A and B. TPS entails a Monte Carlo (MC) search in reactive trajectory space, essentially moving between reactive trajectories. One of the major advantages of this approach is the fact that no information is required in advance about the actual reaction coordinate or TS structure, as long as the reaction basins are properly defined. An additional advantage is that the method does not apply any bias to the system, so in principle the dynamics of a TPS simulation is the true dynamics of the system. One of the disadvantages of TPS is the difficulty in reaching a converged transition path ensemble, resulting in a computationally costly approach [185]. Furthermore, it is not straightforward to obtain information regarding the free energy of a process from TPS since the search is performed in the reactive trajectory space, and little is known about the vast unreactive trajectory space. Several improvements to the TPS method have been suggested to deal with these issues [144,186–190]. Ideally, TPS can be used in conjunction with other methods, which provide free energy information, thus gaining both information regarding reaction rates and true dynamic behavior during reactions. The use of TPS for enzyme systems was pioneered by Schwartz and co-workers [191], and has since been adopted by others [192–194].Even after carefully choosing your preferred Hamiltonian, reaction coordinate and sampling method, there are still plenty of things that can go wrong in QM/MM simulations. In this section we will focus on selected issues that require careful attention.The first point, and possibly the most important one, is the position of the substrate and possible co-factor in the active site. Typically, a QM/MM project starts with a crystal or NMR structure of the target protein. In cases where the enzyme is not fully closed in its pre-organized catalytic form or the active site is empty, great care must be taken in generating an active form of the enzyme. This may entail homology modeling or some form of de-novo design, and might have to be done in an iterative fashion with enzyme mechanistic simulations, until a properly closed and active form of the enzyme is obtained. If the enzyme contains a substrate analog in the active site it might still be necessary to do careful docking of the true substrate into the active site in an attempt to form an active enzyme form. Clearly, if this step is not carried out in the most careful manner, the entire project is in jeopardy.An initial question when embarking on the project is the type of boundary to choose for the system. The most reliable options are periodic boundary conditions (PBC) or stochastic boundary conditions (SBC). In the former setup, the enzyme–substrate complex is embedded in a pre-equilibrated periodic water system, while in the latter it is embedded in a sphere of pre-equilibrated water molecules. The arguably preferred option is PBC, which allows full flexibility of the enzyme and in conjunction with the Ewald summation method [195–197] accounts for long-range electrostatics. SBC can in principle also include full enzyme flexibility, although this requires very large water spheres, and there will still be a boundary effect [198,199]. Other methods include the spherical solvent boundary potential model [200]. Similar but more general approaches include the generalized solvent boundary potential (GSBP), where not only solvent molecules, but also the macromolecule atoms can be treated implicitly [201], and the QM/MM general boundary potential of Benighaus and Thiel [202–205]. The effect of long-range electrostatics may be mimicked via the extended Ewald method of Kuwajima and Warshel [206] or the analytic continuum electrostatic method of Schaefer and Karplus [207]. Still, the number of water molecules to include in the system, and hence the density, is a point of some concern. The ambiguity arises because of the need to delete water molecules overlapping with the enzyme–substrate complex, and it is not always clear which waters one should retain. If one includes too many waters, the system becomes too dense, which in turn will inhibit protein motion. If one includes too few water molecules, the protein might be too flexible and charges may not be sufficiently well screened. In particular, the precise position of water molecules in the active site can be crucial as was shown by Yang and co-workers [208]. Additionally, counter ions should be added to neutralize the system [209] and if possible match the experimental concentration [210].An additional point of concern is the protonation states of protein residues as well as that of substrates and potential co-factors. Although automated tools exist for determining protein pKa values, these often fail for active site residues. This is partly due to the lack of FF parameters for substrates/co-factors in the pKa programs. Additionally, the enzyme is likely not in the active state in the initial structure. Thus, it is usually prudent to manually inspect the hydrogen-bond pattern of all titratable residues to arrive at reasonable protonation states. Following from this, occasional errors in crystal structures must be corrected prior to the in-silico setup (e.g. orientation of side-chains in His, Asn, and Gln residues). It may often be necessary to carry out the same enzyme mechanistic study using different protonation states of the enzyme, or possible using constant pH simulations [211].Finally, although we appreciate that this may be a controversial view due to their prevalent usage, we believe that the enzyme simulations should ideally be free of any restraints (except, out of necessity, for those related to the reaction coordinate in biased simulations). That does not mean that the use of restraints is not a valid approach; only that it can be dangerous to include restraints or constraints, since their introduction can inhibit important local conformational changes occurring along the reaction coordinate. The problem with introducing bias and its unknown impact on the system behavior was discussed in [52], where the study of adenylate kinase using the renormalization approach was conducted. It was also substantially discussed in Dryga and Warshel [212]. This is due to the fact that if one employs any form of restraints one might inhibit potentially crucial protein motions. Such restraint-free simulations are possible today, due to the reliability of modern force fields [213–218]. For instance, introducing even weak restraints on backbone atoms can introduce artifacts to the simulations, and possibly preclude conclusions relating to protein dynamics and their functional role. However, it may often be necessary to include restraints during the MD heating and equilibrium stages to allow the system to reach thermal equilibrium without undergoing large structural perturbations due to “hot pockets” in the enzyme.This list is by no means exhaustive and numerous additional potential pitfalls exist. These include the choice of size of the QM region, the coupling between the QM and MM regions, the treatment of covalent boundaries at the QM/MM interface [219], as well as the choice of simulation thermostat for constant temperature simulations [220]. However, it aims to provide illustrations of some of the key concerns when preparing and assessing the outcome of simulations of enzyme function and catalysis.After discussing numerous aspects of enzyme modeling, ranging from methods to strategies and pitfalls, one should maybe ask the crucial question of whether our simulations really mimic the chemical events as they happen in nature. Thus we should address the fundamental question of how chemical systems cross barriers, and whether standard simulation methods faithfully reproduce the dynamic behavior of the true systems. While clearly not intending to be dismissive of previous excellent computational work, we believe this is an important step toward further improving our understanding of questions of great contemporary interest such as the role of protein dynamics in enzymes. Chemical dynamics has been studied extensively using high-level methods for simple gas-phase chemistry. One of the first such examples was the seminal work of Porter, Sharma, and Karplus on the simple H2+H exchange reaction [221,222]. One of the startling conclusions of this work is that the chemical event (i.e. the reaction) occurs during the course of a few femtoseconds, in spite of the reaction having a considerable potential energy barrier. This illustrates that the rate constant observed in such simple reactions is not low because the individual reactive event is slow: indeed, the reactive event is fast. Rather, most attempts at reactions are unsuccessful because of the large activation energy, and it takes the system a long time to find reactive thermal collisions. This illustrates the nature of chemical reactions: a stochastic hunt in phase space (position and momentum space) for reactive states. Recent years have brought evidence of the accuracy possible with quantum dynamics simulations of simple gas-phase reactions [223,224].Reactions in condensed phase environments are considerably more challenging to treat theoretically, although the nature of the chemical reaction is likely still the same: fast chemical events occurring on the femtosecond–picosecond time-scale [92,225–228]. Thus, crossing of the TS region is usually very fast in condensed phase reactions, and the reaction seems slow because there is a low probability of reaching the reactive phase-space state from which it is possible to rapidly climb to the TS. In the case of condensed phase environments, it is necessary to reorganize the surrounding medium into a phase-space configuration, which is conducive to such fast reactive events. Thus, the environment needs to reorganize itself around the different charge distributions in the reactant and TSs. However, considering the fast bond vibrational motion during the reaction, the environment needs to reach a state capable of stabilizing the TS sufficiently to enable barrier crossing prior to the barrier crossing itself. Herein lies much of the catalytic effect – this reorganization is easier in a pre-organized enzyme matrix than in aqueous solution where the water molecules need to change the hydrogen-bonding environment prior to the chemistry [92,229]. This is not to say that no reorganization occurs during the chemical event, but rather that the majority of this reorganization is carried out before the reactive event. The question of whether enzyme residues have catalytic motions on the time-scale of the chemical event that are coupled to the chemistry is a topic of much current debate [52,230].Recent work has shown that the choice of sampling method can influence the nature of the TS obtained in enzyme simulations [194]. In a comparison between US and the string method on the one hand, and TPS on the other hand, it was found that the biased methods resulted in slightly shorter donor–acceptor distances for the hydride transfer in the enzyme dihydrofolate reductase. This was suggested to possibly be due to the use of a biasing potential, which perturbs the true reaction coordinate dynamics, as well as the assumption of equilibrium solvation, which is intrinsic to the simulation approaches such as US and string methods. Thus, the perturbation of reaction coordinate dynamics and equilibrium solvation might lead to slight changes in the location of the TS, something which might be accounted for via recrossing estimations. Despite this potential limitation in the fine details of the location of the TS, it should be pointed out that both molecular-orbital and EVB based standard QM/MM simulations of this enzyme by many groups have provided excellent agreement with experiment in terms of other observables, such as the reaction rates, temperature dependence of KIEs and the effect of mutations [231,232]. Thus, effects such as reaction coordinate dynamics and equilibrium solvation are likely very small.Computational enzymology has contributed greatly to both our understanding of the phenomenal rate enhancement in enzymes as well as in elucidating enzyme mechanisms. Below we first bring a historical perspective on the first QM/MM study of an enzyme, namely lysozyme, and subsequently describe some of the main lessons we have learned from the computational work on enzymes, and bring some selected examples.To concisely answer the question “how do enzymes really work?” is extremely challenging, and numerous hypotheses have been put forward to try to answer just that over the past few decades. These include preferential stabilization of the TS (i.e. TS stabilization), RS effects such as desolvation, dynamic effects, and nuclear QM tunneling [145,233].Historically, the earliest structure–function hypotheses date back to the crystal structure of lysozyme, mentioned in the Introduction [7], which led to the idea that enzymes work by destabilizing the RS of the reaction. Specifically, lysozyme is a very small glycoside hydrolase (129 amino acids for the hen egg white variant, Fig. 1(A)), catalyzing the hydrolysis of the peptidoglycan layer found in the cell wall of Gram-positive bacteria [7]. The peptidoglycan layer is composed of alternating oligosaccharides (2-acetamido-2-deoxy-glucopyranoside (NAG) and 2-acetamido-2-deoxy-3-O-lactyl-glucopyranoside (NAM)) [234,235]. The enzyme has a cleft that binds six saccharide units of the peptidoglycan. According to the classical “Phillips” hypothesis, an intermediate binds in a distorted conformation; more precisely a NAM subunit adopts a half-chair conformation forming a kink in the position where the bond is being broken. This structure bears greater resemblance to the TS than to the RS of the reaction. Therefore, a logical consequence of this observation appeared at the time to be that enzymes distort the RS of the reaction, and that strain plays a major role in enzyme catalysis. [236,237] In subsequent experimental and computational work on this enzyme, Vocadlo et al. [235] and Mulholland et al. [238] suggested a covalent mechanism in line with Koshland's proposal (Fig. 1) [239]. The reaction was suggested to proceed through an undistorted covalent intermediate; however, in the first step, the Michaelis complex presumably adopts a skew-boat or distorted envelope conformation.The RS strain idea was extended to many different systems over the years, and has been the subject of substantial controversy. While the idea of GS destabilization is still disputed [240–242], there has been substantial progress in resolving the debate through simulations that can probe the details of the catalytic step at atomic resolution. Such simulations have allowed dissecting the different contributions to catalysis [233,243], obtaining information that can only be inferred indirectly through experiment. For example, in the specific case of lysozyme, in the Warshel and Levitt 1976 paper, the factors that affect the intermediate stabilization were analyzed [38]. At the time this intermediate was though to be an oxacarbenium ion, although later proved to be a covalent glycosyl-enzyme intermediate [235]. This work [38] showed that steric strain does not contribute significantly, since the protein/substrate complex relaxes rapidly, however the major difference in electrostatic stabilization of the substrate in the protein in relation to vacuum suggested that electrostatic stabilization was the main factor that accounted for acceleration of the reaction rate. In contrast, already in the 1940s, Pauling (correctly) deduced that a major contributor to the catalytic proficiency of enzymes is their ability to provide much tighter binding of the TS compared to the RS of the reaction [244]. However, in the absence of structural data, it was unclear what the origin of this tight binding of the TS actually is. The advent of not just structural data, but also sufficiently powerful supercomputers allowed this question to be addressed in a direct fashion, and it was suggested already in the 1970s that the tremendous catalytic power of enzymes is the result of the catalytic preorganization of the active site [229], which in turn provides optimal TS stabilization. Conceptually, this is a simple idea: enzymes accelerate the rate of the chemical reaction relative to the corresponding reaction in solution because protein active sites have their dipoles optimally oriented to require minimal reorganization during the course of the reaction. This does not exist in the background aqueous solution reaction, where there is a high energetic penalty for reorienting the solvent dipoles in response to changes in the solute charge distribution (Fig. 6). This groundbreaking contribution by Arieh Warshel has become one of the cornerstones of our contemporary understanding of enzyme catalysis.Even still, it is a subject of debate, for instance in the case of the origin of the remarkable catalytic efficiency of orotidine decarboxylase [80,208,245–247], where desolvation of the RS appears to play an important role, based on experimental work [248], and theoretical studies have been contradictory [145,243]. Often the different conclusions drawn regarding the contributions of RS destabilization and TS stabilization to catalysis are a matter of reference reaction definition.The hypotheses highlighted above (RS destabilization, TS stabilization) are only two of the many hypotheses that have been put forward over the years to try to explain how enzymes actually work. Several additional suggestions that are derivatives of the first two categories have been proposed. Examples of RS destabilization include desolvation [248], steric strain [249], “near-attack conformer” (NACs) [250–253] and entropic changes due to the freezing of key motions [254,255], all of which are hypotheses that have been computationally challenged as playing only a minor role in enzyme catalysis [243]. In contrast, electrostatic complementarity, geometric complementarity, and electrostatic pre-organization of the active site for a given reaction all provide crucial TS stabilization, which, despite Pauling's brave initial hypotheses would have been impossible to quantify without computational effort [229,233]. Moreover, recent years have seen an increased interest in “non-thermodynamic hypotheses” to explain enzyme catalysis, such as dynamical contributions/mode coupling (promoting vibrations) and quantum tunneling to name some examples [82,228,256–266]. There has been lively debate surrounding these issues [233,260,267], and while strong arguments are presented on both sides of this debate, it appears that such effects, while they can be crucial for function (i.e. not the catalytic step itself), appear to have only a minor effect on reducing the overall activation barrier (i.e. catalysis) compared to more conventional thermodynamic and electrostatic effects. Finally, nuclear quantum effects (NQE) such as tunneling have been suggested to play a role in catalysis, as well as being a reporter on enzyme dynamics [228,267]. Extensive work by Gao and Truhlar and co-workers [152], and Layfield and Hammes-Schiffer [268] have suggested that it is essential to include NQE to correctly reproduce absolute rates for hydrogen (H, H+, H−) transfer reactions. However, NQE contributes only modestly to catalysis [82,269], although tunneling has been shown to have a small catalytic effect in the case of proton transfer in nitroalkane oxidase [82].At this point, it is valuable to highlight an important aspect that is often lost in the discussion about the origins of enzyme catalysis and the possible contribution of dynamic effects. Per definition, the term “catalysis”, in a chemical context, refers strictly to accelerating the rate of a chemical reaction, and therefore, in the most rigorous definition the catalytic effect of an enzyme can be taken to be the rate of the catalyzed reaction vs. the corresponding (intrinsic) uncatalyzed reaction [1,51,229]. Clearly, unlike the uncatalyzed counterpart, an enzymatic reaction is a complex, pre- and post-chemical, multistep process involving several conformational changes, substrate binding and formation of the Michaelis complex and subsequent product release, any of which can be rate limiting for a given system. However, under the strictest chemical definition, unless there is a clear memory effect between these steps (which has never been conclusively demonstrated), “catalysis” refers only to the rate of the chemical step in the enzyme compared to that of the corresponding uncatalyzed reaction, i.e. kcat/kuncat. Therefore, for a factor to be making a substantial contribution to the observed rate acceleration, it needs to be either absent in or substantially different from the background reference reaction (see discussion in e.g.[230]). This does not mean that protein flexibility is not crucial for enzyme activity; only that flexibility is usually also present in the background reaction. At the end of the day, enzymes use both pre-organization and flexibility to enhance the rate of chemical transformations. In the absence of proper pre-organization (i.e. folding), enzymes would not catalyze reactions, and without the inherent protein flexibility, most enzymes would likely be very poor catalysts, in spite of proper pre-organization. Indeed, enzymes ultimately only “care” about the fitness of an organism, and not the rate of the corresponding uncatalyzed reaction.A main strength of computer simulations, in particular QM or QM/MM studies, is their power to predict and rationalize biological mechanism. For example, Siegbahn performed extensive DFT geometry optimizations on small models of the oxygen-evolving complex (OEC) in photosystem II [270]. These calculations allowed him to propose a position for the fourth Mn ion (in this complex three manganese metal ions and the calcium form a cubane-like complex, however the position of the fourth Mn ion is uncertain) and to propose a new mechanism for this reaction, where the oxyl radical bounded to the 4th Mn ion forms an OO bond to a bridging oxo ligand in the Mn3Ca cube. A key feature of this mechanism is that the spins of the involved atoms are antiparallel aligned, which stabilizes the TS structure more than the reactants [270].Another example is the work of Prasad et al. in determining the mechanism of phosphate hydrolysis for the activation of Ras GTPase activating protein (RasGAP) and the elongation factor-Tu (EF-Tu) [271]. These authors performed EVB calculations on these systems and found that the preferred mechanism for RasGAP is glutamine-61 assisted proton-transfer to the substrate (Gln-61 changes the P-loop interaction with the TS, stabilizing it) or substrate-assisted catalysis through a second mediating water molecule. For EF-Tu the preferred mechanism is proton transfer assisted by 2 water molecules accompanied by major transition state stabilization, presumably due to an allosteric electrostatic effect. In fact, all the best mechanisms involved electrostatic stabilization of the TS or central intermediate, suggesting that this is a fundamental aspect of GTPases activation [271].Finally, not all enzymes are primarily intended to act as catalysts, as many enzymes control complex multistep reactions for which there simply exists no uncatalyzed counterpart (or the background reaction is so fast that no enzyme is needed). Terpene synthases provide such an example were chemical control is the principal “catalytic” effect [145,272–274]. In the absence of terpene cyclases, it is exceedingly difficult to obtain these molecules in the correct stereo- and regiochemical form [275]. This is due to the extreme reactivity of the intermediate carbocations, rather than insurmountable free energy barriers. Thus, in this family of enzymes chemical control, rather than rate enhancement is the key “catalytic” element [274]. Such control is achieved through an active site contour, which facilitates the correct folding of the initial substrate, sequestering and stabilization of intermediate reactive carbocations, as well as final reaction quenching [273]. QM/MM methods are ideally suited to treat such complex cases of chemical control, as one can readily map out various reaction pathways, including providing detailed information regarding highly reactive intermediates, which are not accessible from experiments. Interestingly, in certain terpene cyclases there is growing evidence for the existence of non-statistical dynamic effects, as has been demonstrated using multiscale simulations (Fig. 7) [276,277]. Such hitherto unknown dynamic effects in enzymes are due to underlying bifurcations on the free energy surfaces [278,279], with several open reaction channels. In order to control product distribution, these enzymes must be able to steer the reaction flux toward the desired reaction channel. This is achieved by using pre-organized electrostatic forces, which modulate the underlying free energy surface and selectively activate vibrations, which correctly channel the reaction flux. Again, in this case QM/MM simulations have been crucial in pinpointing these non-statistical dynamic effects.In some of the above-mentioned cases, rather than accelerating a reaction, the enzyme is acting as a facilitator, allowing for the controlled formation and cleavage of chemical bonds on demand. Therefore, biology does conflate overall function with rate acceleration, and understanding how the features that control either are crucial to understanding how enzymes work. Recent years have shown how limited our knowledge of this area actually is: increasing awareness of enzyme multifunctionality for sometimes completely unexpected side reactions [280] show how little we know about the factors driving enzyme selectivity and specificity; the (sometimes quite drastic) unexpected effects of distant mutations [125,126] show how little we understand about long-range information transfer in biological systems; exhaustive analysis of enzyme kinetic parameters show that the popular enzymes of the 20th century, such as acetylcholine esterase and carbonic anhydrase, fall well away from the median of both kcat and kcat/KMdistributions [281]. This shows that the “average” enzyme has not evolved to be as incredibly efficient as these “super enzymes”, and we need to rethink how we approach our understanding of the evolution and the activity of average enzymes. This is not necessarily to say that the enzyme toolbox has to be very different for average and “super enzymes” – possibly, some enzymes just use the same toolbox more efficiently than others. Finally, enzymes work in networks with potentially discontinuous activity patterns [282], where preventing the enzyme from producing too much product can be as important as catalyzing the actual chemical reaction. As with the problems highlighted above, once again, theory is likely to play a major role in helping resolve these questions as we move further and further into the 21st Century.One can only imagine the joy Lifson would have felt, had he lived to see the seeds he planted at the Weizmann Institute in the late 1960s culminate in the 2013 Nobel Prize in chemistry. In those early days when the first lines of computer codes were written, Karplus, Levitt, and Warshel laid the foundation for the entire field of bio-simulations, which we today take for granted. Since the early work on myoglobin, lysozyme, and the retinal chromophore, the field has advanced immensely. There have been vast improvements in methodology accompanied by concomitant improvements in computer hardware. There is an abundance of software packages, which allows one to perform state-of-the-art simulations of chemical and biological processes at various levels of accuracy and time-scales. Today, one can model the direct folding of small proteins using brute force MD simulations as it happens in nature [283], and compute enzyme rates with near chemical accuracy [284]. Remarkably, with all the improvements in software and hardware and the huge number of researchers in the field, the early insights and views of the 2013 Chemistry Laureates on enzyme catalysis and protein dynamics remain dominant today [20–22]. The concept of enzyme pre-organization in biological catalysis introduced by Warshel in 1978 is a fundamental view on enzyme catalysis, and the idea of protein flexibility, which was born out of the first MD simulations of proteins conducted by Karplus, Levitt, and Warshel, is uniformly accepted today.Fortunately for younger researchers (like the current authors, none of whom were born when the field of biosimulations was conceived), the field of computational enzymology is a dynamic one. At the methodological level, there are continuous improvements being made, which allow increasingly accurate simulations to be performed. At the biological level there are many unanswered questions regarding enzyme catalysis: How come enzymes are so big? Why are amino acid residues distal to the active site sometimes so catalytically important? Are protein dynamics important for chemistry? How come enzymes are often so promiscuous? In this Perspective, we have tried to highlight some of the multitude of ways in which we can now address such problems, however, these questions and many more are certain to keep the multi-scale modeler busy for some time to come.

@&#CONCLUSIONS@&#
