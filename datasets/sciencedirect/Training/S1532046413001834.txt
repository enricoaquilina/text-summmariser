@&#MAIN-TITLE@&#
A framework for unifying ontology-based semantic similarity measures: A study in the biomedical domain

@&#HIGHLIGHTS@&#
Analysis of measures used to estimate the similarity of biomedical concepts.Definition of a theoretical framework unifying several semantic measure paradigms.Identification of the core elements commonly used for semantic similarity design.Benefits for studying, defining and improving semantic measures are highlighted.Comparison of hundreds of semantic measures using SNOMED-CT healthcare terminology.

@&#KEYPHRASES@&#
Ontologies,Semantic similarity measures,Unifying framework,SNOMED-CT,Biomedical ontologies,

@&#ABSTRACT@&#
Ontologies are widely adopted in the biomedical domain to characterize various resources (e.g. diseases, drugs, scientific publications) with non-ambiguous meanings. By exploiting the structured knowledge that ontologies provide, a plethora of ad hoc and domain-specific semantic similarity measures have been defined over the last years. Nevertheless, some critical questions remain: which measure should be defined/chosen for a concrete application? Are some of the, a priori different, measures indeed equivalent? In order to bring some light to these questions, we perform an in-depth analysis of existing ontology-based measures to identify the core elements of semantic similarity assessment. As a result, this paper presents a unifying framework that aims to improve the understanding of semantic measures, to highlight their equivalences and to propose bridges between their theoretical bases. By demonstrating that groups of measures are just particular instantiations of parameterized functions, we unify a large number of state-of-the-art semantic similarity measures through common expressions. The application of the proposed framework and its practical usefulness is underlined by an empirical analysis of hundreds of semantic measures in a biomedical context.

@&#INTRODUCTION@&#
Over the last decade, considerable efforts have been made to standardize our understanding of various fields by means of ontologies, i.e. formal and explicit specifications of shared conceptualizations [1]. Ontologies enable modelling domains through sets of concepts and semantic relationships established between them. Due to the importance of knowledge representation and terminology in biology and medicine, the biomedical domain has been very prone to the definition of structured thesauri or ontologies (e.g. UMLS, SNOMED-CT, MeSH). They enable characterizing medical resources such as clinical records, diseases, genes, or even scientific articles, through unambiguous conceptualizations. To take advantage of this valuable knowledge for information retrieval and knowledge discovery, semantic similarity measures are used to estimate the similarity of concepts defined in ontologies and, hence, to assess the semantic proximity of the resources indexed by them.Ontology-based semantic similarity measures compare how similar the meanings of concepts are according to the taxonomical evidences modelled in the ontology. They are used in a wide array of applications: to design information retrieval algorithms [2,3], to disambiguate texts [4,5], to suggest drug repositioning [6] and to cluster genes according to their molecular function [7], to cite a few. Semantic similarity measures are indeed critical components of many knowledge-based systems [6,8,9]. Moreover, they are nowadays receiving more attention due to the growing adoption of both Semantic Web and Linked Data paradigms [10].A plethora of measures have been proposed over the last decades (see surveys [7,9,11]). Although some context-independent semantic similarity measures have been proposed [12–15], most measures were designed in an ad hoc manner and were expressed on the basis of domain-specific or application-oriented formalisms [8]. Therefore, most proposals related to those measures target a specific audience and fail to benefit other communities. In this way, a non-specialist can only interpret the large diversity of state-of-the-art proposals as an extensive list of measures. As a consequence, the selection of an appropriate measure for a specific usage context is a challenging task. Actually, no extensive studies enabled characterizing the large diversity of proposals, even though few seminal contributions focusing on theoretical aspects of ontology-based semantic similarity measures exist [8,16,17].Despite the large number of contributions related to ontology-based semantic similarity measures, the understanding of their foundations is nowadays limited. For a designer/practitioner, some fundamental questions remain: Why does a measure work better than another one? How does one choose or design a measure? Is it possible to distinguish families of measures sharing specific properties? How can one identify the most appropriate measures according to particular criteria?To fill these gaps, this paper proposes an extensive study of ontology-based semantic similarity measures from which a unifying framework decomposing measures through a set of intuitive core elements is proposed.The framework presented in this paper proposes to model, in a generic and flexible way, the core elements on which most measures available in the literature rely. Thus, particular semantic measures can be properly characterized and can directly be obtained as instantiations of the framework components. This brings new insights for the study of semantic measures:•Distinguishing the core elements on which measures rely. The theoretical characterization of semantic measures helps to understand the different measure paradigms and the large diversity of expressions proposed in the state-of-the-art.Unifying measures through parameterized measures. Based on the characterization of the core elements of semantic measures, our framework enables the identification of commonalities, bridges and equivalences between exiting measures. Indeed, their design could be unified through abstract expressions, even if many of them are (i) of ad hoc nature, (ii) domain-specific or (iii) based on different theoretical principles. Expressing semantic similarity measures through parameterized expressions can therefore facilitate the detection of their common properties and the analysis of their behaviour in specific applications.Selecting appropriate domain-specific measures. Such a framework provides a systematic, theoretically-coherent and direct way to define or tune the semantic similarity assessment for particular application scenarios. Semantic similarity measures expressed through parameterized functions could therefore be used to optimize measure tuning in domain-specific applications.Designing new families of semantic measures. New measures can be easily defined due to the modularity provided by the framework. Their design can take into account (i) the elements that affect the semantic assessment the most (e.g. estimation of concept specificity) and (ii) the particularities of ontology/application to which it will be applied (e.g. the presence of multiple inheritances).Identifying the crucial aspects of semantic similarity assessment. Empirical studies could be used to highlight the core elements best impacting measures’ accuracies. As a result, the framework can be used to guide research efforts towards the aspects that can improve measure performances.Such an approach will not just benefit a single measure designed for a domain-specific application (which is, to date, the focus of most related works) but will rather result in improvements of a wide set of measures and applications.The rest of the paper is organized as follows. Section 2 introduces the reader to ontology-based semantic similarity measures, distinguishing the various paradigms proposed for their design. In addition, this section reviews previous works regarding the unification of semantic measures. Section 3 describes the proposed framework from which state-of-the-art measures are unified, and from which new proposals can be derived. Section 4 illustrates the practical application of the framework in which semantic measures’ behaviours are analysed in a biomedical scenario. Section 5 provides the conclusions as well as some lines of future work.This section reviews the various paradigms used for the definition of ontology-based semantic similarity measures (SSMs). Each paradigm is illustrated by a selection of proposals emphasizing the essence of the approach. We then introduce the reader to existing contributions related to the unification of SSMs.SSMs aim at estimating the likeness of two concepts considering the taxonomical knowledge modelled in ontologies. We consider approaches measuring taxonomic distance/dissimilarity indistinctly; notice that the latter can be converted to similarities by means of a linear transformation. In this section, we present state-of-the art SSMs organized according to the various paradigms proposed for their definition.As a running example to illustrate the study, Fig. 1presents a snapshot of the SNOMED-CT clinical healthcare terminology [18], in which biomedical concepts are organized by taxonomic relationships. The topology of SNOMED-CT defines a partial order⪯between concepts, e.g. ‘Heparin’⪯‘Protein’ means that the concept ‘Heparin’ is subsumed by the concept ‘Protein’, that is, the heparin is a specific class of protein.Edge-based measures estimate the similarity of two concepts according to the strength of their interlinking in the ontology. The most usual approach considers the similarity as a function of the distance which separates the two concepts in the ontology. For instance, Rada et al. estimate the distance of two concepts u,v as the shortest-path linking them (sp(u,v)) [15].(1)DistRada(u,v)=sp(u,v)In Fig. 1, the shortest path between the concepts c5 and c3 is c5→c4→c3. Leacock and Chodorow proposed a non-linear adaptation of Rada’s distance to define the similarity measure SimLC[19]:(2)SimLC(u,v)=-logsp(u,v)2·Max_depthRada’s distance is here normalized by the maximal depth of the ontology, Max_depth, i.e. the longest of the shortest paths linking a concept to the concept which subsumes all the others (the root of the ontology, c0 in Fig. 1).More refined approaches propose to consider variations of the strength of the links between concepts; the deeper two linked concepts are, the stronger their semantic relationship will be considered. In most cases, the semantic similarity of two concepts is estimated as a function of the depth of the Least Common Ancestor (LCA), also named Least Common Subsumer (LCS), i.e. their common subsume, which has the maximum depth. In Fig. 1, the LCA of concepts c5 and c3 is c0. The deepest the LCA, the more specific it is considered and, thus, the more similar the compared concepts are assumed. Based on this strategy, Wu and Palmer proposed SimWP[13] and Pekar and Staab proposed SimPK[20]:(3)SimWP(u,v)=2·depth(LCAu,v)depth(u)+depth(v)(4)SimPK(u,v)=sp(LCAu,v,root)sp(u,LCAu,v)+sp(v,LCAu,v)-sp(LCAu,v,root)Most of these measures fulfil the Identity Of the Indiscernibles property (IOI), i.e. the similarity (resp. distance) of a concept to itself is maximal (minimal), e.g. DistRada(u,u)=0.Table A1 in appendix presents some of the most usually referenced SSMs based on edge counting, some of their properties are also given.Node-based approaches focus on the evaluation of concepts defined in the ontology. Two specific strategies can be distinguished: the feature-based and the one based on Information Theory.Feature-based strategies evaluate a concept as a set of features. These strategies are rooted into the feature-model proposed by Tversky [21]. For ontology-based measures, the features of a concept are usually considered as the set of concepts subsuming it, i.e. its ancestorsA(u)={v|u⪯v}. In other words, a concept is characterized by the semantics that are inherited from its ancestors. In Fig. 1, the concept c6 will therefore be represented by the set of features A(c6)={c0,c2,c3,c4,c6}.Feature-based strategies root semantic similarity in the context of classical binary or distance measures (e.g. set-based measures, vector-based measures). For example, Batet et al. assess taxonomic distance as the ratio between distinct and shared features [22]:(5)DistBatet(u,v)=log21+|A(u)⧹A(v)|+|A(v)⧹A(u)||A(u)⧹A(v)|+|A(v)⧹A(u)|+|A(u)∩A(v)|Another example of feature-based measure is given by Rodríguez and Egenhofer [23]:(6)SimRE(u,v)=|A(u)∩A(v)|γ·|A(u)⧹A(v)|+(1-γ)·|A(v)⧹A(u)|+|A(u)∩A(v)|with γ∊[0,1], a parameter that enables to tune measure symmetry. SimCMatch, the Concept-Match similarity measure [24], is another example of feature-based similarity measure:(7)SimCMatch(u,v)=|A(u)∩A(v)||A(u)∪A(v)|Some proposals adopting the feature-based strategy are presented in appendix Table A2.Approaches based on Information Theory assess the similarity of concepts according to the amount of information they provide, i.e. their Information Content (IC). To this end, Resnik computes the IC of a concept according to Shannon’s Information Theory as a function of its usage in a corpus [14]:(8)IC(u)=-log(p(u))with p(u) the probability the concept u occurs in a document of the corpus. The more a concept occurs, the less informative it will be considered, assuming that a concept also occurs when the concepts it subsumes occur. By definition, the IC monotonically decreases from the terminal concept (leaves) to the root of the ontology. This ensures the taxonomic coherency of the results, i.e.u⪯ν⇒IC(u)>IC(v). In other words, the IC of a concept will always be greater than the IC of one of its subsumers.The similarity of two concepts is usually estimated according to the IC of their Most Informative Common Ancestor (MICA), i.e. the concept which subsumes the two concepts and has the maximum IC. In Fig. 1, concept c1 is the MICA of the pair of concepts (c5,c7). As an example, Resnik proposes to estimate semantic similarity as follows [14]:(9)SimResnik(u,v)=IC(MICAu,v)Resnik’s measure does not explicitly capture the specificity, that is, the IC of evaluated concepts. Indeed, pairs of concepts with the same MICA will have identical similarity, even if, considering the taxonomical structure, their divergence towards their MICA is different. This is the case of the pairs (c2,c3) and (c2,c4), both having c0 as MICA in Fig. 1. Therefore, Lin [25], Jiang and Conrath (JC) [12], Pirró and Euzenat (SimFaith) [26] and Mazandu and Mulder (SimDIC) [27], among others, e.g. [28], refined Resnik’s measure to incorporate the IC of the compared concepts.(10)SimLin(u,v)=2·IC(MICAu,v)IC(u)+IC(v)(11)DistJC(u,v)=IC(u)+IC(v)-2·IC(MICAu,v)(12)SimFaith(u,v)=IC(MICAu,v)IC(u)+IC(v)-IC(MICAu,v)(13)SimDIC(u,v)=2∑c∈A(u)∩A(v)IC(c)∑c∈A(u)IC(c)+∑c∈A(v)IC(c)Note that the above measures only consider the MICA to estimate the information shared by two concepts. Indeed, in most cases, the MICA summarizes the information contained in the set of shared ancestors, as it is subsumed by this whole set. However, in some cases, due to multiple inheritances, the notion of MICA only captures the information shared by two concepts partially. For instance, in Fig. 1, considering that IC(c4)>IC(c2), the MICA of concepts c5 and c6 is c4. However, the amount of information shared by c5 and c6 is composed of the amount of information carried by {c4,c2}, their Set of Least Common Ancestors (SLCAs). In this case, MICA-based measures will only consider the most informative concept shared by two concepts. To better estimate the information shared by two concepts, Couto et al. proposed GraSM and DiShIn strategies in which the IC of the SLCAs of two concepts are aggregated [29,30].In appendix, Table A3 presents some well-known SSMs based on Information Theory.The cornerstone of the above measures is the accurate estimation of the IC of concepts. In order to avoid depending on annotated corpora, whose creation is time consuming, and which are sometimes difficult to obtain (due to data sensibility, e.g. patient record) [31], various intrinsic IC calculus models have been proposed. They estimate the IC of concepts by only considering structural information extracted from the ontology. Intrinsic IC calculus can be based on multiple topological characteristics such as the number of descendants, ancestors and depth [31–33]. Seco et al. [32] propose computing the IC of a concept as a function of its number of descendants:(14)ICSeco=1-log(D(u))log(|C|)withD(u)={v|v⪯u}and C the set of concepts defined in the ontology.In another approach, Sánchez et al. [31] estimate the IC of a concept according to the ratio between the number of terminal concepts (leaves) it subsumes and the amount of ancestors it has:(15)ICSanchez(u)=-log|leaves(u)||A(u)|+1Max_Leaves+1with leaves(u) as the number of leaves subsumed by the concept u (e.g. leaves(c2)={c5,c6} in Fig. 1) and Max_Leaves as the number of terminal concepts of the ontology.Hybrid approaches combine notions from edge-based and node-based approaches [34–37]. They are usually defined as ad hoc and weighted aggregations of ancestors, node degrees and concept specificities (e.g. IC) [35]. In appendix, Table A4 presents some proposals based on this principle.Tversky was the first to formulate a framework of semantic similarity, the feature model, from which a family of semantic measures can be derived [21]. The feature model proposes to characterize semantic objects in a broad sense and was not originally defined for ontological concepts. This model requires the semantic objects to be represented as sets of features. Their similarity is therefore intuitively defined as a function of their common and distinctive features, an approach commonly used to compare sets (e.g. Jaccard index). Tversky defined two parameterized SSMs: the contrast model (SimCM) and the ratio model (SimRM). They can be used to compare two semantic objects (u,v) through their respective sets of features U and V:(16)SimCM(u,v)=γf(U∩V)-αf(U⧹V)-βf(V⧹U)(17)SimRM(u,v)=f(U∩V)αf(U⧹V)+βf(V⧹U)+f(U∩V)with α,β and γ⩾0.Note that, considering f as the cardinality of the set, setting SimRMwith α=β=1 leads to the Jaccard index, and setting α=β=0.5 leads to the Dice coefficient. In other words, set-based measures can be used to easily express abstract formulations of similarity measures.The framework proposed by Tversky “just” defines that a semantic object can be represented as a set of features and that commonalities and differences must be evaluated to assess the similarity. By definition, SimCMand SimRMare therefore constrained in the set-based frame, i.e. they require compared objects to be represented as sets. They are, however, considered abstract measures as they rely on an undefined function f, specifying how to capture the sets of features of compared objects.Among the large diversity of proposals, most set-based measures can be split into two groups: those based on the Caillez and Kuntz σαformulation, and those based on Gower and Legendre σβformulation [17]. Since set-based measures can be used to design semantic measures, σαand σβcan be expressed in a straightforward manner according to the Tversky feature approach.(18)σα(u,v)=f(U∩V)f(U)α+f(V)α21/2(19)σβ(u,v)=βf(U∩V)f(U)+f(V)+(β-2)f(U∩V)The abstract formulation σαcan be used to express, among others, Simpson (α=−∞) and Ochaiai (α=0) coefficients [38]. The σβreformulation enables the definition of other numerous measures, e.g. Sokal and Sneath (β=0.5), and Jaccard (β=1) and Dice (β=2) coefficients [17,38].Blanchard et al. were the first to take advantage, in an explicit manner, of abstract SSMs to compare pairs of concepts defined in an ontology [17]. They focus on an information theoretic semantic similarity to underline relationships between several existing measures. For example, based on the intuitive notion of commonality and differences, they underlined that Wu & Palmer and Lin similarity measures, Eqs. (3) and (10), can be derived from the Dice index. They were also the first to stress the suitability of an abstract framework to define new measures and to study properties of groups of measures [17].Other authors also demonstrated the relationships between, a priori, different similarity measures and took further advantage of frameworks to design new measures [8,26,39-41]. These contributions mainly focused on establishing local relationships between set-based measures and measures based on Information Theory. As an example, Sánchez and Batet [8] proposed a framework, grounded in Information Theory, which allows several measures (i.e., edge-counting and set-based coefficients) to be uniformly redefined according to the notion of IC. Cross et al. also proposed a similar contribution in which feature-based approaches and measures based on Information Theory are expressed through the frame of the fuzzy-sets theory [40–42].Despite the suitability of existing frameworks for studying some of the SSM properties, only a few works rely on them to express measures [8,40]. Moreover, current frameworks only focus on a specific paradigm (e.g. feature-based strategy), which is used to express SSMs. In fact, existing frameworks only encompass a limited number of measures and were not defined in the purpose of unifying SSMs expressed using the variety of paradigms reviewed in Section 2. The following section is dedicated to the definition of such a unifying theoretical framework.The analysis of the state-of-the-art allowed us to distinguish a few core elements underlying most SSMs. Their notation and meaning are given in this section. The abstract measures, which can be defined as a function of those core elements, are then introduced and discussed. Finally, we illustrate the suitability of the proposed framework to express a selection of well-known SSMs available in the literature.In order to ease the readability of this section, the various notations which will be used to present the framework are listed below; provided examples are presented in association with Fig. 2:•G: the semantic graph (ontology) in which concepts are defined.C: the set of concepts in, e.g., C={c0,c1,…,c9}.A(u): the set of concepts including u and its ancestors, defined by the partial order⪯of G, i.e.A(u)={v∈C|u⪯v}. For example, A(c8)={c8,c1,c2,c0}.D(u): the set of descendants of c, i.e.D(u)={v∈C|v⪯u}. For example, D(c8)={c8,c9}.leaves(C): the concepts respecting D(c)={c},e.g. leaves(C)={c9,c5,c7}.{uv}: the set of taxonomic paths leading from concept u to concept v, e.g., {c8c0}={c8→c1→c0,c8→c1→c3→c2→c0,…}.Ωu,v⊆A(u)∩A(v): the Set of Least Common Ancestors (SLCAs) of concepts u and v, i.e., the minimal set of shared ancestors of u and v which are subsumed by the maximal number of common ancestors of the two concepts. The idea is to distinguish the set of concepts which contains all the meaning carried by the concepts subsuming u and v. Ωu,vtherefore corresponds to the set of ancestors of both u and v which are the more specific (e.g., deeper in the ontology) and non-comparable with regard to the partial order defined in G, i.e.,x≰yandy≰x. Otherwise stated, Ωu,vcorresponds to the leaves of the graph induced by the concepts found inA(u)∩A(v). For example, in Fig. 2, we obtainΩC9,C5={c1,c2}andΩC7,C5={c2}. More formally, the SLCAs of the concepts u and v can be defined as the minimal set of concepts respecting⋃c∈Ωu,vA(c)=A(u)∩A(v). The notion of SLCAs have also been introduced through the term Disjoint Common Ancestors (DCAs) in the literature [29]. We abbreviate Ωu,vby Ω when there is no ambiguity.Gc⊆G: the graph induced by a concept c, considering both its ancestors and descendants.Gc+(resp.Gc-): a graph induced by a concept C, only considering its ancestors (resp. descendants). For instance,GC3+is the sub-graph of G, only considering concepts A(c3)={c0,c1,c2,c0} and associated relationships.K: a domain containing any subset or subgraph of G: {uv},A(u),D(u),Gu,Gu+.In this section, we first distinguish the core elements of SSMs; secondly, we will further detail each of them through concrete examples.As stated in Section 2.1, similarity measures are designed according to specific paradigms (e.g., edge-counting or node-based strategies). Therefore, measure designers first adopt a specific paradigm from which estimators of commonalities and differences will be defined. They next adopt a strategy by which those estimators will be aggregated to express a similarity measure or a taxonomical distance. Indeed, in a broad sense, when comparing two things, their commonalities and differences are the only evidences from which similarity (or dissimilarity) can be evaluated. In the aim of distinguishing the core elements of SSMs, estimators of commonalities and differences intuitively appear as critical elements of semantic measures. In fact, they are the roots of all existing similarity measures.The definition of the estimators of commonalities and differences depends on the paradigm chosen to formulate SSMs. For instance, for edge-counting approaches, the difference of two concepts is assessed as a function of the length of the shortest path linking them, while for feature-based approaches, concept differences are computed as a function of the features characterizing a concept, which are not shared with the other.The main differences between existing paradigms depend on the strategy adopted to represent a concept. Such representation will determine the expressions of the estimators of commonalities and differences. Therefore, we formally introduce a function aiming at representing a concept.Definition 1The mapping of a set of conceptC′⊆Cto its semantic representation, denotedC∼′, which encompasses its semantic features, is defined by the functionρ(C′):We also formally define the function aiming to estimate the commonalities and differences of two concepts (u,v), according to their semantic representations(ũ,ũ):Definition 2The commonality of two concept representations(ũ,ũ)is estimated using a functionΨ(ũ,ũ):The amount of knowledge represented inũnot found inṽis estimated using a functionΦ(ũ,ũ):Designers of similarity measures sometimes consider the whole semantic space in which compared elements are modelled [38]. We therefore define a function enabling to capture this information.Definition 4The amount of knowledge defined in G (i.e. modelled in the ontology), which is neither found inũnor inṽ, can be estimated by a functionζ(ũ,ṽ):Most measures can be expressed in an abstract manner using the functions ρ,Ψ,Φ and, in some particular cases, ζ. However, there are situations in which functions Ψ and Φ may also be expressed according to the specificity of a concept or, more generally, according to the amount of information carried by a concept representation (e.g. Lin and Resnik for information theoretic measures). Thus, we further define two functions capturing these notions.Definition 5The specificity of a concept u is estimated by a function θ(u).Finally, we also generalize the notion of specificity of a concept to a representation of a concept:Definition 6The degree of specificity of a concept representationũcan be estimated by a functionΘ(ũ):The semantic representation of a set of concepts contained in an ontology can be viewed as a subset of the knowledge that the ontology models. Thus, the function ρ defines the mapping between a set of concepts and its semantic representation in the ontology. We first consider the case in which the set of concepts only contains a single concept. Fig. 4shows some semantic representations of a concept that are commonly used to design semantic measures.One of the most general semantic representations of a concept u is Gu, i.e. the graph induced by the ancestors (A(u)) and the descendants (D(u)) of u. However, in most cases, SSMs are based onGu+, the graph induced by u, only considering its ancestors Indeed, as stressed in Fig. 4, fromGu+, multiple concept representations can be derived, such as the set of ancestors A(u) or the set of paths linking the concept to the root {uroot} to cite a few. As we have seen in Section 2.1.2.1, representing a concept by A(u) is extensively used to express measures based on the feature approaches [22,23], or based on Information Theory [12,25,44]. Moreover, the representation of a concept through {uroot} is commonly adopted in defining measures based on the edge-counting approach [13,15,20].In order that the function ρ is defined for a set of concepts, we consider that union operators are defined for the proposed concept representations. This is indeed the case of all representations based on sets and of those corresponding to graphs. Formally, the representation of a set of conceptsC′⊆Ccan be derived from the representation of a single concept, i.e.ρ(C′)=⋃u∈C′ρ(u), e.g. definingρ(C′)=⋃u∈C′A(u).Numerous measures rely on the notion of the amount of information captured by a concept. The notion of Information Content (IC) exploited by information theoretic measures was defined for this purpose. Other strategies, which are not grounded in Information Theory, have also proposed to evaluate the specificity of a concept according to, for instance, its depth in the ontology [13]. We therefore generalized the notion of IC by introducing a function θ which estimates the specificity of a concept. Since the central element of the framework is the semantic representation of a concept (ρ), we also introduced a function Θ, which assesses the specificity of that semantic representation; in other words, this function generalizes θ. In the same manner as θ, and in coherency with the taxonomical structure, Θ also decreases monotonically from the leaves to the root of the ontology, i.e.u⪯v⇒Θ(ũ)>Θ(ṽ).Depending on the representation adopted for ρ, various strategies can be defined to evaluateΘ(ũ). Without loss of generality, we focus here on the case whereΘ(ũ)is assessed forũ⊆Gu+, e.g.ũ=A(u). Two commonly used strategies are briefly discussed:•Evaluating the cardinality ofũ. We obtainΘ(ũ)=|A(u)|, which can be substituted by θ(u) so that θ(u)=|A(u)|. In this case, a commonly used strategy is to defineΘ(ũ)=maxc∈A(u)θ(c̃)=θ(u). This strategy was adopted by Lin, Resnik, Wu, Palmer and numerous SSMs.Aggregating the specificity of the concept contained in A(u) by considering a particular θ function. This leads toΘ(ũ)=∑c∈A(u)θ(c), which can be extended toΘ(ũ)=∑c∈A(u)Θ(c̃). Mazandu and Mulder recently proposed an information content using such strategy to evaluate the specificity of a concept [27] (see Eq. (13)).The commonality between concept pairs, evaluated by the function Ψ, can be associated with the amount of information captured by features shared among the semantic representations of these concepts, i.e. intuitivelyΘ(ũ∩ṽ). For example, whenũis associated with sets, e.g.ũ=A(u), a commonly used strategy is to define commonalities by |A(u)∩A(v)| (e.g. Eq. (6), SimRE). In other words, the function Ψ assesses the specificity of the features shared between the semantic representations of the compared concepts.Numerous similarity measures consider taxonomies as tree structures (e.g. [13,20]). In a tree, there is just a single concept ω that subsumes two other concepts u, v such that A(ω)=A(u)∩A(v). The notions of LCA/LCS and MICA correspond to this concept ω. Moreover, in a tree, the depth of a concept is particularly suited to evaluate its specificity as A(u)=1+depth(u). Thus, in trees, the function Ψ can assess the commonalities of two concepts by just consideringΘ(ω̃), defining, for example,Θ(LCA∼u,v). However, because of the presence of multiple taxonomic inheritances in most widely used ontologies (e.g., SNOMED-CT, MeSH), the notion of a single subsuming concept ω characterizing the whole commonality of two concepts is not usually fulfilled. Therefore, in order to capture commonalities,Ψ(ũ,ṽ)must define an aggregation strategy while taking into account the specificity of all concepts composing Ωu,v, that is, the set of non-comparable common ancestors of concepts u and v. In other words, for most ontologies, ω must be generalized to Ωu,v.Each concept in Ωu,vrepresents a particular semantic facet of the commonality between the concepts u and v. Some approaches which evaluate these commonalities explicitly aggregate the amount of information of the semantic facets in Ω [14,25]. However, most measures adopt the maximal strategy as they only exploit ω*, that is, the concept of Ω which maximizes a selected θ function. Measures relying on the MICA (e.g. Lin [25], Resnik [14]) or on the LCA (e.g. Wu and Palmer [13]) are examples of this strategy. Nevertheless, other aggregations have been proposed [29,30]; for example, GraSM propose to average the specificities of concepts in Ω:ΨGraSM(ũ,ṽ)=∑c∈GΩ+θ(c)|Ω|Note that for ontologies incorporating multiple inheritances, the commonality of a pair of concepts can be also estimated by taking into account their common descendants, which can be seen as their shared potential extensions. The problem is symmetrical to the estimation of the commonality based on the shared ancestors Ω (which could be renamed Ω+). Likewise, a set Ω− representing the non-comparable common descendants of two concepts can also be expressed. Estimation of concepts’ commonality based on the study of their descendants has been recently introduced in [45].As we have seen, evaluating the commonalities of two terms is equivalent to evaluating the specificity of the semantic representation built from the group of concepts Ω. Existing approaches (LCA/MICA [13,14], GraSM and DiShIn [29,30]) only aggregate the specificity of the semantic facets represented by Ω.Some measures also rely on the differences between the semantic representations of the compared concepts, to which we refer as function Φ. Considering two concepts u,v, the information contained in u that is not in v is intuitively expressed by:(20)Φ(ũ,ṽ)=Θ(ũ)-Ψ(ũ,ṽ)In practice, Φ is usually computed asΦ(ũ,ṽ)=Θ(ũ)-Θ(Ω∼), withΘ(Ω∼)as the amount of information carried by the set of concepts in Ω. Moreover, similarly to Ψ, numerous Φ approaches only consider ω*, i.e. the concept from Ω, maximizing an expression of the function θ. This results inΦ(ũ,ṽ)=Θ(ũ)-Θ(ω̃*), which is usually expressed byΦ(ũ,ṽ)=θ(u)-θ(ω*)(e.g. [13,14,46]). We present an example of such a formulation used in the well-known Jiang and Conrath measure [12]:DistJC(u,v)=IC(u)+IC(v)-2·IC(MICAu,v)≈Θ(ũ)+Θ(ṽ)-2·Ψ(ũ,ṽ)≈Θ(ũ)-Ψ(ũ,ṽ)+Θ(ṽ)-Ψ(ũ,ṽ)≈θ(u)-θ(ω*)+θ(v)-θ(ω*)≈Φ(ũ,ṽ)+Φ(ṽ,ũ)Thus, by definingΦ(ũ,ṽ)=IC(u)-IC(MICAu,v), we obtain:DistJC(u,v)=IC(u)+IC(v)-2·IC(MICAu,v)For edge-counting approaches, as introduced in Section 3.2, the differences of a concept u with respect to v are usually assessed from the length of the shortest path between the concept and their LCA. Thus:Φ(ũ,ṽ)=sp(u,LCAu,v)Other strategies can be defined to aggregate the differences between the concept and those contained in Ω. As an example, some node-based measures (e.g. SimDICEq. (13)) take into account all the information related to Ω, as follows:Φ(ũ,ṽ)=ΘA(u)⧹⋃c∈ΩA(c)=Θ(A(u)⧹(A(u)∩A(v)))Despite particular instantiations of Φ, the vast majority of measures exploiting semantic differences (e.g. Jiang and Conrath, Lin, Resnik, Rada, Wu and Palmer, see equations Section 2) estimate the dissimilarity between two concepts asΦ(ũ,ṽ)=Θ(ũ)-Ψ(ũ,ṽ).Once we have distinguished the core elements of SSMs, they can be used to express a large diversity of measures by instantiating abstract expressions such as set-based coefficients. To illustrate the generality and possibilities of the proposed framework, we present some instantiations corresponding to existing measures that can be obtained from the Jaccard index. The Jaccard index can be intuitively generalized by considering an abstraction of the set-based operators, i.e., with U,V two sets,Ψ(ũ,ṽ)=U∩VandΦ(ũ,ṽ)=U⧹V, as follows:SimJaccard(u,v)=Ψ(ũ,ṽ)Ψ(ũ,ṽ)+Φ(ũ,ṽ)+Φ(ṽ,ũ)consideringΦ(ũ,ṽ)=Θ(ũ)-Ψ(ũ,ṽ).SimJaccard(u,v)=Ψ(ũ,ṽ)Θ(ũ)+Θ(ṽ)-Ψ(ũ,ṽ)Based on specific expressions of the functions Ψ and Φ (see Table 1), the Jaccard index can be used to express Pekar and Staab (SimPK) [20], SimFaith[26] or SimCMatch[24] (see Eqs. (4), (12), (7), Section 2.1). It can also be used to express SimcGIC, an unpublished pairwise measure based on SimGIC[43], which was initially designed to compare groups of concepts:SimcGIC(u,v)=∑c∈A(u)∩A(v)IC(c)∑c∈A(u)IC(c)+∑c∈A(v)IC(c)-∑c∈A(u)∩A(v)IC(c)The proposed framework also enables taking advantage of studies made for other binary measures (e.g. measures used to compare vectors or sets). Indeed, the proposed core elements can be mapped to existing theoretical tools used by other communities for studying binary measures. Table 2shows abstract expressions of the Operational Taxonomic Units (OTUs), classically used to represent binary measures [38]. Thus, such mapping can be used to easily express SSMs based on binary measure expressions relying on OTUs [38]. In a similar manner to the approach relying on information theory, the amount of information expressed in an ontology G can be viewed as Θ(G). The amount of information encompassed in the semantic representation of a concept is expressed byΘ(c̃), and the amount of information expressed in G, which is not found in c can be defined byΘ(c̃)¯.Therefore, based on the seventy expressions of binary measures distinguished by Choi et al. [38] and the correspondences proposed in Table 2, numerous new measures can easily be expressed. The main idea is to generalize existing binary measures using the proposed core elements of the framework to derive numerous SSMs, as performed above using an abstract formulation of the Jaccard index. Three examples of measures expressions are presented:SimDice(u,v)=2Ψ(ũ,ṽ)2Ψ(ũ,ṽ)+Φ(ũ,ṽ)+Φ(ṽ,ũ)DistHamming(u,v)=Φ(ũ,ṽ)+Φ(ṽ,ũ)SimBraun-Blanquet(u,v)=Ψ(ũ,ṽ)max(Ψ(ũ,ṽ)+Φ(ũ,ṽ),Ψ(ũ,ṽ)+Φ(ṽ,ũ))This section illustrates that distinguishing the core elements of SSMs and defining them at an abstract level is important to highlight relationships between them, and to better understand and capture the semantics associated to a measure. Moreover, based on contributions made for other types of similarity measures, we explicitly established a relationship between existing theoretical tools and show how a large diversity of SSMs can be easily generated.In this sub-section, we demonstrate the relationships between known abstract expressions of measures through the definition of a new parameterized SSM.In previous sections we have identified the core elements of SSMs and we have shown how they can be used to instantiate and design specific measures. Moreover, we have underlined that set-based measures can be used to express abstract measures. By extension, Caillez and Kuntz σαand Gower and Legendre σβformulas (presented in Section 2.2) may be considered abstract parameterized measures. Moreover, by focusing on the unification of measure expressions, we here demonstrate that under some conditions, σαand σβcan be partially unified and extended through a common expression.We first demonstrate that σαcan be easily extended to the well-known generalized mean of order α (Result 1). In addition, we show that σβis a particular case of the ratio model proposed by Tversky (Result 2). Finally, based on Results 1 and 2, we demonstrate that a new abstract tunable measure can be used to express a large diversity of abstract measure (Result 3).Result 1First, note that Cauchy’s mean σα implies a symmetric contribution ofΘ(ũ)andΘ(ṽ). In a straightforward manner, we extend σα to the generalized mean of order α[47]by introducing two parameters x and y, enabling us to tuneΘ(ũ)andΘ(ṽ)contributions.We demonstrate the relationship between σβ and the ratio model (Eqs. (19) and (17)respectively). Recall thatΘũ(resp.Θṽ) represents the specificity of the representation of a concept u (resp. v). σβ is defined by:ConsideringΘ(ũ)=Φ(ũ,ṽ)+Ψ(ũ,ṽ), σβ (Gower and Legendre abstract formulation) is a particular case of the ratio model.Considering the inverse of both σβand the ratio model SimRM, we obtain:□σα,x,y and the ratio model (which includes, see result 2) may be expressed by the general function Σα,x,y,z (shorten by Σ).This section is devoted to the application of the proposed framework in a practical setting. The main goal is to discuss how SSMs can be designed and studied by means of the core elements identified by the framework, from which the most commonly used are:•The estimator of the specificity of a concept defined in an ontology (function θ).The representation of a concept or a set of concepts corresponding to a canonical form which can be processed in order to extract the semantics of the (set of) concept(s) (function ρ).The estimator of the specificity of a concept representation, that is, the amount of information provided by the representation of a concept with regard to the information defined in the ontology (function Θ).The estimator of the commonality of two concept representations (function Ψ).The estimator of the difference between two concept representations (function Φ).In this section, we first detail some guidelines for the practical definition of measures based on the proposed framework. As done in Section 3.3, we define how existing measures can be mapped to the framework and how new proposals can be formulated. Secondly, we use the proposed framework to study and evaluate several SSMs in a biomedical usage context.In this subsection, we define the guidelines to instantiate/design a semantic similarity based on the proposed framework. Two main steps can be distinguished:1.Selection of an abstract measure, such as ∑, σα, SimRM(see Section 3.3, Eqs. (22), (18), and (17)).Definition of the expression of the core elements. This step consists in selecting a specific semantic representation of a concept (ρ function) and the definition of the expression of the abstract operators on which the selected abstract measure relies, for instance to estimate the commonality (Ψ) or the difference (Φ) between two concept representations (e.g. see Table 1).The first step to design a semantic measure is to select an abstract measure relying on some of the core elements distinguished by framework. The multiple set-based expressions introduced in Section 3.3 and the parameterized abstract measures discussed in Section 3.4 can be used to express a large diversity of measures.The selected abstract measure defines the semantics related to the compared concepts which will be taken into account during the comparison, e.g. commonalty (Ψ), difference (Φ), and also their weight in the similarity assessment. As an example, both the Jaccard index and the Dice coefficient can be derived from the Tversky’s ratio model by setting α,β=1 and α,β=0.5, respectively. It is therefore explicit that the Dice coefficient gives more importance to commonalities (and less importance to differences) for similarity estimation compared to the Jaccard Index. The selection of the abstract measure is therefore important to finely control the semantics of the scores produced by a measure. This aspect may be particularly important for context-specific applications.The next step consists in defining how to semantically represent a concept according to the available ontological knowledge. Such representation, i.e. function ρ, is required to define the expression of the operators used by the abstract measure. The selection of a specific representation (ρ function), e.g. set of concepts A(u), partially defines which semantics will be considered in the similarity assessment. Finally, expressions for abstract operators (e.g. estimators of commonalities or differences) must be defined in accordance with the selected expression of function ρ, which defines how to represent a concept.The users will therefore have to consider (i) specific expressions of the primitive functions distinguished by the framework, (ii) abstract semantic measures (e.g. abstract Tversky’s Ratio model) and (iii) specific parameter freedom. Two scenarios can therefore be distinguished:•The designer has a very clear idea about the more relevant elements that guide the similarity assessment in the concrete scenario and their relative weights and thus tunes and obtains the measure accordingly. Some of the parameters on which the measures rely can, for example, be restricted due to constraints defined by the context of use (e.g. the measure must be symmetric – the user will therefore only consider setting where α=β in the abstract ratio model).The designer has a training set of similarity scores (human-rated) that would be expected in such a scenario and that can be used to evaluate the accuracy of measures resulting from the framework instantiation. The set of measures to be evaluated can eventually be restricted according to specific properties induced by specific core element expressions, such as the algorithmic complexity (cf. scenario 1). The selection of the best suited measures will therefore be performed empirically using the training set from which performances of measures can be estimated. Such a training set or test sample must be composed of expected scores of similarity for a reasonable amount of pairs of concepts. It must be built alongside the experts of the domain according to the behaviour we want the system to have. In this case, the process detailed in Section 4.2, that is, the calculus of correlation values can be used to select the most appropriate measure. Using the Semantic Measures Library [59] (http://www.semantic-measures-library.org), the users can indeed easily implement a test in which the correlations between various configurations of semantic measures will be evaluated in order to distinguish the most suited set of parameters and therefore the most suited semantic measures according to a specific use case.With the above-described method, our framework can be used to easily instantiate existing or new SSMs, while finely controlling the semantics considered during the similarity assessment. Such constructive approach draws interesting perspectives for evaluating semantic measures, such as testing the influence of the various SSM components (i.e. abstract measures, core element expressions) over the accuracy of concrete measures in domain-specific tasks.As we have seen, once an abstract measure has been selected, e.g. the Tversky’s ratio model, the various components on which the abstract measure relies need to be defined in order to instantiate a SSM. Among them, we distinguish the parameters of the abstract function, if any (e.g. α,β for the ratio model), but also expressions of the core elements (e.g. the representation of a concept ρ the way to assess the commonality of two concept representations Ψ, their difference Φ).The selection of the parameters governing the measure instantiation is partially driven by the usage context (i.e. ontology). However, values of specific parameters can be difficult to tune and may only be assessed through empirical evaluations. The objective of this section is to discuss the impact of choices made while designing a measure. This study can only be made from the perspective of a specific usage context, since the suitability of SSMs may vary depending on the goal to achieve, in the same manner as human criteria may also vary from one setting to another. As stated in the introduction, due to the importance of SSMs and ontologies in biomedical research, the biomedical domain has been selected as the usage context. Next, we introduce the questions this experiment aims to answer. Then, we detail the experiment design and, finally, we present the results and discussions.The aim of the experiment is to analyse semantic similarity accuracy in a specific biomedical-related usage context. We specifically want to evaluate which are the parameters of SSMs, distinguished by the framework, which best impacts semantic similarity performance:•The selection of the abstract measure and its tuning.The expression of the core elements of the measures.This experiment highlights the relevance of the proposed framework to provide answers in a specific usage context. We left additional experiments in other domains, with the use of other measures and parameters for future work.For this experiment we considered the well-known Pedersen et al. benchmark, which is commonly used in the biomedical domain to evaluate SSMs according to human judgements of similarity [9]. Indeed, to be accurate, most of algorithms and treatments which extensively rely on semantic measures (e.g., in information retrieval, disambiguation and data analysis) require semantic measures to be highly correlated with human judgement of similarity [9,48,49]. Semantic measures are therefore commonly evaluated regarding their ability to mimic human experts’ appreciation of similarity between domain-specific concepts. The accuracy of measures is in this case evaluated regarding their correlations to the similarities assessed by domain experts for a set of concept pairs; the more the results of a measure are correlated to the scores of similarity assessed by the experts, the more accurate the measure will be considered.Pedersen et al. benchmark contains 29 pairs of terms related to the biomedical domain; for each one, the corresponding pair of concepts has been extracted from the SNOMED-CT biomedical ontology [18] (see appendix, Table B1 for the complete list of terms and associated SNOMED-CT concepts). For each pair of concepts, two semantic similarity scores associated to two sets of experts, 9 medical coders and 3 physicians, are given. Those scores have been obtained by averaging the ratings given by the experts of each group. An additional averaged score of similarity is also generally considered for each pair, which is computed by averaging the similarities assessed by both coders and physicians. Evaluation of SSMs is then tackled by computing the Pearson correlation against the similarity ratings given by each group of human experts (physicians, coder, both). In this experiment, the semantic similarity of each pair of concepts has been computed using SNOMED-CT as ontology.The study focuses on two abstract measures: the contrast model (SimCM) and the ratio model (SimRM) (equations 16 and 17).The α and β parameters, which tune the contribution of the information found in u (resp. v) which is not found in v (resp. u), were set from 0 to 15 with a step of 0.1, i.e., 150 α and β values. As a result of this parameter tuning, 22,500 abstract expressions (150*150) of both SimCMand SimRMhave been obtained and systematically evaluated. Notice that in SimCM, the γ parameter, which tunes the contribution of the commonality, was fixed to 1 as we did not want to study the effect of the variation of both the commonality and the difference (α and β) since they are both inversely correlated in most cases. For each abstract expression among the 22,500 evaluated, we further tested the four instantiations of the core elements shown in Table 3.Thus, for each abstract similarity measure, the four instantiations of the core elements lead to 90,000 individual measures, i.e. 22,500*4. Note that IC-dependent configurations used Sánchez et al. IC calculus model [31]. The final experiment is then based on the evaluation of more than 1 million measure configurations, i.e. 360,000 measure configurations for each evaluation benchmark: physicians, coders and the average of both ratings.Notice that some measures available in the literature correspond to particular points into the range of measure instantiations performed in this experiment. Table 4highlights some of these correspondences.Empirical evaluations were performed using the Semantic Measures Library [59]1See dedicated website http://www.semantic-measures-library.org.1a library dedicated to large-scale analysis and computation of SSMs. The source code and detailed documentation related to the experiment is open sourced and available at http://www.lgi2p.ema.fr:8090/~sharispe/publications/JBI2013.

@&#CONCLUSIONS@&#
A large diversity of semantic similarity measures (SSMs) has been proposed over the last decades, most of them focused on specific applications or domains. In this paper, we unified most well-known approaches through the definition of a theoretical framework dedicated to SSMs. The main advantages of the proposed framework rely on the identification of the core elements commonly used to design SSMs. We have indeed underlined that most measures can be expressed considering a limited set of core elements (functions) such as those defining (i) how to represent a concept through a processable canonical form (ρ), (ii) how to estimate its specificity (θ) and the specificity of its representation (Θ), and (iii) how to estimate the degree of commonality (Ψ) and difference (Φ) between two concept representations. In fact, we demonstrate how those core elements can be used to express a large diversity of (existing) measures based on generic parametric measures which can be seen as the backbone of semantic measures. The characterization of the measures through the distinguished core elements enabled us to better characterize measures relying on different paradigms and therefore to better understand the large diversity of measure expressions proposed in the state-of-the-art. In addition to this contribution, we also bring out, through detailed case studies and examples, the practical applications and interesting perspectives this framework provides:•Theoretical analysis and understanding of semantic measures. Distinguishing the core elements on which SSMs are based allow us to highlight narrow relationships between existing proposals. Indeed, we found that SSMs can be easily expressed through the definition of a few intuitive core elements and that most, if not all, measures are just particular expressions of a limited set of abstract measures. We therefore demonstrated that several measures which rely on the same abstract measure (e.g., abstracted ratio model), only differ due to a specific set of parameters selected to instantiate them (e.g., strategy used to represent a concept or to assess the commonality/difference between concept representations). This strong result is therefore important for the theoretical analysis of semantic measures. Indeed, most applications in which the measures are not selected through empirical analyses, expect the measures to fulfil specific properties, e.g., symmetry, respect of the identity of the indiscernible. Thanks to the breakdown of the measures proposed by the framework, properties of measures can be analysed, not only regarding specific measure instantiation (e.g. Lin’s, Resnik’s), but also focusing on both the abstract measures from which they derived (e.g. abstract contrast model) and the properties induced by the core elements. This enables a better understanding and analysis of the algorithmic complexity of measures, which is critical for most application contexts but nevertheless rarely considered in semantic measure proposals.Creation and tuning of semantic measures. The separation of measures from the core elements on which they rely enables researchers to focus not just on new ad hoc measures, but also on the design of specific strategies to improve the assessment of those core elements. As an example, we have seen that an accurate estimator of the commonality between two concepts (Ψ function), which depends on the canonical form adopted to represent a concept (ρ function), is of major importance to define semantic measures. Designers of measures can therefore improve several existing measures by improving the way the Ψ function is estimated with regard to a specific representation of a concept. It is therefore important to understand that improving the assessment of core elements distinguished by the framework leads to improvements in multiple measures, and not just to a specific measure in a concrete context. By distinguishing the core elements of semantic measures, the theoretical tool proposed in the paper therefore opens interesting perspectives for the definition and improvement of semantic measures in general. Moreover, by comparing measure performances in a biomedical-related context, we also illustrate how the framework can be used to express parameterized measures and to guide the adoption of a specific strategy. For example, if one wants to design an application aiming to cluster documents according to the similarity of their semantic annotations, he will have to select an appropriate measure for this task. With the help of our framework, instead of asking experts to evaluate the clusters produced by various similarity measures, which is time consuming, one can just ask them to rate the similarity of pairs of individual concepts and use them as training data to systematically evaluate and select appropriate measures, as done in our experiments. Therefore, the proposed approach can be used to select the most appropriate measures according to particular criteria by driving the decision process, much in the spirit of learning theories (e.g. correlation with respect to expected values given by an expert, algorithmic complexity of measures, etc.).We are currently investigating semantic similarity performances through the new insight provided by the proposed framework. Considering the large diversity of measures available, an important contribution for end-users of SSMs would be to provide tools which enable to select the best-suited measures for domain-specific usage contexts. The framework presented in this paper provides the theoretical frame for developing such tool. In addition, the framework will be used to perform detailed evaluations in other contexts and applications (i.e. other knowledge domains, ontologies and training data). We also plan to extend the framework to support semantic measures other than SSMs, i.e. relatedness [9], by exploiting non-taxonomic relationships available in the ontology and to compare groups of concepts, rather than pairs, which are widely used, for instance, to compare genes annotations [7].