@&#MAIN-TITLE@&#
Advanced conjoint analysis using feature selection via support vector machines

@&#HIGHLIGHTS@&#
Advanced machine learning techniques enhance conjoint analysis capabilities to better identify consumer preferences.Feature selection procedure pools information across consumers while obtaining individual preferences.Applications on experimental data as well as on two empirical studies show that the proposed approaches outperform traditional techniques for conjoint analysis.

@&#KEYPHRASES@&#
Conjoint analysis,Feature selection,Support vector machines,Business analytics,

@&#ABSTRACT@&#
One of the main tasks of conjoint analysis is to identify consumer preferences about potential products or services. Accordingly, different estimation methods have been proposed to determine the corresponding relevant attributes. Most of these approaches rely on the post-processing of the estimated preferences to establish the importance of such variables. This paper presents new techniques that simultaneously identify consumer preferences and the most relevant attributes. The proposed approaches have two appealing characteristics. Firstly, they are grounded on a support vector machine formulation that has proved important predictive ability in operations management and marketing contexts and secondly they obtain a more parsimonious representation of consumer preferences than traditional models. We report the results of an extensive simulation study that shows that unlike existing methods, our approach can accurately recover the model parameters as well as the relevant attributes. Additionally, we use two conjoint choice experiments whose results show that the proposed techniques have better fit and predictive accuracy than traditional methods and that they additionally provide an improved understanding of customer preferences.

@&#INTRODUCTION@&#
Conjoint analysis is one of the research techniques most widely used to identify customers’ preferences (see e.g. Green et al., 2001). Firms’ decisions regarding new product or service design (Kohli and Krishnamurti, 1989) as well as promotional and advertising campaigns increasingly rely on its outputs. Usually, the estimated preferences are the inputs for market simulation techniques that are then used to evaluate different market opportunities. Additionally, conjoint analysis allows estimating consumers’ willingness to pay (WTP), defined as the price of indifference between buying and not buying (Gensler et al., 2012), and thus it helps to make important pricing decisions. Consequently, appropriate conjoint studies and their derived implications can determine the success or failure of new product introductions or marketing campaigns.Originally developed in marketing (Green and Rao, 1971), conjoint analysis has had an increasing impact in many other disciplines such as health care (Bridge et al., 2011; Halme and Kallio, 2011), tourism management (Thyne et al., 2006), transportation (Hensher et al., 1998), and operations management (Dobson and Kalish, 1993), among others. Further applications where this technique has been successfully employed have been presented by Karniouchina et al. (2009).In addition, conjoint analysis is relevant for the Operations Research community for at least the following two reasons. First, conjoint analysis can be used in the context of multi-attribute decision making (MADM), since multiple attributes are considered in a preference measurement process. A comparison to an alternative MADM technique (Analytic Hierarchy Process) has been presented in Scholl et al. (2005). Second, optimization techniques are used in the context of conjoint analysis (see e.g. Camm et al., 2006; Halme and Kallio, 2011; 2014). This fact constitutes an opportunity to develop different types of advanced optimization models to increase the applicability of conjoint analysis.One of the main outputs of conjoint analysis is to identify the relevant attributes at the consumer level. That is, the (subset of) attributes that the consumer considers when evaluating the proposed alternatives. The usual approach to obtain such subset of attributes (or their ranking) is by post-processing the estimated parameters. For instance, the relative range of part-worths can be used to represent attribute importance when using additive models such as a mixed logit model. Such post-processing task implicitly assumes that consumers use all attributes when facing a conjoint decision. However, as shown later, traditional models can have problems eliminating irrelevant attributes across consumers, especially when there is limited individual-level data. Indeed, despite current developments in choice modeling that incorporate non-compensatory preferences, typical models based on conjoint analysis do not allow for “attribute non-attendance” (Hensher et al., 2012). This occurs when customers completely neglect some attributes and focus their attention on a small subset of attributes. As conjoint analysis studies have been incorporating more complex products that are characterized by a larger number of attributes and at the same time more data are available, it is expected that many consumers be more selective regarding the attributes they really consider. Our proposed model contributes in filling this gap in the academic literature and also aims at providing a useful contribution to practitioners.Several approaches from data mining and machine learning have been presented in the last decade in order to achieve better predictive performance in conjoint analysis (Evgeniou et al., 2005) and accurate representations of consumer preferences. These approaches have proved to provide important insights and consequently have gained reputation as valid methods to uncover customers’ preferences. However, they do not address the problem of effectively and efficiently selecting the relevant attributes used by consumers in their evaluation tasks. Attribute (or feature) selection has proved to be an important characteristic that predictive models need to include (see e.g. Blum and Langley, 1997; Guyon and Elisseeff, 2003). Not only because of a more parsimonious representation but also because it can better identify true underlying preferences that can lead to a higher predictive ability of consumer decisions. Table 1presents an overview of the relevant literature studied in this work.We present a novel technique based on Support Vector Machines (SVM) to determine the relevant attributes for estimating customer preferences. The identification of the relevant attributes that customers use to evaluate products, with the corresponding reduction in the dimensionality of customers’ utility functions, is achieved by a backward elimination of attributes procedure based on the individual part-worths. Therefore, such attribute selection is performed simultaneously to the estimation of customers’ preferences. An extensive simulation exercise shows that the proposed approach outperforms existing methods for attribute selection in the context of choice-based conjoint analysis.The contribution of the paper is twofold: (i) it presents a framework that simultaneously identifies the most relevant attributes when estimating customer preferences, and (ii) it shows that the understanding of customers’ preferences and the predictive performance of the proposed approach can be enhanced considering the most relevant attributes.The remainder of the paper is organized as follows. Section 2 discusses previous work. In particular, it describes SVM for CBC and provides a general overview of the different attribute selection approaches for SVM. The proposed method for attribute selection based on SVM for conjoint analysis is introduced in Section 3. In Section 4 we present the results of a simulation exercise that underline our method’s capabilities. Section 5 describes the application of the proposed approaches in two empirical conjoint studies highlighting the managerial implications that can be derived from the respective analyses. Section 6 summarizes the key results and discusses directions for future research.SVM were introduced to conjoint analysis by Evgeniou et al. (2005) and Cui and Curry (2005). Evgeniou et al. (2005) showed that SVM are accurate, robust to noise, and computationally efficient in a conjoint analysis context. Cui and Curry (2005) found that the predictive ability of SVM outperforms competing models such as multinomial logit models in consumer choice experiments. Later, Evgeniou et al. (2007) developed a convex approach for modeling consumer heterogeneity (Natter and Feurstein, 2002) in conjoint analysis, and compared it to Hierarchical Bayes (HB) methods. To the best of our knowledge, the present paper is the first work that adds feature selection to SVM for conjoint analysis.Section 2.1 describes SVM in the context of choice-based conjoint analysis (CBC) (Chapelle and Harchaoui, 2005; Cui and Curry, 2005; Evgeniou et al., 2005). In Section 2.2 we present the state-of-the-art regarding feature selection using SVM.Consider a product profile with J attributes. Each attribute is defined over njlevels, j = 1, …, J. Suppose a consumer evaluates the profiles of K different products and chooses one profile in each of T choice occasions. Finally, consider a sample of N customers.Customer i’s preferences are modeled by an additive utility function, which is assumed to be a linear combination of the partial utilities (part-worths):ui(x)=wiT·x,i = 1, …, N.We consider CBC data with the following information([xit1,…,xitK],yit),wherexitk∈ℜJand yit∈ {1, ..., K} for 1 ≤ i ≤ N, 1 ≤ t ≤ T, and 1 ≤ k ≤ K. The choice yit= k indicates that at occasion t, consumer i prefers the kth alternative among the K product profiles described by[xit1,…,xitK]. That is,ui(xityit)≥ui(xitb),∀b ∈ {1, …, K}∖{yit} (Chapelle and Harchaoui, 2005). Without loss of generality, and following previous research, we assume that for each choice occasion t all customers choose the first profile, i.e. yit= 1, 1 ≤ i ≤ N and 1 ≤ t ≤ T. Thus, the inequalities can be rewritten as(1)wiT·(xit1−xitk)≥0,where 1 ≤ i ≤ N, 2 ≤ k ≤ K, and 1 ≤ t ≤ T.To determine the weightswithe structural risk minimization principle(Vapnik and Chervonenkis, 1991) has been considered. This approach minimizes the Euclidean norm ofwi,with noise penalization via slack variables ξkt(l2-soft margin formulation) that leads to the following quadratic programming problem for each customer i = 1, …, N (Chapelle and Harchaoui, 2005; Evgeniou et al., 2005):(2)minwi,ξ12∥wi∥2+C∑t=1T∑k=2Kξkts.t.wiT·(xit1−xitk)≥1−ξktt=1,…,T;k=2,…,K.ξkt≥0t=1,…,T;k=2,…,K.Model (2) minimizes ξktthat represent inconsistencies in the choice data. This formulation simultaneously controls for the complexity of the model by maximizing the margin (∝1/∥wi∥2). The parameter C determines the trade-off between fitting the data and controlling for the model’s complexity. It can be set exogenously by the researcher or endogenously, using, for example, a cross-validation procedure (see e.g. Toubia et al., 2007a). The components of the vectorwi(corresponding to the individual part-worths) satisfy the stated choice preferences (constraints) (Evgeniou et al., 2005). The solution to this optimization problem yields the part-worthswifor each customer i = 1, …, N.CBC usually lacks sufficient information to estimate individual part-worths independently. Consequently, to allow for unobserved heterogeneity, the SVM formulation applied to CBC pools information across individuals in the same way as hierarchical Bayesian approaches do in discrete choice models (see e.g. Gelman and Pardoe, 2006). This pooling allows capturing general patterns at the population level and avoiding potential overfitting to each individual’s choices. In the SVM literature several approaches have been proposed to deal with this issue. For instance, Evgeniou et al. (2005) suggested a regularization procedure that specifies a hierarchical functional form of the individual part-worths considering a population part-worthw¯=1/N∑iwi. The trade-off between the individual and aggregated part-worth is controlled via cross-validation using a parameter γi∈ [0, 1] in the form of a weighted sumγiwi+(1−γi)w¯. Alternatively, Chapelle and Harchaoui (2005) proposed an optimization formulation that simultaneously computes the individual part-worths for all respondents, considering general patterns in the population. Later, Evgeniou et al. (2007) introduced an alternative approach that jointly obtains the individual part-worths using the information from all customers. Unlike a ridge regression, where a quadratic loss function is used to maximize fit, they suggest shrinking the weights toward a vectorw0,whose components are also decision variables.Feature selection addresses the problem of finding the most compact and informative subset of the original attributes. This is based on the assumption that irrelevant and redundant attributes have a negative effect on supervised learning (Blum and Langley, 1997; Maldonado and Weber, 2009). Feature selection has three important general benefits that can be applied to a choice-based conjoint context (Guyon et al., 2006). First, it improves the understanding of the decision process by obtaining a more parsimonious and meaningful representation of customer preferences. Second, it may improve the predictive performance of the model, especially in high-dimensional applications. This selection procedure can mitigate the curse of dimensionality that prescribes that as the number of attributes increases, an exponential increase in the number of observations is needed to maintain reliable model estimation (Stone, 1985). Additionally, the introduction of noise from irrelevant/redundant attributes results in less accurate predictors. And third, attribute selection limits storage requirements and increases the speed of the estimation algorithms. This is a critical issue in cases where accurate solutions are needed in a relatively short time.Three main approaches have been developed for feature selection: filter, wrapper, and embedded methods (Guyon et al., 2006).Filter methods use statistical feature properties to filter out irrelevant attributes. This is usually performed before applying any supervised or unsupervised model. These methods have advantages, such as their simplicity, scalability, and reduced computational effort. In contrast, they ignore the interactions among attributes and their relationship with the classification algorithm (Guyon et al., 2006).Wrapper methods explore the entire attribute space to score subsets of attributes according to their predictive power. Since the search for an optimal subset of attributes grows exponentially with the number of original variables, heuristic approaches have been suggested to address this combinatorial problem. The most commonly used wrapper strategies are the Sequential Forward Selection (SFS) and the Sequential Backward Elimination (SBE) (Guyon et al., 2006). In the first case, the strategy starts with few variables, and candidate variables are added sequentially to the set of selected features. At each iteration, the variable whose inclusion most improves the classifier’s performance is added to the set of selected attributes. In contrast, SBE starts with the complete set of attributes, and eliminates attributes sequentially.Embedded methods attempt to find an optimal subset of features while constructing the predictive model at the same time. In general, embedded methods present important advantages in terms of variable and model interaction, capturing the dependencies among variables, and being computationally less demanding than wrapper methods (Guyon et al., 2006). However, these techniques are more complex conceptually, and modifications to the classification algorithm may lead to poor performance. Guyon and Elisseeff (2003) presented a well-known embedded method for classification with SVM called Recursive Feature Elimination (SVM-RFE). The goal of this iterative approach, which inspired the method we propose next, is to find a subset of variables which maximizes the classifier’s performance. The feature to be removed in each iteration is the one whose removal minimizes the variation of the objective function. One advantage of this method is the possibility to perform non-linear feature selection. In the following section we present an adaptation of this approach for CBC.We propose methods for feature selection using SVM for conjoint analysis that build on Model (2) presented by Evgeniou et al. (2005). Our method is flexible enough to allow for sparseness in the datasets produced by a consumer partially ignoring the provided information. This flexibility could improve predictive performance by identifying relevant attributes and removing irrelevant ones when estimating customers’ preferences. In Section 3.1 we describe the feature selection approach with linear SVM and present the sequential backward elimination procedure. Then, in Section 3.2 we present the proposed non-linear approach.For the linear case, we first formulate an SVM for each customer i ∈ {1, …, N} to obtain the individual part-worths (Model (2)). Each attribute j has associated njpart-worths (one for each level), and the difference between the highest and the lowest part-worths can be considered as a measure of relevance as in traditional conjoint analysis (see e.g. Green and Rao, 1971). Formally, we define the attribute contribution ACjfor attribute j as:(3)ACj(wij)=maxwij−minwij,wherewij=(wi1j,wi2j,…,winjj)are the part-worths associated with each level of attribute j, whilemax(min)wij:=max(min){wi1j,…,winjj}.In the case of ordered levels in terms of consumer preferences (for example, from highest to lowest price), Eq. (3) becomes simply the difference between the part-worths of the last and the first levels of each attribute j, respectively:ACj(wij)=winjj−wi1j.We propose the following algorithm for identifying the relevant attributes while estimating the individual customer’s preferences. We follow the notation used by Song et al. (2012) whereSdenotes the full set of features. For each respondent a backward algorithm eliminates those attributes that are least important in the construction of the utility functions at each stage, indicated by the attribute contribution criterion.The parameter ε is a relevance threshold for the relative contribution of each attribute. This threshold needs to be sufficiently small to avoid the elimination of relevant attributes. The stopping criterion is reached when the contribution of all remaining attributes is above this threshold, or only one attribute remains.We now introduce an illustrative example to describe the functioning of the proposed algorithm.Illustrative example: Setting. Suppose we have data from a conjoint study for tablet computers described by five attributes: price, brand, screen size, processor and memory. For simplicity assume we estimate one weight (part-worth) per attribute.11In this caseACj(wij)=wij.The extension to our general specification per attribute-level is straightforward.Let consider two customers A and B. Customer A values attributes price and screen size only, whereas customer B values attributes price and processor only (seeTable 2). Both consumers use their underlying preferences to make choices.Illustrative example: Algorithm1. After collecting the choice-based conjoint data of the customers’ decisions, Algorithm 1 proceeds as follows. In an iterative process, it builds a linear utility function for customer A using all available attributes (Step 4), then, it computes the contribution measure (Eq. (3)) for all available attributes, and identifies its minimum value (Step 5). If this minimum value is below a given threshold ε, it removes the corresponding attribute (Step 6). Then, it goes back to Step 4 and builds a new linear function with the remaining attributes and continues with Steps 5 and 6 until all remaining attributes surpass the minimum contribution threshold or only one attribute remains. As the model will most likely assign low weights for attributes brand, processor and memory, this will result in low contribution measures and therefore those attributes will be removed, resulting in a utility function for customer A that includes only price and screen size. The same procedure is performed next for customer B, removing attributes brand, screen size, and memory (seeTable 3).Algorithm 2is a variation of the previous algorithm that includes a part-worth regularization procedure that controls for heterogeneity. The algorithm follows:Notice that Algorithm 2 differs from Algorithm 1 only in the last two instructions, where the part-worths are regularized after the feature selection procedure has been performed and the model has been estimated. The population weight vectorw¯is computed by averaging the individual weight vectorswi0that are obtained before the feature selection procedure is performed (first SVM training, Step 6). The regularization is conducted only for those attributes j that are relevant for a particular customer i (j∈Si,Step 13). Parameters C and γ are obtained via grid search; see Section 4.2. In order to avoid overfitting, the same values of C and γ are considered for all respondents.Illustrative example: Algorithm2. Following with our example described for Algorithm1, assume initial weightswA0=(2, 0.1, 2, 0.2, 0.3) andwB0=(1, 0.1, 0.2, 1, 0.3) for customers A and B, respectively (before feature selection). Assuming again that the weights of the relevant attributes do not change posterior to the backward elimination process, we obtainwA=(2, 0, 2, 0, 0) andwB=(1, 0, 0, 1, 0) for customers A and B, respectively. Following Step 11 in Algorithm2,w¯=(1.5, 0.1, 1.1, 0.6, 0.3) is the population weight vector. Next, the updated weights considering the pooling approach described in Step 13 and γ = 0.5 yieldswA*=(1.75, 0, 1.55, 0, 0) andwB*=(1.25, 0, 0, 0.8, 0). SeeTable 4.In this section we discuss the extension to non-linear utility functions. In the case of non-linear SVM, the data are mapped automatically into a higher-dimensional space H by a function ϕ: x → ϕ(x) ∈ H(Schoelkopf and Smola, 2002). This mapping allows efficiently capturing non-linear dependencies that linear utility functions are unable to uncover. Given that the only values one needs to compute are scalar products of the form ϕ(x) · ϕ(y), the mapping is performed by a kernel functionK(x,y)=ϕ(x)·ϕ(y)that defines an inner product in H (Evgeniou et al., 2005; Schoelkopf and Smola, 2002).The following formulation is solved in order to obtain non-linear utility functions. The detailed derivation of this formulation is given in the online appendix available at the European Journal of Operational Research website.(4)maxα∑t=1T∑k=2Kαkt−12∑t,s=1T∑k=2Kαktαks(K(xt1,xs1)+K(xtk,xsk)−K(xt1,xsk)−K(xtk,xs1))s.t.0≤αkt≤Ct=1,…,T;k=2,…,K.Subsequently, the estimated utility function has the following form:(5)ui(x)=∑t=1T∑k=2Kαkt(K(xt1,x)−K(xtk,x))Our approach for attribute selection is a variation of Algorithm 1, where attribute j’s contribution (ACj) has to be adapted for kernel functions as follows.(6)ACj(α)=|W2(α)−W(j)2(α)|where:(7)W2(α)=∑t,s=1T∑k=2Kαktαks(K(xt1,xs1)+K(xtk,xsk)−K(xt1,xsk)−K(xtk,xs1))and(8)W(j)2(α)=∑t,s=1T∑k=2Kαktαks(K(xt1(−j),xs1(−j))+K(xtk(−j),xsk(−j))−K(xt1(−j),xsk(−j))−K(xtk(−j),xs1(−j)))We note that only the second component of the objective function of Model (4) (12W2(α)) depends on the selected attributes. To identify the relevant attributes we eliminate the attributes whose removal does not significantly affect W2(α).The vectorsxt1(−j)andxtk(−j)used in Eq. (8) result when removing attribute j fromxt1andxtk,respectively. As a consequence,W(j)2(α)is almost identical to W2(α). The only difference is that it uses the reduced attribute vectors, i.e. the attribute vectors where the component j has been removed.Following Model (4), one seeks to minimize the metric W2 in the same manner as in the algorithm SVM-RFE (Guyon et al., 2006). In our attribute selection context, this implies that we want to eliminate those attributes whose removal keeps the value of metric W2 relatively small, leading to a small attribute contribution value ACj. Accordingly, the algorithm for CBC using non-linear SVMs for each customer i is provided in Algorithm 3.Notice that in this approach the regularization procedure used to obtain heterogeneous part-worths is not considered since in non-linear SVM only an approximation of the part-worths can be obtained. As a consequence, the part-worths are not readily available, and therefore it is not feasible to apply the part-worth regularization directly. However, allowing for heterogeneity in non-linear methods could lead to an interesting venue for future research.Several kernel functions are available for non-linear SVMs. The radial basis function (RBF, Gaussian kernel) is preferred in most applications (Maldonado et al., 2011) and has been used in our experiments:K(xi,xz)=exp(−∥xi−xz∥22σ2),where σ > 0 is a parameter controlling the width of the kernel, which determines the shape of the implied non-linear function.The objectives of the simulation exercise are assessing the effectiveness of the proposed estimation procedure in identifying relevant attributes and analyzing the performance of the model presented in Section 3 under different error conditions. Section 4.1 describes the simulation setup. Section 4.2 exhibits the preference models we applied and the performance measures used for their evaluation. Finally, Section 4.3 presents the results of our simulation exercise.We generated different datasets varying the noise condition in consumer choices (low and high noise) and the number of irrelevant attributes (low and high). In each condition we simulated choice data for N = 200 subjects across T = 12 choice occasions. Each choice set had K = 3 product profiles. These product profiles were generated using an orthogonal design with J = 10 attributes, each attribute j having nj= 4 levels, (j = 1, …, 10). The simulated data were then used to estimate all the different preference models by splitting the corresponding datasets into two subsamples: calibration and test. We used the first 10 simulated choice decisions for calibration, and the final two decisions for testing purposes.To vary the amount of noise we use the following procedure used by Arora and Huber (2001) and Toubia et al. (2007b). We first draw a four-level symmetric design of linear part-worths for each attribute j (j = 1, …, 10), generated from a normal distribution with meanμ=(−β,−β3,β3,β);and covariance matrixΣ= βI, where I is the 4 × 4 identity matrix. Note that lower values of β imply higher noise in the choice data. Therefore, and following Arora and Huber (2001), we used the values of β = 0.5 and β = 2 for “high” and “low” noise conditions, respectively. As a consequence, a priori, all attributes are equally important. Next, we generated irrelevant attributes to study the effect of the implied sparseness on customers’ preferences. We randomly selected two and six features for each individual and fixed their corresponding part-worths to zero (μ=0for those attributes) for the low sparseness and high sparseness conditions, respectively. Note that a high degree of sparseness corresponds to customers ignoring a high number of attributes when evaluating the different alternatives in each choice set.We estimated the following preference models:1.LCA: Linear compensatory by aspects, where each attribute level is an aspect which is represented by dummy coding.L-SVMi: Linear SVM using individual part-worths (Formulation (2)).L-SVMic: Linear SVM using individual part-worths, corrected with the aggregated part-worths.NL-SVM: Non-linear SVM (Formulation (4)).LBE-SVMi: Linear SVM with individual part-worths and linear backward elimination (Algorithm 1).LBE-SVMic: Linear SVM with individual regularized part-worths and linear backward elimination (Algorithm 2).NLBE-SVM: Non-linear SVM with kernel-based backward elimination (Algorithm 3).For LCA we formulate a mixed logit model and estimate it using a hierarchical Bayesian Markov chain Monte Carlo method (MCMC; see Rossi et al., 2005). For the proposed approaches we need to calibrate four additional parameters: C, ε, γ (only for regularized methods that control heterogeneity), and σ (only for kernel-based methods) as will be described next.We use a leave-one-out cross validation strategy to tune those parameters using only the training data. This procedure defines iteratively (for each individual) a subset of the training data comprising all questions but one. The individual part-worths are then estimated using this subset and subsequently used to predict the response to the question left out (validation subset). The predictive performance of the solution is assessed using a hit-rate metric. This procedure is repeated many times so that each question in the training sample is left out once and used for validation purposes. The parameters are set to the values that maximize the cross-validation hit rate. Finally, after the parameters have been tuned (and fixed), the utility functions are constructed using the entire calibration set, and the final evaluation is performed in a test set (holdout sample), which remains unused during the calibration process. This well-known machine learning procedure has been used previously in conjoint analysis (Evgeniou et al., 2005; Evgeniou et al., 2007; Toubia et al., 2007a).For the tuning parameters, and based on previous research (Maldonado et al., 2011), we explore the following sets applying grid search:C∈{2−5,2−4,2−3,2−2,2−1,20,21,22,23,24,25},γ∈{0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1},ϵ∈{0.25,0.5,0.75,1.0,1.25,1.5},andσ∈{20,21,22,23,24,25}.We then use these parameters to estimate the preference parameterswi. We estimate the above mentioned preference models that are compared based on the following performance measures.1.In-sample hit rate.Out-of-sample hit rate.Feature usage rate (FU-rate): The average number of attributes used by customers. We compute this measure as follows:(9)FU-rate=∑i=1N|Si|N·J,where|Si|is the cardinality ofSi,the subset of selected attributes for customer i (i = 1, …, N), and J is the number of all available attributes.Section 4.3.1 displays the results for the different models under varying noise and sparseness conditions. The effectiveness for attribute selection is assessed in Section 4.3.2.Table 5summarizes the results for the estimated preference models under different error conditions.Following previous research, we use out-of-sample performance for model selection. Accordingly, the best model is the one that yields the highest out-of-sample hit rate (highlighted in bold). We compare the other models to the best model and test if their holdout performances are statistically different at 1 percent significance level. A t-test is used to make the corresponding pairwise comparisons between average hit rates across customers.Several results can be derived from the simulation exercise. First, as expected, the performance of the different approaches increases as the level of noise (controlled by the magnitude of β) decreases. Second, the performance decreases as the level of sparseness (attributes ignored by customers) increases. Third, as seen in machine learning and forecasting applications, selecting the more relevant attributes improves the predictive performance (out-of-sample hit rate) without significantly decreasing the fit (in-sample hit rate).Fourth, the L-SVMic model, which performs a regularization of individual part-worths, performs worse than L-SVMi in terms of in-sample performance but better when facing out-of-sample data, reducing the risk of overfitting by incorporating general patterns into the individual part-worths. A further step in that direction is given by feature selection, since prediction performance is subsequently improved in three out of four cases. The fact that LBE-SVMic outperforms LBE-SVMi demonstrates the usefulness of allowing for heterogeneity in both cases: with and without feature selection. Finally, linear SVM models tend to perform better than the non-linear (kernel-based) ones, since the data were simulated assuming linear decision rules.We examine the effectiveness of the different feature selection models in identifying relevant and irrelevant attributes. Recall that the relevant attributes are the ones withμ≠0,and that two and six out of 10 attributes were simulated as irrelevant. We consider two types of errors: (i) selecting irrelevant attributes and (ii) eliminating relevant attributes. To evaluate the performance of these methods we consider both false-positive and false-negative predictions. We use the Kullback-Leibler divergence (KLD) measure proposed by Dzyabura and Hauser (2011) to deal with discrete predictions (see Appendix C in Dzyabura and Hauser, 2011). Like Dzyabura and Hauser (2011) we calculate divergence from perfect prediction, thus the smaller the KLD value, the better. Unlike Dzyabura and Hauser who use observed consideration data to build the model and predict considered profiles in a validation sample (see Dzyabura and Hauser, 2011), we use observed choices to build the model (without information about attribute relevance), and we predict ignored attributes.Table 6summarizes the corresponding results for the simulated conditions. We compare the proposed models to two null models that predict that all attributes are relevant and none of the attributes is relevant. We indicate with the superindex † when a model is significantly better (lower KLD) than the null models, and with the superindex * to highlight the best model (or statistically similar to the best model). Again, t-tests were performed to make pairwise comparisons between the feature selection performance of the different approaches.In Table 6 we observe a significantly lower divergence of SVM-based feature selection models compared to null models. Additionally, as expected, the accuracy of the methods decreases with the level of error and sparseness. Interestingly, the model LBE-SVMic performs better than the other models displayed in Table 6 when there is high error and a low number of irrelevant attributes (HL), and when there is low error and a high number of irrelevant attributes (LH). Otherwise, it performs worse than the other feature selection models. It is important to note that the overall performance achieved by the proposed approach in terms of the KL divergence metric is comparable to other studies in which the information of the elements included (typically profiles) is observable. As we mentioned before, our models do not use information of this kind (considered profiles) and predict relevant and irrelevant attributes from observed choices.In this section we illustrate the application and characteristics of the proposed approaches using two existing choice-based conjoint datasets. The first set, analyzed in Section 5.1, is comprised of products (digital cameras) described across five attributes with four levels each (20 aspects in total). The product profiles are presented in choice sets with four alternatives. The second dataset, studied in Section 5.2, is a larger set that represents information usually collected for marketing research purposes. It contains products described across 10 unbalanced attributes with between 3 and 15 levels (51 aspects in total). The products in this dataset are presented in choice sets with three alternatives. In Section 5.3 we provide academic and managerial insights of our work based on these two applications.N = 125 subjects were asked to evaluate different digital cameras in an on-line CBC study. A digital camera in this study is described by J = 5 attributes with nj= 4 levels (j = 1, …, 5):•Price ($500, $400, $300, and $200),Resolution (2, 3, 4, and 5 Megapixels),Battery life (150, 300, 450, and 600 pictures),Optical zoom (2x, 3x, 4x, and 5x), andCamera size (SLR, Medium, Pocket, and Ultra Compact).Subjects responded to 20 choice questions, with each choice question comprised of four product profiles. See Abernethy et al. (2008) for further details about the conjoint experiment.The proposed approach simultaneously uncovers consumer preferences and identifies the most important attributes even when customers completely neglect some attributes while evaluating the product profiles. This is equivalent to a non-compensatory decision process where bad performances in one attribute cannot be compensated with good performances in other attributes. Accordingly, and following the literature in decision process we estimate two types of preference models (i) compensatory approaches and (ii) non-compensatory approaches. For the compensatory models we estimate a state-of-the-art mixed logit model (LCA) and a q-compensatory model that is used to represent strict compensatory preferences (Hauser et al., 2010; Jedidi et al., 2013). The q-compensatory model is an additive model in which the importance of any aspect is no more than q times as large as the importance of any other aspect. Additionally, we include traditional SVM-based models that do not incorporate feature selection to highlight the differences in terms of predictive performance and identification of the relevant attributes. As non-compensatory benchmarks we choose two types of models: Elimination By Aspects (EBA) and Lexicographic. For EBA we use the Gilbride and Allenby (2006) approach, while for the Lexicographic models we use the Jedidi et al. (2013) approach. For these models we estimate three variants: Lexicographic by attributes (LBA), Lexicographic acceptance by aspects (LAL), and Lexicographic elimination by aspects (LEL). All benchmark models were specified as proposed by the authors and estimated using a hierarchical Bayesian MCMC approach (see e.g. Rossi et al., 2005). Our proposed SVM feature selection approach is included in this type of models.We consider the first 16 questions to calibrate the models, tune the respective parameters in the case of SVM methods, and identify the relevant attributes. With the last four questions we test the estimated models.

@&#CONCLUSIONS@&#
In this work, we extend previous literature on conjoint analysis by allowing for feature selection and providing a new methodology to identify the relevant attributes in a choice-based conjoint setting. The proposed new methods are based on statistical learning techniques and perform feature selection simultaneously with estimating customers’ preferences via a modified SVM approach. We adapt linear as well as non-linear SVMs to identify relevant attributes in CBC to improve both performance as well as interpretation of the implied results. A comparison between our approaches and other existing techniques shows several advantages for ours. First, the proposed models outperform the predictive ability of the traditional additive approach and the standard SVM for conjoint analysis. The main source of the improvement is their ability to identify relevant attributes at the individual level (and eliminate seemingly ignored attributes). This therefore reduces the number of attributes needed to represent customers’ utility functions, avoiding the “curse of dimensionality” and consequently the risk of overfitting. Second, the proposed models can be extended to non-linear utility specifications by introducing kernel functions that improve predictive performance through the gain in flexibility. And third, analyzing individual part-worths provides important insights into customers’ preferences as revealed by simplifying heuristics that may lead customers to ignore some attributes. In particular, the results of our empirical applications show that consumers may use just one or two attributes to evaluate the alternatives. These relevant attributes, however, differ importantly across customers. Therefore, it is imperative that the feature selection step be performed individually instead of at the population level. Additionally, we confirm some well-known maxims in both marketing and machine learning fields, such as the importance of estimating individual utility functions to characterize customers’ heterogeneity, and the need for low-dimensional models to avoid the curse of dimensionality. Furthermore, we show that attribute selection could replace—to a certain extent—a regularization procedure. Finally, we provide additional empirical evidence of the usefulness of using machine learning techniques such as SVM to analyze conjoint data.Future work can be carried out in several directions. First, it would be interesting to apply this approach to other conjoint applications such as menu-based conjoint. Attribute selection can improve our understanding of the decision rules employed by customers in these more complex situations. These new contexts could even influence customers to ignore attributes in the different stages of the decision process. Second, the proposed approach could be applied to dynamic settings, where an attribute selection procedure could help to generate more parsimonious choice sets and potentially identify customers’ preferences more efficiently. Third, our approach to identifying relevant attributes could be compared to adaptive methods for choice-based conjoint analysis (ACBC). The latter methods usually consider a Build-Your-Own (BYO) configuration, where a so-called screener generates specific (product or service) concepts. Since the analyzed concepts start from self-generated profiles (BYO), it would be interesting to see how the identification of relevant attributes would be affected. Fourth, we consider the study of different shrinkage specifications for our formulation as another interesting direction for future work. The main challenge is to perform feature selection at the individual level by solving a unique optimization problem. This requires introducing important modifications to state-of-the-art shrinkage specifications (Chapelle and Harchaoui, 2005; Evgeniou et al., 2007). Finally, the use of sparsity terms instead of a backward elimination procedure when selecting attributes could be explored. Although this procedure could be more parsimonious, it is expected to affect the efficiency of the proposed approach significantly.Altogether this paper opens interesting research avenues based on the proposed techniques for simultaneous identification of consumer preferences and relevant attributes for the respective choice decisions.