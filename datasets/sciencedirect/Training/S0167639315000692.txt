@&#MAIN-TITLE@&#
Unsupervised and phonologically controlled interpolation of Austrian German language varieties for speech synthesis

@&#HIGHLIGHTS@&#
We present a method for interpolation between language varieties for speech synthesis.The method is unsupervised, based on dynamic-time-warping.Phone sets and formant analysis for four Austrian language varieties are presented.Subjective evaluation shows that listeners can perceive the intermediate variants.Inclusion of switching rules increases the dialect authenticity of the synthesis.

@&#KEYPHRASES@&#
HMM-based speech synthesis,Interpolation,Austrian German,Innervillgraten dialect,Bad Goisern dialect,Viennese dialect,

@&#ABSTRACT@&#
This paper presents an unsupervised method that allows for gradual interpolation between language varieties in statistical parametric speech synthesis using Hidden Semi-Markov Models (HSMMs). We apply dynamic time warping using Kullback–Leibler divergence on two sequences of HSMM states to find adequate interpolation partners. The method operates on state sequences with explicit durations and also on expanded state sequences where each state corresponds to one feature frame. In an intelligibility and dialect rating subjective evaluation of synthesized test sentences, we show that our method can generate intermediate varieties for three Austrian dialects (Viennese, Innervillgraten, Bad Goisern). We also provide an extensive phonetic analysis of the interpolated samples. The analysis includes input-switch rules, which cover historically different phonological developments of the dialects versus the standard language; and phonological processes, which are phonetically motivated, gradual, and common to all varieties. We present an extended method which linearly interpolates phonological processes but uses a step function for input-switch rules. Our evaluation shows that the integration of this kind of phonological knowledge improves dialect authenticity judgment of the synthesized speech, as performed by dialect speakers. Since gradual transitions between varieties are an existing phenomenon, we can use our methods to adapt speech output systems accordingly.

@&#INTRODUCTION@&#
The flexibility of Hidden Semi-Markov Model (HSMM) based speech synthesis allows for different strategies to manipulate the trained models, such as adaptation and interpolation. In this paper we develop, analyze, and evaluate unsupervised interpolation methods that can be used to generate intermediate stages of two language varieties. “Variety” is a cover term void of any positive or negative evaluative connotations. It comprises dialects, sociolects, and standard languages. In this contribution, we apply this method to perform an interpolation between Regional Standard Austrian German (RSAG) and three dialects/sociolects. The difficulty of dialect interpolation lies in lexical, phonological, and phonetic differences between the varieties (Russell et al., 2013). In this contribution we focus on interpolation of phonetic differences.In recent years there have been several research efforts in the context of language varieties for speech synthesis, reviewed in Russell et al. (2013). Following Russell et al. (2013) we can distinguish between fully-resourced and under-resourced modeling as well as different applications like variety interpolation.In fully-resourced modeling, Richmond et al. (2010) described how to generate pronunciation dictionaries based on morphological derivations of known words. They reported that in preliminary experiments for 75% of tested words, their method produced the correct, fully-specified transcription. This can be used as an extension to existing grapheme-to-phoneme rules to obtain contextual information on out-of-vocabulary words and could be beneficial for building an actual dialect synthesis system that includes interpolation.Nguyen et al. (2013) described the development of an HMM-based synthesizer for the modern Hanoi dialect of Northern Vietnamese, describing special challenges they encountered, comparable to our process of acquiring our dialect corpus.In Toman et al. (2013b) we evaluated different acoustic modeling methods for dialect synthesis. The interpolation technique presented in the present work is compatible to all acoustic modeling methods as long as they produce a HSMM state sequence for a given set of labels.For developing synthesizers for under-resourced languages, different methods have been developed to aid the process of data acquisition and annotation.Goel et al. (2010) evaluated the combination of different lexicon learning techniques with a smaller lexicon available for bootstrapping. In their experiments, their method could increase the Word Recognition Accuracy from 41.38% for a small bootstrap lexicon to 43.25%, compared to 44.35% when using the full training dictionary.Watts et al. (2013) developed methods and tools for (semi-)automatic data selection and front-end construction for different languages, varieties and speaking styles e.g. from audio books. Results from Watts et al. (2013) are published by Stan et al. (2013) who applied these tools on “found speech” to create a standardized multilingual corpus. For our work on dialectal synthesis, such methods are useful for easy acquisition and annotation of dialect data, which is currently a time-consuming process.Loots and Niesler (2011) developed a phoneme-to-phoneme conversion technique that uses decision trees to automatically convert pronunciations between American, British and South African English accents.1The term “accent” is often used for regional differences of English. We avoid the term “accent” in this contribution as it refers to more than one linguistic phenomenon and we specifically treat dialects here.1This method could be used to automatically generate the phonetic transcription for less-resourced dialects from a fully-resourced variety, as a transcription of the dialect utterance is required for our interpolation technique presented here.Voice model interpolation was first applied in HSMM-based synthesis for speaker interpolation (Yoshimura et al., 2000) and emotional speech synthesis (Tachibana et al., 2005). Picart et al. (2011) used model interpolation to create speech with different levels of articulation. Lecumberri et al. (2014) considered the possibility of using extrapolation techniques to emphasize foreign accent as an application for foreign language learning. The methods presented here could also be used to produce an extrapolated dialect, but this is not investigated in the current paper.In language variety interpolation, Astrinaki et al. (2013) have shown how to interpolate between clusters of accented English speech within a reactive HMM-based synthesis system. In this method, phonetic differences between the accent representations were not considered (i.e. the same set of phone symbols and utterance transcriptions was used for all accents).In Pucher et al. (2010), we have shown how to interpolate between phonetically different dialects in a supervised way. In this method, we used a manually defined phone-mapping between Standard Austrian German and the Viennese dialect. Evaluation tests showed that listeners actually perceive the intermediate varieties created by interpolation as such.In this contribution we extend the method from Pucher et al. (2010) to work in an unsupervised way, such that no manually defined mapping is necessary, therefore allowing the fully automatic interpolation. Also, interpolation is performed between RSAG and three dialects/sociolects. This unsupervised method is based on Dynamic Time Warping (DTW) (Rabiner et al., 1978) on HSMM state level and is subsequently described in Section 3. Compared to Pucher et al. (2010), this method introduces one-to-many mappings between states, requiring a more sophisticated duration modeling procedure, which will be described in Section 4.To introduce the integration of phonological knowledge in the interpolation technique, we describe the following alternations, which characterize the RSAG – dialect interaction2// denotes the phonological representation, [] the phonetic realization.2:1.Phonological process: Socio-phonological studies on Austrian varieties demonstrate that certain alternations between two varieties, usually a standard variety and a dialect, are phonetically well motivated and thus can be described as phonological processes, e.g., spirantization of intervocalic lenis stops (Moosmüller, 1991) like•[ɑːb̥ɐ] to [ɑːbɐ] to [ɑːβɐ]aber (engl. “but”) or•[laɛd̥ɐ] to [laɛdɐ] to [laɛðɐ]leider (engl. “unfortunately”).Interpolation can be used to model these gradual transitions.Input-switch rules: Other alternations lack such phonetic motivations because of a different historical development. These alternations are therefore described as input-switch rules, e.g.•/ɡ̥uːt/↔/ɡ̥uɑ̯d̥/ or•/ɡ̥uːt/↔/ɡ̥uːi̯d̥/gut (engl. “good”).No gradual transitions from e.g., /ɡ̥uːt/ to /ɡuɑ̯d̥/ can be observed (Dressler and Wodak, 1982; Moosmüller, 1991). Because of their phonetic saliency, input-switch rules are sociolinguistic markers as defined by Labov (1972), meaning they are subjected to stereotyping and social evaluation (positive or negative). Therefore, interpolation is not feasible in these cases.Pseudo-phonological process: Many input-switch rules involve diphthongs vs. monophthongs; i.e. the standard form is a diphthong, the dialect form is a monophthong. Standard Austrian German features a vast variety of phonetic diphthongal realizations (Moosmüller et al., 2015), so that any (slight) movement in formant frequencies is interpreted as a diphthong (Moosmüller and Vollmann, 2001). Sociolinguistically, the input-switch rule persists; the diphthong is the standard form, the monophthong is the dialect form, such as in the following examples:•/haɛ̯s/↔/hɑːs/heiß (engl. “hot”) or•/kɑɔ̯fsd̥/↔/kɑːfsd̥/kaufst (engl. “(you) buy”)However, the gradual decrease in formant frequency movement can be elegantly captured by interpolation, without attracting negative evaluation from the listener’s part. Consequently, modeling this case using HSMM interpolation is feasible although the alternation is actually an input-switch rule.When input-switch rules are considered, it is not phonetically feasible to interpolate whole utterances. Therefore, we introduce region-based interpolation. This introduces another level of mappings on regions spanning multiple phones. These regions can then be defined as either (pseudo-)phonological process or input-switch rule. For example, the words Ziege (RSAG) vs. Goaß (dialect; engl. “goat”) might form mapped regions that should not be interpolated. This procedure is described in detail in Section 6.The developed interpolation methods have possible applications in spoken dialog systems where we aim to adapt speech output to the user of the dialog system. As soon as the dialect/sociolect of the user is detected, we can use interpolation to create a dialog system persona that fits the dialect/sociolect spoken by the user. In Toman et al. (2013a), we presented a method for cross-variety speaker transformation based on HSMM state mapping (Wu et al., 2009). Transforming the voice of a speaker from one variety to another can be used as a basis for dialect interpolation. For example, a single voice model could be transformed to multiple other varieties and then interpolation can be used to synthesize samples for intermediate stages, enabling a large spectrum of speaking styles. Furthermore, interpolation methods could also be used to extend existing multi-variety speech databases or speech databases with similar languages by augmenting them with interpolated data. In general, our methods can be applied to any interpolation of state sequences of HMM models, which makes it also applicable for facial animation (Schabus et al., 2014).Our HSMM-based synthesizer is an extension of the HSMM-based speech synthesis system published by the EMIME project (Yamagishi and Watts, 2010). The methods for training these kinds of synthesizers and synthesizing from HSMMs were published in a number of papers (Zen et al., 2009; Tokuda et al., 1995; Yoshimura et al., 1999; Yamagishi and Kobayashi, 2007; Tokuda et al., 1999).This contribution is organized as follows: Section 2 describes the corpora and associated phone sets which were used in this work. Section 3 then presents the details of the interpolation methods used to generate intermediate language varieties. Duration modeling in these methods is described in Section 4. Section 5 presents a phonetic analysis of interpolated samples. Rules derived from these results are then incorporated in an extended interpolation method, described in Section 6. Section 7 describes the evaluations we conducted to assess and compare the presented methods. Finally Section 8 discusses results and concludes the work.The work presented here is based on a corpus consisting of three Austrian German dialects: the dialect of Innervillgraten (IVG) in Eastern Tyrol, the dialect of Bad Goisern (GOI) in the South of Upper Austria, and the dialect of Vienna (VD). IVG belongs to the South Bavarian dialect group, VD to the Middle Bavarian dialect group, and GOI belongs to the (South)-Middle Bavarian dialect group.SAG refers to the variety spoken by the upper social classes of the big cultural centers located predominantly in the Middle Bavarian region (Moosmüller, 1991, 2015; Soukup and Moosmüller, 2011). Since the IVG and GOI speakers were genuine dialect speakers, meaning that they were raised in the respective dialect and learned SAG only in school, SAG spoken by these speakers contained also regional features. Therefore, the SAG variety produced by the GOI and IVG speakers is referred to as regional standard Austrian German (RSAG). In Table 1, the difference between SAG, RSAG as spoken in Bad Goisern, and GOI is illustrated.In RSAG, /ɛ/ of Schwester is slightly diphthongized, a process which is not allowed in SAG. Also, the diphthong in meine differs between the two varieties. These two features cue a regional variant of SAG which, in turn, is still completely different from the dialect version of the model sentence.Ten dialect speakers, gender balanced, were recruited for the GOI and the IVG corpus, respectively. The recordings consisted of spontaneous speech, reading tasks, picture naming tasks, and translation tasks from SAG into the dialect. From these recordings, 660 phonetically balanced sentences were selected and a phone set was created for each dialect. For the recording of the 660 phonetically balanced dialect sentences the dialect speakers heard the dialect speech sample they were asked to utter and were also presented with an orthographic transcription of the sentence which was close to the standard language. In addition, these speakers also read a corpus of SAG sentences. The speaker selection and recording process for IVG and GOI has been described in detail in Toman et al. (2013b).The corpus of the VD speakers is different, in as for the synthesis of the Viennese dialects and sociolects, actors and actresses were recruited. 10 actors and actresses were invited for a casting in which they had to perform reading tasks in both SAG and VD. For the VD samples, they had to transform SAG sentences into the VD. Subsequently, the recordings were subjected to analysis and the speaker who performed best was chosen for the VD dialect recording sessions. The speaker selection and recording process for VD has been described in detail in Pucher et al. (2012).Sound samples were recorded at 44,100Hz, 16bits/sample. The training process was also performed using these specifications. Cutting and selection was performed manually. Noise cancellation and volume normalization was applied to the recordings. Synthesized samples used in the evaluation were also volume normalized. A 5ms frame shift was used for the extraction of 40-dimensional mel-cepstral features, fundamental frequency and 25-dimensional band-limited aperiodicity (Kawahara et al., 1999) measures. Speaker-dependent models were trained for the evaluations using the HSMM-based speech synthesis system published by the EMIME project (Yamagishi and Watts, 2010). The interpolation methods presented in this contribution were integrated into this system.Table 2shows a sample of the utterances which were used for the evaluation, and which are also linguistically and phonetically analyzed in Section 5. We interpolate between Regional Standard Austrian German (RSAG) and one dialect variety (Innervillgraten dialect (IVG), Bad Goisern dialect (GOI) or Viennese dialect (VD)). In total, we use 6 different utterances per variety. There are significant phonetic and lexical differences between RSAG and the respective dialect (IVG, GOI, VD). These differences lead to different numbers of phones and different numbers of lexical items between RSAG and dialects (e.g. RSAG “d̥iː ’taːɡɛ – die Tage” vs. GOI “d̥ːɔːk – die Tage”3In GOI, the definite article “die” is reduced to [d̥] and subsequently merged with the initial [d̥] of “Tage”.3occurred in the evaluated samples).Table 3shows the phone sets for the different varieties. Affricates were split into two phones in (R)SAG and VD as these were defined in a previous project. For alveolar stops, a further category had to be introduced in order to capture instances which can neither be assigned to [t] nor to [d̥]. Since consonant duration is the decisive feature in differentiating stops in Austrian dialects, we symbolize this additional category as [d̥ː] (see Moosmüller and Brandstätter (2014) for a discussion).This section describes the interpolation methods used to generate intermediate language varieties.In Pucher et al. (2010) we implemented and evaluated a supervised interpolation method that allows for gradual interpolation between language varieties when a phoneme mapping is given. Here we extend this method by an unsupervised interpolation that is based on Dynamic Time Warping (DTW) (Rabiner et al., 1978; Müller, 2007) of HSMMs. This method implements gradual interpolation between varieties without a phoneme mapping.To obtain the DTW warping path, we use the Kullback–Leibler Divergence (KLD) between the mel-cepstral models of the HSMM as a distance metric. Since the mel-cepstral parameters are modeled by k-dimensional multivariate Gaussian distributions and not Gaussian mixture models, we can use the analytic form of KLD (Hershey et al., 2007). By using a symmetric version of KLD, we ensure that the whole interpolation is also symmetric (Müller, 2007).For each mapping along the warping path, an HSMM state is generated by interpolating the acoustic features and durations of the mapped HSMM states. This sequence of states is then used to synthesize the interpolated utterance. Fig. 1shows an actual DTW alignment for the beginning of a sentence. We see the optimal warping path that closely follows the diagonal. Mel-cepstral,F0and aperiodicity features are linearly interpolated between the HSMM state models mapped by the warping path and according to an interpolation parameterα. The interpolation for acoustic features is described in Yoshimura et al. (2000). Horizontal and vertical parts of the warping path indicate one-to-many mappings between states. These type of mappings have to be treated carefully to assure that the total duration of the segment is correct. Our algorithm to handle these cases is described in detail in Section 4.It should be noted that because the final HSMM is constructed from the DTW warping path, we achieve an approximate interpolation where the endpoints (α=0.0andα=1.0) are not exactly the original models of RSAG and dialect (GOI, IVG, VD).Using DTW it is possible to obtain different warping paths in terms of state alignment for unexpanded states where each state has a duration and expanded states where the states are expanded according to the duration of the state. For example, an unexpanded state with duration n would be expanded to n states with duration 1 (the number of feature frames generated from this model). An illustration for this can be seen in Fig. 2where a 5 state HSMM is shown in unexpanded and expanded form.An example for the effect of this on the DTW result can be seen in Tables 4 and 5. In this example we usecost(b,d)=1<cost(b,c)=2<cost(a,c)=3<cost(a,d)=4as a cost function for DTW. With the unexpanded method we can achieve only the mappings{a↔c,b↔d}, while the expanded method gives us{a↔c,b↔c,b↔d}which shows the greater flexibility of the expanded method.In order to evaluate whether the behavior shown in Table 4 and 5 is actually present in the interpolations of empirical speech samples, we analyzed the alignments of all interpolations of the data set used for our evaluation which we present in Section 7. Fig. 3shows that there is a significant amount of additional unique mappings with the expanded alignment method. There are 3656 mappings between expanded states which do not exist when the unexpanded method is used (e.g. the illustrative mappingb↔cin Table 5).This chapter describes how the durations for the final HSMM, which is constructed from the DTW warping path, are calculated.For a given utterance, a HSMM state sequence is constructed for each variety involved in the interpolation. HSMMs are retrieved from the voice models by classifying the utterance labels using the voice model decision trees as described by Zen et al. (2009). DTW is then used to calculate an optimal state mapping of the two HSMM state sequences. The result of DTW might contain one-to-many mappings. This is always true when the number of states is different for the two sequences. We have to handle these cases so that the total duration of the final HSMM state sequence is also the result of a linear interpolation between the total durations of the individual sequences with respect toα.For the standard interpolation (Yoshimura et al., 2000) between random variables, we can use the fact that a linear combination of normally distributed random variables is again normally distributed with meanaμ1+bμ2and variance a2σ2+b2σ2.However, for one-to-many mappings we have to interpolate one random variable with multiple other random variables, resulting in a non-linear combination. Consider a mapping of two random variablesX,Ywith one random variable Z. To interpolate betweenX,Yand Z, we define the resulting random variables U and V as shown in Eqs. (1) and (2).(1)U=(αX+αY+(1-α)Z)XX+Y(2)V=(αX+αY+(1-α)Z)YX+YU and V then model the durations of the two resulting states of the interpolated HSMM sequence. Fig. 4shows a graphical interpretation of the two Eqs. (1) and (2). In the case of an interpolation value of1.0,U=XandV=Y. In the case of an interpolation value of 0.0, the value of Z is distributed on U and V according to the relative values of X and Y. In other words, the durationX+Yis interpolated with duration Z according toα. The resulting duration is then distributed to U and V according to the ratio of X and Y.To obtain the distribution for Eqs. (1) and (2) in general, we would have to take into account that the product of normally distributed random variables is not normally distributed (Springer and Thompson, 1970) and that the reciprocal of a normally distributed random variable is also not normally distributed.However, in the synthesis system used here (Zen et al., 2009), the mean values of the duration random variables are used as the actual state durations, as long as no modification of the speaking rate using variance scaling is desired (Valentini-Botinhao et al., 2014). So for the implementation of the interpolation algorithm, we can apply Eqs. (1) and (2) directly on the final duration valuesdi(i.e. the means) of the two utterances that we want to interpolate. Durationdithen specifies the number of feature frames that are generated for state i in the DTW path. This means that for every mapping along the DTW path, a single HSMM state with durationdiis generated.For both methods (expanded and unexpanded), we compute the interpolated duration value of the i-th state in the alignmentdas shown in Eq. (3). Here〈d1,1,…,d1,m〉and〈d2,1,…,d2,n〉are the mean duration sequences involved in the interpolation. For a one-to-one mapping this results inm=n=1, reducing the formula to standard interpolation. For one-to-many interpolation, we get eitherm=1,n>1orm>1,n=1.(3)di=∑j=1md1,jα+∑k=1nd2,k(1-α)d1,i∑jd1,jd2,i∑kd2,k.Fig. 5illustrates the duration interpolation for a representative case of unexpanded and expanded states with interpolation weightα=0.5. For the unexpanded example (left), the mapped sequences that we derive from the DTW alignment are shown in Eqs. (4) and (5). According to the first part of Eq. (3), the total duration ofd1andd2is given byd1,1α+(d2,1+d2,2)(1-α)=(d1+d2)=6.1. The second part of Eq. (3) then distributes the total duration tod1andd2according to the relation betweend2,1andd2,2, resulting ind1=1.5andd2=4.6.(4)〈d2,1〉=〈3.4〉(5)〈d2,1,d2,2〉=〈2.1,6.7〉The example on the right side of Fig. 5 shows an unexpanded case. Here, the interpolated total duration (1.5) is distributed uniformly to the two warping path statesd1=0.75andd2=0.75as all mapped state durations are 1.0.Both methods might produce states with durations smaller than 1. To cope with this, we accumulate the durations and skip states according to Algorithm 1. The algorithm loops the final HSMM state sequence. The duration of each state (as calculated before) is accumulated in accdur. Ifaccdur<1then the current state is skipped. Else the state is added to the final model and its duration subtracted from accdur.Algorithm 1Algorithm for skipping states.1:accdur←02: for allduration,statein HSMM state sequence do3:accdur←accdur+duration4:ifaccdur⩾1then5:accdur←accdur--duration6: add(state)7:end if8: end forThe final HSMM state sequence is then used as input for the parameter and waveform generation (Zen et al., 2009). Unsupervised interpolation allows us to generate intermediate variants of utterances for any given utterance pair. In addition, it is also possible to deal with missing words. In terms of linguistic correctness we might produce utterance variants that are wrong in the sense that such intermediate variants do not exist or that co-occurrence4Co-occurence requirements refer to the fact that within an utterance, it is not allowed to arbitrarily mix standard and dialect forms (Scheutz, 1999).4requirements are not met.We applied the previously presented interpolation methods to the sample utterances and dialects as described in Section 2. Here we present an analysis of the input-switch rules and of the processes involved in the interpolation from the (R)SAG input to the dialect output. While all utterances used in the evaluation have been analyzed to extract the necessary information for the extended method presented in Section 6, here we only present one sample utterance per variety. Subsequently, narrow phonetic transcriptions of the diverse interpolated steps from (R)SAG to the respective dialect (IVG; GOI; VD) are shown. 0.0 denotes the (R)SAG synthesis as derived/synthesized from the Standard corpus produced by the respective IVG, GOI, or VD speaker, 1.0 denotes the dialect synthesis. 0.2, 0.4, 0.6, 0.8 are the intermediate forms created by interpolation.The example sentence analyzed in this section is shown in Table 6.•/ʃneː/↔/ʃnea̯/: A prominent South Bavarian characteristic is the diphthongization of Middle High German (MHG) <ê> in e.g., Schnee, (engl. “snow”). Consequently, in IVG, we find [ʃnea̯] for RSAG [ʃneː]. Since this is a historical process, the alternation from /ʃneː/↔/ʃnea̯/ cannot be captured as a diphthongization process, but has to be described as an input-switch rule; speakers can realize either [ʃneː] or [ʃnea̯], but no in-between forms are allowed.<-gt>↔<-t>: In liegt, a similar case is at hand. In IVG and in this area of Eastern Tyrol, the final consonant clusters <-gt> are dissolved (Kranzmayer, 1956). These consonant clusters evolved in the course of contraction processes from Old High German (OHG) ligit to New High German (NHG) liegt and is already described for MHG. From a synchronic perspective, however, we find either [liːt] which is indicative of a dialectal pronunciation, or [liː̥ɡ̥t] which indexes RSAG./ɪ, ʊ/5The vowel inventory of RSAG comprises 13 vowels: /i, ɪ, y, ʏ, e, ɛ, ø, œ, u, ʊ, o, ɔ, ɑ/5↔/i, u/6The vowel inventory of the Bavarian dialects comprises 7 vowels /i, e, ɛ, u, o, ɔ, ɑ/. As a result of the Viennese monophthongization, two further long vowels have been added for the VD /æː, ɒː/.6: Phoneme inventories of the Bavarian dialects contain no high lax vowels, therefore, RSAG /ɪm/ alternates with IVG /in/.7In the Bavarian dialects, dative and accusative often collapse, therefore in IVG it reads in Garten or in Haus, while in SAG it is im Garten or im Haus (engl. “in the garden”, “in the house”).7In the Bavarian dialects, [ɪ, ʊ] might occur as a result of the reduction processes, therefore, on the phonetic level, both tense and lax vowels turn up. Moreover, even in SAG, the distance between high tense and high lax vowels is rather small (Brandstätter and Moosmüller, 2015; Brandstätter et al., 2015), consequently, this input-switch rule is easily feasible in interpolation./ɑ/↔/ɔ/: In many cases, [ɔ(ː)] is used instead of SAG [ɑ(ː)]. Consequently, the vowel /ɑ/ in SAG Garten (engl. “garden”) is realized as [ɔ] in IVG.<-e>: A further input-switch rule affects the suffix <-en> in Garten. In SAG, this suffix is either fully pronounced, resulting in [ɛn], or, in most cases, the vowel is deleted and the remaining nasal consonant is syllabified, resulting in [n̩]. In IVG, however, MHG morphological traces are still apparent: OHG garto changed to MHG garte. This form is still preserved in IVG.Nasal place assimilation is a phonological process present both in RSAG and in the dialects. In IVG, nasal place assimilation of /n/→[ŋ] takes place in front of the velar plosive [ɡ̥] of Garten. However, nasal place assimilation is not applied in RSAG, because the bilabial nasal consonant /m/ is not subjected to place assimilation in the Austrian varieties. Therefore, a difference between IVG and RSAG is present at a higher, syntactic level which affects the phonological level.In RSAG Garten, /ʀ/ is vocalized. Vocalization is a phonological process which is applied in all Middle Bavarian dialects. Preceding consonants or word-final, /ʀ/ is vocalized to [ɐ], following the vowel [ɑ], the result of the vocalization, [ɐ], is absorbed and compensatory lengthening of [ɑ] takes place. Therefore, the sequence /ɑʀ/ is pronounced [ɑː] in SAG. Vocalization of /ʀ/ does not hold for the South Bavarian dialects. Since /ʀ/ is generally not pronounced as a trill, but rather as a fricative, mostly unvoiced, the sequence /ɔʀ/ is pronounced as [ɔχ] (see also Hornung and Roitinger (2000)).The analysis of the interpolation output is shown in Table 7and described subsequently.Although phonologically, /eː/ to /ea̯/ in Schnee is modeled as an input-switch rule, because only either /eː/ or /ea̯/ are possible output forms, this input-switch rule can easily be captured as a gradual process of diphthongization in interpolation, as it can be observed in Fig. 6. The first change affects the offset of the vowel. The last 30% of the vowel are marked by a lowering of F2 and F3 in step 0.2. In 0.4, the lowering of F2 and F3 starts even earlier, so that a slight diphthongization [eɛ̯] is perceivable. From now on, F3 stays stable. In 0.6, F2 is lowered in the first part of the vowel, yielding a change in vowel quality from [e] to [ɛ] in the first part. This change is further enhanced in step 0.8 and the final step 1.0 by a substantial raise in F1. At the same time, F2 of the third half of the vowel experiences a further lowering plus a gradual and substantial raise in F1, yielding the vowel qualities [æ] and, finally, [a̯]. Tentatively, it can be concluded that after step 0.4, changes are such that they proceed into the direction of the dialect pronunciation.This assumption is strongly supported by the analysis of the input-switch rule /ɑ/↔/ɔ/ in Garten. Fig. 6 shows that F2 trajectories of step 0.0, 0.2, and 0.4 (mean=1262Hz) are clearly set apart from the trajectories of steps 0.6, 0.8, and 1.0 (mean=963Hz). The difference in F1 is not as straightforward, and the changes from step 0.0 to 1.0 appear more gradual, nevertheless F1 of step 0.0, 0.2, and 0.4 occupies the higher frequency range (mean=680Hz), whereas F1 of steps 0.6, 0.8, and 1.0 is located in the lower frequency range (mean=534Hz). Consequently, the vowel clearly has the quality [ɑ] in steps 0.0, 0.2, and 0.4, whereas a quality change from [ɑ] to [ɒ] starts in step 0.6, which is finally changed to [ɔ] in steps 0.8 and 1.0. Voicing of the velar stop /ɡ̥/ of Garten starts in 0.2. Voicing of stops preceding nasal consonants is a general process in all varieties of Austrian German, though restricted to word-internal positions in SAG.In SAG and in the Middle Bavarian varieties in general, /ʀ/ is vocalized. This is not the case in the South Bavarian varieties. Therefore, the IVG sequence /ɔʀ/ is pronounced [ɔχ]. In interpolation from [ɑː] to [ɔχ], [χ] has to be inserted. Insertion starts in step 0.4. First, the final part of the long vowel [ɑː] is fricatized, the fricative [] is still short and voiced, and therefore, the vowel is only slightly shortened to [ɑˑ]. In step 0.6, the fricative is devoiced, simultaneously, fortition of [d̥] takes place. The suffix <-en>, which reads <-e> in IVG, still needs to be changed. In 0.6, [ɛ] is inserted, leading to the sequence [ˈɡɒχtɛn], however, this output sequence of Garten violates co-occurrence restrictions, since the nasal consonant is still fully pronounced. Deletion of the nasal only starts in 0.8, together with the full pronunciation of the fricative [χ]. Nasal assimilation of /n/→[ŋ] is accomplished in 0.8.The critical steps in the derivation are to be found in 0.4 and in 0.6. 0.6 is affected the most. Whilst 0.4 might still be evaluated as SAG, 0.6 can neither be assigned to SAG nor to IVG, either because the deviation of the intermediate steps is too large from both SAG and IVG, as, e.g., in Schnee [ˈʃnɛӕ̯], or because co-occurrence restrictions are violated, as in Garten [ˈɡɒχtɛn]. Nasal assimilation of IVG /n/→[ŋ] and the deletion of the final nasal consonant in Garten should start one step earlier.The example sentence analyzed in this section is shown in Table 8.•/kɔmt/↔/kimd̥/ kommt: In the dialects, the root vowel of the verb kommen (engl. “to come”) is either /e, i/ or /u/. In GOI, the first variant is used, leading to the following inflections: /kim, kimd̥, kimsd̥, kemɑn, kemd̥s, kemɑnd̥/.[maɛ̯nɛ]↔[mãẽ̯] meine: Final nasals have been deleted in Bavarian dialects and the preceding vowel/diphthong has been nasalized. Therefore, we find [mãẽ̯] in GOI, opposed to RSAG [maɛ̯nɛ]. Again, historically, these are phonological processes, yet, in the synchronic view, no intermediate steps can be observed./ɛ/↔/e/ in Schwester: The pronunciation of the e-vowels is often reversed. Therefore, we find /ɛ/ in SAG, but /e/ in GOI./p, t/↔/b̥, d̥/: Front plosives are neutralized in the Bavarian dialects, as, e.g., in GOI [ˈʃvesd̥ɐ].In morgen (engl. “tomorrow”), different phonological inputs have to be assumed. However, phonological processes are quite similar. In SAG, /ˈmɔʀɡɛn/ is assumed. /ʀ/ is vocalized to [ɐ̯], resulting in [ˈmɔɐ̯ɡŋ̩̩] in (R)SAG. The deletion of unstressed [ɛ] with subsequent nasal place assimilation finally results in [ˈmɔɐ̯ɡn] or even [ˈmɔɐ̯ŋ̩].For GOI, /ˈmɔʀiɡ̥ɛn/ has to be assumed, since the insertion of an epenthetic vowel which splits the clusters /ʀ/ plus velar or labial obstruents goes back to the 13th century and is retained in some persistent dialects (Kranzmayer, 1956). In the same way as in SAG, deletion of the unstressed vowel [ɛ] with subsequent nasal place assimilation takes place. Subsequently and unlike SAG, the velar plosive is deleted, leading to the output [ˈɔʀɪŋ].8Deletion of the stop takes place in all dialects, in VD, e.g., the phonetic output would read [ˈmɔɐ̯ŋ].8Vocalization of /ʀ/ is applied in Schwester (engl. “sister”) in both SAG and GOI.The analysis of the interpolation output is shown in Table 9and described subsequently.As outlined in Section 5.2.2, for morgen (engl. “tomorrow”), two different forms have to be assumed phonologically. However, the processes involved can be easily captured by interpolation. As a first step, (R)SAG [ɔɐ̯] has to be monophthongized to [ɔ]. Monophthongization only affects the offset of the diphthong, since r-vocalization needs to be undone. From step 0.0 to 0.4 a gradual lowering of F2 at the offset of [ɔɐ̯] is visible in Fig. 7. In step 0.6, monophthongization is accomplished with respect to F2. A slight movement of F1, equal to step 0.4, is still distinguishable. Also, F3 of steps 0.4 and 0.6 lies inbetween the RSAG and the dialectal form. Step 0.4 is the most crucial one; a slight diphthong is still audible, but simultaneously, the velar stop has been changed to a fricative and the vowel [ɪ] has been inserted, which changes its quality to [i] in step 0.8.The input-switch rule /ɔ/↔/i/ in kommt (engl. “come”, 3rd sg.), involves dramatic changes which predominantly affect F2, since a relatively low F2 for the back vowel [ɔ] has to be changed to a high F2 for the front vowel [i] (see Fig. 7). However, F1 and F3 are affected as well: F1 has to be lowered and F3 has to be raised in order to attain [i]. Steps 0.0 and 0.2 are quite similar with respect to formant trajectories, yet, the auditive impression changes a bit, maybe due to a decrease in duration.Steps 0.4 and 0.6 are severely impaired; in step 0.4, F2 could not be fully detected, and the auditive result is not reliably assignable to any vowel, apart from its being heavily nasalized. The transcription offered has therefore to be understood as a compromise. F1 of step 0.4 is rather low (mean=282Hz), thus suggesting [i]. F3, however, is rather low as well (mean=2536Hz), thus indicating [ɔ]. In step 0.6, F3 could not fully be detected, however, it is rather high (mean=2718Hz) and suggests to proceed in the same way as steps 0.8 and 1.0. Therefore, F3 indicates [i]-quality, along with a high F2 and a low F1. The auditive result is clearly [i]. The quality of the vowel [i] is fully accomplished in steps 0.8 and 1.0.The diphthongization of Schwester (engl. “sister”) is smoothly captured by interpolation (see Fig. 8). As discussed in Section 2, the speaker realizes a rather regional variety of SAG which demands [ɛ]. Our speaker, however, starts off with a slightly diphthongized vowel [e̝e]. The first part, which is responsible for the diphthongization, takes up the same time span over all interpolation steps, however, from step to step, diphthongization becomes more pronounced. F1 and F3 play no role, most probably, because the starting point already strongly resembles the endpoint.In the Bavarian dialects, unstressed syllables are generally not reduced to a schwa-like vowel, but the full vowel quality is preserved. This is obvious in the final syllable <-er> of Schwester (engl. “sister”). In interpolation, this process is observable by the changes of F1 which is gradually raised (see Fig. 8). Raising of F1 starts at 0.4 and affects approximately the first third of the vowel. It stays constant in step 0.6, but both step 0.4 and 0.6 already give the auditive impression of a full vowel. In step 0.8 and 1.0, the raise of F1 affects most of the vowel, thus indicating the full vowel quality.The example sentence analyzed in this section is shown in Table 10.•/viʀ/↔/miʀ/: Alternation of the initial consonant in wir: /viʀ/↔/miʀ/. The bilabial nasal consonant is used in all Bavarian dialects of Austria./sɪnd̥/↔/sɑn/: The alternations of the verb sein (engl. “to be”) are manifold, in the VD, we find mostly /sɑn/ for the plural./ʊ/↔/u/: As has already been described for IVG, phoneme inventories of the Bavarian dialects contain no high, lax vowels. Consequently, in lustige (engl. “funny”), VD /u/ is opposed to SAG /ʊ/./ɔɛ̯/↔/æː/: The vowel inventory of the Bavarian dialects contains no front, labial vowels. This holds also for the diphthong /ɔɛ̯/ which is delabialized to /aɛ̯/. Therefore, in the Bavarian dialects, we find [laɛ̯d̥] with different degrees of diphthongization./aɛ̯, ɑɔ̯/↔/æː, ɒː/: In VD, the diphthongs /aɛ̯/ and /ɑɔ̯/ have been monophthongized to /æː/ and /ɒː/, respectively. Consequently, in the VD, the phonetic output for Leute is [ɫæːd̥].For both the Bavarian dialects and SAG, r-vocalization applies in /viʀ/ and /miʀ/, resulting in [viɐ̯] and [miɑ̯] or [miɐ̯], respectively.In lustige (engl. “funny”), spirantization of the intervocalic velar plosive takes place in VD, resulting in [ˈlusd̥iɣɛ]. This process might even occur in SAG, especially in spontaneous speech and in high frequency words.The analysis of the interpolation output is shown in Table 11and described subsequently.The input switch rule /ɪ/↔/a/ in sind (engl. “are”) involves a similar dramatic change as has been described for the input-switch rule /ɔ/↔/i/ in GOI, with the difference that now a front vowel has to be transformed into a back vowel.9/a/ is a back vowel with respect to the location of constriction (pharyngeal) and a front vowel with respect to the highest point of the tongue (Fant, 1965).9The changes to be performed from /ɪ/ to /a/ involve a substantial lowering of F2, a lowering of F3, and a substantial raise of F1 (see Fig. 9). The outputs of steps 0.0 and 0.2 contain no changes, the first change starts at step 0.4 which reveals a rather weak, but substantially lowered F2. The quality of [ɪ] is still preserved in F1 and F3, consequently, the auditive impression resembles a heavily nasalized [Ĩ]. In step 0.6, F3 is lowered as well, but F1 has not changed yet. This enhances the a-quality, which is transcribed as [ɐ̃]. It is only in step 0.8 that a substantial part of F1 is raised, thus producing the quality of a full vowel [ã] which is still nasalized due to the low F1 in the final third of the vowel. Finally, in step 1.0, the whole vowel shows a raised F1.As outlined in Section 5.3.1, the SAG diphthong /ɔɛ̯/ of Leute (engl. “people”), realized/synthesized as [ɒɛ̯] in 0.0, does not belong to the phoneme inventory of the Bavarian dialects. In the Bavarian dialects, the first part of the diphthong is delabialized, resulting – depending on the dialect – in [aɪ̯], [aɛ̯], or [ae̯]. In the Viennese dialect, the diphthong is additionally monophthongized, resulting in [æː]. Derounding starts in step 0.4, by a slight raise of F2 and F3. A pronounced diphthongal movement is still evident at step 0.4. The de-rounded diphthong already indicates a dialectal pronunciation, however, some co-occurrence restrictions are violated. First of all, the final [ɛ] of Leute is not yet fully deleted. Then, the lateral is already velarized. The velarized variant of the lateral is restricted to the area of Vienna, therefore, it has to co-occur with a monophthongized [æː]. In step 0.6, the final vowel [ɛ] has been deleted, but a diphthongal movement is still present in the diphthong which needs to be monophthongized, although F2 of the first part of the diphthong has been substantially raised. In steps 0.8 and 1.0, monophthongization is accomplished, the slight raise in F2 in the first 15 frames is due to the transition from the velarized lateral into the vowel.Based on the results of the phonetic analysis in Section 5, we extended our interpolation algorithm to handle input-switch rules for those parts the of the utterances for which interpolation is not phonetically feasible.To incorporate input-switch rules, we add meta information to each pair of utterances(A,B)to be interpolated. For utterance A and B, we define a set of regionsR(A)andR(B)on a phone level. Every regiona∈R(A)andb∈R(B)has to consist of at least one phone and can, at maximum, span the whole utterance. Also, a region must not necessarily consist only of consecutive phones (i.e. the region can be split up across the region) but the ordering of the phones must be preserved. The regions have to be selected so that a bijectionM:R(A)↦R(B)can be defined. This means, every utterance is split into regions and these regions are then mapped between the utterances. For every region mappingm=(a,M(a))∀a∈R(A), a procedure to be applied during the interpolation process can be defined. For our experiments, we set the procedure for every mapping in the evaluation data to either “feature interpolation” or “feature switch”. Feature interpolation is used in case of a phonological process or a pseudo-phonological process as described in Section 1. We use the feature switch procedure in case of an input-switch rule. Both procedures are described in Section 3. If each utterance forms a single region and the mapping between these two regions is associated with the feature interpolation procedure, we get the basic interpolation method as described previously. To summarize, regions define which procedure from Section 1 should be used for this part of the utterance.For our experiments, we defined the mappings M and the associated procedures according to the results of our phonetic analysis as follows: From the evaluation data, extract a list of word mappings with an input-switch rule that cannot be modeled by interpolation. If such a word mapping occurs in a sentence, compare the prefix and suffix of the words on a phonological level. If the phone symbols are the same, a region for feature interpolation can be formed. This is useful as the acoustic realization of the phonetic symbols will still differ slightly for all dialects. The remaining, differing phones form a region that will use the feature switch procedure. Finally, merge regions next to each other that have the same procedure assigned to them. A more sophisticated algorithm could involve DTW on the phonological level, combined with a list of phone level mappings that should be realized using a feature switch procedure. For mapping on the word level, machine translation methods (Neubarth et al., 2013) could be employed.An example for a defined region mapping and procedures can be seen in Fig. 10. As described previously, /viɐ̯/ and /miɐ̯/ are connected by an input-switch rule. As the suffix /iɐ/ is the same in both utterances, it can form the feature interpolation region 2. /v/ and /m/, on the other hand, form the feature switch region 1. It can also be seen that region 2 is merged with the /s/ from the following word, because this is again a word beginning with the same phones and also uses feature interpolation.The functionIlinearapplies DTW on the HSMM states and linearly interpolates the associated features as well as the durations as described in Section 3 and returns the newly generated states.The functionIswitchdoes a feature switch for given HMM states and is shown by Eq. (6).(6)Iswitch(a,b,α)=aα⩽0.5bα>0.5The inputs for the extended, region-based interpolation algorithm are: phonetic transcriptions of two utterances, the two associated voice models, the interpolation parameterαand the region information including region mapping and region procedure for each mapping. Algorithm 2 presents the subsequent steps. First, for each region, the indices of the HSMM states representing each phone in this region are retrieved using the voice model decision trees.value(index)is then used to access the actual HSMM state model for the given index.is_switchandis_interpolationare functions that return true if the supplied region has to use the feature switch or feature interpolation procedure respectively. In case of feature switch, the functionIswitchis used to retrieve the resulting indices for the current region and is then, together with the associated values, appended to the results list. If the region has to be interpolated, DTW is applied, which returns a list of tuples of indices, representing the optimal warping path (as described in Section 3). All HSMM states along this warping path are then interpolated usingIlinearand the resulting, new HSMM states are returned. These are, together with the associated indices for each utterance, also appended to the results list. Finally, results is sorted according to a function ordering (described in 6.4) which defines if the states should be in order of utterance A or utterance B.Algorithm 2Region-based interpolation algorithm.1:results←list(){result list}2: for allr∈R(A)do3:idxA←indices of HMM states in r4:idxB←indices of HMM states inM(r)5:if is_switch(r) then6:values=Iswitch(value(idxA),value(idxB),α)7: append(idxA,idxB,values)to results8:else if is_interpolation(r) then9:dtwpath←DTW(idxA,idxB)10:values=Ilinear(dtwpath,α){dtwpath column 0 is indices for first utterance, column 1 for second utterance}11: append(dtwpath[0],dtwpath[1],values)to results12:end if13: sort results byresults[Iorder()]14: end forIn the extended method, duration modeling for feature interpolation is as described in Section 4. For feature switching, this method is not necessary, the number of states and their durations can just be taken from one of the two involved utterances depending onα.This extended interpolation method can also be used for utterances with a different syntactic structure. Consider the example of a translation from Standard German into SAG syntax that can be seen in Fig. 11: “Ich ging weg” (engl. “I left”)↔“Ich bin weg gegangen” (engl. “I have left”).In this case, the region definition is a bit different because there are no neighbored regions that could be merged. The region-based interpolation algorithm would then again apply the associated procedures (feature interpolation or feature switch) on each mapped region. Feature interpolation creates states for each mapping along the DTW path. Feature switch uses states, durations and features from either utterance A or from B, depending onα. The states of the regions are then concatenated in the order of A or B, also depending onα. So ifα⩽0.5, the ordering of A is used, else ordering of B. For the example shown in Fig. 11 this means that forα⩽0.5the ordering of “Ich ging weg“ is used, forα>0.5the ordering of “Ich bin weg gegangen”. The evaluations presented in this study did not include utterances which required reordering.

@&#CONCLUSIONS@&#
