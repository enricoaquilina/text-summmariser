@&#MAIN-TITLE@&#
A real time algorithm for people tracking using contextual reasoning

@&#HIGHLIGHTS@&#
We improve people tracking using object past history to make more reliable choices.We model object history using a Finite State Automaton.The method tracks objects in complex scenarios with a large number of occlusions.The method is robust with respect to most problems caused by the detection phase.

@&#KEYPHRASES@&#
Video surveillance,Real-time object tracking,Finite State Automata,

@&#ABSTRACT@&#
In this paper we present a real-time tracking algorithm that is able to deal with complex occlusions involving a plurality of moving objects simultaneously. The rationale is grounded on a suitable representation and exploitation of the recent history of each single moving object being tracked. The object history is encoded using a state, and the transitions among the states are described through a Finite State Automata (FSA). In presence of complex situations the tracking is properly solved by making the FSA’s of the involved objects interact with each other. This is the way for basing the tracking decisions not only on the information present in the current frame, but also on conditions that have been observed more stably over a longer time span. The object history can be used to reliably discern the occurrence of the most common problems affecting object detection, making this method particularly robust in complex scenarios. An experimental evaluation of the proposed approach has been made on two publicly available datasets, the ISSIA Soccer Dataset and the PETS 2010 database.

@&#INTRODUCTION@&#
Object tracking plays a fundamental role in several video analysis applications, including video surveillance, traffic monitoring, ambient intelligence, human–computer interaction. The object tracking problem is deceptively simple to formulate: given a video sequence containing one or more moving objects, the desired result is the set of the trajectories of these objects. Unfortunately, in real world scenarios, there are several issues that make this result far from being easy to achieve: the objects of interest may not be easily separated from the background; the background may change because of light changes, camera motion, small movements of background elements (e.g. tree leaves moved by the wind); objects may become partially or totally occluded by background elements or by other objects; and so on. Because of these difficulties, many tracking algorithms have been proposed in the last years, but the problem is still considered open.The algorithms present in the literature can be roughly divided into two categories. In the first one, tracking is performed after an object detection phase: objects are detected in each frame using either some form of change detection (e.g. differences from a background model) or an a priori model of the objects. Algorithms in this category are usually faster, but they have to consider also the errors of the detection phase as spurious and missing objects, objects split into pieces, multiple objects merged into a single detected blob). As an example, the papers by Sethi and Jain [1], by Rangarajan and Shah [2] and by Intille et al. [3] use a greedy algorithm that matches each object to its nearest neighbor, with constraints based on proximity. The first method assumes that the number of objects is constant, so it does not deal with object entries, exits and occlusions; the later methods add the ability to deal with entering or exiting objects and to recognize that an occlusion has occurred (but they do not restore the original object identities after the end of the occlusion).The W4 system by Haritaoglu et al. [4] uses the overlap of the areas as a criterion to find a correspondence between the objects at the current and at the previous frame. When this criterion selects multiple objects, the algorithm considers split or merge hypotheses to deal with detection errors or with occlusions. After an occlusion, an appearance model of the objects is used to reassign the original object identities. Also, when an object is seen for the first time, the algorithm waits for a fixed number of frames before assigning it an object identifier, in order to filter out spurious objects due to detection errors. The use of overlap works well with high frame rates and objects that are not very fast, but might fail in other conditions. The method proposed by Chen et al. [5] formulates the tracking problem as a bipartite graph matching, solving it with the well-known Hungarian algorithm. It recognizes an occlusion, but is able to preserve the object identities only if the horizontal projection of the detected blob shows a separate mode. The method by Pellegrini et al. [6] tries to predict the trajectories on the scene using a set of behavior models learned using a training video sequence. The method is very effective for repetitive behaviors, but may have some problems for behaviors that do not occur frequently. The method by Ess et al. [7] uses stereo vision, coupled with a motion dynamic model and an object appearance model to perform the tracking. The method is not applicable where a stereo camera is not available; furthermore its computational cost is significant, requiring 0.3s per frame only for the tracking part. Dai et al. [8] have proposed a method able to track pedestrians by using shape and appearance information extracted from infra-red imagery. The method may have some problems when objects quickly change their appearance or during occlusions.Several recent methods, such as Eshel and Moses [9], Tong et al. [10], Khan and Shah [11], Berclaz et al. [12,13], Figueroa et al. [14], Delamarre and Faugeras [15], Arsic et al. [16] use the information from different cameras with overlapping fields of view in order to perform the occlusion resolution. The data provided by each camera are usually combined using a probabilistic framework to solve the ambiguities. This kind of technique is limited to the cases where multiple cameras are available; furthermore, most of the methods adopting this approach require a full calibration of each camera, which could make the deployment of the system more complicated.In the second category, detection and tracking are performed at once, usually on the basis of an object model that is dynamically updated during the tracking. These methods are computationally more expensive, and often have problems with the initial definition of the object models, that in some cases has to be provided by hand. The paper by Comaniciu et al. [17] proposes the use of Mean Shift, a fast, iterative algorithm for finding the centroid of a probability distribution, for determining the most probable position of the tracking target. It requires a manual selection of the objects being tracked in the initial frame, and deals only with partial occlusions. Tao et al. [18] have proposed a method based on a layered representation of the scene, that is created and updated using a probabilistic framework. Their method is able to deal with occlusions, but is extremely computational expensive, requiring up to 30–40s per frame. The method by Wu and Nevatia [19] tracks people in a crowded environment. However it uses an a priori model of a person, that is not extendable to other kind of objects. The method by Bhuvaneswari and Abdul Rauf [20] uses edge-based features called edgelets and a set of classifiers to recognize partially occluded humans; the tracking is based on the use of a Kalman filter. The method does not handle total occlusions, and, because of the Kalman filter, it works better if the people are moving with uniform direction and speed. The method proposed by Han et al. [21] detects and tracks objects by using a set of features, assigned with different confidence levels. The features are obtained by combining color histograms and gradient orientation histograms, which give a representation of both color and contour. The method is not able to handle large scale changes of the target objects. The method by Yogameena et al. [22] uses a skin color model to detect and then track the faces in the scene. The method is able to deal with crowded scenes where the persons are dressed with very similar attire, but it works only as long as the face of each person remains clearly visible.Several recent methods (e.g. Cai et al. [23], Wang et al. [24], Hu et al. [25], Saboune and Laganiere [26], Bazzani et al. [27], Yin et al. [28], Medeiros et al. [29], Breitenstein et al. [30]) have investigated the use of Particle Filters, that are a tool based on the approximate representation of a probability distribution using a finite set of samples, for solving the tracking problem in a Bayesian formulation. Particle Filters look very promising, since they make tractable a very general and flexible framework, which can incorporate in the probability distribution also information related to the past history of the objects. However, the computational cost is still too high for real-time applications, especially with multiple occluding targets, with a processing time ranging from 0.5 to 45s per frame.A recent, promising trend in tracking algorithms is the use of machine learning techniques. As an example, the method by Song et al. [31] improves the ability of tracking objects within an occlusion by training a classifier for each target when the target is not occluded. These individual object classifiers are a way of incorporating the past history of the target in the tracking decision. However, the method assume that each object enters the scene unoccluded; furthermore, it is based on the Particle Filters framework, and so it is computationally expensive. Another example is the method by Wang et al. [32] that uses manifold learning to build a model of different pedestrian postures and orientations; this model is used in the tracking phase by generating for each object of the previous frame a set of candidate positions in the current frame, and choosing the candidate that is more close according to the model.In this paper we propose a fast tracking algorithm for real-time applications; the algorithm belongs to the first category outlined above, and in particular it assumes that an object detection based on background subtraction generates its input data. The algorithm is robust with respect to the errors generated by the object detection (spurious or missing objects, split objects) and is able to work with partial and total occlusions.Most of the algorithms in the first category make their tracking decisions by comparing the evidence at the current frame with the objects known at the previous one; all the objects are dealt with in the same way, ignoring their past history that can give useful hints on how they should be tracked: for instance, for objects, stable in the scene, information such as their appearance should be considered more reliable.To exploit this idea, the algorithm adopts an object model based on a set of scenarios, in order to deal differently with objects depending on their recent history and conditions; the scenarios are implemented by means of a Finite State Automaton, that describes the different states of an object and the conditions triggering the transition to a different state. The state is used both to influence which processing steps are performed on each object, and to choose the most appropriate value for some of the parameters involved in the processing. The choice of basing the tracking method on the first of the two approaches outlined above is motivated by the fact that, in this case, the variables characterizing the tracking process are explicitly defined and easily understandable, and so can be used in the definition and in the manipulation of the state; an approach of the second kind, especially if based on machine learning, would have hidden at least part of the state, and thus the history of the objects would not have been explicitly manageable through a mechanism such as the Finite State Automaton.Although a limited a priori knowledge about the objects of interest is required, in order to differentiate between single objects and groups, the proposed method can be rapidly adapted to other application domains by providing a small number of object examples of the various classes.In the following, we will provide in Section 2 a description of the rationale of the algorithm; Section 3 is devoted to present the common problems affecting a tracking algorithm: our approach focuses on them for suitably defining the scenarios to deal them properly. Section 4 describes the proposed system architecture together with its components, each presented in detail in dedicated subsections. Section 5 will describe an experimental validation, performed on the standard PETS 2010 database [33] and on the ISSIA Soccer Dataset [34], and will discuss the obtained results. Finally, in Section 6, we will present our conclusions and will outline some future developments.Before starting the description of the algorithm, we need to introduce some terminology and notations.A blob is a connected set of foreground pixels produced by a detection algorithm, which usually finds the foreground pixels by comparing the frame with a background model; then the foreground pixels are filtered to remove noise and other artifacts (e.g. shadows); finally, the foreground pixels are partitioned into connected components, which are the blobs. The tracking algorithm receives as its input the set of blobs detected at each frame. We assume that the detection phase operates using a dynamic background model so as to deal with lighting changes; noise reduction, shadow and small blob removal are further carried out. See details in [35].An object is any real-world entity the system is interested in tracking. Each object has an object model, containing such information as the object class (e.g. a person or a vehicle), state (see Section 4.1), size, position, trajectory and appearance (see Section 4.4). A group object corresponds to multiple real-world entities tracked together; if a group is formed during the tracking (i.e. it does not enter the scene as a group), its object model maintains a reference to the models of the individual objects of the group.The task of the tracking algorithm is to associate each blob to the right object, in such a way as to preserve the identity of real-world objects across the video sequence; in the process the algorithm must also create new object models or update the existing ones as necessary.In real cases, the detection phase produces some common errors:•spurious blobs, i.e. blobs not corresponding to any object; they can be caused by lighting changes, movements of the camera or of the background, and other transient changes that the detection algorithm was not able to filter out. An example is the blob identified by number 19, shown in Fig. 1. In such situation the background updating algorithm is not fast enough to compensate for the movement of the ribbon caused by the wind;ghost blobs, i.e. blobs appearing where there was an object previously considered as part of the background, that has moved away (e.g. a parked car that starts moving);missing blobs, i.e. undetected objects, for instance as too similar to the background behind them;split blobs, i.e. objects divided into multiple blobs. Fig. 1d shows an example of an object split into three different blobs, identified by numbers 5, 9 and 10, because of camouflage.In addition the algorithm must also handle partial or total occlusions, ensuring that object identities are not lost across the occlusion. An example of multiple occlusion is shown in Fig. 1c, where three different individual objects are merged into a single blob, identified by number 7.A key idea behind the proposed algorithm is to base the decisions regarding an object not only on its current conditions, but also on its past history; in this way spurious observations can be easily ignored, and the decisions can be based on stable properties of the object. To this aim, the object history is encoded using a state, belonging to a finite set of possible values. The transitions between states are explicitly described through a Finite State Automaton (FSA), and are triggered by such events as the permanence of the object in the scene, its disappearance, the outcome of the object classification and the participation to an occlusion.Fig. 2shows the modules composing the tracking system, and their interdependencies:•the state manager, which maintains and updates an instance of the FSA for each object;the association manager, which establishes a correspondence between objects and blobs, solving split-merge events and performing occlusion reasoning;the object classifier, which assigns objects to a set of predefined classes; the object class is used both during the update of the FSA state and during the association between objects and blobs to solve split/merge cases;the similarity evaluator, which computes a similarity measure between objects and blobs, considering position, size and appearance; this similarity is used during the association between objects and blobs.The above modules share a set of objects models, which, as previously said, contain all the relevant information about each object.Fig. 3shows an outline of the tracking algorithm. The algorithm operates at the arrival of each new frame, receiving as its inputs the existing object models and the blobs discovered by the detection phase for the current frame; for the first frame of the sequence, the existing object models are initialized as an empty set. The output of the algorithm is a set of updated object models, which possibly include new objects. The steps of the algorithm are the following:•first, the classifier is applied to the current blobs, which are annotated with the information on the assigned class;then, the algorithm computes the similarity between each object and each blob, activating the similarity evaluator; the similarity information is kept in a similarity matrix S;at this point, the algorithm is ready to perform the association between objects and blobs, including the split/merge and occlusion reasoning;on the basis of the associations found, the algorithm update the models for each object; if new objects are detected, their models are created at this step;finally, the state manager updates the FSA states, using the previous state and the information gathered by the previous steps and stored in the object models.In the following subsections, more details are provided for each module of the system.In this section we examine some typical problems of people detection, in order to see how they can be solved by incorporating information about the history of the objects.One of the most frequently encountered issues is related to the objects entering the scene, which have a very unpredictable appearance during the first frames of their life. Fig. 4shows a typical example, in which the person is split by the detection phase into two different blobs (i.e. legs and arms). The problem here is that after a few frames the parts appear to merge forming a new group of objects; since the occlusion resolution requires that object identities are preserved within a group, the tracking algorithm would continue to keep track of two separate objects (labeled 1 and 2 in the figure). To solve this problem, the tracking algorithm has to use different rules for dealing with merging objects when they are just entering or when they are stably within the scene.Missing blobs are another typical problem affecting the detection; they can be caused either by camouflage, occurring when the foreground object is very similar to the background, or when occlusions between moving objects arise. The latter case is really difficult to deal with as information about the occluded part is totally missing and consequently to be restored by suited reasoning.Fig. 5shows an example of the above mentioned problem: the person that passes behind the tree is detected in the first and in the third frames of the sequence, but not in the second one. Thus, the tracking algorithm would find at the third frame a blob having no corresponding object in the previous frame, and would assign it to a newly created object, if it does not keep some memory about the object even when it is not visible in the scene. On the other hand, the tracking algorithm should not preserve information about objects that are truly leaving the scene: doing so it would risk to reassign the identity of an object that has left the scene to a different object that is entering from the same side.Other issues are related to objects occluding each other, forming a group. In Fig. 6a it is shown a problem with the stability of group classification: in the first frame, the two persons in the group are perfectly aligned, and so a classifier would not be able to recognize that the object is a group. On the other hand, in the following frames the object is easier to recognize as such. Thus, in order to obtain a reliable classification the tracking algorithm has to wait that the classifier output becomes stable, before using it to take decisions.Another problem related to groups is the loss of the identities of the constituent objects. An example is shown in Fig. 6b, where objects 1 and 2 first join a group and then are separated again. When the objects become separated, the tracking algorithm would incorrectly assign them new identities, if the original ones where not preserved and associated with the group. Notice that in this case it would not have been possible to simply keep tracking separately the two objects using some kind of motion prediction until the end of the occlusion, because as a group the objects have performed a drastic change of trajectory (a 180° turn).The analysis conducted in this Section about the typical problems in a real-world setting shows that in a lot of situations a tracking system cannot be able to correctly follow the objects without knowing additional information about their history. In the next subsection, we will see how the proposed FSA is able to provide this information.In this section we will describe in more details the four subsystems composing the proposed tracking algorithm.The state manager has the task of maintaining and updating the FSA state of each object; the FSA state embodies the relevant information about the past history of the object, which can be used by the other parts of the tracking system. What pieces of information are actually relevant is a decision depending somewhat on the specific application; different problems may require the algorithm to keep different information in order to deal appropriately with them, and so may require an entirely different FSA.Although we present only a single formulation of the FSA, it should be considered that the methodology is more general, and easily extendable to other cases, since the knowledge about the states and the transitions between them is declaratively specified in the automaton definition, and not hidden within procedural code.In order to deal with the issues discussed in Section 3, we propose a state manager based on the Finite State AutomatonAdepicted in Fig. 7. It can be formally defined as:(1)A=〈S,Σ,δ,s0,F〉where S={s0,…,sm} is the set of the states; Σ={a0,…,am} is the set of the transition conditions, i.e. the conditions that may determine a state change; δ:S×Σ→S is the state-transition function; s0∈S is the initial state and F⊂S is the set of final states.The proposed Finite State Automaton states and transitions are shown in Table 1. In particular, the set of states S is shown in Table 1a; we choose s0 as initial state, since each object enters the scene by appearing either at the edge or at a known entry region (e.g. a doorway). Furthermore we choose s5 as final state, since each object necessarily has to leave the scene. The set Σ of transition conditions and the state-transition function δ are shown respectively in Table 1b and c.It is worth noting that each state has been introduced in order to correctly solve one of the issues described earlier, as we will detail below. So it is possible to extend the FSA with the addition of other states and transitions, in order to deal with some other problem that should rise in a specific application context.The meaning of the states and the conditions triggering the transitions are detailed below:•new (s0): the object has been just created and is located at the borders of the frame; if it enters completely, and so does not touch the frame borders (a0), it becomes to be classified; otherwise, if it leaves the scene (a1), it immediately becomes deleted.The introduction of new state solves the problem related to the instability of the entering objects, since it makes the system aware of such scenario and then capable to react in the best possible way, as shown in Fig. 8a. Moreover, this state allows the algorithm to quickly discard spurious objects due to detection artifacts, since they usually do not persist long enough to become to be classified.to be classified (s1): the object is completely within the scene, but its class is not yet considered reliable; if the classifier assign the same class for at least two frames (a3), it becomes classified; if the association manager detects that the object has joined a group (a6), it becomes in group; if the object disappears (a1), it becomes frozen; if the object is leaving the scene, then it is not completely within it (a8), it becomes exiting.The to be classified state solves the issues of the objects entering the scene as group, discussed in Fig. 6a. Thanks to this state, the class of the object is only validated when the system is sure about them. An example is shown in Fig. 8c. We have to remember that the objects class is very important, since it makes the association manager able to take the correct decisions about the resolution of split and merge patterns.classified (s2): the object is stable and reliably classified; if the classifier assigns a different class (a4), it becomes to be classified; if the association manager detects that the object has joined a group (a6), it becomes in group; if the object disappears (a1), it becomes frozen; if the object is leaving the scene, then it is not completely within it (a8), it becomes exiting.The distinction between classified and to be classified objects is used by the association manager when reasoning about split objects and group formation.frozen (s3): the object is not visible, either because it is completely occluded by a background element, or because it has left the scene; if the object gets visible again (a7), it becomes to be classified; if the object remains suspended for more than a time threshold Td(a2), it becomes deleted; currently we use Td=1s.The frozen state avoids that an object is forgotten too soon when it momentarily disappears, as it happens in Fig. 5.in group (s4): the object is part of a group, and is no more tracked individually; its object model is preserved to be used when the object will leave the group; if the association manager detects that the object has left the group (a5), it becomes to be classified.The in group state has the purpose of keeping the object model even when the object cannot be tracked individually, as long as the algorithm knows it is included in a group. Thanks to this state, the proposed method is able to correctly solve the situation shown in Fig. 6b, related to group objects.exiting (s5): the object is located at the borders of the frame; if it disappears from the scene (a1), it becomes deleted.The exiting objects differ from the frozen ones because of the system have not to preserve their identity, since their are leaving the scene.deleted (s6): the object is not being tracked anymore; its object model can be discarded.Fig. 9shows a detailed example of how the object state management works.The tracking system needs an object classifier to determine if a blob corresponds to a group, an individual object, or an object part. Currently we have applied our system to people tracking, so we have only two classes of individual objects: person and baggage. We adopt a multi-class Support Vector Machine (SVM) classifier using the Histogram of Oriented Gradients (HOGs) [36] as descriptor. The latter, which has already proved to be very effective for pedestrian detection, allows to describe the patterns by using the distribution of local intensity gradients or edge directions: the image is partitioned into cells and a local 1-D histogram of gradient directions or edge orientations over the pixels of each cell is computed. The accuracy of the descriptor is improved by contrast-normalizing the local histograms: a measure of the intensity across a larger region of the image, a block, is computed and it is used to normalize all the cells within the block. Such normalization makes the descriptor invariant in changes in illumination and shadowing. In Fig. 10we show the description of different entities using HOG.Once extracted the features vector, a multi-class SVM has been applied. Being our problem multi-class, a N one-against-rest strategy has been considered; the ith classifier is trained on the whole training data set in order to classify the members of ith class against the rest.At this point one can note that object evolution is not dependent on its class (e.g. group or individual object), but only on its actual state. As a matter of fact, only object information is related to object class, while object state only determines the reliability of such information. In particular, for individual object we have information about appearance and shape: we consider the area and the perimeter of an object, its color histograms and its real dimensions, that is width and height, both obtained using an Inverse Perspective Mapping. Moreover we have information about the observed and predicted position of the object centroid. The predicted position is obtained using an extended 2D Position-Velocity (PV) Kalman filter, whose state vector is:(2)ξ=[xc,yc,w,h,ẋc,ẏc,ẇ,ḣ,]where (xc,yc) is the centroid of the object, w and h are the width and the height of the object minimum bounding box in pixels,(ẋc,ẏc)and(ẇ,ḣ)are respectively the velocity of the object centroid and the derivative of the minimum bounding box size. It is worth noting that such a PV Kalman filter is very effective when the object motion is linear and the noise has a Gaussian distribution.Group objects contain also information about occluded objects. In this way the system can continue to track the in group objects when they leave the group.The association algorithm is used in two distinct phases of the tracking system, as shown in Fig. 11a: once for stable objects (that is, objects in the to be classified and in the classified states) and once for unstable ones. The rationale behind this choice is that the reasoning about split and merge transformations only makes sense if applied to stable objects, since the high variability of unstable objects would expose this part of the algorithm to a high risk of introducing unwanted artifacts (e.g. two parts of a same object might be kept separate because, when the object is initially unstable, there is a frame in which they are split into two regions). So the algorithm first processes the stable objects, possibly applying split and merge transformations; then, the remaining foreground regions are considered, creating new objects if needed.Fig. 11b gives a description of the association algorithm for stable objects; it is based on a greedy association using a suitably defined similarity matrix S between objects and blobs. In particular, the generic element sijof the matrix represents the similarity between the blob bjand the object oi. Details about the defined similarity index are given in sub Section 4.4.The algorithm starts by choosing the maximum element of the matrix, say skl, and if its value is above a given similarity threshold τ, the association between the blob bland the object okis considered accepted.Subsequently, the associated blob blis compared to other objects to find whether there is an object ohdifferent from okthat is close enough to it and not to any other blobs. This condition may hold either when an occlusion is starting (and thus ohis the object being occluded by ok), or when a previously detached object (such as a luggage) becomes attached to ok.These situations are distinguished by properly using the output of the classifier: in the first case the algorithm creates a group object and links it to the individual objects forming the occlusion (okand oh).A similar check is performed to verify if the associated object okis similar enough to other blobs different from bl; the situations in which this may happen are the dual of the previously considered ones: either a group that is splitting into the individual component objects, or the end of an occlusion. The classifier is used to recognize the correct event; in the case of an ending occlusion, the algorithm uses the individual object models linked to the group object to reassign the correct identity to the objects leaving the group, changing their state from in group to to be classified.Finally the algorithm removes from the similarity matrix all the rows and columns corresponding to objects and blobs it has used, and repeats the choice of the maximum element in the matrix. If no element is above the threshold τ, all the remaining unassigned objects are put in the frozen state and the first phase terminates.The second phase is shown in Fig. 11c. It follows a similar scheme, except that it considers only the objects in the new state, and does not perform the checks for merges, splits, starting and ending occlusions. Moreover, the similarity matrix is built using less features than in the first phase since we have experimentally verified that only the position information (see Section 4.4) is sufficiently reliable for such objects. At the end of this phase, any remaining unassigned blobs are used to create new object models, initialized to the new state.As already mentioned, the similarity matrix is used to match one or more blobs with one or more objects. An example is depicted in Fig. 12. In order to measure the similarity between an object oiand a blob bj, the tracking system uses an index based on three kinds of information: the position, the shape and the appearance:(3)sij=αp·sijp2+αs·sijs2+αa·sija2αp+αs+αaAs described below, sijvalues identify similarity metrics and α values are weights chosen according to the state of the object and the association management phase. In particular:•sijpis the position similarity index, that is the distance between the estimated centroid of an object oiand the centroid of a blob bj;sijsis the shape similarity index between an object oiand a blob bj;sijais the appearance similarity index between an object oiand a blob bj, based on color histograms;αp, αsand αaare the weights of position, shape and appearance similarity index respectively;All α values have been chosen by experimentation over a training set. Namely, in the first phase, selected values for objects in the to be classified and classified state are αp=αs=αa=1while for objects in the in group state selected values are αs=αa=1; αp=0 since in this context shape and appearance similarity perform better than position one. Finally, in the second phase that evaluates new objects, we choose to consider the only reliable feature, that is the position. Thus selected α values are αs=αa=0; αp=1.For the position, as we have already seen, the system uses a Kalman filter, based on a uniform velocity model, to predict the coordinates of the object centroid at the current frame. The predicted coordinates are compared with the center of the blob, using Euclidean distance, obtaining for each object oiand each blob bjthe distance dij. The position similarity index is then computed as:(4)sijp=1-dij/dmaxwhere dmax is a normalization factor depending on the maximum velocity of objects representing the maximum displacement of an object between two frames.For characterizing the shape similarity, the system uses the real height and the area of the blob and of the object model; in particular if we denote as Δhijthe relative height difference between oiand bj, and as ΔAijthe relative area difference, the considered shape similarity index is:(5)sijs=1-(ΔAij)2+(Δhij)22Finally, as a representation of the appearance we have used the color histograms computed separately for the upper half and for the lower half of the object or blob (Image Partitioning). We have experimented with several criteria for comparing the histograms, and we have found that the most effective value is the χ2 distance:(6)qij=1M∑khio(k)-hjb(k)2hio(k)+hjb(k)where index k iterates over the bins of the histogram,hiois the histogram of object oi,hjbis the histogram of blob bj, and M is the number of bins. The appearance similarity index is:(7)sija=1-1-qijup2+1-qijlow22whereqijupis the value of qijcomputed using only the upper half of the object/blob, andqijlowis the value computed using only the lower half.In order to assess the performance of the method with respect to the state of the art, we have used two different datasets: the publicly available PETS 2010 database [33], currently used by many research papers, and the ISSIA Soccer Dataset [34]. The former is made of seven videos captured in a real-world environment, containing several occlusions between a person and an object, two persons or among several persons. Fig. 13shows an example for each considered view of the PETS 2010 database. We have computed in each view the maximum speed of the objects, from which we have derived the optimal values of the dmax parameter of Eq. 4, that are dmax=100 for views 1, 3 and 4 and dmax=150 for view 5, 6, 7 and 8.Figs. 15i and Fig. 16 show some excerpts of the video sequences, with a complex occlusion pattern among three persons; as it can be seen, the system preserves the object identities across the occlusions. Fig. 17i shows an excerpt with a complex split situation caused by the occlusion between a person and the pole. As it can be seen, the system preserves the object identity.The latter dataset, the ISSIA Soccer Dataset, is composed by six synchronized videos acquired at 25fps during a match by six Full-HD cameras, located along the major sides of the playing-field. Examples of the different views are given in Fig. 14; the difficulty arising during the tracking in this context is related to the rapid changes of trajectories of the players and their visual appearance: as a matter of fact, except for the referees and the goalkeepers, all the players, depending on their team, have a white or a blue1For interpretation of color in Fig. 14, the reader is referred to the web version of this article.1uniform. Such an issue could make very difficult to recover occlusions by exploiting visual dissimilarities between involved players.As for the PETS dataset, we have computed in each view the maximum speed of the moving objects, from which we have derived the optimal values of the dmax parameter of Eq. (4), that is dmax=80 (for all the views). An example of correctly solved occlusion is shown in Fig. 15ii, while Fig. 17reports an example of split caused by an error during the detection phase and properly adjusted.The quantitative evaluation of the method has been carried out using the performance indexing proposed in the PETS 2010 contest [37]. In particular, we have used the following indices: the Average Tracking Accuracy (ATA), the Multiple Object Tracking Accuracy (MOTA) and the Multiple Object Tracking Precision (MOTP). In the following we introduce some notations useful to formally define them.Let Giand Dibe the ith ground truth object and the ith detected one at the sequence level, respectively;Gi(t)andDi(t)denote the ith ground truth object and the detected one in frame t;NG(t)andND(t)denote the number of ground truth objects and detected ones in frame t, respectively, while NGand NDdenote the number of ground truth objects and unique detected ones in the given sequences. Nframesis the number of frames in the sequences. Finally, Nmappedrefers to the mapped system output objects over an entire reference track, taking into account splits and merges andNmapped(t)refers to the number of mapped objects in the frame t.ATA is a spatiotemporal measure that penalizes fragmentations in spatiotemporal dimensions while accounting for the number of objects detected and tracked, missed objects, and false positives. ATA is defined in terms of Sequence Track Detection Accuracy STDA:(8)STDA=∑i=1Nmapped∑t=1NframesGi(t)∩Di(t)Gi(t)∪Di(t)NGi∪Di≠0The latter measures the overlap in the spatiotemporal dimensions of the detected object over the ground truth, taking a maximum value of NG. The ATA is defined as the STDA per object:(9)ATA=STDANG+ND2As already mentioned, the MOTA is an accuracy score that computes the number of missed detections, false positives and switches in the system output track for a given reference ground truth track. It is defined as:(10)MOTA=1-∑t=1Nframescm·mt+cf·fpt+cs(ist)∑t=1NframesNG(t)where mtis the number of misses, fptis the number of false positives, and istis the number of ID mismatches in frame t considering the mapping in frame (t−1); c values are weights chosen as follow:cm=cf=1;cs=log10(·)Finally, the MOTP is a precision score that calculates the spatiotemporal overlap between the reference tracks and the system output tracks:(11)MOTP=∑i=1Nmapped∑t=1Nframes(t)Gi(t)∩Di(t)Gi(t)∪Di(t)∑t=1NframesNmapped(t)The results of the proposed method are shown in Figs. 18 and 19for the PETS and the ISSIA Soccer Datasets. In particular, Fig. 18a shows the results of the proposed method over the PETS dataset related to the individual sequences. We can note that the performance is strongly influenced by the complexity of the single sequence. This complexity is not determined only by the typology of the single view (i.e., the presence of the pole in the first view or the presence of the tree in the third one, which covers one-third of the scene taken by the camera), but also by the interactions among the tracked objects.At this point we can examine in detail the considered views, analyzing their performance in relation to the complexity of the scene. The first view presents interactions among two or three objects; the only difficulty is due to the presence of the pole and of the sign hanged on it, which causes a lot of splits. Note that the proposed method proves to be particularly robust with respect to the split situations on this view.Views 3 and 4 are the most complex, as shown by the results displayed in Fig. 18a. Indeed, as already mentioned, view 3 is characterized by the presence of a large tree (about one-third of the scene), occluding a lot of individual or group objects. The situation is further complicated by the complexity of interactions among the objects, which involves in the average 2−5 objects for view 3 and 2−6 for view 4. Another problem in view 4 is the presence of a white-orange ribbon, continuously moving because of the wind. Such situation causes a lot of problems also in the detection phase.The problem of the moving ribbon is also present in views 5–8, even if it is less visible. We can note that the performance obtained in views 6 and 7 is generally lower than that obtained on other sequences; this is related to more complex interactions between the tracked objects, having a very high number of occlusions associated to objects that are entering the scene (unstable objects).It is worth noting that the method, during an occlusion, does not attempt to find the exact position of an object inside a group; it continues to track the group as a whole, using the Kalman filter for obtaining a prevision of the position of each object inside the group itself; this choice obviously causes a degradation of the performance if it is measured using indices defined assuming that objects are always tracked individually.Fig. 19a and b shows the performance of the method, compared with the approaches that participated to the PETS 2009 and 2010 competitions, which in the figures are indicated, respectively, as BerclazLP [13], Arsic [16], Conte [35], Alahi Ogreedy and Alahi Olasso [38]. It is worth noting that the results presented to the PETS by other competitors in some cases only refer to a subset of the views (views 1, 5, 6 and 8). For such reason, in order to have a proper comparison with these methods, we present the experimental results computed both over all the views and over the same subset of views as these methods. In particular, the results in Fig. 19a refer to all the views of the PETS database, while Fig. 19b only refers to views 1, 5, 6 and 8. We can note that in the first comparison our method outperforms the others on the precision index (MOTP), while in the second one it clearly outperforms all the other contest participants on these views on all the indices.Table 2presents for each view of the PETS dataset a detail of the performance of our algorithm, in terms of miss and false positive occurrences, while in Table 3a detail is shown about the resolution of split and occlusion patterns. We can note that the proposed method is more robust with respect to the occlusion occurrences, rather than to the split ones.As for the ISSIA Soccer Dataset, the performance over the single scenarios are shown in Fig. 18b, which highlights similar values over all the different views. It is worth pointing out that the database contains some very complex occlusion patterns, involving a variable number of players, ranging from two to eight, that strongly influence the performance of the entire system.Although a direct comparison with the state-of-the-art methods is not possible as the results reported over this dataset are provided in a qualitative form, the high values obtained by our method both in terms of accuracy and precision confirm the validity of the proposed approach also in the sport applicative domain.A further experimentation, shown in Table 4, presents some more general evaluation criteria, which reflect the possibility to correctly follow a trajectory, assigning it one or more id [9]. In particular:•TP (True Positive) refers to the number of trajectories followed for more than the 75% of their life, also with different identifiers. An example of TP is shown in Fig. 21a;PTP (Perfect True Positive) refers to the number of trajectories followed for the 100% of their life with the same identifier. An example of PTP is shown in Fig. 21b;FN (False Negative) refers to the number of trajectories followed for less than the 75% of their life;FP (False Positive) refers to the number of spurious trajectories followed for more than 2s;IDS (ID-Switch) refers to the number of times an object changes its identifier. Fig. 20 shows the histogram of the id-switch number for the considered PETS 2010 dataset.Av refers to the average trajectories length;Min refers to the minimum trajectories length;Max refers to the maximum trajectories length;AvIds refers to the average number of id-switches for each trajectory;In particular, the first part of the table refers to the PETS dataset, while the second part refers to the ISSIA Soccer Dataset. It is worth noting that the high number of false positive trajectories, especially in views 4 and 6 of the PETS dataset, is caused by very frequent detection errors: in particular, as already mentioned, in view 4 it is caused by the presence of the white-orange moving wire, while in view 6 it is related both to the presence of the wire and to the white car, wrongly identified as an object by the detection phase during all the sequence. All these kinds of detection errors, identified in Table 4 by the numbers in brackets, could be reduced in a very simple way, by applying a filter to the detection phase, taking into account the particular shape and appearance of the spurious objects. Since we are only interested in the proposed tracking method, we do not investigate into the performing of the detection phase.As for the computational cost, the system runs at 16ms per frame on 4CIF images, using an Intel Xeon processor at 3.0GHz. In Fig. 22there is a detail of the execution time of the algorithm over the considered views. It is worth noting that the computational cost of the algorithm is only slightly dependent on the number of objects.The promising results obtained by the proposed method over two very different application fields, a video-surveillance domain (PETS dataset) and a sports one (ISSIA Soccer Dataset), confirm the robustness of the approach and its applicability to any real-time context. Almost all the problems arising during the detection step, ranging from split blobs to undetected objects, are easily managed by the different states and complex occlusions correctly solved thanks to the introduction of the group objects.

@&#CONCLUSIONS@&#
In this paper we have presented a tracking algorithm that is both efficient and robust, being able to overcome many of the problems induced by errors in the object detection phase, as well as total or partial occlusions. The algorithm has been experimentally validated on two public databases, showing a significant performance improvement over the participants to an international competition. Its comparatively low computational cost makes this algorithm well suited to real-time tracking applications.