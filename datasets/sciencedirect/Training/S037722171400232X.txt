@&#MAIN-TITLE@&#
On the student evaluation of university courses and faculty members’ teaching performance

@&#HIGHLIGHTS@&#
We develop a statistical framework based on Statistical Quality Control – SQC mainly.It can be used in order to exploit student evaluations as much as possible.We present two directions/axes of data monitoring and analysis.The first make use of the most important SQC tool, namely control charts.The second exploit “multiple” hypotheses testing.

@&#KEYPHRASES@&#
Higher education,Student evaluation,Questionnaire,Statistics,Control charts,Hypotheses testing,

@&#ABSTRACT@&#
Trying to determine higher education quality, one gets quickly to one of its significant dimensions, namely the quality of faculty members’ teaching. The latter and, overall, the quality of any university course should be certainly evaluated by their recipients, namely students. In this paper we develop a statistical framework based on Statistical Quality Control mainly, which can be used in order to exploit student evaluations as much as possible. More specifically we present two directions of data monitoring and analysis; the one uses control charts and the other hypotheses testing. The results that can be raised through both directions are crucial for any decision maker.

@&#INTRODUCTION@&#
For many years now, there have been numerous researchers and practitioners who have studied and written a lot about quality of products and, lately, about quality of services. It is well known that the differences between those two quality variations are many and unquestionable. For instance, consider the following indicative dimensions of service quality which are not so popular in product quality: courtesy, friendliness and responsiveness of employees, accessibility of service and convenience of clients, time and timeline issues, etc.Focusing on services, things get more complicated, especially when we consider educational issues. The traditional concept of quality is inadequate when it comes to referring to and assessing quality in education and more specifically in higher education. Even a simple definition like “fitness for purpose”, when it is interpreted for higher education it depends on the values and priorities set. In other words, the outcomes might be very different depending on who defines the purpose of higher education. Harvey and Green (1993) address the nature of the concept of quality in relation to higher education and conclude that quality is “stakeholder-relative”. For example, the attention for students and teachers might focus on the process of education, while employers might focus on the outputs of higher education. There are various “stakeholders” in higher education including students, parents, employers, teaching and non-teaching staff, government and its funding agencies, assessors (including professional bodies) and auditors (Burrows & Harvey, 1992). Consequently, it is not right to consider quality a unitary concept.Overall, the mission of higher education institutions is fulfilled by two main activities (Green, 1994):1.Producing graduates to meet the human resource needs of (any type of) organizations.Advancing knowledge via research.The first activity is closely related with a significant dimension of higher education quality, namely the quality of teaching. Moreover, given that the direct receivers of the teaching service are students, it is absolutely normal and expected that they express their opinion and insights on the quality of the offered services. Their experience is crucial as per any monitoring of higher education quality, despite the fact that often institutions choose alternative or at least additional ways of evaluating their quality, such as external accreditation or peer review (Douglas & Douglas, 2006).Quite a few researchers have dealt with the evaluation of higher education quality. Politis and Siskos (2004) attempt to evaluate the performance of processes in a Greek engineering department and to construct a framework for the evaluation of organizations using quantitative methods. Their methodology is based on multicriteria analysis principles, assess the department’s operating level and identify some crucial matters. Ray and Jeon (2008) provide a different perspective on the ranking of the various MBA programs and evaluate their efficiency levels, employing a measure of Pareto–Koopmans global efficiency.Very recently, Kuzmanovic, Savic, Gusavac, Makajic, and Panic (2013) propose an approach for conducting an objective evaluation of university teachers, which is based on previously obtained conjoint analysis data concerning the importance of criteria – from a student’s point of view – that are preset by universities and used by students to rate their teachers. Lupo (2013) considers a recent extension of the ServQual model in order to evaluate the student satisfaction level of Italian higher education. Their method uses both the Analytic Hierarchy Process as well as the Fuzzy Sets Theory to effectively handle uncertainty in service performance analyses.In this paper we develop and present a statistical framework based mainly on Statistical Quality Control (SQC), which can be used in practice by Institution decision makers in order to analyze and exploit student evaluations as much as possible. More specifically, we present two directions/axes of data monitoring and analysis; the one make use of the most important SQC tool, namely control charts and the other exploit “multiple” hypotheses testing.In what follows, we first present the student evaluation procedure (Section ‘The student evaluation procedure’), making references to the practices followed in Greece. Then (Section ‘The statistical dimension of student evaluation’), we refer to various statistical issues of the procedure, while in Section ‘The proposed statistical framework’ we present the basic points of our statistical framework. In Section ‘The proposed methodology for continuous evaluation of one course assignment in consecutive semesters – Axis 1’ we examine in more detail the Axis 1 of our framework, while in Section ‘The proposed methodology for instantaneous evaluation of all course assignments at the same semester – Axis 2’ we do the same for Axis 2. In both these sections we also present numerical case studies. We conclude our research in Section ‘Discussion and conclusions’, referring also to some future research ideas.Student evaluations of the quality of university courses and faculty members’ teaching ability have been a routine and mandatory part of undergraduate and graduate education for a long time period (Mohanty, Gretes, Flowers, Algozzine, & Spooner, 2005). The style of student evaluations varies from country to country, or even from institution to institution, but, generally, the most common form of this type of evaluation is completion of a multi-item survey/questionnaire assessing areas such as specific and general ratings of course effectiveness (for example course content, organization and difficulty, etc.) and/or specific and general areas of teacher’s effectiveness (for example his/her communication skills, organization, enthusiasm and knowledge of the taught subject, etc.).In Greece, quality assurance of educational and other services in higher education has gained lately significant attention. The same stands also for the evaluation of higher education institutions. According to the relevant Greek legislative framework, the quality evaluation procedure is conducted periodically, in two ways: internally, by the Schools and the Departments of institutions, and externally, by accredited third parties. Moreover, in order to enable the evaluation procedure, the legislative framework provides for the establishment of a new organizational structure at every institution, namely the Quality Assurance Unit (MODIP), which acts in parallel and in collaboration with the rest units of the institution. The coordinating body of all higher education institutions’ MODIPs is the Quality Assurance Authority (ADIP), which oversees the evaluation of institutions nationwide and sets the rules, standards and criteria for the quality evaluation.The evaluation of quality of university courses and faculty members’ teaching performance is conducted in Greece by students, through questionnaires. ADIP has designed a standard questionnaire, which has been adopted and is used by all Greek higher education institutions. Through its questions students state the level of agreement or disagreement (on a 5-point Likert scale) regarding several quality characteristics of the educational work of their institutions, grouped in five categories:•Course issues.Teacher/instructor’s characteristics.Teaching assistant(s)’ characteristics.Lab issues.Students’ participation.Moreover, there is a section of general comments, which is completed by students using free text. Through this general, university-wide questionnaire every institution can benchmark courses and faculty members, not only within the different institution units (e.g. departments), but also at a national level, considering the respective metrics of other institutions.Departments as well as individual teachers have the option to add their own questions to the evaluation questionnaire, giving a more personal and occasionally a more useful mapping of factors that are important to higher education quality. For example, those that rely a lot on laboratory work, studio sessions, or other specific forms of pedagogy may ask questions that are related to these elements.In Greece, the evaluation of every assigned course and its teacher is conducted during a lecture that takes place in the period between the 8th and the 10th week of every academic semester. Questionnaires are completed by students anonymously, either electronically or in a paper form. In order for a student to participate in the evaluation process, he/she has to be present in the classroom on the date the questionnaires are distributed. Participating in the evaluation process is completely optional.A course assignment is the basic evaluation unit. For example, a teacher that has undertaken two classes (i.e. groups of students) on a laboratory course and one class on a theoretical course, is considered to have two course assignments, namely one assignment for the laboratory course (consisting of two classes) and another assignment for the theoretical course (consisting of one class). Thus, he/she will be evaluated regarding those two assignments.Every educational work of an Institution occurs within a system of interconnected processes, which contain many sources of variation. For example, teachers have different upbringings, educational backgrounds and working experiences, which make each one of them unique in terms of personality and values. They work with different students, which have unique personalities, while they interact with various individuals on campus (other teachers, administrators and staff) and perform different kinds of tasks. Moreover, they often utilize a variety of resources (e.g., textbooks, reference books, notes, writing instruments) and their work involves the use of different kinds of equipment, with varying features, capability, and performance. They work under different supervisors, who may have a variety of management styles and they are also affected by many environmental conditions (e.g., family relationships, noise level, the collegiality of the work environment, morale level, weather patterns, etc.) that exist at home, in their classrooms and labs, and within the institution as a whole (Maguad, 2007). Despite the fact that the number of sources of variation is large, a process (e.g. an educational one) that is affected by this type of variation is said to be “stable” and is considered to be “in-control”. Common-cause variation comes as a result of the design of the system.The other type of variation that may exist in an educational process results from special/assignable causes. The latter can be attributed to external sources that are not inherent in a process; they produce unnatural variation and are rather easily detectable using statistical methods. Moreover, they can (and should) be prevented or corrected. When special cause variation exists the process is said to be “out of control”. Some indicative examples of special causes in education that could affect the performance of teachers are:•hiring of unqualified, incompetent or untrained faculty members or staff,admission of students who are unprepared to do college work,malfunctioning equipment,inadequately equipped laboratories and libraries,dysfunctional interpersonal relationships,management by fear,a teacher’s serious illness or accident,excessively warm or cold classroom temperatures,campus crime or civil unrest,flooding, fire or natural disaster,So far, the occurrences of assignable causes during a process that alter its characteristics as well as their elimination have been studied extensively – especially for production processes – using SQC techniques. The most significant statistical tool for this purpose is the control chart which constitutes a special type of scatter plot and illustrates the alterations of a quality characteristic through time. More specifically, samples are taken from the monitored process periodically and the values of an estimator θ (e.g. sample averageX‾) are calculated and plotted as consecutive points on the control chart (Fig. 1). The use of these charts aims at locating changes on the examined process and eliminating the assignable causes that provoke these changes (Tagaras, 2001).The most usual form of a control chart is depicted in Fig. 1, which presents the central line (CL), which corresponds to the in-control operation of the monitored process and denotes the desired value of the quality characteristic θ that is monitored, the upper control limit (UCL) and the lower control limit (LCL).The (design and operation) control chart parameters are the size of each sample (n), the sampling interval (h) between successive samples and the position of control limits (k). An absolutely necessary precondition of the design and operation of any control chart is the exact knowledge of the statistical behavior of the process when it is in-control, because only then the detection of deviations from the in-control state is possible. Therefore, the first, necessary step of the Statistical Process Control procedure is the systematic and exhaustive study of the process characteristics when the process is in-control (Tagaras, 2001). More specifically, in the case of an educational process it is important to determine the state of the classroom process after the potential outliers have been eliminated (Maguad, 2006).If a data point falls outside the control limits, then there is a strong indication that there has been a deviation of the monitored process from the in-control state and that one or more assignable causes exist and affect the process negatively. In this case, the process should not be considered stable; thus, an investigation is warranted to find and eliminate (through appropriate corrective actions) the assignable cause or causes (Tagaras, 2001). In the special case of an educational process, if the (student) evaluation regarding the quality of an instructor’s teaching performance or, in general, of an assigned course is found outside the control limits, then it can be concluded that something abnormal has occurred, which should be investigated properly. Bear in mind that this evaluation might be based either on the values of only one parameter or on the values of a weighted performance index.Focusing on the part of the student evaluation that refers on an instructor’s teaching ability and performance, then teachers can be classified into three groups:•Group 1 – teachers who are above the UCL,Group 2 – teachers who are below the LCL andGroup 3 – teachers who are between the control limits.Broadening the last remark, it should be mentioned that although it may be true that in some cases a “worker” may be partly to blame for a “mistake”, in most instances differentiations or even problems arise due to the inherent variation of the system (Maguad, 2006). In other words, differences between levels or points within control limits (i.e. teachers in Group 3) come from the system itself – therefore they must be ascribed to the system – not from teachers (Maguad, 2005). However, it is common when analyzing the statistical figures to compare an individual performance against the group norm. But what does it really mean when one’s performance is below or above the average? Always, when an average is computed, about half of the group is expected to be above the average and half below! Consequently, there is no reason to brand somebody as a below-average performer.The evaluation of courses or, more specifically, the evaluation of any instructor’s teaching performance, as well as the monitoring of the quantitative data of an evaluation can be made in two main directions/axes:•Axis 1: a continuous evaluation of one course assignment or, more specifically, of the teacher’s performance in consecutive semesters, using control charts such as theX‾– chart for the “central tendency” of the performance and S chart for its “dispersion” (Maguad, 2007).Axis 2: an instantaneous evaluation of all course assignments or, more specifically, of all teachers’ performance at the same time, in order to classify teachers into one of the aforementioned Groups, i.e., 1, 2 or 3. In this case the use of control charts is not appropriate, while the “multiple” hypotheses testing should be preferred instead.Taking into consideration the notation of Table 1, every answer, i.e. every value 1–5, for each of the k questions of the questionnaire is considered to be a measurement. That is, for each assigned course (sample j) and for each question k, we gather the following xk,imeasurements: xk,1,xk,2,…,xk,nk,j, which are parameter or performance index values, or values of a statistic Xk. In case a student does not answer a question, no measurement is registered.Bear in mind that the xk,imeasurements are Likert scale values with which several critical factors are associated with, such as:•Only a few options are offered to the evaluators, with which they may not fully agree.Evaluators may become influenced by the way they have answered previous questions. For example, if they have agreed in several consecutive questions, they may either continue to agree or, on the contrary, deliberately disagree with a statement, with which they might otherwise have agreed.An odd number of choices allow evaluators to select a central – neutral choice – value.Some evaluators avoid making extreme choices (e.g. values 1 and 5 of the 5-point Likert scale) as they prefer to be considered as moderate rather than extremist evaluators.Researchers have not given definite solutions to all these challenges, but address them ad hoc, using, however, typically a 5-point Likert scale (although there have been arguments for using other odd or, sometimes, even number Likert scales), and asking from time to time reversal questions in order to force evaluators to break patterns. As far as the distribution of the statistic Xkis concerned, it is obvious that Xkis not continuous but discrete, taking each time one of the aforementioned five possible values, namely 1, 2, 3, 4 and 5. Although the frequently used Likert scales are not necessarily evenly spaced along the agreement/disagreement continuum, Blaikie (2003) mentions that it has become common practice for researchers to assume that they are, and treat the collected measurements as values of a continuous variable. That is what we choose to do in what follows.Moreover, the (relative) frequency distribution of Xkis usually asymmetric (as there are usually more 4s and 5s, than the smaller values), resulting in negative skewness and left asymmetry (see for example Fig. 2). Both these facts ask for large sample sizes (of at least 30 answers/measurements per question), that is the number of students that complete the questionnaires of a course should be larger than 30. This way, the distribution ofX‾k(on which the evaluation is based) can be considered to be normal (Central Limit Theorem – CLT).For example, even without conducting Chi-square goodness of fit test for the normal distribution of the data of Fig. 2, it is obvious that the student evaluations (Xkvalues) are not normally distributed. Consequently, according to Table 2(rows 2 and 3), we come to the conclusion that large samples (i.e. nk,j>30) are essential in order to take advantage of the CLT and achieve reliable results, based on the normal distribution ofX‾k.All (i.e. nk,j) responses for question k of the completed questionnaires for one course (e.g. j), constitute a sample j of nk,junits – measurements. For example, consider the case where “Maths – Theory” is the assigned course. Suppose also that 30 completed questionnaires have been collected, while only 27 out of 30 students have answered question j, namely “Which is your teacher’s overall evaluation?”. In this case, the sample consists of 27 answers, namely nk,j=27.On the one hand, for every assigned course/sample (e.g. j) the following parameters should be determined or calculated:•nk,j: the number of answers/measurements for a specific question (k).fk,1, fk,2, fk,3, fk,4, fk,5: the frequencies of the values 1, 2, 3, 4 and 5, for which it is fk,1+fk,2+fk,3+fk,4+fk,5=nk,j.Course averagex¯k,j: the ratio of the sum of measurements (arising from the respective answers) in question k of the course/sample j to the number of measurements of the course (i.e. nk,j) for question k.•Course standard deviation sk,j: standard deviation of measurements (arising from the respective answers) in question k of the course/sample j. It is calculated using the following equation:On the other hand, as far as all measurements – regarding question k – at all courses/samples of a Department2If appropriate, courses should be grouped into smaller groups of courses according to their subject/relevance or other grouping criteria.2are concerned, the following metrics should be calculated:•Departmental averagex¯¯k: the weighted average of the average valuesx¯k,jof all (m) samples (Montgomery, 1996; Site 1; Site 2). The sizesnk,jof those (m) samples should be used as weights:•Departmental standard deviations¯k: the weighted average of all standard deviations sk,jof measurements of the m courses of the Department. It is calculated using once again the sizes nk,jof the (m) samples as weights:In order to design the limits of the necessary control charts (phase I of control chart design, Axis 1) – for instance theX‾– chart for the central tendency of the performance and S chart for its dispersion – or to determine the critical region limits (Axis 2), we need to calculate the respective measures μ0,kand σ0,k(i.e. a reliable estimate of σk) of central tendency and dispersion of the Department. As far as σ0,kis concerned the weighted average value of the standard deviations sk,j(namelys¯k) has to be corrected through the constant c4, according to the following formula:(6)σˆ0,k=s¯kc4The constant c4 can be calculated either by reference tables (for n⩽25), such as Table 3, using as n value the average value of the nj’s of all (m) courses or approximately by the following equation (for n>25):(7)c4,j=4(nk,j-1)4nk,j-3Regarding μ0,k, the following choice is suggested:(8)μ0,k≡x¯¯kHowever, several other ways could be adopted to determine μ0,k. For instance:•Arbitrarily, in a way, based on which (average) performance is considered satisfactory for the decision makers of an Institution/Department. Perhaps, the “arbitrary” determination of μ0,k could be combined with the “arbitrary” determination ofσˆ0,k, which would lead to the indirect determination either of the control limits (Axis 1) or of the critical region limits (Axis 2).Subtracting from the m assigned courses all those that during phase I of control chart design – Axis 1 (or during the initial hypotheses testing implementation – Axis 2) are found out of control limits (or in the critical region, respectively). The measurements of those courses should not affect the determination of μ0,k and σ0,k.Reducing the number of courses taken into consideration in the determination of μ0,k., depending on the inspected parameter or performance index value. In other words, from the total number of courses (m), only those related with the inspected parameter or performance index should be taken into account. For instance, some indicative groups of course assignments are (i) laboratory courses, (ii) quantitative courses, (iii) theoretical courses, (iv and v) courses with big or small audience, (vi and vii) courses with or without homework assignments, (viii and ix) courses with or without mid-term exams, etc.From the total of m courses, taking into consideration only the–most recent ones (based on a predetermined time threshold/criterion) orassignments of adjunct teaching faculty or the assignments of tenured faculty members orcourses where nk,j>30, etc.Finally, bear in mind that the above ways to determine μ0,kcould be used either one at a time or in various combinations.First, for every course (sample j) the averagex¯k,jand the standard deviation sk,jare determined using (1) and (2) respectively. Then, the departmental averagex¯¯kand standard deviations¯kare calculated from (3) - or (4) - and (5) respectively. In order to determine the control limits – of kk3The subscript k is determined according to the notation presented in Table 13standard deviations – of theX‾and S control charts (which should be used simultaneously in order to monitor both the central tendency and the dispersion of X), we use the following formulas:•X‾- control chart:•S – control chart:We calculated the control limits of kk=3 standard deviations of theX‾and S control charts using data from the student evaluations, regarding the question “Which is your teacher’s overall evaluation?”, for three semesters of three indicative courses (named as Course 1, 2 and 3) of the Department of Business Administration of the Technological Education Institute of Central Macedonia in Serres4Additional studies regarding the Technological Education Institute of Central Macedonia can be found in Zafiropoulos, Frangidis, Kehris, Dimitriadis, and Paschaloudis (2005) and Zafiropoulos, Kehris, and Dimitriadis (2005). The first one present the implementation of SERVQUAL at the aforementioned institute and compare the arising results with the results reported by other institutions, while the second one present the experience gained by the Department of Business Administration of the specific institute in adopting three student satisfaction assessment tools, i.e. SERVQUAL and two structured questionnaires.4. The results are presented in Figs. 3–8. In this subsection we make some comments on the various indications of control charts, similarly to what should be done in practice when this type of control charts is applied in full scale.Considering the combination of control charts presented in Figs. 3 and 4 it should be noticed that the central tendency of student evaluations regarding the specific question of Course 1 is under statistical control; there are no indications out of control limits. In other words, it seems that there is no assignable cause affecting negatively (or even positively) the average performance of the instructor teaching Course 1. On the other hand, we notice that the evaluations’ dispersion on semesters 1 and 3 is out of control. More specifically, the standard deviations of semesters 1 and 3 are above the upper control limit which means that for an unknown to us reason the teacher that is evaluated by students, collected highly scattered evaluations during these semesters. Obviously, this trend needs to be studied thoroughly in order to eliminate the assignable cause (e.g. differentiation in evaluations between girls and boys, new and old students, etc.) that caused it.Focusing in Figs. 5 and 6, namely on the data corresponding to Course 2, we can figure out clearly the effect of nk,jon the control limits of both charts. Due to the large value of nk,jin semester 3 in comparison to the values of the other semesters, the control limits of both control charts are narrower at this sampling instant. This effect is direct on theX‾– control chart limits and indirect (through the determination of c4) on the S control chart.Moreover, we notice an out of control indication in the S control chart during semester 2 (Fig. 6), which almost leads the indication of theX‾– control chart out of control (Fig. 5). This means that with a closer look at both control charts we can figure out that with a “normal” (or in other words in statistical control) dispersion during semester 2 (and not so small, as indicated by the S control chart – Fig. 6 and the respective s value) we could have an out of control indication at theX‾– control chart. This trend would ask for a more careful examination (for example determine the updatedX‾– control chart limits using the temporarily small s value of semester 2) in order to find out if there is really an assignable cause affecting the average of evaluations. In case of a positive response at the previous question the assignable cause affecting the average would be either a best practice of the teacher (which should be rewarded and also followed by other teachers too) or a negative habit, appreciated though by students (e.g. unreasonably high marks, small duration of lectures, etc.).Finally, taking a look at Figs. 7 and 8 we see that everything is OK as far as the dispersion of Course 3 is concerned, while the average of semester 3 is below the lower control limit, a fact that reveals a low teaching performance at that semester.According to the hypotheses testing philosophy (which is largely similar to the rationale of control charts), a critical region (or, in other words, region of rejection) should be determined for every course and every null hypothesis H0, which corresponds to a question j of the questionnaire. Hypothesis H0 is always tested against an alternative hypothesis HA. For example, H0: “The total presence of teacher is deemed satisfactory” orμ=μ0,kcould be tested against HA: “The total presence of teacher is not deemed satisfactory” orμ≠μ0,k. In the determination of the critical region the measures of central tendency and dispersion, μ0,kand σ0,krespectively, of the Department should be taken into consideration. They are calculated based onx¯¯kands¯kas mentioned previously.The determination of the critical region limits of the central tendency of a parameter or performance index or, equivalently, statistic Xk, expressed by the sample averagex¯k,j, is based on the calculation of the following limitsLLx¯k,jandULx¯k,j:(13)ULx¯k,j=min{μ0,k+A3,j.s¯k,5}LLx¯k,j=max{μ0,k-A3,j.s¯k,1}The parameter A3,jcan be calculated for every course (sample j) as follows:(14)A3,j=kkc4,jnk,jwhere kkis a function of the significance level α; for example kk=3 corresponds to α=0.27%, while kk=2.5 corresponds to α=1.24%, etc. As mentioned previously, the constant c4,jcan be calculated either by reference tables or by (7).It should be mentioned that formulas (13) can be also written as follows:(15)ULx¯k,j=μ0,k+kkc4,js¯knk,j=μ0,k+kkσˆ0,knk,jLLx¯k,j=μ0,k-kkc4,js¯knk,j=μ0,k-kkσˆ0,knk,jAs far as the determination of the critical region limits of the dispersion of a parameter or performance index or statistic Xkis concerned, it is based on the calculation of the limitsLLsk,jandULsk,jof the sample standard deviation sk, according to the following equations, which are similar to (11) and (12):(16)ULsk,j=B4,j·s¯kLLsk,j=B3,j·s¯kThe parameter B3,jcan be calculated for every course (sample j) as follows:(17)B3,j=1-kkc4,j2(nk,j-1)=1-kkc4,j1-c4,j2and, likewise, B4,jas follows:(18)B4,j=1+kkc4,j2(nk,j-1)=1+kkc4,j1-c4,j2According to this Axis of monitoring the quantitative data of an evaluation we have determined the critical region limits for the average and the standard deviation using the aforementioned Eqs. (7) and (15)–(18). This has been done using kk=3 standard deviations and data from the student evaluations as per the question “Which is your teacher’s overall evaluation?”, for eleven courses/teachers of the Department of Business Administration of the Technological Education Institute of Central Macedonia. The evaluation of courses/teachers is conducted through the following hypotheses regarding central tendency:H0:μ=μ0,kandHA:μ≠μ0,kand the following hypotheses regarding dispersion:H0:σ=σˆ0,k=s¯kc4andHA:σ≠σˆ0,k=s¯kc4The results we have taken are represented in Figs. 9 and 10.Focusing in Fig. 10, the first remark that should be made is that there is one teacher (the one that was responsible for Course 10) whose student evaluations present high dispersion. This fact would ask for a more careful examination; for instance to determine the updated limits of the rejection region of central tendency (average) of Course 10 using the large s value of this course. However, most probably the rejection of H0 regarding the standard deviation is not the reason of the rejection of H0 regarding the average of Course 10, which can be noticed in Fig. 9. It seems that the teacher of the specific course is far below the acceptable average limitsLLx¯k,jandULx¯k,j, according to the student evaluation.Similarly, there is one more course/teacher which have been evaluated negatively by students (Course 7), while two more teachers have been evaluated marginally better than the upper acceptable average limitULx¯k,j: the teachers of Courses 9 and 11.

@&#CONCLUSIONS@&#
In Table 4we present the total number of combinations that can be met in practice for Axis 1 of student evaluation exploitation; note, however, that a similar table could be used for Axis 2 also. This is a somehow “pivot table” which in comparison with the statistical framework we present earlier can enable institutional decision makers, at least in Greece, first to evaluate faculty members and teachers generally. More importantly, though, they can enable decision makers to argue about the performance of any instructor, without encountering negative reactions from the latter. Unfortunately, this is the usual practice in Greece; either there is not significant exploitation of the student evaluations or, if there is, the negative responses of the evaluated teachers is the most usual situation.Consequently, we consider our methodology as well as the SQC-based statistical framework as the most important contribution of this paper. Of course, this research effort can be continued and completed in many directions. For example, taking into consideration the negative skewness and left asymmetry of the (relative) frequency distribution of Xk, then the use of theX∼– control chart, instead of theX‾– chart, could be examined as a more convenient option.Additionally, though in a different research perspective, the development and the use of a relative MIS could facilitate seriously the exploitation of evaluation data, not only in the directions presented in this paper but in much more directions: e.g. easy determination of control chart or critical region limits, through various ways, philosophies and formulas, ranking of teachers, memory acquisition of the evaluation system, etc.