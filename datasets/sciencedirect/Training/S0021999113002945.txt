@&#MAIN-TITLE@&#
Optimized explicit finite-difference schemes for spatial derivatives using maximum norm

@&#HIGHLIGHTS@&#
We propose an optimized scheme using the maximum norm and the simulated annealing.The maximum norm offers the largest set of possible solutions for solvers to search.The explicit finite-difference operator is greatly improved by our optimized scheme.We use a tight error limitation to make accuracy improvement to be high and solid.Our optimized scheme allows greater saving of computational cost and memory demand.

@&#KEYPHRASES@&#
Optimized scheme,Explicit finite-difference,Numerical dispersion,Maximum norm,Simulated annealing algorithm,

@&#ABSTRACT@&#
Conventional explicit finite-difference methods have difficulties in handling high-frequency components due to strong numerical dispersions. One can reduce the numerical dispersions by optimizing the constant coefficients of the finite-difference operator. Different from traditional optimized schemes that use the 2-norm and the least squares, we propose to construct the objective functions using the maximum norm and solve the objective functions using the simulated annealing algorithm. Both theoretical analyses and numerical experiments show that our optimized scheme is superior to traditional optimized schemes with regard to the following three aspects. First, it provides us with much more flexibility when designing the objective functions; thus we can use various possible forms and contents to make the objective functions more reasonable. Second, it allows for tighter error limitation, which is shown to be necessary to avoid rapid error accumulations for simulations on large-scale models with long travel times. Finally, it is powerful to obtain the optimized coefficients that are much closer to the theoretical limits, which means greater savings in computational efforts and memory demand.

@&#INTRODUCTION@&#
The explicit finite-difference (FD) scheme is one of the most popular approaches used in various numerical simulations, because it is simple in its numerical implementation and is powerful in handling complex media. However, the conventional explicit FD method has serious numerical artifacts in the presence of high-frequency components and/or coarse grids. This problem would dramatically increase both the demands on the memory and computational cost, especially for large-scale models [7], since a fine grid should be properly designed and a high-order FD operator should be applied. A popular way to avoid this problem is to manually decrease the dominant frequency. This method could result in an acceptable running time, but would result in very limited spatial resolutions, because high-frequency components are necessary for improving the final resolutions. Another way is to apply advanced methods that have less numerical dispersion, such as optimized explicit FD methods and implicit FD methods (either conventional or optimized). Compared with implicit FD methods, explicit FD methods usually have much less computational cost. Therefore, we prefer to develop the optimized scheme of explicit FD methods to further reduce the numerical dispersions while maintaining the computational cost.Optimized schemes of FD methods appeared two decades ago [10,17]. It has been widely used to reduce the numerical dispersions in many practical applications, such as acoustics, seismology, and electromagnetics. The basic idea of the optimized scheme is to increase the accurate wave number coverage of the FD operator within a tolerable error range by modifying the constant coefficients. The main advantages of using optimized explicit FD methods are that we can significantly improve the numerical results by maintaining the algorithm structure, the source code and the computational efficiency. In addition, we can use a relatively coarser grid as well as larger time step, hence the memory demand and the total running time would further decrease.Holberg [10] suggests using the group velocity, which is related to dispersion errors. Etgen [6] suggests employing the phase velocity rather than the group velocity, since the phase velocity is more straight-forward than the group velocity. Some works propose adding a weight function to the objective functions to enhance the influence of the wave number of interest [9,13,16]. However, almost all traditional optimized schemes use the least squares to minimize the objective functions. Thus the forms and the contents of the objective functions are fairly limited and inflexible. Unfortunately, the forms and the contents of the objective functions greatly affect the extent of the improvement in accuracy. In other words, the coverage of accurate wave numbers obtained by traditional optimized schemes is not wide enough because of the limitations created by the objective functions. Therefore there is still some development space for optimized schemes.In addition, traditional optimized schemes do not pay enough attention to the proper selection of error limitation, which is found to be critical for solid accuracy improvement. Usually traditional optimized schemes tend to employ relatively large error limitation to obtain a wide-enough accurate wave number coverage. Typically, the error limitations shown in the literature range from 0.0003 to 0.03 [10]. Although we may have seen a large accurate wave number coverage range in theoretical analyses, we would ultimately find that an optimized FD operator using a large error limitation is actually apt to obtain less improvement or even worse results compared with un-optimized FD operators [26]. Therefore, we should try to obtain a reasonable accurate range that is as wide as possible; meanwhile we should carefully select the error limitation to guarantee that the accuracy improvements are tangible.Almost all previous optimized schemes use the 2-norm to construct the objective functions, because such objective functions can be easily solved by the least squares (e.g., [2,3,13,16,23,27]). Whereas, the main task of optimization is to improve the accurate wave number coverage, as widely as possible; thus all our works should contribute to this task, including constructing the objective functions and solving the objective functions. We should not consider too much whether the objective function is easy to solve by some existing method; after all, either the 2-norm or the least squares is just one possible choice. Besides the 2-norm, we can use the 1-norm, the p-norm and the maximum norm. Besides the least squares we can use many other advanced optimization approaches, such as the simulated annealing algorithm [14], the genetic algorithm [11] and the particle swarm optimization [5]. Therefore we may obtain much greater accuracy improvement, or even reach the theoretical upper limit, if we adopt a reasonable objective function, a powerful solver and a proper error limitation.In this paper, we employ the maximum norm to construct the objective functions and use the simulated annealing algorithm to minimize the objective functions. The maximum norm provides us with an intuitive and effective measure of the optimized FD operator. The simulated annealing algorithm provides us with a more powerful tool in searching for the optimal coefficients. Without being constrained by the solver as before, we can freely design the forms and contents of the objective functions and try to make the objective functions more reasonable. The maximum norm and the simulated annealing algorithm allow us to use much smaller error limitations to make the accuracy improvements more concrete.Traditional optimization methods have difficulties in evaluating the error before performing the optimization. Thus, they empirically give a possible wave number range that the optimized operator may cover. In consequence, the maximum error will be very high if the expected input wave number is too big; otherwise, the accurate wave number range will be underestimated. In contrast, the new scheme proposed in this paper does not have such a problem. One can determine the preferred error limitation according to the modeling tasks by either the size of the model or the duration of the record. Then the program will try its best to find the optimal coefficients under the given error limitation.According to the sampling theory of discrete signals, the band-limit continuous signal f(x) can be recovered by a sinc interpolation of uniformly sampled signals fnas follows(1)f(x)=∑n=-∞∞sinπΔ(x-nΔ)πΔ(x-nΔ)fn,where Δ is the grid interval of spatial direction x, and fn≡f(nΔ) are the sampled values on the discrete positions nΔ. For simplicity, we define φ≡π/Δ and θ≡(x−nΔ)φ; hence the first four orders of spatial derivatives are(2)∂f(x)∂x=φ∑n=-∞∞-sinθθ2+cosθθfn,(3)∂2f(x)∂x2=φ2∑n=-∞∞2sinθθ3-2cosθθ2-sinθθfn,(4)∂3f(x)∂x3=φ3∑n=-∞∞-6sinθθ4+6cosθθ3+3sinθθ2-cosθθfn,(5)∂4f(x)∂x4=φ4∑n=-∞∞24sinθθ5-24cosθθ4-12sinθθ3+4cosθθ2+sinθθfn.These expansions at x=0 are as follows(6)∂f(x)∂xx=0=∑n=-∞∞cos(nπ)-nΔfn,(7)∂2f(x)∂x2x=0=∑n=-∞∞2cos(nπ)-n2Δ2fn,(8)∂3f(x)∂x3x=0=∑n=-∞∞π2nΔ3-6n3Δ3cos(nπ)fn,(9)∂4f(x)∂x4x=0=∑n=-∞∞4π2n2Δ4-24n4Δ4cos(nπ)fn,respectively. We see that there is a singularity when n=0. To avoid this singularity, we can also express the expansions according to the symmetry as follows [16](10)∂mf(x)∂xmx=0=2∑n=1∞an′(fn-f-n)formis odd,(11)∂mf(x)∂xmx=0=a0′f0+2∑n=1∞an′(fn+f-n)formis even,where m is the order of the derivatives, andan′are the coefficients of fnin Eqs. (6)-(9), n=1,2,…,∞.The conventional explicit FD operators are actually truncated Nth-order expansions multiplied by a window function an[8,4], where N is an even integer and anis the constant coefficient defined by the binomial coefficient formula(12)an=NN2+nNN2.For the optimized scheme, the basic aim is to search for a new group of coefficients that are different from the above expansions and have better numerical performance. The final form of the optimized FD operator can be expressed as follows:(13)∂mf(x)∂xm≈1Δm∑n=-N/2N/2bnfn,where bnare the coefficients that are ready to be optimized.According to the Fourier transform theory, the spatial derivatives can be equally expressed in the wave number domain as follows [15](14)∂mf(x)∂xm⇌(ikx)mF(kx),where kxis the wave number, F(kx) is the forward Fourier transform of f(x), andi=-1. Eq. (14) is the analytical expression of the spatial derivatives in the wave number domain, which covers the whole Nyquist bandwidth. Thus we can examine the accuracy of our optimized FD operators by comparing their Fourier transforms with the analytical wave number (ikx)m. When m=1, applying a spatial Fourier transformation to (13), we obtain the following relation(15)ikxF(kx)≈iΔ∑n=-N/2N/2bnsin(nkxΔ)F(kx)≡ikx∗F(kx),wherekx∗is defined as the wave number of the optimized FD operator, andkx∗is an approximation of the analytical wave number kx. When m=2, we obtain the following relation(16)-kx2F(kx)≈1Δ2∑n=-N/2N/2bncos(nkxΔ)F(kx)≡-(kx∗)2F(kx).The 2-norm is the most popular criterion used to construct objective functions (e.g., [2,13,16,19,23,27])(17)E=∫0kc|(ikx)m-(ikx∗)m|2w(kx)dk,where kcis the maximum accurate wave number under a given error limitation,kx∗is the approximated wave number, and w(kx) is some weight function. The least squares are usually used to find the optimized coefficients that minimize the objective functions. The optimized coefficients could be determined by setting(18)∂E∂bn=0,and solving the resulting system of linear algebraic equations. The advantage of using the 2-norm and the least squares is that we can obtain a unique group of optimized coefficients. However, the forms and the contents of the objective functions are somewhat limited; that is, one cannot design the objective functions arbitrarily since the designed objective functions should be solvable by the least squares. This limitation makes it difficult to find the optimal group of optimized coefficients, because the flexibility of designing the objective functions would severely influence the final accuracy.In fact, the 2-norm is only one of the candidates for examining the optimized coefficients. We can also use the 1-norm or the maximum-norm (i.e. the infinite-norm). Following the theory proposed by Tam and Webb [23], Bogey and Bailly [3] minimize the relative difference rather than the traditional absolute difference. Using 1-norm and proper weight functions, obtain much higher accuracy than the standard explicit high-order methods. For all traditional optimization methods, however, it is difficult to provide a proper accurate wave number range, which is required before performing the optimization but is in fact critical for the success of optimization; in addition, they all fall into the least square when minimizing the objective function.The p-norm is defined as(19)||y||p=∑j∈N|yj|p1/p,j=1,2,…,J.For p=2, Eq. (19) denotes the 2-norm; for p=∞, Eq. (19) denotes the maximum-norm, which can also be expressed as(20)‖y‖∞=max(|y1|,|y2|,…,|yJ|).Recalling that(21)‖y‖∞⩽‖y‖2,we see that the maximum-norm is not so strict as the 2-norm. Generally, if p>r>0, we have(22)‖y‖p⩽‖y‖r.Inequality (22) indicates that the maximum-norm is the loosest among all p-norms. Fortunately, this loosest constraint would not seriously affect the accuracy since the value of ||y||∞ is comparable to that of the 2-norm and 1-norm. The maximum-norm provides us with the largest number of possible solutions under a given error limitation [24]. This would greatly enhance the possibility of finding a group of optimized coefficients when scanning a vast solution set. On the other hand, checking the maximum deviation sounds more reasonable than checking the “distance” between the accurate and approximated wave numbers since it is not working in the space domain. Therefore, we chose the maximum-norm as our criterion for designing the objective functions to extend the accurate wave number coverage as widely as possible.In this paper, we examine the absolute error between the analytical wave numbers and the approximated wave numbers using the maximum norm. For the optimized FD operators of the frequently-used first- and second-order derivatives, the objective functions are(23)E(kc,ε)≡max0⩽kx⩽kckxΔ-∑n=-N/2N/2bnsin(nkxΔ)⩽εand(24)E(kc,ε)≡max0⩽kx⩽kc-kx2Δ2-∑n=-N/2N/2bncos(nkxΔ)⩽ε,respectively, where kcis the maximum accurate wave number range that the optimized FD operator can handle, and ε is the error limitation, also called the tolerant threshold. This is probably the most straightforward and simplest objective function that we can find in the literature. It is an intuitive and effective measure of the optimized FD operator.Despite being straightforward and simple, the maximum-norm is actually seldom used in designing the objective functions; in contrast the 2-norm is popular. The main reason is that the maximum-norm cannot be solved easily by the least squares. Holberg [10] presents the absolute error of the group velocity based on the maximum-norm; whereas he uses the 4-norm in practice in order to still use the least squares for determining the optimized coefficients. Lele [17] suggests using the relative error of the optimized FD operator based on the maximum-norm when designing a compact scheme (i.e., optimized Padé scheme); however he determines the optimized coefficients by solving the linear algebraic equations on several specified wave numbers.Obviously, it is difficult to solve the maximum-norm problem; thus we have to employ a much more complex optimization approach. In this paper, we use the simulated annealing algorithm [14,22], as it has good flexibility in handling various optimization problems. The simulated annealing algorithm is also famous for searching global minima that are buried among many local minima. Therefore it is suitable for our purposes. Fig. 1shows the flowchart of solving the objective functions based on the maximum norm using the simulated annealing algorithm. In fact, we would obtain many quite different groups of reasonable solutions under a given error limitation rather than only one group as with the least squares; thus we can further select the best one by some tradeoff between the accurate-wave number coverage and the total error (or the peak error).The flowchart shown in Fig. 1 tries to find the best group of the optimized coefficients under the given error threshold T by continuously searching until it cannot get a better group within the given iteration number N. The best group of optimized coefficients means that it provides the widest accurate-wave number range of [0,kc]. The temperature S basically controls the range of perturbation on the solution; for a high temperature, there is a high possibility to reach a wide range, and vice versa. For each searching procedure at the kth wave number, the temperature S would be very high (e.g., 10000) at the beginning to guarantee that the best solution can be obtained. The temperature would gradually decrease by a factor of α=0.99. For any estimated coefficients c (i.e.,b-N/2∼bN/2), some small perturbations δ are added to test whether there are any better coefficients around c. If there are some, the perturbed coefficients a=c+δ will be set to be the initial values for the next searching procedure at n+1; if there is no, they will still be taken as the potential candidate of the initial values with a random possibility by R[0,1]<exp{[E(c)−E(a)]/S}, where E is the maximum of the absolute errors between the analytical wave numbers and the approximated wave numbers for 0 to k (see Eqs. (23) and (24)). If some group of coefficients satisfies E(a)<T, then it will be remembered as the potential solutions b for 0 to k. Next, we would further test whether there is some solution for 0 to k+1. If there is no solution for 0 to k+1, the coefficients b for 0 to k would be the final solution under the given error threshold T, and the maximum accurate wave number kcis k×π/K.The total number of optimized coefficients is N+1, i.e.,b-N/2∼bN/2, which is difficult to determine with the simulated annealing algorithm when N is large. To reduce the optimization effort, Zhang and Yao [26] set up three criteria according to the theories of sinc interpolation [4] and finite impulse response [21] for the second order derivative. We extend Zhang and Yao’s criteria to more general cases: (1) the coefficients should be real numbers bn∊R, and the operator should be symmetric for even order derivatives (i.e., b−n=bn) and be anti-symmetric for odd order derivatives (i.e., b−n=−bn); (2) the total energy of the optimized FD operator should be zero for both even and odd order derivatives, that is∑n=-N/2N/2bn=0; (3) the coefficients should have an amplitude of damped oscillation away from the center position (n=0), that is |bn|>|bn+1| and bnbn+1<0 forn⩾0; (4) to cover a much wider wave number range, all coefficients should be as large as possible (including b0 and bN/2o).Rules 1 and 2 reduce the actual number of coefficients to only N/2, sinceb0=-2∑n=1N/2bnfor even order derivatives andb0=2∑n=1N/2bn=0for odd order derivatives. Thus we can obtain the whole operator by purely determining the independent coefficientsb1∼bN/2(orb-N/2∼b-1). Rules 3 and 4 greatly decrease the scope of the search and make the simulated annealing algorithm affordable for high-order FD operators. In fact, the original coefficients of the conventional FD operators also obey these criteria.The error threshold ε plays an important role in the optimized scheme of the FD operator [25,26]. For a small error limitation (e.g., 0.00001) it can guarantee the accuracy of the resulting operators, but would make it difficult to gain an apparent improvement. For a big error limitation (e.g., 0.0003–0.03 as suggested by Holberg [10] and by many other works) it can easily cover a much wider wave number range; unfortunately, the practical performances shown in numerical experiments may greatly deviate from the theoretical analyses, especially for large travel times or at large distances. Therefore it is necessary to select a proper error limitation for the objective functions to guarantee that the accuracy improvements are apparent and solid.Using the phase velocity is more convenient than using the group velocity when designing the objective function [6,13,16]. Basically, the absolute error of the operator in the wave number domain is similar to the phase velocity. Whereas Holberg [10] points out that the objective function based on the phase velocity should have a much smaller error limitation than that based on the group velocity, which is about one order of magnitude smaller. However, our experiments show that the practical error limitation is not necessarily as small as suggested by Holberg [10] to obtain the same accuracy. Nevertheless, we still suggest using a tight error limitation since in practice the requirement on the accuracy is always increasing.We evaluate the accuracy performance of the optimized FD operators by examining its absolute errors in the wave number domain. First, we show the accuracy influences caused by different error limitations. In Fig. 2we take the 8th-order FD operator of the second derivative as an example. Obviously, the absolute-error curves of the conventional FD operators increase gradually with increasing wave numbers; whereas the absolute-error curves of the optimized FD operators vibrate rapidly within the given error limitation. The optimized FD operators have a much wider accurate-wave number coverage at the cost of much more errors that are evenly distributed within the “accurate-wave number” coverage. Fortunately these error limitations are small enough for many practical applications.The error limitations listed in Fig. 2 (i.e., 0.00005–0.0004) are all far lower than those listed in the literature (e.g., 0.0003–0.03). Obviously, a bigger error limitation would lead to greater accuracy improvements but much larger peak errors; meanwhile, we see that only a doubled error limitation would earn similar accuracy improvements, as indicated by the black dots and the vertical arrows. Therefore, we have to make a balance between the accuracy improvements and the peak errors. We prefer a tight error limitation to avoid rapid error accumulation, especially for large-scale and long-term problems. We select 0.0001 as our error limitation for later experiments. Tables 1 and 2list the optimized coefficients under this error limitation for the first and second derivatives, respectively.Figs. 3 and 4show the 4th- to 12th-order optimized FD operators for the first and second derivatives, respectively. Obviously, the accuracy of the optimized FD operator has a wider accurate wave number coverage than does the conventional same-order FD operator. In addition, the higher-order optimized FD operators have much wider accurate wave number coverage than do the lower-order optimized FD operators. For example, the accuracy of the optimized 4th-order FD operator is only slightly higher than that of the conventional 4th-order FD operator. In contrast, the accuracy of the optimized 8th-order FD operator is much higher than that of the conventional 8th-order FD operator, and even reaches that of the conventional 12th-order FD operator. Furthermore, the accuracy of the optimized 12th-order FD operator is much higher than that of the conventional 12th-order FD operator and even reaches that of the conventional 24th-order FD operator. Therefore, we suggest using the higher-order optimized FD operators in practical applications since they have much higher accuracy compared with the lower-order optimized FD operators.To illustrate the optimized scheme proposed in this paper, we compare our optimized FD operators with Kam and Webb’s optimized FD operator (1993). We consider the 1D advection equation(25)∂u∂t+∂u∂x=0,with an initial disturbance(26)u(x,t=0)=12exp-ln2(x-20)2σ,where0⩽x⩽400and the grid interval Δ=1. Fig. 5shows the curves of three different 6th-order optimized FD operators using error limitations of 0.01, 0.005 and 0.0001. The optimized coefficients used in Fig. 5 are listed in Table 3. Obviously, our three operators have smaller peak errors than that of Tam and Webb [23]. In addition, one of our operators shows wider wave number coverage as well as smaller peak errors than do Tam and Webb’s operator. This indicates that our maximum-norm objective functions as well as the simulated annealing algorithm are better than the 2-norm objective functions and the least squares. On the other hand, the other two operators obtained by our scheme seem to have a narrower wave number coverage than Tam and Webb’s operator; surprisingly, our numerical experiments show that this is actually not the case.We also compare our optimized coefficients with some existing optimized coefficients for high-order FD operators [3]. Bogey and Bailly call the 8th-, 10th- and 12th-order operators here as 9-, 11- and 13-point stencils. Fig. 6show the difference between our results and their results. For each order listed, our optimized operator generally has quite similar wave number range but much smaller maximum error. For example, our operators have an error limitation of only 0.0001 (see the red curves), but theirs have error limitations of 0.0011, 0.0002 and 0.0006 (see the blue curves), respectively. If we use much looser error limitation, such as 0.0005 (see the green bold curve), the wave number coverage will be much wider compared with the blue bold curve. We do not show the waveform comparison since the difference in waveform is not so significant in small scale model or short duration of the record. However, note that a big error in wave number domain always has a big risk in the presence of either long-period or over-size problems.As shown in Fig. 7(corresponding to σ=8), the optimized FD operator using 0.005 is apparently superior to Tam and Webb’s operator that uses 0.01; in addition, the optimized FD operator using the error limitation of 0.0001 is better than that using 0.005. Fig. 8(corresponding to σ=3) shows the case of the 12th-order optimized FD operators, which also indicates that a small error limitation is better than a large error limitation. Therefore, we should use a small error limitation from now on rather than purely pursuing much wider wave number coverage by arbitrarily relaxing the error limitation.In this section, we consider the 2D scalar wave equation(27)∂2u∂x2+∂2u∂z2=1ν2(x,z)∂2u∂t2+δ(xs,zs)S(x,z;t),where t is the time, v(x,z) is the 2D velocity function, and u≡u(x,z;t) is the wave field. We take the Ricker wavelet(28)S(x,z;t)=(1-2π2ω2t2)exp(-π2ω2t2)as the initial waveform, where ω is the dominant frequency.First, we illustrate the above absolute-error analyses by impulse responses in a 2D homogeneous medium, where-2500⩽x⩽2500m and-2500⩽z⩽2500m with a uniform grid spacing of 5m. The source location is located at xs=0m and zs=0m. The velocity is v=2000m/s and the dominant frequency ω=50Hz. Note that the dominant frequency and the velocity used here almost reach the upper limit that is fairly difficult to handle in practice, and the scale of the model is also typical in practical applications in geophysical exploration (e.g., [25]). Figs. 9(a)–(c) show the wavefield snapshots at 3s, 6s and 9s, respectively. Fig. 10shows the local details of Fig. 9. Obviously, the optimized 12th-order FD methods obtain much better results compared with the conventional 12th-order FD method. In addition, the results obtained by the optimized 12th-order FD method are quite similar to those obtained by the conventional 24th-order FD method. Figs. 9 and 10 indicate that the improvement after using our optimization scheme is significant, even for the large-scale model at a long travel time. Note that these numerical experiments show perfect agreement with the theoretical analyses in the previous section.To verify the capabilities of our optimized FD operators, we simulate the wave field propagations on a modified SEG/EAGE salt model [1]. Fig. 11(a) shows the velocity model and Fig. 11(b)–(d) show the waveforms at three time windows. For convenience of comparison, we take the waveforms generated by the conventional 36th-order FD method as references, as shown by the dashed curves. We see that the waveforms generated by the conventional 12th-order FD method evidently deviate from the reference waveforms due to numerical dispersion. In contrast, the waveforms obtained by the optimized 12th-order FD method are almost the same as those obtained by the conventional 24th-order FD method. In addition, the waveforms obtained by the optimized 12th-order FD method only show slight differences from the reference waveforms at 10s (see Fig. 11(d)). Fig. 11 indicates that the optimized FD method is superior to the conventional FD method for the same order. Again, these conclusions are consistent with the theoretical and numerical analyses in the previous sections.

@&#CONCLUSIONS@&#
We present a new optimization scheme to reduce the numerical dispersions of high-order explicit FD methods. The objective functions are constructed with the maximum norm rather than the traditional 2-norm; in addition, we solve the objective functions using the simulated annealing algorithm rather than the traditional least squares. The maximum norm provides us with the largest number of possible solutions, which greatly enhances the possibility of finding the optimized coefficients for the simulated annealing algorithm over a vast solution set.We show that the error limitation is essential for solid accuracy improvements. A small error limitation is superior to a large error limitation, although we may draw the opposite conclusion according to the theoretical analyses. This indicates that we should use a small error threshold (e.g., 0.0001) to guarantee accuracy for large-scale modeling with long travel times, rather than purely pursuing the accurate wave number coverage by arbitrarily relaxing the error limitations.For both the first and second spatial derivatives, our optimized 8th-order FD method has the same accuracy as the conventional 12th-order FD method, and our optimized 12th-order FD method has the same accuracy as the conventional 24th-order FD method. This means we can greatly save on both memory demand and computational cost when using our optimized high-order FD methods.