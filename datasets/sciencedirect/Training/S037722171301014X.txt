@&#MAIN-TITLE@&#
Column-generation based bounds for the Homogeneous Areas Problem

@&#HIGHLIGHTS@&#
We model the organization of a help-desk as a new combinatorial optimization problem.We provide a compact and an extended ILP formulation of the problem.We compute a lower bound with a column generation technique.We compute an upper bound with a Least Discrepancy Search heuristic.We apply the methods to real-world, realistic and random instances.

@&#KEYPHRASES@&#
Graph partitioning,Column Generation,Tabu Search,

@&#ABSTRACT@&#
Given an undirected graph and a collection of vertex subsets with suitable costs, we consider the problem of partitioning the graph into subgraphs of limited cost, splitting as little as possible the given subsets among different subgraphs. This problem originates from the organization of a region (the graph) including several towns (the vertices) into administrative areas (the subgraphs). The officers assigned to each area take care of activities which involve several towns at a time (the subsets). An activity involving towns from more areas engages the officers of all those areas, leading to redundancies which must be minimized.This paper introduces a column generation approach to compute a lower bound for the problem. Since the pricing subproblem isNP-hard, we solve it with a Tabu Search algorithm, before applying a suitably strengthened multi-commodity flow formulation. Moreover, we also compute an upper bound for the overall problem with a primal heuristic based on the idea of diving and limited discrepancy search. The computational results refer to two real-world instances, a class of realistic instances derived from them, and two different classes of random instances.

@&#INTRODUCTION@&#
The Homogeneous Areas Problem (HAP) can be formulated as follows. LetG=V,Ebe an undirected graph withV=1,…,n,S⊆2Va collection of subsets of vertices,q:S→R+a cost function defined onSand Q a cost threshold. Finally, let k be an integer positive number. Given any subset of verticesU⊆V, we denote the cost of the subgraph induced by U as∑S∈S:S∩U≠∅qS, i.e. the sum of the valuesqSfor all subsets S intersecting U. The problem requires to partition graph G into at most k vertex-disjoint connected subgraphsGi=Ui,Eisuch that the cost ofGidoes not exceed Q for all i and the total cost(1)ϕ=∑i∑S∈S:S∩Ui≠∅qSis minimum. The HAP is stronglyNP-hard (Ceselli, Colombo, Cordone, & Trubian, in press).This problem derives from a practical requirement, concerning the partitioning of two administrative regions in Northern Italy (the provinces of Milan and Monza) into “homogeneous areas”. In that case, the vertices correspond to towns, the edges to pairs of adjacent towns, each subsetS∈Srepresents an activity involving a subset of towns and requiring from the officers of the province administration a known amount of working hours,qS. The aim of the problem is to divide the province into connected areas (subgraphs) and to assign a team of officers to each area, in such a way that each activity is split as little as possible among different areas. This is due to the fact that the officers in charge of an area need to be trained on all the activities involving the towns of the area and, therefore, splitting an activity implies a redundancy (more officers trained on the same topics). The cost of a subgraph expresses the number of working hours required from the officers in charge of the corresponding area. The limited number of working hours available for each officer imposes an upper threshold on the cost of each area. The value of this threshold can also be tuned to improve fairness among the teams.In Fig. 1we report a sample instance and some of its solutions. Fig. 1(a) provides a graph G with 7 vertices and 9 edges, three subsets with costsqS1=10,qS2=9andqS3=11, a cost thresholdQ=25and a maximum number of subgraphsk=3. If the nodes could be partitioned so as to keep all subsets inSunsplitted, the overall cost would hit the theoretical lower bound∑S∈SqS=30. This value, however, cannot be reached due to the cost threshold imposed on each subgraph. Fig. 1(b) shows an optimal solution, with two subgraphs and a total cost equal toqS1+2qS2+qS3=39(subsetS2intersects both the subgraphs). Fig. 1(c) shows a suboptimal solution with three subgraphs, in which bothS1andS2intersect two subgraphs, so that the overall cost is2qS1+2qS2+qS3=49. Finally, the solution in Fig. 1(d) is unfeasible since the subgraph induced byU=1,2,3,4,5intersects all three subsets and its costqS1+qS2+qS3=30exceeds the thresholdQ=25.This work proposes a column generation approach to the HAP. Since in our decomposition the pricing subproblem is itselfNP-hard, we apply a customized heuristic before solving it exactly with an Integer Linear Programming (ILP) formulation, for which we introduce some valid inequalities. The heuristic is a Tabu Search algorithm, while the exact approach exploits a multicommodity flow formulation. Section 2 introduces a compact formulation of the problem, and an extended one, solved by a column generation approach. Section 3 deals with the pricing subproblem, discussing its formulation with some strengthenings, its computational complexity and a heuristic approach to solve it. Section 4 presents a primal heuristic for the HAP. It is based on the column generation framework and exploits the concepts of diving and limited discrepancy search. The final section presents the computational results.The HAP can be seen as a variant of the Graph Partitioning Problem (GPP). This section provides some references to the huge literature on the GPP, a small example to illustrate the specific features of the HAP and a discussion of the differences it exhibits with respect to the other related models. A more detailed discussion, with counterexamples to the possibility of easily reducing the HAP to a standard GPP, can be found in Ceselli et al. (in press).Given an undirected edge-weighted graphG=(V,E), the most common versions of the GPP ask to divide the vertex set V into a given number k of nonempty, pairwise disjoint subsets, such that the edge-cut, i.e. the total weight of the edges that connect vertices in different subsets, is minimized. This basic problem admits a number of variations; see, e.g., the survey in Fjällström (1998). Several different approaches have been proposed to solve them, such as hierarchic multi-level heuristics (Sanders & Schulz, 2011), geometry-based and flow-based methods (Arora, Rao, & Vazirani, 2008), genetic approaches (Kim, Hwang, Kim, & Moon, 2011), spectral methods (Donath & Hoffman, 1973), mathematical programming approaches (Fan & Pardalos, 2010), local search metaheuristics and integrated approaches (Osipov, Sanders, & Schulz, 2012). The HAP differs from these classical GPPs both in the constraints and in the objective function, which pose specific challenges to a solving algorithm.In the GPP, the number k of vertex-disjoint subsets is usually given, and the subsets are required to be nonempty, since their cardinalities,n1,…,nk, with∑j=1knj=|V|are explicitly imposed (Guttmann-Beck & Hassin, 2000) or constrained to be approximately of the same size, see e.g. Osipov et al. (2012). In the HAP, k is just an upper threshold, so that the subsets of verticesUiare allowed to be empty. In fact, merging two subsets into a single one is always profitable, and only the cost threshold Q possibly forbids to do it.The HAP is related to the Node-capacitated Graph Partitioning Problem (Ferreira, Martin, de Souza, Weismantel, & Wolsey, 1998), in which the total weight of each subset in the partition is limited by a threshold. However, the threshold is managed differently in the HAP: since it is not associated to single vertices, the cost of a subset U does not increase linearly as new vertices are included, but stepwise as new subsetsS∈Sintersect U. Such a nonlinear dependence is much harder to handle.The connectivity constraint is usually not imposed in GPPs, where the edges of the graph are taken into account only when computing the objective function. Quite commonly, the edge costs model a proximity measure, and the subsets end up naturally to be connected in the optimal solution. In the HAP, on the contrary, the edges determine the feasibility of the solutions, since each subset must induce a connected subgraph on G, but they have no relation with the objective function. In fact, even considering the smaller benchmark instances, which can be solved exactly, the optimal result obtained relaxing the connectivity constraints is on average 35% lower than the one obtained respecting them (Ceselli et al., in press). This suggests that neglecting the connectivity constraint would not provide meaningful information on the original problem and that classical methods ignoring this constraint would not provide useful solutions.The objective function of the classical GPPs depends linearly on the cost of the edges whose extreme vertices belong to different subgraphs. Sometimes, this cost is tuned by a function of the cardinality of the subgraphs; see, e.g., Matula and Shahrokhi (1990). The objective function of the HAP is completely independent from the edge set E, and depends nonlinearly on the intersections between the subsets inSand the subsets of vertices of the subgraphs.These remarks on the difference between the constraints and the objective function of the HAP with respect to other GPPs have moved us to develop ad hoc methods, instead of straightforwardly adapting algorithms drawn from the literature.Hereafter, we present two different formulations of the HAP. The first one is a compact multicommodity flow formulation that can be directly solved using a commercial ILP solver. The second one is an extended formulation which associates a variable to each feasible subgraph. At the end of the section, we describe the column generation approach used to solve the continuous relaxation of the extended formulation.The HAP admits a multicommodity flow formulation based on an auxiliary directed graphG′=V,E′, derived from G replacing each edge in E with two opposite arcs. This allows to enforce the connectivity requirement by representing each subgraph as a rooted arborescence. There are at most k roots, i.e. nodes generating flow. Each other node receives flow generated from exactly one root, absorbs a single unit of flow and transmits the rest. The indexℓof the root denotes in the model both the corresponding commodity and the arborescence whose nodes receive the flow. To avoid multiple equivalent solutions, without loss of generality, we require the root of each arborescence to be the node with minimum index. Therefore, any arborescence rooted inℓis restricted to the subgraphGℓ=Vℓ,Eℓ, whereVℓ=v∈V:v⩾ℓandEℓ=u,v∈E′:u⩾ℓ,v⩾ℓ. For example, referring to Fig. 1a, subgraphG4consists of four vertices (V4=4,5,6,7) and two edges (E4=4,5,6,7). Finally, for each nodev∈Vwe denote the collection of subsets fromSwhich contain v asSv=S∈S:v∈S. For example,S4=S2,S3.Given the following decision variables:•xvℓ=1if an arborescence rooted inℓincludesv∈Vℓ,xvℓ=0otherwise;zSℓ=1if an arborescence rooted inℓintersectsS∈S,zSℓ=0otherwise;fuvℓis the flow on directed arcu,v∈Eℓgenerated by nodeℓ∈VEq. (2a) defines the objective functionϕ. Constraints (2b) state that each vertex belongs to exactly one arborescence. Constraint (2c) imposes the correct number of arborescences, sincexℓℓ=1if and only if the solution contains an arborescence rooted inℓ. Constraints (2d) state that, ifℓis the index of an arborescence, its cost should not exceed the threshold. Constraints (2e) state that, if a node v is assigned to an arborescence, all subsetsS∈Swhich contain v contribute to the cost of the arborescence. Constraints (2f) state that if a node belongs to arborescenceℓ, nodeℓis the root of the arborescence. Constraints (2g) state that the root of an arborescence generates one unit of flow for each other node of the arborescence. Constraints (2h) state that, if a node receives flowℓ, it belongs to arborescenceℓ. CoefficientVℓcan be replaced by any upper bound on the number of the nodes in arborescenceℓ, since each node absorbs exactly one unit of flow. As discussed in the following, tighter values should be preferred because they strengthen the continuous relaxation of the model. Constraints (2i) guarantee the conservation of flow, while the following ones impose integrality or nonnegativity on the decision variables. Notice that, if thexvℓvariables are binary, constraints (2e) and the objective function trivially guarantee that also thezSℓvariables are binary. Formulation (2) can be strengthened by fixing the values of some variables and introducing additional logical constraints, as described in Ceselli et al. (in press).It is possible to derive from Formulation (2) an alternative one, which implies a column generation approach. LetGℓbe the collection of all connected subgraphs of G which haveℓas the node with minimum index and whose cost is not larger than Q. We define a binary variableyiℓfor each element ofGℓ, withi=1,…,Gℓ, such thatyiℓ=1if the ith subgraph is used in the solution,yiℓ=0otherwise.(3a)minϕ=∑ℓ∈V∑i=1|Gℓ|ϕiℓyiℓ(3b)∑ℓ∈V∑i=1|Gℓ|aivyiℓ=1v∈V(λvfree)(3c)∑ℓ∈V∑i=1|Gℓ|yiℓ⩽k(μ⩽0)(3d)yiℓ∈{0,1}ℓ∈V,i=1,…,Gℓwhereϕiℓis the cost of the ith subgraph andaivindicates whether node v belongs to the ith subgraph or not.Eq. (3a) defines the objective functionϕ. Constraints (3b) state that each node should belong to exactly one subgraph; eachλvrepresents the dual variable of the corresponding constraint in the continuous relaxation of the problem. Constraint (3c) imposes the correct number of subgraphs;μrepresents the corresponding nonpositive dual variable.To solve Formulation (3) we start by relaxing the integrality conditions on the binary variablesyiℓ, generating the so called master problem (MP). Since the MP has an exponential number of variables, we apply a column generation approach (Desrosiers & Lübbecke, 2005). In particular we define the reduced master problem (RMP) that, at the beginning, considers only a small collection of feasible subgraphs,G‾ℓ, for eachℓ∈V. Then we solve the RMP and we use the optimal dual values either to generate non-basic variables with negative reduced costs or to prove that the current basic solution is optimal also for the full MP. This requires to solve, for eachℓ∈V, the so called pricing problem (PPℓ), as described in Section 3. When the full MP is solved to optimality, its optimum provides a lower bound for the original problem.Hereafter, we present two different approaches to solve each pricing problem PPℓ. The first one is a flow formulation which can be solved exactly by a commercial ILP solver. The second one is a Tabu Search heuristic, which is used to quickly identify variables with negative reduced costs. Section 5.3 describes the strategy used to combine these two approaches to efficiently obtain a lower bound for the HAP. Section 4 describes the role of the Tabu Search heuristic as a component of a primal heuristic for the HAP.The pricing subproblem for each nodeℓ∈Vlooks for a connected subgraph inGℓwith the minimum reduced cost. We represent each subgraph as a rooted arborescence to enforce the connectivity requirement, as already done for the compact formulation. The pricing subproblem PPℓ can be formulated introducing the following decision variables:•xv=1if the arborescence includes nodev∈Vℓ, andxv=0otherwise;zS=1if the arborescence intersects subsetS∈S, andzS=0otherwise;fuvis the flow on directed arc(u,v)∈EℓThe objective functionϕ¯ℓ, given by Eq. (4a), is the reduced cost of a column (of indexℓ) of Formulation (3). It can be also interpreted as the difference between the cost of the column, on one hand, and the sum of the prizesλvof the nodes included in the corresponding subgraph plus the constant termμ, on the other hand. Hereafter, for any feasible solution of Formulation (4), we will denote the term∑S∈SqSzSas the cost of the corresponding arborescence, as opposed to its reduced cost, which is the objective of the pricing subproblem. Constraint (4b) limits the cost of the arborescence. Constraints (4c) state that, if a node v is assigned to the arborescence, all subsetsS∈Swhich contain v contribute to the cost of the arborescence. Constraint (4d) guarantees that the root node belongs to the arborescence. Constraint (4e) sets the amount of flow generated by the root of the arborescence equal to the number of the other nodes belonging to it. Constraints (4f) state that, if a node receives one unit of flow, it must belong to the arborescence. Constraints (4g) guarantee the conservation of flow, while the following ones impose integrality or nonnegativity on the decision variables.The problemPPℓisNP-hard. Let us consider the problem PP which requires to identify simultaneously both the rootℓand the corresponding minimum reduced cost arborescence among the optimal solutions of all thePPℓfor eachℓ=1,…,n. It is straightforward to observe that, in order to solve PP, it is sufficient to solve all thePPℓproblems and to identify the solution with minimum cost. Hence, if we can solve eachPPℓin polynomial time, we can also solve PP in polynomial time. The following proposition proves that PP isNP-hard, thus implying thatPPℓisNP-hard, too.Proposition 1The PP problem isNP-hard, even ifλv⩾0for eachv∈VandQ=+∞.The proof is based on a reduction from the Maximum Weight Connected Subgraph (MWCS) problem, which isNP-hard (Ideker, Ozier, Schwikowski, & Siegel, 2002) and is defined as follows. Given a graphG∼=(V∼,E∼), and a weight function defined on the vertices,w:V∼→R, find a connected subgraphG∼′=(V∼′,E∼′)ofG∼with maximum total weightwG∼′=∑v∈V∼′wv.We now show that from any given instance of the MWCS problem, it is possible to build an instance of problem PP, such that their optimal solutions correspond one-to-one. GraphG=(V,E)coincides withG∼. We setQ=+∞andμ=0. For each vertexv∈V∼such thatwv<0, we define a singleton subsetS=v∈Swith costqS=-wvand we setλv=0. For each vertexv∈V∼such thatwv⩾0, we setλv=wv.Each feasible solutionG′=(V′,E′)of problem PP is a connected subgraph, and therefore is also feasible for the MWCS problem. Its cost is equal to∑v∈V′:wv<0(-wv)-∑v∈V′:wv⩾0wv=-∑v∈V′wv, which is the opposite of the objective function of the MWCS. Thus, minimizing the objective of PP corresponds to maximizing the objective of the MWCS.□Formulation (4) can be strengthened by adapting the improvements introduced in Ceselli et al. (in press) for the compact formulation (2). These improvements introduce new constraints and fix the value of some variables. First of all, the cost constraint (4b) induces logical constraints on the node variablesxv. These constraints derive from the computation of lower bounds on the cost of the solution, that is on the left-hand side of constraint (4b), under the assumption that one or two nodes different from the given rootℓare imposed or forbidden in the solution. If any such lower bound exceeds Q, the constraint is certainly violated and the assumption must be reversed. Since all feasible solutions are connected subgraphs, lower bounds can be obtained computing the minimum cost of simple paths on auxiliary node-weighted graphs. In particular, given the subgraphGℓdefined above, we introduce the auxiliary cost functionpℓut=∑S∈St⧹(Sℓ∪Su)qS|VS|t∈Vℓ⧹ℓ,u0t∈ℓ,uwhose valuepℓu(t)represents a lower bound on the contribution of node t to the cost of any arborescence rooted inℓand including node u. In addition, the cost of such an arborescence must necessarily include the cost of all subsets inSℓandSu.Proposition 2Given a rootℓ∈Vand a nodeu∈Vℓ, letπℓube the minimum cost of a path on subgraphGℓbetween nodesℓand u, with respect to the cost functionpℓu. Ifπℓu+∑S∈(Sℓ∪Su)qS>Q, thenxu=0in any feasible solution of Formulation(4).Any feasible solution of the pricing problem PPℓ is a connected subgraph including vertexℓ. Let us assume, by contradiction, that the solution also includes vertex u. The minimum possible left-hand side of constraint (4b) is associated with a path connectingℓand u. Its cost is underestimated byπℓuplus the total cost of the subsets containingℓand u. If this underestimate exceeds the cost threshold Q, the assumption must be reversed, fixingxu=0.□Given a rootℓ∈Vand two nodesu,v∈Vℓwithu≠v, letπ¬vℓube the minimum cost of a path on subgraphGℓbetween nodesℓand u which does not use v, with respect to the cost functionpℓu. Ifπ¬vℓu+∑S∈(Sℓ∪Su)qS>Q, then any feasible solution of Formulation(4)satisfies the following binding constraintxu⩽xvLet us now assume, by contradiction, that the solution includes vertex u and does not include vertex v. The minimum possible left-hand side of constraint (4b) is associated with a path connectingℓand u, and not visiting v. The cost of such a path can be underestimated byπ¬vℓu, plus the total cost of the subsets containingℓand u. If the underestimate exceeds Q, the assumption must be reversed, stating that if the solution includes u, it necessarily includes also v.□A complementary implication can be derived by defining the following auxiliary cost functionpℓuv(t)=∑S∈St⧹(Sℓ∪Su∪Sv)qS|VS|t∈Vℓ⧹u,v0t∈ℓ,u,vwhose valuepℓuvtrepresents a lower bound on the contribution of node t to the cost of any arborescence rooted inℓand including u and v.Proposition 4Given a rootℓ∈Vand two nodesu,v∈Vℓ⧹ℓwithu<v, the minimum cost of a path betweenℓand u (ℓand v, or u and v) with respect to the cost functionpℓuv, plus the constant term∑S∈(Sℓ∪Su∪Sv)qSis a lower bound on the cost of any arborescence rooted inℓincluding both u and v. If any of these three lower bounds exceeds Q, then any feasible solution of Formulation(4)satisfies the following incompatibility constraintxu+xv⩽1By contradiction, we assume that the solution includes both vertex u and vertex v, besides the rootℓ. We underestimate the left-hand side of constraint (4b) by requiring the solution to connect two of the three vertices, instead of all of them. This yields three possible lower bounds, given by the total cost of the subsets containingℓ,uand v, plus the minimum cost of a path between two of the three vertices. If any of these underestimates exceeds Q, the assumption must be reversed, stating that u and v cannot be both included in the solution.□The|Vℓ|-1coefficient of Constraints (4f) can be eventually reduced by replacing|Vℓ|with any upper boundMℓon the number of nodes which can belong to a feasible arborescence rooted inℓ.Proposition 5The optimum of the following problem provides an upper bound on the number of nodes of a feasible arborescence rooted inℓ.(5a)maxMℓ=∑v∈Vℓxv(5b)∑S∈SqSzS⩽Q(5c)xℓ=1(5d)xv⩽zS∈Vℓ,S∈Sv(5e)xv∈{0,1}v∈Vℓ(5f)0⩽zS⩽1S∈SThe problem can be obtained from the pricing subproblem PPℓ by relaxing all constraints concerning the flow variables, that is by neglecting the connection requirement. The objective function maximizes the number of vertices in the subgraph. This necessarily provides an upper bound on the number of vertices of each feasible solution of PPℓ.□Of course, the bound improves if one includes in Formulation (2) the fixings, the binding and the incompatibility constraints described in Propositions 2–4.It is worth noticing that all the bounds and strengthenings described above depend on the labels assigned to the nodes when defining set V as1,…,n. In fact, they depend on subgraphGℓ, which strictly depends on these labels. Consequently, different labellings of the nodes could yield more or less effective strengthenings. According to our experience, a good heuristic labelling can be obtained by computing for each node v the number of incident arcsδvand the total cost of the subsets which include v, i.e.γv=∑S∈SvqS, and sorting the nodes, first by nonincreasingδvand then by nonincreasingγv.Since the pricing subproblem isNP-hard and since the systematic application of an ILP solver proved rather inefficient, we developed a Tabu Search heuristic, PrTS, to quickly identify negative reduced cost columns. Hence, we limit the use of the ILP solver to the cases in which the heuristic PrTS fails to provide any.Tabu Search is a well-known local search metaheuristic approach which allows the visit of nonimproving solutions. It is controlled by memory mechanisms to avoid the insurgence of cyclic behaviours. It was introduced by Glover (1986) and the interested reader can find in Glover and Laguna (1997) a detailed treatment of its applications and variants. In the following, we mainly focus on the specific aspects of our implementation.First of all, the heuristic PrTS does not impose a fixed root node, thus implicitly solving the overall pricing problem PP instead of the single subproblems PPℓ for eachℓ∈V. PrTS starts from a given feasible solutionG′=(U,E′), which corresponds to a subgraph of G, possibly empty. We defined two simple moves: the addition (removal) of a node to (from) the current set of nodes U. At each iteration, the heuristic evaluates all nodesv∈V, one at a time: ifv∈U, it computes the value of the reduced cost of the graph induced byU⧹{v}; ifv∈V⧹U, it computes the value of the reduced cost of the graph induced byU∪{v}. In either case, the move is forbidden if the resulting subset of nodes does not induce a connected subgraph or if its cost exceeds the threshold Q. The neighbourhood of each solution, therefore, is the set of all the solutions which can be obtained by applying one of the two kinds of move, and it contains at most n members.As for any Tabu Search method, PrTS classifies the solutions in the current neighbourhood either as tabu or non-tabu. The presence of a tabu mechanism has the purpose to avoid visiting solutions obtained previously by forbidding to reverse the effect of moves performed too recently. The common way to implement this mechanism is to maintain, explicitly or implicitly, a list of attributes of performed moves and to forbid the execution of moves whose attributes are in the list. The list has a limited length, say tt, usually called tabu tenure, and it is managed as a FIFO list. This implies that, after lasting tt iterations in the list, an attribute is removed from it and all the moves which have that attribute can now be performed.In our algorithm, for each nodev∈V, we save inIvthe last iteration in which v changed status, either entering or leaving the solution. In technical terms, the attribute of a move is the index v of the node which is removed or added. As a consequence, the solution obtained adding (removing) a node v to (from) the current set of nodes U is tabu if the value of the current iteration counter is smaller thanIv+tt, meaning that v was moved for the last time less than tt iterations ago.In the Tabu Search literature there are two mainstreams, respectively adopting a fixed and a variable tabu tenure. In the latter case, the tenure is usually updated depending either on the quality of the last move performed or on the cardinality of the neighbourhood. More specifically, it is common to decrease the value of the tenure when the last move performed is improving and to increase it in the opposite case; as well, it is common to decrease the value of the tenure when the neighbourhood becomes smaller and to increase it when it becomes larger (Glover & Laguna, 1997). The purpose of these adaptive mechanisms is to favour the exploration of more promising regions of the solution space and to drive the search away from less promising ones.Since we visit only feasible solutions, which correspond to connected subgraphs of G, and since in general graph G is not complete, the size of the neighbourhood defined above can vary significantly from iteration to iteration. For this reason, the use of a fixed tabu tenure proved very ineffective, and even the standard adaptive mechanisms, based on the quality of the last move or on the size of the current neighbourhood, failed. In fact, we frequently observed that the value of the tabu tenure could not keep pace with the current situation. For example, the moves whose attributes were saved in the tabu list were quite often nearly all unfeasible, and therefore unnecessarily tabu. On the other hand, the insurgence of a cyclic behaviour triggered the standard anti-cycling mechanism of increasing the tabu tenure, until nearly all feasible moves became tabu. This worsened the quality of the available solutions. The result was that the search moved alternatively between cycles and bad solutions.In order to solve this problem, we decided to get rid of the tabu tenure, while preserving the basic idea of Tabu Search. At each iteration, we compute the number k of feasible moves and we consider tabu the∊kmoves with the most recent attributeIv. Parameter∊∈0;1is defined by the user. Please notice that, due to the above rounding and since∊<1, at least one move is always non-tabu.In general, the move selected at each step is the one which produces the non-tabu solution with the minimum reduced cost in the neighbourhood, but we also apply the standard aspiration criterion: if a solution is tabu, but its reduced cost is the smallest one found so far, the tabu status is overridden.We also apply the following anti-cycling mechanism: if for a given number of consecutive iterationsKacmthe same sequence of moves generates the same sequence of objective function values, we assume this as a hint that a cyclic behaviour is occurring, and consequently increase∊to∊′∈∊;1for otherKacmiterations, in an attempt to break the cycle.Finally, we adopt a frequency-based diversification strategy. We save the numbernvof visited solutions which contain nodev∈Vand the numbernSof visited solutions which contain subsetS∈S. If the objective function does not improve forKniconsecutive iterations, we start a diversification phase, which lasts forKdivconsecutive iterations. During this phase, we replace the objective function with the following oneϕ̃=∑S∈SnSmaxS∈SnSqSzS-∑v∈V1-nvmaxv∈VnvπvxvThe aim of this change is to decrease the cost of the subsets and to increase the prize of the nodes which have occurred less frequently in the visited solutions.Heuristic PrTS requires a starting solution. As we embed it in a column generation approach, we restart PrTS from the subgraphs associated with all thek′basic variablesyiℓwhich have a strictly positive value in the current optimal solution of the RMP. These solutions are promising starting points because, by definition, they have a zero reduced cost. Our experiments show that, using this warm start strategy, the overall column generation algorithm requires less computing time.To improve the column generation convergence rate, instead of the best solution, we save all the negative reduced cost columns found by PrTS, checking them so as to avoid duplicates (in general, the limited memory mechanism of Tabu Search allows to visit the same solution more than once). When the heuristic terminates, we add all the saved columns to the RMP.PrTS has three stopping criteria. First of all, it stops as soon as it has foundCmaxcolumns. In fact, adding several columns in each iteration decreases the number of iterations required to obtain the optimal solution, but also increases the time required to solve the RMP. So, we need to find a trade-off between these two effects. Second, for each of thek′starting solutions, PrTS performs at leastImin/k′iterations. If during this search it finds at least one negative reduced cost column, it moves to the next starting solution. Otherwise, it proceeds until either it finds a negative reduced cost column or it performsImax/k′iterations, and moves to the next starting solution.Besides a lower bound, the column generation approach also provides useful information to build good heuristic solutions. In the literature, there are different strategies to exploit such information. For example, Cacchiani, Hemmelmayr, and Tricoire (2014) propose an effective heuristic for the Periodic Vehicle Routing Problem, based on the combination of column generation with Tabu Search, while Prescott-Gagnon, Desaulniers, and Rousseau (2009) combine branch and price with Large Neighbourhood Search (Pisinger & Ropke, 2010) to obtain quasi optimal solutions for the Vehicle Routing Problem with Time Windows. In this paper, we develop a heuristic for the HAP following another strategy, proposed in Joncour, Michel, Sadykov, Sverdlov, and Vanderbeck (2010).This algorithm, denoted in the following as HAP-LDS, can be seen as a truncated exploration of a branching tree in which the branching variables are columns of the extended formulation. Since the pricing problem isNP-hard, when we process a node of the branching tree, the associated MP is solved only heuristically. In other words, when the Tabu Search procedure fails to find variables with negative reduced cost, instead of applying the ILP solver to Formulation (4), the column generation algorithm terminates. Hence, the final value of the RMP is not guaranteed to be a lower bound for the original problem, and the final solution of the RMP is not guaranteed to be optimal for the MP. However, this solution probably includes useful information.HAP-LDS maintains a list L of tabu columns, which is empty at the root node. At each node of the branching tree, the algorithm selects the column with the largest value in the solution of the current RMP among those which are not tabu. Then, it fixes the selected column to 1 and updates the RMP accordingly: the right-hand side of Constraints (3b) turns from 1 to 0 for the nodes belonging to the subgraphs associated to the fixed columns and the right-hand side of Constraint (3c) decreases by the number of fixed columns. In the end, the algorithm reoptimizes the RMP with column generation, always applying only the heuristic pricing procedure. In practice, it is very easy to take into account the variable fixing when solving heuristically the pricing subproblems: we just remove from graph G all nodes belonging to the columns which have been fixed to 1 so far, and solve the pricing subproblem on the remaining subgraph. The columns fixed to 0 are not forbidden in the Tabu Search procedure, but they are simply not returned to the RMP. In fact, the same check which avoids introducing duplicates in the solution pool of the pricing subproblem (see the end of Section 3.2) allows to identify these columns and to avoid returning them. The process of fixing columns and reoptimizing the RMP is called diving. It terminates either when the current RMP has an integer solution or when it becomes unfeasible. The best integer feasible solution is saved. Then, the search backtracks, guided by two parameters: the maximum depthDmaxand the maximum lengthLmaxof the tabu list. In detail, the backtracking stops when the current depth becomes<Dmaxor when the length of the current tabu list becomes<Lmax. When it is no longer possible to backtrack, HAP-LDS terminates. Otherwise, it creates a new child node, whose tabu list includes all the columns which were tabu in the parent node plus those that have been fixed in the previous sibling nodes.An example of branching tree is illustrated in Fig. 2, forDmax=2,Lmax=2. The labels of the branching nodes indicate the order in which they are visited. The label of each arc reports in round parenthesis the tabu list L which constrains the choice of the next fixed column and the column which is fixed into the solution. Let the candidate columns at the root node beya,ybandyc, in nonincreasing order of value. After fixing columnyaand reoptimizing, let the candidate columns at node 1 beyd,yeandyf. Since list L is empty, algorithm HAP-LDS choosesydand reoptimizes the RMP. Then, it “dives”, i.e. it keeps fixing other columns until the current RMP has an integer solution or becomes unfeasible. It backtracks up to the first level whose depth is<Dmax=2, i.e. up to node 1, inserts columnydinto list L and creates a new child node 3, fixing columnye. From there, the algorithm dives again, and backtracks once more up to 1. Now, it inserts columnyeinto list L and creates a new child node 4, fixing columnyf. From node 4, HAP-LDS first dives and then backtracks up to node 0; node 1, in fact, cannot generate other children, because the length of list L has grown equal toLmax=2. Back at the root node, the algorithm puts columnyainto L and fixes columnyb. Then, it proceeds as reported in Fig. 2. In particular, notice how parameterLmaxlimits the number of children at node 5 and directly imposes to dive at node 8.The best feasible integer solution found during the diving phases is saved. Since the HAP is a partitioning problem and imposes an upper limit k on the number of subgraphs, the algorithm does not guarantee to always obtain a feasible solution. At the end of the branching process, however, the columns generated in all branching nodes (including the root) form an ILP problem, which is a reduced instance of Formulation (3). We solve it by means of a general-purpose solver, possibly obtaining a feasible solution, which cannot be worse than the best one found (if any) during the diving phases.In this section, we first describe the benchmark instances used, then we report the comparison between the lower bounds computed with the column generation approach and with the multicommodity flow formulation (2). In the end, we compare the upper bounds obtained by the primal heuristic of Section 4 with those obtained by the alternative heuristic VLNS-TS proposed in Ceselli et al. (in press) and with the best known lower bounds. All the algorithms presented above have been implemented in C++ language and run on an Intel Pentium Core 2 Duo E6700 2.6gihahertz with 3gigabytes of RAM. The LP and ILP problems have been solved with CPLEX 12.2.We tested our algorithms on two real-world instances, on a benchmark set A of 25 instances extracted from the real ones and on two benchmark sets B and C of, respectively, 72 and 60 randomly generated instances. All the tested instances are available at http://homes.di.unimi.it/cordone/research/hap.html.The real-world instances correspond to the Italian provinces of Milan and Monza: the former has 134 vertices and 774 subsets, the latter 55 vertices and 426 subsets. The adjacency graph G derives from geographical data. The number k of areas, the subsetsS, the costqSof each subset and the threshold Q have been provided by the staff of the two provinces. In particular,k=9for Milan andk=3for Monza.The benchmark set A was generated merging the graphs of Milan and Monza, which are geographically adjacent, and extracting subgraphs from their union, according to the following strategy. First, we selected the five towns involved in the most costly activities, that is with the largest value of∑S∈SvqS. Starting from each seed town, we extracted the first n vertices found during a breadth-first visit of the adjacency graph, withn=50,60,70,80and 90. This allowed to produce five different instances with a shape reasonably similar to a standard province and centred on a reasonable main town. We included in each instance all the subsets containing the extracted towns. We set the cost threshold Q to the same value used for the whole province, and identified reasonable values for the maximum number of subgraphs, k, running algorithm VLNS-TS. This produced5·5=25instances overall.In the random benchmarks B and C, the structure of graph G, and the cardinality of the subsetsS∈Smirror those of the real-world instances, but the vertices composing each subset are randomly distributed. The graph G is a random planar graph built as follows. First, we uniformly generate n points in a Euclidean squared plane. Then we build a triangulation of these points: we consider, in turn, all pairs of points by non-decreasing distances, and we draw the corresponding segment if and only if it does not cross the previous ones. Each point turns into a node and each segment into an edge of graph G. Given the number of nodes, n, the number of subsets is|S|=2n, and the number of subgraphs isk=n/5. The collection of subsetsSis randomly generated, devoting special care to guarantee that each node belongs to at least one subset and that no subset is empty. The average cardinality of the subsets S is set toα|S|, withα∈{0.05;0.1}.The two random benchmark sets differ with respect to the distribution of the values of functionqS. In B, the distribution mirrors the rather involved way in which costs are assigned both to subsets and to single vertices in the real-world instances (see Ceselli et al. (in press) for more details). In C, the cost of each subset directly depends on the cost of the vertices it contains. In fact, the valuesqSare computed asqS=∑v∈Swv, wherewvis either fixed to 1 or randomly extracted (with a uniform distribution) from{1,…,10}or{1,…,100}.As for the cost threshold Q, this is defined in both benchmark sets asQ=β∑S∈SqSmink,|VS|kwhich is a rough estimate of the average cost of each subgraph, where coefficientβ∈{1.00,1.15}allows to produce tight instances or loose ones.Benchmark B combines 9 sizes (n ranges from 30 to 70 by steps of 5), two average cardinalities for the subsets, two distributions forqSand two cost thresholds Q, and therefore includes9·2·2·2=72instances overall. Set C combines 5 sizes (n ranges from 30 to 70 by steps of 10), three ranges forwv(i.e. three alternative distributions forqS) and two cost thresholds Q, and therefore includes5·2·3·2=60instances.The objective functionϕas defined in Eq. (1) is equivalent to the following one:ψ=ϕ-∑S∈SqSsince we are subtracting a constant term toϕ. Functionψmeasures the excess cost with respect to the ideal situation in which no subset is split among different subgraphs. Moreover, functionψremoves an offset which unduly reduces the gap between upper and lower bounds, or between different heuristic solutions. In fact, given two solutions A and B with the corresponding objective function valuesϕAandϕB, orψAandψB, the following relation is always true:|ϕA-ϕB|minϕA,ϕB⩽|ϕA-∑S∈SqS-ϕB-∑S∈SqS|minϕA,ϕB-∑S∈SqS=|ψA-ψB|minψA,ψBIn all the following tables we report gaps referring to functionψ.This section compares the lower bound obtained by solving with CPLEX the multicommodity flow formulation (2) with the lower bound achieved by solving the extended formulation (3) with our column generation approach. Formulation (2) has been strengthened as in Ceselli et al. (in press) with inequalities and parameter settings corresponding to those given by Propositions 2–5 for the pricing subproblem.We adopt the following strategy to speed up the column generation process. At first, we apply heuristic PrTS with the parameter setting reported in Table 1. As long as PrTS finds negative reduced cost columns, we add them to the RMP, and reoptimize it. After that, for eachℓ∈Vin turn, we invoke CPLEX to solve problemPPℓ. As soon as CPLEX finds a negative reduced cost column (no matter if it is an optimal solution or not), we add it to the RMP and reoptimize, as we do with the Tabu Search heuristic. When CPLEX proves thatPPℓadmits no such column, we select the next rootℓand proceed with the associated pricing problem. When CPLEX fails to identify a negative reduced cost column for allℓ∈V, the optimal solution of the RMP is optimal also for the MP and provides a lower bound for the extended formulation. The whole process benefits by considering the rootsℓin increasing order, because in this way we solve first the larger subproblemsPPℓ, which are more likely to provide negative reduced cost columns. Notice that in our first experiments we directly applied CPLEX to solve the pricing subproblems, because we had not yet implemented the Tabu Search heuristic. With that configuration, we could not compute the continuous relaxation of the extended formulation in a reasonable amount of time even for small instances. Only after developing the Tabu Search heuristic, it became possible to solve the MP. Moreover, our analysis of the computational experiments shows that the time required to solve the smallest pricing subproblems is negligible w.r.t. the time required to solve the biggest ones. As a consequence, we did not enumerate the smallest solutions, and we did not develop any ad hoc exhaustive search procedure, but we decided to simply apply the commercial ILP solver.Tables 2and 3report the results obtained, respectively, on the benchmarks B and C of random instances, and on the instances A extracted from the real ones. The first columns of the three tables identify each tested instance, while column BK contains the best known value of the objective function. Under labelCFroot(2) we report the percentage gap between the best known value and the lower bound provided by the continuous relaxation of the compact formulation (2) (column Δ (%)). The gap is computed asBK-LB/LB. This bound can be computed in less than one second for all the considered instances of benchmarks B and C. For the instances of benchmark A, Table 4includes an additional column, labelled CPU, which reports the time required, in seconds. As the percentage gaps are huge (the upper bound is often several times larger than the lower bound), the following two columns, labelled CF(2) report the percentage gap and the computational time obtained by CPLEX, with a time limit of one hour. If an instance can be solved to optimality, the gap column reports a “–” label and the CPU column reports a value lower than 3600; otherwise, the gap column reports the residual gap and the CPU column reports the label “TL”. Finally, the columns labelledEFroot(3) report the percentage gap and the computational time required to solve the continuous relaxation of the extended formulation (3).Notice that in these tables we report only the results obtained on the smallest instances in the considered benchmarks. On the larger instances, in fact, even the best lower bounds found by CPLEX in one hour exhibited very large gaps w.r.t. the best known upper bounds.The results for the multicommodity flow formulation describe what can be achieved directly applying a general-purpose solver in a reasonable time limit. In all cases, the gap at the root node is very large. Anyway, the branching process allows to solve to optimality within one hour some instances with 30 vertices and a single instance with 35 vertices of the random benchmarks B and C, considered in Tables 2,3. By contrast, for the instances A extracted from the real ones, considered in Table 4, CPLEX is able to solve in one hour all the instances up ton=60and some instances with 70 and 80 nodes. The gap increases steeply when passing from 30 to 35–40 vertices for benchmarks B and C and from 70 to 80 vertices for benchmark A, and the computation requires to analyze hundreds of thousands of branching nodes, with a non-negligible memory consumption.The column generation approach, on the contrary, does not require any branching operation and its results are obtained in a matter of a few seconds (few minutes for the instances of benchmark A). The gap is quite stable with respect to the size of the instance and mainly depends on the value of the other parameters: the hardest instances for benchmark B are those withα=0.10,qmax=10andβ=1.15; the hardest ones for benchmark C haveα=0.10andβ=1.15. For these reasons, in the following experiments we take into account only the lower bound provided by column generation.This section compares the results of heuristic HAP-LDS, described in Section 4, with those obtained by a local search metaheuristic, denoted as VLNS-TS, which combines the Tabu Search and the Very Large Neighbourhood Search approaches (Ceselli et al., in press). Both heuristics have been executed on the same machine described at the beginning of Section 5.In order to solve the pricing subproblem, algorithm HAP-LDS uses the Tabu Search procedure PrTS with the same parameters reported in Table 1, except for the stopping criteria, which have been modified settingImin=Imax=10,000. In this case, in fact, we are mainly interested in finding a large number of columns for the final ILP model. For the other parameters of HAP-LDS, we have setLmax=2andDmax=4. As discussed in Section 4, the aim of column generation here is to quickly obtain heuristic solutions, and not to prove their optimality through the computation of a tight lower bound. Hence, we do not apply CPLEX to solve the pricing subproblems, but we terminate the generation process as soon as PrTS proves unable to identify negative reduced cost columns. For the same reason, we impose a time limit of 5000seconds on the execution of HAP-LDS, because for a few large instances the resolution of the final ILP problem requires a high amount of computational time, most of which is spent in proving the optimality of a solution found much earlier.Tables 5–7report the results obtained by the two heuristics, respectively, on the random benchmarks B and C and on benchmark A. The first column of each table reports the number of vertices n. Column CPU reports the computational time in seconds required by HAP-LDS; only on one of the largest instances of benchmark A, the computation reached the time limit of 5000seconds. For the sake of fairness, the same time was assigned, instance by instance, to algorithm VLNS-TS. The following two columns report the percentage gap between the result of HAP-LDS and the lower bound obtained by column generation, and the number of columns of the final ILP problem. The gap is computed as(UB-LB)/LB, where UB stands for the heuristic value obtained. The last two columns report the percentage gap achieved and the time required by VLNS-TS to reach its best solution. All the reported values are averaged over the instances of the same size.The performance of HAP-LDS on the random benchmark B (see Table 5) is similar to that of VLNS-TS: the average gap obtained is slightly lower and the result is better in 22 cases out of 80 versus 15 better results obtained by the competing algorithm. In order to evaluate whether these differences are statistically significant, we have applied Wilcoxon’s matched-pairs signed-ranks test (Wilcoxon, 1945), which estimates a probability ⩽0.1065 that such differences are due to random fluctuations. Hence, the two algorithms have a nearly equivalent performance, when given the same amount of time.The random benchmark C (see Table 6) appears to be harder than B. In this benchmark the cost function does not mimic the distribution of the original real-world instances, as in the first one; in fact, the costs are generated with a more straightforward random process. The performance of HAP-LDS is still similar to that of VLNS-TS, but the average gap obtained is larger and HAP-LDS obtains a better result in 5 cases out of 60 versus 17 better results obtained by VLNS-TS. Wilcoxon’s test (Wilcoxon, 1945) estimates a probability⩽0.005that such differences are due to random fluctuations. Hence, the primal heuristic is slightly worse than VLNS-TS, when given the same amount of time. A similar relation can be observed on benchmark A, whose instances are extracted from the real ones. Wilcoxon’s test (Wilcoxon, 1945) suggests that the performance of VLNS-TS is better than that of HAP-LDS with a probability ⩽0.00003 of random fluctuations.An interesting remark on the percentage gap of both heuristics is that it almost consistently tends to decrease as the size of the instances increases. Since the trend affects both heuristics and since we do not know the optimal solution of most instances, this probably means that the quality of the lower bound provided by column generation improves as size increases, and is in general rather tight. There is also a clear dependence on other parameters: the gap tends to increase moving from benchmark B to C to A, when comparing instances of the same size. Focusing on benchmarks B and C, the instances withα=0.10andβ=1.15, i.e. with a looser cost threshold and subsets of higher cardinality tend to have larger gaps.Finally, it is worth noticing that in a majority of cases the result of the final ILP problem improves upon that of the diving phase (18 out of 25 for benchmark A, 64 out of 80 for benchmark B and 44 out of 60 for benchmark C). For two instances of benchmark B and one of C the diving phase is unable to find feasible solutions. In other words, the combination of columns obtained in different stages of the diversification mechanism, allowed by the final ILP problem, is a crucial step and significantly improves the results of HAP-LDS.The real-world instance concerning the province of Monza proves easy to solve: the optimum can be found in 98 s by applying CPLEX to the multicommodity flow formulation. The province of Milan, on the contrary, is challenging: one hour of computation approximately corresponds to solving the continuous relaxation of the multicommodity flow formulation at the root node and provides a lower bound equal to 4609.81. By contrast, the column generation approach provides a lower bound equal to 9668.84 in 1918seconds.We then applied HAP-LDS imposing a time limit of 5000seconds on the diving phase. During this phase the algorithm found the best known solution having a cost equal to 9785.58, with a 1.21% gap. The final ILP problem was solved to optimality in about 300seconds, but without improving the best known solution. By contrast, Algorithm VLNS-TS found in the same time a solution with a cost equal to 9968.19, with a 3.10% gap. This solution is found after 1043seconds, but never improved in the following.

@&#CONCLUSIONS@&#
This paper considers a graph partitioning problem which models the partition of an organization into administrative areas. It proposes a column generation approach which obtains tight lower bounds (within a few percent units from the best known results) also for realistic size instances (from 70 to 90 vertices), as opposed to a multicommodity flow formulation which solves instances up to 30 vertices for random instances and 70 vertices for realistic ones, but yields large gaps as soon as the number of vertices exceeds these limits. We also propose a primal heuristic, based on the column generation approach, which combines the generation of promising columns based on their reduced cost, a heuristic limited discrepancy search mechanism and the solution of a final set partitioning problem on the whole set of columns generated. This approach, though its computational burden is intrinsically heavy, proves competitive with a refined local search metaheuristic drawn from our previous research on the topic. On random instances which mimic the structure of realistic problems, it achieves slightly better heuristic solutions, whereas on more general random instances and on instances drawn from the real ones its performance is slightly worse. When applied to the real-world instance of Milan (133 vertices), the gap between the result of the primal heuristic and the bound provided by column generation is as small as 1.21%.