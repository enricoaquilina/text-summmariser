@&#MAIN-TITLE@&#
Efficient matrix-free GPU implementation of Fixed Grid Finite Element Analysis

@&#HIGHLIGHTS@&#
Efficient strategy for GPU computing of FGFEA is proposed.Data locality is exploited to achieve notable GPU performance.Required device memory is minimized replacing it by efficient parallel calculations.Significant speedup with respect to sparse-matrix CPU implementation is achieved.

@&#KEYPHRASES@&#
GPU computing,Fixed grid,Finite element analysis,Matrix-free,High performance computing,

@&#ABSTRACT@&#
This paper proposes a strategy for the efficient implementation of Fixed Grid Finite Element Analysis (FGFEA) method on Graphics Processing Units (GPUs). Such a strategy makes use of grid regularity of FGFEA to reduce drastically both the memory required by the implementation and the memory transactions to perform the operations with the common elemental stiffness matrix. The matrix-free method is adopted (i) to reduce the memory requirements obviating the assembly process of FEA and (ii) to exploit the parallelization potential of GPU architectures performing matrix–vector products at the Degree of Freedom (DoF) level. The underlying idea is to exploit data locality and maximize the use of on-chip memory, which increase notably the performance of GPU computing. The numerical experiments show that the proposed matrix-free GPU instance of FGFEA can achieve significant speedup over classical sparse-matrix CPU implementation using similar iterative solver.

@&#INTRODUCTION@&#
The Finite Element Method (FEM) is widely used for numerical simulation of partial differential equations (PDEs) in science and engineering. The FEM has been successfully applied to simulate complex industrial problems, such as aeronautical, aerospace, naval and nuclear applications. The advances in the computational resources developed during the past decades have enabled to increase both the complexity and the fidelity of the simulation models. Despite the exponential increase in the computational resources provided by the Moore׳s law [1], the computationally-expensive simulation is still a long-standing and cutting-edge challenge due to the ever-increasing complexity and fidelity of finite-element models [2,3].The FEM involves two computationally intensive tasks: the assembly step and the solve step. The former assembles the local element equations into a global system of linear equations, while the latter aims to solve the system of equations. Both tasks can lead to an unaffordable problem in terms of performance and memory consumption for large-scale models. Iterative solvers and assembly-free methods have been widely used for reducing the memory requirements at the cost of increasing the processing time of the solve step. The solve step is responsible for 70–80% of the total computational time [4] when iterative solvers are used, and thus it becomes the bottleneck of the analysis. Nevertheless, the use of High-Performance Computing (HPC) techniques can alleviate the computational cost of iterative solvers in large finite element models, which allows to solve the finite element problems within a reasonable computational time [5–8].Over the last years, the rapid emergence of many-core technologies have led to an increasing interest in implementing numerical methods on Graphics Processing Unit (GPU) architectures. The General-Purpose computing on Graphics Processing Unit (GPGPU) has shown a growing research interest due to the great potential of GPU as high-performance co-processors. GPU computing consists of the use of a GPU together with a CPU to accelerate computational intensive applications. The use of these graphic cards in non-graphics HPC applications is becoming very popular due to their high computing capacity. GPU computing has been successfully applied in a wide spectrum of scientific, engineering and enterprise applications, such as electromagnetic simulations [9], fluid–structure interaction [10], numerical simulation in neuroscience [11], N-body simulations [12], seismic computations [13], structural topology optimization [14], and medical image processing [15], to name but a few. The reader is referred to [16–18] for comprehensive reviews of this research field.The development of programming languages and tools that facilitate the use of GPU has been crucial in the increased popularity of GPU computing. This is evidenced by the increasing number of publications in this research field since the first release of the general purpose SDK for Nvidia GPUs, the so-called Compute Unified Device Architecture (CUDA). Fig. 1shows the evolution of the research interest in GPU computing as number of publications per year. The data are obtained using the entry {“GPU” or “Graphics Processing Units”} and {“Finite Element Method” or “simulation” or “mechanical computation”} on the search engines of Scopus and Web of Science. Even though such programming languages and tools make the GPU development considerably easier, the programming skills to being able to fully utilize GPU hardware can be considered as an art that can takes months or years to master [16].The high computing capability and low cost of GPUs offer a great opportunity to speedup computationally demanding tasks that arise in the FEM pipeline. The GPU implementations of FEM presented in the literature are largely focused on two computer demanding tasks: the assembly of the stiffness matrix resulting from the FEM discretization [19–21] and the resolution of the sparse linear system of equations [22–24]. These previous works conclude that both steps provide enough parallelism to be successfully accelerated using GPUs architectures. However, they also highlight some limitations related to the implementation of the FEM pipeline on these devices, namely memory related problems such as excessive global memory transactions, non-coalesced global loads and stores that degrade global memory bandwidth, and shared memory accesses inducing bank conflicts, to name but a few. Besides, the proper problem formulation able to exploit the Single Instruction Multiple Data (SIMD), for which GPU architectures are designed, is a must. Such issues constrain the potential acceleration of FEM subroutines on GPU architectures, and therefore suppose a cutting edge challenge that requires further research.The paper is organized as follows. Section 2 presents the literature review related with this work. Section 3 provides an overview of the GPU architecture and the CUDA programming model. The bases of FGFEM are described in Section 4. The proposed strategy to implement efficiently the FGFEM on massive parallel architectures is described in Section 5. Section 6 presents the numerical experiments and the performance evaluation of the proposed matrix-free GPU instance of FGFEM with respect to the classical sparse-matrix CPU implementation. Finally, the conclusion of the proposed strategy and the performance evaluation with respect to the widely used sparse-matrix approach is presented in Section 7.Last years, the FEM implementation on many-core architectures has sparked a broad interest giving rise to a multitude of studies. These studies have largely focused on the two principal computationally intensive tasks involved in the FEM pipeline: (i) the assembly of the local element equations into a global system of linear equations, and (ii) the resolution of such a global system of equations.For the assembly step, several works have proposed assembly strategies for specific applications, such as geometric flow on an unstructured mesh [24] and FEM cloth simulation [25]. These methods derive simple expressions for each non-zero in the global system of equations taking advance of the specific characteristics of their applications. Such expressions permit us to compute each non-zero independently, which makes these approaches very well suited for many-core architectures. However, these approaches suffer from a lack of generality since the derivation of simple expressions is not possible for a wide range of FEM problems. To develop a generic many-core FEM assembly method, strategies such as graph coloring[20], graph partitioning[26] and reduction lists[27] have recently been proposed. These strategies aim to assemble in parallel a set of non-overlapping elements into the global system of equations. The principal disadvantage of these strategies relies on the fact that some preprocessing is required, which is time consuming and hardly parallelizable. Recent studies [28] have proposed a compact sparse-matrix data structure and an agglomeration strategy for the assembly step, which is based on atomic addition operations in the device memory. This strategy permits to reduce the memory footprint and avoid the preprocessing.Even though it has been demonstrated that the assembly step may be greatly accelerated using GPUs [19,20], the large storage requirements of global matrix for “real world” FEM problems makes difficult its use for these purposes. This is mainly due to the limited memory capacity of GPU devices. The domain decomposition strategies [29] divide the global matrix into submatrices which fit the device memory. These submatrices are then transferred to the device and the operations are performed in parallel achieving considerable speedups. However, the memory transactions may cause a degradation of the achieved acceleration. On the other hand, the matrix-free or assembly-free methods, developed for low memory computers in the early eighties [30], obviate the assembly step. This is done by performing certain calculations at finite element level without storing the global matrix. The principal problem of matrix-free methods is that memory is saved at the cost of increasing the computational cost meaningfully, which can be alleviated using parallel computing. These methods have been recently revisited for the parallelization of matrix–vector multiplications on GPUs [5,31,32].For the resolution step, the GPU implementation of Preconditioned Conjugate Gradient (PCG) algorithms has been widely studied in the literature. Despite most of the GPU implementations are based on sparse-matrix representation [33–37], the matrix-free methods are receiving an increasing attention for their inherent parallelism. These matrix-free methods have been used to compute the coefficients of the preconditioner for the resolution of large scale problems [38]. They also have been used to accelerate the GPU implementation of Element-by-Element (EbE) FEM by decomposing the matrix–vector calculations of Conjugate Gradient (CG) algorithm at the element level [31]. This is also done for solving anisotropic elliptic PDEs for the pressure correction in numerical weather forecast [32]. Recently, the GPU implementation of matrix-free methods is used to accelerate topology optimization algorithms for large-scale finite element models using whether voxelization [39] or structured meshes [14].One can conclude that the GPU devices offer significant computing capabilities to accelerate the FEM pipeline when the algorithm (assembly and solver) fits to the specific GPU parallel architecture, i.e. limited memory storage and data-level parallelism. In this vein, significant accelerations have been achieved in finite element problems using a structured grid. In these cases, the regularity of the grid permits us to simplify the memory accessing process and to limit memory related problems. However, FEM often requires the use of unstructured grids to conform to complex boundaries. For these problems some preprocessing is required to obtain a set of non-overlapping elements and to avoid memory related problems. Even though some proposals have been done in this sense, e.g. graph coloring and graph partitioning[20,26,27], the efficient division of the grid into sets of non-overlapping elements is still an open challenge.This paper aims to improve the performance of the GPU implementation of Finite Element Analysis (FEA) proposing a well suited strategy for both calculating on massive parallel architectures and alleviating memory related problems. This is done making use of the grid regularity of FGFEA method, which allows us to use the same local stiffness matrix for the whole finite elements of the grid. The adoption of matrix-free method allows then not only to obviate the assemble step, but to save the memory of local stiffness matrices. This allows us to minimize the amount of device memory used, which affects seriously to the GPU performance. This common local stiffness matrix can be efficiently handled using, as much as possible, on-chip memory and exploiting data locality. The regular grid also permits us to perform the matrix–vector operations of matrix-free method at the Degree of Freedom (DoF) level, which is especially suitable to make use of the massive parallel capabilities of GPU architectures [40].GPU devices offer incredible computational resources for both graphics and non-graphics processing. This massively parallel architecture was initially designed to satisfy the market demand of real-time and realistic 3D visualization [41]. The use of Nvidia devices and its programming model CUDA is the prevailing tendency. Nevertheless, the low-level API Open Computing Language (OpenCL) for heterogeneous computing, available for different GPU manufacturers, permits us to launch parallel code using a limited subset of the C programming language. On the other hand, CUDA provides a comprehensive development environment for building GPU-accelerated applications using high-level C programming language. The CUDA environment allows us to view the GPU as a compute device able to run a lot of threads using SIMD, which typically exploits data-level parallelism. The parallel code (single instruction) is defined as a C Language Extension function, called kernel, which is executed by a lot of CUDA threads using multiple data. The kernel call should specify the number of CUDA threads organized as a grid of thread blocks.The CUDA threads have only access to the device SGRAM (Synchronous Graphic Random-Access Memory), a type of DRAM (Dynamic Random-Access Memory) with high bandwidth interface for graphics-intensive functions, and to the on-chip SRAM (Static Random-Access Memory) through the memory spaces depicted in Fig. 2(a). The blocks are batch of threads able to cooperate by sharing data through shared memory and synchronize their execution coordinating memory accesses. The threads also have access to a fast private memory and to a rather slow global memory; constant and texture memory for read-only memory access and local and global memory for r/w memory access. Fig. 2(a) shows how kernels are invoked from the host (CPU) to the device (GPU) as a batch of threads organized as a grid of thread blocks.The CUDA memory hierarchy, shown in Fig. 2(b), is a key point to optimize memory access and achieve a reasonable performance. GPU architecture has two levels of cache memory to access to the SGRAM memory. Such a memory includes global, local, constant and texture memory. The local memory uses interleaved addressing for SGRAM accesses, which is a bit faster than global memory blocking each thread׳s data. One can observe in Fig. 2(b) that each multiprocessor or Streaming Multiprocessor (SM) has the following on-chip SRAM memory: one set of registers per processor and a shared memory, a read-only constant cache and a read-only texture cache which are shared by all cores of such a SM. Constant cache and texture cache are used to speedup reads from the constant memory and texture memory respectively, which reside in the device SGRAM.We have to remark that on-chip memory is much faster than SGRAM memory with a limited size, and thus its appropriate use can increase significantly the GPU performance. The main constrain of this fast memory is the limited size. The registers and shared memory of each SM are split among all the threads of the batch of blocks. Thus, the amount of blocks a SM can process at once depends on the number of registers per thread and the shared memory per block required for a given kernel. Register allocation process is made automatically by the compiler and its optimizer, which can deteriorates the performance depending on the GPU architecture. When the limit of registers per thread is exceeded, the register variables are spilled to local memory, which deteriorates seriously the GPU performance. The use of shared memory is especially beneficial for multiple accesses of same data per thread and for sharing data between the threads of block. The use of shared memory can however show relatively poor performance when using large arrays due to its limited amount per multiprocessor. This is due to the fact that CUDA cannot schedule more blocks to SMs than the multiprocessors can support in terms of shared memory and register usage, and thus the occupancy (number of active warps) is deteriorated. A key point for the proposed GPU instance is that the constant memory is stored in SGRAM but data are read through each multiprocessor constant cache, which is on-chip memory. Constant memory is also optimized for broadcast, i.e. when warp of threads read same location, but it is however serialized when warp of threads read in different locations.The software developments using CUDA consist of the following steps: (i) memory allocation and transaction, (ii) kernel execution on GPU and (iii) copy back the results to the host. The strategies to optimize code in GPU computing can be summarized as follows: (i) optimization of parallel execution to achieve maximum use of cores, (ii) optimization of memory access to avoid concurrency, (iii) optimization of instruction usage to achieve maximum instruction performance, and (iv) optimization of communications to achieve minimal synchronization between parallel executions. The improvement or degradation of GPU performance can be normally explained using these optimization criteria.The FGFEA method [42,43] permits us to analyze complex finite element models using a structured grid. Such a method allows us to disassociate the boundary of the physical domain from the mesh. This permits us to analyze modifications of boundary model without the regeneration of the mesh. This increases the efficiency of FEA processes that require of mesh regeneration, such as recursive FEAs performed in the field of structural optimization. Such an approach [44,45] is adopted in this work.Let us consider an elastic domain Ω enclosed by a boundary Γ. The fixed grid domainΩFGis defined as the minimum-volume bounding box that encloses Ω, as shown in Fig. 3(a). The fixed grid domain is discretized into a set of rectangular/cubic equally sized elements. Then, three types of elements can be created: elements located inside Ω (I elements), elements located outside Ω (O elements), and boundary intersected elements (B elements), as shown in Fig. 3(b). These elements differ only in the material properties: I elements have the material of the model, O elements are void, whereas the material properties of the B elements are approximated by homogenization techniques.The global stiffness matrixKof linear elasticity problems can be obtained from the element stiffness matrixKeusing the assembly operator A[46] as follows(1)K=Ae∈ECeTKeCe,with(2)Ke=∫ΩeBTDBdΩe,whereEdenotes the set of elements, the matrixCerepresents the transition between local and global numbering of DoF for the e-th element,Bis the strain-displacement matrix,Dis the constitutive material matrix and Ωe is the element domain. The strain-displacement matrixBcontains the derivatives of the shape functions and depends on the local coordinates and the geometry of the element. Since all the elements in the fixed grid have similar geometry, the strain-displacement matrixBis also the same for the whole elements. TheDmatrix contains the material properties of the elastic domain, i.e. the Young׳s modulus E and the Poisson׳s ratio ν. For isotropic materials, the matrixDis given by(3)D=k[1−ννν0001−νν0001−ν0001−2ν200sym1−2ν201−2ν2],withk=E(1+ν)(1−2ν).The material properties are constant for I and O elements. The constitutive matrixDIfor inside elements I is based on the material properties of the elastic domain (E and ν). The constitutive matrixDOfor outside elements is obtained by the product ofDIand a small magnitude Δ close to zero, typically 10−6[45], to prevent numerical instabilities. The fixed grid approximations for the elemental stiffness matrix of the I elements and O elements are as follows:(4)KIe=∫ΩeBTDIBdΩe(5)KOe=Δ·∫ΩeBTDIBdΩe=Δ·KIe.The boundary elements B are constituted by a weighted average approximation of inside I and outside O materials. The result is an homogeneous isotropic element that best approximate the original bi-material element. The element stiffness matrix of B elements is split into two contributions corresponding to the parts of the element domain that are inside ΩeIand outside ΩeOof Ω, respectively, as follows:(6)KBe=∫ΩIeBTDIBdΩIe+∫ΩOeBTDOBdΩOe,which is approximated by introducing the volume ratioξe=VIe/Veas(7)KBe=ξe∫ΩeBTDIBdΩe+(1−ξe)∫ΩeBTDOBdΩe=ξeKIe+(1−ξe)KOe.Considering thatDO=Δ·DI, the stiffness matrix for the boundary elements can be redefined as(8)KBe=(ξe+Δ(1−ξe))∫ΩeBTDIBdΩe=(ξe+Δ(1−ξe))·KIe.Note that the stiffness matricesKIeandKOecan be obtained from (8) withξe=1andξe=0respectively. The FGFEA formulation only requires calculatingKIe, which is denoted asKein the rest of the paper for simplicity. The use of a regular grid permits us to compute thisKematrix only once at the beginning of the analysis. This permits us to create and handle effectively theKematrix to alleviate mostly of memory related problems in the proposed GPU instance.Algorithm 1Preconditioned Conjugate Gradient (PCG) method.The Conjugate Gradient (CG) method is an iterative algorithm for solving large linear system of equations. For large FEM problems the CG method permits us to reduce the memory requirements of the matrix inversion operation at the cost of increasing the processing time of the solve step. We consider the linear system of equations resulting from the discretization of the linear elasticity problem as(9)Ku=f,whereKis the global stiffness matrix,uthe vector of unknowns, andfthe vector of nodal forces. The CG method shows good performance for well-conditioned linear system of equations. The condition number of the system matrix of mostly real-life applications is however high, and thus the efficiency of CG algorithm can be deteriorated. Preconditioning is a technique used to improve the efficiency of the CG method for ill-conditioned linear system of equations. The Preconditioned Conjugate Gradient (PCG) methods use a preconditioner M to replace the original system (9) by the equivalent system(10)M−1Ku=M−1f,whereM−1is chosen in such a way that the condition number ofM−1Kis closer to one. The Algorithm 1 shows the pseudo-code of the PCG method [47] adopted in this work. Such an iterative solver requires two computationally demanding operations: matrix–vector products, indicated in lines 3 and 8, and vector-inner products, shown in lines 5, 9 and 12. These demanding operations are performed on GPU using matrix-free method at the degree of freedom level to take advantage of massive parallel architectures.The proposed strategy for the efficient GPU implementation of FGFEM takes advantage of the potential parallelization of matrix-free methods to perform the demanding operations of the PCG solver at the DoF level. This is specially indicated for massive parallel architectures to balance the workload of CUDA threads [40], and thus increasing the use of cores. The underlying idea is that the slowest link (most computationally intensive process) determines the potential of parallelization. This proposal also makes use of grid regularity of FGFEA to reduce drastically the use of GPU memory, which affects seriously the GPU performance. The key idea behind the proposed strategy is to exploit data locality by keeping in the cache the information needed by the calculations of FGFEA.The regular grid of FGFEA method consists of a set of equally sized elements used to tessellate the physical domain. This discretization is composed of a set of nodesN, a set of elementsEand a set of DoFsU. For the three-dimensional case, the cells of the regular grid are first-order solid isoparametric hexahedral elements, which can be numbered following the x, y and z directions sequentially. The node index nindexfor a certain position〈i,j,k〉is provided by the expression(11)nindex(i,j,k)=i+j(cx+1)+k(cx+1)(cy+1),where cxand cyare the number of divisions in the x and y directions respectively. The element e, located at the indexed position〈i,j,k〉, contains eight nodes located at the corners of the hexahedral element. The indexes of these nodes can be obtained asElemente.node1=i,Elemente.node2=cx+i+1,Elemente.node3=cx+i+1+cx+1cy+1,Elemente.node4=i+cx+1cy+1,Elemente.node5=i+1,Elemente.node6=cx+i+2,Elemente.node7=cx+i+2+cx+1cy+1,Elemente.node8=i+1+cx+1cy+1.This numbering prevents the use of a global index table, which is typically used in FEM implementations. The indexes and the mapping between local and global unknowns can be calculated from the number of divisions (cx,cy).Algorithm 2DbD matrix–vector product (dbdMVP).Let us consider the product of the global matrixKand a certain vectorp, which is required by the PCG iterative solver used by FGFEA. For performance reasons, the proposed strategy stores separately one elemental matrixKeof equally sized elements and the corresponding densities of each elementd(e)=ξe+Δ(1−ξe)as specified in (8). The matrix–vector product is performed at the DoF level using a matrix-free approach. The workload assigned to each thread is therefore lighter, which is especially suitable for massive parallel architectures [40]. The elemental matrix of each element is obtained implicitly performing the corresponding operations on the GPU device. This allows us to reduce drastically the amount of device memory, which is of paramount importance to achieve reasonable speedups.Algorithm 2 describes the implementation of the matrix-free DoF-by-DoF (DbD) Matrix–Vector Product (dbdMVP) for FGFEA method. The algorithm requires as input data the vectorpto perform the matrix–vector product, the vectordcontaining the material density for each element, the common local element matrixKe, and the number of DoFs per element ne. The algorithm consists of three inner loops. The first loop applies to the DoFsu∈U, the second loop operates on the elementse∈E(u)containing the unknown u, while the third loop goes through local DoFs of the element e and performs the inner-product of the i-th row ofKeand the vectorp(e), which contains the values ofpatU(e). The result is a vectoraof dimension the DoFs of the model. The u-th term ofais given by the addition of each element contribution containing u.The regularity of the grid permits us to determine the setsE(u)(line 2) andU(e)(line 4) given the index of u. This is done using the numbering presented above, which only requires the number of divisions (cxand cy) in each grid dimension. In the same vein, the transformation between global and local unknowns is also defined by the location of u within the setU(e)(line 5). The GPU implementation of matrix-free dbdMVP is performed assigning one CUDA thread to each DoFu∈Uof the regular grid. The matrix-free dbdMVP operation only calculates the required elements of the corresponding row of the local stiffness matrices contributing to each DoF. The implicit access to the elemental matrices permits us to reduce drastically the device memory, which normally requires storing all the elemental matrices in assembly-free approaches or the assembled global stiffness matrix in methods dealing with (9). Finally, memory accesses are markedly diminished because indexing, contributions to each DoF, is calculated when needed instead of read from global memory. All these features increase the GPU performance replacing memory storage and memory transactions by calculation on CUDA threads, which is effectively performed in parallel.The GPU instance of the PCG iterative solver requires diverse arithmetic operations on vectors, such as vector addition and dot product. Such operations do not depend on the grid connections and their parallel implementation is straightforward. For instance, the product of two vectorsα=aT·pusing GPU is calculated as each DoF separately. Each scalar-scalar operation is assigned to a thread, which has to synchronize to perform the addition of all the individual operations, typically it using atomic operations in the CUDA kernel. Making use of the basic vector operations on GPU the Algorithm 1 can be formulated in terms of DoF-wise computations. This allows us to assign one CUDA thread to the operations required by each DoF.Algorithm 3DbD Jacobi-PCG algorithm (dbdPCG).The DbD formulation of the PCG iterative solver (dbdPCG) is shown in Algorithm 3. The regular grid provides a set of elementsE, a set of nodesNand a set of unknownsU. The dbdPCG algorithm requires as input data the elemental matrixKe, the vector of forcesf, an initial vector of displacementsu0, the vectordcontaining the elemental densities, the tolerancetol, the maximum number of iterationskmaxand the number of DoFs per element ne. The labeled operations using “DOF loop” indicate that the computation of the vector arithmetic is performed on GPU at the DoF level.The preconditioning traditionally requires the global stiffness matrix in its assembled form. The assembled matrixKis not available using matrix-free approaches, which makes difficult the implementation of highly effective preconditioning techniques. Although some matrix-free preconditioners have been proposed in the literature, such as the Element-By-Element (EBE) preconditioners [48,49], their effective implementation on GPU architectures is still an open field that requires further research. The Jacobi preconditioner is adopted in this work because the data needed to calculate it can be obtained from elemental stiffness matrices. Its implementation consists of two inner loops (line 6–14). The first loop applies to the DoFsu∈Uwhereas the second loop operates on the elementse∈E(u)containing the unknown u. The matrix–vector products are performed invoking the CUDA kernel implementing the dbdMVP algorithm (line 15 and line 25) presented above.Fig. 4shows the flowchart of the matrix-free GPU implementation of FGFEA. The dbdPCG algorithm is entirely implemented on GPU using CUDA kernels which are invoked from the host. The element matrixKe, the initializationu0of vector of displacementsu∈U, the vector of nodal forcesf, and the vector of material densitiesdare calculated on CPU. Once the geometric parameters of the fixed grid are defined and the element stiffness matrix is calculated, the memory transaction of vectorsu0,f,dandKefrom host (CPU) to device (GPU) memory is performed. The allocation of these vectors should be made before memory transactions. The length size of vectorsu0,fanddis the number of DoFs of the regular grid of FGFEA whereas the vector storingKehas the sizens=[ne·(ne+1)/2], with nethe number of DoFs per finite element, corresponding to the upper triangular matrix of elemental stiffness.Fig. 5shows the GPU memory required by the GPU instance of PCG for FGFEA. In addition to the vectors mentioned previously, the GPU instance of PCG for FGFEA requires two additional vectors,randp, to store the residual and update vectors of the iterations of the PCG solver. The scalar values to configure the iterative solver (cx,cy,tol,kmaxandne) and to store temporal data (ρk, γk and k) are allocated in the initialization of the GPU instance. A key point is that common elemental stiffnessKeis stored in constant memory obviating the symmetric information. This allows us to save bandwidth because constant memory is cached, and thus consecutive reads of the same address does not incur any additional memory traffic. Besides, one single read from constant memory is broadcast to the threads of a half-warp. The consecutive read of same address is facilitated by only storing symmetric information ofKe. The regularity of the grid of FGFEA also facilitates designing kernels with this memory access pattern in mind.The dbdPCG iterative solver is organized into a sequential host program invoking the corresponding kernels. These CUDA kernels require as many threads as DoFs with contributions of inside I and boundary B elements. All computations are performed using double precision floating point representation. All the threads are synchronized to update the global variables sequentially. The global variables ρk, γk and k are copy back to the CPU memory in the iterations to evaluate the stopping criterion on the host.The performance of the proposed matrix-free GPU implementation of FGFEA is evaluated using three numerical simulations [43]: 3D l-shaped beam, 3D square plate and 2D square plate experiments. The GPU instance of dbdPCG algorithm is compared with the CPU implementation of classical FGFEA using sparse-matrix representation of the assembled stiffness matrix, called PCG sparse-matrix solver in the rest of the paper for simplicity. This sparse-matrix solver makes use of one thread for the sparse-matrix calculations of the comparison. The inclusion/exclusion of outside elements in the stiffness matrix is also analyzed in terms of efficiency for both the reference CPU sparse-matrix and the GPU matrix-free implementations. This is done because outside elements increase the condition number of the stiffness matrix, and therefore degrade the convergence rate of the PCG iterative solver. The reader is referred to [44] for comprehensive study about accuracy of FGFEA with respect to classical FEA.The numerical experiments are performed using a computer, with a couple of Intel Xeon E5606 2.13GHz, equipped with two GPUs: Nvidia Tesla C2070 and Nvidia Quadro4000. The former incorporates 448 cores while the latter is equipped with 256 cores. Despite several factors can affect the GPU performance the number of cores is used as reference to study the scalability of the GPU instance using different graphic cards. The factors affecting the GPU performance include memory clock, processor clock, memory bandwidth, and complex Arithmetic Logic Units (ALUs), to name but a few. The GPU matrix-free code is compiled using the NVIDIA CUDA Toolkit 6.5. The numerical experiments are run on 64 bits Linux OS with the NVIDIA Driver Version 340.76. It is important to remark that the development environment and the graphics driver updates often show significant performance improvements.The 3D l-shaped beam benchmark consists of a beam fixed at one extreme while the other endpoint is loaded with a shear force. Fig. 6(a) shows the parameterized geometry and mesh (upper) and the material density field (lower) to solve the problem using FGFEM. The density field only contains inside I and outside O material elements because of the regularity of the domain. The 3D square plate benchmark consists of a 3D plate with a circular hole in the center, which only requires analyzing a quarter of the structure due to the symmetry. The geometry and mesh (upper) and the density field for FGFEA approach (lower) are shown in Fig. 6(b). One can observe that the density field incorporates boundary B material elements to adapt to the non-regular geometry. The 2D square plate benchmark consists of a 2D square plate with a circular hole, as shown in Fig. 6(c). This experiment is used to evaluate the performance of GPU matrix-free instance of FGFEA for plane stress finite element models.The numerical experiments consist of analyzing the three benchmarks using different grid resolutions of FGFEA method. These experiments use the same material properties (E=1e9andν=0.2) and small magnitude (Δ=10−6) for FGFEA approach. The maximum residual error is set to 10-8 for the PCG instances and the calculations are performed using double-precision floating-point format. The fixed grid is composed of eight-node hexahedral linear brick elements for the 3D experiments and four-node quadrilateral plane-stress elements for the 2D simulation.The comparison between the proposed fine-grained GPU matrix-free instance and the reference CPU sparse-matrix implementation obviates the memory related problem of large-scale finite element models, which is one of the main advantages of matrix-free methods. This comparison provides the speedups of the proposed matrix-free GPU instance on two graphic cards with respect to the reference CPU sparse-matrix implementation.Fig. 7shows the wall-clock time per iteration of the fine-grained matrix-free GPU instance of the PCG algorithm, considering and obviating outside elements, using different grid sizes to analyze the 3D l-shaped benchmark using FGFEA. One can observe a reduction of 50% of wall-clock time per iteration when the outside elements O are excluded from the stiffness matrix. This is in line with the reduction of DoFs obviating the contribution of outside elements O [44], as shown in Fig. 7; the upper x-axis indicates the DoFs of model obviating outside elements O, whereas the lower x-axis specifies the DoFs corresponding to the whole fixed grid used by FGFEA. For this reason, the numerical simulations are performed obviating the DoFs with only contributions of outside elements. The GPU matrix-free instance only applies the dbdPCG algorithm to the DoFs with some contribution of inside I and/or boundary B elements. For the purpose of a fair comparison between the GPU matrix-free instance and the CPU sparse-matrix implementation, the DoFs with only contributions from outside elements O are removed from the system of Eq. (9) of the linear elasticity problem on the CPU sparse-matrix implementation.The wall-clock time per iteration of a battery of experiments using different fixed grid resolutions of FGFEA is shown in Fig. 8. The upper x-axis shows the number of DoFs used to solve the finite element model whereas the DoFs of fixed grid are shown in the lower x-axis. One can observe the linear relationship between the computational time and the grid resolution of FGFEA expressed as number of DoFs. The speedups obtained by the matrix-free GPU instance of FGFEA with respect to the CPU sparse-matrix implementation of FGFEA are shown in Fig. 9. These results show (lower) that the acceleration is achieved from a certain number of DoFs. This is due to the time needed for memory transactions between host and device, which can be prevalent with respect to the benefit of performing the calculations on the GPU. One also can observe that the speedup of matrix-free GPU instance of FGFEA with respect to the CPU sparse-matrix implementation is increased with the number of DoFs using both graphic cards (upper). This acceleration increases until it asymptotically reaches its limit that depends on the GPU capabilities. The maximum speedup achieved by Nvidia Tesla C2070 is in fact twice the accelerations obtained by Nvidia Quadro4000. This reveals a relationship between the numbers of CUDA cores (448 vs 256) and demonstrates that the proposed matrix-free GPU instance of FGFEA is scalable with the capabilities of GPU architectures, which are in constant progress today.

@&#CONCLUSIONS@&#
