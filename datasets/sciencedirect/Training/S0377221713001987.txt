@&#MAIN-TITLE@&#
An effective PSO-inspired algorithm for the team orienteering problem

@&#HIGHLIGHTS@&#
A new fast optimal split for the team orienteering problem.An effective particle swarm algorithm (PSOiA).PSOiA is robust and outperforms the state-of-the-art algorithms in the literature.

@&#KEYPHRASES@&#
Vehicle routing,Knapsack problem,Interval graph,Optimal split,Swarm intelligence,

@&#ABSTRACT@&#
The Team Orienteering Problem (TOP) is a particular vehicle routing problem in which the aim is to maximize the profit gained from visiting customers without exceeding a travel cost/time limit. This paper proposes a new and fast evaluation process for TOP based on an interval graph model and a Particle Swarm Optimization inspired Algorithm (PSOiA) to solve the problem. Experiments conducted on the standard benchmark of TOP clearly show that our algorithm outperforms the existing solving methods. PSOiA reached a relative error of 0.0005% whereas the best known relative error in the literature is 0.0394%. Our algorithm detects all but one of the best known solutions. Moreover, a strict improvement was found for one instance of the benchmark and a new set of larger instances was introduced.

@&#INTRODUCTION@&#
The term Orienteering Problem (OP), first introduced in [19], comes from an outdoor game played in mountainous or forested areas. In this game, each individual player competes with the others under the following rules. Each player leaves a specific starting point and tries to collect as many rewards as possible from a set of check points in a given time limit before returning to the same starting point. Each check point can reward each player at most once and each player is aware of the position of each check point as well as the associated amount of rewards. There always exists an optimal strategy to achieve the maximum amount of rewards. In general, finding such a strategy (or solving OP) is NP-Hard [19], the player should select a correct subset of check points together with determining the shortest Hamiltonian circuit connecting these points and the starting point. OP and its variants have attracted a good deal of attention in recent years [1,6,35,39] as a result of their practical applications [13,19,24,36] and their hardness [11,18,22]. Readers are referred to Vansteenwegen et al. [37] for a recent survey of these problems.Adding the cooperative aspect to OP, without neglecting the competitive one, yields to the Team Orienteering Problem (TOP) [14]. In this problem, the players are partitioned into teams and players of a team work together to collect as many rewards as possible within the time limit. Each check point can reward each team at most once. The specific vehicle routing problem, analogous to this game that we also denote by TOP, is the problem where a limited number of vehicles are available to visit customers from a potential set, the travel time of each vehicle being limited by a time quota, each customer having a specific profit and being visited at most once. The aim of TOP is to organize an itinerary of visits so as to maximize the total profit. Solving this problem is also NP-Hard [14]. The applications of TOP include athlete recruiting [14], technician routing [9,33] and tourist trip planning [39,37]. In this paper, we are interested in TOP as the core variant of OP for multiple vehicles. This work was motivated by several lines of research first put forward by Veolia Environnement [9,8].As far as we know, there are only three exact algorithms for TOP [10,12,27]. In contrast to exact solving approaches, a number of heuristics and metaheuristics have been developed for TOP. Two fast heuristics were developed by Butt and Cavalier [11] and by Chao et al. [13]. Tang and Miller-Hooks [33] proposed a tabu search embedded in an adaptive memory procedure. Two tabu search approaches and two versions of a Variable Neighborhood Search (VNS) algorithm were developed by Archetti et al. [2]. Those four methods make use of infeasible tours and of a repairing procedure. Among these, the slow version of the VNS (SVNS) gave very good results on the standard benchmark. Later, Ke et al. [21] developed four versions of an Ant Colony Optimization (ACO) approach. A guided local search and a skewed variable neighborhood search were then proposed by Vansteenwegen et al. [38,39]. More recently, Bouly et al. [8] introduced giant tours, i.e. permutations of all customers, to represent solutions of TOP and designed an effective Memetic Algorithm (MA). The results of MA [8] were as good as those of SVNS [2] with several strict improvements. Souffriau et al. [32] submitted two versions of a Path Relinking (PR) approach and independently produced the strict improvements. Like [2], PR approach uses a repairing procedure during the relinking phase to deal with infeasible tours. Those tours are obtained from a gradual combination of each of the random generated solutions with the best ones. The slow version of the Path Relinking (SPR), despite its name, required very small computational times. It is also worth mentioning that Tricoire et al. [35] proposed a VNS algorithm for a generalized version of OP and provided their results on the original TOP instances. Furthermore, there are two methods based on Particle Swarm Optimization (PSO) designed to TOP: Bonnefoy [7] developed a PSO algorithm combined with a linear programming technique whereas Muthuswamy and Lam [25] introduced a discrete version of PSO (DPSO) to solve TOP.In short, three methods stand out as the state-of-the-art algorithms for TOP: the slow version of the VNS (SVNS) in [2], the MA algorithm in [8] and the slow version of the PR (SPR) in [32]. Unlike the other two, MA proposed an interesting technique to represent the solutions of TOP, known as giant tours. This technique was previously introduced in [5] for the Vehicle Routing Problem (VRP). According to a recent survey on heuristic solutions for variants of VRP [40], it is classified as an indirect representation of the solution space. Indeed, each giant tour represents a neighborhood of solutions from which the best one can easily be extracted by an evaluation process. A heuristic using this representation tends to have better visions during the search and a better chance to reach the global optimum. Several search algorithms exploiting this strategy have been discussed in [28] for the case of VRP and variants.In this paper, we propose an effective PSO-inspired Algorithm (PSOiA) for TOP. This work is based on our preliminary study of a PSO-based memetic algorithm (PSOMA), which was communicated in [16]. The main contribution of our paper is a faster evaluation process than the one proposed in [8]. This enables PSOiA and possibly further methods in the literature to examine a larger number of neighborhoods and explore faster the search space. Experiments conducted on the standard benchmark of TOP clearly show that PSOiA outperforms the existing solution methods of the literature. It achieves a relative error of 0.0005% and detects all but one of the best known solutions. Moreover, a strict improvement was found for one instance of the benchmark. The remainder of this paper is organized as follows. Section 2 provides a formal formulation of TOP. PSOiA and the new optimal split procedure are described in Section 3. The dynamic management of the parameters and computational results on the standard benchmark are described in Section 4. In Section 5, we introduce a new set of large instances and provide the respective results. Finally, some conclusions and further developments are discussed in Section 6.TOP is modeled with a graph G=(V∪{d}∪{a}, E), where V={1, 2, …, n} is the set of vertices representing customers, E={(i, j)∣i, j∈V} is the edge set, d and a are respectively departure and arrival vertices for vehicles. Each vertex i is associated with a profit Pi, and each edge (i, j)∈E is associated with a travel cost Ci, jwhich is assumed to be symmetric and satisfying the triangle inequality. A tour R is represented as an ordered list of q customers from V, so R=(R[1], …, R[q]). Each tour begins at the departure vertex and ends at the arrival vertex. We denote the total profit collected from a tour R asP(R)=∑i=1i=qPR[i], and the total travel cost/time asC(R)=Cd,R[1]+∑i=1i=q-1CR[i],R[i+1]+CR[q],a. A tour R is feasible if C(R)⩽L with L being a predefined travel cost/time limit. The fleet is composed of m identical vehicles. A solution S is consequently a set of m (or fewer) feasible tours in which each customer is visited at most once. The goal is to find a solution S such that∑R∈SP(R)is maximized. One simple way of reducing the size of the problem is to consider only accessible customers. A customer is said to be accessible if a tour containing only this customer has a travel cost/time less than or equal to L. For mixed integer linear programming formulations of TOP see [10,12,21,27,37].Particle Swarm Optimization (PSO) is a swarm intelligence algorithm proposed by [23] with the basic idea of simulating the collective behavior of wild animals in the nature. PSO was first used for optimization problems in continuous space as follows. A set known as a swarm of candidate solutions, referred to as particles, is composed of positions in the search space. The swarm explores the search space according to Eqs. (1) and (2). In these equations,xitandvitare respectively the vectors of position and velocity of particle i at instant t. Three values w, c1 and c2, called respectively inertia, cognitive factor and social factor, are parameters of the algorithm. Two values r1 and r2 are random numbers generated in the interval [0, 1]. Each particle i memorizes its best known position up to instant t asxilbest, and the best known position up to instant t for the swarm is denoted as xgbest.(1)vit+1=w·vit+c1·r1·xilbest-xit+c2·r2·xgbest-xit(2)xit+1=xit+vit+1With this design, PSO is highly successful at performing optimizations in continuous space [3,20]. In contrast, when applied to problems of combinatorial optimization, PSO encounters difficulties in interpreting positions and velocities, as well as in defining position update operators. As a result, there are a variety of discrete PSO variants (DPSO) [4], and it is difficult to choose an appropriate variant for any given combinatorial optimization such as TOP.Our PSO works with a population of particles, so called the swarm and denoted S. Each particle memorizes its current position, i.e. a representation of a solution, and its best known position, called local best position, according to an evaluation process. A basic iteration of the algorithm consists of updating the position of each particle in the swarm. In the standard PSO, this update is influenced by PSO parameters and it takes into account the current position, the local best position and the global best position. In our method, each particle also has a small probability ph to be moved out of its current position and transfered to a completely new position. This new position is generated using a randomized heuristic. Moreover, each new position has pm probability to be improved through a local search process. The algorithm is stopped after itermax consecutive position updates have failed to give rise to new local best. Because itermax is usually set to be proportional tonm[8,16], then from now when we say the stopping condition is k, that meansitermax=k·nm.For convenience, the current, local best and global best positions of a particle x are denoted respectively S[x].pos, S[x].lbest and S[best].lbest. The global scheme is summarized in Algorithm 1. Its components are detailed in the next sections.Algorithm 1Basic algorithmA position in our PSO is a permutation π of all accessible customers, usually referred to as a giant tour, in a particular problem scenario. The principle of the split technique that optimally extracts a solution from a giant tour was introduced by [8] for TOP. The basic idea is the following. All possible subsequences of π, denoted by (π[i], …, π[i+li]) or 〈i, li〉πfor short, that can form a feasible tour of TOP are considered. For convenience, we use the term extracted tours or simply tours in this section to refer to these subsequences. The goal of a split procedure is then to find a set of m distinct tours (without shared customer) such that the sum of their profits is maximized. Such a procedure guarantees that if a set of tours forming an optimal solution for the TOP is currently present as subsequences in a permutation π∗, the application of the split procedure on π∗ will return the optimal TOP solution.The authors of [8] proposed a split procedure for TOP. The algorithm requires to find the longest path in an acyclic auxiliary graph. This graph represents the successor relations between extracted tours, i.e. the possibility of a tour to follow another in a valid solution. They also introduced the notion of saturated tours, i.e. a tour in which liis maximal (denoted bylimax), and proved that solutions containing only saturated tours are dominant. Therefore, only saturated tours were considered in their procedure and the number of arcs in the acyclic graph is reduced. The worst case complexity of their procedure is O(m·n2).In this work, the limited number of saturated tours is exploited more efficiently to reduce the complexity of the evaluation process. Before going in the detail of our new split procedure, we recall the definition of a knapsack problem with conflicts (KPCG) [41] as follows. In a KPCG, we have a set of items to be put into a knapsack. A value and a volume are associated to each item. The knapsack has a limited volume, so it cannot generally hold all items. In addition to the knapsack volume, some items are in conflict with each other and they cannot be put in the knapsack together. The aim of the KPCG is to find a subset of items to fit into the knapsack such that the sum of their values is maximized. In such a problem, the conflicts between items are usually modeled with a graph, called conflict graph. We also recall the definition of an interval graph [34] as follows. A graph G=(V, E) is called an interval graph if there is a mapping I from V to sets of consecutive integers (called intervals) such that for all i and j of V, [i, j]∈E if and only if I(i)∩I(j)≠∅. Then the following proposition holds for the split procedure of TOP.Proposition 3.1The split procedure can be done optimally in O(m·n) time and space.Each possible tour extracted from a giant tour is in fact a set of positions of customers in the giant tour. Since these customers are adjacent in the giant tour, the positions are consecutive integers and the set of extracted tours can be mapped to the set of vertices of an interval graph X. Additionally, an edge of X (or a non-empty intersection between two sets of positions) indicates the presence of shared customers between the associated tours. As mentioned above, a split procedure looks for m tours without shared customer such that the sum of their profit is maximized. So this is equivalent to solve a knapsack problem with X as the conflict graph, a unitary volume for each item and m as the knapsack’s volume. In this particular knapsack problem, the number of items is equal to the number of possible tours. This number is equal to n when only saturated tours are considered. Based on the work of Sadykov and Vanderbeck [30], we deduce that such a problem can be solved in O(m·n) time and space.Our new evaluation process is summarized as below. For each saturated tour starting with customer π[i], we use P[i] to denote the sum of profits of its customers. Its first successor, denoted by succ[i], is computed as follows:(3)succ[i]=i+limax+1ifi+limax+1⩽n0otherwiseA two-dimensional array Γ of size m·n is used to memorize the maximum reachable profit during process. The algorithm then browses the saturated tours in reversed order, meaning from customer π[n] to customer π[1], and updates Γ based on the recurrence relation described in Eq. 4.(4)Γ(i,j)=max{Γ(succ[i],j-1)+P[i],Γ(i+1,j)}if1⩽i⩽nand1⩽j⩽m0otherwiseAt the end, Γ(1, m) corresponds to the profit of the optimal solution. A simple backtrack is then performed on Γ in order to determine the corresponding tours. That is to say if Γ(succ[i], j−1)+P[i] is used over Γ(i+1, j) in the relation, then the saturated tour starting with customer π[i] belongs to the optimal solution.Fig. 1depicts the same example of the split problem described in [8] but with the new evaluation process. More precisely, in this problem we have 8 customers with π= (1, 2, 3, 4, 5, 6, 7, 8), profits (10, 30, 10, 40, 40, 50, 10, 120), L=70 and m=2. According to the distances given in the figure, the saturated tours are 〈1,0〉, 〈2, 2〉, 〈3, 1〉, 〈4, 1〉, 〈5, 2〉, 〈6, 1〉, 〈7, 1〉 and 〈8, 0〉 with profits 10, 80, 50, 80, 100, 60, 130 and 120 respectively. The interval model is shown in Fig. 1b and the detail of the first successor relations as well as solving steps are given in Fig. 1c. The new algorithm actually returns the same solution composed of the same saturated tours (starting with customers 5 and 8) as expected in [8].Particle positions in the swarm, including local best positions, are initialized to a random sequence. In order to accelerate the algorithm, a small portion of the swarm containing NIDCHparticles will have their local best positions generated using a good heuristic. During the search, a faster heuristic is occasionally used to generate a completely new position for a particle. The heuristics that we use are randomized variants of the Iterative Destruction/Construction Heuristic (IDCH) of [8].The core component of IDCH is a Best Insertion Algorithm (BIA). Our BIA considers a partial solution (which can be empty) and a subset of unrouted customers to be inserted in the solution. This constructive method then evaluates the insertion costCi,z+Cz,j-Ci,j(Pz)αof any unrouted customer z between any couple of successive customers i and j in a tour r. The feasible insertion that minimizes the cost is then processed and the method loops back to the evaluation of the remaining unrouted customers. If more than one possible insertion minimizes the insertion cost, one of them is chosen at random. This process is iterated until no further insertions are feasible, either because no tour can accept additional customers, or because all the customers are routed. The only parameter of BIA is α and it is set to 1 in [8,32]. In this work, a random value of α is generated each time BIA is called. This generation makes our IDCH less predictable and actually a randomized heuristic. The computational method used to generate α is detailed in Section 4.IDCH is described as follows. Firstly, BIA is called to initialize the current solution from scratch. On following iterations a small part of the current solution is destroyed by removing a limited random number (1, 2 or 3) of random customers from tours, and a 2-opt procedure is used to reduce the travel cost of tours. A reconstruction phase is then processed using a Prioritized Best Insertion Algorithm (PBIA). The destruction and construction phases are iterated, and each time a customer remains unrouted after the construction phase its priority is increased by the value of its associated profit. In the PBIA, the subset of unrouted customers with the highest priority is considered for an insertion using a BIA call. When no more of these customers can be inserted, unrouted customers with lower priorities are considered, and so on. The idea behind this technique is to explore solutions composed of high profit customers. IDCH memorizes the best discovered solutions so far and stops after a fixed number of Destruction/Construction iterations without improvement of this solution. This number is set to n for the fast version of IDCH. This version is used to generate a new position for a particle when it is moved out of its current position. For the slower version used to initialize the PSO, this value is set to n2. In the slow version, after n iterations without improvement a diversification process is applied. This involves destroying a large part of the solution while removing a number (bounded by n/m rather than by 3) of customers from tours then applying 2-opt to each tour to optimize the travel cost, and finally performing the reconstruction phase.In our PSO, whenever a new position, i.e. a new permutation, is found, it has a pm probability of being improved using a local search technique (LS). This LS contains 3 neighborhoods which were proved to be efficient for TOP [8]:•shift operator: evaluate each permutation obtained by moving each customer i from its original position to any other position in the permutation.swap operator: evaluate each permutation obtained by exchanging every two customers i and j in the permutation.destruction/repair operator: evaluate the possibility of removing a random number (between 1 andnm) of customers from an identified solution and then rebuilding the solution using BIA procedure described in the previous section.The procedure is as follows. One neighborhood is randomly chosen to be applied to the particle position. As soon as an improvement is found, it is applied and the LS procedure is restarted from the new improved position. The LS is stopped when all neighborhoods are fully applied without there being any improvement. In addition, we enhanced the randomness of shift and swap operators. That is to say the possibilities of moving or exchanging customers in those operators are evaluated in random order.In combinatorial optimization, the particle position update of PSO can be interpreted as a recombination of three positions/solutions according to inertia, cognitive and social parameters. There are various ways of defining this kind of recombination operator [4]. In our approach, the recombination operator is similar to a genetic crossover whose core component is an extraction of l customers from a permutation π. To make sure that a customer can be extracted at most once from sequential calls of the core component, a set M is used to mark extracted customers from previous calls. The extracted subsequence is denotedπMland the procedure is described as follows:•Step 1: generate a random location r in π and initializeπMlto empty.Step 2: browse customers from π[r] to π[n] and add them to the end ofπMlif they are not in M. If|πMl|reaches l then go to Step 4, otherwise go to Step 3.Step 3: browse customers from π[r] down to π[1] and add them to the beginning ofπMlif they are not in M. If|πMl|reaches l then go to Step 4.Step 4: add customers fromπMlto M.With the core component, the position update procedure of particle x from the swarm S with respect to the three PSO parameters w, c1 and c2 is as follows:•Phase 1: apply sequentially but in a random order the core component to extract subsequences from S[x].pos, S[x].lbest and S[best].lbest with a common set M of customers to be skipped. M is initialized to the empty set and the desired numbers of customers to be extracted from S[x].pos, S[x].lbest and S[best].lbest are respectively w·n,(1-w)·n·c1.r1(c1.r1+c2.r2)and(1-w)·n·c2.r2(c1.r1+c2.r2). Here r1 and r2 are real numbers whose values are randomly generated in the interval [0, 1] with a uniform distribution. Real numbers obtained from those computations are truncated to integral values.Phase 2: link three extracted subsequences in a random order to update S[x].pos.To illustrate the update procedure, we consider an arbitrary instance of TOP with ten customers and an arbitrary particle x with S[x].pos=(4, 5, 2, 6, 10, 1, 7, 8, 9, 3), S[x].lbest=(4, 2, 3, 8, 5, 6, 9, 10, 7, 1) and S[best].lbest=(1, 2, 4, 9, 8, 10, 7, 6, 3, 5). PSO parameters are w=0.3, c1=0.5 and c2=0.3. Random variables r1 and r2 generated are respectively 0.5 and 0.5. Then the desired numbers of customers to be extracted for S[x].pos, S[x].lbest and S[best].lbest are respectively 3 (=⌊0.3∗10⌋), 4 (=⌊(1−0.3)∗10∗0.5∗0.5/(0.5∗0.5+0.3∗0.5)⌋) and 3 (=10−3−4). Random extraction order in Phase 1 is (S[x].pos, S[x].lbest, S[best].lbest) and random linking order in Phase 2 is (S[x].lbest, S[x].pos, S[best].lbest). Fig. 2gives an example of the update procedure that indicates the new position for the particle x of (8, 5, 6, 9, 10, 1, 7, 2, 4, 3).Our particle position update procedure therefore works with the standard PSO parameters w, c1 and c2, the only restriction being that w has to be in the interval [0, 1[. Our PSO approach can be classified as PSO with position only, given that no velocity vector is used [26]. It is noteworthy to mention that the core component was created to adapt to a linear permutation order, but it can easily be adapted to a circular order by changing Step 3.In some situations, PSO can be trapped in a local optimum, especially when all the local best positions of particles in the swarm are identical. In our approach, the fact that a particle can be randomly moved out of its current position reduces this premature convergence. However, the effect of this reduction is only partial because the probability to move a particle out of its current position is set to a small value. This setting is due to two main reasons: firstly, a frequent use of the IDCH heuristic to generate new positions is time-consuming and secondly, a frequent use of perturbing operations is undesired in a PSO algorithm [42].So then to strengthen the diversification process, whenever a new position is found by a particle x in the swarm S, instead of updating S[x].lbest, the algorithm will search for an appropriate particle y in the swarm using a similarity measure and update S[y].lbest. The similarity measure is based on two criteria: the total collected profit and the travel cost/time of the identified solution. Two positions are said to be similar or identical if the evaluation procedure on these positions returns the same profit and a difference in travel cost/time that is lower than a value δ. Our update rules are based on Sha and Hsu [31] but simplified as follows. For convenience, the particle having the worst local best position of the swarm is denoted as S[worst].•Rule 1: the update procedure is applied if and only if the performance of new position S[x].pos is better than the worst local best S[worst].lbest.Rule 2: if there exists a particle y in S such that S[y].lbest is similar to S[x].pos, then replace S[y].lbest with S[x].pos.Rule 3: if no such particle y according to Rule 2 exists, replace S[worst].lbest with S[x].pos. Each successful application of this rule indicates that a new local best has been discovered by the swarm.The implementation of these rules was made efficient through the use of a binary search tree to sort particles by the performance of their local best positions using the two criteria. In the next section, the performance of our PSO on the standard benchmark for TOP is discussed.PSOiA is coded in C++ using the Standard Template Library (STL) for data structures. The program is compiled with GNU GCC in a Linux environment, and all experiments were conducted on an AMD Opteron 2.60GHz. In order to compare the performance of our approach with those of the existing algorithms in the literature, we use 387 instances from the standard benchmark for TOP [14]. These instances comprise 7 sets. Inside each set the original number of customers and customer positions are constant, however the maximum tour duration L varies. Therefore the number of accessible customers are different for each instance. The number of vehicles m also varies between 2 and 4.Our approach was tested using the same protocol as in [21,25,32]. For each instance of the benchmark, the algorithms were executed 10 times. The average and maximal scores as well as the average and maximal computational times were recorded. In order to evaluate separately the performance of different configurations or methods, the best known result in the literature for each instance, denoted by Zbest, is used as the reference score of the instance. These best results for all instances of the benchmark are collected from [2,8,16,21,32,33] and also from our PSO algorithms, but not from Chao et al. [13] because the authors used a different rounding precision and some of their results exceeded the upper bounds given in [10].For an algorithm tested on an instance, obtained solutions of 10 runs are recorded and we use Zmaxand Zavgto denote respectively the maximal and average scores of these runs. Then the relative percentage error (RPE) and the average relative percentage error (ARPE) are used to evaluate the performance of the algorithm. RPE is defined as the relative error between Zbestand Zmax. It was used in [25,32] to show the performance of the algorithm over 10 runs.(5)RPE=Zbest-ZmaxZbest·100ARPE is defined as the relative error between Zbestand Zavg. It was used in [25] to show the robustness of the algorithm over 10 runs. In other words, a small value of ARPE indicates a higher chance of getting a good score (or a small RPE) for a limited number of runs of the algorithm on the instance. The instances, for which there is no accessible customer (or Zbest= 0) are discarded from the comparison. The number of instances is then reduced to 353.(6)ARPE=Zbest-ZavgZbest·100For a set of instances, the respective average values of RPE and ARPE of the instances are computed to show the performance and robustness of the algorithm. For a benchmark composed of different sets, the average value of the latter ones on all the sets is computed to show the overall performance and robustness of the algorithm on the benchmark. As a complement measure for a benchmark, NBest is used to denote the number of instances in which Zbestare reached.Values of some parameters are directly taken from the previous studies of [8,16]. Therefore, we did not do further experiments on those parameters:•N, the population size, is set to 40.NIDCH, the number of local best positions initialized with the slow version of IDCH, is set to 5.pm, the local search rate, is set to1-iteritermax.δ, the similarity measurement of particles, is set to 0.01.c1, c2, the cognitive and social factors of the particles, are set to 0.5 (c1=c2=0.5).w, the inertia parameter, decreases gradually as the algorithm proceeds. It is initialized to 0.9 and multiplied by 0.9 after each iteration of the PSO.α, the control parameter of intuitive criteria of the BIA heuristic, is generated as follows. Two random numbers r1 and r2 are first generated in [0, 1] with a uniform distribution, thenα=1+2·r1r1+r2is computed.The most important parameter which could be up for discussion is the stopping condition k. We tested PSOiA on the 353 instances of the benchmark using varied values of k from 10 to 100 with steps of 10. In order to maximally exploit in these tests the crossover operator and the evaluation process, we set the probability ph of a particle to be moved out of its current position equal to 0.1. We will return to the ph parameter later (once k is fixed) to check whether it is over-tuned.Fig. 3illustrate the evolution of RPE, ARPE and the average computational time in terms of k. One may notice that from k=40, the algorithm starts to provide the best RPE and interesting values of ARPE. On the other hand, the computational time linearly increases in terms of k, hence the value k=40 were selected to present our final results of PSOiA.Next, we set k to 40 and varied the value of ph from 0 to 1 with a step equal to 0.1. Fig. 4show the evolution of RPE, ARPE and the average computational time in terms of ph. In these tests, the computational time linearly increases in terms of ph (with a small exception for ph=1.0) and value 0.1 is the right choice for the parameter.The results of PSOiA (k=40) on instances of Chao’s benchmark are then compared with the state-of-the-art algorithms in the literature:•SVNS proposed by Archetti et al. [2], tested on an Intel Pentium 4 2.80GHz.MA proposed by Bouly et al. [8], tested on an Intel Core 2 Duo 2.67GHz.SPR proposed by Souffriau et al. [32], tested on an Intel Xeon 2.50GHz.PSOMA (with w=0.07, the best configuration) described in [16] as the preliminary study of this work, tested on an AMD Opteron 2.60GHz.On the comparison between computers in use, machine performances of PSOiA, PSOMA, MA/MA10 [8] and SPR [32] are almost the same: recent dual-core processors with clock frequency varying from 2.50GHz to 2.67GHz. SVNS [2] used a computer with higher clock frequency (2.8GHz) but that was a Pentium 4. It is supposed to have a lower performance than the others.In [32], the authors of SPR algorithm talk about the 157 relevant instances of sets 4, 5, 6 and 7 and show only their results on these instances. Therefore, we will provide the comparison focused on these 157 instances. We also noted that results of SVNS were taken from the website of the first author of [2]. These results were updated in 2008 and the rounding convention problem reported in [8,21] was corrected. It also appears that these results are better than the ones published in the journal article [2]. Additionally, a different testing protocol which considered only 3 runs for each instance of the benchmark had been used for SVNS and MA. So in [16], the source code of MA [8] was received from the authors and turned to match the new testing protocol: 10 executions per instance. Results of this new test for MA is denoted by MA10.Our results are also compared with the other swarm intelligence algorithms available in the literature:•Sequential version of the Ant Colony Optimization (ACO) proposed by Ke et al. [21], tested on an Intel CPU 3.0GHz,Discrete Particle Swarm Optimization (DPSO) proposed by Muthuswamy and Lam [25], tested on an Intel Core Duo 1.83GHz.Table 1reports RPE averages for each data set of all methods. From this table, we observe that PSOMA (with very basic PSO components) already outperforms the other methods in the literature. This motivates our choice of testing the new optimal split procedure on PSO scheme instead of MA one. Regarding PSOiA, the results are almost perfect with zero RPE for sets 5, 6, 7 and only one instance was missed for set 4 with a very small value of RPE. Table 2reports ARPE averages for each data set of the standard benchmark. From that table, we observe that PSOMA is less robust than MA10 on data sets 4 and 7. However, it is more robust than MA10 on data sets 5 and 6. Finally, PSOiA is the most robust method. The ARPE average on all data sets of PSOiA is 0.0436% which almost equivalent to the RPE averages on all data sets of the state-of-the-art algorithms (SVNS, SPR and MA) reported in the literature (ranging from 0.0394% to 0.0519%) as shown in Table 1.Tables 3 and 4report the average and maximal CPU times respectively for each data set of all methods. From this table, we notice that ACO and SPR are fast methods. However, their performances are not as good as SVNS, MA/MA10, PSOMA and PSOiA as seen in Tables 1 and 2. SVNS is slower than the others. PSOMA is quite faster than MA10 but as mentioned above, MA10 is more robust than PSOMA. Computational efforts required for PSOiA and MA10 are almost the same. Based on a remark of [35] and our own verification, it is worthy to mention that the maximal CPU times of ACO method reported in [21] are in fact the maximal average CPU times, i.e. for each instance the average CPU time is computed, then the maximal value of these CPU times is reported for a whole set. Therefore, the maximal CPU times of ACO method are marked as n/a (not available) in Table 4.Table 5reports the number of instances (in percent) for which the value of ARPE is zero, which means that the results of all runs are identical or that the algorithm is stable. From this table, one may notice that PSOiA is stable in most cases (72%). Additionally, the performance analysis of SVNS, MA, SPR, PSOMA and PSOiA indicates that the results from data sets 4 and 7 are generally less stable than those from data sets 5 and 6. This can be explained by the differences between the features of those instances. Data sets 4 and 7 contain up to 100 customers for which both profits and positions are randomly distributed. On the other hand, data sets 5 and 6 have at most 64 customers arranged in a grid such that large profits are assigned to customers located far away from the depots.Finally, detailed results of MA10, PSOMA and PSOiA for the 157 instances are reported in Tables 7–10. For each instance, columns CPUavgreport the average computational time in seconds of the ten runs. Complete results of the 353 tested instances are available at http://www.hds.utc.fr/∼moukrim. According to these results, p4.4.n is the only instance of the whole benchmark from which PSOiA was not able to find the best known solution. One unit of profit was missed for this instance. Furthermore, a strict improvement was detected for instance p4.2.q with a new score of 1268 instead of 1267.From the previous section, we observe that PSOiA achieves a value of RPE of 0.0005%. Therefore, it would be very difficult to develop better heuristics for the current standard benchmark instances. In order to promote algorithmic developments for TOP, we introduce a new set of benchmark instances with a larger number of customers. Our new instances are based on the OP instances of Fischetti et al. [18] with the transformation of Chao et al. [14]. This transformation consists of designing the travel length limit of vehicles for TOP asLTOP=LOPm. In this formulation, m is the number of vehicles of the new TOP instance and LOPis the travel length limit of the vehicle of the former OP instance.We used instances from the two classes described in [18] to generate TOP instances. According to the authors, the first class was derived from instances of the Capacitated Vehicle Routing Problem (CVRP) [15,29] in which customer demands were transformed into profits and varied values of LOPwere considered. The second class was derived from instances of the Traveling Salesman Problem (TSP) [29] in which customer profits were generated in different ways: equal to 1 for each customer (gen1); using pseudo-random function so that the output values are in [1, 100] (gen2); using distance-profit function such that large profits are assigned to nodes far away from the depots (gen3).In total, 333 new instances were used in our test. It can be seen that PSOiA is very stable for a large part of those instances, especially the ones from the CVRP benchmark. So Table 11reports the results of PSOiA for which the value of ARPE is non-zero. A complete specification, including the number of accessible customers n, the number of vehicles m, the travel length limit L and the way to generate the profits for the customers gen, is also given for each instance. The values in the last row corresponding to Zavgand CPUavgcolumns respectively indicate the ARPE and the average computational time on the set of instances. All tested instances and the other results are available on the previously mentioned website.In addition, we also analyzed the computational behavior of PSOiA on the new instances according to the various generations of the profits (namely gen1, gen2 and gen3). In this analysis, for each TSP instance, three variants of TOP instances are available. This implies the same sample size of 93 instances per generation and provides a fair comparison. Table 6 reports the number of instances (in percent) for which the ARPE is zero and the average computational time CPUavgfor each generation. From this table, one may notice that PSOiA is more stable and requires less computational effort on generation gen1 (equal profits) than on generations gen2 (random profits) and gen3 (large profits distributed to customers located far away). Finally, it should be noted that the sample size to analyze the stability of PSOiA according to the positions of the customers is not statistically large enough to reveal the detail.

@&#CONCLUSIONS@&#
