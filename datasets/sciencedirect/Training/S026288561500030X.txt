@&#MAIN-TITLE@&#
A Self-adaptive CodeBook (SACB) model for real-time background subtraction

@&#HIGHLIGHTS@&#
This paper presents a Self-adaptive CodeBook background model for moving object segmentation in a video.Several new techniques are introduced to enhance the performance of standard CodeBook model.The proposed model gives better processing speed than the standard CodeBook model.New color model and the automatic parameter estimation mechanism help to achieve better accuracy than the standard CodeBook model.The proposed model gives a real-time performance and a good balance between segmentation accuracy and processing efficiency.

@&#KEYPHRASES@&#
Background subtraction,Video processing,Parameter learning,Background modeling,CodeBook model,

@&#ABSTRACT@&#
Effective and efficient background subtraction is important to a number of computer vision tasks. In this paper, we introduce a new background model that integrates several new techniques to address key challenges for background modeling for moving object detection in videos. The novel features of our proposed Self-adaptive CodeBook (SACB) background model are: a more effective color model using YCbCr color space, a statistical parameter estimation method, and a new algorithm for adding new background codewords into the permanent model and deleting noisy codewords from the models. Also, a new block-based approach is introduced to exploit the local spatial information. The proposed model is rigorously tested and has shown significant performance improvements over several previous models.

@&#INTRODUCTION@&#
Foreground objects in videos carry important information and are the main focus in most of the high-level computer vision applications. Usually, static or quasi-periodic regions of the visual scene are regarded as background, while moving objects in the scene are considered as foreground objects [1–5]. The precise localization of foreground objects from a video is a typical first step in many computer vision applications [6].Background subtraction is a common approach used for separating foreground objects from the background. The background subtraction framework comprises four steps: preprocessing, background modeling, foreground detection, and post-processing [1–4,7–8] as shown in Fig. 1.The preprocessing step includes removal of imaging noises by applying smoothing filters and sub-sampling to achieve fast processing. Background modeling is the core of any background subtraction algorithms and in this step an accurate model of the background of a visual scene is built. The background model can be a single reference image or an advanced statistical model, depending on the complexity of the scene. A good background model should be robust against video sensor noises and environmental changes in the background, but at the same time it should be sensitive enough to detect all objects of interest.In the foreground detection phase, each video frame is compared against the background model, and those pixels significantly deviating from the background model are classified as the foreground. Finally, blob analysis and image denoising methods are applied as the post-processing step to refine the foreground map. The final foreground map contains a mask for foreground objects only.Real-world video sequences contain several difficult situations, which makes background modeling and foreground detection a difficult problem [9–13]. The major challenges for background modeling techniques are as follows:•Camera shaking. If the video is taken with a hand-held camera, there are high chances that it will have an effect of camera shaking. Even with a slight change of the camera angle, pixel-level dynamics of video sequence can change quite drastically. Background modeling techniques not taking camera movement into account may give a large number of false positive results, i.e., labeling background pixels as foreground.Dynamic backgrounds. The video background can be dynamic, for example, water flowing in the river or trees swaying in the background. This motion is usually quasi-periodic, and thus gives multiple layers of background, which demands multi-model background modeling techniques. There are many environmental factors in outdoor videos that control the behavior of background layers, and at times it changes from quasi-periodic to irregular motion. This irregular background motion is difficult to distinguish from foreground motions.Illumination changes. Illumination changes can be local (e.g., moving objects cast shadows or highlights due to the reflection from the surface of moving objects), or global (e.g., a person entering a room and switching on a light will change the global illumination conditions of the scene). This type of illumination changes (both local and global) is usually rapid and traditional adaptive background modeling techniques cannot deal with them effectively.Real-time and online processing. Most computer vision applications demand real-time and online processing, and clean labeled training data are not usually available beforehand. This suggests that background modeling techniques should be based on online learning and have the capabilities to deal with noisy training data. In addition, this imposes the constraint for real-time processing.In this paper, a Self-adaptive CodeBook (SACB) background model is presented for moving object segmentation in a video. The proposed SACB model is designed to address the limitations of the CodeBook model and to exploit the close proximity within the local neighborhood using a new block based approach. The major components of the paper are a more effective color model, a statistical parameter estimation method, a new algorithm for keeping the background model compact, and a block based approach to use local spatial information.The rest of the paper is organized as follows. Section 2 describes the CodeBook model and highlights its limitations. Our proposed model is presented in Section 3. Section 4 describes the evaluation methodology, while experimental results and discussions are presented in Section 5. Finally, Section 6 highlights the main conclusion of this study and points to future directions.The CodeBook is one of the popular real-time background models for moving object detection [14]. Several of its enhancements have been proposed [15,16]. The basic CodeBook model has two main phases: an initial training phase where the codebook is constructed and a foreground detection phase.In the initial training phase, a new sample is compared with existing codewords in codebook to determine the matching codeword. If matching codeword is found, it is updated. Otherwise, new codewords are created and added into the codebook. In this model, color distortion and brightness bound are used in the criterion to determine the best matching codeword [14].The codebook obtained in the previous step may become quite large because it may include moving objects in the training sequence. Thus, at the end of the training process, every codeword is analyzed and the codewords having large λ the longest interval during which it has not re-occurred are considered as foreground codewords, thus filtered-out from the codebook.The basic Codebook model is effective enough to model a scene where geometry of the scene does not change over time. However, in reality scenes may change after the training, for instance, a parked car starts moving, or the sun comes out. To cope with these types of situations, a layered CodeBook model [14] was proposed by introducing an additional layer called the cache Codebook (H).In the layered model, during the foreground detection phase, the incoming pixel's value is compared against the permanent CodeBook model. If a matching codeword is found, the pixel is classified as background and is updated. Otherwise, the pixel is classified as foreground and checked against the cache CodeBook model. If a matching codeword is found in the cache CodeBook model, the model is updated. Otherwise, a new codeword is created, and added into the cache CodeBook H. Then, codewords staying long enough in the cache CodeBook are moved to the permanent CodeBook model. After that, noisy codewords are removed from both cache and permanent CodeBook models [14].The CodeBook model [14], and its enhancements [15–17] perform better than a number of state-of-the-art methods both in terms of segmentation accuracy and processing time. The cylindrical color model, unconstrained training and novel matching criteria are some of its distinguished features. However, it is a parametric model with the following key parameters:•The thresholds used to filter out noisy codewords (Tdel).A threshold used for moving codewords from cache to the main codebook (Tadd).Color distortion thresholds used in the training (ε1) and segmentation (ε2) phases.A learning rate (α).It is not always adequate to use predefined values for these parameters. Thus, manual parameter tuning is required to achieve good results for a typical scene, which is often a cumbersome and tricky task. Therefore, there is a need for further investigation in this regard to devise an automatic mechanism for optimal parameter selection. To the best of our knowledge, no study exists on the investigation of how different configurations of the CodeBook parameters affect its performance.Furthermore, the color model used in the CodeBook model has limitations. Usually, variation in bright pixels value due to the illumination variation is higher than variation for dark pixels. Consequently, the CodeBook model is likely to create much more codewords for bright pixels than dark pixels. Ideally, one codeword should be able to model illumination variations. However, this is not the case for the basic CodeBook model due to the fixed codeword matching criteria. Moreover, the CodeBook color model involves a lot of calculations. Therefore, there is a need to revisit the CodeBook color model.In this section, we extend the basic CodeBook model by proposing several new techniques:•A modified efficient color model using the YCbCr color space;A statistical parameter estimation method;A new algorithm for adding new background codewords into the permanent model and deleting noisy codewords from the models;A new block based approach to utilize the local spatial information.Usually, variation in bright pixel values due to the illumination variation is higher than the variation for dark pixels. As you can see in Fig. 2, even in the same video scene, the brighter pixels experience significantly higher illumination variation compared to the darker pixels. For example, pixels 1 and 4 have bigger and thinner spreads compared to the smaller and fatter spreads of pixels 2 and 3 in Fig. 2. Therefore, in this case, due to the fixed color distortion threshold, the CodeBook model will create more codewords for bright pixels than those for dark pixels. As most of these codewords will belong to the same layer of background, it thus makes the CodeBook model less stable for dark pixels. Moreover, the CodeBook color model involves a lot of computational complexity.To remedy these limitations, we introduce a modified color model, where we employ YCbCr color space to efficiently and effectively compute the color distortion. As the YCbCr color space is less sensitive to noise [18] and models color illumination information separately, it is a good choice for separate evaluation of color distortion and brightness bounds. In the modified color model, the input pixel is represented by Xt=(Cb,Cr), and the mean value for a codeword ciis notated asVi=C¯b,C¯r. The color distortion is computed as follows:(1)colorDistortionXt,Vi=δ=Xt2−P2,where P2 is calculated as follows:(2)P2=Xt2cos2θ=Xt,ViVi2,whereXt2=Cb2+Cr2,Vi2=C¯b2+C¯r2andXt,Vi=CbC¯b+CrC¯r. In contrast to the original color model, we use only color information to compute color distortion between an input pixel value and a codeword.Also, the computation of brightness valueI←R2+G2+B2is modified to I←Y. In the proposed model, we separately store the color and brightness information, so the new structure of the codeword is Vi,Īi, and a six-tuples auxi={ĬIˇ,Î,f,λ,p,q}. The brightness range is calculated as follows:(3)brightnessBoundIIlowIhi=true,ifIlow≤I≤Ihi,false,otherwise,where(4)Ilow=maxIˇ−σ,0,and(5)Ihi=minI^+σ,255whereσ=σY¯,t2. Here, instead of scaling in a fixed ratio as in the original model, we use σ to control the lower and higher brightness bounds. Since σ is directly estimating the variations from data, it can give a better estimation of sensor noises.We maintain mean and variance statistics for each codeword to automatically estimate the color distortion and brightness bound thresholds. The proposed model is adapted using an exponential smoothing filter. Specifically, the mean and variance of codewords are updated as follows:(6)Vt=1−αVt−1+αXtI¯t=1−αI¯t+αYt,and(7)σCb,t2=1−ασCb,t−12+αC¯bt−Cbt2,σCr,t2=1−ασCr,t−12+αC¯rt−Crt2,σY¯,t2=1−ασY¯,t−12+αY¯t−Yt2,where α is a learning rate, Xt=〈Cb, Cr〉, and Ytis an intensity value for current pixel value (Y, Cb, Cr). In the real world videos, each channel experiences different levels of variation, thus they cannot be treated as identical. Therefore, we model Y, Cb, and Cr channels separately.In the basic CodeBook model, parameters γ and ρ control the adaptation speed, but finding an optimal value of the learning rates for a particular scene is a hard task. In this regard, instead of two different learning rates γ and ρ, we introduce a local adaptive learning rate α for each codeword. The original idea is inspired from the adaptive learning rate proposed by [19]. However, instead of formulating it in terms of some global fixed learning rate, we use frequency of occurrence (f) to model the local adaptive learning rate as:(8)α=1f,where f is the frequency of occurrence set to one at the creation of codeword. This means, initially, that the learning rate is high and model is updated quickly. However, as more sample matches with the codeword, its frequency increases and learning rate is gradually reduced. Thus, the codeword update becomes more stable. This simple modification significantly improves the model convergence speed and accuracy as shown later in Section 5.Algorithm 1Block based CodeBook construction (proposed).Furthermore, we use the standard deviation as a threshold for color distortion as follows:(9)εt=σCb,t2+σCr,t22.Our empirical study of different scenes revealed that the most background codewords have less variance than foreground codewords. Fig. 3gives an example, where four codewords related to selected pixel are displayed. As can be seen, the background codewords (CW1 and CW2) have much less variance compared with other foreground codewords. This motivated us to use variance (σi2) defined in Eq. (7), to refine the codebook.There are some scenarios where background codewords may have high variance because of, for example, reflecting surfaces in the background or irregular motions. As can be seen in Fig. 4, a pixel at the tip of the branch of the tree experienced an irregular motion, leading to its background codeword with a higher variance. Similarly, in some cases some foreground pixels may have low variance, for example, some smooth colored big object moving very slowly in video. To handle these cases, we introduce the minimum and maximum (TD1, TD2, TM1, TM2) bounds along with the variance. Here, TD1, TD2, TM1 and TM2 are the thresholds used to delete foreground codewords from models M and H and move background codewords from models H to M respectively. These thresholds are preset fixed values which keep the simplicity of the models. The detailed formulation is as follows:(10)ΘMcx=true,ifσx2>16∧p−q>TD1∨p−q>TD2false,otherwise(11)ΘHcx=true,ifσx2>16∧p−q>TD1∨p−q>TD2false,otherwise(12)Φcx=true,ifσx2>16∧λ>TM1∨λ>TM2false,otherwise.Here, ΘM(cx) is a function for deleting codeword cxfrom M, ΘH(cx) is a function for deleting codeword cxfrom H, while Φ(cx) is a function for moving codeword cxfrom H to M. Also, σx2 is an average variance of all three channels (Y, Cb, Cr) as computed in Eq. (7). Moreover, since most of the background codewords have variance less than 16, we thus selected 16 as the threshold criterion to differentiate between background and foreground codewords.We also introduce a new block-based CodeBook model in which instead of codebooks for individual pixels, the codebooks are constructed for non-overlapping blocks. In this model, instead of using global features of the block, we use each pixel in the block as a training datum. In this way many benefits can be achieved. Firstly, fewer codewords will be created, which in return means less memory and less computation. Second, we are also able to exploit local spatial dependency between neighboring pixels for better segmentation results. A detailed procedure for the proposed model is presented in Algorithms 1 and 2.We used CDnet dataset [10]11changedetection.net.to evaluate the performance of our proposed model and compare it with the state-of-the-art methods.Algorithm 2Block-based layered CodeBook model, foreground detection and online model update.The CDnet dataset contains 31 indoor and outdoor video sequences, which covers various challenging real life sceneries. The environments in these videos are offices, highways, sidewalks, sports, rivers, fountains, parkings, streets, bus stations, library, dinning room and other private and public sites. These video sequences capture a variety of different real life objects such as pedestrian, cars and boats. Also, different weather conditions such as sunny, cloudy and raining along with day and night-times are covered in this dataset. The spatial resolution of the video sequences varies from 320×240 to 720×576. The length of videos varies from 1000 to 8000 frames. The videos are grouped into six different categories, where each category represents a different type of challenge such as dynamic background and shadows. Importantly, for precise validation, this dataset provides ground truth at pixel resolution for each frame in a video. This makes detailed quantitative analysis and comparison possible.The qualitative and quantitative measurements were used to gauge segmentation accuracy. We use Percentage of Wrong Classifications (PWC), Recall, Precision and F-measure [8,19–22] for quantitative performance analysis. These are defined as follows:(13)PWC=100×FN+FPTP+FN+FP+TN,(14)Recall=TPTP+FN,(15)Precision=TPTP+FP,(16)F‐Measure=2×Precision×RecallPrecision+Recall,where TP is true positive, FP is false positive, FN is false negative and TN is true negative.In this section, we present and analyze the experimental results obtained by the proposed model for different challenging video sequences. More specifically, we comprehensively examine the different aspects of the proposed model such as the proposed color model and automatic parameter selection techniques. Furthermore, we present the results for the experiments that we performed to determine the optimal block size. In the final part of this section, we present a comparative evaluation of the proposed model with 8 state-of-the-art methods. The parameter settings for the standard CodeBook (CB) and the proposed model (SACB) are listed in Table 1.To determine the performance of the proposed color model, we compared the results obtained by the standard CodeBook model and the standard CodeBook model with the proposed color model. In these experiments, we keep all the other parameters unchanged except the color model. Figs. 5 and 6present PWC for the “bungalows” and the “peopleInShade” video sequences in the CDnet dataset. Both of these outdoor videos contain complex illumination variations and shadows. As can be seen in Figs. 5 and 6, the proposed color model significantly improved the performance of the standard CodeBook model. These results are further confirmed by the qualitative results presented in Fig. 7, where our new color model helps to produce better foreground maps.To analyze the effect of the proposed local adaptive learning rate, we performed experiments on two video sequences (“cubicle” and “peopleInShade”) in the CDnet dataset. These indoor and outdoor videos contain complex illumination fluctuations and present higher sensor noise. We use quantitative results to evaluate the performance of the proposed local learning rate in comparison with the single fixed global learning as in the standard CodeBook model.Figs. 8 and 9illustrate the codeword updates for two selected pixels in the two different video sequences. In these experiments, to examine the effect of proposed learning rate, we obtained the results for the standard CodeBook and the standard CodeBook with the local adaptive learning rate. Here all the other parameters were kept the same except the learning rate. As you can see in Figs. 8 and 9, the proposed model converged to the desired mean value of the codewords significantly faster than the standard CodeBook model did. The standard CodeBook model with the proposed learning rate takes less than 100 frames to converge to the desired mean value of the codeword, whereas the standard CodeBook model with single global learning rate takes more than 400 frames to converge to the desired mean value. In this way, our proposed learning rate can save a significant number of false positives in the beginning. Furthermore, as more evidence accumulated for the codewords, the proposed learning rate is adapted to more stable model updating.To determine the optimal block size for the proposed block-based model, we performed experiments on different block sizes and analyzed both quantitative and qualitative results on several video sequences from the CDnet dataset. Fig. 10presents the PWC obtained at different block sizes ranging from 2×2 to 9×9 for two different video sequences. As one can see, the accuracy of the proposed model up to the block size 3×3 is quite good, but the detection accuracy starts to deteriorate for block size 4×4 onwards. The main reasons for this is the wrong detections at the boundaries of the foreground objects and coarse detection, as confirmed from the qualitative results shown in Fig. 11.Fig. 12presents processing speed obtained by the proposed model on different block sizes. As one can see, there is a significant improvement in processing speed up to the block size 3×3. Therefore, we chose a block size of 3×3 as a good trade-off between accuracy and speed.For a comprehensive comparative evaluation of the proposed model, we have selected 8 state-of-the-art background subtraction algorithms. The original codebook model [14] along with two frequently cited models representing the two important families of background models: Kernel Density Estimation [2] and Mixture of Gaussian (MoG) model [23]. Also, we selected 3 well known recent improved versions of these models: AMoG [24], KMoG [25] and SMoG [26]. Furthermore, we also include two top performing SAmple CONsenses (SACON) methods: ViBe [27] and PBAS [11]. These are compared with our proposed model labeled as SACB. In all of our experiments, we have used the parameter setting as described in Tables 1 and 2for all experiments unless otherwise stated.We present both qualitative and quantitative results on the “shadow” and the “dynamicBackground” categories in the CDnet dataset. The “dynamicBackground” category contains 6 video sequences, and each video presents different dynamic background challenges such as flowing water and swaying trees in the background. Fig. 13shows the precision-recall scatter plot of the results given by the algorithms on this category. This figure illustrates the trade-off between precision and recall. As one can see, SACB obtains better precision and recall for all the video sequences, while all the other methods do not obtain good trade-off between precision and recall. For instance, the CB model gives good precision and recall for three video sequences, but obtains lower precision or recall for other three videos as shown in the second column of the first row in Fig. 13. Also, the MoG, AMoG, SMoG and KDE give good recall, but significantly lower precision for some of the video sequences.Fig. 14presents a comparative box plot of the F-Measure results for dynamic backgrounds. SACB along with ViBe achieves better overall F-Measure for dynamic background than other models. The proposed model along with KMoG gives very small variation in their performance for different video sequences. However, KMoG gives very low performance for one video sequence shown as an outlier. This is confirmed by the example qualitative results presented in Fig. 15. The proposed model along with ViBe obtains a foreground detection map very close to the ground truth.Fig. 16shows comparative precision-recall results obtained by the algorithms for the “shadows” category in the CDnet dataset. It contains 6 indoor and outdoor challenging videos, which presents variety of illumination conditions, such as reflective surfaces in the background, highlights and shadows. PBAS, SACB and ViBe give better precision and recall trade-off for all video sequences compared with the other models. Furthermore, Fig. 17presents the box plot of average F-Measure result obtained by the algorithms. As shown in the figure, PBAS achieves the best results along with SACB, which is confirmed by the qualitative results presented in Fig. 18.To check the statistical significance of the results, we performed a paired t-test (two-tailed) with a significance level of 0.05. Table 3presents the values of the t-test results. As we can see, our proposed model gives significantly better results compared with most of the previous models. However, PBAS and ViBe gives fairly comparative results for dynamicBackground, while SMoG, ViBe and KDE gives fairly comparative results for the shadow category.To measure the processing speed, we implement all the algorithm in C++ using the OpenCV library and ran them on an Intel machine with 2.6GHz CPU, 3GB RAM and the Microsoft Windows XP operating system. Fig. 19presents the comparative results for processing speeds for videos of resolution 320×240 (width×height). As shown in the figure, ViBe gives the highest number of frames per second. Furthermore, the CodeBook-based and MoG-based techniques give fairly good processing speeds. However, PBAS and KDE give very low processing speed, and thus are not suitable for real-time applications.

@&#CONCLUSIONS@&#
In this paper, we have presented a Self-adaptive CodeBook background model for moving object segmentation in a video by introducing several new techniques. Our proposed model gives better processing speed than the standard CodeBook model. This improvement is obtained by the efficient color model and the block-based modeling. Also, the modified color model and the automatic parameter estimation mechanism helps to achieve better accuracy against illumination changes than the standard CodeBook model. Furthermore, the proposed model achieved favorable or competitive performance when compared with a few state-of-the-art methods. The proposed model gives better precision-recall balance than the other methods. Finally, it gives a good balance between segmentation accuracy and processing efficiency.Nevertheless, our proposed SACB model and most other models compared within this study are all pixel-based. They all have their limitation when dealing with scenes shot by a moving camera. There are, however, a few studies on adapting pixel-based background models for moving-camera scenes [28,29]. There is also a notable direction of using superpixels for efficiency improvement [30,31]. Using these ideas, we intend to extend our computational model to background–foreground separation in moving-camera scenes.