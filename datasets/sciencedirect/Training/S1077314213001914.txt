@&#MAIN-TITLE@&#
SnooperText: A text detection system for automatic indexing of urban scenes

@&#HIGHLIGHTS@&#
We describe SnooperText, a new text detector specialized for photos of urban scenes.Candidate characters are found by toggle segmentation and winnowed by shape descriptors.Candidate characters are validated by a HOG-based descriptor.SnooperText is better or at least as accurate and robust as other state-of-the-art detectors on standard benchmarks.SnooperText was used in iTowns, a prototype system for virtual street-level urban navigation.

@&#KEYPHRASES@&#
Text detection,Text region classification,Histogram of oriented gradients for text,Text descriptor,Textual indexing in urban scene images,

@&#ABSTRACT@&#
We describe SnooperText, an original detector for textual information embedded in photos of building façades (such as names of stores, products and services) that we developed for the iTowns urban geographic information project. SnooperText locates candidate characters by using toggle-mapping image segmentation and character/non-character classification based on shape descriptors. The candidate characters are then grouped to form either candidate words or candidate text lines. These candidate regions are then validated by a text/non-text classifier using a HOG-based descriptor specifically tuned to single-line text regions. These operations are applied at multiple image scales in order to suppress irrelevant detail in character shapes and to avoid the use of overly large kernels in the segmentation. We show that SnooperText outperforms other published state-of-the-art text detection algorithms on standard image benchmarks. We also describe two metrics to evaluate the end-to-end performance of text extraction systems, and show that the use of SnooperText as a pre-filter significantly improves the performance of a general-purpose OCR algorithm when applied to photos of urban scenes.

@&#INTRODUCTION@&#
Here we describe SnooperText, an algorithm for the detection of text embedded in images or videos of urban scenes. This is a challenging problem in computer vision [1] with many potential applications, such as traffic monitoring, geographic information systems, road navigation, and scene understanding.SnooperText was developed specifically for use in iTowns [2], a pilot project for resource location and immersive navigation in urban environments, similar to Google’s Street View [3]. The main raw data for the iTowns project is a collection of GPS-tagged high-resolution digital photos of the city, including building façades, taken with a set of car-mounted cameras. The mean viewpoint spacing between photo sets is about 1m. See Fig. 1.The frontal images of the building façades are processed offline to extract any legible textual information, such as street and traffic signs, store names, and building numbers. The extracted strings are then stored in a geo-referenced database, which is used to answer textual queries by users—for example, to locate the addresses of stores with a specified name or selling a specified product. The user is then offered a navigable 3D view of the location, created by suitable projection of pre-stitched image mosaics. See Fig. 2.A project like iTowns could easily generate hundreds of thousands of such mosaics in a single city. The manual annotation of all these images with the visible textual information would be very time consuming and probably impractical. Clearly, automated algorithms for this task are highly desirable.The difficulties in this task mainly come from the diversity of the texts (including extreme text size and font variations, and tilted or curved baselines), the complexity of the backgrounds (including many vaguely text-like objects such as fences, windows, and cobblestones) and difficult illumination conditions. OCR algorithms designed for scanned documents perform very poorly on such photos. See Fig. 3(a). Much better results are obtained by applying an OCR algorithm to the output of a text detector designed specifically for such images, as illustrated in Fig. 3(b).The SnooperText detector initially locates candidate characters on the images by using image segmentation and shape-based character/non-character binary classification. The candidate characters found in this step, represented by their bounding boxes, are grouped by simple geometric criteria to form either candidate words or candidate text lines. These candidate text regions are then validated by a binary text/non-text region classifier that rejects any candidate region that does not appear to contain a single line of text. This classifier uses the T-HOG descriptor [4], which is based on the multi-cell histogram of oriented gradients (HOG) of Dalal and Triggs [5]. These steps are performed in a multi-scale fashion, in order to efficiently handle widely different character sizes and to suppress irrelevant texture details inside the characters. Finally, the regions found by SnooperText are fed to Tesseract’s back-end for OCR processing [6]. See Fig. 4.Tests show that the accuracy of SnooperText on street images is comparable to that of the best text detectors described in the literature [1,7–11], and better than Tesseract’s own text detector.The SnooperText detector described and tested here is an improved version of the detector presented at ICIP 2010 [12] and differs from the work of Fabrizio et al. [45] in several ways. The differences include the use of the T-HOG descriptor for text-region validation, the multi-resolution scheme to perform text detection, the OCR integration for text recognition, the embedding of the whole process in a real search-engine scenario, and various novel experimental evaluations. In addition, an optimization of the whole text detection system is proposed, including the tuning of various internal parameters of the algorithm, such as the number and spacing of levels of the multi-scale pyramid and the range of character sizes considered at each level. All these improvements make the whole system performing significantly better than [12,45] with various evaluations on four recent text detection and recognition benchmarks. Unlike [45], we also propose in this paper the use of SnooperText in the search-engine application of the iTowns project and as a pre-filter for general-purpose OCR algorithms when applied to photos of urban scenes. SnooperText is implemented in Java and its source code is available at the project’s site [13].This paper is organized as follows. In Section 2 we review the literature on text detectors and text/non-text region classification, with emphasis on urban photos. The SnooperText detector is described in Section 3, and its experimental evaluation is reported in Sections 4 and 5. The limitations of SnooperText are described in Section 6.There is an extensive literature on text detection. The surveys of Jung et al. [14] and Liang et al. [15] cover some systems up to 2005. Many published text detectors are devoted to specific contexts, such as postal addresses on envelopes [16], cursive handwriting [17], and license plates [18]. Only a few systems have been designed specifically for photos of outdoor scenes [1,7,8,19–21].Text detection algorithms can be classified into two categories. A bottom-up algorithm first attempts to identify probable characters, which are then grouped into words or texts. A top-down algorithm first attempts to find regions of the image that appear to contain text, and then tries to split those regions into characters.The system of Hinnerk Becker [22] (winner of the 2005 ICDAR challenge) is an example of bottom-up solution. It uses an adaptive binarization scheme to extract character regions which are then combined into text lines according to certain geometrical constraints. The detector of Alex Chen et al. [22] (second place in the 2005 ICDAR challenge) follows the top-down approach: it identifies probable text regions of the image by their statistical properties, which are then segmented into presumed characters.In 2007, Mancas-Thillou and Gosselin [19] proposed another top-down approach. They focused on the segmentation and extraction of characters, assuming that the text-containing regions were previously identified. They used pixel clustering by color similarity and log-Gabor filters to segment characters. This approach is prone to fail in those texts with similar colors for foreground and background.In 2010, Epshtein et al. [7] proposed a bottom-up approach, that they called Stroke Width Transform (SWT), to detect characters in images. They used the pixel gradients orientation over image edges to determine a “local stroke width”, and gathered pixels with similar stroke widths into candidate characters. (They also provided one of the image datasets [23] that we used in our tests.)In 2011 and 2012, Chen et al. [8] and Neumann et al. [10], proposed bottom-up methods based on Extremal Regions (ER). Chen et al. used Maximally Stable Extremal Regions (MSER), which are a subset of ER, for edge-enhancement in order to candidate character detection. The letter candidates were then filtered out using stroke width information computed by the SWT. However, as observed by Neumann et al., MSER detectors have problems with blurry images and characters with low contrast. Therefore, Neumann et al. used all ERs and classified them as being characters or not by developing original features.Also in 2011, Pan et al. [9] proposed an hybrid multi-scale approach for text detection. They used a sliding window detector, in each scale of the pyramid, composed by a cascade of classifiers trained over HOG features, to initially estimate the text positions. The estimates were then used to enhance the original image in order to help the character segmentation. Then, the authors performed character classification and grouping.In 2012, Yi et al. [11] and Yao et al. [1] proposed bottom-up methods that use the stroke width information to character identification. As observed by Yi et al., the letter boundary, used by the SWT, may be broken or connected to a non-text object due to background interference. To avoid this problem, the authors proposed to combine edge pixel clustering and the structural analysis of the stroke boundary with a color assignment procedure. Characters were grouped by geometric criteria. Yao et al. used the SWT for character extraction and two layers of filters based on geometric and statistical properties, as well as a classifier trained with scale and rotation invariant features to reject non-text characters found by the SWT. The character grouping was done by considering the stroke properties, geometric and color features of nearby characters. A greedy hierarchical agglomerative clustering method was also applied to aggregate character pairs into candidate chains.Comparatively little has been published about text/non-text region classification algorithms, although they are often present as post-filters in many text detectors.Text/non-text region classification is often cast as a texture classification problem, and several texture descriptors have been considered in the literature. For instance, in 2004, Kim et al. [24] described a text recognizer that decomposes the candidate sub-image into a multi-scale 16×16 cell grid and computes wavelet moments for each block. Then each block is classified as text or not using an SVM. The ratio of text to non-text outcomes is used to decide if the entire sub-region is text or non-text. In 2005, Ye et al. [25] described a similar text recognizer based on multi-scale wavelet decomposition; however, they used more elaborate features, including moments, energy, and entropy. In 2004, Chen and Yuille [26] proposed a descriptor that combines several features, including 2D histograms of image intensity and gradient, computed separately for the top, middle and bottom of the text region, as well as for more complex slices subdivisions of the image—89 features in total.Other text detectors, such as the one described by Anthimopoulos et al. [27] in 2010, have used descriptors based on multi-scale local binary patterns (LBP) introduced by Ojala et al. [28]. Their descriptor has 256 features.In 2012, Yi et al. [11] proposed a text line descriptor that combines the Gabor filter with gradient and stroke information. They used the block patterns proposed by Chen and Yuille. Their descriptor has 98 features.The use of gradient orientation histograms (HOGs) as texture descriptors was introduced by Dalal and Triggs in 2005 [5], for human recognition. HOG-based descriptors have since been used for other object recognition and tracking problems [29]. They have been used in some recent text recognizers. The classifier described in 2008 by Pan et al. [30] partitions the candidate sub-image into 14 cells, as proposed by Chen and Yuille, but computes for each cell a 4-bin HOG complemented by a 2×3 array of LBP features. Their resulting descriptor has 140 features.Other HOG-based text recognizers have been proposed in 2009 by Hanif and Prevost [31] for single-line text, and Wang et al. [32] for isolated Chinese and Roman characters as well as single-line text. Hanif and Prevost’s descriptor has 151 features (16 cells, each with an 8-bin HOG and a standard deviation, plus 7 cell mean differences). The descriptor of Wang et al. has 80 features (8 cells, each with a 8-bin HOG, 1 mean difference and 1 standard deviation).The last stage of a text extraction system is to parse the candidate text-containing regions of the images to yield the text strings.Robust OCR algorithms especially designed for images of urban scenes is an active area of research, and some recently advances were described by Wang et al. [33], Mishra et al. [34] and Neumann and Matas [10]. However, an evaluation of OCR algorithms is beyond the scope of this paper, since our contribution is limited to the text detection part.As shown in Fig. 4, the SnooperText detector consists of four main modules: image segmentation, character filtering, character grouping, and text region filtering. These modules are applied at various scales of resolution, as described in Section 3.5.The segmentation algorithm used in SnooperText was developed by Fabrizio et al. [35]. It is a modified version of Serra’s toggle mapping[36], a morphological operator for local contrast enhancement and thresholding, using morphological erosions and dilations [37] to define the local foreground and background levels.Specifically, in order to segment an input imageI, we first compute a local background imageBby gray-scale erosion (neighborhood minimum) and a local foreground imageFby gray-scale dilation (neighborhood maximum), using an 11×11 square structuring element. Note thatB(p)⩽I(p)⩽F(p)for every pixel p. Then each sampleI(p)is mapped to a ternary class valueD(p)∈{0,1,2}as follows. If|F(p)-B(p)|is less than a fixed threshold cmin, thenD(p)is set to 1 (indeterminate). Otherwise,D(p)is set to 0 (presumed background) or 2 (presumed foreground) depending on whether the relative brightness|I(p)-B(p)|/|F(p)-B(p)|is less than or greater than another threshold cmed.Since the thresholding is not symmetrical between dark and light regions, and target scenes often have light text on dark background, the segmentation is repeated on the negative (pixel-wise complemented) image. See Fig. 5.The segmented foreground regions of the positive and negative images are then screened to identify plausible characters. First, the module checks simple size and aspect ratio constraintshmin⩽h⩽hmaxrmin⩽h/w⩽rmaxwhere w and h are the width and height of the segment’s axes-aligned bounding box, and hmin, hmax, rmin and rmax are parameters of the module. See Fig. 6(a). Each segment that satisfies these constraints is then tested with a more elaborate character/non-character classifier based on the shape of the segmented region; see Fig. 6(b).The shape classifier is based on three scale- and rotation-invariant shape descriptors extracted from the segmented region: Fourier moments, pseudo-Zernike moments, and an original polar encoding [35]. These descriptors are fed to three separate SVM classifiers, whose numeric outputs are packed as a three-dimensional vector and fed to a final SVM classifier [38]. The output of the final SVM is then thresholded to yield a binary character/non-character decision. See Fig. 7.SnooperText’s character grouping module joins the candidate characters found by the character detector into text regions – which may be either words or text lines – according to geometric criteria defined by Retornaz and Marcotegui [39]. These criteria take into account the heights h1, h2 and widths w1, w2 of the two bounding boxes, as well as the coordinates (x1,y1) and (x2,y2) of their centers. See Fig. 8.Specifically, leth=min(h1,h2),dx=|x1-x2|-(w1+w2)/2anddy=|y1-y2|.Note that dxis negative if and only if the two boxes overlap in the x direction. Then the two boxes are said to be compatible—that is, assumed to belong to the same text word or line—if and only if|h1-h2|<t1hdx<t2hdy<t3hwhere t1, t2 and t3 are parameters of the module. The parameter t2, in particular, determines whether the groups will be words or text lines.These criteria are applied to all pairs of detected characters. The groups are the equivalence classes of the transitive closure of this compatibility relation.In the iTowns application we found that grouping characters into words often failed for store names because of extra-wide spaces used between characters. Increasing t2 to cover those cases was not feasible because it would cause words to be joined in texts with normal inter-character and inter-word spaces. To get around that problem we run the character grouping module twice, with increasing values t2′ and t2″ for the relative separation limit, but considering in the second pass only those character candidates that were not joined to any group by the first pass. Characters candidates that remain ungrouped after both passes are discarded; this requirement normally eliminates a large fraction of the false positives (non-character regions classified as characters by the previous steps).Each group is then summarized by a single axis-aligned rectangle, which is the bounding box of its component characters. See Fig. 9.The grouping module is applied separately to the candidate characters found in the segmentation of each version of the image, positive and negative. Each version produces a list of rectangles, each rectangle being a candidate text line region. These two lists are then merged, and any two regions that have significant overlap (70% of the area of the smaller box) are fused into a single candidate text line region.The task of SnooperText’s text filtering module is to examine the image contents of each candidate region that is output by the character grouping module, and discard those that do not seem to contain a text line. Specifically, this stage is intended to eliminate those spurious text line candidates that result from two or more non-character image segments that passed the character filtering module and were accidentally grouped together. See Fig. 10.The text filtering module is basically a texture classifier based on the T-HOG descriptor [4]. The latter is a variant of Dalal and Triggs’s R-HOG descriptor [5], specialized to capture the gradient distribution characteristic of character strokes in occidental-like scripts. The T-HOG descriptor is fed to an SVM classifier, whose output is thresholded to give a binary text/non-text region classification.The T-HOG descriptor is based on the observation by Chen and Yuille (2004) that different areas of a text have distinctive distributions of gradient orientations [26]. The reason is that the strongest gradients are generally perpendicular to the strokes that form the characters.The HOG-based text/non-text discriminators described in the literature, generally divide the image into a two-dimensional array of nx×ny cells and compute a separate histogram of gradient orientations with a fixed number nb of bins within each cell, as Dalal and Triggs did for human body recognition [5]. The resulting multi-hog descriptors, that are often complemented with other statistics, typically have more than 100 features.However, while a two-dimensional cell array may be justifiable for isolated characters, it does not seem to be useful for multi-character texts of variable width. In such texts, the gradient distribution is largely independent of horizontal position; therefore, a cell layout with vertical cuts increases the size of the descriptor without providing any additional relevant information. Indeed, through extensive experiments [4] we confirmed that, for any descriptor length, partitioning the image into a small number of horizontal stripes (between 3 and 7) was generally more effective than a two-dimensional cell arrangement. Moreover, near-optimal results could be obtained with relatively small descriptors.We also found that pre-scaling the given text region to a small fixed height H (between 20 and 30 pixels), preserving its aspect ratio, was more effective than computing the HOGs at the original image resolution. This resizing step seems to provide a good balance between preservation of useful detail and removal of noise and spurious texture.The detailed description of the T-HOG descriptor and its experimental analysis have been published separately [4]. In brief, the sub-image delimited by the candidate rectangle is extracted from the input imageI, converted to gray-scale, scaled to the fixed height H, and normalized with a Gaussian weight window to compensate for local variations of brightness and contrast. This normalized image is then divided into ny horizontal stripes, the image gradient is computed at each pixel within the stripe, its direction is quantized into a small number nb of equal angular ranges, and the corresponding bins of the histogram are incremented. Opposite directions are identified, so each bin is π/nb radians wide. The T-HOG descriptor is the concatenation of those ny histograms.The contribution of each pixel to the histogram is weighted by the gradient’s norm, so that the small gradients that result from camera and quantization noise are largely ignored. Both the stripes and the histogram bins have gradual boundaries in order to minimize the impact of small vertical shifts and rotations of the text inside the bounding box.The basic SnooperText algorithm as described above performs rather poorly on images that contain characters of widely different font sizes and styles, as usually happens in photos of urban scenes. In particular, characters that are much larger than the structuring element used in the morphological thresholding are often over-segmented. To overcome this problem, the basic SnooperText detector is applied in a multi-scale fashion [40].More precisely, for each imageI, SnooperText first builds a multi-scale image pyramidI(0),I(1),…,I(m). The baseI(0)of the pyramid is the original imageI, and each subsequent image (level)I(k)is a copy of the preceding oneI(k-1), reduced in width and height by a factor 1/μ, for some real parameter μ greater than 1. Therefore levelI(k)has 1/μ2kas many pixels as levelI(0). The maximum level m depends on the size of the original image and the minimum size of the characters to be detected.The character detection and character grouping modules are applied separately to each level of the pyramid. As described in Section 3.2, at each level k the algorithm only looks for characters whose heights lie in a bounded range [hmin…hmax] which corresponds to the range [μkhmin…μkhmax] in the original image. The parameters hmin and hmax should be chosen so that there is some overlap between two consecutive scales k and k+1, namely hmax>μhmin. Segmented regions whose height fall outside the interval [hmin…hmax] are ignored, since they are expected to be found at other scales. See Fig. 11. One advantage of the multi-scale approach is that we can use a structuring element of fixed (and modest) size in each morphological operation, with significant speed gains. Note that the cost of processing the whole image pyramid, for characters of any size, is only∑i=0m1/μ2i≈μ2/(μ2-1)times the cost of processing the original image for characters with the height range [hmin…hmax].Another advantage of the multi-scale approach is that it makes the segmentation algorithm insensitive to character texture – high frequency details that are much smaller than the characters themselves. Those details may cause each character to be split into several separate segments, and will tend to confuse the character/non-character classifier. With the multi-scale approach, these problems are largely avoided when the segmentation procedure is applied at the scale where the characters are still legible but those finer details have been blurred away. See Fig. 12.The asymptotic running time of the image segmentation step is O(N) where N is the number of pixels in the input image. Since the size of each segment that passes the size/aspect criteria is bounded, the cost of computing the shape invariants for each segment is also bounded, and therefore the running time of the character filter is proportional to the number M1 of segments that pass those criteria. The cost of the T-HOG text region validation module is at worst linear on the total area of the text regions, which is bounded by a multiple of the number M2 of characters that enter the grouping step. Since M2⩽M1⩽N, these parts of SnooperText run in O(N) time.The naive implementation of the character grouping module enumerates all pairs of candidate characters that pass through the character filter module, and therefore has costΘ(M22). In practice this is usually not a problem, since very little computation is spent on each pair, and M2 is almost always much smaller than N (typically, hundreds of character candidates against millions of pixels). However, some peculiar images—such as a wall covered with many round dots—may generate tens of thousands of character candidates, in which case the naive grouping algorithm may become the bottleneck. Fortunately, since the characters have bounded size, it is possible to implement the grouping step to run in O(M2) time with a proper data structure.In this section we compare the performance of SnooperText with that of other text detectors published in the literature in their ability to locate text lines on images of building façades.For our tests, we used four image collections, described below. For each of these datasets we have a reference file containing the bounding boxes of the text regions visible in each image, and the corresponding text contents, in a simple XML format. This data was obtained by human inspection of the images, as opposed to examination the actual scene.1.ITW: a subset of the iTowns Project’s image collection [2], consisting of 100 frontal high-resolution color photos of Parisian façades, with 1080×1920 pixels, as taken by the iTowns vehicle. The reference file, containing 848 readable text words, is available on-line [13].SVT: a public benchmark of 249 urban photos selected from the Google Street View images by Wang et al., ranging from 1024×768 to 1918×898 pixels. The reference file contains 647 readable words [41].EPS: the benchmark used by Epshtein et al. [7], with 307 color images of urban scenes, ranging from 1024×768 to 1024×1360 pixels, taken with hand-held cameras. The reference file, containing 1981 readable text lines, was provided by the authors [23] and converted to XML by us [13].ICD: the “testing” half of the 2005 ICDAR Challenge collection [42], consisting of 249 color images, ranging from 307×93 to 1280×960 pixels, captured with various digital cameras, of book covers, road signs, posters, etc. [42]. The reference file has 1107 readable text words.The ICD collection is not very appropriate for our purposes, since it includes images that differ substantially from photos of storefront signage in many ways (such as sharpness, brightness, contrast, angle of view, font variety, and the frequency of occlusions). We included it because it is a popular benchmark for text detection, and it is the only one for which we have reliable data on the performance of several other detectors.In all these tests, the SnooperText parameters were set as follows. In the image segmentation module: minimum contrast cmin=30/255, relative segmentation threshold cmed=0.79. In the character filtering module: character height limits hmin=13px and hmax=78px, aspect ratio limits rmin=0.5 and rmax=8. In the character grouping module: max relative height difference t1=0.70, max relative y offset t3=0.40. In the text filtering module: extracted image height H=24px, number of T-HOG cells (horizontal strips) ny=7, bins per histogram nb=9. The detector was applied on an image pyramid with reduction factorμ=2and maximum level m=9 (10 levels).The t2 parameter (maximum relative letter spacing) of the character grouping module was set differently for each database, so that characters would be grouped in the same way (words or lines) as in the corresponding reference file. Thus, for the EPS dataset we used a single grouping pass with t2=1.4 to get the characters grouped into lines. For the ITW, ICD and SVT we performed two grouping passes, with relative spacing limits t2′=0.38 (to get isolated words of normal text) and t2″=1.1 (to get isolated words of store names and other texts with extra-wide inter-character spacing).In the text filtering module, T-HOG descriptors with ny=7 stripes and nb=9 bins provided the best scores overall. However, in separate tests we found that the settings ny=4 and nb=5, that reduce the descriptor size from 63 to 20, would have lowered the scores by only 1–2% on average. The text filtering SVM was configured to use a Gaussian χ2 kernel, whose standard deviation was found by cross-validation.The four SVM classifiers in the character filtering module (Section 3.2) were trained on a dataset of bi-level images prepared by Fabrizio et al. [35,43]. The set contains 16,200 instances of uppercase and lowercase letters (positive samples) and 16,200 non-letter shapes (negative samples). See Fig. 13.To train the SVM of the text filtering module we used true and false examples produced by SnooperText’s character detection and grouping modules. Since these databases are fairly small, instead of splitting each database into training and evaluation halves, we used the whole database for evaluation and a combination of the other databases for training. Specifically, for the tests with the ICD and SVT datasets we trained the SVM with the “training” subset of the ICDAR Challenge dataset (with 258 images and 1157 hand-annotated words), together with all of the ITW and EPS datasets. For the tests with the EPS collection we trained with the ICDAR “training” subset together with the whole ITW dataset. Finally, for the tests with the ITW collection we trained with the ICD “training” subset and the whole EPS dataset.In each case, we ran SnooperText’s character detection and grouping modules on the chosen collection of training images. The resulting set X of rectangles was compared with the set Y of rectangles in the corresponding reference files, and separated into true positives (X+) and false positives (X−). Note that the rectangles in X+ were similar, but not identical, to the corresponding reference rectangles. We also identified the set Y− of the false negatives, that is, rectangles in the reference files that did not match any rectangle in X. The text filter was then trained with the T-HOG descriptors of X+∪Y− as the positive samples, and of X− as the negative samples.We compared SnooperText against several state-of-the-art text detectors described in the literature. Specifically, we compared it with the contestants of the ICDAR Challenge [22], and also with the detectors of Epshtein et al. [7], H. Chen et al. [8], Pan et al. [9], Neumann et al. [10], Yi et al. [11], and Yao et al. [1]. (The system of Mancas-Thillou and Gosselin [19] uses the text detector of Alex Chen, which is included in our set.)We added to the list of competing text detectors the front-end module of the popular open-source Tesseract OCR software, denoted here TessFront. The front-end’s task is to locate the candidate text regions in the input image before calling the back-end (TessBack) to parse them. Tesseract is considered one of the best OCRs publicly available today [6]; however, its front-end was designed for scanned documents, and it usually reports a large number of false positives when applied to photos of 3D scenes. Finally, we also added to the list of competing detectors the combination of TessFront with the T-HOG as an output filter (TessFront+T-HOG).The quantitative criteria we used to compare these text detector systems are based on the ICDAR 2005 measure of similarity [22] between two rectangles r, s, defined as(1)m(r,s)=A(r∩s)A(r∪s)where A(t) is the area of the smallest rectangle enclosing the set t. The function m(r,s) ranges between 0 (if the rectangles are disjoint) and 1 (if they are identical). See Fig. 14.The function m is extended to a set of rectangles S by the formula(2)m(r,S)=maxs∈Sm(r,s)From this indicator one derives the ICDAR precision P and recall R scores [22],(3)P=∑r∈Em(r,T)#ER=∑r∈Tm(r,E)#Twhere T is the set of rectangles in the reference file, and E is the set of rectangles reported by the detector.For ranking purposes, the ICDAR 2005 Committee used the F-measure[22], which is the harmonic mean of precision and recall: F=2/(1/P+1/R).There are several ways of averaging the P, R, and F scores over a multi-image database. The approach used by the ICDAR 2005 scoring program (method I) is to evaluate P, R and F separately for each image, and then compute the arithmetic mean of all three scores over all images. Another approach (method II) is to compute P and R for each image, then take the arithmetic means of all P and R values, and compute F from these means. Yet another approach (method III) is to compute the precision and recall formulas (3) by taking E and T as the union of all text regions in all images.We note that averaging method I suffers from higher sampling noise and a negative bias compared to the other two, because it gives equal weight to each image irrespective of the number of recoverable text objects in it, and because the F-score is a non-linear function of the P and R ratios. In particular, the averaged F score (method I) tends to be lower than the harmonic mean of averaged P and R (method II). This point must be considered when comparing F values reported by different authors, since it is not always clear how they were averaged.The text detection scores on the four image collections are shown in Tables 1–4. In most cases, neither the program nor the detected regions were available; therefore, we had to rely on the scores reported by the authors. For the reasons above, we recomputed the overall F score ourselves (according to the averaging method II), for all detectors, from the global P and R scores reported by the authors. We could not use method III because, for most competitors, the required data (the set of rectangles detected in each image) was not available.As can be seen from Tables 1–4 the performance of SnooperText is comparable to that of the best detectors described in the literature. In particular, Table 3 shows that it outperformed the Stroke Width Transform detector [7] of Epshtein et al. on its own dataset.The average execution time of SnooperText for each dataset is shown in Table 5. The tests were carried out on an Intel Core i7 machine (3.4GHz) with 32GB of RAM running Linux and Java.To conclude this article, we report on the effectiveness of SnooperText as the front end for its motivating application, the iTowns application.In order to maximize the portability of their software, the iTowns project chose the open-source Tesseract package as the OCR engine. As we observed in Section 1, Tesseract on its own performs rather poorly on the iTowns images. We obtained better results after replacing its built-in text detector (TessFront) by SnooperText.Besides SnooperText and TessFront, we evaluated two other detector algorithms, namely: the “best possible” detector (HandCrop), that returns the manually annotated text regions as defined in the reference file; and the TessFront module with its output filtered by SnooperText’s text region validation module (TessFront+T-HOG). Each of these detectors has a similar output, namely a list of rectangular sub-images that are presumed to contain the individual words in each image. Each of these rectangular sub-images were fed to the TessBack module with option 8 (“parse single word”). Tesseract can use a list of valid words to improve its accuracy, but we disabled that feature in order to make the test language-independent.For these comparisons, we used two scoring functions that take into account the correctness of the OCR-extracted text. Both functions assume that the strings are converted to lower case, because it is often impossible to tell whether a text in urban signage is in upper or lower case (for example, any text that uses only the letters C, O, S, U, V, X, W and Z).We assume that the OCR algorithm attaches to each rectangle r reported by the detector the corresponding text string, denoted by r.ocr. We define the rigorous OCR similarity score m′ for two rectangles r and s as(4)m′(r,s)=1ifm(r,s)⩾λandr.ocr=s.ocr0otherwisewhere m is the rectangle similarity function defined by formula (1), and λ is a fixed threshold (0.2 in our tests).The scoring function m′ may be considered too rigorous, because at the current state of the art one cannot expect that an OCR algorithm will correctly read store and product names which are missing from its spell-checking dictionary. Therefore, we also defined a tolerant OCR similarity score m″ that gives credit for partially correct OCR readings; namely,(5)m″(r,s)=1-dist(r.ocr,s.ocr)max(|r.ocr|,|s.ocr|)ifm(r,s)⩾λ0otherwiseHere|u|denotes the length of string u, and dist denotes the Levenshtein distance between strings [44]. The latter is defined as the minimum number of edit operations needed to transform one string into the other, where each operation is the insertion, deletion, or substitution of a single character. Since the Levenshtein distance does not exceed the length of the longest string, the metric m″(r,s) ranges between 0 (when the strings have no characters in common) and 1 (when the strings are equal).As in Section 4.5, we extend the scoring function m′ to a set of OCR-scanned rectangles S by the formula(6)m′(r,S)=maxs∈Sm′(r,s)We then define the rigorous OCR performance scores P′ (precision) and R′ (recall) by the formulas(7)P′=∑r∈Em′(r,T)#ER′=∑r∈Tm′(r,E)#Twhere T is the set of manually identified text regions in all input images, with the fields ocr set to the visually extracted text values, as recorded in the reference file; and E is the set of text regions reported by the detector, with the TessBack-computed .ocr fields. As before, we combine the OCR precision and recall into a single OCR score F′=2/(1/P′+1/R′). The tolerant OCR performance scores P″ R″, and F″ are defined in the same way, using m″ instead of m′ in formulas (6) and (7). See Fig. 15.Table 6shows the end-to-end scores obtained with the various front-ends on the iTowns dataset. We note in Table 6 that the rigorous OCR score F′ obtained with SnooperText (23%), while low in absolute terms, is 79% of the score obtained with TessBack on hand-cropped word images (29%). The tolerant OCR score F″ of SnooperText (40%) is 80% of the hand-cropped score (50%). In both aspects, SnooperText is significantly better that Tesseract’s front end, even when the latter is combined with the T-HOG text region validation module. Therefore, we can say that the OCR algorithm, not the text detector, is the main bottleneck of the iTowns system at present.

@&#CONCLUSIONS@&#
The experiments reported in Section 4 show that SnooperText is comparable to state-of-the-art text detection algorithms for images of building façades. In particular, it can accurately locate more than 60% of the text regions present the ICDAR Challenge benchmark, with less than 30% of false positives, even though that benchmark is not representative of the intended application. We attribute SnooperText’s good performance mainly to its use of multi-scale processing for segmentation and character detection, and to its effective text region validation module based on the T-HOG descriptor.The SnooperText detector was very effective also in the iTowns project. On a sample of the iTowns images (which are somewhat more difficult than those of the ICDAR Challenge) SnooperText was able to accurately locate about 50% of the legible text regions, with less than 30% of false positives. As reported in Section 5, the end-to-end performance of the system (including the external TessBack OCR module) was rather low, with only 21% of those texts being successfully parsed. However, that result was still 4 times the success rate of the unaided Tesseract reader, and more than 70% of the success rate obtained by applying TessBack on the hand-cropped texts.