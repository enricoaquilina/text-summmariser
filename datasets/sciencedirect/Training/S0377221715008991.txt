@&#MAIN-TITLE@&#
Parameters measuring bank risk and their estimation

@&#HIGHLIGHTS@&#
The paper develops parameters of risk and financial stability.The framework is expected utility of profit maximization.Measures of risk aversion and prudence can be estimated from the model.Moreover, we obtain estimates of bank allocative efficiency.

@&#KEYPHRASES@&#
Financial stability,Banking,Expected utility maximization,Sub-prime crisis,Financial crisis,

@&#ABSTRACT@&#
The paper develops estimation of three parameters of banking risk based on an explicit model of expected utility maximization by financial institutions subject to the classical technology restrictions of neoclassical production theory. The parameters are risk aversion, prudence or downside risk aversion and generalized risk resulting from a factor model of loan prices. The model can be estimated using standard econometric techniques, like GMM for dynamic panel data and latent factor analysis for the estimation of covariance matrices. An explicit functional form for the utility function is not needed and we show how measures of risk aversion and prudence (downside risk aversion) can be derived and estimated from the model. The model is estimated using data for Eurozone countries and we focus particularly on (i) the use of the modeling approach as a device close to an “early warning mechanism”, (ii) the bank- and country-specific estimates of risk aversion and prudence (downside risk aversion), and (iii) the derivation of a generalized measure of risk that relies on loan-price uncertainty. Moreover, the model provides estimates of loan price distortions and thus, allocative efficiency.

@&#INTRODUCTION@&#
Recent problems with the banking sector have put issues of modeling risk at the forefront of research. Traditional risk measures like the standard deviation of the returns on assets (ROA) or the return on equity (ROE) are used widely but have no formal economic justification. In turn, questions related to financial stability which are intimately related to risk, remain unanswered. As one analyst remarks: “The problems of creating a solvency test for financials that operate in real-time or relative real-time is not easy. It’ s not hard to do after the fact, but at any one time, no one – not even the directors of the company really know what is embedded in the books at a financial [institution]. Société Générale, Barings, Northern Rock, Bear Stearns – the list of surprise blowup go on and on.” (Hui, 2008).Before proceeding it is important to clarify what we mean by risk in our setting. Each bank has a utility function (defined over profit) which is known to the bank but unknown to us. Outputs of the bank are various kinds of loans, and inputs include capital, labor and deposits -the so called financial intermediation approach to modeling banking technology. Output prices are stochastic and the bank maximizes expected utility of profits. Alternative approaches exist and we compare and contrast our results in Section 7.In this paper we propose a structural model (although not a general equilibrium model as in Aspachs, Goodhart, Segoviano, Tsomocos, & Zicchino, 2006). The essential features of the model are, however, the same as we rely on the metrics of bank profitability and the probability of default explicitly through the financial institution’ s optimization problem. The model can be estimated using standard econometric techniques, like GMM for dynamic panel data and latent factor analysis for the estimation of covariance matrices. The theoretical model relies on expected utility maximization for a financial institution under uncertain loan prices. An important feature of the model is that it relies on the neoclassical approach to optimization with a well-defined technology set provided by its representation via a distance function. An explicit functional form for the utility function is not needed and we show how measures of risk aversion and prudence (downside risk aversion) can be derived and estimated from the model. The model is estimated using data for Eurozone countries and we focus particularly on:(i)the use of the modeling approach as an “early warning mechanism” to the extent possible since only one crisis observation is available;the bank- and country-specific estimates of risk aversion and prudence (downside risk aversion); andthe derivation of a generalized measure of riskthat relies on the loan-price uncertainty. The generalized measure of risk is generic to the banking system but it is time-varying.(i) and (ii) rely on an expansion of the unknown utility function of the banks to estimate risk aversion (αit) and downside risk aversion (δit) for bank i at time t. Risk aversion isα=−u′′(μΠ)u′(μΠ),the Arrow–Pratt measure, andδ=u′′′(μΠ)u′(μΠ)as the measure of downside risk aversion or “prudence”, where u() is the utility function and μΠdenotes expected profits. It is important to mention that these measures can be estimated on a time-varying basis from the first order conditions of expected utility maximization of profits and that they are different for each bank.The aim of the paper is to provide measures of risk close to what is known as “systemic risk”, an issue that has naturally attracted considerable attention in the literature; see, for example Yang, Zhou, and Wang (2010) and Yang and Zhou (2013). Yang et al. (2010) develop conditional co-skewness in the context of a bivariate regime-switching model and show that it is important for portfolio construction. In our context, conditional co-skewness matters as it relates to “prudence” through the third derivative of the utility function. Yang and Zhou (2013) focus on spillovers among financial institutions. Spillovers and interrelations among banks in this paper are taken into account via a dynamic factor model so an important aspect is taken into account.The remainder of the paper is organized as follows. The theoretical model and main results are presented in Section 2. The generalized measure of risk discussed in Section 3. Data and empirical results are presented in Section 4. In Section 5 we show how allocative efficiency can be measured and we provide the results. Skewness and co-skewness statistics are presented in Section 6. We compare and contrast our results with alternative approaches in Section 7. The paper concludes with a summary of the approach and its results.Supposex∈ℜ+Kis a vector of inputs,w∈ℜ+Kis the vector of input prices,y∈ℜ+Ma vector of outputs andp∈ℜ+Mis the vector of their prices. We think of outputs as various types of loans while inputs include capital, labor etc.The technology set is defined by:T={(x,y)∈ℜK+M:xcanproducey}.It is known that(x,y)∈Tif and only if: F(x, y) ≥ 1, for a general transformation function F(x, y). See, for example, Kumbhakar and Lovell (2000, pp. 30–32 and pp. 205–208). The banks face uncertain output prices as some loans might not perform or, alternatively, they perform to an extent that is unknown to the bank -which is the source of risk here. Suppose:(1)p=μ+C′ɛ,whereμis a location parameter (the mean, if it exists),ɛ∼NM(0,I)andC′C=Σ,the covariance matrix, assuming it exists. The lower diagonal matrix C is related to the Cholesky decomposition ofΣwhich is assumed positive definite. The profit of the bank is:Π=p′y−w′x−κwhere κ denotes fixed costs -see also Appelbaum and Ullah (1997).11See papers by Kumbhakar (2002a, 2002b), Kumbhakar and Tsionas (2010) and Kumbhakar and Tveterås (2013). These papers do not deal with the (arguably more difficult) case of multiple outputs and thus many output prices which is the focus of the present paper.The bank maximizes the expected utility of profits:Eu(Π),where the expectation is taken with respect to the joint distribution ofε. Profits can be written as:(2)Π=(μ+C′ɛ)′y−w′x−κ=μΠ+ɛ′Cy,where expected profit is:μΠ=μ′y−w′x−κ.The problem of the bank can be restated as:(3)max(x,y)∈ℜ+K+M:Eu(μΠ+ɛ′Cy),s.tF(x,y)≥1.From the first-order conditions of the expected utility maximization problem we have:(4)wkw1=∂F(x,y)/∂xk∂F(x,y)/∂x1,k=2,…,K;(5)μm+σmΛmμ1+σ1Λ1=∂F(x,y)/∂ym∂F(x,y)/∂y1,m=2,…,M,whereσm2is the mth diagonal element ofΣandμ=[μm,m=1,…,M]. Eq. (4)shows that expected utility maximization requires cost minimization, as expected. In (5) we have defined the following expressions which will become quite important in our discussion:(6)Λm≡E{u′(Π)ɛm}Eu′(Π),m=1,…,MFrom (5) it is clear that expected utility maximization is consistent with ordinary profit maximization at relative output (or shadow) prices which are given by:(7)p˜m≡μm+σmΛmμ1+σ1Λ1,m=2,…,M.If, in fact, the transformation function is an output distance functionKumbhakar and Lovell (2000, pp. 30–32) then exploiting its linear homogeneity with respect to outputs, we can establish (5) in a somewhat different form:(8)(μm+σmΛm)ym∑m′=1M(μm′+σm′Λm′)=∂logF(x,y)∂logym,m=1,…,M.since∂logF(x,y)∂logym=∂F(x,y)∂ymymandF(x,y)=1. We denoteα=−u′′(μΠ)u′(μΠ)as the Arrow–Pratt measure of risk aversion andδ=u′′′(μΠ)u′(μΠ)as the measure of downside risk aversion or “prudence”. We have α > 0 and, normally, we would expect that δ ≥ 0. The left-hand-side of (8) would provide the virtualrelative prices,p˜mthat would be consistent with classical profit maximization - prices are normalized so that they lie on the boundary of the unit simplex in ℜM. Besides the first two moments of prices, these virtual prices depend on Λms which are related to the underlying utility function as in (7). Naturally, we wish to avoid expressing the utility function in a specific functional form since that would make the analysis specific to the particular functional form. In that way, we see that what we have assumed so far is enough to deliver measures of Arrow–Pratt measures of risk aversion as well as downside risk aversion.As we show below, we have:(9)Λ≃−α1+δAPTARANORMAL·tr(yyyyyyyyayy′Σ)·Cywhich is an M × 1 vector whose elements are the Λms.Proof of Eq. (9)Our purpose is, first of all, to derive expressions forEu′(Π)as well asE{u′(Π)ɛ},whereΠ=μΠ+ɛ′Cy. Using a Taylor approximation aroundɛ=0(M×1)we have:u′(Π)≃u′(μΠ)+u′′(μΠ)ɛ′Cy+u′′′(μΠ)ɛ′Cyy′C′ɛ.Taking expected values we obtain:Eu′(Π)≃u′(μΠ)+u′′′(μΠ)Etr(yy′C′ɛɛ′C)=u′(μΠ)+u′′′(μΠ)tr(yy′Σ).Moreover,E{u′(Π)ɛ}≃u′′(μΠ)E{ɛ′Cyɛ}+u′′′(μΠ)E{[ɛ′Cyy′C′ɛ]ɛ}.By the symmetry assumption the last term is zero and therefore:E{u′(Π)ɛ}≃u′′(μΠ)Cy.Using the definitionsα=−u′′(μΠ)u′(μΠ)andδ=u′′′(μΠ)u′(μΠ), we obtain:Λ≡E{u′(Π)ɛ}Eu′(Π)≃−α1+δtr(yy′Σ)·Cy□Therefore, the Λms can be related to two fundamental characteristics of risk, namely the Arrow–Pratt measures α and δ. It is important to emphasize that these measures depend on underlying bank profitability (since the banks maximize expected utility of profit) as well “probability of default” in the sense that loan price uncertainty is explicitly taken into account.Next, we show how to estimate risk aversion and prudence parameters under output price uncertainty. The data requirements are inputs xitand outputs yitof the bank and their prices witand pitrespectively. Suppose the output distance function is F(xit, yit;θ), whereθ∈ℜdθdenotes a vector of unknown technology parameters. Efficient output and input vectors inTcorrespond, by definition, toF(xit,yit;θ)=1.Our approach to the problem relies on using Eqs. (7) and (8). From (8) we have:(μm+σmΛm)ym(μ1+σ1Λ1)y1=∂logF(x,y)/∂logym∂logF(x,y)/∂logy1,m=1,…,M,from which we obtain:(10)p˜mymp˜1y1=∂logF(x,y)/∂logym∂logF(x,y)/∂logy1,m=2,…,M,wherep˜mis defined in (7). These equations can be written in the following form:(11)logp˜mp˜1+logym−logy1=log{∂logF(x,y)/∂logym∂logF(x,y)/∂logy1}.The econometric model consists of the output-distance-function:(12)F(xit,yit;θ)=ηit,1,where ηit, 1 is an error term, along with the first-order conditions:(13)logp˜m,itp˜1,it+logym,it−logy1,it=log{∂logF(xit,yit;θ)/∂logym,it∂logF(xit,yit;θ)/∂logy1,it}+ηit,m,m=2,…,M,(14)logwk,itw1,it+logxk,it−logx1,it=log{∂logF(xit,yit;θ)/∂logxk,it∂logF(xit,yit;θ)/∂logx1,it}+eit,k,k=2,…,K,wherep˜mis a complicated function of α, δ,μ, C and y as defined in (7). Recall thatC′C=Σ. Finally, shadow prices and actual prices are related as follows:(15)logpm,itp1,it=γm,it+logp˜m,itp˜1,it+ξit,m,m=2,…,M,whereγit=[γ2,it,…,γM,it]′is a vector of systematic deviations andξit=[ξit,2,…,ξit,M]′is a vector of random variables. In fact, these are related to allocative inefficiency distortions (Kumbhakar, 1997; Kumbhakar & Tsionas, 2005).The error terms areφit=[ηit,2,…,ηit,M,eit,2,…,eit,K,ξit,2,…,ξit,K]′. We do not impose particular assumptions about these error terms other than they are orthogonal to variables involved in our moment conditions. For example, we do not make distributional assumptions. The output distance function is defined by a flexible, translog functional form which is used extensively in the literature:(16)F(xit,yit;θ)=ai+bt+∑k=1Kαkxit,k+12∑k=1K∑j=1Kαkjxi,tkxi,tj+∑m=1Mβmyi,tm+12∑m=1M∑j=1Mβmjyi,tmyi,tj+∑k=1K∑m=1Mγkmxi,tkyi,tm+ηit,1,where aiand btrepresent bank and time effects22In addition we include effects for commercial, investment, cooperative and savings banks., K is the number of inputs (xi, tkin logs), M is the number of outputs (yi, tmin logs) andi=1,…,nrepresents a particular observation for the vector of all inputs xit∈ ℜKand outputs yit∈ ℜM. Vectorθ∈Θ⊆ℜdθdenotes the unknown parameters in (16), viz. ai, bt, αk, αkj, and βm, βmj, γkm. Efficient input-output combinations imply that the transformation functionF(xit,yit)=1. From linear homogeneity of output distance function with respect to outputs, in log terms, we haveF(xit,h+yit)=h+F(xit,yit)for any constant h ∈ ℜ so we may choose the constant to be the first output which results in:(17)−yit,1=ac+bt+∑k=1Kαkxit,k+12∑k=1K∑j=1Kαkjxi,tkxi,tj+∑m=2Mβmy˜i,tm+12∑m=2M∑j=2Mβmjy˜i,tmy˜i,tj+∑k=1K∑m=2Mγkmxi,tky˜i,tm+ηit,1,wherey˜it,m≡yit,m−yit,1,m=2,…,M.We allow for the endogeneity of yit, xitand output prices pitusing GMM with instruments being log-relative input prices and time effects. The instruments are motivated by the first-order conditions of profit maximization. Our implementation of GMM is the so-called CUE (continuously-updated-estimator) which has been shown to have better finite sample properties. Joint estimation of the first-order conditions in (13)–(17) is quite difficult because of the dependence on Λms which are highly nonlinear functions of α and δ.33Another difficulty is that the first order conditions must be estimated in log form and we need to take account of the Jacobian of transformation due to endogeneity of the variables involved. Using GMM this is automatically taken into account.In terms of estimation we proceed as follows:(i)We estimate jointly (17) and (14) by GMM to estimate the technology parameters,θ.Given estimatesθ^we recoverlogp˜m,itp˜1,itfrom (13) as follows:(18)logp˜m,itp˜1,it=−logym,it+logy1,it+log{∂logF(xit,yit;θ^)/∂logym,it∂logF(xit,yit;θ^)/∂logy1,it}.In turn, givenlogp˜m,itp˜1,itwe solve44This involves a solution in terms of α, δ of a nonlinear system of equations given the data, (7) and (9).for bank-specific and time-varying estimates of αit, δitgiven the expressions in (7) and (9). ParametersμandΣare assumed the same for all banks and years in the same country but different across countries and are estimated using their sample counterparts from (1). The assumption can be defended on the following grounds. The restrictive alternative is to assume thatμandΣare the same for all banks and time periods, a specification that cannot be defended. The flexible alternative is thatμandΣare different for each bank and each time period. Unfortunately, we cannot have identification under the flexible alternative. So the feasible alternative is to assume that banks in the same country generate a common distribution with momentsμandΣbut these parameters are different across countries. This seems a reasonable approach to deal with cross-country heterogeneity although it would be an interesting issue for future research to account for bank heterogeneity in the same country. This, naturally, would complicate the statistical procedures, considerably.Parameters γmand estimatesξ^it,mcan be recovered, if desired, from GMM estimation of (15).The fundamental merit of this estimation procedure is that we can obtain bank-specific and time-varying estimates of αit, δitwithout a random effects assumption which necessitates distributional assumptions.Next, we use the normalizing restriction∑m=1M(μm+σmΛm)=1that output prices lie on the boundary of the unit simplex in ℜM. The restriction implies a relationship between α and δ. In turn, from Λmin (9) we can recover one of α or δ while the other can be recovered from the restriction. AsΛis both time-varying and bank-specific we can exploit this fact to deliver time-varying and bank- specific estimates of both α and δ.The output distance function and first order conditions (14) as well as (15) are estimated separately for all banks in each country. Estimation of the output distance function includes bank- and time- specific fixed effects and is performed using the CUE version of GMM starting from a random selection of initial conditions for the parameters, centered around the OLS estimator.Random initial conditions are generated using 100 different random seeds. Different initial conditions did not lead to different parameter estimates suggesting that, in this instance, the objective function is not multimodal.Given an R × 1 set of moment conditionsEg(θ,Yi)=0(R×1)whereYidenotes all available data, the empirical analogue is55We rely on moment conditions using as instruments the inputs and time as well as their squares and interactions plus country-specific effects. Hansen’s J test has a p-value of 0.31 failing to reject the null hypothesis of orthogonality between output distance function errors and the specific instruments. Exclusion of country-specific effects produces a Hansen test whose p-value is 0.001. The minimization problem is solved using a standard conjugate-gradients algorithm. More computational details are available on request.:minθ∈Θ:f(θ)=[N−1∑i=1Ng(θ,Yi)]′W[N−1∑i=1Ng(θ,Yi)],for some weighting matrix W, where N denotes the number of available observations. In the first stage we setW=I,the identity matrix. Since the optimal weighting matrix isW−1∝E[g(θ,Y)g(θ,Y)′]the problem is re-solved usingW−1=N−1∑i=1Ng(θ,Yi)g(θ,Yi). Therefore, the optimization problem for CUE-GMM is the following:(19)minθ∈Θ⊆ℜdθ:f(θ)=[N−1∑i=1Ng(θ,Yi)]′[N−1∑i=1Ng(θ,Yi)g(θ,Yi)]−1×[N−1∑i=1Ng(θ,Yi)].GivenG=N−1∑i=1N∂g(θ,Yi)∂θ,the well-known asymptotic result can be used to obtain the asymptotic covariance matrix of the estimator:(20)N1/2(θ^GMM−θ)→Ndθ(0,[G′WG]−1).The covariance matrix can be estimated in practice using[G^′WG^]−1whereG^=N−1∑i=1N∂g(θ^,Yi)∂θ,Moreover, Hansen’s J-statisticJ=Nf(θ^)→χR−dθ2. The same strategy is used to estimate, first, (17) and (14), and, second, (15).For the M × 1 vector of output prices p we have observations for a given country (c=1,…,C) and a given bank within a country (b=1,…,Bc). So, in practice, the vectorpct=[pcb,t]is possibly very high-dimensional for a given time period (t=1,…,T). This is especially the case because we want to combine countries, for example, peripheral countries versus non-periphery. The peripheral countries are Portugal, Italy, Ireland, Greece and Spain. We define the “core” of the EU as France and Germany. Moreover, in order to estimate the time-varying conditional covariance matrix of prices,Σt, we must account for the fact that they do not have a constant conditional mean. Since we do not typically have large T (but we do have large N where N is the number of banks and T is the number of time periods) we assume the following process:(21)pcb,t=ab+Acpcb,t−1+vcb,t.The vector pcb, tis M × 1 which is low-dimensional since in the leading case we have two or three outputs. NotationAcmeans that estimation is performed using data for all banks in a givencountry c. Here, abis M × 1 and A is matrix M × M. The process is a vector autoregression with a time-varying covariance matrix. The notation abmeans that we include bank-specific effects. The statistical model in 21 is estimated using data for a given country and all banks and time periods available for that country (or group of countries.) The model can be estimated using standard GMM techniques for dynamic panel data. We use moments of the type proposed in Arellano and Bover (1995) and Blundell and Bond (1998), see also Handbook of Econometrics, chapter 53.The modeling of a dynamic covariance matrix in large dimensions is well known to be an exceedingly difficult matter and standard extensions of the GARCH model do not work well, see for example Engle and Kroner (1995) and Silvennoinen and Teräsvirta (2009).66See also Bai and Shi (2011) for the wider issues in high-dimensional settings, along with Bai and Ng (2010), Bai and Li (2010), Bai (2010), Chamberlain and Rothschild (1983), and Jones (2001).Given the residuals77Supposev^t=[v^c1,t,…,v^cB,t]′. It is to be noted that sincev^t=vt+(v^t−vt)=vt+etandet=op(1)the analysis carries through in the sense that the principal component analysis is asymptotically valid.v^cb,twe follow Connor and Korajczyk (1986, 1988), Stock and Watson (2002), Bai and Ng (2002); Bai and Shi (2011) and Bai (2003) to model the time-varying covariance matrix using a latent factor model with the principal components simplification, which has been shown to work well in practice (see in addition Bai and Li (2010), Forni, Hallin, Lippi, and Reichlin (2000) and Lehmann and Modest (1988)). SupposeV^t=[v^c1,t,…,v^cBc,t]′. The latent factor model is:(22)V^it=ζi+λi′ft+ξit,where the factors ft(r × 1) and factor loadingsλi(r × 1) are unobserved. Here, the index i corresponds to (c, b). In compact notation we have:(23)V^t=ζ+Γft+ξt,whereΓ=[λ1,…,λN]′andζ, ξtare defined in conformable manner. Moreover,(24)ψt≡E(V^t|ft)=ζ+Γft,is the time-varying conditional mean. The principal components estimator makes use of the decomposition:(25)S=∑i=1Ndi2hihi′,where S is the sample covariance matrix,di2is the ith largest eigenvalue and hidenotes the corresponding eigenvector Bai and Shi (2011). The principal components estimator forΓis then given by:(26)Γ^=[d1h1⋮⋯⋮drhr],which gives:(27)Σ^=Γ^Γ^′+Ω^,whereΩ^=diag(S−Γ^Γ^′)is the estimator for the diagonal covariance of the error termsξt. The advantage of the estimator despite, of course, its simplicity is that it can be applied for a given time period, treating different banks as variables within a given country and a given time period, yielding consistent estimators forΣ^t(Bai, 2003, 2004). There are a number of procedures to select the number of factors, r, see Bai and Ng (2002) who proposed information criteria.Estimatesψ^tandΣ^t,can be obtained as follows. First,ψ^tcan be obtained from (24) afterψandΓhave been estimated by standard maximum likelihood techniques. Since estimation is done period-by-period an unrestricted covariance matrixΣtcan be estimated based on Bai (2003, 2004) to avoid restrictive assumptions about its elements.The data set88The author is grateful to Natasha Koutsomanoli-Filippaki who generously provided the data set and its description.includes commercial, cooperative, savings, investment and real-estate banks in Eurozone countries that are listed in the IBCA-Bankscope database over the period 2001–2011. After reviewing the data for reporting errors and other inconsistencies we obtain an unbalanced panel dataset of 29,023 observations, which includes a total of 4065 different banks. For the definition of bank inputs and outputs, we follow the vast majority of the literature and employ the financial intermediation approach99For a review of the various approaches that have been proposed in the literature for the definition of bank inputs and outputs see Berger, Humphrey, and Griliches (1992).proposed by Sealey and Lindley (1977), which assumes that the bank collects funds, using labor and physical capital, and transforms them into loans and other earning assets. In particular, we specify three inputs, labor, physical capital and financial capital, and two outputs, loans and other earning assets (which include government securities, bonds, equity investments, CDs, T-bills, and equity investment). With respect to input prices, the price of financial capital is computed by dividing total interest expenses by total interest bearing borrowed funds, while the price of labor is defined as the ratio of personnel expenses to total assets. Moreover, the price of physical capital is defined as the ratio of other administrative expenses to fixed assets. Regarding the calculation of output prices, the price of loans is defined as the ratio of interest income to total loans, while the price of other earning assets is defined as total non-interest income to total other earning assets1010The Bankscope database reports published financial statements from banks worldwide, homogenized into a global format, which are comparable across countries and therefore suitable for a cross-country study. Nevertheless, it should be noted that all countries suffer from the same survival bias..The number of banks by year included in our sample is: 2001: 2214; 2002: 2028; 2003: 1909; 2004: 1994; 2005: 3053; 2006: 3075; 2007: 3042; 2008: 2990; 2009: 2951; 2010: 2949 and 2011: 2818. The additions to the sample are not necessarily new market entrants, but rather successful banks that are added to the database over time. Exits from the sample are due either to bank failures or to mergers with other banks or are a consequence of changes in the coverage of the Bankscope database. Our sample covers the largest credit institutions in each country, as defined by their balance sheet aggregates. Due to the specific features of the German banking system (large number of relatively small banks), our sample is dominated by German banks. Descriptive statistics of the data are provided in the Appendix A.Breaking the sample into periphery versus non-periphery as well as France and Germany separately, we can apply the factor model in (23) for separate samples whose dimensionality is less than the dimensionality of the full sample in terms of the number of banks. In turn we can apply the principal-components estimator in (26) and (27). The principal-components estimator has also been applied to each country separately following preliminary estimation of (21). For the factor models, the use of Schwarz information criterion (Bai & Ng, 2002) which turned out to give one factor for the vast majority of cases, including the groups of periphery, non-periphery as well as France and Germany taken separately.In Figs. 1and 2we provide sample kernel densities of these key parameters across all financial institutions and years. Relevant estimates are provided in Tables 1, 2and 2 in the next section (where we provide a comparison with other risk measures). These densities, which combine evidence from all countries and all time periods are clearly multimodal indicating at least that there is considerable heterogeneity either over time or across banking systems in different countries. We use the term “periphery” for Portugal, Ireland, Italy, Greece and Spain.In Fig. 3, estimates of risk aversion are reported for the European banking system excluding the periphery (thick line), France, Germany and the periphery’s banking system. It is impressive that risk aversion in the system as a whole (excluding peripheral countries) started increasing before the sub-prime crisis. Yet, for peripheral countries this happened after the crisis had been developed. It remains important, however, that an early warning signal is indeed at work here as overall risk aversion for the system started increasing at least one year before the sub-prime crisis. However, as only one crisis observation is available, we prefer to take this as evidence rather than as definitive conclusion.In Fig. 4, measures of prudence (downside risk aversion) are reported. For the system as a whole (excluding the periphery) as well as for France and Germany prudence increases steadily throughout 2001–2011. For the periphery the 2000s start with negative downside risk aversion which increases to values around zero and ends up with values close to 1.5.In Fig. 5, the generalized risk (variance) measures are reported. These measures are increasing throughout the 2001–2011 period indicating the accumulation of risk resulting from the expansion of credit. Possibly as the result of adopting policies of fiscal restraint1111Fiscal restraint refers to consolidation in national budgets, and the restructuring of policies related to taxes and public expenditures in the periphery. Countries like Ireland, Portugal and Greece were subjected to so called “austerity” or adjustment programs in order to make their debt sustainable and recapitalize the banking sector. These policies are well known in the EU and the international press, have been active in criticizing or supporting them over the years after 2009. Recapitalization of banks for example and bail out packages were huge. We prefer not to model these variables explicitly for three reasons. First, we do not know from the data base we use (or any other publicly available) how each bank was bailed out. Second, the rescue plans are reflected in the background to our model and, therefore, they should how up as deep restructuring in the deposits – loans relation, the loan totals and their composition. Third, as a result of that they are expected to show up in different estimates of risk aversion, prudence and generalized risk if the model is anywhere close to reality., the generalized risk especially for the periphery seems to decrease during 2010–2011 for the system as a whole (except the periphery) as well as France and Germany but notably not for the peripheral countries themselves.In Fig. 6, risk aversion coefficient (α) sample distributions are reported for the peripheral countries over time. These distributions result from bank- and country-specific measures. The distributions are, evidently, non-normal and show the evolution of risk aversion over time from, basically, lower to higher values. It is not until 2008 that the financial system in the peripheral countries starts to develop increasing aversion towards risk, that is in the middle or even after the sub-prime crisis. The distributions clearly shift to the right after 2007–2008 showing that the banking system adjusted with a considerable lag, contrary to the non-periphery whose (aspects of) sample distributions are summarily reported in Fig. 3. For non-peripheral countries the distributions of risk aversion started shifting to the right as early as 2006 providing most likely an “early warning mechanism” with regard to the following sub-prime crisis: We consider this an important aspect of the model in terms of modeling and forecasting financial stability -although, as we mentioned before, only one crisis observation is available.In terms of interpreting our results, the following point is important: In the core countries (France and Germany) it was the toxic assets that were the issue. The crisis emanated from the banks. This is true for some peripheral countries as well, e.g. Spain. In peripheral countries such as Greece, for example, the crisis moves from the sovereign to the banks. This distinct aspect of the crisis, with its dichotomy between periphery versus non-periphery, provides an explanation for the findings in Fig. 3 and onward. It is, for example, well known that the financial turmoil in late 2007 resulting from the collapse of the mortgage market was due to the unprecedented issuance volume of credit default swaps (CDS) from 1998 to 2007 (Stanton & Wallace, 2011). Buch, Eckmeier, and Prieto (2014) analyzed a macroeconomic vector autoregression for the United States with a set of factors summarizing conditions in about 1500 commercial banks. Their main findings are as follows: “Backward-looking risk of a representative bank declines, and bank lending increases following expansionary shocks. Forward-looking risk increases following an expansionary monetary policy shock. There is, however, substantial heterogeneity in the transmission of macroeconomic shocks, which is due to bank size, capitalization, liquidity, risk, and the exposure to real estate and consumer loans”. Another channel through which substantial heterogeneity in the transmission of macroeconomic shocks can arise, is precisely because of, possibly substantial, heterogeneity in the degree of risk aversion which, in this paper, is the main focus of our modeling and estimation. We find that risk aversion as well as downside risk aversion vary over time. This leaves open the possibility that it is the risk attitudes of banks that are responsible for the heterogeneity in their responses instead of “forward-looking” or “backward-looking” risk. At any rate, through the application of simple estimation techniques, we are able to deliver bank-specific and time-varying estimates of important aspects of risk along with a generalized risk measure of the banking sector with a solid foundation in economic theory.We continue with some additional remarks. The expected utility framework provides a wealth of information regarding risk in financial intermediation. First of all, we report results1212To avoid cluttering the paper, we present results in graphical form. All estimation results are available on request.related to Arrow–Pratt measures of risk aversion (α) and downside risk aversion (δ). These measures are bank-specific as well as time-varying due to the solution of equation in (15). Second, the use of a latent factor model as in (23) provides a generalized measure of risk which is given by the determinantdet(Σt)or its log. This measure of risk relies on all output prices collectively. Although it reflects the “risk” of the banking system as a whole it should be accompanied by considerations of risk aversion, formalized by estimates of α and δ. This is important as there cannot be a single measure of “risk” without a reference framework provided by a behavioral assumption which, in this paper, is expected utility maximization. In that way, financial stability depends not only on underlying, statistical or econometric measures of risk like the z-score or even the generalized variancedet(Σt)but rather on the propensity of the system to increase risk aversion and prudence (downside risk aversion) during periods of crises. Conversely, an increase of risk accompanied by an increase in risk aversion and prudence can show that a crisis is developing and can, at least in principle, provide us with an early warning mechanism.There remains the question of whether measures of risk aversion and prudence are rather retrospective in nature since, for example, risk aversion can rise in response to a crisis. From the perspective of expected utility maximization and almost every other behavioral assumption based on optimization (excluding cost minimization) this cannot be the case. Since the bank has more information about its assets, capital and loan performance, it will react to a deterioration of its financial position by taking the appropriate measures, before a generalized crisis has taken place. The bank will not react to the generalized crisis but rather to its own deterioration of financial indices based on its own optimizations using its own information. Distress signals from the entire banking sector will contribute to an increase of risk aversion and prudence, but this is the retrospective, not the prospective element lying at the heart of an increase in measures of prudence and risk aversion.In Tables 1, 2 and 3 we report estimates of risk aversion, downside risk aversion and the generalized risk measure for certain groups of countries over time for the period 2001 through 2011.Some interesting conclusions are as follows: (i) Risk aversion increases over time. (ii) Prudence increases for the periphery only after 2010 but increases considerably after 2008 for all other countries. (iii) Generalized risk continues to increase in the periphery even during 2011 but this is not so for all other countries.For related or similar issues see Bolt and Humphrey (2015), Francis, Gupta, and Hasan (2015), Kenjegalieva, Simper, Weyman-Jones, and Zelenyuk (2009), Michaelides et al (2015) and Saha, Subramanian, Basu, and Mishra (2009).As we noted in Section 3, there is an increase in the number of banks after 2005. These are not necessarily new market entrants but rather successful banks that are added to the database over time. It is a possibility that the composition of the panel affects the results. To examine this possibility we re-estimate the econometric models using the same procedures -viz. CUE-GMM and factor analysis for the generalized measure of risk. Our results for the important parameters of the model are reported in Table 4. Since the estimates are obtained on a per country basis (using all banks and years available for the given country) it is sufficient to look at the results for the overall cases (excluding the periphery) to determine whether there are any differences.A comparison of the results in Tables 4 and 1 through 3, shows that estimates of risk aversion and downside risk aversion are much larger when the new entrants in the data base are considered. Some differences in generalized risk show that the new entrants reduce the overall variance after 2010, so the successful banks helped in the stabilization of the banking system but in other respects they did not systematically affected risk during 2005–2009. Indeed, during 2005–2011 the generalized risk measures are 1.90, 3.76, 6.12, 9.13, 15.4, 17.2, 11.2 when we use the same banks as in 2001, and 3.22, 4.88, 7.25, 11.45, 17.2, 21.1, 15.32, when we use the entire data set, thus allowing for the new entrants.To assess this difference statistically, we use the bootstrap where all yearly observations of a bank are treated as a block and we implement the bootstrap on a per country basis (as our estimation technique is performed individually for each country). The results are reported in Table 5.The reported p-values test the significance of the generalized risk measure obtained from (i) the entire sample, and (ii) the sample that results when do not include new entrants in the data base. For each of the 10,000 bootstrap replications the models are re-estimated under (i) and (ii) and the empirical distribution of generalized risk measures is obtained. From the p-values in Table 6it is clear that the differences are statistically significant at conventional levels of statistical significance. The differences in risk and downside risk of the new entrants, which are successful banks, can probably be taken as responsible for the relative stabilization of the overall banking system after 2010 as well as the containment of overall volatility in the years of turmoil.Our specification in (15) allows to estimate price distortions and, in turn, allocative inefficiency. It is known that technical inefficiency results when a decision-making-unit does not produce maximum output given its inputs (Ruggiero, 2000). Allocative inefficiency results when the input or output mix is not optimal (Bogetoft, Färe, & Borge, 2006; Brissimis, Delis, & Tsionas, 2010; Staub, Suza, & Tabak, 2010). Here, the notion of optimality refers to expected utility maximization. As we remarked in the discussion of (17) and (13)–(15), observed output prices are pmtand virtual or shadow prices that would satisfy the first-order-conditions for expected utility maximization arep˜mtsubject to some normalization. The differencespmt−p˜mtare known as price distortions for all outputs (m=1,…,M). Therefore, although the bank should use output pricesp˜mt,the bank uses the observed prices, pmt. To determine the cost of this deviation from optimality we use the allocative inefficiency measure:(28)AIit=|pit−p˜it|′yitpit′yit,wherepit,p˜itare the M × 1 vectors of observed and virtual prices, and yitis M × 1 the vector of outputs.Such estimates can be obtained easily from (15) which is estimated separately from (17) and (13). Given estimated parametersθ^from (17) and (13), and estimated of the risk parameters from (14), equation (15) is estimated using a random-effects specification forγit=[γit,1,…,γit,M]′. The model we estimate is:(29)logpi,tm=γit,m+logp˜it,m+ξit,m,m=1,…,M,(30)ξit=[ξit,1,…,ξit,M]′∼i.i.d.NM(0,Ωξ),(31)γit=[γit,1,…,γit,M]′=ιi+τt.We use the normalizing restriction∑m=1M(μm+σmΛm)=1,viz. output prices lie on the boundary of the unit simplex in ℜM. In the specification (31) we decompose the systematic component γitinto a bank specific component, ιi, and a time-varying effect τt. The overall error term ξitcan be interpreted as a bank-time effect. No assumption of independence between ιiand τtis necessary nor assumptions of independence with ξit. The statistical model can be estimated using GMM using the same instruments as the ones that were used in estimating (17) and (13). Our allocative efficiency estimates, defined as:AEit=exp(−AIit),are provided in Table 6.There is an interesting pattern emerging from the estimates in Table 6. First, there is a decline in allocative efficiency of the banking system in the Eurozone excluding the peripheral countries from 2001-2002 and the trend reverses only after 2009. This decline followed by a recovery is evident also in France and Germany. Second, the same is true for the peripheral countries but not in Greece whose allocative efficiency declined from 82.6 percent in 2001 to nearly 51.5 percent in 2010 and 2011. The banking system in the periphery as a whole, however, increased its efficiency from 74.4 percent in 2001 to 88.5 percent in 2011. The (almost) general increase in allocative efficiency after 2010 can, probably, be attributed to the bail-out plans of the EU but also the re-orientation of the banking system towards more sound investments along the lines of expected utility maximization. In this sense, recapitalization of European banks through the intervention of the ECB resulted in better performance with the exception of Greece, whose problems of the banking sector can be attributed to structural weaknesses in its economy.The important evidence is, additionally, the decline in allocative efficiency throughout 2001–2009. It is not entirely out of line to attribute this development to the policies of expanding credit and low interest rates of the ECB. Artificially “cheap money”, quite naturally made many investments appear more profitable than they truly were (in view of differential information between banks and investors seeking funding of their projects) and, in that way, they magnified the wedge between actual and virtual loan prices or costs. In turn, this is responsible for lower allocative efficiency over time. The liquidation of unprofitable projects resulted in capitalization problems of the banking system that were resolved only after the bail-out policies of the ECB and the exercise of more prudence by the banks. As as saw, this resulted in higher risk aversion as well as downside risk aversion or prudence.Yang et al. (2010) introduced measurement of co-skewness in stock and bond returns through a Markov Switching model. As they mention: “regime-switching model allows us to obtain precise conditional estimates in a flexible manner, as it captures skewness and other higher moments as a function of the mean, variance, and persistence parameters of the underlying states.” Regarding the meaning of co-skewness they mention: “[C]onditional co-skewness is proportional to the conditional covariance between one asset’ s return and another asset’ s volatility. More specifically, conditional stock co-skewness captures the conditional relation between stock return and bond volatility, while conditional bond co-skewness captures the conditional relation between bond return and stock volatility. [...] [C]oskewness captures certain extreme co-movement which correlation does not, as co-skewness is about the co-movement in the long tail”.In our context, co-movement in the long tail of (changes in) the two outputs is important as a credit expansion, such as the one we witnessed in the 2000s was generalized. Co-skewness can help to identify whether this was, indeed, the case. Motivated by Yang et al. (2010) we report skewness and well as co-skewness descriptive statistics in Table 7.The empirical results show two things. First, skewness decreases during and in the aftermath of the sub-prime crisis for the core (France and Germany) and the periphery as well. Second, co-skewness, that is dependence in the long tail increases. Taken together this provides useful evidence that excessive loans have been restricted after the sub-prime crisis and this took place in tandem for both the core and the periphery. This corroborates nicely our empirical evidence on risk aversion, prudence and generalized risk.A well known risk measure is the z-score defined asz=R+capσR,where R is a measure of return like the return on assets (ROA) or the return on equity (ROE), cap stands for the capitalization ratio, and σRis a measure of volatility of return. The z-score is widely used in the banking literature to measure a bank’s probability of insolvency. It is attributed to Boyd and Graham (1986), Hannan and Hanweck (1988) and Boyd, Graham, and Hewitt (1993), and plays an important role in the assessment of both individual bank risk as well as overall financial stability. Boyd and Graham (1986) and Boyd et al. (1993) have pointed out that 1/z2 is an upper bound of the probability of insolvency (that is the probability of a bank having a negative capital asset ratio plus ROA) from which it follows that the z-score can be used in the wider context of insolvency, prudence and stability of financial institutions. Lepetit and Strobel (2015) made an excellent point in arguing that, in fact,11+z2provided a tighter bound on the probability of insolvency while1z2provides a good upper bound on the odds of insolvency. The two concepts are closely related as they are functionally related through a simple mathematical transformation.In view of our theoretical model and the measures of risk aversion and prudence, the problems with such a catch-all measure are many: (i) It does not account for the degree of risk aversion implicit in the construction of portfolios of financial institutions. (ii) It does not provide by itself an estimate, when used in practice, of the proper measure of variance; which is why in many studies panel data is used (see the excellent study by Lepetit and Strobel, 2015). (iii) It is evident that it is difficult to defend an ad hoc proper measure of “risk”, such asvar(X),making the calculation of z-scores problematic and the theoretical validation of “early warning signals” quite difficult. For example, in time series studies, the measurement of variance is a problem, see for example, Lepetit and Strobel (2015).1313The use of the z-score as an indicator of bank stability has a long history, see for example De Nicollo (2000), Cihak (2007) and Maechler, Srobona, and Worrell (2007). Its widespread use is evidenced by such papers as Bannier, Behr, and Guettler (2010), Barry, Lepetit, and Tarazi (2011), Beck, Demirgüç-Kunt, and Levine (2010), Carbo-Valverde, Hannan, and Rodriguez-Fernandez (2011), Demirgüç-Kunt and Detragiache (2011), Demirgüç-Kunt and Huizinga (2010), Foos, Norden, and Weber (2010), Houston, Lin, Lin, and Ma (2010), Koetter, Kolari, and Spierdijk (2012), Laeven and Levine (2009) and Schaeck, Cihak, Maechler, and Stolz (2012).(iv) The development of the literature on financial stability tends to move away from the simple z-score and adopts a wider perspective on “risk”. There is, for example, a related literature on identification of banking crises and construction of early warning mechanisms, see, for example, Berg (1999), Disyatat (2001), Demirgüç-Kunt and Detragiache (2011), Kaminsky and Reinhart (1996, 1999), Logan (2000), and Vila (2000). Our structural theoretical model solves important problems of implementation of this approach.As stated in a seminal paper by Aspachs et al. (2006):“In the ECB Financial Stability Review (December, 2005, p. 131), it is stated bluntly that “there is no obvious framework for summarising developments in financial stability in a single quantitative manner.” This is, to say the least, a considerable disadvantage when attempting to analyse financial stability issues. As the same ECB Special Feature on ‘Measurement Challenges in Assessing Financial Stability’, (ibid) put it, ‘Financial stability assessment as currently practiced by central banks and international organizations probably compares with the way monetary policy assessment was practised by central banks three or four decades ago – before there was a widely accepted, rigorous framework’.The authors proceed to argue as follows:“The point to note here is not so much the details […] but that the crucial aspects of the impact of shocks on the banking system are contained in two variables, bank profitability and bank repayment rate, which in turn is equivalent to its probability of default (PD)…” (page 8).Another approach is based on the concept of the Value at Risk (VaR). For any random variableXwith distribution functionFand a givenα¯∈(0,1)we have:VaR=min{y|F(y)≥α¯}=α¯. If the distribution is continuous,F(VaR)=α¯so it is a particular quantile. For example, the Derivatives Policy Group in 2005 proposed a standard for over-the-counter derivatives broker-dealer reports to the Securities and Exchange Commission that would set a time horizon of two weeks andα¯=0.01. For more details see Basel Committee on Banking Supervision (2004).Hanschel and Monnin (2004) and Illing and Liu (2003) are examples of two papers that search for a metric of financial stability, without relying on an explicit structural model and focusing on the separate cases of Canada and Switzerland. The structural model is provided in this paper and it is shown that estimation is not particularly difficult or involved.Here, we would like to compare and contrast our estimates with some alternative measures of risk. Based on the z-score we compute, directly from the data, the upper bound on the probability of insolvency,PI=11+z2,as argued by Lepetit and Strobel (2015)1414To compute the z-score we have used ROE.. This measure is provided in Table 8.The comparison is interesting in the sense that both risk aversion measures and generalized risk (Tables 1 through 4) start increasing even before the sub-prime crisis but the probability of default, in Greece for example, starts increasing considerably only after, roughly, 2009 or 2010 when the pressure on banking systems was already felt. In this sense, z-based measures are retrospective but both risk aversion measures and generalized risk are not, as they rely on more detailed (yet widely available) information on the price of loans.This paper has developed a new theoretical model of expected utility of profit maximization for financial institutions, subject to the neoclassical production possibility restrictions. The essential feature of the theoretical model is loan-price uncertainty in a multivariate context, an issue that has not been considered so far in the literature. The theoretical model can be estimated using standard econometric techniques: GMM for dynamic panel data along with latent factor analysis for the estimation of covariance matrices. An explicit functional form for the utility function is not needed and we show how measures of risk aversion and prudence (downside risk aversion) can be derived and estimated from the theoretical model. The model is estimated using data for Eurozone countries and we focus particularly on: (i) the use of the modeling approach as an “early warning mechanism”; (ii) the bank- and country-specific estimates of risk aversion and prudence (downside risk aversion); and (iii) the derivation of a generalized measure of risk that relies on loan-price uncertainty. The empirical results show that prudential behavior and risk aversion differ substantially among the peripheral countries and the rest of the Eurozone, as well as compared to French and German banks. Risk aversion in the French and German banking systems began to increase well in advance of the sub-prime crisis. The same is true for the Eurozone excluding the periphery. For the periphery, risk aversion followed the sub-prime crisis and started increasing only after 2008. Our generalized measure of risk shows that risk has been building up in the Eurozone since the early 2000s and is still at high levels although it has already begun to decrease for the large financial sectors of France and Germany. Moreover, the model provides estimates of loan price distortions and thus, allocative efficiency in the banking sector. Additionally, we estimate a model of co-skewness that allows for co-movement in the long tail of our two outputs, viz. loans and other earning assets. Estimates of allocative efficiency, co-skewness and their pattern over time, provide corroborating evidence to risk aversion, prudence, and our generalized risk measure.

@&#CONCLUSIONS@&#
