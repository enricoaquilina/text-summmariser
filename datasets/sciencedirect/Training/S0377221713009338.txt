@&#MAIN-TITLE@&#
Decision dependent stochastic processes

@&#HIGHLIGHTS@&#
A new Bayesian inferencing method for decision dependency in a stochastic process.Real world examples provided using preventive and corrective maintenance in a nuclear power plant.A stochastic optimization problem is solved using the decision dependent probability distribution inferences.

@&#KEYPHRASES@&#
Bayesian inference,Decision dependence,Endogenous,uncertainty,Weibull,

@&#ABSTRACT@&#
Managers, typically, are unaware of the significant impact their decisions could have on the random mechanism driving a data generating process. Here, a new parametric Bayesian technique is introduced that would allow managers to obtain an estimate of the impact of their decisions on the stochastic process driving the data; this, in turn, should enhance a company’s overall decision-making capabilities. This general approach to modeling decision-dependency is carried out via an efficient Markov chain Monte Carlo method. A simulated example, and a real-life example, using historical maintenance and failure time data from a system at the South Texas Project Nuclear Operating Company, exemplifies the paper’s theoretical contributions. Conclusive evidence of decision dependence in the failure time distribution is reported, which in turn points to an optimal maintenance policy that results in potentially large financial savings to the Texas-based company.

@&#INTRODUCTION@&#
Managers are frequently tasked with making critical decisions in the face of uncertainty without the tools to assess the impact of their choices. These decisions can influence a stochastic process in complex ways that are not intuitive and are currently difficult to quantify. Decision makers can thus benefit tremendously from estimating the dependency between decisions and a statistic of interest.This article presents a new Bayesian inferencing technique to detect and estimate decision dependency in a stochastic process. The technique allows one to form a precise estimate of the influence a managerial decision exerts upon the process, and consequently estimate the influence the decision makes upon variables that depend on that process. There does not currently exist any similar technique that can quantify decision dependency in the generality shown in this paper. Moreover, this new approach is set in a Bayesian framework and a practitioner can easily take advantage of numerous existing Bayesian methods for assessing statistical significance of the dependency estimates, as demonstrated throughout the numerical examples. This paper applies this method to analyze the details for the case of maintenance decisions and their influence on components of a nuclear power plant.In Section 1.1 a brief review is provided of the related literature in both reliability theory and stochastic optimization. Section 2.1 defines the set of maintenance decisions that can act upon the system and affect the failure time distribution. Section 2.1 then describes the model for decision dependent failures in a system at a nuclear power plant. The estimation procedure is implemented in Sections 2.2 and 2.3 by applying modern Markov chain Monte Carlo (MCMC) techniques to estimate the model parameters. First, a numerical example is provided in Section 2.2 that relies on artificial data which is useful to demonstrate the ability of the method in a controlled setting and to provide confidence. A second example presented in Section 2.3 uses the maintenance history and failure times from a system at the South Texas Project Nuclear Operating Company (STPNOC) located in Bay City, Texas. Section 3 describes the ensuing optimization problem to determine the optimal decision policy using the Bayesian inferencing results, resulting in a sequence of future decisions that lead to potentially large financial savings.Barlow and Hunter (1960) published an early and important work in maintenance policy theory. They define two types of maintenance policies for performing preventive and corrective maintenance in a nuclear power plant example. The first policy requires performing preventive maintenance after a fixed amount of time of continuous operation. If the system fails before the scheduled maintenance, maintenance is performed immediately and the next preventive maintenance is rescheduled. Maintenance of any type under this policy is assumed to restore the system to as good as new status. The second policy schedules preventive maintenance at fixed times, regardless of any failures and corresponding repairs between those fixed times. In this second policy however, the maintenance following a failure merely brings the item back to the status it was in just before failure, so that the age of the item is not reset. The second policy is exactly like the policy defined below in Section 2.1 except for two new important generalizations. The present article makes allowance for the failure rate to depend on previous types of maintenance performed, differing from that of Barlow and Hunter (1960), and this new approach allows the failure rate to change after performing the repairs following a failure event, unlike Barlow and Hunter (1960).Nguyen and Murthy (1981) develop optimal maintenance policies while considering the case of a failure rate that increases with the number of repairs. Their work, like most of the work on these topics, assumes an infinite time horizon for constructing an optimal policy. It is also possible to improve upon the notion that preventive maintenance perfectly restores an item. Nakagawa (1988) introduced the idea of an improvement factor, such as maintenance that can not only reduce the hazard rate of a system, but also reduce the age of a system without perfectly restoring it. Nakagawa (1988) forms optimal policies of sequences under both of these scenarios using the usual assumption of a Weibull shaped failure time distribution. Nguyen and Murthy (1981) and Nakagawa (1988) are important contributions for constructing a modern approach to modeling failure times, however neither project addresses the effect of decision dependency.Singh (2011) further developed models for periodic preventive maintenance using virtual-age based age-reduction factors as well as factors describing the change in the failure rate of the item. The parameters in these models are estimated in a Bayesian framework using a Markov chain Monte Carlo method and are then input into a two-stage stochastic optimization program for determining the optimal interval to perform periodic imperfect preventive maintenance.The problem of choosing the optimal time interval for scheduled maintenance is also explored in the work of Damien, Galenko, Popova, and Hanson (2007). This work demonstrates a semiparametric Bayesian model to determine the optimal time to schedule preventive maintenance while allowing for corrective maintenance in order to minimize the expected cost of operating a single item in the context of a nuclear power plant. Their work is the first to report that the problem of scheduling preventive maintenance and performing corrective maintenance after failures has a decision dependent influence on the expected lifetime and total cost of operation. This finding is just one of many decision dependent scenarios and serves as the ideal motivation for the present article to quantify decision dependent uncertainty.The related literature on stochastic programming can be divided into two classes, problems with exogenous uncertainty and problems with endogenous uncertainty. Exogenous uncertainty in a stochastic process is uncertainty that is not influenced by optimization decisions. Endogenous uncertainty in a stochastic process is uncertainty that depends on the optimization decisions, either explicitly or implicitly. Goel and Grossmann (2004) make this distinction in their literature review and note that most previous work focuses on exogenous uncertainty, where the optimization decisions cannot influence the stochastic process. Goel and Grossman also note that Pflug (1990) was the first work towards solving problems with endogenous uncertainty. Goel and Grossmann (2004) further classify endogenous uncertainty by two types of effects produced. The first type of decision dependent uncertainty is where a decision-maker may change the probability distribution of the process by making one outcome more likely than another. The second type of decision dependent uncertainty is that of increased information available to the decision-maker by partially resolving a particular uncertainty. Goel and Grossmann (2004) further clarify the distinction by noting that in the first case the decision-maker can force one possibility to become more probable. In the second case the decision-maker can only become more sure as to which possibility may occur in the future. The decision dependency estimated in the present article falls into the first category of endogenous uncertainty, wherein the decision alters the underlying probability distribution.Goel and Grossmann (2004, 2006) present a planning problem for gas fields where the decisions influence the available information by further resolving uncertainty about the potential yield from that area. A costly investment can be made to build infrastructure in a gas field that gives information about the region and reduces uncertainty. The decision of whether to build the infrastructure in a particular region influences when the uncertainty is resolved. In Goel and Grossmann (2006) a branch and bound method is demonstrated, and in Goel and Grossmann (2004) a scenario tree is used to describe the evolution of the stochastic process and also the uncertainty that is resolved after each decision. The solution for this class of problems uses a hybrid mixed-integer disjunctive program that includes non-anticipativity constraints. It is clear from this and other important works that each instance of decision dependency must be solved with the context of the problem in mind, greatly increasing the difficulty of generalizing solutions to decision dependent problems.An interesting example of decision dependency in an optimization method itself is presented in Pflug (1990). Pflug (1990) introduces a method for determining the minimization of an objective function involving a Markovian process with a recursive estimation procedure. His method simulates several steps of the Markov process, each under different control values, then computes a stochastic gradient. Depending on the results, the procedure is iteratively repeated with different control values. This method is similar to a gradient descent method in deterministic systems, however the decision of how to adapt the control variables changes the ultimate results of the optimization routine, making the result inherently decision dependent. The presentation of decision dependency in an optimization method itself, not just in a system to be modeled, demonstrates the ubiquitous nature of decision dependency and also underscores the importance for further understanding.Peeta, Salman, Gunnec, and Viswanath (2010) solve a two-stage stochastic program where investment decisions must be made for strengthening a highway network before a disaster. The first stage decisions influence the probability distributions for the subsequent damage to the network links. The first stage variables are restricted to integers and the distributions of the random parameters depend on these variables. The problem thus falls under the umbrella of stochastic integer programming, although it has a decision dependent structure that adds tremendous complexity. Peeta et al. (2010) state that models with decision dependent probabilities are typically known to be quite difficult to solve and suggests trying the sample average approximation (SAA) method originated in Ahmed and Shapiro (2002) and Kleywegt, Shapiro, and Homem-de Mello (2002). The present article takes advantage of some empirical structure found in the objective function values and uses a genetic algorithm to quickly converge to an optimal solution.In this section the definition and estimation of a decision dependent stochastic process in a Bayesian setting are introduced. The stochastic process is described in the context of maintenance planning at a nuclear power plant where the Bayesian estimation technique is introduced in order to quantify the dependency of the maintenance decisions on the lifetime of equipment at the power plant. Results are reported from testing the method on both simulated and real data from the STPNOC.A nuclear power plant is comprised of numerous systems, each of which require frequent maintenance and fails at random times. Assume for a particular system that the failure time distribution of the system isf(t|θ), whereθis a vector of parameters indexing the density from a class such as the Weibull family of distributions. Now consider the case of two decision types, where each type refers to a different type of maintenance performed on a system. A decision value of 0 corresponds to preventive maintenance (PM) which resets the age of the item to zero. This may be achieved via replacing components of the system or by a major restoration of the system. A decision value of 1 corresponds to corrective maintenance (CM) which repairs the system such that the age of the system is restored to “as good as old” status. When a system fails, a CM is performed and the system is restored to the condition it was in immediately preceding the failure. The assumption that CM does not alter the age of the system is an approximation that is most accurate in the case of partial repairs and upgrades such as lubrication to components of the system. Additionally, restricting the decision set to only two decisions is an approximation that is useful for illustrative purposes, but in general one could use several different decision states to model numerous decisions affecting the failure process.Assume there exists a set of increasing times defined by{t0,t1,…,tn|t0=0,ti<ti+1,tn=tmax∀i<n}such that a decision of either PM or CM is made at each point in time. Define the time series of decisions{di}i=0nsuch thatdi∈{0,1}wheredi=0if PM is performed attiordi=1if CM is performed. Furthermore, denote the time that the most recent PM was performed byti∗=max{tj|j<i,dj=0}. Note that if preventive or corrective maintenance is performed at timetiwithout an accompanying failure, the failure time stemming from the previous decision is right-censored, indicated byδi=0, otherwise all uncensored data hasδi=1. Thus the complete data set is a stochastic vector process{(ti,di,δi,xi)}i=1n, wherexiis a p-dimensional vector of exogenous covariates which may affect system reliability such as operating temperature and humidity, or other characteristics such as the make and model of a pump.Bayesian models are constructed by defining a likelihood function and then further specifying the prior distribution. Utilizing Bayes theorem, the posterior distribution is proportional to the product of the likelihood function and the prior distribution which are defined separately below.The likelihood function is defined such that it accumulates probability contributions from a distinct distribution associated with each decision in the set of possible decisions. The framework below is easily generalized to the case of multiple decisions, but first consider the case where the likelihood function depends on two decisions, PM and CM. When a PM decision is made, i.e.di-1=0, the lifetime of the system is restored to zero at timeti-1. A failure at timetithen has the likelihood contributionf(ti-ti-1|θ), whereθis the parameter vector indexing the density associated with a PM distribution. If the failure time is censored, the contribution is not from the density, but instead from the survival functionS(t|θ)=1-F(t|θ). When a CM decision is made, i.e.di-1=1, the system age is brought back to the state it was in immediately before failure, and additional aging must be accounted for since the last preventive maintenance decision at timeti∗. The failure time is thus truncated atti-1and the system has accumulated wear sinceti∗, so the likelihood contribution isf(ti-ti∗|θ+β)/S(ti-1-ti∗|θ+β)whereθ+βis a parameter vector for the distribution associated with the CM decision. Each component of theβparameter is the amount added toθto construct a separate distribution that accounts for the decision dependency. Note that a component ofβmay be negative, so that each component ofθ+βis free to obtain any value inR. The likelihood functionL(θ,β)is thus defined as(1)L(θ,β)=∏i=1nf(ti-ti-1|θ)δiS(ti-ti-1|θ)1-δi1-di-1*f(ti-ti∗|θ+β)δiS(ti-ti∗|θ+β)1-δiS(ti-1-ti∗|θ+β)di-1and when a prior distribution is specified on bothθandβwithp(θ,β), the resulting posterior distributionπ(θ,β)is given by(2)π(θ,β)=L(θ,β)p(θ,β)∬L(θ,β)p(θ,β)dθdβwhere the integrals are evaluated over all possible values for a given choice of parametric distribution. Note that one need not solve these integrals explicitly to use the posterior distribution, because modern MCMC techniques allow one to sample fromπ(θ,β)indirectly as demonstrated in Section 2.2. This method gives a decision maker the ability to quantitatively estimate the extent to which choosing between performing PM or CM affects the failure time distribution by examining the relative sizes of each component ofθandβ. If one determines the magnitude ofβito be significant, i.e.βi≉0, then the data is said to be decision dependent with respect to that parameter of the distribution.The impact of decisions is directly quantified via the formulation of the likelihood function in Eq. (1) at every point in time. The impact of any decision, moving forward in time, is captured in the posterior distributions of the parameters that are directly impacted by the decision to perform PM or CM at any point in time; these posterior distributions are then used to calculate PM and CM predictive distributions obtained from the Bayesian Weibull model. These predictive distributions show the impact of a PM or CM decision in the evolution of the system. The predictive distributions quantify the impact of decisions taken; i.e., a prediction is a measure of the impact of decisions modeled via the time-dependent likelihood function in Eq. (1). It is for the above reasons we succinctly labeled our approach as a decision-dependent stochastic process formulation.Further statistical tests to confirm decision dependence are provided in Section 2.2 using the Kolmogorov–Smirnov test and the Kullback–Liebler divergence. See Borovkov and Moullagaliev (1999), Liao (2011), and Casella and Robert (2004) for complete discussions of the two sample Kolmogorov–Smirnov test and the Kullback–Liebler divergence.The managerial influence upon the process is directly observed as a result of the distinction in the failure time distributions associated with PM and CM in the likelihood function in Eq. (1). Once these two distinct distributions are estimated for a given data set, the decision maker can specifically assess how a PM or CM decision influences the failure time distribution with a specific mean and variance: this “Bayesian learning” is the estimation phase. Two key outputs from the analysis are the unique PM and CM predictive distributions whose characteristics will help a manager determine which decision has impacted the system for better or for worse, since the predictions quantify the systems future performance, based on the “learning phase”. For example, if the PM decision in the estimation phase is expensive, but drastically extends the expected lifetime of the item in the prediction phase, then a decision to do PM impacts the system in a desirable manner. One of the main strengths of a Bayesian approach is precisely this quantification of uncertainty via predictive distributions which can be obtained using MCMC methods; see, for instance, Gamerman and Lopes (2006), Chib (2013), and Hanson and Jara (2013).Before tackling a real world system, simulated decision dependent data is constructed to test the implementation of the Bayesian inferencing method outlined in Section 2.1 and to ensure that the algorithm can correctly estimate the distribution parameters with an acceptable error size. Once the detection algorithm is deemed reliable, it is applied to the maintenance decision and failure time history for a condenser system at the STPNOC. The estimated failure parameters are used to form two predictive distributions for the time to failure, one for each of the two decision types.The following steps are implemented to create a simulated data set with decision dependency using the statistical package SAS 9.2 (2010). Before generating the simulated data a Weibull distribution is chosen to model the failure time distribution. The Weibull distribution is parameterized in the usual way,(3)W(t,α,λ)=αλtλα-1e-t/λαwhereαandλare the shape and scale parameters respectively. The Weibull shape and scale values are selected for each distribution associated with PM and CM decisions, and then a fair coin is tossed at each time step to determine whether corrective or preventive maintenance will be performed. The inverse CDF method is used for sampling from each distribution, while also keeping track of the age of the system via the length of time since a previous PM renewed the lifetime. This creates a simulated time series to test the Bayesian inference method’s ability to pick out the Weibull parameters used to generate the simulated data.The estimation procedure relies on the well known Gibbs Sampler MCMC method which is discussed extensively in Casella and Robert (2004), and the algorithm is written in the R statistical language developed by the R Development Core Team (2011). Once the simulated data is created, the Gibbs Sampler method forms a Markov chain for each parameter. For additional background on MCMC methods, see Gamerman and Lopes (2006) and Albert (2009). When the Markov chain has run sufficiently long, the transition distribution is essentially stationary and a sample from the chain is representative of the posterior distribution for each parameter. The Markov chain is thinned by using only the samples taken at prescribed intervals to reduce autocorrelation. Autocorrelation in the Markov chain may result from poor mixing, i.e. difficulty in walking around in the parameter space. The convergence of the Markov chain is monitored using the CODA package by Plummer, Best, Cowles, and Vines (2006) to perform a number of diagnostics. After running the Gibbs sampler for a large number of cycles, an estimated burn-in period is removed and then after deciding on a thinning interval, all diagnostics are performed on the remaining samples taken from the thinned chain. Larger thinning intervals correspond to smaller lag 1 autocorrelation which adds confidence that the Gibbs samples are representative of the posterior, although the increased confidence brings increased computational cost. The Geweke convergence diagnostic, discussed fully in Bartholomew, Knott, and Moustaki (2011), is computed and compares the mean of two different fractions of each chain using a z test and then computes a Z-score to help diagnose convergence. A large Z-score indicates that the two means are unlikely to come from the same distribution and that convergence is unlikely to have been reached, hence a small Z-score is sought for each parameter.Once the MCMC chain burn-in period is removed and the chain thinned, the sampled distribution for each parameter is taken as representative of its posterior distribution. Note that the entire MCMC chain is incorporated in the predictive densities below, rather than a point estimate of the parameter values, so that full information is retained from the Bayesian inference stage.(4)p1(t)=limM→∞1M∑i=1MW(t,α1i,λ1i)(5)p2(t)=limM→∞1M∑i=1MW(t,α2i,λ2i)whereθ=α1,λ1andθ+β=α2,λ2, so thatβ=α2-α1,λ2-λ1. An astute decision maker would utilize the difference in expected time to failure for each predictive distribution when choosing between CM or PM in the next decision point. Better still, a manager should use these estimates as an input to an optimization model and ultimately form an optimal strategy as demonstrated in Section 3.The Gibbs sampler method described above is now applied to a simulated data set from two Weibull distributions,W(α1=1.5,λ1=5)andW(α2=3.0,λ2=10). 150 data points are created in order to stay consistent with the approximate length of the largest STPNOC data set. 75,000 Gibbs cycles are computed, then the first 25,000 burn-in cycles are removed. The chain is further culled using a thinning interval of 300 points, which leaves 167 samples from the stationary distribution of the Markov chain. Fig. 1shows the trace and density for each of the four Weibull parameters as the chain grows.The estimation resultes provided in Fig. 1 and Table 1above provide convincing evidence that the MCMC routine is able to pick out the correct parameter estimates of both distributions. The true distributions used to create the data are the Weibull (α1=1.5,λ1=5) and the Weibull (α2=3.0,λ2=10). The algorithm, which has no prior knowledge about the parameter values, estimates the distributions as Weibull (α1=1.37,λ1=5.24) and the Weibull (α2=2.64,λ2=10.68). The standard deviations are small enough to have confidence that the estimated mean values have been correctly identified for each parameter. Additional estimations were made with even larger artificial data sets that demonstrate smaller errors around the true distribution parameters. The lag 1 autocorrelation for the second Weibull distribution remains slightly high, considering the large thinning interval. The Geweke Z-score, computed by comparing the mean of the first 20% with the mean of the last 30% of the chain, appears reasonable and does not indicate that the chain failed to converge. Notice that the choice of different parameters for the two Weibull distributions implies an a priori decision dependence. The significant difference in Weibull parameter estimates in the real data from STPNOC is interepreted as evidence of decision dependency. Figs. 2 and 3display the two predictive densities and show the close agreement between the algorithm’s inference and the true density used to create the decision dependent data. Additional examples are presented in Fig. 4to highlight the inferencing capability across five separate pairs of PM and CM distributions. The simulated data for these examples are generated by the following pairs of distributions:•{PM=Weibull(1.5,5),CM=Weibull(3,10)}.{PM=Weibull(1.5,1),CM=Weibull(3,1)}.{PM=Weibull(0.8,2),CM=Weibull(1.2,2)}.{PM=Weibull(1.1,2),CM=Weibull(2,2)}.{PM=Weibull(1.1,2),CM=Weibull(1.3,2)}.The algorithm is successfully tested with PM and CM distributions from a wide range of Weibull parameters, underscoring the ability for successful detection of decision dependency in pairs of distributions that are both similar and quite different. This a strong testament to the robustness of this estimation procedure, which utilizes the decision history in the likelihood contribution from each data point.Visual inspection of Figs. 2 and 3 clearly demonstrates the success of the Bayesian inferencing approach. The discrepancy between the inferred and true parameters can be further reduced with additional simulated data. The distinction between the Weibull distribution governing the PM and the Weibull governing the CM can be quantified using the Kullback-Liebler divergence and also by the two-sample Kolmogorov–Smirnov test. The Kullback-Liebler divergence, defined as(6)DKL(P,Q)=∫-∞∞p(x)logp(x)q(x)dxis estimated to be 42.95 withp(x)=p1(x)andq(x)=p2(x). This value indicates a substantial divergence and that the two Weibull distributions are distinct, as expected in this simulated case.The two-sample K–S test, a hypothesis test with the null hypothesis that two empirically sampled distributions are identical, has a test statistic D defined as(7)Dn,n′=supx|Pn(x)-Qn′(x)|This test is performed twice, once comparing the sampled distributions of the shape parameters, and a second time with the scale parameter distributions. In practice the true distributions are unknown and a manager will test for decision dependence by testing for distinction between the PM and CM distributions built into the likelihood function. The two shape distributions have a K–S test statistic valueD=0.982with a p-value equal to 2e−16, indicating one must reject the null hypothesis and accept that they are clearly distinct distributions. The two scale parameter distributions have a K–S test statisticD=0.999, with a p-value equal to 2e−16. These two measures, the Kullback–Liebler divergence and the Kolmogorov–Smirnov test, both overwhelmingly support the fact that the two distributions are different in the case of the artificial data set. The full power of these tests is realized in the next section with a real data set where the two shape and two scale distributions are more similar and the underlying true distributions are unknown.Note that having two distinct Weibull distributions implies that the expected time of the next failure differs for each decision because the expectation is taken with respect to the particular distribution associated with that decision. In the following section the same approach is applied to the maintenance decision and failure time data from STPNOC.The maintenance decision and failure history for a vacuum pump in the condenser system from STPNOC contains 147 data points spanning February 1999 to December 2009. The Gibbs sampler algorithm is run for 185,000 cycles, removing the first 10,000 as burn-in and using a thinning interval of 1000 points, so that 175 points are sampled for posterior estimation. The scale parameters associated with PM and CM decisions are essentially identical, however a significant difference in shape parameters is detected. Fig. 5shows the trace and density of each parameter.The results presented in Table 2indicate a clear decision dependency. The difference in the means of the two shape parameters is substantial, and their respective standard deviations are small enough to easily distinguish them as different parameters, thus implying a decision dependency in the process. The Kullback–Liebler divergence, defined in the same way as with the artificial data, is estimated to be 23.25. The two shape distributions have a Kolmogorov–Smirnov test statistic valueD=0.7485with a p-value equal to 2e−16, indicating that one must reject the null hypothesis and accept that the shape distributions are distinct. The two scale parameter distributions have a K–S test statisticD=0.1257, with a p-value equal to 0.1426. The value of D, combined with the large p-value, does not compell a rejection of the null hypothesis that the scale parameter distributions are distinct. These two measures, the Kullback–Liebler divergence and the Kolmogorov–Smirnov test, both support the conclusions indicated by the CODA summary and error estimates.In order to further confirm the detection of decision dependency, the following procedure is performed on the four Markov chains. Assuming a null hypothesis that the shape parameters for both PM and CM are not statistically different, one may seek to determine if it is possible to reject that hypothesis using standard techniques with a kernel density estimator. The difference of each pair of parameters in the MCMC output is calculated, and then the pairs are sorted by size. The sorted list is fit using a kernel density estimation, which is a nonparametric technique for determining the probability density of the sorted differences. Thus, if the kernel density estimations are centered far away from zero, decision dependency is believed to exist. Figs. 6 and 7confirm that the shape parameters are indeed decision dependent, and the scale parameter does not demonstrate decision dependency.By inspecting the above graphs, K–S test statistics, and Kullback–Liebler divergences, one is compelled to reject the null hypothesis for the shape parameters and accept the null hypothesis for the scale parameters, i.e. decision dependency is present in the shape distributions. Given that the PM and CM decisions lead to different failure rates, the predictive densities for each distribution take on increased importance and are shown in Figs. 8 and 9.The benefit of this new Bayesian estimation method is clear; a manager can now easily identify the two distributions and use that information for making a more informed decision. Specifically, the Risk Management Group at STPNOC may take advantage of the difference in the two shape parameter distributions because it implies that the PM Weibull distribution places significantly more likelihood of failure at later times. The predictive densities above show how the expected time of failure is distributed when CM and PM decisions are made.A decision maker who applies the inference technique described in section 2.1 will naturally desire an optimal decision strategy to minimize the costs. The following section defines the optimization problem by specifying an objective function to evaluate the expected costs of an arbitrary sequence of PM and CM decisions. The manager is thus interested in finding the set of decisions to make a priori that minimize the expected costs over a finite horizon. Two separate algorithms are presented which provide distinct approaches for determining the optimal decision set leading to the lowest expected cost. The comparative advantages of each algorithm with respect to computational time and accuracy are then discussed.The following objective function is constructed while keeping the previous nuclear power plant example in mind. Assume that maintenance decisions of either PM or CM must be made at evenly spaced intervals{t0,t1,…,tn-1}until a finite horizon time. The motivation for evenly spaced time intervals stems from the current STPNOC policy of regular 12week maintenance with a government mandated shutdown and overhaul every 18months. This leads to six decision points where either PM or CM can be made and it is desirable to determine the sequence of PM and CM decisions that minimize the expected cost.Define the fixed costs of both PM and CM maintenance asCpmandCcm. If the system fails between maintenance times, CM is performed for a total cost ofC‾=pCd+(1-p)Ccm, where p is the probability that the failed system will trigger power reduction andCdis the associated downtime cost. Denote bydTi=1if CM is performed at timeTianddTi=0if PM is performed at timeTi. The decision dependency is modeled by assuming Weibull failure times with shape parameter defined asθi=θ+dTi-1β. Therefore, if PM was performed atTi-1the shape parameter equalsθand if CM was performed it equalsθ+β.The objective function to be minimized has two summands: fixed cost (FC) and (variable) failure cost (VC). The fixed cost equalsFC=∑i=1n-1Cpm1-dTi+dTiCcm,and the failure cost equalsVC=C‾∑i=1n-1EdTi-1N(Ti-1,Ti),whereN(Ti-1,Ti)is the random variable of the number of failures in the interval[Ti-1,Ti),i=1,…,nand the expectation is taken with respect to the most recent decision-dependent failure time distribution. Note that the expectation utilizes the entire Markov chain associated with each decision, so that the distribution of both the shape and scale parameters for each decision are fully represented from the data.It is desirable to find the collection of decisions{dT0,dT1,…,dTn-1}that minimize the total expected maintenance cost, thus the optimization is precisely defined as:(8)mindT0,dT1,…,dTn-1∑i=1n-1Cpm1-dTi+dTiCcm+C‾∑i=1n-1EdTi-1N(Ti-1,Ti)The following list describes in detail the computation of each term of the objective function. The objective function must add the fixed PM and CM costs at each decision point as well as the costs due to the expected number of failures. It must also take into account the non-renewal effect of CM and the differing Weibull failure parameters. In order to keep track of the renewal points, the algorithm must count the number of consecutive CM decisions,n∗, preceding each PM decision. This is analogous to thet∗used in the estimation of the likelihood function from historical decision points in Section 2.1. Note that when calculating the expected number of failures with respect to the CM distribution the lifetime is already one interval old.The objective function is constructed for a particular decision setdˆ={dT0,dT1,…,dTn-1}by the following rules:1.Calculate the objective function by first computing the fixed cost and expected number of failures over the first interval.Each time a CM decision is made, incrementn∗and add the fixed costCcm.Each time a PM decision is made, add the fixed costCpmand then:(a)Look backwards and add up the expected failures with respect to previous CM distribution,EN(n∗+1)Δt|θ+β-ENΔt|θ+β(Note: the subtraction is due to the lifetime already being one interval old)Then look forward and add the expected failures over the immediate right-hand time intervalENΔt|θAt the final decision point, check for any expected failures due to previous CMs that were not accounted for since last PM.The objective function is thus calculated by computing the first, middle set, and last decisions and then adding them.z0=1{d0=0}Cpm+C‾ENΔt|θ+1{d0=1}Ccmz1:n-2=∑i=1n-21{di=0}Cpm+1{di=1}Ccm+1{di=0}C‾ENΔt|θ+EN(n∗+1)Δt|θ+β-ENΔt|θ+βzn-1=1{dn-1=0}Cpm+1{dn-1=1}Ccm+1{dn-1=0}C‾ENΔt|θ+EN(n∗+1)Δt|θ+β-ENΔt|θ+β+1{dn-1=1}C‾EN(n∗+1)Δt|θ+β-ENΔt|θ+βThe objective function is sufficiently complicated that building an intuition about its behavior through analytical techniques is quite difficult. Its dependence on the particular sequence of decisions make a computational approach essential, because the objective value can change substantially from one decision set to another by modifying only a single decision point. The following section explores how to determine the optimal set of decisions to minimize the objective function.The objective function defined above is highly path dependent and one might initially attempt a dynamic programming approach to solve it. This approach is not applicable however, because dynamic programming relies on recursively taking expectations beginning with the final decision and working backward. For more information on dynamic programming, see Bellman (2003). The evaluation of this objective function requires that the time since the most recent PM must be known at each decision point, thus making the problem “forward looking”.The first approach to finding a solution is to evaluate the objective function over all possible decision combinations and sort the objective values to determine the decision set with the minimum expected total cost. The optimal cost may not result from a unique decision set, but in fact may be realized by multiple decision paths. An example of such a case is provided below. Note that evaluating the objective function for all possible decision sets scales asO(2n)where n is the number of decisions to be made until the horizon. Such scaling problems prohibit this approach for large n without utilizing significant parallel computational resources or approximation methods. The evaluations of different decision sets are independent making the large n problem “embarrassingly parallel.” Fortunately, the full computation of all decision sets is feasible for the practical size problems faced at the STPNOC.The following optimization uses the decision dependent estimation results from the STPNOC example presented in Section 2.1 and Table 2 and Fig. 5. The rest of the parameter values used in the objective function are: CM cost of $7000, PM cost of $35,000, downtime (reduced power) cost of $4,500,000, and the probability of triggering reduced power outage isp=0.005. The optimization runs over an 18month time horizon and assumes maintenance decisions are performed approximately every 12weeks, i.e.n=6. The optimization yields two optimal decisions:{0,1,1,1,1,1}and{1,1,1,1,1,0}both with a total cost of $543,714, where 1 corresponds to corrective and 0 to preventive maintenance.It is desirable to know the behavior of the objective function as the decision set is varied. Consider the decision set as the binary representation of an integer. Then each of the n-dimensional decision sets can be represented on a single dimension, namely the natural numbersN0={0,1,2,…}. Transforming the decisions from a binary representation to a decimal representation enables one to plot the objective function against all decisions on a single graph. Of course, the objective function is still dependent on the other parameters, but this transformation is useful for analyzing a particular item where the parameters are fixed and the decision maker can only affect the decision set. Fig. 10displays the objective function value over all possible decision sets for the item and parameters discussed above.The clustering that appears at particular values of the objective function has been observed for several different distributions and is not particular to this individual item. The structure of the clustering varies, but so far every item modeled has displayed some form of clustering at distinct bands of objective values. An algorithm is developed in the next section to take advantage of the banded structure for larger decision sets. Fig. 11shows the objective values when the decision number is increased ton=12.The clustering of costs into only a few bands is now extremely apparent, however it is still difficult to predict which band each decision set will fall. The banded clustering becomes increasingly noticeable as the decision number is increased. Forn=15, the computational time required to compute all objective values on a modern laptop is approximately an hour. Parallel techniques could be used to tackle larger n, although one should consider a heuristic approach that negates the need for large scale computation. Section 3.3 describes how to approach the case where n is prohibitively large.If a manager faces a large number of decisions so thatn≫1, then the computational expense of computing the objective function for all decision combinations grows much too fast to be feasible. For the case of binary decisions, such as a decision of PM or CM, the computational cost grows asO(2n)and another method must be used to provide an optimal policy in a reasonable amount of time. A heuristic approach is reasonable in cases where there is a large amount of structure in the objective function costs, such as the clustered values seen in Fig. 11. One seeks an optimization algorithm that can intelligently evaluate candidate decision sets and improve the optimal choice efficiently towards finding a minimum objective value. Fortunately, a genetic algorithm for minimizing a function of a sequence of binary variables has been developed that perfectly suits this scenario by Willighagen (2005). This genetic algorithm views each combination of decisions as a gene and each binary decision of PM or CM as a binary chromosome that makes up the gene. As the algorithm evaluates different genes, it generates new candidate genes by combining chromosomes from different genes that show promise for improving the minimization. The result is that the algorithm will run for a specified number of iterations and then report the gene, or decision set, that is the minimum value tested. Although no guarantee of optimality can be made with this method, the computational gains far outweigh the prohibitive costs of more certain algorithms.Fig. 12displays the dramatic decrease in the expected costs as the genetic algorithm iterates and tests different decision sets forn=12. For then=12case, one can be confident of convergence not only because the expected cost plateaus, but also because the optimal decision evaluated is the same as that outlined in Section 3.2.1. In Fig. 13the results are displayed for a much larger problem,n=50. These results are obtained in minutes on a modern laptop with the genetic algorithm, but would take more than several weeks to obtain using an exhaustive search. The optimal decision set for the same problem withn=50is all CM decisions with an expected cost of $2,268,506.

@&#CONCLUSIONS@&#
A new Bayesian inferencing method is demonstrated to quantify decision dependency in a stochastic process. The method relies on a likelihood function constructed from decisions associated with two distinct probability distributions. The method has been implemented and correctly detects decision dependency in both a simulated data set, and real historical data from a condenser system at the STPNOC. The estimation procedure reveals distinctly different failure time distributions when CM and PM decisions are made.Managers can use this estimation procedure not only for maintenance decision dependency in nuclear power plants, but also for any stochastic process wherein the decision choices affect the process. The impact of any decision, moving forward in time, is captured in the posterior distributions of the parameters that are directly impacted by the decision to perform PM or CM at any point in time; these posterior distributions are then used to calculate PM and CM predictive distributions obtained from the Bayesian Weibull model. These predictive distributions show the impact of a PM or CM decision in the evolution of the system.Additionally, an objective function is introduced that describes the total costs over a finite horizon for PM and CM decisions when the future failure rate and system age is dependent on the sequence of decisions. Two optimization methods are provided to solve the optimization problem so that a manager can utilize the decision dependency information to determine the optimal decision policy. The decision maker will prefer one optimization algorithm over the other depending not only on the number of decisions to be made until the horizon, but also the computational resources available.