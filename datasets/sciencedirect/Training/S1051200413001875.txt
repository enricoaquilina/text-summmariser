@&#MAIN-TITLE@&#
Signal segmentation using changing regression models with application in seismic engineering

@&#HIGHLIGHTS@&#
We present a method for optimal segmentation with application in seismic signal processing.Some interpretations and connections with other approaches and computational aspects are discussed.The case studies and comparisons in simulation confirm the efficiency of the proposed approach.The segmentation results in seismic signal analysis are compared with the time–frequency analysis.The approach could provide new physical insight into seismic waves propagation and soil properties.

@&#KEYPHRASES@&#
Change detection,Data segmentation,MAP estimator,Monte Carlo simulation,Seismic signal processing,

@&#ABSTRACT@&#
The change detection and segmentation methods have gained considerable attention in scientific research and appear to be the central issue in various application areas. The objective of the paper is to present a segmentation method, based on maximum a posteriori probability (MAP) estimator, with application in seismic signal processing; some interpretations and connections with other approaches in change detection and segmentation, as well as computational aspects in this field are also discussed. The experimental results obtained by Monte Carlo simulations for signal segmentation using different signal models, including models with changes in the mean, in FIR, AR and ARX model parameters, as well as comparisons with other methods, are presented and the effectiveness of the proposed approach is proved. Finally, we discuss an application of segmentation in the analysis of the earthquake records during the Kocaeli seism, Turkey, August 1999, Arcelik station (ARC). The optimal segmentation results are compared with time–frequency analysis, for the reduced interference distribution (RID). The analysis results confirm the efficiency of the segmentation approach used, the change instants resulted by MAP appearing clear in energy and frequency contents of time–frequency distribution.

@&#INTRODUCTION@&#
The problem of change detection and diagnosis has gained considerable attention during the last three decades in the research context and appears to be the central issue in various application areas. From a statistical point of view, change detection tries to identify changes in the probability distribution of a stochastic process. In general, the problem involves both detecting whether or not a change has occurred, or whether several changes might have occurred, and identifying the times of any such changes.In the off-line applications, it is available a batch of data, and the goal is to find the time instants for system changes as accurately as possible. This is usually called segmentation. Deeper interactions between the control, signal processing, and statistical communities have recently contributed to the creation of new insights into the change detection problem in a significant way.The analysis of the behavior of real data reveals that most of the changes that occur are either changes in the mean level, variance, or changes in spectral characteristics. In this framework, the problem of segmentation between “homogeneous” parts of the data (or detection of changes in the data) arises more or less explicitly.In our opinion, a coherent methodology is now available to the designer, together with the corresponding set of tools, which enables him to solve a large variety of change detection problems in dynamical systems. It is interesting to note that the theory has been used in many successful applications [1,2]. Also, many books, journals and conference publications are concerned with these applications. Among them can be mentioned applications in mechanical engineering [2–4], industrial process monitoring [5–7], civil infrastructure [8–10], medical diagnosis and treatment [11–13], speech segmentation [14,15], underwater sensing [16,17], video surveillance [18,19], and driver assistance systems [20–22].The detection of events in seismic signals has been a subject of great interest during the last thirty years. Most of the methods in this area have been based on detecting special patterns or clusters in seismic data [23–28]. Other approaches make use of AR and ARMA models, used in conjunction with the Akaike information criterion (AIC) method for change detection and isolation, as well as to detect the primary (P-waves) and secondary (S-waves) waves [8,29–34]. These techniques currently employed for event detection in seismic waves use single- or three-component recordings.Another class of methods makes use of time–frequency analysis. Many earthquake engineering processes are characterized by non-stationary and nonlinear features that are often obscured in the traditional Fourier-based analysis schemes. As these representations provide an averaged-sense of frequency content, they do not distinguish noteworthily certain frequency components that are of short duration and high frequency as well as those arising from persistent, low-amplitude features. The ability to differentiate these contributions is critical in earthquake engineering. The time–frequency methods are capable of resolving energy content in such signals with both frequency and time. As a consequence, it was required to develop and use some non-stationary spectral analysis techniques. In this sense, significant efforts have been made in order to represent the temporal evolution of non-stationary spectral characteristics, assured by time–frequency analysis [35–37] among others. A new method based on a time–frequency analysis through the Wigner Distribution (WD) is presented and applied in [38] and [39]. The method consists on defining an appropriate entropic measure through a suitable time–frequency distribution, acting as probability distribution function. The information entailed by WD is explored by means of Rényi entropy. The method is based on identification of the events as those temporal clusters having the highest amount of information (entropy). In [40], the seismic signal analysis is performed by Smoothed Pseudo Wigner–Ville Distribution (SPWVD) in order to obtain instantaneous frequency (IF) information. Based on the time–frequency behavior, a pattern to characterize the seismic signal is estimated. In the same time it is analyzed the energy signal envelope, which is the derivative of the filtered cumulative energy, serving to estimate the different transitions along the seismic signal. Other applications of time–frequency analysis in earthquake engineering are presented in [41,42] and [43], among others.The outline of this paper is as follows. In Section 2, it is presented the segmentation problem formulation from a statistical perspective. In Section 3 the conceptual description of the statistical criteria for segmentation: the Maximum Likelihood (ML) and Maximum A posteriori Probability (MAP) estimate, some interpretations and connections, as well as computational aspects in this field are discussed. In Section 4, some experimental results obtained by Monte Carlo simulations for signal segmentation using different signal models, referred in literature, are presented and the effectiveness of the proposed approach is proved. Section 5 presents some comparisons of MAP approach with other change detection and segmentation methods, in the same simulation framework. Finally, Section 6 presents the experimental results obtained in segmentation of the two-component recordings (NS and WE) of the Kocaeli, Turkey, August 1999, earthquake, Arcelik station (ARC). The efficiency of the segmentation approach, in this case, is proved by time–frequency analysis, when segmentation results are evaluated using the reduced interference distribution (RID).We consider the following linear regression model with piecewise constant parameters,(1)yt=ϕtTθt+et,E(et2)=Rt,as a good description of the observed signalyt. Hereθtis a d-dimensional parameter vector,ϕtis the regressor, and the measurement vector is assumed to have dimension p. The noiseetis assumed to be Gaussian with a known time-varying noise varianceRt, for generality.The task of determiningθtfromytis referred as estimation, and change detection is the task of finding abrupt, or rapid, changes inθt, which is assumed to start at time k, referred as the change time. The basic assumptions on model (1) in change detection are the following:•The componentθtundergoes an abrupt change at timet=k. Once this change has been detected, the procedure will start all over again to detect the next change. The alternative is to considerθtas piecewise constant and focus on a sequence of change timesk1,k2,…,kn. This sequence is denotedkn, where bothkiand n are free parameters. The segmentation problem is to find both the number and locations of change times inkn.In statistical approaches, the noise will be assumed to be white and Gaussianet∈N(0,Rt).We introduce now the general segmentation problem for linear regression model with piecewise constant parameters. As we mentioned above, in segmentation the goal is to find a sequence of time indiceskn=k1,k2,…,kn, where both the number n and the locationskiare unknown, such that a linear regression model with piecewise constant parameters,(2)yt=ϕtTθ(i)+et,E(et2)=λ(i)Rtwhenki−1<t⩽kiis a good description of the observed signalyt. Hereθ(i)is the d-dimensional parameter vector in segment i,ϕtis the regressor andkidenotes the change times. The noiseetis assumed to be Gaussian with varianceλ(i)Rt, whereλ(i)is a possibly segment dependent scaling of the noise andRtis the nominal covariance matrix of the noise; the model (2) represents an extension of model (1). We can think of λ either as a scaling of the noise variance or variance itself (Rt=1). Neitherθ(i)orλ(i)are known. The Gaussian assumption on the noise is a standard one, partly because it gives analytical expressions and partly because it has proven to work well in practice. We will assumeRtto be known and the scaling as a possibly unknown parameter. The model (2) is referred to as changing regression, because it changes between regression models. Its important feature is that the jumps divide the measurements into a number of independent segments, since the parameter vectors in different segments are independent. Some important cases of the model (2) are the changing mean model, the autoregressive (AR) model, the autoregressive model with exogenous variable (ARX) and finite impulse response (FIR) model, etc., whereϕthas different expressions.The assumption on the regression models in (2) is not too restrictive since many stationary processes encountered in practice can be closely approximated by such models. The identification and parameters estimation methods represent only tools to perform change detection and segmentation. Good and precise models offers high performance in these schemes, but also biased parametric models can be used for change detection and segmentation. This bias decreases, but does not annihilate the performance of the detection and segmentation procedures.One way to guarantee that the best possible solution is found, is to consider all possible segmentationkn, estimate one linear regression model in each segment, and then choose the particularknthat minimizes an optimality criteria:(3)knˆ=argminn⩾1,0<k1<⋯<kn=NV(kn).For the measurements in the ith segment, that isyki−1+1,…,yki=yki−1+1ki, the least square estimate and its covariance matrix are denoted:(4)θˆ(i)=P(i)∑t=ki−1+1kiϕtRt−1yt,(5)P(i)=(∑t=ki−1+1kiϕtRt−1ϕtT)−1.The following quantities, V – the sum of squared residuals, D –−logdetof the covariance matrix P and N – the number of data in each segment, are given by(6)V(i)=∑t=ki−1+1ki(yt−ϕtTθˆ(i))TRt−1(yt−ϕtTθˆ(i)),(7)D(i)=−logdetP(i),(8)N(i)=ki−ki−1and represent sufficient statistics in each segment. The data and quantities used in segmentation procedure are shown in Table 1.Note that the segmentationknhasn−1degrees of freedom. Two types of optimality criteria have been mainly proposed in this field:•Statistical criteria: the Maximum Likelihood (ML) or Maximum A posteriori Probability estimate (MAP) ofkn.Information based criteria: the information of data in each segment isV(i)(the sum of squared residuals) and the total information is the sum of these. Since the total information is minimized for the degenerated solutionkn=1,2,…,N, givingV(i)=0, a penalty term is needed. Penalty terms occurring in model order selection problem can be used in this case, like Akaikeʼs AIC [44], or the asymptotic equivalent criteria: Akaikeʼs BIC [45], Rissanenʼs Maximum Description Length (MDL) approach [46], and Schwartz criterion [47].In the following section we give the conceptual description of the statistical criteria for segmentation: the Maximum Likelihood (ML) and Maximum A posteriori Probability (MAP) estimate and present some computational aspects. Maximum-likelihood (ML) is widely adopted in parameter estimation in different application fields. The maximum a posteriori (MAP) estimation is more sophisticated but less used approach. As it is known, while ML adopts a Fisherian approach, i.e., only experimental measurements are supplied to the estimator, MAP estimation is a Bayesian approach, i.e., a priori available statistical information on the unknown parameters is also exploited for their estimation. The last approach will be investigated in simulation and in a case study, having as subject seismic signals segmentation.In the case of ML, the sequence of change times sequencekn=k1,k2,…,knis estimated from the data sequenceyt. Using the likelihood for data, results the vector of change pointsp(yt|kn). The data and probabilities in ML change time sequence estimation are given in Table 1.Using independence of θ in different data segments gives(9)p(yt|kn)={p(yt),ifn=0,p(y1k1)∏i=1n−1p(yki+1ki+1)p(ykn+1t),ifn>0and the maximum likelihood estimate is:(10)(n,knˆ)=argmax(n,kn)p(yt|kn).It can be noted that:•The advantage of dealing with multiple change times is that it provides an elegant approach to the start-up problem when change is detected. A consequence of this is that very short segments can be found.The disadvantage is that the number of likelihoods (9) increases exponentially with time as2t.In the Bayesian case the change time has to be interpreted as a random variable, and one idea is to assign a probability q at each time instant, and to assume independence:P(change at timei)=q,0<q<1.The a posteriori probability for k is defined byp(k|yt). Bayesʼ rule thus gives(11)p(kn|yt)=p(kn)p(yt)p(yt|kn),where the denominatorp(yt)is a scaling factor independent of the change times, uninteresting for our purposes.The last term in (11) is recognized as the likelihood (9), so the difference to the ML estimate ofknstems from the last factor (the prior forkn). The maximizing argument is called the maximum a posteriori (MAP) estimate, which is not influenced by the scaling factorp(yt),(12)(n,knˆ)=argmax(n,kn)p(yt|kn)p(kn)=argmax(n,kn)p(yt|kn)qn(1−q)t−n.One advantage of the MAP estimator might be that we get a tuning knob to control the number of estimated change points. Note that withq=0.5the MAP and ML estimates coincide.Letkn,θn,λndenote the sets of jump times, parameter vectors and noise scalings, respectively, needed in the segmentation model (2). The likelihood for datayN, given all parameters, is denotedp(yN|kn,θn,λn). Assuming independent Gaussian noise distributions,(13)p(eN)=∏i=1n∏t=ki−1+1ki(2πλ(i))−p/2(detRt)−1/2exp(−etTRt−1et/(2λ(i)))it is obtained:(14)−2logp(yN|kn,θn,λn)=Nplog(2π)+∑t=1NlogdetRt+∑i=1nN(i)log(λ(i)p)+∑i=1n∑t=ki−1+1ki(yt−ϕtTθ(i))TRt−1(yt−ϕtTθ(i))λ(i).Here and in the sequel, p is the dimension of the measurement vectoryt. To eliminate the nuisance parametersθn,λnin [1] is used marginalized likelihood, and (14) is integrated with respect to a prior distribution of nuisance parameters, resulting:(15)p(yN|kn)=∫θn,λnp(yN|kn,θn,λn)p(θn|λn)p(λn)dθndλn.In (15), the prior forθ,p(θn|λn)is a function of the noise variance scaling λ, but is usually chosen as an independent function. The maximum likelihood estimator is given by maximization ofp(yN|kn). The a posteriori probabilities can be computed from Bayesʼ law,(16)p(kn|yN)=p(yN|kn)p(kn)p(yN),wherep(yN)is just a constant, and Maximum A posteriori Probability (MAP) estimate is given by maximization. In this way, a prior on the segmentation can be included.The priorp(kn)=p(kn|n)p(n)or, equivalently,p(δN)on the segmentation is a userʼs choice (the only one). The user can assume a fixed probability q of jump at each new time instant, that is to consider the jump sequenceδNas independent Bernoulli variablesδt∈Be(q), which means:δt={0with probability1−q,1with probabilityq.Since there is a one-to-one correspondence betweenknandδN, both priors are given by(17)p(kn)=p(δn)=qn(1−q)N−n.A q less than 0.5 causes a penalty term increasing linearly in n. As it was mentioned before, forq=0.5, the MAP estimator equals the Maximum Likelihood (ML) estimator. In some practical cases it is possible to have a priori information about the physical process to be segmented, and an appropriate value for q can be chosen. Such information can come from the correct scientific knowledge of the physical process or from previous empirical evidence.We present in the sequel the different steps in the MAP estimator, for the data and quantities given in Table 1:1.Examine every possible segmentation, parameterized in the number of jumps n and jump timeskn, separately.For each segmentation, compute the best models in each segment parameterized in the least square estimatesθˆ(i)and their covariance matricesP(i).Compute in each segment the sum of squared prediction errorsV(i)andD(i)=−logdetP(i).The MAP estimate of the model structure for the three different assumptions on noise scaling ((i) knownλ(i)=λ0, (ii) unknown but constantλ(i)=λand (iii) unknown and changingλ(i)) is given by the following equations [1]:The last two a posteriori probabilities (19) and (20) are only approximate; the exact expressions can be found in [1]. The evaluations involved statistics (6), (7) and (8). In all cases, constants in the a posteriori probabilities are omitted. The difference in the three approaches lies basically only in how to treat the sum of square prediction errors. A prior probability q causes a penalty term increasing linearly in n forq<0.5. The derivations of (18) to (20) are valid only if all terms are well-defined. The condition is thatP(i)has full rank for all i, and that the denominator underV(i)is positive. That is,Np−nd−4>0andN(i)p−d−4>0, in (19) and (20), respectively. The segments must therefore be forced to be long enough.The real challenge in segmentation is to cope with the problem of the dimensionality. It can be noted that the number of segmentationsknis2N, because it can be a change or no change at each time instant. Several strategies have been proposed for MAP segmentation, mainly, implementing numerical searches based on dynamic programming or MCMC (Markov Chain Monte Carlo) techniques, and recursive local search techniques. A review of MCMC and recursive local search techniques are given in [1].Computing the exact likelihood or information based estimate is computationally intractable because of the exponential complexity. Numerical approximations that have been suggested include dynamic programming [48], batch-wise processing where only a small number of jump times is considered [49], and MCMC methods [50]. MCMC algorithm is a general computing technique that simulates a Markov chain whose invariant states follow a given probability in a very high dimensional state space. The MCMC algorithm proposed by Fitzgerald et al. [51], for segmentation, is a combination of Gibbs sampling [52], and Metropolis algorithm [53]. It is based solely on the knowledge of the likelihood function for data given a certain segmentation. It can be noted that the Metropolis algorithm has been the most successful and influential of all members of the computational species that used to be called the “Monte Carlo method”. Today, topics related to this algorithm constitute an entire field of computational science supported by a deep theory and having applications ranging from physical simulations to the foundations of computational complexity.We present in the following some aspects concerning the computational complexity of the MCMC technique. An important problem in the implementation of MCMC algorithms is to determine the convergence time, or the number of iterations before the chain is close to stationarity. For many Markov chains used in practice this time is not known. There does not seem to be a general technique for upper bounding the convergence time that gives sufficient sharp bounds, useful in practice, in all cases of interest. Thus, practitioners like to carry out some form of statistical analysis in order to assess convergence. Sampling and computing expectations via these algorithms are approximate, because the chain is stopped after a finite number t of transitions. If it is fixed a measure of the error committed, it may consider the number of iterationst⁎needed to make this small. We are interested in the behavior oft⁎as a function of n, the number of variables. This is the computational complexity for approximate convergence. In [54] are stated conditions under which the complexity for approximate convergence is polynomial in n. Oftent⁎=O(nlogn)asn→∞, if the target field satisfies certain spatial mixing conditions. Otherwise, if the field has a potential with finite interaction range independent of n, the complexity is exponential innγ, withγ<1, which is still more favorable than enumerating all the states. When the interaction range grows with n, the algorithms can converge exponentially in n. Analogous results are also provided in [54], for an expectation approximated by an average along the chain. An overview of the literature concerning the computational complexity of the Metropolis–Hastings based MCMC methods for sampling probability measures in high dimensions is given in [55]. The computational complexity of MCMC-based estimators in large samples is discussed in [56], where the implications of the statistical large sample theory for the computational complexity of Bayesian and quasi-Bayesian estimation carried out using Metropolis random walks are examined.The progress in MCMC has been impressive and seems to be accelerating. Problems that appeared impossible have been solved. For combinatorial counting problems, recent advances have been remarkable. Despite of this fact many problems in this field are still open, and a solid theory for these approaches is still almost nonexistent [57].The MAP segmentation algorithm uses as input, the data vector, the model structure (mean model, regression model, AR(na) model, ARX(na, nb, nk), etc.), treating mode of the measurement covariance (MAP estimate with known noise scaling λ, MAP estimate with unknown noise scaling, use of different penalty term for complexity: AIC, BIC/MDL, etc.), the probability that the system jumps at each sample, q, and some design parameter for the search scheme: the number of filters used (M), the minimum lifelength of a jump sequence guaranteed (ll) and the minimum allowed segment size (mseg). In practice, if we have no other information, it is recommended to use the following values: the number of filter, M, is recommended to be chosendim(θ)+8, the choice of minimum lifelength, ll, is related to the identifiability of the model, and should be chosen larger thandim(θ)+2, and the minimum allowed segment size, mseg, can be chosen 0, when a change or no change can be produced at each time instant. Finally, the jump probability, q, is used to tune the number of segments. As we mentioned in Section 3.2, based on some information from the correct scientific knowledge of the physical process or from previous empirical evidence, an appropriate value for q can be chosen.We illustrate the MAP procedure by applying it to a number of segmentation problems referred in literature. The results are obtained by Monte Carlo simulation for different signal models, including models with changes in the mean, in FIR, AR and ARX model parameters. We briefly discuss the obtained results, when the MAP estimator with unknown and constant noise scaling and MCMC numerical search procedure were used. A number of different values on the jump probability, q, were examined, to prove the robustness to this parameter of the MAP approach. These signal models are used in many case studies for performance evaluation of change detection and segmentation algorithms, making the object of investigation in [1,2] and [58] among others, where the change instants or true parameters are known.Experiment 1Segmentation in the mean modelSuch model assumes a piecewise constant mean model, having the following structure:(21)yt=θ(i)+et,whereetis a random sequence of zero mean and varianceE(et2)=λ(i)σt2, withσt2=1, for segment i. The changes were produced at the instants 100 and 300, for the parameterθ(i)andλ(i).The Monte Carlo simulation consisted in simulation of the model (21), with the model parameterθ(i)andλ(i)values given in Table 2, for 500 realizations of the random sequence,et, and in applying of the segmentation algorithm. The signal and real change times, as well as the real and averaged values of the parameter and noise variance estimates are presented in Fig. 1. The histograms of the detected change instants resulted are given in Fig. 2. The results are given for MAP with unknown and constant noise scaling, and MCMC algorithm with a value of jump probability,q=0.3,M=8,ll=4andmseg=0.We give in Table 3the number of the change instants detected with 0 delay, corresponding to 100 and 300 instants, for different values of the jump probability q.Experiment 2Segmentation in FIR model parametersThe following model structure was used:(22)yt=θ1(i)⁎ut−1+θ2(i)⁎ut−2+et,whereutandetare random sequences of zero mean and variance 1 andE(et2)=λ(i)σt2, withσt2=1, respectively, for segment i. The model parametersθ1(i),θ2(i)andλ(i)values are given in Table 4. The changes produced, for the parameterθ1(the parameterθ2is kept constant during the experiment), at the instants 300 and 700.Monte Carlo simulation experiment was performed in the same conditions as in the previous example, with the model parameters and λ values given in Table 4. The results are presented in a similar way, in Fig. 3, the signal with real change times, as well as the real and averaged estimated model parameters and λ values, and in Fig. 4the histograms of the detected change instants. As in the previous case, the results are given for MAP with unknown and constant noise scaling, and MCMC algorithm with a value of jump probability,q=0.3,M=10,ll=6andmseg=0.Similarly to the previous experiment, we give in Table 5the number of the change instants detected with 0 delay, corresponding to 300 and 700 instants, for different values of the jump probability q.Experiment 3Segmentation in AR model parametersIn this case the following model structure was used:(23)yt=−ϕ1(i)⁎yt−1+ϕ2(i)⁎yt−2+et,whereetis a random sequence of zero mean and varianceE(et2)=λ(i)σt2, withσt2=1, for segment i. The model parameters and λ values are given in Table 6. The changes were produced, for the parameterϕ1(the parameterϕ2is kept constant during the experiment), at the instants 300 and 700.The experimental results, for MAP with unknown and constant noise scaling, and MCMC algorithm with a value of jump probability,q=0.3,M=10,ll=6andmseg=0, are given in Fig. 5and in Fig. 6, under the form of the signal with real change times, real and averaged values for the parameter and noise variance estimates, λ, and of the histograms of the detected change instants, respectively.Table 7gives the number of the change instants detected with 0 delay, corresponding to 300 and 700 instants, for different values of the jump probability q.Experiment 4Segmentation in ARX model parametersIt was used the following model structure:(24)yt=−ϕ1(i)⁎yt−1−ϕ2(i)⁎yt−2+θ1(i)⁎ut−1+θ2(i)⁎ut−2+etwhereutandetare random sequences of zero mean and variance 1 andE(et2)=λ(i)σt2, withσt2=1, respectively, for segment i. The model parameters and λ values are given in Table 8. The changes were produced, for the parameters ϕ, θ, and for λ, at the instants 400 and 700.We present in Fig. 7and Fig. 8the Monte Carlo simulation results in similar forms, like in the previous examples, for MAP with unknown and constant noise scaling, and MCMC algorithm with a value of jump probability,q=0.3,M=12,ll=8andmseg=0.We give in Table 9the number of the change instants detected with 0 delay, corresponding to 400 and 700 instants, for different values of the jump probability q.All the presented examples point out that the change instants were detected at the real change instants, with minimization of the false changes rate and of the delay for detection. Also the model parameter and noise variance estimates were very close to the true values. It can be noted that a reasonable performance is obtained for almost any choice of the q design parameter, proving the robustness of the MAP approach to this design parameter.Several methods for model segmentation have been suggested earlier, see e.g. [1,2,58], among others. They typically employ multiple detection algorithms [59], hidden Markov models, [60], explicit management of multiple model, AFMM (adaptive forgetting by multiple models) [61], or formulate the segmentation problem as a least-squares problem with sum-of-norm regularization over the state parameter jumps, a generalization ofl1-regularization [58].We present in the following some simulation results obtained by Monte Carlo simulation for different signal models including only models with changes in the mean, and ARX model parameters, when the following techniques, presented in [1] and [2] have been investigated: filtering techniques with a whiteness test, using the Cumulative Sum (CUSUM), and Geometric Moving Average (GMA), techniques based on sliding windows and distance measures, using Generalized Likelihood Ratio (GLR) and Divergence Test (DIV), and MAP estimator with unknown and constant noise scaling, when MCMC numerical search procedure has been used.Experiment 5Change detection in the mean of a signalThe results are obtained by Monte Carlo simulation, for 1000 noise realizations, in the case of a change in a model of the form (21), whereetis a random sequence of zero mean and varianceE(et2)=λσt2, withσt2=1, for each experiment. The model parameters are given in Table 10. The change detection results for different level of the noise, λ, are given in Table 11for filtering approach, in Table 12for sliding windows approach and in Table 13, for MAP approach with unknown and constant noise scaling and MCMC algorithm (with a value of jump probability,q=0.3,M=8,ll=4andmseg=0), respectively; the results represent the number of the real change instants detected (0 delay in detection).Experiment 6Change detection in parameters of an ARX modelThe ARX model used is given in (24), whereetis a random sequence of zero mean and varianceE(et2)=λσt2, withσt2=1, for each experiment. The model parameters are given in Table 14; the inpututwas a random signal of mean zero and variance 1.As in the previous case, the experiments were performed, for constant value of λ in each experiment. The change detection results for different level of the noise, λ, are given in Table 15, for filtering approach, in Table 16for sliding windows approach and in Table 17, for MAP approach with unknown and constant noise scaling and MCMC algorithm (with a value of jump probability,q=0.3,M=12,ll=8andmseg=0), respectively; the results are given under the form of the number of the real change instants detected (0 delay in detection).Experiment 7Robustness evaluationThe robustness of the algorithms to the model structure, has been tested, also by Monte Carlo simulation, for an underestimated model order ARX (1 1 1) and for an overestimated model orderARX(3,3,1). It resulted that the performances are affected, when the model order is underestimated, for all methods, especially when the noise level increases. If the model order is overestimated, the performances are not affected to a great extend for MAP segmentation procedure. The results are given in Table 18, Table 19and Table 20, for filtering, sliding windows and MAP approach with unknown and constant noise scaling and MCMC algorithm (q=0.3,M=14,ll=10andmseg=0), respectively. As in the previous cases, the results are given under the form of the number of the real change instants detected (0 delay in detection). So, when no information on the model order is available, the best solution is to use an high-order model to perform signal segmentation.Based on the obtained results it can be noted that the performances of the MAP segmentation approach are superior to the other approaches investigated, but with the price of the computation effort. The last aspect does not rise problems in practice, due the fact that the MAP procedure is applied off-line to data. The performances of the filtering and sliding windows approaches depend to a great extend on choosing the design parameters (see [3]), used in the stopping rule: the drift parameter ν, influencing the low-pass effect of the filter, and the threshold h influencing (also with ν) the performance of the detector. Also, a forgetting factor λ is used to tune the low-pass effect; in practice, choosing these parameters could rise problems. In the case of MAP algorithm the single design parameter that could create problems in practice is q, the probability that the system jumps at each sample, usuallyq=0.5, when MAP and ML estimates coincide. The experiments 1, 2, 3 and 4 proved the MAP robustness to this design parameter. Other design parameters (see Section 3.3) could be chosen without any problems. All the algorithms need the model structure, usually overestimated.Seismic signal segmentation constitutes a very interesting and challenging task. The main difficulty is attributed to the fact that both the statistical properties of seismic noise, as well as the characteristics of the recorded events, are generally unknown.Several methods for seismic signal segmentation have been suggested earlier. Some of these methods typically employ either an autoregressive modeling of the data and a generalized likelihood ratio test to detect the significant statistical changes in the waveform [62,63], a best-basis searching algorithm [64], based on binary segmentation constructed by Coifman and Wickerhauser [65], or by exploiting the particular nature of the signals, and by using some interesting properties that obeys difference based test statistic as well as its ingredients [66].The seismic signal segmentation in the present paper was performed with the MAP estimator with unknown and constant noise scaling, and MCMC numerical search procedure. The results have been evaluated using the time–frequency analysis, by reduced interference distribution (RID), for horizontal seismic components: NS and WE. The vertical component of the seismic motion, VE, does not offer new elements from the segmentation point of view. The efficient time–frequency concentration measurement, can provide a qualitative criteria to confirm the effectiveness of the segmentation approach, for the first time applied in seismic signal analysis.The earthquake accelerations used in this case study were recorded during the Kocaeli seism, Turkey, August 17, 1999, Arcelik station (ARC), a strong to moderate ground motion. The map with the heavily damaged regions, primarily the provinces of Kocaeli (Iznit) and Sakarya (Adapazari), is given in Fig. 9(see [67]). The data were sampled with a sampling period of 0.005 seconds, for around 30 seconds, and were previously corrected to remove the measurement noise effects. The source of data is http://peer.berkeley.edu/svbin/GeneralSearch?geo1=B. The data are given under the form of acceleration, velocity and displacement, including some characteristics, such as HP, LP, PGA, PGV, PGD and spectra 0.5–20% dumping; source record is processed by Pacific Engineering.The components of the same seismic signal make the object of the investigation in [68], where a signal analysis technique based on harmonic wavelet analysis was used to look at the earthquake induced accelerations. The analysis was performed by a group of academics and professionals, members of Earthquake Engineering Field Investigation Team (EEFIT), under the auspices of Institution of Structural Engineers, UK. The aim was to quantify the effects, identify the reasons for poor performance of structures and suggest improvements where possible. This seismic motion has constitute the subject of investigation in many papers which offer elements on how the signals were acquired and preprocessed, as well as on the main spectral components, and other specific characteristics, see e.g. [67–69], among others.The present analysis will be useful in interpretation of the results obtained in seismic data segmentation and in their validation by time–frequency analysis. We present in Fig. 10the NS seismic component and its Fourier amplitude spectrum. In the frequency domain representation of the NS component, at least six frequency components, located at 0.6, 2, 2.6, 2.9, 3.4 and 3.8 Hz are clearly seen. These frequencies were labeled with NS1, NS2, NS3, NS4, NS5 and NS6, respectively. Of these frequency components, the dominant is located at around 2.6 Hz. Beyond 4 Hz the spectral amplitudes are small. For the same component the time domain representation indicates that the maximum amplitude is located at around 12 s. The frequency appears to change from high to low and then slightly high frequencies are observed in the time interval 8 to 14 s. A low-frequency component may be observed all along the record. However, in certain small time window interval a high-frequency component is also evident.The same type of analysis has been performed for the WE seismic component. We present in Fig. 11the seismic component making the object of investigation and its Fourier amplitude spectrum.It can be noted a similar behavior of this component with the NS seismic component. Briefly, we present the result analysis. In this case, also, at least six frequency components, located at 0.6, 1.4, 1.9, 2.7, 3 and 3.5 Hz are clearly seen. These frequencies were labeled with WE1, WE2, WE3, WE4, WE5 and WE6, respectively. Of these frequency components, the dominant is located at around 1.9 Hz. Beyond 5 Hz the spectral amplitudes are small. The time domain representation indicates that the maximum amplitude is located at around 9 s. Like in the previous case, the frequency appears to change from high to low and then slightly high frequencies are observed in the time interval 8 to 14 s. Also, a low-frequency component may be observed all along the record. However, in certain small time window interval a high-frequency component is also evident.Visual inspection for both signals, in the preliminary analysis, shows that the onset time is clearly visible as a change in energy and frequency content, which is a suitable problem for an AR model. Our experience is that, for this problem, as for many other signal processing ones, the AR model leads to a satisfactory trade-off between complexity and efficiency of the corresponding algorithms for the off-line estimation of the change time. The segmentation procedure has been applied for an AR(1) model, model structure resulted after some experiments with AR models of different orders.(25)yt=−ϕ1(i)⁎yt−1+et,whereetis a random sequence of zero mean and varianceE(et2)=λ(i)σt2, withσt2=1, for segment i.The optimal segmentation using MAP with unknown and constant noise scaling, and MCMC procedure for different values of the jump probability, q, and the following design parameters in search scheme:M=9,ll=5andmseg=0, gives the following estimated change time sequences:knˆq=0.01=(1.80,8.35,9.47,12.28),knˆq=0.03=(1.80,8.35,9.48,12.13,12.36,14.89),knˆq=0.05=(0.73,8.35,9.47,12.13,12.36,14.89),knˆq=0.10=(0.73,8.35,9.47,12.11,12.36,14.89),knˆq=0.20=(0.73,8.35,9.48,12.12,12.36,14.89),knˆq=0.30=(0.73,8.35,9.39,12.11,12.36,14.89),knˆq=0.50=(0.73,8.35,9.40,12.10).The segmentation results, obtained for the jump probabilityq=0.3, are given in Fig. 12.The evaluation of the MAP segmentation results is performed with the time–frequency analysis, when the reduced interference distribution (RID) [70], has been computed (see Fig. 13). The reduced interference distribution (RID) belongs to Cohenʼs class, and is an extension of the Wigner–Wille distribution. The RID overcomes some problems in time–frequency analysis to a significant extent, even though some small synchronization and cross-terms may be present, and it appears to be a better choice for the seismic signal analysis [71].The RID was computed with a kernel based on the Hanning window, for the number of frequency bins, Nf=4096, identical with the time instants, time smoothing window, Lg=204, frequency smoothing window, Lh=512, and a threshold of 5%. In this figure at linear scale, at least five smooth well-defined frequency components can be observed. The strongest amplitude is located at 2.9 Hz and the component of large duration is located at 2.6 Hz. The large duration is in relative comparison with the frequency component located at 2.9 Hz.The results from Fig. 12 and Fig. 13 point out that MAP segmentation is according with the changes in energy and frequency content of the NS seismic component. The MAP segmentation forq=0.50, when MAP estimator equals ML estimator, does not determine all the changes in the signal dynamics.The segmentation procedure has been applied for the same model as in the previous case, AR(1). The optimal segmentation using MAP with unknown and constant noise scaling, and MCMC procedure for different values of the jump probability, q, and the same design parameters in search scheme, like in the previous case, gives the following results:knˆq=0.01=(0.58,8.06,11.93,12.52),knˆq=0.03=(0.58,8.06,11.93,12.52),knˆq=0.05=(0.58,7.65,11.93),knˆq=0.10=(0.58,7.65,8.48),knˆq=0.20=(0.58,7.65,8.48),knˆq=0.30=(0.58,7.65,8.42,11.93,12.52),knˆq=0.50=(0.58,7.65,8.42,11.98).The segmentation results obtained with MAP approach, for WE seismic component andq=0.30, are given in Fig. 14.The results for time–frequency analysis of the WE seismic component are presented in Fig. 15for RID, with a Hanning window and the same parameters like for NS seismic component. We remember that the frequencies of interest are located at 0.6, 1.4, 1.9, 2.5, 3 and 3.5 Hz.Like in the previous case, it can be noted that the segmentation results, forq=0.30, are in accordance with the changes in energy and frequency content, resulted by time frequency analysis. Also, forq=0.50, when MAP estimator equals ML estimator, not all changes in the signal dynamics are determined.The segmentation results, obtained in both cases and the comparison with the reduces interference distribution (RID), point out the efficiency of the MAP segmentation approach, even in the case of a reduced order model, despite of the fact that the seismic signal dynamics is more complex.For the real signals is difficult to establish if the change detection and segmentation results are correct, especially in case the change instants or true parameters of the signals are unknown. The time–frequency approach, a non-parametric modeling technique, offers correct information on signalʼs energy and frequency content evolution in time, and it is easy to evaluate on this basis, if the change detection instants resulted with MAP segmentation procedure are correct. So, it is possible, from the time–frequency analysis to determine by visual inspection the possible instants where the energy and frequency content of the signal change, to associate these instants with the change instants resulted with MAP and to decide if the segmentation results are plausible. The use of other techniques, based on parametric models and statistic tests, could be less suitable, for the signals with a priori unknown change instants, to evaluate the performances of a method of the same type.The analysis results in both cases confirm the efficiency of the segmentation approach used. The change instants resulted by MAP approach appear clear as rupture points in energy and frequency contents of the seismic signal. The proposed approach can be used for the onset time estimation, a typical off-line estimation problem, after P-wave and S-wave detection in a seismic motion, see [63].

@&#CONCLUSIONS@&#
The paper gives the conceptual description of a segmentation method, based on maximum a posteriori probability (MAP) estimator for optimal segmentation, with application in seismic signal processing; some interpretations and connections with other approaches in change detection and segmentation, as well as computational aspects in this field are discussed. The efficiency of the segmentation method is proved by Monte Carlo simulation for different signal models, including models with changes in the mean, in FIR, AR and ARX model parameters, as well as by comparisons with other methods for change detection and segmentation. Finally, an application of optimal segmentation in the earthquake records during the Kocaeli seism, Turkey, August 1999, Arcelik station (ARC) a strong to moderate ground motion is presented. The results are compared with time–frequency analysis, for the reduced interference distribution (RID), as non-parametric technique, offering information on the evolution in time of energy and frequency content of the seismic motion.