@&#MAIN-TITLE@&#
Toward robust and quantifiable automated IFC quality validation

@&#HIGHLIGHTS@&#
We identify quantifiable rules for testing a good quality IFC file.We define classifications of the validation rules including syntax and semantics.We outline initial list of validation rules covering all aspects of IFC testing.The validation rules include well defined rules for geometry and spatial related tests.We propose rules for automated IFC import testing.

@&#KEYPHRASES@&#
IFC,Quality,Automatic validation,Export test,Import test,

@&#ABSTRACT@&#
The use of IFC as a standard format in exchange processes has been increasing as the industry begins to address the need of interoperability. A current problem with the use of IFC is in the quality of product models. Much effort has addressed this issue in the form of certifications to ensure a minimum quality of exchange requirements for an IFC file. However, even with the recent increasing awareness and effort to improve the quality of IFC files, the process is still too tedious, time consuming and requires manual efforts by experts. Even if those resources are available, there is currently no clear and quantifiable definitions of what exactly is a good quality IFC file. Without such measures, the adoption of IFC as standard exchange format will be hindered and the industry will be left with only restricted alternatives that are mostly vendor dependent. This paper sets out to address this issue in two aspects: first by defining what a good quality IFC model is, and second by proposing rules that can be automated to measure with confidence the completeness and correctness of the IFC model. These two aspects will serve as a starting point toward a more comprehensive and quantifiable measures of the quality of an IFC file. This proposed goal is represented by using well defined and well documented rules collected from various projects the authors had experienced over the years. The rules include all known aspects of IFC, including geometry, which currently requires mostly manual validation. The paper also proposes a method to address import validation that is under-developed compared to the export validation.

@&#INTRODUCTION@&#
Quality of an IFC file has been a persistent concern in the architecture, engineering, construction, and owner-operated (AECO) community. We have only begun to address this issue and start to develop a better way to assess the quality of an IFC file. The concerns on quality range from validating that an IFC file is syntactically correct to evaluating whether a model is well-formed in terms of being testable, to more versatile types of checks: for satisfying programmatic requirements or building code checks. Current uncertainty on the quality of IFC models has prevented or at least has put a psychological barrier to wholehearted adoption of BIM and IFC among end users. This calls for an urgent need to define robust and rigorous test criteria, processes and tools. However, defining and validating data quality is not easy since it should take into account multiple characteristics of data, including syntactic well-formedness, consistency across multiple redundant representations, integrity of translation from the sources, the accuracy of derived data, and others. Reliable and automated means to assess all of these aspects are important for all organizations that need to rely on model data. Model data in building and construction involves structuring of data that includes systems (structural, electrical, piping and others), geometry, personnel, process and schedules; each of these have their own well-formedness conditions. This paper aims to lay the groundwork toward achieving well defined, unambiguous, and precise criteria of IFC quality testing. It starts with defining the most fundamental requirements of what a good IFC model is. The authors present this work to serve as a basis to start focused discussions and further research to incrementally build increasingly complete and robust testing criteria. We also consider tools facilitating robust and trustworthy IFC model test facilities that can be readily accessible to the general public.The topic of the quality of an IFC model in the interoperability scenario has been discussed for many years. Laakso and Kiviniemi [1] provided a well-ordered historical overview of the development of IFC as a standard and its various issues. Amor [2] discussed the issue and compared it with similar issues in other industries, namely healthcare and STEP-based manufacturing. He proposed a combination of processes, best practices and tools to accomplish what other industries have achieved to manage the challenges of interoperability. Amor’s paper for the first time explicitly proposed tools that are able to perform geometry based comparison used in STEP-based manufacturing domain. Similarly, Lipman et al. [3] discussed in more detail the same issues and highlighted examples from various IFC conformance testing and made comparisons with similar conformance testing in other domains such as ISO AP 227 (Plant Spatial Configuration) from the Plant domain. Kiviniemi [4] and a report from IAI Denmark [5] highlighted issues with the IFC certification process in the past and the need to define a more robust certification regime that is understandable by end users and more predictable in the real world by including more realistic test cases.To address the model exchange use cases using IFC, it is important that we look at the typical or expected user workflow. Fig. 1shows simplified diagram of the typical model exchange workflow that shows three general steps that may contribute to the quality of the model. Unfortunately, most of the steps are simply black boxes to the users (Fig. 2). In most cases, recipient will see errors that may come from any of the previous steps almost without any control to influence the outcome. Even for the software developers, it is often that the burden falls to the importing applications since errors are mostly reported at the recipient’s end. While software developers have access to one of the black boxes, e.g. the importing application, tremendous efforts and time are usually required to identify the real source of errors. Certification process in many ways reduce a number of issues and greatly improve the quality of the exchanged models. However, there are several issues that are not addressed well with the certification:1.Certification is still relatively closed loop process. It provides a higher level confidence on the exchange model, but it does not provide consistent guarantee as it can only cover limited exchange cases of the allowed combinatorial set used within the certification process. It still tilts towards certifying the capability of the software application to export or import IFC files.The criteria for quality assessment within the certification are still not fully well-defined. The current criteria vary from arbitrarily set by the certifying parties or are collected as part of experience accumulated from the previous certification activities. There has not been enough systematic/theoretical support for more rigorous test criteria.Not all errors are equal in term of their impact downstream to the receiving application within the exchange workflow. Many criteria defined as part of domain specific MVDs (Model View Definition) can be treated using statistical significance. For example, in the domain of rule checking there are many criteria that require combinations of multiple properties from the model to derive a new property. One such example is deriving the envelope of a building using object geometries, property that specifies a building element faces external space, type of the building element. When the information is incomplete or partially complete, the impact may not be as severe since it may just disallow such check to be complete or give meaningful report, but not a complete failure or worst still give a wrong result.Current certification still rely on a few experts to manually evaluate the model, especially for geometry related tests that are difficult to automate. This significantly reduces the comfort of the end users who work with IFC based exchanged as they lose control of the process and the quality of the exchange model without a good support to fix any issue when it is detected often far downstream in the workflow.Figs. 3–6show several examples from the actual reports of the model exchange issues between various BIM tools and certifications. For each cases, tremendous amount of efforts and expertise had to be invested to identify issues and to fix them. All of them involve IFC certified applications.In this paper, we set to address core requirements for a robust exchange workflow. In defining the core requirements we focus on preservation of the model, i.e. the model must be identical or maybe identically mapped using different representations. In both case, measured from the end user perspective, the model is preserved. We exclude most of domain specific MVD requirements as they have fuzzier criteria where ranking or statistically significant measurement may be applied. In the simple term, we simply want to define a simple measure to give confidence to the users that the model exported and then imported are the “same”. The aim is to provide an open and well-defined/well-accepted criteria for defining a robust exchange measurement and to encourage an open source development of a tool that checks the criteria, effectively removing the issue of closed loop-ness of the current certification process. We think that it is critical since availability of such tool will ensure that the quality validation does not only occur within the certification process, but it is accessible to the end users for every exchange. This increases confidence for the robustness of model exchange and minimizes the reliance on the expert to analyze errors or potential errors.We start the paper by reviewing the evolution of IFC testing methodologies, followed by defining the quality dimensions for the IFC exchange scenarios, and defining the classification system as a way to standardize the way we catalog the criteria that allows extension over time. We close the paper with overview of additional criteria critical for robust measurement of the IFC exchange that require additional information and steps external to the IFC file.Prior to the Coordination View 2.0 certification, IFC model testing had been done mainly through manual and visual inspection. Implementers contributed to the development of test cases, exchanged the test cases with each other and manually and visually inspected the results. Certification was awarded usually at the certification workshop when pairwise inspections were done between two applications. The process was qualitative with emphasis on the appearance of correctness from the results of the export and import. Kiviniemi [4] highlighted the weaknesses of this approach, i.e. human interpretation allows for human accommodation of “close enough” errors. The appearance of correctness was not necessarily a proof of actual correctness of the model data. Another issue was that the reporting varied by individuals. While the process improved over the time, indicated by the increase in the number of test cases: around 32 test cases for IFC2x certification, which increased to 264 test cases for IFC2x3 Coordination View 1.0, the certification still fell short from providing assurance to the customers that IFC was a robust way for exchanging building model [4].Beginning with IFC2x3 Coordination View 2.0 (CV 2.0) certification, a conscious effort has been made to address this lack of assurance. The new certification process put more emphasis on the quality of IFC produced by a specific application. It follows a similar practice of the Plant domain with STEP AP 227 [3]. As part of the certification requirements, the certification committee documented the complete Model View Definition (MVD) and made it available in the BuildingSMART Tech website [6]. The MVD is used as a reference to define test cases with precise exchange requirements and instructions for various objects that software developers have to re-create in their native application and export the results as an IFC files. The files are subjected to self-service test through an online application GTDS [7], which checks the resulting IFC files based on 3 categories of tests: IFC schema syntax and where rules, rules from implementer’s agreement, and limited numbers of simple semantic checks based on the MVD. The rules are developed using specifications based on mvdXML [8]. However, the final step in the certification process still relies on human manual checks to verify the IFC file against the reference model, and to evaluate and approve exceptions, i.e. specific application’s lack of support for certain entities, relationships and properties in the reference model. In addition, geometry related verification is still done manually. The test regime and validation criteria are well developed for export cases. For import there is very minimum support for automated and rigorous checks currently due to the complexity of proprietary mapping and lack of consistent mapping in the implementations in the native authoring tools.Separately, similar processes using Exchange Requirements, MVDs and automated tests combined with manual tests, are being developed and used for the GSA Concept Design BIM 2010 (GSA CD BIM 2010) by Digital Alchemy. GSA CD BIM 2010 is an extension from the earlier validation process in 2007 with GSA Spatial Program Validation. The GSA CD BIM 2010 includes the original 2007 spatial program validation, energy performance analysis, circulation/security rules analysis in courthouse design, and quantity takeoff/cost estimating [9–11]. Another independent validation driven by the US Army Corps Engineers with COBie challenge concerns BIM for Facilities Management Handover. It follows similar process with a self-service test tool made available (COBie Toolkit) and workshops that are scheduled regularly twice a year on average [12].Even with the current development, the validation that the data in an IFC model is syntactically well-formed is still an open question because the conditions defining well-formedness or syntactic correctness are not well defined. A survey on research in this area results in few publications. Those available either fall into the same situations with lack of rigor in defining well-formedness, or just repeating what has been done in this area. The lack of definitions and rigorous criteria restrict testing to only small scale models. Unfortunately, small scale models often fail to represent the real-world situation with large and complex models and complex design requirements. Testing such models becomes impractical unless robust automation is applied.From the authors’ involvement in BIM model testing for programmatic requirements, standardizing the development of Model Views [13], and rule checking projects [14,15], there is an urgent need to have well-defined methods to assess the coverage and correctness of a building model based automated rule checking. This paper is an early effort to develop criteria that allow quantitative measurements to identify the level of adherence of an IFC file against a specific model view. Where the model view is defined at different levels of process: syntactic, explicit constraint (EXPRESS WHERE rules, explicit model view semantic restrictions, for example). This paper aims to provide a starting point toward that goal by outlining initial criteria meant to lead to comprehensive well-formedness and providing itemized test requirements that are developed based on schema conformance, existing testing or certification programs and comparisons from other domains [2,3]. The criteria are meant to cover complete aspects of IFC schema used currently including geometry related tests. Since the main focus of this paper is to identify standardized, measurable and quantifiable test types and test cases that assess them, it does not deal with the actual implementation and platform tools to implement the tests.Data quality has been a general topic in the Information System domain. Wand and Wang laid an ontological foundation to define the data quality dimensions [16]. In their paper, they focus on the intrinsic data quality and define four general data quality dimensions shown in Table 1.In this paper, only the intrinsic data quality is in the scope and it does not deal with the details of internal representation or the physical appearance of the data [16]. There are various other works that extend this framework for data quality that covers context specific categories and their respective dimensions. The nice thing about the [16] work is that they categorize and summarized many quality dimensions into just four. For example Strong, Lee and Wang define four categories: Intrinsic, Accessibility, Contextual and Representational [17]. There are several other works that address the data quality issues from the broader perspectives that include the consumers of the data. Batini et al. [18] and Scannapieco and Catarci [19] covers the overview of various approaches in their papers.In general, data quality is an open system. Quality needs to be defined relative to some explicit criteria. It is generally recognized that the specification of an exchange is defined by a model view [ref]. Thus exchange quality can be defined by two types of information: (1) identifying the issue(s) that motivate the rule and its public health or safety motivation; in terms of the knowledge regarding standards of representation for measurement of the rule, for example Concrete Slump Test – ASTM C143, that defines the procedures for carrying out the measurement [20,21]; and (2) as the degree of realization of an explicit subset of the absolute requirements. Thus the range of quality includes the accuracy of measurement (especially allowed deviations of geometric measurements, including level of detail and eventually surface accuracy where relevant.Relevant to our work to measure data quality of the IFC exchange, we take and adapt the framework of Wand and Wang [16] proposed for the intrinsic quality of the data and also part of the extension that includes the domain perspective or context specific from Strong et al. [17]. Since the IFC exchange focuses on a very specific area, we modify the data quality dimensions to suit our need. We cover two categories of data quality, i.e. the intrinsic quality of the IFC model and the domain context of representational quality. Table 2describes them in more details.Based on the above data quality dimensions, we adopt a simple ratio measure [22] to quantify the overall quality score or confidence of an IFC exchange model (Eq. (1)). This fits our purpose very well since the goal for an IFC data exchange is very high confidence and ideally the model should be 100%, i.e. a perfect translation, even though in practice a small tolerance may be accepted. It is because some applications may not be able to achieve 100% without major internal native model modifications. The near 100% requirement is especially true because building designs can influence the safety of both of the occupants and the workers who build or maintain it [23–25].(1)C(model)=∑(T(locallyvalid))∑(T(locallyvalid)+T(locallyinvalid))whereCis a function that reflects confidence of the quality of the model.Tis a function of all the relevant tests that discriminate model correctness from inaccuracy or errors in data integrity. Such tests employ a logical process instead of Boolean. The local tests may result in three outcomes: locally valid, locally invalid, missing data and not testable.In this paper, we outline our main contribution to define and categorize various rules to measure various data quality dimensions. The list of rules should be comprehensive enough from the start to ensure the acceptance of the measurement by all parties within the exchange process. In the next section we lay out the details of rules that will help us measure the IFC exchange model.Through this research work, we look into the issue of the quality of IFC file with the aim to provide a comprehensive first list of rules and provide a general classifications for the rules. With such classifications, we would share effort to cover wide range of validation rules needed today and to develop the longer term support needs to develop strong method for determining model quality. The needs are defined through collections of rules that need in the short term, prior to the research on algorithm assessment for quality, which are derived from well-known and well-defined rules, usually from the schema, from practical experience encountered throughout the years of involvements in BIM software development, the certification processes, and from the logical consequences of the domain specific concepts. We will treat export first and then on import in a dedicated section in the later part of this paper, since they involve quite distinct requirements. When dealing with export validation, the highest level question that needs to be answered is:Given a drawing or other representation of a building model, can the systems export the target model accurately representing the intent in IFC?Export cases are more direct compared to import since an export model has a publicly defined mapping representation. This allows a common set of tests to be applied across all application translators. Import model testing needs to recognize the varied native model structure each BIM supplication is made of. The need to access the native file that an IFC file is derived from, the import translation, must be required for tests to measure completeness.There are two types of broad groupings of rules: One that is solely applicable within the IFC file, i.e. all test rule works with information contained inside the IFC file (self-contained), and the other one is the group that requires extra information (external dependency). The self-contained tests are useful to validate consistency of data already exported from the originating application. However, they cannot capture errors that may occur during export translation by the authoring tool. Additional information to verify completeness where there is no “missing information in translation” or “error during transmission” would be needed. We will discuss this issue in more detail in upcoming section, “Tests with Additional External Information”. In particular, the first two subsections that deal with Export Summary, Geometry Mass Properties and Sampling Points.As the first step toward a rigorous validation of well-formedness of an IFC model, we collect various rules that need to be tested to ensure quality of IFC model. They are large number of rules known so far, both formally in form of schema validation such as those typically done by the IFC toolkits or a dedicated validation tool such as Expresso (http://sourceforge.net/projects/exp-engine/) and those already used in the certification processes, and those informally as practice in the QA process of IFC supporting applications and the visual check portion of the certification. For long term support, it is expedient for us to organize the rules into a well-defined classification with rule categories and sub-categories. This will help us organize and manage additional rules that will be added over time. In defining the categories and sub-categories, we considered a closed structure ontology, but realized that there is no definition of completeness of a classification system. Instead we assembled this list in a more functional listing and its associated data quality dimension. We define one main classification and three other minor classifications: source, subdomain, and the current degree of difficulty for implementation of the automatic rule checking. The guidelines for assigning the degree of difficulty is outlined in [26]. They are independent but complimentary allowing users to view the rules from various perspectives.To begin with, we consider the intrinsic quality of the IFC data by validating the correctness of the data by enforcing rules that are defined in the relevant version of the IFC schema [27]. This includes syntactic check, validation of entity instance types, valid enumeration values, and geometry and topology related rules. Most of the rules, except geometry and topology related have been either included by the IFC toolkit or by certification rule sets. We treat geometry to be an equally important quality that has to be integrated as part of the automatic rule validation. Geometry rules include correctness of the geometry entities and also informal propositions as specified in the schema that are derived from ISO-10303 part 42 [28]. Topology related rules include crucial tests according to Euler formula. It is an important criterion that will saves users from getting an unpredictable outcome. The Euler formula will give a quick indication whether a geometry is a valid manifold solid. These sets of rules are grouped into one class and named as schema data structure validations.Table 3lists more detailed functional classifications that measure correct data quality dimension. The heading of the classification for this dimension is called schema data structure validations (Category I, rows no. 1–11) that has subclasses that deal with syntactic check (rows no. 2–6), including general schema structure (row no. 3), schema WHERE rules (row no. 4), valid entity check (row no. 5), and valid enumeration check (row no. 6). The geometry tests (rows no. 7–9) includes informal proposition defined in the schema (row no. 8), and geometry integrity check based on the definition of various geometry in the schema (row no. 9). Besides the geometry, related tests on topology are also important (rows no. 10–11). There are a long list of rules that fall into the geometry and topology tests (see Table 6 under I.B. and I.C.). The topology here concerns the relationship at the level of data structure and geometry. For example relationship between face, edges and vertices. In Table 3, we give some examples of the rules whenever appropriate. It is not meant to be exhaustive but just to give an overview. Table 6 shows the complete listing of rules that we know to be relevant.Under this dimension, data quality is evaluated based on the expected “behavior” of the object under the definitions. What fall into this category are all those data that structurally are correct but do not make sense from the behavior point of view. For example, if a type with a specific name and attributes is defined, having two separate instances of types with two identical, or minor variation of the data are not meaningful (case I.D.4.3 in Table 6). Another example is a containment relationship. From containment structure point of view, as long as an object is contained in a container object, it is a valid structure, but viewed from a spatial perspective, it does not make sense if the object’s geometry is physically disjoint with the container (I.D.5.3 in Table 6). Many of the cases that fall into this category are related to topology. It is distinguish with the earlier dimension since the topology here refers to a topological relationship between entities and their spatial relationship.Conformance tests are validation steps beyond the schema that help determine whether an IFC instance file conforms to the specific exchange requirement. Table 3 Category II (row number 18–23) list down general test categories typically included in conformance tests. They are often called “business rules” in IFC literature. It is not the purpose of this paper to discuss in depth all applicable conformance requirements, but generally there are recurring themes that the conformance tests usually perform.Table 7 of the appendix lists the general themes of conformance testing. Each of dedicated MVD will have to provide the list of precise test items, valid (supported) entities, properties and proper values and use functionality of the common theme to perform the validation. The most common MVD is the IFC Coordination View 2.0 certification that is administered by the buildingSMART International to certify the IFC2x3 compliance. Many other MVDs that have been published have specific requirements that define further constraints to the coordination view or define some exceptions to it. Therefore it is important that the rules defined for MVD are logically consistent and not contradictory or lead to recursive condition.Conformance tests has been actively developed as it is very essential in the support of robust exchange requirements between applications. They have included in automated tests in IFC CV2.0 certifications and other like certifications.This dimension is unique and generally different from other rule category since to test completeness of export or import require additional information outside of the IFC file. We define three sub-categories: export summary, geometry mass property and sampling point, and import tests. IFC exchange completeness can only be assured if the information from the source, i.e. the originating BIM tool, is known and therefore can be used as a reference to compare with what inside the IFC file. We cover the completeness tests using the external information in a separate section.This extended table cannot be assessed as to its completeness; it is rather a classification for tests, to build up a relatively comprehensive classification. Over time, such definitions should support various important benefits:(1)improved definition of types of rules and their coverage;provide a basis for defining rule checking methods, support a taxonomy of rule types;The tests described here and further elaborated with examples in Appendix A are embedded in a large contextual structure with different levels. Some apply to particular object types, for example those dealing with stair well-formedness. Others such as geometry well-formedness rules also apply to all instances of geometry, at the multiple levels of the model hierarchy, possibly at the site, building, building element, space, embed, furniture and other objects in the aggregation spatial structure. The logical coverage is a requisite aspect of model checking. This paper does not address these contextual model conditions.Category I criteria generally provides the comprehensive coverage of testing any IFC model. One key limitation in this test is that it is unable to test a “good” error; the “good” error is a correct and consistent data in the model that is not the same as the original data, for example an object may be correct in term of its local definition, location, etc., but it is shifted from the original location in the source drawing due to defect in the translation process. Without means to validate the correct translation, even a few errors would create uncertainty and significantly affect translation confidence. To be able to test correctness of the data compared to the source, we may need to have information that the producer of an IFC model must provide in addition to the IFC data. This information will facilitate further automated testing of export and later on import. Table 8 in the appendix list the rules that support this category of tests as defined in Table 3 Category III (row number 24–27).One simple but important test of completeness of a translation process is a summary that provides object counts, and entity mapping information. The minimum requirement for object count should be provided as a log file during export or import process. This log file should be included inside the .ifczip format together with the IFC P21 file. Fig. 6 shows the example of an export log information. On the left (A) is the example of the current log file from AutoCAD Architecture application that already output object count information. It could be improved to contain more information including the mapping between the entities in AutoCAD Architecture and IFC (B). Such information is very useful to ensure that the translation process has been completed as expected, even though it is only possible now with manual eyeball inspection. To facilitate automation, the log file should be written into a standardized xml file. Usually, a validation process will look at each count of object and compare it with the actual content in the IFC file. It will also check the error count as a possible issue in the translation process. Fig. 7shows one such schema that can be used to facilitate the standardize log information.One issue with this approach is that the BIM software developers should try to ensure the information is as useful as it can be. The log information is very useful for tracking issues of IFC export (and import) in day-to-day operations and not just for certification purposes. The natural way to view it is that the information should be already there for the certification to make use of it as part of the completeness validation purpose. Mapping information can be captured in the same log by separating the count for different mappings even though they may come from the same representation in the originating model (see example in the first 2 lines in Fig. 6(B)). Explicit logging information for mapping could help to ensure transparent mapping process and help tremendously to track completeness of the translation.The object count provides only one rough means to test consistency. For geometry related validation, mass properties of geometry from a native file and face sampling points can provide a good way to verify a level of consistency of the geometry exported to IFC. The exporting application needs to compute mass properties for each shape it exports and chose sampling points on the surface of the geometry to be exported automatically during the process of translation/export. Any point that lies on the surface of the geometry may be selected as sampling points, hence vertices would be obvious straightforward candidates for this purpose. These information will be used by the test rule to be compared with the re-created geometry from the IFC file. The combination of mass properties and sampling points provide representation of the original geometry and they offer good enough validation information to ascertain correct geometry translation from the source. Table 8 Items IV.B.1 and IV.B.2 show the rules that check and make use of the mass properties and sampling point information to test correctness of geometry translations.We propose to store information on mass properties and sampling points into the IFC model as a new context identifier (=“ValidationData”) in the IfcRepresentationContext. The actual sampling points are identified using RepresentationIdentifier=“SamplingPoints(WCS)” in the IfcShapeRepresentation with Items of RepresentationType=“Point”. The Items contain set of IfcCartesianPoints from the sampling points taken randomly at the surface of the object geometry in the World Coordinate System (WCS). The mass properties should be captured into special Property Set Validation_Pset_Mass_Properties that stores three information, i.e. volume, surface area and tight bounding box. Fig. 8describes the MVD diagram of this mini “exchange” requirement. With possible differences in a faceting mechanism, these comparisons will not yield 100% equivalent results. Combining the sampling points and mass properties, complemented with a small tolerance, will give generally high confidence of geometry consistency.Exporting application that supports this test feature may provide an option to enable this information to be exported for a validation purpose that can be turned off in the normal use.Import is a more challenging issue to deal with as each BIM application has its own proprietary way of representing and managing objects. What we are interested in is to make sure that the application consistently imports all supported objects and geometries – by supported we mean that the application specifies that it is able to import them correctly. In dealing with import, we are addressing the questionGiven an IFC instance file, does the import translator read the file and “capture” all the meaningful data intended?Import tests strongly require assessment of the native BIM model representation of the imported data, in its own proprietary format. Current IFC2x3 Certification relies solely on manual verification for import due to heavy dependency on the native format, which the respective vendor knows. This process takes a lot of expert knowledge and time. At this time, none of the import validations are automated.To achieve the goal for automating the validation process including import, we propose two possible ways to do this, i.e. using a re-export method (to IFC) similar to the one outlined by Lipman et al. [3], and using another format that may be simpler commonly used for visualization. It is recognized that it is not in our interest to have an identical model in an imported, then re-exported file, but it must be consistent. Table 4describes the minimum requirements each of the application must support to allow automatic validation to be possible. The geometry requirements may be reduced to just BREP geometry and/or simple extrusions akin to the proposed reference view MVD [29].The second method is making use of a common format for visualization such as X3D, VRML, COLLADA, or OpenCTM [30–33] (it would be good to decide on one format), plus simple metadata to capture the original IFC entity and its properties of the geometry. In both cases, the validation tool will perform comparisons entity to entity, property to property and geometry to geometry (for overall shape, extent and location). The minimum requirement for the content in this format is identical with the IFC format described in Table 4. Since the geometry is often more simplified and approximately represented than the original geometry, this test must accept a small tolerance to allow minor differences due to the approximation effect. The exact tolerance value will need to be determined with empirical tests that will be good enough and avoid false positive results.A generic model comparison tool is necessary for most types of validation. It needs to support IFC-to-IFC comparison and also IFC to the 3rd party format. Preserving GUID is crucial in this process since GUID is used as the primary key in a comparison step. The model compare tool must provide the comparison features of both properties and geometry. The model comparison tool such as EVASYS provides a good coverage of comparing two IFC P21 files [34]. To be complete it needs to have additional support for geometry tests to ensure consistency of geometry after import. Borrmann et al. has extensively researched the subject of geometry approximation using Octree indexing that provides suitable tools to perform quick geometry comparisons based on the Octree indexes, unless more precise comparison is required [35], in which case a solid modeler will be required.

@&#CONCLUSIONS@&#
