@&#MAIN-TITLE@&#
‘Big data’, Hadoop and cloud computing in genomics

@&#HIGHLIGHTS@&#
Ever improving next generation sequencing technologies has led to an unprecedented proliferation of sequence data.Biology is now one of the fastest growing fields of big data science.Cloud computing and big data technologies can be used to deal with biology’s big data sets.The Apache Hadoop project, which provides distributed and parallelised data processing are presented.Challenges associated with cloud computing and big data technologies in biology are discussed.

@&#KEYPHRASES@&#
Cloud computing,Bioinformatics,Big data,Genomics,Hadoop,

@&#ABSTRACT@&#
Since the completion of the Human Genome project at the turn of the Century, there has been an unprecedented proliferation of genomic sequence data. A consequence of this is that the medical discoveries of the future will largely depend on our ability to process and analyse large genomic data sets, which continue to expand as the cost of sequencing decreases. Herein, we provide an overview of cloud computing and big data technologies, and discuss how such expertise can be used to deal with biology’s big data sets. In particular, big data technologies such as the Apache Hadoop project, which provides distributed and parallelised data processing and analysis of petabyte (PB) scale data sets will be discussed, together with an overview of the current usage of Hadoop within the bioinformatics community.

@&#INTRODUCTION@&#
Advances in next generation sequencing technologies [1] has resulted in the generation of unprecedented levels of sequence data. Therefore, modern biology now presents new challenges in terms of data management, query and analysis. Human DNA is comprised of approximately 3 billion base pairs with a personal genome representing approximately 100 gigabytes (GB) of data, the equivalent of 102,400 photos. By the end of 2011, the global annual sequencing capacity was estimated to be 13 quadrillion bases and counting, enough data to fill a stack of DVDs two miles high [2].Moore’s Law describes a trend coined by Intel co-founder Gordon Moore which states that “the number of transistors that can be placed on an integrated circuit board is increasing exponentially, with a doubling time of roughly 18months” [3]. Put more simply: computers double in speed and half in size every 18months. Similar phenomena have been noted for the capacity of hard disks (Kryder’s Law) [4] and network bandwidth (Nielsen’s Law and Butter’s Law) [5]. This trend has remained true for approximately 40years, until the completion of the Human Genome project in 2003. Since then, a deluge of biological sequence data has been generated; a phenomenon largely spurred by the falling cost of sequencing [6]. Sequencing a human genome has decreased in cost from $1 million in 2007 to $1 thousand in 2012 [7]. As further evidence of this, the 1,000 Genomes project [8], which involves sequencing and cataloguing human genetic variation, has deposited two times more raw data into NCBI’s GenBank during its first 6months than all the previous sequences deposited in the last 30years [9] and with mobile sequencing thumb drives on the horizon [10], it shows no sign of slowing. Over the coming years, the National Cancer Institute will sequence a million genomes to understand biological pathways and the genomic variation. Given that the whole genome of a tumour and a matching normal tissue sample consumes 1TB of uncompressed data (this could be reduced by a factor of 10 if compressed); one million genomes will require 1 million TB, equivalent to 1000 petabyte (PB) or 1 Exabyte (EB) [11].Until recent years, Moore’s law managed to keep ahead of the genomic curve, slightly outpacing the generation of biological sequence data by its growth in storage and processing capacity. However, since 2008, genomics data is outpacing Moore’s Law by a factor of 4 [12]. Biology’s big data sets are now more expensive to store, process and analyse than they are to generate. This explosion of data is not exclusive to the life sciences and has also impacted on other diverse sectors; with the International Data Corporation (IDC) calculating worldwide data at 0.8ZB (a trillion GB) in 2009, with estimates that this will increase to 40ZB by 2020 [13]. Indeed, the big data problem has been at the forefront of the technology sector over the last 5–8years as a result of the widespread rollout of high speed wide area network access and a proliferation of next generation applications, combined with the advent of social media [14]. A similar deluge of data is being experienced in diverse sectors such as finance, retail, smart sensor networks (the Internet of Things) and the physical sciences [15] where telescopes capture high quality images, from the 1.6TB generated each day by NASA’s Solar Observatory to the 140TB gathered daily from the Large Synoptic Survey Telescope [16]. The advent of such large datasets, has significant storage and, more importantly, computational implications. Given that processing and analysis of such datasets may be bursty in nature e.g., tasks such as read assembly is more computationally intensive than subsequent tasks, such high performance compute power may not be fully utilised over time. This has spurred the use of cloud or utility computing (also known as elastic computing) where users can hire infrastructure on a “pay as you go” basis, thereby avoiding large capital infrastructure and maintenance costs. Due to advances in virtualisation, such customised hardware and computational power can now be provisioned instantaneously using user friendly web interfaces.However, the solution does not lie in cloud computing alone [17]. Big data presents problems in that it can deviate from traditional structured data (organised in rows and columns) and can be represented as semi-structured data such as XML, or unstructured data including flat files which are not compliant with traditional database methods. Furthermore, cloud computing alone does not address the challenge of big data analytics (known as Data as a Service) where large scale processing is required, particularly when the scale of the data exceeds a single machine. In this case it is necessary to develop applications that will execute in parallel on distributed data sets, a non-trivial task that has been the focus of grid computing research for many years. This explosion in big data and the need for the application of big data technologies has generated significant demand for “data scientists”, computer scientists and mathematicians with expertise in big data, analytical techniques, statistics, data mining and computer programming. As evidence of this, the Harvard Business Review has heralded the Data Scientist as the “sexiest Job of the 21st Century” [18]. This hype is not without foundation. The market for big data is $70 billion and appears to be growing by 15% a year [19]. If this trend continues, it is estimated that by 2018 the US will need between 140,000–190,000 people with deep analytical skills and a knowledge of big data technologies [20] To address this, in recent months the US government has announced the “Big Data Research and Development Initiative”, committing $200 million to big data research initiatives, including the National Institutes of Health (NIH) to “improve the tools and techniques needed to access, organise, and glean discoveries from huge volumes of digital data” [21].In the healthcare sector, according to the McKinsey Global Institute, if big data is used effectively, the US healthcare sector could make $300 billion in savings per annum, reducing expenditure by 8% [22].Therefore, while the life sciences are not the only sector experiencing big data overload and while the challenges in applying big data technologies are currently not trivial, particularly given their infancy, the benefit to mankind of deciphering such big biological data sets make it the ultimate use case.Cloud computing provides a scalable and cost efficient solution to the big data challenge; although largely ill-defined and widely abused to represent anything that is ‘online’, the National Institute for Standards and Technology (NIST) defines Cloud computing as “a pay-per-use model for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction”.While some of the concepts behind cloud computing, such as distributed systems, grid computing and parallelised programming are not new, one of the primary enablers for cloud is virtualisation technology. This has facilitated the business model for clouds to evolve, enabling widespread rollout [23]. Using virtualisation technology, a single physical machine can often host multiple virtual machines (VMs), ensuring maximum utilisation of the hardware and capital investment. A virtual machine (VM) is a software application that emulates a physical computing environment in which an Operating System (OS) and associated applications can be run with multiple virtual machines installed on a single machine. A Hypervisor, a virtualisation management layer, translates the requests from the VM to the underlying hardware (CPU, memory, hard disks and network connectivity). The distinction between these is noted in Fig. 1.Similarly, advances have been made in the robustness of distributed computing and parallelised programming [24]. Traditionally in grid computing, most effort was spent on maintaining the resilience and robustness of the cluster itself rather than on solving the actual problem. For example, if one processor was to fail or hang this could jeopardise the entire analysis until the processor recovers. Modern big data technologies have devised solutions to overcome such limitations and provide solutions to process and analyse massively parallelised data sets using scalable and cost effective techniques. The distinction between big data technologies and cloud computing is commonly overlooked as cloud computing is often used to facilitate the cost effective storage of such large datasets. Furthermore big data technologies are often offered as Platform as a Service (PaaS) within a cloud environment. It is however important to note that technologies, while often coinciding, are distinct and can operate mutually exclusively.IaaS is where Cloud Computing providers incur large capital expense to invest in establishing and maintaining High Performance Computing (HPC) infrastructure or data centres that are then made available to their customers on a “pay for what you need” basis, with associated fine grained billing. It means end users incur none of the cost of building a HPC environment, especially given that use of such environments is often “bursty” in nature with the infrastructure often underutilised [25]. Users access this hardware using their broadband connection gaining access to server or storage infrastructure, and, similar to an electricity or gas bill, pay for what they use. As a result, IaaS is often referred to as utility computing or “elastic” computing, i.e., the ability to scale up or down on demand. IaaS providers employ virtualisation where users can create their own virtual machines, specifying the operating system and in some case the applications required [26].Amazon Web Services (AWS), are the leaders in IaaS, with some estimates suggesting that AWS holds 70% of the total IaaS market share [27]. They have a number of offerings; the most popular being the Elastic Compute Cloud (EC2) and Simple Storage Service (S3). AWS EC2 enables users to rent servers, accessing these via a VM image that they have chosen. Users can either build their own Windows or Linux based VMs from a menu of configurations or choose a pre-built bioinformatics specific image. AWS S3 is an online storage service. By paying a nominal fee, users gain access to the world’s largest data centre, accessing whatever type of infrastructure they require, for as long as they need, charged only for CPU usage, data storage and data transferred to and from Amazon. Furthermore, AWS offerings are extremely competitive, with instances of the EC2 service costing as little as 2 cents per hour for allocation of a VM (dependant on user requirements), while S3 pricing starts at 15 cents per GB stored per month. A more comprehensive review of the use of AWS in biomedical computing is provided in [28].It should be noted that the aforementioned solutions involve a public cloud. In this context, a public cloud refers to resources (infrastructure, applications, platforms, etc.) made available to the general public, typically on a “pay as you go” basis, and accessible only over the Internet. However, other cloud deployment architectures exist, such as private or hybrid clouds. Private clouds refer to virtualised cloud infrastructure owned, housed and managed by a single organisation. Hybrid clouds refer to the connection of two or more types of cloud, e.g., private and public, typically via Virtual Private Networking (VPN) technology for the purpose of scalability and fault tolerance. There is a trend towards a fourth model referred to as Community Clouds, where organisations with a common purpose, e.g., public sector organisations, contribute financially towards a cloud infrastructure, typically managed by a third party.The market is changing rapidly, with significant fluctuations in the IaaS landscape over the last few months; HP, Microsoft and Google are all vying for market supremacy. HP are now offering HPCloud based on OpenStack a cloud IaaS software originally developed by Rackspace and NASA. Microsoft, already established in the cloud platform field with the Windows Azure platform, is digressing into IaaS offerings, while Google have launched their IaaS offering called the Google Compute Engine. Other IaaS cloud providers of IaaS include Rackspace and Joyent.SaaS refers to the process of availing of applications run on a remote cloud infrastructure, accessible via the Internet. SaaS represents a shift away from installing software locally, on individual PCs, towards a trend of “thin clients”, where all the user requires is an Internet connection and a browser, such as Internet Explorer or Google Chrome. The user then connects to a desktop environment via a VM where all the software is installed. It should be noted that while the cloud service provider provides more functionality to the user in SaaS, the user has less control over their environment, as illustrated in Fig. 2. Therefore, the choice of option is very much dependent on the users’ desired level of control. SaaS is analogous to the 1980s approach of operating “dumb terminals” connected to a mainframe containing all the required applications.Unfortunately, most bioinformatics based applications are difficult to build, configure and maintain, primarily because they are, for the most part, open source in nature, lacking good documentation and require many programming library dependencies [29]. As a result, this requires an advanced level of technical expertise on the behalf of the biologist and, as such, is a common bottleneck in the adoption of bioinformatics based applications. However, as all software applications are installed and configured within the VM, SaaS provides the perfect solution.Cloud BioLinux [30], created by the J. Craig Venter Institute (JCVI) is an example of SaaS. It is a publicly accessible virtual machine that is stored at Amazon EC2, is freely available to EC2 users, and is based on an Ubuntu Linux distribution. It comes with a user friendly Graphical User Interface (GUI), along with over 100 pre-installed bioinformatics tools including Galaxy [31], BioPerl, BLAST, Bioconductor, Glimmer, GeneSpring, ClustalW and EMBOSS utilities, amongst others. While Linux based Bioinformatics distributions such DNALinux, BioSlax BioKnoppix, DebianMed, are not unusual, they are built to run on standalone local machines. SaaS initiatives such as BioLinux have been known to be referred to as Science as a Service (ScaaS). Another significant advantage of using such SaaS VM images on a public cloud, such as Amazon, is that Amazon provides access to several large genomic data sets including the 1000 Genome project, as well as NCBI, GenBank and Ensembl. CloVR [32] provides a similar image with pre-installed packages. Standalone bio/medical software applications/suites with a cloud backend include Taverna [33], FX [34], SeqWare [35], BioVLab [17] and commercial equivalents such as DNAnexus [36].PaaS allows users to build software applications by building on software libraries or development platforms already developed by the cloud provider. Within the technology field, platforms include Google App Engine, Microsoft Azure and MapReduce/Hadoop amongst others. MapReduce/Hadoop is a data processing and analytics technology that has been revolutionary in the realm of computer science and is one of the hottest technologies in the big data space [37].MapReduce is Google’s solution for processing big data [38] and was developed as large Internet search engine providers were the first to truly face the “big data tsunami”, indexing billions of webpages in a quick and meaningful way. Map Reduce is a software framework, written in Java, designed to run over a cluster of machines in a distributed way. The data itself is split into smaller pieces and are distributed over thousands of computers, known as the Google File System (GFS) and a parallelised programming API called MapReduce is used to distribute the computations to where the data is located (Map) and to aggregate the results at the end (Reduce). Hadoop, an open source implementation of Google’s solution, comprised of MapReduce and the Hadoop Distributed File System (HDFS), is used by leading technology companies such as Facebook, Amazon, Twitter and is based on a strategy of co-locating data and processing to significantly accelerate performance. In May 2009, Hadoop broke a world record, sorting a PB of data in 16.25h and a TB of data in 62s [39]. Hadoop clusters can be run on private infrastructure, however public offerings such as the Amazon Elastic MapReduce service (EMR) are proving popular, with EMR enabling users to easily and cost-effectively process large data sets and apply additional analytical techniques such as data mining, machine learning and statistical analysis. It should be noted however that programming Hadoop is not a trivial task; requiring significant expertise in Java to develop parallelised programs. As such, Hadoop has largely only been embraced in the technology sector.Still evolving at an extremely rapid pace, the application of this technology is now being considered in order to make big data discoveries outside of the technology sector. Given that this platform has now evolved into a widely supported and powerful framework for parallelisation and distribution; two paradigms that are particularly applicable to large genomics and medical data sets, Hadoop has enormous potential for making medical discoveries, if and when applied to the life sciences. Furthermore, given that public clouds such as AWS are now offering DaaS by providing a repository of public data sets, including GenBank, Ensembl, 1000 Genomes, Model Organism Encyclopaedia of DNA Elements, Unigene, Influenza Virus, this potential is becoming an imminent reality [40].Applying big data platforms and analytics in the realm of natural science not only has the potential to change lives, but also to save them [41]. Medical/genomics research is thus the dream use case for big data technologies which, if unified, are likely to have a profoundly positive impact on mankind. Currently, a much bigger issue than simply storing big data is processing it in a timely manner, and subsequently analysing the data for meaningful deductions. It is recognised that applying leading technology big data solutions, such as Hadoop, is revolutionary and there have been some early adopters in this space. A categorisation of bioinformatics projects that utilise the Apache Hadoop platform are detailed in Table 1with a more detailed review recently undertaken by Zou et al [42].One of the first MapReduce projects applied in the biotechnology space resulted in the Genome Analysis Tool Kit (GATK) [43]. This was followed by several subsequent Hadoop enabled biological contributions [44–46]. CloudBurst was one of the first of these, developed by Michael Schatz et al. [76] when working at the University of Maryland. Schatz later developed Crossbow for SNP identification, using Hadoop’s massive sort engine to order the alignments along the genome and then genotyping sample using SoapSNP. As input it aligns a mix of 3 billion paired-end and unpaired reads, equivalent to 110GB of compressed sequence data, and as output it catalogues all the SNPs in the genome. According to Langmead et al. [46], CrossBow can genotype a human in approximately 3hours on a 320 core cluster, discovering 3.7 million SNPs at>99% accuracy for $100 (including data transfer comprising an hour) using AWS EC2. However, although Crossbow is considered the state of the art in large scale cloud based SNP detection, it does not address the need to make the read mapping and SNP calling configurable, and only considers short reads of approximately 30 base pairs. Thus, Nguyen et al. [47] present CloudAligner as a user friendly interface application, which in comparison to local-based approaches show a higher quality performance, due to the partition and parallel processing of genome and reads. Langmead, funded via a grant from Amazon Web Service (AWS) and NIH, developed Myrna. Using the AWS cloud, Myrna calculated differential expression from 1.1 billion RNA sequence reads in less than 2h, at cost of about $66 [48].With the exception of these early adopted projects, applying these powerful big data technologies to biological problems has largely not been adopted into the mainstream. The technology, though extremely powerful, was built for those who are technically savvy and requires a high level of computational know-how, as applications built on top of the platform must be developed to be parallelised in nature. Few bioinformatics tools are designed to run in parallel, an issue which is not trivial. Until now, while the potential has been recognised in academic environments, little action has been taken for broader uptake and large scale development.Notwithstanding, in recent months, serious steps have finally been taken to turn this significant potential into a reality outside of academia, with many leading technology multinationals announcing their intention to progress the application of big data technologies to biological problems. Firstly, Dell announced that it is donating unused server capacity to the first FDA-approved personalised medicine trial for pediatric cancer, specifically Neuroblastoma, fostering a specialised software system between the Translational Genomics Research Institute (TGen) and the Neuroblastoma and Medulloblastoma Translational Research Consortium (NMTRC) [49]. Specifically in the PaaS space, Intel announced that they are collaborating with NextBio to optimise the Hadoop Distributed File System, Hadoop and HBase for genomics, with any improvements contributed back to the open source community [50]. Importantly, Cloudera have teamed up with the Institute for Genomics and Multiscale Biology at the Mount Sinai School of Medicine, in a pioneering effort to aid researchers in applying big data technologies in the field of genomics and multi scale biology to “diagnose, understand and treat disease”. Areas of research include analysis of human and bacterial genomes; study of the metabolic pathways of normal and disease states in the organism; structure and function of molecules used in treatment of disease, and more [51]. What is particularly noteworthy about this announcement is that both these organisations are committed to cutting edge research in their fields, and together form a formidable collaboration.Cloudera are widely recognised as being the leading Apache Hadoop software and service provider in the big data landscapeAt the cutting edge of big data technology development, Cloudera contributes more than 50% of its engineering output into open source Apache licensed projects, e.g., Hive, Avro, HBase and towards the further development of the Hadoop framework. This is not surprising given that it was established by three leading engineers from Google, Yahoo and Facebook (namely Christophe Bisciglia, Amr Awadallah and Jeff Hammerbacher) in conjunction with Mike Olson, a former Oracle executive. They were later joined by Doug Cutting the founder of the Apache Hadoop project. Cloudera in particular has also blossomed in pioneering cloud and big data technologies in the biological research and medical space. Jeff Hammerbacher, Cloudera’s chief scientist and co-founder, has committed to dedicating 25% of his time to the initiative with Mount Sinai also heavily committed to the project with Eric Schadt (a visionary in the use of computational biology in genomics [52]) coordinating the initiative.Thus, with leading pioneers in the fields of big data and computational biology as well as leading multinationals now committed to seeing progress in making medical discoveries by analysing large biological data sets, and with a projected annual growth for healthcare computing of 20.5% through 2017 [53], we are on the cusp of a technology uprising that is likely to have a profound impact on the diagnosis, understanding and treatment of disease.It must be emphasised that big data technologies are very much in their infancy and that although powerful, have a long way to go. Programming Hadoop requires a high level of Java expertise to develop parallelised programs. Efforts have been made to simplify this process, even in the technology sector, with developed software libraries such as Hive to add a “SQL” like interface that will generate parallelised Hadoop jobs in the background. Python streaming has also been made available to circumvent complex Java programming by wrapping it in Python, a more lightweight scripting language. Another point of consideration is that Hadoop MapReduce is designed by default to treat each line as an individual record. As many standard sequence formats involve multiple lines per sequence it is necessary to manipulate the data into one line formats, or to program custom Hadoop input formats and record readers – a less than trivial task.Furthermore, there is a current trend towards further developing analytics and visualisation technologies on top of the Hadoop platform, to enable better standardisation of reporting and summarisation of results. This is a problem which is not adequately addressed in the technology sector and is vital if the technology is to be widely embraced by diverse Industry sectors. Hadoop, is currently still very much a “behind the scenes” technology with no front end visualisation, powerful only if in the right hands and still difficult to set up, use and maintain. There are concerted efforts being made towards adding developer friendly management interfaces or GUIs on top of Hadoop systems to move away from shell or command line interfaces. Recently, Schoenherr et al. [54] presented Cloudgene for this precise purpose. Cloudgene provides a standardised graphical execution environment for currently available and future MapReduce programs, which can all be integrated by using its plug-in interface.There are also drawbacks associated with the utilisation of cloud computing. One of the most significant challenges, given the scale of the genomic data being generated, is that transmitting such data over the Internet or any other form of communication media takes prolonged periods of time, sometimes even in the region of weeks. Thus, the bottleneck is the rate of data transfer, i.e., getting data into and out of the cloud. As outlined in [55], in an interview with Vivien Bonazzi, program director for computational biology and bioinformatics at the National Human Genome Research Institute (NHGRI), “putting data into a cloud cluster by way of the Internet can take many hours, even days, so cloud customers often resort to the “sneaker net”: overnight shipment of data-laden hard drives”. In fact, BGI, one of the world’s leading genomics research institutes produces 200 genomes a day, with disks transported manually via FedEx [56]. AWS are actively trying to overcome this by introducing a multi-part upload and companies such as Aspera are also designing a new layer to operate on top of the TCP (Transport Control Protocol) transport layer protocol in an attempt to alleviate this issue [57]. This high speed file transfer (expected to be between 10 and 100 times faster than traditional FTP and HTTP approaches) has already been integrated into the BGI EasyGenomics SaaS solution showcased at the Bio-IT World Conference & Expo in 2012. BGI also integrated EasyGenomics with the Hadoop platform. This is notable as Hadoop and other scale out big data technologies exhibit a distinct advantage over traditional data management approaches in that the computation is moved to the data. By utilising local commodity hardware, data is distributed across a cluster of machines each utilising local processing, storage and memory and is processed in parallel, negating the need to transfer the data across the network from its storage location to be processed, as is typically the case with traditional HPC solutions. Furthermore, the aforementioned upload challenges are typically only faced by the large sequencing centres or represent a once off challenge. In contrast, as recently noted by Andreas Sundquist, CEO DNAnexus, the upload of sequence data, produced in real time from a single modern sequencing instrument requires a lower bitrate than streaming a movie over the Internet [58].As recently noted by Schadt [59], the ability to protect medical and genomics data in the era of big data and a changing privacy landscape is a growing challenge. While cloud computing is championed as a method for handling such big data sets, it’s perceived insecurity is viewed by many as a key inhibitor to its widespread adoption in the commercial life sciences sector. Indeed, this may explain why its employment has primarily been adopted by research and academic labs. However it should be noted at the outset that in many cases cloud solutions can provide equivalent, if not improved, security depending on the local security policy employed. Clinical sequencing, however, must meet rigorous regulatory requirements, primarily from the Health Insurance Portability and Accounting Act of 1996 (HIPAA) and thus cloud computing is being cautiously considered in such cases. HIPAA standards require compliance across service, platform and infrastructure layers, i.e., across IaaS/PaaS and SaaS. As this is difficult to enforce and validate in a public cloud, with third party contributors, Amazon is not HIPAA compliant. Amazon has however released a paper that will allow customers to develop healthcare applications that comply with a subset of HIPAA’s regulations [60]. Hybrid clouds are thus considered a more secure approach with hybrid FDA and HIPAA compliant clouds used as part of the collaboration discussed in the previous section between Dell and TGen to support the world’s first personalised medicine trial for paediatric cancer [61] and commercial solutions such as those provided by GenomeQuest and DNANexus. However, fundamental aspects of data security will need to be addressed before widespread adoption of cloud-based clinical sequencing can occur. Some of the key issues include encryption mechanisms (particularly key management), the vulnerabilities of Internet based customer access interfaces, replication in the case of disaster recovery, along with inadvertent data access via incorrect data ‘deletion’ i.e. reassignment of virtual resources allowing customer access to other customers’ ‘deleted data’. This will not be an overnight solution and with increasingly advanced decryption and de-anonymisation techniques, the privacy of “anonymised” sequence data or Electronic Health Records (HER) may be extremely difficult to definitively guarantee. In the case of highly sensitive data, when all the technical precautions are provided, the weakest link in the chain may, as has traditionally been the case, be the human one. Nonetheless, the increasing impetus to utilise such technologies in order to exploit their economic benefits has highlighted the need for increased legislation in this area [62]. Data tenancy is another perceived challenge, particularly with public cloud usage i.e., availability of data should a commercial cloud service provider cease trading. This was evidenced when Google discontinued Google Health in early 2012, giving users a year within which to make alternative arrangements for their data. Furthermore, should this occur or if another motivating factor causes a user to decide to move their data to another provider, the ease with which this transition can occur largely depends on the interoperability of the initial cloud service.Unfortunately, most cloud infrastructures provide very little capability on data, application, and service interoperability. This makes it difficult for a customer to migrate from one provider to another, or move data and services back to an in-house IT environment. Finally, a further challenge relates to data privacy legislation (e.g., data in the EU cannot be stored in a US region) as well as legal ownership and responsibility pertaining to data stored between international zones (e.g., the 1000 Genome project exists only in US zone, not the EU zone) [63].Applications such as those outlined in Section 3 illustrate that cloud computing and big data technologies have a significant future role in the life sciences; facilitating high throughput analytics that allow users to rapidly interrogate vast data sets. It must be noted however that Hadoop is not a “catch all” technology but rather is best suited to batch processing applications, as opposed to real time ad-hoc queries. The application of this technology to suitable high impact areas, such as metagenomics [64], personalised medicine, systems biology and protein function and structure prediction [65] has the potential for killer applications.As the genomics revolution gives way to metagenomics – the functional and sequence based analysis of the collective microbial genomes (microbiome) in a particular environment or environmental niche – Biology’s big data sets are about to become several orders of magnitude bigger [64]. When one considers that the total number of bacterial cells on earth is estimated to be ∼4–6×1030, and that the majority of these are uncharacterised; this diversity represents a vast, and as yet largely untapped genetic bounty that can be exploited for the discovery of novel genes, entire metabolic pathways and potentially medically valuable end-products. These novel genes encode new and as yet uncharacterised proteins whose structure and function will have to be elucidated [66,67]. This is likely to put significant stain on existing in silico based solutions, such as Folding@Home [68] and Foldit [69]; distributed computing projects which employ a network of idle home PCs to resolve protein 3D structure and infer function. Replacing internet-connected home PCs with cloud based VMs will ultimately facilitate improved protein function prediction.Furthermore, such computational power, as represented for example by CrossBow which can genotype a human genome in ∼3h at a cost of just $100, opens the very real possibility of personalised medicine; the ability to treat individuals on a case by case basis, tailored to their specific genomic blueprint [70]. This is the holy grail of medicine, which ultimately has the potential to do away with traditional chemotherapeutic regimens (often more damaging than the disease being treated) in favour of a more benign targeted approach.The big data processing capabilities of cloud computing facilitating the analysis of all the variables at once is a significant enabler of the new area of systems biology [71–73] – a holistic approach which helps to visualise the ‘omics’, as an interacting network, creating a paradigm shift – allowing us to move from hypothesis-driven to hypothesis-generating research. An example of such an approach is the recent publication of a whole-cell computational model of the uropathogenic bacterium Mycoplasma genitalium[74]. This bacterial avatar represents the first truly integrated effort to simulate the complete workings of a free-living microbe in silico[75]. The overall model is based on over 900 peer reviewed publications and includes more than 1900 experimentally observed parameters. Model training and parameter reconciliation was achieved by recreating 128 different M. genitalium culture simulations – each predicting both molecular and cellular properties of the in silico cell – recapitulating the key features of the training data. Model validation was achieved using data sets not used in the construction of the model and which encompass multiple biological functions (from transcriptomics to metabolomics) and scales (from single cells to microbial populations). Systems and synthetic biology based projects, like the one described above, represent an obvious application for next generation cloud based computational biology and has the potential to revolutionise the life sciences.

@&#CONCLUSIONS@&#
