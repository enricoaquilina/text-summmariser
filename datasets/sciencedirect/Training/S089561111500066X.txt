@&#MAIN-TITLE@&#
Recovery of CT stroke hypodensity – An adaptive variational approach

@&#HIGHLIGHTS@&#
We designed and verified computerized support of acute stroke diagnosis.Novelty is application of integrated variational approach to detect ischemic edema.New, content oriented image restoration and descriptors of asymmetric hypodensity distribution were proposed.Clarified visualization of direct symptoms of pathology was completed with automated recognition of ischemic stroke.Achieved recognition accuracy was 1 for test set of over 500 CT scans.

@&#KEYPHRASES@&#
Computerized support of hyperacute stroke diagnosis,Ischemic sign recognition,Variational image processing,Compressed sensing,Content-oriented image restoration,Extraction of hypodense regions,

@&#ABSTRACT@&#
The present research was directed to effective image restoration with the extraction of ischemic edema signs. Computerized support of hyperacute stroke diagnosis based on routinely used computerized tomography (CT) scans was optimized to visualize the infarct extent more precisely. In particular, a beneficial support of time-limited appropriate decision of whether to treat the patient by thrombolysis is expected. Because of a limited accuracy in determining the area of core infarction, particularly in the early hours of symptoms’ onset, a variational approach to sensed data recovery was applied. Proposed methodology adjusts fidelity norms and regularization priors integrated with simulated sensing procedures in a compressed sensing framework. Experimental study confirmed almost perfect recognition of ischemic stroke in a test set of over 500 CT scans.

@&#INTRODUCTION@&#
Optimal reperfusion therapy of the hyperacute stroke patient recognizes the “time is brain” paradigm with rapid response requiring a multidisciplinary approach involving inter alia neurovascular imaging experts [1]. The proposed computerized support of hyperacute ischemia diagnosis was based on routinely used CT scans. More clear and precise visualization of the infarct extent with probable penumbra surroundings and edema manifestations, in general called hypodensity, was obtained. Additionally, automatic recognition of a resultant, such symptom-driven asymmetric distribution of tissue density was used for objectified confirmation of restored stroke symptoms. The novelty of the paper includes content oriented image restoration based on an integrated variational approach and descriptors of asymmetric tissue-density distribution along the brain left–right axis (i.e. LR asymmetry) adjusted to detect ischemic edema. Experimentally achieved, high recognition accuracy of stroke CT scans confirmed the usefulness of the presented methods.An introductory description justifies the meaning of asymmetric hypodense changes of imaged brain tissue as a clue, strictly time-dependent symptom of a hyperacute ischemic stroke. Because of a complex nature of the physio-pathological cause and ambiguous picture of this symptom, variational image processing with adaptive semantic criteria integrated with CT scan sensing, recovery and analysis is proposed as an accurate method for computerized support of stroke diagnosis.Human nervous tissue is rapidly and irretrievably lost as acute ischemic stroke progresses. The average duration of nonlacunar stroke evolution is 10h (range: 6–18h); the typical patient loses 1.9million neurons each minute of untreated stroke [2]. Thus, time urgency of stroke care is strictly required. Reliable evaluation confirming stroke occurrence with calculation of how much brain is just lost and, if applicable, therapeutic interventions should be pursued urgently. Current advances in stroke neuroimaging and quantitative neurostereology enable assessment of how much brain is lost in cerebral ischemia and are fundamental for a decision to administer beneficial thrombolytic treatment with intravenous (recombinant) tissue plasminogen activator – (r)tPA within the first few hours of the onset of symptoms. However, the benefit of such treatment in acute ischemia is time dependent. In general, rapid treatment is associated with better outcomes. Earlier thrombolysis leads to reduced mortality and symptomatic intracranial hemorrhage, and is associated with higher rates of independent ambulation at discharge and patient discharge to home following an acute focal brain ischemia [3]. All these findings support intensive efforts to accelerate all hospital procedures and thrombolytic treatment in patients with stroke whereby an efficacious (and positive) imaging system plays an essential role.Steadily perfected imaging technologies provide continuous refinement to the confirmation of clinical suspicions and accurate verification of laboratory test results when investigating a possibility of an acute stroke, which, however, still remains mainly subjective and qualitative evaluation, subject to inter- and intraindividual inconsistency and variation [4]. CT is still the method of first choice for imaging patients with acute stroke to differentiate between various forms of the stroke syndrome [5,6]. Although most stroke physicians and radiologists have recognized for years the superiority of diffusion-weighted imaging (DWI), a form of magnetic resonance imaging (MRI), over CT in patients with acute focal neurological deficits, predominant usefulness of CT imaging in the clinical practice of acute stroke diagnosis, particularly in severely ill patients, is undeniable. The most recent fascinating example is ambulance-based thrombolysis [7]. In general, CT imaging is less costly and easier to use compared to MRI. The access to CT systems is practically unlimited (round-the-clock and on weekends). On the other hand, the number of patients excluded from much less available MR imaging is large (e.g. because of claustrophobia, pacemakers, aneurysm clips, etc., aphasia, impaired consciousness, or confusion) [5]. However, the most important benefit of CT imaging is minimization of the prehospital and intrahospital management delay (“onset-to-door” and “door-to-needle”). Patient's transfer time to CT scanner can be minimized by its location within the premises of the emergency unit (strict site demands are not applicable). Placement of the patient into the CT scanner is more time consuming, patient monitoring during the study is simpler and scanners are usually immediately available – the scanner becomes available for a stroke patient approximately 5min after previous patient imaging. Motion artifacts are significantly smaller because of a short scan time.Managing doctors are expected to recognize quickly and correctly signs of an early infarction on CT to make the right decision regarding treatment. The only direct finding enabling ischemia recognition is the extent of hypodense tissue in baseline CT (which means a pathological decrease in X-ray attenuation) significantly facilitating stroke diagnosis. However, in the hyperacute phase of ischemia and even up to the first 24h after its onset, CT scans are often normal or subtle tissue density differentiation is almost imperceptible for standard forms of CT image visualization and review protocols. In general, CT imaging tends to underestimate the infarct size because of distorted linearity, degraded spatial and contrast resolution, the “fogging” effect resulting in isodense appearance of some infarcted areas and unstable technological conditions of image acquisition, case-dependent artifacts, significant noise (mostly quantum) level, etc. Therefore, a standard visual analysis of CT images is often difficult and may be insufficient to verify and confirm the treatment decision in the context of other clinical studies (neurological findings, laboratory results, risk factors, etc.).Therefore, interpretation of early changes on CT images suggesting acute ischemia is often an ambiguous, indirect and difficult task even for experienced observers. As it is not highly reproducible, it indeed depends on the subjective ability of the examiner. Worryingly, the only direct symptom of ischemia, i.e. hypodense infarcted area that is particularly used in the guidelines to identify the extent of the infarct is the least well recognized sign [8]. Wardlaw et al. also reported poor observer sensitivity of CT-based hypodensity recognition with an acceptable level of specificity. Even knowledge of the stroke symptoms has not affected recognition difficulties. Consequently, poor CT scan interpretations have had a noticeable impact on poor treatment decisions. Reading of CT images requires instructions and wide experience in recognizing anatomy and pathology with understanding grounded in the physical conditions of image contrast distribution. A suggested improvement of early infarction recognition is an extensive training involving two dominant factors: simplification of the interpretation procedure and a clearer description of the infarct extent. Only the most important signs to recognize with their clarified and objectified definitions are suggested as a focused subject of trained recognition skills.Relative definition of hypodensity imposes the characteristics of X-ray attenuation decrease by comparing the density of an affected structure to its normal density. The symmetry of the brain structures in transaxial planes facilitates such comparison [6]. The delayed appearance of parenchymal hypodensity after arterial occlusion means that the CT finding of hypodensity is highly specific for irreversible tissue damage (i.e. for a permanent ischemic lesion, which cannot be diminished by thrombolysis – ischemic edema becomes irreversible approximately within 2h and indicates irreversibly damaged brain tissue [6]). On the contrary, imperceptible hypodensity means that the infarct has not developed yet and a still reversible ischemic edema may be present or there is no stroke. Considering appropriate treatment decisions, reliable ischemia recognition and accurate assessment of edema extent are the essential purposes of the designed computerized support. It means that a sensitivity threshold adapted to the imaged tissue density asymmetry should be calculated to define the hypodense extent of edema, highly specific either for irreversible tissue damage or still reversible ischemic changes.In this paper, the crucial sign of ischemia, i.e. hypodensity extent of the brain edema, is proposed as a simplified recognition factor directly related to treatment determinants. Since hypoattenuating brain tissue noticeable in CT scans means ischemic infarct confirming irreversible injury, infarct extent and location might be associated with the patient's prognosis and response to thrombolysis treatment [6]. However, complementary understanding and identifying the ischemic penumbra related to the infarct core make this prognosis more accurate. A patient with a large penumbra in comparison to a small core is most likely to benefit from reperfusion therapies. Therefore, computerized image processing was used to clarify visualization of the ischemic tissue extent in areas susceptible to stroke and amplify recognition of all edema effects according to the optimized numerical criteria and algorithmic implementations.Digital image processing enhances imaging visualization to amplify the message of informative image content. Optimized procedures are even capable to detect subtle structures or objects which are imperceptible in standard subjective rating of images. Previously unidentified but computationally extracted information is potentially indicative of a disease [4]. The crucial question is: what is the required threshold of ischemic edema to establish hypodense changes detectable by processing algorithms? So the next pragmatic question is how to make such a computerized procedure an objective, repeatable and case-adaptive cause of substantial improvement in the early diagnosis of stroke? The answer has been investigated for several years and it is just sought in this paper.Generally, hypodensity extraction procedures optimized to recognize or percept infarct extent are mostly based on (a) adaptive filtering in image domain, mostly directed to noise reduction and contrast enhancement, (b) transform-based estimates of target function, and (c) statistical analysis of image texture with supervised or non-supervised classification. Different forms of normalization were used to standardize, objectify and make the analysis repeatable, including correction of the acquisition parameters and procedures, segmentation of brain structure or stroke susceptible areas, data dynamics and resolution adjustment, finding a symmetry of the brain, spatial normalization or transformation of individual's brain to match the size and shape of a brain atlas or template CT scans, etc. [9–12]. Some methods come down to the extraction of the infarct extent in a visualized form exposing most of all the investigated areas of hypoattenuating brain tissue (e.g. [13–16]. Other methods add quantified ischemia recognition calculating location, extent of the infarct and estimated probability of stroke occurrence [17,10]. Most of them were verified by receiver operating characteristic (ROC) analysis to evaluate observer's performance in the detection of ischemic stroke.Oguar et al. suggested resized and filtered in a simple manner CT scans to enhance the signs of hypoattenuation specific to ischemia [18]. In turn, Lee et al. proposed an adaptive partial median filter (APMF) to reduce noise without degrading the edge of gray-white matter interface [15]. Image data denoising and local contrast enhancement in multiscale domain of wavelets was proposed to extract hypodense areas in CT scans [14,16]. Rutczynska et al. combined adaptive filtering (APAF implementation) and finite Gaussian Mixture Modeling (GMM) of brain structures with context-based enhancement [19]. Symmetry of tissue density distribution was statistically analyzed with a density-difference diagram calculated by digital subtraction of the histograms of the left and right hemispheres [20]. Pixels within the highest density-difference ranges were highlighted. Similarly, in [9] histograms of the voxel density distributions were calculated for the lentiform nucleus and the insula and compared with the contralateral side using the Wilcoxon two-sample rank sum statistic. A non-parametric statistic was used because the distribution of attenuation change may display a non-Gaussian distribution. Statistical method of z-score map calculation and display on the basis of a voxel-by-voxel analysis has been developed by Takahashi to quantify and visualize the extent of hypoattenuating areas of ischemia. Additionally, the territory of the middle cerebral artery (MCA) was divided into ten specified regions, according to a visually quantitative CT scoring system (the Alberta Stroke Programme Early CT Score – ASPECTS). Each of the specified regions was classified as hypoattenuation or normality by linear discriminate analysis of z-score maps [13]. Oliveira et al. proved the differences between the texture parameters of control and patients’ tissues [21]. Similar suggestion was given by Ostrek et al. presenting substantial efficiency of numerical hypodense tissue recognition [22]. They investigated specificity of tissue texture features to ischemic regions in both image and wavelet domains. To optimize the analysis procedure, various classifiers were examined on large feature data sets. Tang realized a complex method of image adaptive filtering (similar to APMF with circular filter support), brain symmetry analysis and textural feature extraction (mainly based on co-occurrence matrix) [10]. Usinskas applied eighteen features to describe specifically brain texture in head CT images [23]. Chawla et al. proposed a method of detection of mid-line symmetry and classification of abnormal slices. Domain knowledge of the skull and the brain anatomical structure was used to detect abnormalities in a rotation- and translation-invariant manner. Tissue descriptors calculated in both the image and the wavelet domains were confirmed to be effective [24]. Recently, automatic infarct delineation in stroke CT images with accurate normalization of CT scans into template space and the subsequent voxel-wise comparison with control CT images to define areas with hypointense signals were proposed [12]. The statistical analysis used was a voxel-based outlier detection procedure based on the Crawford-Howell parametric t-test for case–control comparisons.However, the essential limitations of existing arrangements contributing to the improvement of diagnostic accuracy for acute ischemic stroke are as follows: (a) the ambiguity and insufficient clarity of the interpretation procedure based on processed visualization forms and calculated factors or indicators, (b) difficulty in determining the accuracy of the infarct extent, (c) unspecified reliably repeatability of computerized results for a diversified class of imaging conditions and case specificity, and (d) the absence of a comprehensive optimization procedure to adjust algorithm parameters to integrated normalization, processing, extraction and recognition criteria to represent accurately and clearly the areas of infarction in the simplest way. One of difficulties associated with computerized stroke approach is that the numerical procedures and the parameters are selected in ad-hoc manner. Manually adjusted solutions depend on utilized test set of CT scans, stroke evidence and manifestations, diagnostic protocols, experts’ experience and preferences, etc. To our knowledge, the incidence of early CT findings was never assessed with computerized support in an unselected population of stroke patients. Patient selection and the capability to recognize ischemia on CT scans biased assessment of supporting tool efficiency.Instead of local image filtering or transform coefficient thresholding, one can use variational processing embedded in optimization procedure of semantic image recovery. In the context of computerized stroke support, the variational approach (VA) is considered to be extremely useful because of the following items: (a) possible integration of adaptive filtering with sparse model of data in transformed domain and statistical texture analysis in one formal platform of optimization, (b) flexible adjusting of respective fidelity factors and regularizers with norms (sometimes pseudo or quasi) and numerical procedures able to minimize cost functional with acceptable computational complexity, (c) origins in fundamental inverse problem to recover or restore real image from noisy, blurred and distorted measured (observed) data. Because of that, the whole process of acute stroke diagnosis could be numerically modeled starting from acquisition limits of CT imaging and ending with interpretation criteria of processed image content. However, an informative content model, interpretation criteria and flexible adaptivity of parameter estimation are crucial to the effective solution of the stroke diagnosis support.Variational image processing, explosively developed in last years [25], exploits operators of the Laplacian or total variation (TV) as regularization or penalty prior. Dominant application is restoring a noisy image using TV regularization [26]. In addition to the noise reduction, the VIP is used for inpainting, component selection or compressed sensing. Images are viewed as minimizers of certain energy functionals including relevant fidelity priors and adequate regularizers. Variational image processing treats an image as a reality function whose sampling or sensing corresponds to the matrix form of a given discrete image. It enables recovery of useful properties for functional signal model like geometry, shapes, edges and more subtle smoothness distribution, slow signal changes, decomposed oscillatory components, etc., to achieve even sub-pixel level accuracy in high-resolution image processing.Variational image processing potentially integrates variational image smoothing with transform-based sparse model of the content and fidelity factor (related to sensing and interpretation priorities) in weighted optimization procedure. The dominant advantage is flexibility of controlled restoration framework, susceptible to adaptive content models, conditions of sensing and forms of output recovery, possibly adapted to conditions and requirements of acute stroke diagnosis. Moreover, it is easily expandable to acquisition limits modeled with sensing matrix. Integration of CT imaging characteristics with quality factors and informative content models in iterative calculation engine gives opportunity for a more complete and reliable numerical description of infarct extent to form a synergistic effect of stroke recognition. Compressive sensing (CS) integrates variational image processing of sparse signals with acquisition procedure of reduced number of incoherent linear measurements to recover the signal with high enough resolution and sufficient accuracy. Generally, image recovery is an optimization procedure (where output is a minimizer of a certain functional constituting a convex problem) with relaxation of important sparsity prior in terms of more computationally tractable norms, greedy alternatives and adaptively formulated semantic criteria of the accuracy.The fundamental objective of the presented study is extraction of the most useful content from routinely used CT scans to improve acute stroke diagnosis. In particular, beneficial support of time-limited appropriate decision of whether to treat the patient by thrombolysis is expected. Because of limited accuracy in determining the area of ischemic occlusion, particularly in the early hours after symptom onset, variational approach to measured data recovery was optimized with semantic criteria of stroke disease recognition based on the emphasized extent of hypodense tissue. Last years’ achievements of compressed sensing [27,28] and Sparse Land development [29] give new capabilities to effectively model the CT limitations and sensitively represent the ischemic edema signs.In general, a distribution of image edges, ridges and textures over specific region of interests (ROI) plays a decisive role in content-oriented medical image recovery. However, denoised changes of tissue density distribution and approximated local signal tendency are more specific for acute stroke diagnosis. Accurate visual perception of extracted diagnostic information (lesion symptoms, signatures or any specific features experienced as a direct or indirect sign of pathology) is the key condition of correct image interpretation. Thus separation and noticeable extraction of diagnostic components significantly improve medical image recovery.Compressed sensing platform assures optimized recovery of informative content which is based on image variational processing integrated with acquisition limits. The CS scheme of limited number of noisy measurements with a variety of common noise models and sensing matrix designs adequately describes acquisition limitations of real imaging systems, especially CT, with respect to high resolution (potentially continuous) noiseless image model as reliable function of interests. Fixed in advance random or deterministic matrices of CT sensing procedures with respective random noise can simulate the acquisition limitations due to radiation dose, time and resolution-limits, movement and physical artifacts, technological noise, unstable detector sensitivity and contrast, etc. Weighted priors of the iterative recovery form are important tools to control semantic recovery of diagnostically improved CT scans. Image restoration as a solution of the ill-posed inverse problem typically concentrates on content models of sparse, locally supported signal segments or components in a space-scale varying context. Sometimes instead of an equal access to the whole sensed data to compute a user-defined solution, only locally defined data, pre-defined geometric component or the like are taken with higher weight to estimate an adaptively and locally sparse solution [30]. Moreover, adaptive regularization is applied with family of priors taking into account the geometric properties of the image [31] or corresponding to the ROI preselected by sensing procedure [32].Sparsity of unknown images is exploited to recover the image information from relatively small number of linear measurements, significantly fewer than required by Nyquist–Shannon theorem. Sparsity means that information contained in a signal/image is much smaller than its effective bandwidth. However, in most cases of natural signals we have transform's economical signal representation in some dictionary (base, frame) Φ. An additional necessary condition is incoherence between a measurement matrix (i.e. a real imaging modality model) and a sparsifying model Φ. For example noiselets [33] are useful to measure sparse signals because they have pseudorandom dense representation and the signals compact in the wavelet domain are spread out in the noiselet domain.Long term study has proved sparsity of CT brain images in wavelet or wavelet-like bases [14]. Selected sparsifying transforms were previously applied to optimize compact image representation with satisfactory fidelity criterion. Intensive research was directed to an other multiscale and local base/frame with nonseparable kernels in 2D/3D space to optimize approximately sparse content representation [34]. The linear or nonlinear approximation procedures were used to select the important nonzeros and extract a target function of informative signal components. Unrepresentative noise and artifacts and possibly other uninformative signal components were excluded from restored image information.Accurate restoration of the CT scans is considered as an ill-posed or ill-conditioned inverse problem solved against degradations of real acquisition systems. It means that to ensure existence of a unique and well-defined solution, additional optimization criteria are required. Moreover, such criteria could be additionally fitted to a content model and specific (inter alia diagnostic) characteristics of the CT images.Formally, noisy sensing with a full-rank matrixA∈ℝM×Nis formulated as(1)y=Axr+ηwhere compressed (e.g. low resolution or feature-oriented) or degraded measurement vector y of M observations is used to recover sparsex˜with high enough resolution N>M or even N≫M as reliable approximation of unknown real xr(possibly with physically infinitive resolution but pragmatically limited to content clarity criteria). The sensing matrix A is a linear operator of an underdetermined system having infinitely many solutions. A random noise vector η reflects a statistical nature of the acquired data (i.e. a finite number of CT projections). Because of low dose settings in real CT systems, the sensing process is not deterministic. X-ray interaction and detection process with photon counts is modeled as a Poisson distribution plus a background Gaussian noise with zero mean [32,35]. Applied statistical models assume deterministic measurements to be average values over geometry and time limits of a detector and models. The statistical iterative reconstruction is formulated as the problem of maximizing the log-likelihood function of the joint probability distribution characterizing the data acquisition process. Assuming the Gaussian approximation of sensed data statistics and second-order Taylor's expansion of exponential probability distributions, the image reconstruction problem can be expressed in the following manner:x˜=arg minx{(1/2)(y−Ax)TD(y−Ax)+R(x)}, where D is the diagonal matrix with the maximum likelihood estimated of the inverse of the projection measurement variance. The R(x) is regularization term representing prior knowledge about the required image content. Such quadratic noise model can be incorporated into general solution of inverse problem in CS scheme with respective regularizers, possibly adaptive and adjusted to the specificity of CT-based stroke diagnosis.In this paper, a real sensing procedure of CT with a briefly characterized above, statistical nature of the measurements was relaxed to the standard CS framework with only simulated acquisition effects. The reason for that is that the primary purpose of the reported study was optimized variational approach to recovery of hypodense changes in imaged soft tissue. The verified parameters of the CT sensing procedure were the Gaussian noise model and the number and kind of compressed measurements. The unknown real xris approximated by a digital xd, i.e. finite-length vector of Nyquist-rate samples of originally reconstructed CT scans. Relaxing D, directly acquired compressive measurements were simulated as: (a) randomly selected M pixels of the given x and (b) the most significant Fourier coefficients of the transformed x.In the standard CS framework, inversion of the problem (1) is a variational minimization in the following adaptive form of the Lagrangian formula(2)x˜=arg minxF(x,y)+λ(x)R(x)with criterion of the fidelity to the observationsF(x)=∥y−Ax∥p1s1and λ-weighted priorR(x)=∥Tx∥p2s2that determines regularity of the solution (∥·∥p1and∥·∥p2are given norms). The operator T represents the minimized image features and depends on the purpose of image recovery. Fidelity measure needs to take into account the highly structured features of medical images and evident properties of the human visual system (HVS). Important efficiency criteria of the restoration procedures include recovery accuracy, computational complexity (possible linear or convex programming) and convergence speed. The most commonly used method of regularization is Tikhonov regularization in which∥y−Ax∥22+λ∥Tx∥22is the minimized functional.A strictly convex minimized function guarantees a unique and computationally tractable solution, which means that squared Euclidean norml22(i.e. measure of energy) and l1 are preferable. However, highly desired sparsity of restored information forces minimization of the pseudo-norm ∥x∥0=#{i;xi≠0} which is a classical, generally NP-hard problem of combinatorial search. Imposing certain matrix A conditioning, i.e. incoherence and restricted isometry property (RIP), convex optimization with l1 or simple greedy algorithms gives the sparsest solution of l0-based optimization [28]. A more general way of problem relaxation is solving an lp-minimizing problem where∥x∥p=∑i(|xi|p)1/pbut every choice 0<p<1 gives a concave functional [36].Briefly, often used fidelity criteria are as follows•least squares (LS), i.e. squared l2 as∥y−Ax∥22; integrated with TV prior is preferable for images corrupted by Gaussian noise with satisfactory edge preservation;recursive last squares (RLS) that minimizes weighted linear least squares cost function assuming deterministic signal model, able, e.g. to estimate consistently the sparse signal's support and its nonzero entries [37];mean square error (MSE) or l2 in the constrained form ∥y−Ax∥2≤τ[27,38];l1-norm used with TV prior that is useful to remove impulsive noise [39,40];general lpfidelity constraint, i.e. ∥y−Ax∥p≤τ where p≥1 and τ is chosen depending on the noise pth momentE(∥η∥p); for uniform quantization noise p=∞ is a good choice [41];Dantzig selector [42] that uses indication function of ∥A*(y−Ax)∥∞≤τ, where A* is the adjoint of A; the Dantzig selector is robust against measurement errors and more adaptive in a sense of fidelity criteria – with l1 prior can be recast as a linear program.The regularization priors often used for image restoration are as follows:•based on derivatives of the image to impose some smoothness on the recovered image:–the Sobolev prior which is l2 norm of the gradient approximated with a finite difference scheme R(x)=∑i∥∇xi∥2; it favors uniformly smooth images;the total variation (TV) prior as R(x)=∑i∥∇xi∥1; it assumes x∈l1(Ω) (Ω is the image domain) and favors piecewise constant images with edge discontinuities of small perimeters [26]; TV functional is non-differentiable but still convex and causes sparsity of the solution [43];the weighted TV model (WTV) with certain discretization of TV with anisotropic weights defined for local support (e.g. 5×5 support of 4/8/16 pixel neighborhood in [44]);sparsity prior while #(supx)≤K<M with unknown support distribution; NP-hard solution requires relaxation–to convex l1 minimization (i.e. Laplacian prior) with computationally tractable implementations;to lp: 0<p<1 minimization;in a form of greedy algorithms.sparsifying decompositions that produce a sparse representation of natural signals; it is based on a reliable theory of sparse signal models which means that the signals can be well-approximated as a linear combination of only a few elements (vectors) from adaptively adjusted basis or dictionary; we have–orthogonal/biorthogonal bases of tensor wavelets, redundant frames of curvelets with nonseparable kernels, contourlets, complex wavelets, etc.;SVD (Singular Value Decomposition), KLT (Karhunen–Loeve Transform) and other singular/independent vector extractors [45].The l1 minimization is a fundamental approach to recover a sparse signal from a limited number of measurements. It provides a useful framework to perform accurate recovery by means of convex optimization problems. Stable signal recovery in noise is possible under a variety of common noise models, e.g. uniformly bounded noise or Gaussian noise. Both the RIP and uncoherence are useful to establish performance guarantees in noise. The l1-based relaxation of l0 pseudonorm is realized in standard decoder of basis pursuit (BP) with LS fidelity criterion for noiseless and noisy data (well known realizations of lasso or extended with fidelity criteria of the Dantzig selector).Moreover, there is a variety of greedy methods to recover sparse signals. Greedy algorithms abandon exhaustive search for a series of locally optimal single-term updates. They rely on iterative approximation of the signal nonzeros (i.e. signal support with refined coefficients), either iterative identifying the support until a convergence criterion is fulfilled or subsequent signal estimation to provide matching to the measured data. Both essential approaches are applied, greedy pursuits (e.g. Orthogonal Matching Pursuit – OMP with iteratively adding new components that are estimated to be nonzeros) and greedy thresholding algorithms with element pruning steps (nonzero elements are removed iteratively from further analysis, e.g. the Iterative Hard Thresholding – IHT).In greedy pursuit, starting from x(0)=0 a k-term approximant x(k) is iteratively constructed by providing a set of active columns of A successively expanded at each next stage. The column selected at successive stage maximally reduces the residual l2 error in approximating y from the currently active columns. An important example of such greedy strategy is the OMP, where the consecutive approximation of x is updated by projecting y orthogonally onto the columns of A representing current support estimate.The Compressive Sampling Matching Pursuit (CoSaMP) [46] keeps the nonzero support and either adds or removes elements in each iteration; new x estimate is restricted to new smaller support. An alternative approach for recovery of sparse signals is combinatorial algorithms [47].Extraction of informative signal in stroke CTs requires additional prior to regard the data dependencies characterizing distribution of tissue density. A possible way is a second regularizer of a minimal TV norm among all possible candidate solutions [48,49,31]. The design of adaptive image recovery, i.e. with adaptive regularization, can be formulated according to the following optimization procedure(3)x˜=arg minx,c(·)κ(x)∥y−Ax∥p+λ1(x)∥Φx∥1+λ2(x)TV(x)with matrix Φ sparsifying signal x and even three possibly signal-dependent regularization parameters c(·)=[κ(·), λ1(·), λ2(·)] of adapted signal sparsity and smoothness control. The weighted fidelity norm ∥y−Ax∥pwas adapted to stroke image specificity of emphasized ischemia susceptible regions with local intensity minima. Furthermore, applied regularization form include two priors of l1-minimized sparsity and TV-based smoothness. The regularization parameters were adaptively weighted to local intensity distribution. However, in general (3) is not a convex problem and we need simplified relaxation of adaptivity and selection of regularization parameters in ad-hoc manner to make it computationally less intensive.A solution of (3) called RecPF (with fixed κ=1) was applied for tomographic data reconstruction from reconstruction from partial Fourier measurements. The achieved results confirm improved quality of reconstruction [48]. RecPF compared with two state-of-the-art algorithms occurred highly efficient, stable, robust and very fast because of a small number of iterations (the benefit of Bregman iterative algorithm). As a fundamental concept of solution, a classic approach for optimization problems with separable variables was proposed.We proposed alternative form of adaptive image recovery to extract the edema effects in CT scans. The conducted design study was directed to: (a) convex optimization of l1 prior to verify weaker quality of the image recovery of greedy algorithms, (b) alternative fidelity factors, (c) more comprehensive implementations of the TV operator, and (d) simulated interior tomography.Adaptive recovery of the infarct extent was based on two integrated procedures, similarly adopting the concept of optimization with separable variables:(a)the greedy algorithm of hypodensity extraction with emphasized asymmetric decrease of imaged tissue attenuation; it prefers fidelity criterion LS integrated with TV prior for CT scans corrupted approximately by Gaussian noise with satisfactory edge preservation for the areas of diversified tissue attenuation; the TV prior that favors piecewise constant images with edge discontinuities of small perimeters effectively adjusts the extraction effects to subtle regions of slightly different density;a convex method of image restoration from a possibly limited number of measurements, according to properly selected fidelity criterion used primarily for the extracted content components with regularizer of sparsity to maximally concentrate correlated informative content of hypodensity.

@&#CONCLUSIONS@&#
The advantages of the CS-based methods were verified in selected several cases of a difficult-to-diagnose stroke. CT scans of hyperacute examinations with the lack of stroke symptoms and follow-up CT studies with convincing symptoms of hypodensity were processed and visualized ad hoc. The pronounced extraction of the reduced density tissue was observed in each of the cases. Such confirmation of stroke occurrence at the acute stage of symptom onset is a really useful aid to the diagnosis of stroke. However, the method requires optimized visualization of extraction effects. Additionally, more comprehensive segmentation that limits effectively the areas of stroke susceptibility can give more explicit results to provide high enough specificity of diagnosis aided in such a way.Possible optimization of the image restoration with high enough accuracy of diagnostic components was proposed. The variational approach was applied to improve computerized support of CT-based acute stroke diagnosis. We analyzed fundamental principles and real limitations of edema extent recognition in stroke diagnosis and concluded that the necessity of hypodensity sign extraction is the most important, direct sign of ischemia. The proposed solution integrates image restoration with a possibly real acquisition model and selective image processing to enhance asymmetric distribution of brain tissue density. Adaptation of the optimization criteria was directed to local minima of denoised image function (functional image model) to approximate a variation of tissue density in CT brain images.The advantage of image processing and recovery based on integrated CS framework is a possibility of using a more complex, deeper or “genetic” signal analysis to precisely select and reconstruct hidden components of high diagnostic importance. More degrees of freedom and possible step-by-step forming of image components, together with adjusted adaptive regularization, give an opportunity to extract the hypodensity even in very difficult cases. However, design and optimization of such flexible model of whole image recovery is neither a convex nor a numerically tractable problem. Instead of that we have a heuristic integration problem of numerically tractable partial subproblems.Novelty of the paper is the application of an integrated variational approach to detect ichemic edema. New, content-oriented image restoration and effective descriptors of asymmetric hypodensity distributions were proposed. As a consequence, designed and verified computerized support of acute stroke diagnosis was proposed. Clarified visualization of direct symptoms of the pathology was completed with automated recognition of ischemic stroke. The achieved recognition accuracy was almost perfect for a test set of over 500 CT scans.Future research will primarily focus on specific, rapid and convergent implementation of the outlined concepts. Especially, medical image applications tend toward perfected adaptation of optimization terms according to semantic models of diagnostic images.