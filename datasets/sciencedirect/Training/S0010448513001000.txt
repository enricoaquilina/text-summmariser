@&#MAIN-TITLE@&#
An adaptive normal estimation method for scanned point clouds with sharp features

@&#HIGHLIGHTS@&#
Reliable estimation of normals for scanned point clouds containing sharp features.A robust method for noisy point clouds with outliers.Automatic evaluation of the local adaptive parameters employed in the method.

@&#KEYPHRASES@&#
Normal estimation,Scanned point clouds,Noise and outliers,Sharp feature,Anisotropic neighborhood,

@&#ABSTRACT@&#
Normal estimation is an essential task for scanned point clouds in various CAD/CAM applications. Many existing methods are unable to reliably estimate normals for points around sharp features since the neighborhood employed for the normal estimation would enclose points belonging to different surface patches across the sharp feature. To address this challenging issue, a robust normal estimation method is developed in order to effectively establish a proper neighborhood for each point in the scanned point cloud. In particular, for a point near sharp features, an anisotropic neighborhood is formed to only enclose neighboring points located on the same surface patch as the point. Neighboring points on the other surface patches are discarded. The developed method has been demonstrated to be robust towards noise and outliers in the scanned point cloud and capable of dealing with sparse point clouds. Some parameters are involved in the developed method. An automatic procedure is devised to adaptively evaluate the values of these parameters according to the varying local geometry. Numerous case studies using both synthetic and measured point cloud data have been carried out to compare the reliability and robustness of the proposed method against various existing methods.

@&#INTRODUCTION@&#
Reliable estimation of the normal vector at each point in a scanned point cloud has become a fundamental step in point cloud data processing. For example, many surface reconstruction algorithms require accurate normals as input in order to generate high-quality surfaces  [1,2]. The performance of common point-based rendering techniques is much dependent on the accuracy of the input normals  [3,4]. Direct applications of normal estimation can also be found in segmentation  [5], smoothing  [6], simplification [7,8], shape modeling  [9] and feature detection and extraction [10,11]. To summarize, all these applications would benefit from reliable estimation of normal vectors for the input point cloud.Given a scanned point cloud from a physical object, the problem being studied in this paper is how to estimate the underlying surface normal vector at each data point without changing the point positions. The main issue is to estimate the normals reliably in the presence of inevitable noise in the measured point data. In particular, when the underlying surface of the scanned object contains sharp features, which are common in mechanical parts, normal estimation becomes very challenging as ordinary scanning noise compensation techniques will also smooth out sharp features. The resulting normals are thus not accurately evaluated around sharp features, which negatively impacts subsequent applications of the scanned point cloud. For instance, the reconstructed surface and point-based rendering based on inaccurate estimated normals would lose sharp features (Fig. 1(b)). If the normals around sharp features can be accurately estimated, the sharp features will be well preserved and the reconstructed surface will correctly represent the physical object geometry (Fig. 1(c)). It is thus of much importance to develop an accurate normal estimation method for scanned point clouds which can preserve sharp features well.Normal estimation for a given point cloud has received much attention in the past decade. For point clouds with no or very low noise, the Voronoi diagram of the given point set appears to be useful. Amenta and Bern  [12] showed that the direction from a point to the furthest Voronoi vertex in the point’s Voronoi cell, referred to as the pole, can be used to approximate the normal at the point. OuYang and Feng  [13] established a local Voronoi mesh neighborhood at each point and estimated the normal via constructing quadric curves from the neighborhood points. The main issue with these two methods is that the pole and local Voronoi mesh become unreliable when relatively large noise is present. As the scanning noise is unavoidable and the noise level varies with different scanning device and scanning conditions, normal estimation methods that are robust towards the measurement noise are highly desirable.To better handle scanning noise, Dey and Goswami  [14] extended the use of Voronoi cell poles to the use of large incident Delaunay balls. The issue is that, as the noise level increases, the number of points without such large incident Delaunay balls also increases. As a result, no normals could be evaluated for these points and this makes the normal estimation process incomplete. In addition to the Voronoi/Delaunay-based methods, effective normal estimation methods based on parametric surface fitting have been proposed thanks to the evident noise-smoothing effect of fitting. The basic principle is to locally fit a parametric surface in a point’s neighborhood and use the fitted surface normal as the normal approximate at the point. An earlier method  [15] estimated the normal through locally fitting a least-squares plane from thek-nearest neighbors at each point. Further effort has been made to optimize the selection of the neighborhood size  [16] and the weighted contributions of neighboring points to the local plane fitting  [7]. Besides fitting a plane, some methods fitted a quadric surface  [5] or a sphere  [17] to the local neighborhood at each point to approximate the normal. A detailed comparison of most of these existing methods is available  [18].When the underlying surface of a point cloud contains sharp features, the quadric surface and sphere fitting methods described above for normal vector estimation are inapplicable near the sharp features due to the implied first-order continuity. To improve the normal estimation accuracy around sharp features, Mederos et al.  [19] applied the M-estimator  [20] to robustly fit a local plane at each point by penalizing neighboring points with large fitted residuals, which usually correspond to points across sharp features. The normal estimation can thus be improved around sharp features. Unfortunately, this method can lead to bias fitting in smooth curved regions due to over penalizing the fitted residuals. Li et al.  [21] proposed a local tangent plane detection method using a computationally expensive random sampling strategy, which is robust to outliers and can preserve sharp features well. Recently, Boulch and Marlet  [22] adopted a robust randomized Hough transform method to estimate normals with parameters that compromise between accuracy and computational efficiency. The main issue of the above two methods is that they require dense points around sharp features, which is not always available in a scanned point cloud data  [23,24].As the initially estimated normals are likely to be noisy, normal mollification methods  [1,25] have been proposed to smooth out the noisy normals. As demonstrated by Li et al.  [21], if the input normals are not sufficiently accurate to preserve sharp features, mollification is unable to recover sharp features reliably. Moreover, several bandwidth parameters, which require manual adjustments, are introduced in these methods. Another category of methods is to extract normals by defining point set surfaces. The well-known moving least-squares (MLS) surfaces  [26] and its variants  [1–3,27–29] locally approximate the points using a polynomial surface. Such methods are not only able to compute normals and curvatures  [30] but also facilitate point cloud smoothing, resampling, and mesh triangulation  [31]. Instead of defining a parameterized surface, some methods directly project the noisy points onto an approximated smooth surface  [32,33] and generate denoised, evenly distributed points through resampling. In particular, Huang et al.  [24] proposed an edge-aware resampling algorithm to generate a point set with normals and sharp features. The resampled point set loses the original point coordinates while the present work focuses on estimating normals without changing the original point positions, which is essential to many practical applications such as geometric inspection and metrology  [23].One common issue in normal estimation is that, for a point close to a sharp feature, which can be the boundary of two surface patches (an edge) or more (a corner), the employed point neighborhood would enclose points from more than one surface patch. This makes the estimated normal biased and inconsistent with the correct surface normal. Such inaccuracy directly results in the loss of sharp features in the constructed surface mesh and in the point-based rendering result. Methods that can preserve sharp features are rare and all have specific requirements such as user-specified parameters  [1,19] and high point density  [21,22].To address this outstanding issue, a robust normal estimation method is proposed in this work to estimate normals through iteratively reweighted plane fitting. In particular, the M-estimator  [20] in robust statistics was adopted to effectively reduce the influence of scanned data outliers and neighboring points belonging to different surface patches across a sharp feature. More importantly, for a point around a sharp feature, an anisotropic neighborhood containing points solely from the same surface patch is to be formed through incorporating three kinds of weight functions related to point distance, fitted residual, and normal difference. The normal can thus be accurately estimated from the properly identified neighborhood. It will be shown that the normal estimation process is not only highly robust with respect to scanned data noise and outliers, but is also capable of handling sparsely and non-uniformly sampled point data.Researchers have included weight functions in their normal estimation methods. However, some user-specified parameters are incorporated, which need to be adjusted in a tedious trial and error process  [21]. More importantly, it is very difficult to find a global parameter that is suitable to different local geometric shapes. To overcome this challenge, the proposed method automatically generates locally adaptive parameters so that normals can be reliably estimated in smooth regions as well as around sharp features.The rest of this paper is organized as follows: Section  2 reviews the related studies in weighted plane fitting from which the proposed method is derived. The proposed iteratively reweighted plane fitting method is detailed in Section  3. Section  4 presents the automatic selection of applicable parameters in the weight functions. The proposed method has been implemented and evaluated against several existing methods in Section  5, followed by the concluding remarks in Section  6.The proposed method essentially estimates the normal at a pointpby fitting a local plane. The initial plane fitting based study appears to be done by Hoppe et al.  [15] as a module in their surface reconstruction algorithm. Under the assumption of a smooth underlying surface, the normal atpcould be approximated by fitting a plane from thek-nearest neighbors ofp. The fitted plane would approximate the local tangent plane. Although the associated standard least-squares fitting method was not robust to outliers  [20], the concept of locally fitting a plane to approximate the local surface geometry was widely adopted  [7,16]. In particular, Mitra et al.  [16] systematically investigated the effect of noise, curvature and point density on selecting the optimalk-nearest neighborhood size for the plane fitting. Based on the observation that neighboring points closer topshould contribute more to the fitted plane than points far fromp, Pauly et al.  [7] proposed a weighted plane fitting method by assigning Gaussian weights to the distances betweenpand its neighbors. However, thek-nearest neighborhood ofp, even weighted by the distance, is isotropic and thus contains neighboring points from different surface patches than the patchpbelongs to whenpis close to a sharp feature. To improve the normal estimation accuracy around sharp features, Mederos et al.  [19] applied the M-estimator  [20] to robustly fit a local plane for each point by penalizing neighboring points with large fitted residuals. This fitting process reduced the influence of neighboring points across sharp features because these points often result in large fitted residuals and are assigned small weights. Unfortunately, several bandwidth-related parameters need to be manually adjusted. And these parameters are global and incapable of addressing different regions in a surface. For example, a small bandwidth parameter is preferable around sharp features but it would cause biased normal estimates in smooth regions (see the uneven shading in the boxed region in Fig. 1(b)).The above weighted plane fitting methods are good at compensating noise and even outliers. The key issue is how to properly assign the weights to the neighboring points, especially when the neighborhood is across a sharp feature where different surface patches meet. Ideally, for a pointp, only the neighbors from the same surface patch aspshould contribute to the fitted plane while neighbors belonging to the other surface patches should be discarded (by assigning them negligible weights). To achieve this, the iterative reweighted plane fitting method is proposed and detailed in the next section.The robust normal estimation method for noisy point clouds with sharp features is presented in this section. To define sharp features, it is commonly assumed that the underlying surface of a point cloud is piecewise smooth  [27,29]. A sharp feature can be an edge or a corner where different surface patches meet. Sharp features may be considered as sharp edges or corners withG1discontinuity or round edges or corners with very small blending radii  [34].Due to the limitations of common scanning operations, some spatial outliers with large measurement deviations may exist in a scanned point cloud. Also, for a neighborhood at a pointpclose to a sharp feature, the neighboring points located on different surface patches from whatpbelongs to should be treated as outliers [6,19,29], which can be referred to as normal outliers since the normals of points across sharp features vary significantly  [1]. To deal with both spatial and normal outliers, the M-estimator from robust statistics  [20] is employed and extended in this work to ensure a robust local plane fitting through reducing the effect of these outliers.The initial normal estimation method via weighted plane fitting by Mederos et al.  [19] can be formulated as:(1){d,n}=argmin∑ρ(d+(xi−x)Tn)wd(xi)‖n‖=1wherewd(xi)=exp(−‖xi−x‖/σd2)is a Gaussian weight function related to the distance from a pointxto its neighborxiandρ(x)=σr22(1−exp(−(x/σr)2))is the Welsch function.σdandσrare respectively the distance and fitted residual bandwidths, which control the relative contribution of each neighbor.dis the distance fromxto the fitted plane andnis the plane normal. In the present work, instead of solving Eq. (1) using the Newton’s method  [19], an iterative reweighted least-squares scheme  [20] is applied, which essentially solves Eq. (1) by iteratively refitting a plane on a point’s neighborhood with updated weights. More specifically, by takingwr(x)=dρdx/x, Eq. (1) can be solved iteratively as:(2){dk,nk}=argmin∑rikwd(xi)wr(rik−1)‖nk‖=1whererik=dk+(xi−x)Tnkis the fitted residual of a pointxiat thekth iteration.wris also a Gaussian weight function related to the fitted residual,wr(ri)=exp(−(ri/σr)2). The iteration starts withwr(ri0)=1. As the weights are fixed in each iteration step, Eq. (2) becomes a constrained least-squares problem, which can be reduced to an eigenvector problem using the Lagrange multiplier. The corresponding covariance matrixCis(3)C=[x1−x−x̄x2−x−x̄⋯xN−x−x̄]T[x1−x−x̄x2−x−x̄⋯xN−x−x̄]x̄=∑wd(xi)wr(ri)(xi−x)∑wd(xi)wr(ri)whereNis the total number of neighboring points considered. The eigenvector corresponding to the smallest eigenvalue ofCis the normal vectornof the current iteration, andd=x̄Tn. Eq. (2) can thus be interpreted as iteratively reweighted plane fitting at a pointxfrom its neighboring points considering both the distance and fitted residual weights. The distance weights effectively define the local neighborhood while the residual weights control the contributions of each point to the fitted plane according to its fitted residual. As shown in Fig. 2(a), the initial fitted planep0with distance weight considers the neighbors ofxon two surface patches (in the red circle) and is biased towards the surface patch on the left. Then, a normal is estimated at each point according to the same plane fitting procedure. In the subsequent iteration of Eq. (2), the residual weight starts to play a role (Fig. 2(b)) and the fitted plane is gradually tilted towards the surface patch on the right via penalizing the neighbors with large fitted residuals (on the left). The iteration terminates when the fitted plane is stabilized. The convergence of Eq. (1) has been proven by Mederos et al.  [19].It should be noted that Eq. (1) has limited capability to handle sharp features. As shown in Fig. 2(b), the effective neighborhood (shown as the red ellipse) still contains points from the surface patch on the left. The reason is that Eq. (1), which penalizes neighboring points with large fitted residuals via assigning small weights, considers large residuals to be caused solely by sharp features. Nonetheless, relatively large fitted residuals can also result from curved surface geometry. As a result, penalizing points with large residuals is unable to effectively reduce the influence of neighboring points across sharp features without over penalizing essential neighbors in curved surface regions. The resulting biased normal estimates, leading to uneven shading in smooth regions, have been clearly illustrated in Fig. 1(b). To further illustrate this issue, the combined distance and residual weights assigned to the neighbors of three typical points on the smooth region, edge and corner of the Fandisk model surface are shown in Fig. 3(a). It can be seen that in order to provide reasonably unbiased normal estimates for smooth region points, weights around sharp feature points are assigned to neighbors belonging to more than one surface patch. It should be emphasized that to accurately fit a local plane around sharp features, the best strategy is to only include neighbors belonging to the same surface patch as the point of interest and ignore neighbors belonging to the other surface patches. Eq. (1) thus needs to be further improved to be more accurate around sharp feature as well as in smooth surface regions.The normal at a point is in general close to the normals of its neighbors located on the same surface patch, but is quite different from those on the other surface patches across a sharp feature. The neighbors belonging to different surface patches can be treated as outliers in the normal field, referred to as the normal outliers. This essentially suggests the addition of another weight function related to the normal differencewn(ni)=exp(−‖ni−n‖/σn2)to penalize neighbors having normals much different from the normal of the point. The improved minimization procedure becomes(4){dk,nk}=argmin∑rikwd(xi)wr(rik−1)wn(nik−1)‖nk‖=1whereniis the estimated normal from Eq. (2) andσnrepresents the normal difference bandwidth. Eq. (4) can be solved using Eq. (3) with the added termwn(ni). The overall process is to firstly estimate the initial normals using Eq. (2) and then further improves the normals by Eq. (4). As shown in Fig. 2(c), for neighboring points on the left side of pointx, the normals estimated from the previous iteration are quite different from the estimated normal ofx. Thus, these neighbors will effectively be discarded due to their negligible normal difference weights and only the neighbors on the right side will be considered in the current iteration of Eq. (4). Meanwhile, the spatial outliers corresponding to large fitted residuals are also discarded due to their negligible fitted residual weights. As shown in Fig. 2(c), the effective neighborhood (shown as the red ellipse) only encloses points from the surface on the right.Three kinds of weight exist in Eq. (4). The underlying concept is to firstly employ distance weightwdin order to include nearby points and ignore points relatively far away. Then, neighbors with large fitted residuals (spatial outliers) and those belonging to different surface patches across a sharp feature (normal outliers) are discarded by assigning negligible residual weightswrand normal difference weightswnduring the iterative reweighted plane fitting process. An anisotropic neighborhood only enclosing neighbors from the same surface patch is formed for each point. The fitted plane based on such a neighborhood can thus reliably approximate the local surface shape around a sharp feature. Fig. 3(b) shows the outcome from combining the three kinds of weight. It can be seen that neighbors belonging to different surface patches across a sharp feature are correctly neglected since their estimated normal vectors are quite different from the normal of the point of interest. As mentioned previously, convergence of Eq. (1) has been proven  [19]. Eq. (4) is in effect an extension of Eq. (1) by adding the normal difference weightwn. The normal differencewnweight behaves similar to the residual weightwr. Both weights promote neighbors similar to the point of interest and downplay the others. Although Eq. (4) is able to provide improved normal estimation around sharp features, its effectiveness relies on the selection of proper bandwidths for the weight functions. To address this issue, it is proposed to firstly use empirical bandwidths for Eq. (2) to estimate the initial normal to infer sharp features, then use these initial normals to evaluate the locally adaptive parameters for the bandwidths, and finally improve the normals via Eq. (4) with the evaluated bandwidth parameters.Bandwidth selection is a common and critical problem for all methods using weight functions since the selected bandwidth exhibits strong influence on the estimation results  [20]. In particular, the bandwidth governs the rate with which the weight varies. Many existing methods involving bandwidth selection require users to manually adjust the bandwidth parameters in a trial and error manner  [1,3,6,9,19,25]. For example, the bandwidths for the distance and residual weight functions are usually set as a user-specified constant multiplied by the local point spacing. The main issue for this common approach is that the user-specified constant is global (fixed) and likely inapplicable to all situations. The use of an adaptive bandwidth has thus been receiving much interest for improved algorithm performance  [35]. In particular, Wang et al.  [28] attempted to find the optimal bandwidth for the distance weight function for MLS surfaces. It should be noted that the derivation of one bandwidth for nonlinear kernel regression in 3D is a highly complex task already. And in the present case, each iteration of Eq. (4) requires the derivation of two more bandwidth parameters (a total of three bandwidth parameters). One approach to find the proper bandwidths is cross validation. However, to identify the optimal combination of the three parameters is non-trivial and computationally expensive. To facilitate a more practical alternative, a heuristic approach is proposed in this work to analyze the effect of bandwidths related to different local geometry and select the proper parameter values accordingly.The first of the three weights in Eq. (4), the distance weight, should be formulated to contain a relatively large bandwidthσdin order to ensure that sufficient neighbors are included. The residual and normal difference bandwidths then play an important role in assigning their respective proper weights to these neighbors. In existing studies,σdhas been selected as a fixed ratio with respect to the local point spacingδlaround the point of interest:σd=cd⋅δl. The local point spacing is evaluated as the average distance from a point to its 6 nearest neighbors. A smallcdshows poor resistance to noise in the point cloud while a largecdincreases the computational time. The issue with this global ratiocdis that the resulting neighbors for a sharp feature point will be much less than those for a point in smooth regions, as shown in Fig. 2(c). Consequently, the estimated normals around sharp features will be much affected by noise due to the smaller neighborhood. To compensate for the excluded points, a larger bandwidth is preferred around sharp features. As a result,cdshould adapt to the proximity to sharp features.After enclosing sufficient neighbors, the spatial outliers and normal outliers should be assigned negligible residual and normal difference weights. To achieve this, the residual bandwidthσrand normal difference bandwidthσnshould be relatively small in order for the involved weight functions to decrease rapidly. Nonetheless, for a point neighborhood in curved surface regions, normals always gradually vary among the neighboring points and the plane fitting naturally leads to varying fitted residuals. Small residual and normal difference bandwidths will result in too much penalty in curved surface regions and cause the plane fitting to falsely bias towards some neighbors. This would in effect suggest choosing large bandwidths in order to yield relatively flat weight functions in curved surface regions. As an extreme, when infinite bandwidths are used, the residual and normal difference weights both become one. In this case, only the distance weight is considered in Eq. (4) and the resulting method is similar to method of Pauly et al.  [7]. The above statements can be easily verified via testing on a noisy point cloud synthesized from a cylinder, which contains both sharp features and curved surface regions. Fig. 4illustrates the normal estimation errors using Eq. (4) with different bandwidths. It can be seen that infinite bandwidths work very well for smooth cylindrical regions but fail around sharp features (Fig. 4(a)). When small bandwidths are employed, negligible weights would be assigned to points with large fitted residual or normal difference. This leads to much better normal estimation near sharp features, but worse in cylindrical surface since the rapidly decreasing weight functions causes biased plane fitting (Fig. 4(b)). It is then evident that if we could adaptively apply large residual and normal difference bandwidths to points in smooth regions and gradually decrease them towards the sharp features, the normal estimation would be good for the entire object surface (Fig. 4(c)). In summary, the bandwidths for the residual and normal difference weights should be locally adaptive according to whether a point’s neighborhood is located in smooth regions, sharp feature regions, or transition regions in-between. A feature coefficient for each point,Fcoe, is thus introduced to quantify how close a point is to a sharp feature. The feature coefficient value ranges from 0 (a point’s neighborhood in a smooth region) to 1 (a point’s neighborhood on a sharp feature) and it will be used to determine the proper bandwidths, as detailed in the following subsections.The evaluation of feature coefficient values is essentially similar to the work on sharp feature detection. There are many general feature detection methods for point clouds  [10,36]. However, few approaches are dedicated to detecting sharp features only. Although sharp features correspond to first-order discontinuity and do not have normals theoretically defined, normals have practically been used to facilitate sharp feature detection [11,37,38]. In particular, Weber et al.  [38] proposed a normal clustering approach based on Gauss map clustering to detect sharp feature points. This algorithm aimed to classify points into sharp feature and non-sharp feature points, while the requirement in this work is to identify not only sharp feature and non-sharp feature points but also points in the transition regions in-between. Also, this existing algorithm clustered the normals of potential mesh triangles. The calculated normals are quite sensitive to measurement noise since they are calculated via interpolating three triangle vertices. The present work improves on the method of Weber et al.  [38] by using the point normals estimated by Eq. (2) as the basic elements for Gauss map clustering. The point normals are determined via local plane fitting, which offers better robustness towards noise.In general, a Gauss map directly maps the normals of a surface in the Euclidean space onto a unit sphere. Based on the fact that surface normals vary continuously within smooth regions but change dramatically when crossing sharp features, the normal vectors for a point’s neighborhood in a smooth region should form one single cluster in the Gauss map, whereas a neighborhood near a sharp feature should form multiple clusters. Each cluster corresponds to a distinct surface patch. Fig. 5shows Gauss maps for three typical point neighborhoods on the Fandisk model surface. Such clustering results are to be utilized to quantify the feature coefficient at each point. Ideally, when moving a point of interest gradually from a smooth region towards a sharp edge, the normals within its neighborhood will initially be in one cluster and then changed into two clusters. The ratio of the size of the smaller cluster to that of the bigger (primary) cluster gradually increases from 0 to 1, with 1 meaning that the point is exactly on the sharp edge. A primary cluster is defined as:primary cluster:cluster size≥number of neighbors/number of clusters .The feature coefficient,Fcoe, for a specific point is then evaluated as:(5)Fcoe={0number of clusters=1min(s×∑Non-Primary Clustersδl2∑Primary Clustersδl2,1)number of primary clusters=11number of primary clusters>1whereδlis the local point spacing. When there is only one primary cluster, the ratio between the number of points in non-primary clusters and the primary cluster is a good measure on how close the neighborhood of a point is to a sharp feature. However, such a measure would be biased towards the surface patch with high point density when the points around a sharp feature are not evenly distributed. Since scanned point clouds are often characterized with non-uniform point density, the local point spacing of each neighbor is employed in Eq. (5) so that sparse point distribution is not unintentionally penalized. The ratio formulated in Eq. (5) for a sharp edge point is seldom to be exactly 1. A rounding factorsis employed to indicate the proximity to a sharp feature. The factorsis set as 2 in this work based on a large number of test cases. This means that a point with a ratio value above 0.5 is considered as a sharp feature point. In summary, the evaluatedFcoeis to be used as follows: the neighborhood of a point is deemed to be in a smooth region ifFcoe=0, in a transition region if0<Fcoe<1, or on a sharp feature ifFcoe=1.To reliably evaluate the feature coefficient of a point, the normals of its neighbors must be correctly clustered. In this work, the hierarchical agglomerative clustering method is employed  [39]. The elements to be clustered are the initially estimated normals by Eq. (2) with small bandwidths to favor sharp features. The normals are first mapped to points on a unit Gauss sphere and the angle between two normal vectorsnaandnbbecomes the geodesic distancedbetween the two mapped pointsaandbon the sphere:(6)d(a,b)=cos−1(na⋅nb).The distance indicates the similarity of the two normals. To cluster points on the Gauss sphere, the hierarchical agglomerative clustering method treats each point as a distinct cluster in the beginning and then gradually merges two closest clusters into one larger cluster until the distance between any two clusters are larger than a specified threshold. To define the distance between two clustersAandB, also known as the linkage criterion, the commonly used average distanceDshows the best performance in the presence of noise:(7)D(A,B)=1|A|⋅|B|∑a∈A∑b∈Bd(a,b).The common challenge of this clustering method is how to find the proper angle threshold to produce reliable clustering results. A small angle threshold can correctly classify the neighbors of a point around a sharp feature into distinct clusters while it may falsely generate more than one cluster for a point in a smooth region. Inversely, a relatively large angle threshold can form one cluster for a point in a smooth region but falsely merge all the normals across a sharp feature into one cluster as well. As a result, it is proposed in this work to automatically determine the proper angle threshold by exploiting the varying trend of the sharp feature point percentage with respect to the angle threshold.Let the angle threshold start from zero and increase with small increments. For each threshold, the feature coefficient at every point is calculated based on the corresponding clustering results from Eq. (5). The percentage of sharp feature points in the point cloud is then readily available, which always decreases with increased angle threshold. The decreasing rate of the sharp feature points is to be used to determine the proper threshold. Fig. 6shows the variation of the sharp feature point percentage with the angle threshold for the Fandisk, Rolling-Stage and Bunny models. Initially, the percentage is at 100% because a zero threshold makes every neighbor a distinct cluster, which implies that every point is a sharp feature point. When the angle threshold reaches 180°, the percentage becomes zero since all neighbors would form one single cluster. In other words, all points in the point cloud are initially regarded as sharp feature points and then gradually transformed into non-sharp feature points with increasing angle threshold.For the Fandisk and Rolling-Stage models with varying sharp feature sizes, points in smooth regions are the first to change their feature coefficients and points around sharp features then follow. The reason is that normal vector variation for a neighborhood of points in smooth regions is generally much smaller than that for neighborhood points across sharp features. As a result, the sharp feature point percentage would decrease rapidly in the beginning until points in smooth regions have mostly been changed to non-sharp feature points. The sharp feature point percentage then becomes stable since most points on sharp features would remain unchanged. It should be noted that when the angle threshold becomes larger than the variation in normals around a sharp feature, for example, a small dihedral angle between two surface patches around a sharp edge, the sharp feature point percentage will again drop rapidly. Then, the percentage may become stable again before reaching another larger dihedral angle and then dropping again. More than one stable region may occur depending on the different dihedral angles of sharp features. The first stable region separates smooth points (on the left) and sharp feature points (on the right) well. To determine this stable zone, the second derivative of the sharp feature point percentage is calculated using the low-noise Lanczos differentiator  [40]. In Fig. 6(a) and (b), the first zero second-derivative point (the inflection point), shown as a red dot in the second-derivative curve, is taken as the angle threshold in this work. With the automatically determined angle threshold, normal clustering for every point in a given point cloud can be done. The feature coefficient can then be calculated from the clustering results. For the very complex corner of the Icosahedron model with 0.5% imposed noise, reliable clustering result can still be achieved (Fig. 7).The Bunny model (Fig. 6(c)) contains smooth surfaces with many non-sharp feature sizes. The gradually-changing curvatures make the sharp feature point percentage curve decrease continuously without showing apparent stable regions and any sudden drops. More importantly, the inflection point does not show in the second-derivative curve. The point cloud is thus considered to be smooth without any noticeable sharp features. It is possible to falsely treat a highly curved region as a sharp feature, especially when the curved region is sparsely sampled. Such a situation is not common in a scanned point cloud because smooth regions often contain denser scanned points than around sharp features. Fig. 8shows the well-estimated feature coefficients for several models with noise and outliers. The common challenge in reliable feature detection is the ambiguity between noisy smooth surfaces and sharp features  [29]. As noise level increases, a sharp edge with large obtuse dihedral angles can become indistinguishable (see the noisy Fandisk model).With the feature coefficientFcoeat each point evaluated by automatically clustering normals estimated using Eq. (2), the three bandwidths in Eq. (4) can now be determined in order to further improve the estimated normals. In the present work, the distance weight bandwidth is set adaptively according toFcoeas:(8)σd=(1+Fcoe)⋅δl.The distance bandwidth is thusδlfor points in smooth regions(Fcoe=0). For a sharp feature point(Fcoe=1), the distance bandwidth of2δlwould enclose more neighbors in order to compensate for the loss of neighbors discarded by the residual and normal difference weights. From all the tests in this work, the adaptive bandwidthσdshows a good compromise between noise smoothing and computational efficiency.The fitted residual and normal difference weight bandwidths are respectively set as an adaptive fraction of the maximum fitted residual and normal difference from the previous iteration:(9)σr=rmaxk−1/3Fcoeσn=‖ni−n‖maxk−1/3Fcoe.The scalar 3 in the above equation is employed such that neighboring points around sharp features with large fitted residuals or normal differences are to be assigned negligible weights of around 10−4 (e−9), whereaswrandwnfor neighboring points in smooth regions are effectively equal to 1 and thus, only the distance weightwdplays a role. These bandwidth related parameters are determined from extensive computational tests and the resulting default values are used for all the case studies reported in this work. Regarding points in transition regions, the bandwidths would be moderate and neighboring points with relatively large fitted residuals or normal differences are penalized accordingly but not excessively.The presented normal estimation method is straightforward to implement. The three main steps are listed as follows:Step 1: estimate an initial normal for each point. This is done by iterative reweighted plane fitting using Eq. (2) withσdset as the local point spacingδlandσrone third of the maximum fitted residual. The initial fitted residual weight for each point is set as 1. The iteration terminates when the involved weights stabilize or the iteration number exceeds a set limit of 20.Step 2: estimate the local adaptive bandwidths. For an arbitrary point, the normals of its neighbors within a distance of3δlare clustered in order to automatically determine its feature coefficient. When generating the sharp feature point percentage curve, the angle threshold starts from zero and increases by an increment of 2.5°. After finding the proper threshold and estimating the feature coefficientFcoeat each point, the three weight bandwidths are set using Eqs. (8) and (9).Step 3: improve the initial normals. This is done by using Eq. (4) with the adaptive bandwidths determined in Step 2. The iterative calculation terminates in the same way as that in Step 1.To demonstrate how the estimated normals are improved by Steps 2 and 3 as well as the advantage of employing the three weights, a randomly sub-sampled scanned point cloud obtained from the Gear model is analyzed and the results are shown in Fig. 9. In Step 1, the initial normals are estimated usingwdandwrwith global bandwidth-related parameters (Fig. 9(c)). In Step 2, the feature coefficient at each point is estimated through automatic normal clustering (Fig. 9(b)). For Step 3, Fig. 9(e) shows the estimated normals usingwd,wrandwnwith global parameters. Compared with Fig. 9(c), the normals around sharp features (circled in red) are more accurately estimated due to the added consideration ofwn. However, the estimated normals in smooth regions (circled in green) are still noisy and do not point to the circular profile center. This is because the small global bandwidth suitable around sharp features would exaggerate the normal difference in smooth curved regions. If adaptive bandwidth parameters are used (Fig. 9(d) and (f)), the normals in the smooth region are much more accurate. This clearly demonstrates the importance of applying the adaptive bandwidth parameters. It is also evident thatwris essential in the estimation. Withoutwr, as seen in Fig. 9(d), the normals around some sharp features (circled in red) are seen to be biased toward their adjacent surface patches, whereas Fig. 9(f) shows the best estimated normals with all three weights considered.To evaluate the effectiveness of the proposed normal estimation method, both quantitative and qualitative comparisons have been made. For synthetic point cloud data, the normal estimation errors can be distinctively evaluated since the reference normals are known. The error is quantified as the angle between the estimated normal and the reference normal. Gaussian noise was superimposed on ideal synthetic point cloud data along random directions. The standard deviation of the noise, referred to as the noise level in this work, was specified as a percentage of the diagonal length of the bounding box of the point cloud. A number of sparse outliers (10%) were also added. One synthetic point cloud uniformly sampled from an icosahedron was generated and used to compare the performance of the algorithms in the presence of sharp edges with obtuse dihedral angles and very complex corners. Another synthetic point cloud randomly sampled from a cylinder was also tested due to the presence of both sharp edges and smooth curved surfaces.The proposed method has been compared with five existing methods  [1,7,16,19,22]. All the methods involve selecting suitable values for the associated parameters while the proposed method automatically determines the parameter values with no user input. For the comparison, the parameters used in these existing methods were, according to their respective procedures, either manually adjusted to attain the best results or set as the suggested values. It should be noted that compromise always has to be made between noise smoothing and sharp feature preservation. More specifically, the normal mollification method  [1] uses the normals estimated by Pauly et al.  [7] as input with a small neighborhood size of 40 neighbors in order not to overly smooth out sharp features. 0.5 is used for the normal weight bandwidth to infer sharp features and the distance weight bandwidth is adjusted to be 4 to compensate noise. Boulch and Marlet  [22] use RRHT_Cubes with 500 neighbors as a compromise between computational accuracy and speed. Fig. 10shows the mean normal estimation errors for the various methods under different noise levels. The proposed method exhibits the best normal estimation results in all cases. Fig. 11illustrates the normal estimation error maps on the noisy Icosahedron. It can be seen that the proposed method estimates the normals most accurately around sharp features while the other methods exhibit quite significant errors. For the noisy Cylinder data (Fig. 12), the proposed method still outperforms the others. In Fig. 12, the method by Mederos et al.  [19] using a small fitted residual bandwidth is able to estimate normals more accurately than the first three existing methods for points near sharp edges. However, the normals for points in the smooth cylindrical region are worse. Similar to the observation made by Li et al.  [21], the mollification method enhances sharp features in the precondition that the initial normals are accurate to a satisfactory level. Otherwise, the mollification can only smooth noisy normals but is incapable to preserve sharp features. Notably, the proposed method using locally adaptive bandwidth values is able to estimate normals accurately both in the smooth curved region and around sharp features.Regarding the tests on real scanned point clouds, quantitative evaluations are not possible since the exact reference normals are unknown. Alternatively, two different qualitative (visual) comparison approaches are employed. As the quality of the point-based rendering result of a point cloud highly depends on the accuracy of the input normals  [25], straightforward visual comparison can be used to compare the point-based rendering results based on normals estimated by the various methods. Fig. 13shows the point-based rendering results of the Carter point cloud. It can be seen that sharp edges are well preserved using the estimated normals from the proposed method whereas the estimated normals from the other methods result in loss of the sharp edges. Moreover, similar to that indicated in Fig. 1(b), the uneven shading in the curved surface regions based on the normals estimated by Mederos et al.  [19] clearly indicates that global bandwidth parameters are inapplicable to handling sharp features and smooth regions simultaneously. The method by Boulch and Marlet  [22] also shows good results around sharp features. However, the normals are noisy and the sharp edges are rough since the randomized Hough transform does not smooth out noises. Thanks to the smoothing effect of plane fitting, the proposed method yields smooth normals and shows the best rendering quality due to its adaptive bandwidth values.The second qualitative comparison approach is to visually compare the triangle mesh surfaces that are constructed by a method which requires accurate normals as input. The robust implicit moving-least-squares method  [1] has been chosen to construct the triangle mesh surfaces since it is able to reliably reconstruct sharp features if accurate normals are available. Identical parameters have been used in the surface construction method so that the results would only reflect the quality of the input normals. Fig. 14shows the constructed surfaces based on normals estimated by the various methods. The constructed surfaces based on normals from the proposed method are seen to preserve sharp features well. For the constructed surfaces based on normals estimated from the other methods, the edge and corner geometry is rounded and not as sharp. The last three methods are observed to preserve sharp features well. Fig. 15further compares the estimated normals from these three methods and it is clear that the proposed method exhibits the best capability in preserving the sharp features.Unlike the existing methods  [21,22], which require densely distributed points around a sharp feature, the proposed method is able to reliably process sparse and non-uniform point clouds. More specifically, for a point around a sharp feature, the proposed method only requires several neighboring points for feature coefficient evaluation and plane fitting whereas the other two methods require hundreds of neighboring points around the sharp feature in order to reliably implement their random sampling procedure. Fig. 16shows the feature coefficient evaluation and surface construction results for some randomly and sparsely sampled point clouds of about 10,000 points (sub-sampled point clouds from the original data sets). Although small geometric details are lost due to the much reduced point density, most sharp feature points can still be correctly detected, leading to well-preserved sharp features. Another important characteristic of the proposed method is that it is equally applicable to noisy point clouds with outliers, as illustrated in Fig. 17. It can be seen in this figure that although the point-based rendering results of the Carter and Ring data with the added noise and outliers, show some visually uneven surfaces, the normals for points around sharp features can still be reliably estimated with the sharp features clearly visible.Since the proposed method iteratively fits a local plane and updates the associated fitted residual and normal difference weights at each point, it is expected to be less computationally efficient than the method by Pauly et al.  [7], which only performs the plane fitting once. Test computations were carried out on a PC with a 3.30 GHz Intel Core processor and 2 GB RAM without parallel computing. The number of points in each test data set and the resulting computational time are listed in Table 1. The recorded computational time is not significant but is a few times larger than that required by the method of Pauly et al.  [7], as reported previously in evaluating the existing normal estimation methods  [41]. Regarding the iteration process, as illustrated in Fig. 18, the number of iterations required to estimate an initial normal for each point via Eq. (2) is typically fewer than 10. And the number of iterations required to improve the initial normals via Eq. (4) is typically under five. It is seen that the required number of iterations is generally small in smooth regions and slightly larger around sharp features. In the absence of a theoretical proof, extensive tests have been performed. For all the test cases, the proposed method always converges and the required number of iterations never reaches the set limit of 20.For the three implementation steps outlined before, Step 2 is the most computationally expensive step. While it is plausible to iteratively repeat Steps 2 and 3 in order to take advantage of the improved normals from Step 3 as input to Step 2, the resulting improvement has not been found significant enough to warrant the much added computational load of Step 2. In fact, the normal clustering in Step 2 has shown robustness towards noise and is able to capture the overall disparity in the initial estimated normals. As a result, the evaluated feature coefficients do not change much from iteratively repeating Steps 2 and 3.

@&#CONCLUSIONS@&#
A robust normal estimation method has been presented in this paper to reliably estimate normals for noisy point clouds with outliers. The proposed method incorporates three weights to effectively reduce the influence of outliers both in the spatial and in the normal field on the normal estimation. It does not impose strict restrictions on point density. Also, the proposed method is applicable to points in smooth regions as well as around sharp features via employing locally adaptive bandwidth parameters for the weights. The adaptive bandwidth parameter values are set according to the automatically evaluated feature coefficient at each data point.Compared with non-iterative normal estimation methods, the proposed method requires more computational time. The most time-consuming part in the proposed method is to evaluate the feature coefficient at each data point. The longer computational time is in fact well spent since the evaluated feature coefficients can be used not only for the adaptive bandwidth value determination but also for segmentation and feature extraction. As sharp feature information is well preserved in the estimated normals, further post-processing of the scanned point cloud for sharp feature recovery may not be required.The proposed method does not require normal orientation information. However, the lack of the normal orientation information makes it challenging to identify the proper neighborhood for a point when an irrelevant surface is close-by. The neighboring points on the irrelevant surface can be penalized by the residual weight but these points may still contribute to the normal estimation if the fitted residuals are not large enough. If the normal orientation information is available, these neighbors can then be effectively penalized by the normal difference weight since the normals of these neighbors usually deviate much from the normal of interest.Similar to the normals estimated by some existing methods [21,22], the normals estimated by the proposed method may produce point-based rendering results showing jagged feature lines (Figs. 13 and 17), especially for sparse point clouds. A possible solution to smooth out these jagged feature lines is to reconstruct the original smooth feature lines using the evaluated feature coefficients.