@&#MAIN-TITLE@&#
Stochastic Data Envelopment Analysis—A review

@&#HIGHLIGHTS@&#
Semi-parametric stochastic frontier analysis and chance constrained DEA are compared.Focus on general tendency in data is compared with focus on individual performance.A typical management science application is evaluated from the statistical approach.A typical statistical efficiency application is evaluated from the management science approach.Approaches to disentangling of random noise and random efficiency are discussed.

@&#KEYPHRASES@&#
Data Envelopment Analysis,Stochastic DEA,Stochastic frontier analysis,Stochastic production possibility sets,Review of stochastic DEA,

@&#ABSTRACT@&#
This paper provides a review of stochastic Data Envelopment Analysis (DEA). We discuss extensions of deterministic DEA in three directions: (i) deviations from the deterministic frontier are modeled as stochastic variables, (ii) random noise in terms of measurement errors, sample noise, and specification errors is made an integral part of the model, and (iii) the frontier is stochastic as is the underlying Production Possibility Set (PPS).Stochastic DEA utilizes non-parametric convex or conical hull reference technologies based upon axioms from production theory accompanied by a statistical foundation in terms of axioms from statistics or distributional assumptions. The approaches allow for an estimation of stochastic inefficiency compared to a deterministic or a stochastic PPS and for statistical inference while maintaining an axiomatic foundation. Focus is on bridges and differences between approaches within the field of Stochastic DEA including semi-parametric Stochastic Frontier Analysis (SFA) and Chance Constrained DEA (CCDEA).We argue that statistical inference based upon homogenous bootstrapping in contrast to a management science approach imposes a restrictive structure on inefficiency, which may not facilitate the communication of results of the analysis to decision makers. Semi-parametric SFA and CCDEA differ w.r.t. the modeling of noise and stochastic inefficiency. The two approaches are in spite of the inherent differences shown to be complements in the sense that the stochastic PPSs obtained by the two approaches share basic similarities in the case of one output and multiple inputs. Recent contributions related to (i) disentangling of random noise and random inefficiency and (ii) obtaining smooth shape constrained estimators of the frontier are discussed.

@&#INTRODUCTION@&#
Measuring the relative efficiency and ranking productive performance of Decision Making Units (DMUs) were originally the primary purpose of the Data Envelopment Analysis (DEA) proposed in Charnes, Cooper, and Rhodes (1978) and Banker, Charnes, and Cooper (1984). Each DMU is characterized in these models by an input vector that allows for production of a corresponding output vector. The models are based on sets of axioms that characterize unknown Production Possibility Sets (PPSs). The models constitute an axiomatic approach that explicitly states properties of the reference technology used to measure the relative performance of individual DMUs. The axioms are used to define an estimator of the PPS. Using a given data set that satisfies certain data assumptions, see e.g., Charnes, Cooper, and Thrall (1991), it is possible to measure or estimate, e.g., radial inefficiency of a given observed DMU in input and/or output direction. Indeed, the reference technology reflects both the chosen axioms and the set of observed DMUs, using the principle of minimal extrapolation.DEA was originally developed within the Management Science (MS) framework, but without any axiomatic consideration concerning distributional characteristics of the deviation of inefficient DMUs from the best practice frontier and without any specification of noise, i.e., without consideration for measurement errors, sample noise and specification errors. Any given observed set of DMUs was not seen as the result of some sampling process from a larger population. It is interesting to note that when papers on Stochastic DEA began to appear, they took off in two very different directions. One approach, initiated by Banker (1993) included statistical axioms defining a statistical model and a sampling process into the DEA framework. If the analyst accepts these rather restrictive axioms then DEA provides a consistent but biased estimator of the true frontier. Korostelev, Simar, and Tsybakov (1995a, 1995b) proved the consistency of the DEA in a more general setting and provided results on the rate of convergence. Kneip, Park, and Simar (1998) proved consistency in the multivariate setting. Models for approximating sample distributions using bootstrapping procedures were first developed in Simar and Wilson (1998, 1999) to allow for inference on the estimated efficiency scores, see also Simar and Wilson (2007) for a related bootstrapping approach that focuses on the impact from environment.The other approach, initiated by Land, Lovell, and Thore (1993), Olesen and Petersen (1995) and Cooper, Huang, Lelas, Li, and Olesen (1998) (see also Cooper, Huang, & Li, 1996; Olesen, 2006) focused on specifying a random reference technology by replacing the observed input and output data used in a DEA with DMU-specific distributions with supports being subsets of the input output space. Consequently, the performance of the DMUs was not seen as random draws from a common density on the input output space but each DMUs performance was represented by a DMU specific distribution. The theory of chance constraints was used to formalize an efficiency evaluation relative to the random best practice frontier. Some of the advocates of the first approach claim that it is difficult to identify what this second approach is estimating, because no formal statistical model with a sampling process is specified.The focus in this paper is on a number of different approaches to Stochastic DEA. By Stochastic DEA we mean an efficiency analysis using non-parametric convex hull/convex cone reference technologies based on either statistical axioms or distributional assumptions that allow for a random (estimator of the) reference technology.11We do not consider Imprecise DEA and Fuzzy DEA within the field of Stochastic DEA, since the statistical foundation for these approaches is non-existing. For a recent review of the fuzzy DEA literature, see Hatami-Marbini, Emrouznejad, and Tavana (2011).As noted by Banker (1996), the original DEA formulations assume that the included inputs and outputs are measured without noise, and do not forward any axioms on the distributional structure of deviations from a best practice frontier. The classical DEA models (e.g., the CCR-model and the BCC-model) can be interpreted as providing a deterministic frontier and are for that reason often denoted deterministic. By maintaining two additional statistical axioms Banker (1993) provides an extended environment for DEA that indeed allows for inference. However, this statistical framework comes at a cost22In Section 2.1 we provide an illustration of what we denote a MS DEA application. Specifically, we highlight with reference to a specific example inspired from Sherman and Zhu (2006) that if we apply the convenient but not very realistic assumption of a common inefficiency distribution then this will allow us to get some inference and estimate confidence intervals of the efficiency scores using the homogeneous bootstrap. However, this access to the statistical framework jeopardizes a constructive acceptance from the involved DMUs. It is probably not very convincing to argue that an outstanding performance from a bank branch with an excellent manager is “a lucky draw” and a draw from the (wrongly) estimated common inefficiency distribution. Implicitly, this type of argument assumes that bad performance is going to be a possible outcome next year.which will be discussed in details in this review.In this paper we view and interpret the notion of Stochastic DEA as comprising a number of proposed methodologies that extend the original idea or framework behind DEA in several different directions:1.The first direction extends DEA to be able to handle estimated deviations from frontier practice as random deviations.The second direction extends DEA to be able to handle random noise in the form of either measurement errors or specification errors.33Typically, in econometrics there is a distinction between (i) errors in variables and (ii) errors in equations, see, e.g., chapter 9 in Kmenta (1971).The third direction extends DEA to be able to regard or conceive the PPS as a random PPS, based on the random variation in data.Extending DEA in the first direction can, under appropriate assumptions (e.g., a statistical model and a sampling process) be handled within a statistical (or econometric) framework. A statistical framework requires an axiomatic approach to a statistical model including a specification of a sampling procedure sometimes denoted a Data Generating Process (DGP). A given set of data is regarded as a sample from a large population and a set of efficiency scores is considered one out of many possible outcomes. Consistent estimators and inference in the form of, e.g., confidence intervals have high priority.Banker (1993) shows that DEA (with one output) provides a consistent estimator of the best practice frontier as a piecewise linear monotonically increasing and concave production function, if one is willing to accept the following two additional axioms: (i) the deviations from the frontier are iid distributed on a one-sided support (only negative residuals are allowed in an output oriented model with one output and multiple inputs), and (ii) the corresponding density function is monotonically decreasing in the absolute size of the residuals. Hence, the DEA estimator can provide inference if one is willing to accept that these axioms are reflecting reality. These necessary assumptions are, however, restrictive but convenient in the sense that they allow for consistency of the DEA based estimation. Banker and coauthors have subsequently published a series of papers showing how to use parametric assumptions on the asymptotic distribution of the inefficiency residuals to define statistical tests of hypotheses related to returns to scale, input substitutability and model specification, see Banker (1996), Banker (1989), Banker and Chang (1995), and Banker, Chang, and Sinha (1994). Simar and Wilson (2002) criticize the semi-parametric assumptions used in Banker (1996) and propose a set of related non-parametric tests on returns to scale using a bootstrapping approach. Simar and Wilson (2001) propose a set of related statistical procedures for testing model structures such as (i) possible input or output aggregations, and (ii) the possible presence of irrelevant inputs or outputs in non-parametric efficiency analyses.The approaches suggested by Banker and coauthors do not include any estimation of the sampling distributions of the estimated efficiency scores. This implies that no information can be extracted on the confidence intervals for each estimated efficiency estimator. A significant and important contribution from Simar and Wilson can be found in a series of papers showing how to use various bootstrapping approaches to get approximations of these unknown sampling distributions and extract confidence intervals on the estimated efficiency scores.44There are a few successful attempts to derive the asymptotic distributions of the DEA efficiency scores, see, e.g., Gijbels, Mammen, Park, and Simar (1997) for the case of one input and one output.The popular homogeneous bootstrap proposed in Simar and Wilson (1998, 1999) is another example of a methodology providing inference in a DEA context but at the cost of a similar set of restrictive but convenient assumptions. This bootstrapping approach is relatively easy to use and works well with relatively few observations in a moderate dimensional input output space. It will, however, be argued below that these necessary assumptions in some cases involve unacceptable structure in the sense that the purpose of the efficiency analysis is violated. These assumptions are convenient by adding structure which implies that the analysis requires less data in order to provide estimators with apparent desired properties.The homogeneous bootstrap in a cross-section setting can be used to approximate the sample noise and its effect on confidence intervals. But in our opinion, in many practical applications it requires controversial assumptions to use this bootstrap, and one could suspect that the high prevalence of usage of the homogeneous bootstrap is related to the fact that the alternative, the use of a heterogeneous bootstrap, see Simar and Wilson (2000), is prohibitive, because the necessary data typically are not available or difficult to collect. It is, perhaps, in some applications tempting to impose the assumption of a common inefficiency distribution because it provides access to “the relatively easy part” of the bootstrapping apparatus and thereby, perhaps, increases the perceived scientific validity of the analysis and its results. If a homogeneous bootstrap is applied out of pure convenience then one may see discrimination (or less discrimination) between DMUs caused by a possibly wrongly imposed structure not reflecting the variation in data.From the MS perspective it is important that the involved DMUs can recognize their own data and how these data are related to the results from the performance evaluation. It is important that the DMUs are provided with intuition about the results, e.g., in the form of potential radial input savings and the set of peers providing the estimated benchmarks. Otherwise, it is difficult to get a constructive acceptance from the involved DMUs on the validity of the analysis. In many practical applications we doubt that DMUs will recognize an assumption of identical random inefficiency distributions across the involved DMUs.Below we will discuss this aspect in detail in relation to a chosen representative MS application. Another technical but nevertheless restrictive assumption requires a strictly positive probability for realizing a performance in any open neighborhood below and close to the true best practice frontier. The assumption is needed to get the consistency property of the frontier estimator and is presented, e.g., in Simar and Wilson (2007) merely as “a regularity condition”. It is postulated that this is a quite reasonable assumption with reference to microeconomic theory predicting that in a competitive environment inefficient firms will be driven out of the market55In the context of public production, it seems to be less convincing to refer to competitive pressure, driving out inefficient DMUs, as the main reason to maintain this assumption needed for consistency.The argument put forward is somewhat different from what was stated in an earlier paper (Simar & Wilson, 2001): Standard microeconomic theory suggests that with perfectly competitive input and output markets, firms which are either technically or allocatively inefficient will be driven from the market … However, in the real world, even where markets may be highly competitive, there is no reason to believe that this must happen instantaneously. Indeed, due to various frictions and imperfections in real markets for both inputs and outputs, this process might take many years, and firms that are initially inefficient in one or more respects may recover and begin to operate efficiently before they are driven from the market.in the long run.Nonparametric efficiency analyses are often applied to gauge performance of production units within the public sector. One reason for the popularity of nonparametric efficiency analyses in this context is that the analyses do not rely on the existence of output prices. However, there are several important characteristics that distinguish the public and the private sector (Lovell, 2002). The managers of public production are, e.g., typically not profit maximizers. Furthermore, it is often questioned whether or not manager behavior reflects a cost minimization objective. Lack of competitive pressure within public production sectors is seen in the literature on bureaucracy66We would like to thank one of the referees for suggesting to include this literature.(Niskanen, 1971) as an opportunity to pursue other objectives, such as budget maximization. The dynamics within some of these bureaucracy models hinge on the so-called reward structure which specifies the bureaucrat’s income and perquisites from his position as a function of two factors: (i) the output of the bureau and (ii) the so-called discretionary budget. The model creates a trade off between two extreme situations: if the main source of the bureaucrat’s income and perquisites is the size of the discretionary budget then it is shown that the bureau’s output level is optimal for government. However, it is argued that this solution implies high perquisites and inefficiency. In contrast, if the main source is the size of the bureau’s output, then we see a higher level of output produced at minimal cost. In this case the bureaucrat is not in a position to appropriate any of the discretionary budget.A nonparametric efficiency analysis performed in the context of public production characterized by such phenomena is clearly a challenge. Assuming a statistical model with a true best practice frontier the corresponding DGP would have to reflect the fact that the reward structure probably implies a mixture between the two extreme situations mentioned above. The regularity assumption of a strict positive probability for realizing a performance in any open neighborhood below and close to the true best practice frontier can for this reason be highly problematic.Applying a homogeneous bootstrapping procedure is not necessarily meaningful if the efficiency analysis is a short run analysis designed to get information to managers of productive units on how to improve performance next month or next year. A given very talented manager may find it difficult to accept confidence intervals based on a view that his outstanding performance is simply a consequence of a lucky draw and that he next year may be unlucky enough to draw a large inefficiency residual. The response to such an objection from advocates of the statistical framework will probably be that if the inefficiency is a consequence of lack of management talent then data measuring or correlated with this phenomenon should be included in the analysis, e.g., as a structure that influences the moments of a DMU specific inefficiency distribution. This is from a theoretical point of view of course a valid objection, but in reality data of this kind are rarely available and as performance analysts we are often forced to omit such relevant variables.77It is well known from econometrics that the omission of relevant variables in a simple regression model implies bias and increased noise if the omitted variable is correlated with the included variables. Hence, part of the impact is absorbed into the noise term. In an SFA approach with an error term composed of both noise and inefficiency an interesting problem is, whether such an omitted variable increases the noise part or the inefficiency part or both. In the situation outlined here the omitted variable will probably increase both the noise and the inefficiency part, since the omitted variable is (assumed) strongly correlated with the inefficiency term.Extending DEA in the first and second direction complicates matters in a very well known manner.88In the context of panel data extensions in the second direction only can be found in Gong and Sickles (1992) and Ruggiero (2004). Both papers take advantage of panel data to correct for measurement error by averaging over time. The first paper takes averages over efficiency scores over time while the second paper takes averages of inputs and outputs over time before doing DEA.This is the situation characterizing the Stochastic Frontier Analysis (SFA) proposed in Aigner, Lovell, and Schmidt (1977) and Meeusen and van den Broeck (1977). The specification of the randomness in the basic statistical model and the randomness in data imply that random inefficiency and random noise interact and need to be disentangled.99One could argue that the parametric or the semi-parametric SFA approach implicitly defines a random PPS and thereby also is related to the extension in the third direction. It is at least correct that it is fairly easy to obtain this random PPS. We will return to this aspect of SFA in Section 3. However, a random PPS is not at focus in, e.g., Aigner et al. (1977). In fact, the fact that the PPS is random is creating unusual problems in the form of “non-standard” efficiency measures that depend on an unobservable noise part in the composed error term. In a footnote the authors underline, that measuring efficiency relative to a stochastic frontier is a strange efficiency measure since it is stochastic and depends on this unobservable noise part of the residual.In a DEA context a similar semi-parametric model was introduced in Banker and Maindiratta (1992). The authors show how to interpret a simultaneous estimation of all DEA residuals relative to a non-parametric frontier specification where the residuals are derived based on a maximization of the likelihood function reflecting a specific distributional structure of the composed residuals. Sarath and Maindiratta (1997) prove consistency of the estimators in Banker and Maindiratta (1992). This approach has been refined in several ways by Kuosmanen (2008), Kuosmanen and Kortelainen (2012), and Kuosmanen and Johnson (2010).These successful interpretations of DEA inspired models as a semi-parametric estimation procedure of a stochastic frontier have clearly narrowed the gap between the DEA and the SFA approach for efficiency and productivity analysis. As stated in Fried, Lovel, and Schmidt (2008)“Both techniques are more robust than previously thought. The gap is no longer between one technique and the other, but between best-practice knowledge and average practice implementation. The challenge is to narrow the gap”1010As suggested by one of the referees this gap between best practice knowledge and average practice implementation is at least partly due to lags in the development of software. Many researchers focusing on applied research prefer dedicated software routines like, e.g., SFA or DEA modules within Excel/VBA, R-libraries, Matlab-libraries, Limdep, Stata, SAS, etc. It is more demanding from a computational perspective to apply recently developed nonparametric frontier estimators like Stoned, CNLS or the smoothed frontier estimators summarized in Section 5 below. However, some collections of GAMS-code and Matlab-code are currently available. For recent introductions with explicit links to such collections in relation to CNLS and Stoned, see Kuosmanen, Johnson, and Saastamoinen (2015) and Johnson and Kuosmanen (2015). Online support material to Kuosmanen, Saastamoinen, and Sipiläinen (2013) includes a Gams setup of the reported simulation. Some additional Gams code is available from http://www.nomepre.net/stoned/codes.htm. Andy Johnson has a section of StoNED, CNLS and kernel weighted convex regression on http://www.andyjohnson.guru/code. R code to replicate an example on nonparametric kernel regression is provided in the appendix of Racine, Parmeter, and Du (2009).. And in Cooper and Lovell (2011)“The distinction between “ stochastic but parametric” and “ non-parametric but deterministic” has blurred almost to the point of extinction”.It has always been a challenge to estimate inefficiency within an SFA model. Typically, in a cross-section context a Jondrow, Lovell, Materov, and Schmidt (1985) estimator is used based on the mean inefficiency conditioned on the composite residual. However, as noted in Greene (2008) this is not without problems. Greene argues that in a cross section context the estimator from Jondrow et al. (1985) cannot be used, as suggested in the literature to estimate confidence intervals for the inefficiency part of the residuals.1111“The JLMS estimator is not a consistent estimator of uit either, again for the reasons noted above. No amount of data can reveal uit perfectly in the stochastic frontier model. It can in a panel-data model in which it is assumed that uit is time invariant, since then, like any common-effects model, a method-of moments estimator of the“ effect” is consistent in T. But, absent this possibility, the JLMS estimator does not converge to uit”. Greene (2008, p. 179).“Although the received papers based on classical methods have labeled this a confidence interval for ui, I emphatically disagree. It is a range that encompasses 100(1 -α) percent of the probability in the conditional distribution of ui|εi. The range is based on E[ui|εi], not ui itself. This is not a semantic fine point. Once again, note that, in a sample that contains two identical observations in terms of yi, xi, they could have quite different ui, yet this construction produces the same “interval” for both. The interval is “centered” at the estimator of the conditional mean, E[ui|εi], not the estimator of ui itself, as a conventional “confidence interval” would be.”Greene (2008, , p. 185).As noted above, the restrictive assumptions typically employed in SFA on functional form and distributional characteristics on the distribution of the inefficiency may in some cases involve unacceptable structure in the sense that they will potentially violate the purpose of the efficiency analysis. These assumptions are convenient in the sense that they allow for an estimator of the expected value of inefficiency given the composed error. However, as shown in Ondrich and Ruggiero (2001), rankings based on this conditional expectation of the inefficiency simply reflect rankings based on the total composed error term. It is argued that if the true inefficiency is distorted by the presence of noise then we would expect to see higher rank correlation between the estimated and the true inefficiency if the estimates are based on the conditional expectation of the inefficiency compared to estimates based on the total composed error. However, these rankings seem to be identical. In addition, notice that typically one common inefficiency distribution is assumed. Hence, much of the discussion and critique of this assumption mentioned above is valid here as well.Extending DEA in the second and third direction can, under appropriate assumptions on the specific distributional randomness in performance be handled within anMS framework. One type of this kind of extensions of DEA is the Chance Constrained DEA (CCDEA). One possible interpretation of these approaches is that we replace the basic assumption of having observed relevant inputs and outputs with an assumption that we have information on random disturbances in data in the form of knowledge or approximate knowledge of the distributions involved. At first glance such an assumption seems to imply that CCDEA is excessively data demanding compared to e.g. SFA, because estimators of these distributions require a lot and very detailed data. However, the MS tradition to performance evaluation typically relies on management involvement in the analysis, and at least in some applications, see e.g. Sueyoshi (2000), this allows management judgements to be used in the analysis,1212Assurance Regions, see Thompson, Langemeier, Lee, Lee, and Thrall (1990) and Cone-Ratio models, see Charnes, Cooper, Huang, and Sun (1990) have been developed within the MS framework to allow the inclusion of this type of “data”.and a rough description of random characteristics could to some extent originate from this type of “not normal data”. However, it will be argued that the imposed assumptions will in some cases involve an inconsistent structure. The advocates of the statistical framework have often highlighted that it is very difficult to identify a coherent DGP comprising (i) DMU specific random input and output data and (ii) a random best practice frontier derived from a random PPS. In other words, there seems to be a disconnect between how random data are represented and the axiomatic approach defining the random best practice frontier.A related peculiar aspect of these types of extensions, found e.g. in Olesen and Petersen (1995) and Cooper et al. (1998), is the fact that the authors are very silent on the matter of how to measure or estimate inefficiency. A random PPS is relatively easily established, but it is not an easy task to bring into play a measure of the random deviation from the (random) best practice boundary part of the PPS. As argued below in Section 3 the “solution” suggested is simply to limit the efficiency analysis to a test of whether or not any given DMU's performance distribution contributes to the spanning of this random best practice boundary part of the PPS.As indicated in the Introduction there are different views and opinions on how to extend DEA to situations where we want to impose either statistical axioms or distributional assumptions that allow for a random (estimator of the) reference technology. Seen from the MS framework, within which the CCR and BCC models were originally developed, DEA focuses on measuring or estimating deviations of inefficient DMUs from the best practice frontier, but without any reference to a specific statistical model involving a specific DGP. Stochastic DEA related to the MS framework focuses on incorporating measurement error (and possibly random inefficiency) into the specification of the reference technology, e.g., by replacing the observed input output observations with DMU specific distributions in input output space.The statistical (or econometric) framework insists on an axiomatic approach to a statistical model including a specification of a DGP. Data are regarded as a sample from a large population. The axioms specify how the inefficiency deviations from a true frontier are generated. The framework insists on a consistent estimator of the true frontier to allow for an estimation of the impact from sampling noise on the estimated efficiency scores.To illustrate the substance behind these differences in arguments and the critical attitude of each of these two “competing” frameworks toward the “other” framework we will now focus on a typical MS (DEA) application and a typical econometric DEA related application of a nonparametric frontier estimation. For each of these two applications we will include critical remarks from the competing framework. This will highlight the impact of the included axioms and necessary assumptions, and how these axioms and assumptions may introduce problems with the usefulness of the obtained results.Managing bank productivity is at focus in Sherman and Zhu (2006). A DEA analysis is used to evaluate the performance of 33 bank branches based on five inputs and five outputs.1313The five inputs are (i) customer service (tellers), (ii) sales service, (iii) manager personnel (FTE), (iv) other expenses, and (v) physical size of branch. The five outputs are (i) deposits, withdrawal and checks cashed, (ii) bank checks, traveler checks, bonds: (sold, redeemed or coupons), (iii) night deposits, (iv) mortgage loans, customers loans: (referrals, applications and closings), and (v) new accounts: (time, savings, certification of deposits).Ten branches turn out to be efficient in a CCR efficiency analysis. The obtained efficiency scores were subsequently used to highlight what work patterns were associated with best practice. From field visits several interesting aspects of the less than best practice were discovered. Although the job of the managers was defined consistently across the branches, it was noticed that the managers displayed different styles. Differences in the management and rewarding of employees resulted in different performance. It was also noticed that several branches on the best practice frontier relied on cross-training of employees and a noticeable difference in use of part time employees between best practice and less than best practice was noticed.We will now illustrate typical critical remarks from the competing statistical framework, and how these critical remarks may “translate” into “missing” modelling structure. We extend the application to a stochastic context by assuming specific distributions on the inefficiency of the branches. Let us focus on only five hypothetical branches and assume that the performance of these branches is stochastic that the inefficiency follows double truncated normal distributions, and that randomness is only related to inefficiency. This is presently a possible context within the statistical framework that allows for the use of bootstrapping for getting inference.Specifically, we assume that two outputs are represented in polar coordinatesy=(η,ω)where η is the angle (the output mix) and ω is the modulus1414The output vector (y1, y2) is converted to polar coordinates as follows:tan(η)=y1y2,ω2=y12+y22.and that every branch is using the same amount of only one input. We assume that ω is distributed according to a double truncated normal distribution (mean μ and variance σ2), where this normal distribution is truncated to the interval [a(τ), b(τ)] with end points depending on a management quality variable τ ∈ [0, 1]. τ is a measure of the management talent at each bank branch. We use the following notation to indicate this distribution: N[a(τ), b(τ)](μ, σ2). The inefficiency distributionsN[a(τj),b(τj)](μj,σj2),j=1,…,5of the five branches are assumed to be characterized as indicated in Table 1.We assume a DGP that specifies a random modulus ωj, conditioned on the choice of output mix ηj.ωj=ωf+ωjineff,whereωf=8is the frontier output, andωjineffis a random draw fromN[a(τj),b(τj)](−1,1),j=1,…,5.The design of this stylized example partly reflects the discussion of the application in the paper and the subsequent discussions with the branches being evaluated. As indicated above, it is explained how the subsequent field visits and the discussion with managers revealed several interesting “explanations” for the inefficiency. It is argued that management style varies and that this variation has an important impact on the performance. In the short run the allocation of the existing managers across branches is fixed and nondiscretionary for the headquarter. However, layoff of less competent managers is an option for headquarter, seen over a longer period or time.This situation is illustrated in Fig. 1, which depicts the five DMUs in the output space, each with an inefficiency distribution reflecting the assumptions in Table 1. We assume in the context of the example two distinct structures. First, we have an “inefficiency-environment” variable τ that affects the support of the inefficiency distribution.1515If we for the moment consider τ as an observable environment variable then this structure will on purpose violate the separation condition mentioned in Simar and Wilson (2007, pp. 36–37) and further analyzed in Simar and Wilson (2011). Interesting, in this survey we propose exactly the opposite of what Simar and Wilson are advocating. They forward the opinion that a second stage regression is only valid if the true PPS is independent of environment. In other words, the environment is only affecting the inefficiency distribution and not the support of the distribution. What essentially happens in the stylized example is that the follow up analysis looks for explanations of low inefficiency—without needing an assumption of inefficiency supports being independent of management talent.Recently, Badin, Daraio, and Simar (2012) have extended this modeling approach. They state: The paper contributes … It first extends previous research on conditional measures by showing that a careful analysis of both full and partial conditional measures allows us to disentangle the impact of environmental factors on the production process in its two components: impact on the attainable set in the input output space, and/or impact on the distribution of the efficiency level. To the best of our knowledge this was never developed before.Second, we assume, based on the information from the case that this manager talent variable τ is an unobservable variable or at least a variable that is very difficult to measure. However, we include this variable in the discussion since, conceptually it is well known that excellent management competencies are crucial for best practice.Inspired from the application in Sherman and Zhu (2006) , let us imagine that a DEA performance evaluation is based on a data set with one realization from each of these five branches. DMU 1 and 5 will probably turn up as spanning the frontier, while the three other DMUs will probably get an efficiency score greater than one, indicating that an output expansion seems to be possible. Let us imagine that the headquarter, as a consequence of the performance evaluation, decides to move, for a short period of time, managers from high performing branches to low performance branches, and vice versa. In the example, this change of management at the branches will of course affect the support of the inefficiency distributions. For example, the true inefficiency distribution for DMU 2 will have a strict positive probability for a realization in every open subset close to and below the frontier along the relevant output mix, if we swap managers between branches 1 and 2.This stylized example is on purpose constructed such that it violates the assumption of one common frontier with a density specifying a positive probability of achieving best practice for any output mix. In addition, the inefficiency distributions are very different, which prohibits the use of the homogeneous bootstrap.The response to this setup from advocates for the statistical framework is probably that the particular five realizations observed and used in the DEA is just a sample, and that the sample distribution of the efficiency scores should be estimated or approximated, e.g., using a bootstrapping procedure. Clearly, we cannot use a homogeneous bootstrap in the situation outlined here, and in this particular application we do not have the data available to estimate the density necessary to apply a heterogeneous bootstrap, see Simar and Wilson (2000). Hence, the application is caught in a dilemma.One approach is to apply the convenient but not very realistic assumption of a common inefficiency distribution. This will allow us to get some inference and estimate confidence intervals of the efficiency scores. But this inference comes at a very high price! The whole analysis is discredited in the eyes of the people whose performance is being evaluated. It is probably not very convincing to argue that an outstanding performance from a branch with an excellent manager is “a lucky draw” and a draw from the (wrongly) estimated common inefficiency distribution corresponding to a very bad performance is going to be a possible draw next year.A second approach would focus on incorporating management talent directly into the inefficiency distribution. In the context of an environment vector z this is suggested in Simar and Wilson (2007) . This paper suggests a DGP based on a joint density covering inputs x, outputs y, and environmentz∈Rp. Hence a given sample (xj, yj, zj), j ∈ N are realizations of iid random variables with a density function f(x, y, z) which has a support overP×Rp,where P is the true PPS. Using an output orientation, y is represented in polar coordinatesy=([η1,…,ηs−1],ω)and the joint density is broken down as usual as:(1)f(xj,[η1,…,ηs−1]j,ωj,zj)=f(xj,[η1,…,ηs−1]j|ωj,zj)×f(ωj|zj)×f(zj)The focus in Simar and Wilson (2007) is a meaningful DGP that allows for a second stage regression explaining the radial efficiency estimates by environment. For that reason the authors carefully avoid any impact from environment on the support of the inefficiency distribution.1616The authors need an identical support of the inefficiency distributions for all DMUs regardless of environment. They write: In our formulation, the environmental variables z influence the mean and variance of the inefficiency process, but not the boundary of its support (Simar & Wilson, 2007, footnote 4). They suggest an output expansion θjworking through a functionθj=ψ(zj,β)+ɛj≥1.In the stylized example above the interval that defines the double truncation is determined from the management talent τ, which could be modelled as an environment variable.1717Notice, that it is not clear whether or not τ is part of an environment being nondiscretionary to headquarter and the branches. Often, management quality is regarded as a discretionary characteristic.Hence, one could argue that focus should be on the common conditioned inefficiency distribution f(ωj| τj) where we control for management talent and not the unconditioned distributionf(ωj|τj)f(τj)=f(ωj,τj). In principle that would probably be “the correct thing to do”. If we condition on the best management talent then in terms of Fig. 1 it would correspond to changing the truncation intervals to e.g.[−10,0]for all five DMUs. This situation is illustrated in Fig. 2, where the fact that inefficiency distributions now are identical for all output mixes allows for inference from a homogeneous bootstrap. But notice, manager talent is typically an unobservable variable. Conceptually we know that excellent management competencies are crucial for best practice. If the structure outlined here reflects reality then this fact needs to be incorporated into the DGP used to get inference. Indeed, if an unknown part of the inefficiency, as in this stylized example, is not random, and if the variation in this unobservable determines the support of the random part of inefficiency then clearly this setup does not fit the traditionally stated DGPs, typically used within the statistical framework. These critical remarks are related to the suggestions in Simar and Wilson (2007, 2011) and recently explored in Badin et al. (2012), a paper trying to disentangle the impact from environment on (i) the supports of the inefficiency distributions (which by assumption spans the true PPS (no noise is present by assumption)), and (ii) the impact from environment on the shape of the inefficiency distribution.An applied performance evaluation within the statistical framework is characterized by several unique features. First of all, the statistical framework requires an axiomatic approach to a statistical model and a DGP. Hence, any given set of data of inputs and outputs is regarded as a particular sample from a large population. A set of efficiency scores from a specific DEA is considered one out of (infinitely) many possible outcomes. Inference in the form of confidence intervals on the efficiency scores is estimated using typically a bootstrapping procedure, since closed form expressions of the sampling distribution rarely are available.The statistician asks questions like: What can be learned by observing a sample of n observations? How can this information be used to get interval estimates of the efficiency and the best practice frontier? Statisticians typically insist that the analysis should provide information on statistical properties of the estimators of interest (e.g., the best practice frontier). Consistency and asymptotic unbiasedness of the frontier estimator are the most important properties in the context of this survey.1818As discussed in the previous section, using the MS perspective implies that the analyst typically is not that concerned with asymptotic properties of the estimation. The number of observations included in the sample is typically seen as a consequence of data available, with a specific concern about problems with non-homogeneous DMUs.Specifically, the statistical framework insists on axioms providing (i) a statistical model, (ii) a so-called regularity condition1919This regularity condition states that the density is strictly positive on the boundary of the PPS. We have already discussed this condition in the Introduction and in the previous subsection. In our opinion, this is not just a technical assumption that is trivially fulfilled in most applications. This condition is a restrictive assumption that may well be violated in at least some applications within the MS framework, as illustrated in the previous section.needed to get consistency of the estimator of the PPS, and (iii) a DGP. In Simar and Wilson (2008) it is argued that a convenient axiom that can be used to provide inference is to assume that the sampling process (the DGP) works through independent draws from the relevant density defined in the statistical model. However, it is also noted that there exists a well known fundamental trade-off: the probability model must provide strong enough assumptions to permit estimators with desirable properties but “yet not so strong as to impose conditions on the DGP that do not reflect reality”, p. 424, our emphasizing.Related to practical applications there is often a trade off between sample size and realism and validity of the results from an analysis based on this sample. Including too many (and too different) data points in the sample will provide estimators with more information (less sample noise), but these properties may come at the cost of decreasing validity of the results as a consequence of violation of the assumptions of a common technology and perhaps a common random inefficiency process.The general idea behind the (homogeneous) bootstrapping procedure in a cross-section context is as follows: The axioms provide a statistical model, which in the case of the application presented below, is a joint density defined on the support of the PPS. In addition, the axioms provide a DGP which is completely determined from this density function and typically some additional assumptions on the sequence of decisions on input and output mixes and volume characterizing all DMUs. In the true world, the PPS, the joint density and the DGP are unknown.From a BCC analysis we know the position of the projected data points on the boundary of the convex hull estimator of the PPS. Under some restrictive assumptions, we can use the estimated efficiency scores to get an estimator of the part of the density that relates to the random inefficiency. For example, we can use the estimated output oriented radial efficiency scores to get an estimator of the conditional density of the modulus in output space conditioned on the output mix and any given input vector. From these estimators (which typically rely on axioms with the desirable property of providing consistent estimators) we define a virtual/simulated/bootstrapped world. In this world the estimators of the PPS and the estimated inefficiency distribution play the role as the true PPS and the true density. Next, we now mimic the true world by a simulation experiment from the virtual world. For any given sample size and for any fixed observation we (i) draw with replacement from the virtual world and (ii) do a BCC analysis and get point estimates of the inefficiency of this fixed observation. We repeat this experiment B times and the outcome of this exercise will be B efficiency scores that can be used to approximate the sample distribution for this fixed sample size for this particular fixed observation.As a representative application within the statistical framework we focus on Mouchart and Simar (2002, 2003)2020We would like to thank L. Simar for having provided us with copies of these two consulting reports., an application highlighted both in Simar and Wilson (2008) (Section 4.3.5.5) and in Daraio and Simar (2007). The focus is on the technical efficiency of European air traffic controllers in 2002 analyzing 37 units and including four outputs and two inputs.2121The four outputs are (i) total flight hours controlled, (ii) number of air movements controlled, (iii) number of sectors controlled, and (iv) sum of sector hours worked. The two inputs are (i) the number of air controllers (FTE), and (ii) the total number of hours worked by air controllers.However, the authors are faced with the all too well known problem of few observations and many inputs and outputs.The analyst is faced with the trade-off mentioned above. Using the data without any aggregation provides an analysis where the results are easily recognized in a subsequent discussion with the aircontrollers. However, with this small number of observations a bootstrapping procedure using the data in the full six dimensional input outputs space would probably inflate the confidence intervals compared to the alternative chosen in the application, namely to aggregate the six inputs and outputs into one input and one output.2222The authors include in Mouchart and Simar (2003) an analysis of the efficiency with three different aggregation structures. First efficiency scores are estimated based on both inputs and all four different outputs. Second, only one input is used together with the first output and an aggregate of the last three outputs. Finally, one aggregate input and one aggregate output are used. The authors argue that in the first model “quality of the estimators is questionable due to the small number of observations”, (Mouchart & Simar, 2003, p. 21). They argue that since the difference of the efficiency scores from the two last models is quite small, they proceed the analysis with only one aggregate input and one aggregate output.Clearly, if the two inputs and the four outputs were perfectly correlated then it would not matter to do the analysis with or without the aggregation. However, looking closely at Table 4, p. 22, this is clearly not the case.In this situation there seems to be at least two strategies.One approach is to acknowledge that the amount of data points compared to the number of dimensions of the inputs output space does not allow for a reasonable estimator of a common inefficiency distribution. Too many data points will get an efficiency score of one, and this phenomenon is not a consequence of structure in data but is reflecting the slow convergence. We lack data to get an estimator of the PPS reasonably close to the true frontier. Indeed, the curse of dimensionality is probably blocking for inference useful for discriminating between significant best practice and inefficient performance. Notice, if all observed data are contained in a lower dimensional linear subspace, and if it assumed that the DGP reflects this fact, then we only need an estimator of the true frontier intersected with this linear subspace (assuming radial efficiency scores). However, this is rarely the case in real life applications.A second approach is to adopt an aggregation of the four outputs and two inputs. However, as shown in Färe and Lovell (1988) for any true technology, any aggregation will distort the radial efficiency unless the technology is homothetic. Nevertheless, this is the approach suggested in Mouchart and Simar (2002). According to Daraio and Simar (2007)“(t)he basic principle … is to find an “input factor” , a linear combination of the inputs, which best summarizes the information provided by all the inputs”. Interestingly, the advocates of this second approach are apparently content and satisfied with an efficiency analysis based on the “general tendency” in the input and the output data. The resulting analysis is only providing correct results if all 37 DMUs are producing the output in exactly the same mix while consuming the two inputs in exactly the same proportion. The correlations between the inputs and between the outputs are reported to be high, but the fact that they are not perfectly correlated will imply an unfair performance evaluation of the DMU located with the highest angle-difference compared to the ray-vector used to perform the linear aggregation.2323The linear aggregations used in Mouchart and Simar (2002) utilize (scaled) principal components to extract the fixed weight vectors. Such a linear aggregation of the two inputs and the four outputs using fixed weight vectorvaggr∈R+2anduaggr∈R+4corresponds in terms of the BCC model to imposing assurance regions stating that the virtual multiplier(u,v)=μ1(uaggr,0)+μ2(0,vaggr),μi≥0,i=1,2.The microeconomic interpretation of the aggregation is an assumption of linear isoquants in input and in output space. Clearly, blocking for any curvature of the isoquants will imply a possible exaggeration of the amount of inefficiency. Notice, that this implies that the estimated radial distances exaggerate the amount of inefficiency as a consequence of the modelling structure (the overly restrictive assumption on the shape of the isoquants) and NOT the variation in data.Remarkably, the statistical framework has so far neglected the possibility of using less restrictive aggregation structure, e.g. the probabilistic assurance regions suggested in Olesen and Petersen (1998).In a sense, it makes sense that a statistical framework is more concerned with linear combinations of inputs and outputs which best summarize the information provided by all DMUs. The statistician is not concerned about the individual residual. The individual data point is not at focus, because data are regarded as a sample affected by sample noise from a particular DGP. Hence, the statistical (or econometric) approach to estimating and testing technology characteristics is typically to see to what extent the general tendency in data questions a given hypothesis. However, when the residual is composed of both noise and inefficiency then it becomes problematic to disregard atypical residuals and focus on the general pattern because sometimes the atypical observations provide valuable information of potential performance improvements. Notice however, that there seems to be an important difference between testing general technology characteristics and estimating confidence intervals for inefficiency of each of the observed data points. In the latter case we have focus on each observation, which in our opinion makes it more dubious to introduce an aggregation that favors some points but not others, by referring to what structure best summarizes the information in data.To extract what structure that best summarizes data, the statistical framework focuses in this application (and in applications in Daraio & Simar, 2007) on (scaled) principal components. It is argued that typically in efficiency analysis a strong correlation exists among all inputs and among all outputs, and that as a consequence we can provide sensible two dimensional pictures, without losing too much information(Daraio & Simar, 2007, p. 145).2424“Indeed the curse of dimensionality implies that working in smaller dimensions tends to provide better estimates of the frontier” (Daraio & Simar, 2007, p. 148). “(I)t is certainly appropriate to summarize the information of the full data matrix by these two one-dimensional factors, without loosing too much information” (Daraio & Simar, 2007, p. 149). “This analysis shows that we may describe the production activity of all these units by only one input factor and one output factor” (Daraio & Simar, 2007, p. 149).What is the meaning of the phrase “without losing too much information”? As mentioned above, the statistician is not concerned with the individual residual, because the residuals in econometrics typically consist of noise components of different types. However, in the efficiency analysis presented in the application the residuals consist of inefficiency. Hence, to provide a precise and “fair” performance evaluation each of the residuals should be estimated with equal care. In our opinion it is problematic to disregard atypical residuals with an argument that only little information is lost and aggregation is essential because of the curse of dimensionality. Assume that data are sampled from a true DGP based on a technology with isoquants that are highly non-linear. Since the correlation between the inputs and between the outputs are reported to be very high we would expect that the DGP specifies a very high probability of selecting an input mix and output mix close to a common input mix and output mix. A linear aggregation corresponds to an implicit (and problematic) assumption of linear isoquants. However, the true isoquants are, here by assumption, highly nonlinear, which implies that the linear approximation will only be reasonably accurate close to the common input mix and the common output mix. Since the correlation is not perfect we cannot avoid that units with an input and an output mix relatively far from the common mix will get an overly pessimistic radial efficiency estimate. Such units are being penalized for not producing with input and output mix close enough to what is commonly chosen among the evaluated units. Apparently, within the statistical framework it is legitimate to penalize such atypical units because it allows for a reduction of the dimensionality and thereby allows for “better estimates of the frontier”, see footnote 24. Only typical DMUs are allowed to define the linear aggregation procedure necessary to get the subsequent bootstrapping procedure to work.The argument why this procedure is valid relates to the logic behind the statistical framework. Testing whether or not an aggregation is acceptable within a statistical approach naturally relates to whether or not the obtained fit of the model changes significantly with and without the aggregation. This is exactly what is suggested in Simar and Wilson (2001). By setting up two models, one with and one without the proposed aggregation, the authors argue (p. 169) that the test should decide “whether the differences betweenD^oout(x+,y)andD^out(x,y)are large enough to cast doubt on the null”, Ho, whereD^oout(x+,y)andD^out(x,y)are the inverse output distance functions evaluated at (x, y) and at(x+,y)relative to the convex hull estimator of the PPS based on the observed input output combinations with and without a specific input aggregation. Hois1≥Dout(xa,y)=Dout(x,y),∀(x,y)∈Tand H1: 1 ≥ Dout(xa, y) > Dout(x, y) for some (x, y) ∈ T, where xais an aggregated version of the input vector x and Dout(x, y) is the ratio of the moduli of (x, y) and the projection of this point to the true frontier.However, seen from an MS perspective, this may be unacceptable. An interesting part of the efficiency analysis from the MS perspective is the subsequent discussion with the involved DMUs, comparing and searching for explanations for best practice performance compared to inefficient performance. A crucial aspect of this part of the analysis is that the (self-reported) performance data provided by the DMUs is included in the analysis. Indeed, it is often the DMUs that are doing things differently compared to “how we all do it” that provide the most interesting benchmarks. Focusing on the general tendency in data implies a potential danger that few “outliers” will perhaps not be presented in a fair way in a performance evaluation based on the statistical framework, because within this framework we rely on “how we used to organize production” and not how a few experimenting DMUs are performing.Let us now turn the attention to areas where we find extensions that comprise all three directions mentioned in the Introduction, i.e., (i) extensions capable of handling estimated deviations from the frontier practice as random deviations, (ii) extensions that are able to handle random noise in the form of either measurement errors or specification error, and (iii) extensions that focus on the defined PPS as a random PPS based on the random variation in data. A prominent approach to be interpreted as providing an extension in all three directions is the semi-parametric SFA proposed in Banker and Maindiratta (1992) (BM92) and further developed2525Several of the recent developments of the semi-parametric SFA proposed in Banker and Maindiratta (1992) were anticipated in this paper from 1992.in Kuosmanen and Kortelainen (2012), Kuosmanen (2008), and Kuosmanen and Johnson (2010). In Section 3.1 we show some of the details that justify this conclusion and we relate these models to CCDEA models. An interesting working paper Banker (1988) proposes a related semi-parametric SFA based on a minimization of the sum of the absolute value of all composed error terms subject to the same type of Afriat constraints used in (BM92). Some of the main results from this paper have recently been published in Banker, Kotarac, and Neralic (2015).Applications of these semi-parametric SFA models are limited. Banker (1988) has been applied to software maintenance projects in Banker, Datar, and Kemerer (1991) and the StoNED model proposed in Kuosmanen and Kortelainen (2012) has recently been applied to the Finnish electricity distribution networks and to bank performance in Kuosmanen et al. (2013), Kuosmanen and Kortelainen (2012), and Eskelinen and Kuosmanen (2013).It is argued in the Introduction that the CCDEA approach proposed by Cooper et al. (1998) extends DEA in the second and third direction. The authors of this paper propose a Stochastic DEA approach by explicitly modelling a random PPS. However, it has often been argued2626See, e.g., Simar (2007).that it is difficult to identify a DGP that relates to this CCDEA approach. Indeed, no sampling noise is included explicitly in Cooper et al. (1998), and the random variation in data are mainly reflecting measurement error. The authors claim that the proposed approach relates to the SFA approach with a composite error structure. We will, therefore compare and relate these approaches to one another. It turns out that Banker and Maindiratta (1992) provide an approach that makes this possible.The CCDEA approach proposed by Olesen and Petersen (1995) also extends DEA in a similar fashion in the second and third direction, but working from the multiplier space the certainty equivalent of the random PPS is expressed as the intersection of halfspaces supporting confidence regions in input output space at any prespecified set of probability levels above 0.5. Observations are allowed to be positioned outside confidence regions, and hence possibly on the wrong side of the frontier because of random noise in data. Somewhat imprecise it is argued in the paper that “The specification of the model allows for a decomposition of the impact of the disturbance terms on the total variation in data for DMUj into the two complementary components (1) variation caused by noise and (2) variation caused by efficiency. The split of the total variation in data are determined by the choice of the probability level Ωj, and the variation in data caused by stochastic variations in efficiency relates to the confidence region”, p. 451. Wei, Chen, and Wang (2014) present an extension using this CCDEA model and combining it with reliability constraints.In this section we will focus on a comparison of the explicitly defined stochastic PPS in Cooper et al. (1998) and the corresponding stochastic PPS in BM92.Notice first that in the classical parametric SFA a random PPS, based on the random variation in data, is actually easy to obtain and consequently2727Typically, there is little difference between the imposed distributional structure of the composed error term in a parametric (classical) SFA and the recent semi-parametric SFA models. In Section 4 we will, however, look at some recent truly nonparametric approaches that allow for disentangling of random noise and inefficiency in a cross-section context.also easy to obtain from the recent semi-parametric SFA. One of the peculiar characteristics of the SFA approach is the notion of a stochastic frontier, which of course implies that the PPS is random. Let the DGP be specified asy=f(x;β)+v−u,where u and v are random inefficiency and noise, respectively. The stochastic frontier is consequently defined asf(x;β)+v. Notice that normally productive efficiency is defined relative to the frontier, which in this case is random. This peculiar aspect causes Aigner et al. (1977) to volunteer the following statement: “Another implication of this approach is that productive efficiency should, in principle, be measured by the ratioyif(xi;β)+virather than by the ratioyif(xi;β)”, p. 25. However, a random PPS is indeed not at focus in this classical paper on SFA. In a footnote the authors underline that this first “ratio” is, indeed, a strange efficiency measure. “As defined, … is a strange efficiency measure, since it is stochastic and depends on an unobservable, vi. It is offered here only to support the argument”.Hence, there seems to be a reluctance to define an efficiency measure relative to the stochastic frontierf(xi;β)+vi. Focus is on measures relative to the deterministic “core” f(xi; β) and this is actually why we are of the opinion that parametric SFA, belonging to the statistical framework, neglects the importance of a focus on a random PPS. This aspect of SFA is of course closely related to the problem related to disentangling noise and inefficiency in an SFA model. Hence, one could argue that the Jondrow et al. (1985) estimator is an attempt to define an efficiency measure relative to a stochastic frontier.Let us derive the stochastic PPS implicitly defined in BM92.2828In the comparison we simplify the presentation of BM92 by omitting the environment variable Z.We assume that we have a random sample of n firms, given by the data(Yj,Xj)∈R+×R+m,j=1,…,n. We assume that data are generated according to a DGP that is very similar to the SFA process. Based on the choice of inputsx∈R+mthe output y is generated randomly according to a production functiony=φ(x)e−uev,where u and v are random variables, u ≥ 0 with means and standard deviations:u¯,0,σu,σv. φ(•) is an unknown true frontier function andYif=φ(Xi),i=1,…,nare the unknown frontier output values corresponding to the n input vectorsXi,i=1,…,n. The Stochastic Frontier (SF) is φ(x)ev. However, we do not choose a specific functional form for φ(x). We maintain instead that φ(•) belongs to the family of production functionsF1that satisfies (i) monotonicity and (ii) concavity in inputs:(2)F1={φ(•)∣X1≥X2⇒φ(X1)≥φ(X2),∀X1,X2∀ν∈[0,1],∀X1,X2:φ(νX1+(1−ν)X2)≥νφ(X1)+(1−ν)φ(X2)}We assume that the observed output from an arbitrary DMU j is an independent draw from the distribution of the random variableY˜=φ(Xj)e−uev. We assume thatui,i=1,…,nare iid random inefficiency andvi,i=1,…,nare iid random noise. In addition uiand viare independent of each other and of the inputs Xi.e−ui∈[0,1]reflects loss in production due to technical inefficiency. Let us assume that random inefficiency and random noise are half normalN+(u¯,σu2)and normalN(0,σv2),respectively. Hence, the true random PPS can be written using the SF φ(x)evas(3)TBM={(y˜,x),x∈R+m∣y˜=φ(x)ev−s+,v∼N(0,σv2),s+≥0}Let us define the following n auxiliary functions(4)gi(Y1f,…,Ynf)=max[∑j=1nλijYjf∣∑j=1nλijXj≤Xi,∑j=1nλij=1,λij≥0],i=1,…,nBM92 show that if there exists a functionφ(•)∈F1such thatYif=φ(Xi),∀ithenYif≥gi(Y1f,…,Ynf),and conversely, ifYifis such thatYif≥gi(Yf),∀ithen we can construct at least one functionφ′(•)∈F1,φ′(x)=max[∑j=1nμjYjf∣∑j=1nμjXj≤x,∑j=1nμj=1,μj≥0,∀j]such thatφ′(Xi)≡gi(Y1f,…,Ynf)=Yif,∀i.2929The second part follows from (i)Yif≥gi(Y1f,…,Ynf)by assumption, and (ii)Yif≤gi(Y1f,…,Ynf)=φ′(Xi)by feasibility in the LP defining φ′(•).Maximum Likelihood (ML) estimates ofY1f,…,Ynf,u¯,σu,σvfollow from(5)max∑i=1nlnh(vi−ui;u¯,σu,σv)s.t.lnYi−(lnYif+vi−ui)=0i=1,…,NYif−gi(Y1f,…,Ynf)≥0i=1,…,Ngi(Y1f,…,Ynf)isdefinedin(4)i=1,…,NYif,ui≥0,∀i,σu≥0,σv≥0,u¯∈Rvi∈R,∀iwhereh(•;u¯,σu,σv)is the convoluted density function of the composite residualv−u.Let us for a moment digress and look at the true random PPS in a slightly modified parametric SFA model. In a parametric SFA we havey=f(x)e−uev,where we maintain a specific functional form off(x)=f(x;β). This implies that we avoid the choice variablesYif,∀i,because we maintain thatYif=f(Xi;β),∀i. The true random PPS, here denoted TALS is in this case simply(6)TALS={(y˜,x),x∈R+m∣y˜=f(x;β)ev−s+,v∼N(0,σv2),s+≥0},and an estimatorT^ALSfrom ML estimatesβ^,σ^v2would be:(7)T^ALS={(y˜,x),x∈R+m∣y˜=f(x;β^)ev−s+,v∼N(0,σ^v2),s+≥0}As noted by BM92, the estimation of the SF within the semi-parametric context differs from the estimation in the parametric SF case. The non-parametric SFA only provides a unique estimator at the observed input vectorsX1,…,Xn. In other words, the frontier function φ′(•) provides n unique outputsYif=φ′(Xi),i=1,…,n,but for all other not observed input vectors we have no unique frontier output level. This aspect has been analyzed in some detail by Kuosmanen (2008) and Kuosmanen and Kortelainen (2012). It is shown that among the alternative solutions in the form of frontier functionsφ′(•)∈F1the tightest piecewise linear functionφmin′(x)is3030φmin′(x)is actually the specific functionϕ^(x)chosen by BM92 in the proof of Proposition 1 in Banker and Maindiratta (1992).(8)φmin′(x)=minα,w≥0[α+wTx∣α+wTXj≥Yjf,∀j]=max{∑j=1nμjYjf∣x≤∑j=1nμjXj,∑j=1nμj=1,μj≥0,∀j}Replacing the true unknownY1f,…,Ynf,σv2with their ML-estimatesY^1f,…,Y^nf,σ^v2,this tightest piecewise linear functionφmin′(x)can now be used to express an estimator of the random PPS,T^BM. Let(9)φmin′^(x)=minα,w≥0[α+wTx∣α+wTXj≥Y^jf,∀j].As noted in Theorem 3.2 in Kuosmanen and Kortelainen (2012),φmin′^(x)is equivalent to the variable returns to scale frontier estimator applied to(Xj,Y^jf),j=1,…,nandY^jf=φmin′^(Xj),j=1,…,n. Hence, we can expressT^BMin the following form(10)T^BM={(y˜,x),x∈R+m∣y˜=yev,y=∑j=1nμjY^jf−s+,∑j=1nμjXj≤x,∑j=1nμj=1,μj≥0,∀j,v∼N(0,σ^v2),s+≥0}The estimator of tightest piecewise linear stochastic frontier is defined from (9) asφmin′^(x)ev. Hence, to defineT^BMwe focus on the convex hull of(Xi,Y^if),i=1,…,nbut allow the frontier output to be perturbed by the noise factor ev.The random PPST^BMin (10) is related to the random PPS in Cooper et al. (1998). To clarify this relationship, let us compare the random PPS,T^BMwith the random PPS in Cooper et al. (1998) with m deterministic inputs and one random outputy˜. We assume that we have observations from n DMUs in the form of input vectorsX1,…,Xnand n random output levelsY˜1,…,Y˜n,where we have information on the distribution ofY˜j,j=1,…,n. The random PPS, here denoted TCooper can be expressed as(11)TCooper={(y˜,x),x∈R+m∣y˜=∑j=1nμjY˜j−s+,x≥∑j=1nμjXj,∑j=1nμj=1,μj≥0,∀j,s+≥0}Note, the similarities between (11) and (10) withY˜jreplaced byY^jfevj,vj∼N(0,σv2). However, there are some interesting differences reflecting the following observations:1. In BM92 the SF is constructed by noise perturbations of the piecewise linear envelopment of the ML-estimates(Xj,Y^jf),j=1,…,n.2. In Cooper et al. (1998) the random PPS is the convex hull of(Xj,Y˜j),j=1,…,nset-added toR+m×R−. Hence, if we for comparison reasons regardY˜jas a noise perturbedYjf,i.e.,Y˜j=Yjfevjthen the input, mean-output combinations along the SF will not be piecewise linear, reflecting the fact that a convex combination of iidN(0,σv2)residuals will not have a constant variance. Hence, we can either express a random PPS T1 using the same approach as in TCooper, namely to accept convex combinations of unknown random frontier output vectorsYjfevj,j=1,…,n:(12)T1={(y˜,x),x∈R+m∣y˜=∑j=1nμjYjfevj−s+,x≥∑j=1nμjXj,∑j=1nμj=1,μj≥0,vj∼N(0,σv2),∀j,s+≥0}or we can express a random PPS T2 generated by noise perturbations of any feasible output given an input vector derived from the convex hull of the deterministic vectors(Xj,Yjf),j=1,…,n.(13)T2={(y˜,x),x∈R+m∣y˜=evy,y=∑j=1nμjYjf−s+,x≥∑j=1nμjXj,∑j=1nμj=1,μj≥0,v∼N(0,σv2),∀j,s+≥0}In (12) we generate the stochastic PPS T1 using the same philosophy as in TCooper, namely to accept convex combinations of unknown random frontier output vectorsYjfevj,j=1,…,n. In (13) we generate the stochastic PPS T2 by noise perturbations of any feasible output given an input vector derived from the convex hull of the deterministic vectors(Xj,Yjf),j=1,…,n.T2 is clearly the correct specification in the context of BM92. BM92 maintain a true concave production function and data are generated as in the SFA-approach by the following DGP: (i) choose inputs, (ii) conditioned on this input generate frontier output on the graph of this production function, and (iii) multiply with a composite error termev−u,based on random draws of u and v.T1 is clearly the correct specification in the context of Cooper et al. (1998). ReplacingY˜jwithYjfevjin (11) reflects the fact that the random PPS in Cooper et al. (1998) is based on, e.g., convex combinations of random performance of the n included DMUs.The difference between T1 and T2 is rather subtle, but relates to the effect on the standard deviations of convex combinations of random variables. As illustrated in Cooper et al. (1998), certainty equivalents of convex combinations of random variables can in some cases create certainty equivalents of the PPS with non-convexities.How well are applications of CCDEA performing relative to similar semi-parametric SFA applications? This question is difficult to answer for several reasons. Indeed, the framework of the CCDEA often leads to applications that at least partly incorporate randomness in the form of predictions from managers on future possibilities. A particularly nice application within this tradition is presented in Sueyoshi (2000). A list of potential applications can be found in Cooper et al. (1998). Predictions from managers on future possibilities are rarely part of the data used in a semi-parametric SFA analysis where randomness in data and the estimation is incorporated in the form of appropriate assumptions on the structure of random inefficiency, random measurement error and sample noise. Hence, a semi-parametric SFA analysis avoids data that is not part of either a cross-section or a panel data set of observed inputs and outputs.Both CCDEA approaches, i.e. Cooper et al. (1998) and Olesen and Petersen (1995), mentioned in the introduction to Section 3 are characterized by the fact that the authors are very silent on the matter on how to define an appropriate and meaningful measure of inefficiency relative to the upper boundary of the estimated random PPSs. It is hardly surprising that this aspect of the CCDEA is creating some very difficult problems. This problem has haunted the SFA since this approach was first proposed in 1977. As mentioned in Section 3.1., the situation is much easier to handle, if focus within SFA is on measures relative to the deterministic “core”, f(x; β) in the parametric SFA and the “upper” boundary of the convex hull of(Xi,Y^if),i=1,…,nset-added toR+m×R−in the semi-parametric SFA. However, SFA recognizes of course that the residuals will be affected by both inefficiency and noise, relative to these “core” production functions. A focus on the residuals relative to the core production function will only provide approximately correct results if σuis large compared to σv. Typically, the Jondrow et al. (1985) estimator of the mean value of the inefficiency conditioned on the size of the residual is used as a measure of the inefficiency relative to the SF, but there seems to be very little justification for this approach (see, e.g., the critical remarks from Greene in Section 1). Alternatively, the estimated frontier, based on a minimization of the sum of squared residuals, can be shifted upward according to the size of the third moments of the composed residuals, but the use of this approach requires specific parametric assumptions on the inefficiency distribution. We will return to these suggestions and some new promising results from Simar and co-authors in Section 4.No efficiency measure relative to the random PPS is proposed in Cooper et al. (1998). But the authors do propose tests of α-stochastically efficiency, reflecting a corresponding concept of α-stochastically dominance defined as follows: “For a given scalar α (0 < α < 1), DMUois not stochastically dominated in its efficiency if and only if there is a joint probability less than or equal toαthat some other observed DMU displays efficiency dominance relative to DMUo”, (Cooper et al., 1998, p. 58). Following the tradition within the literature on efficiency measurement the corresponding concept of stochastic efficiency is derived by allowing dominance from not only observed performance but also virtual performance extracted from, e.g., axioms on convexity and disposability related to production possibilities.The definition of “α-stochastically efficiency” associated with a reference technology T defined in Cooper et al. (1998) can in relation to TCooper with one random output and m deterministic inputs be stated asDefinition 1For0≤α<1,(Xo*,Y˜o*)∈TCooperis “α-stochastically efficient” associated with TCooper if for any μ satisfying∑j=1nμjXj≤Xo*,∑j=1nμj=1,μ≥0,s−≥0,we have(14)P(∑j=1nμjY˜j−s−≥Y˜o)≤αWith only one random output and deterministic inputs we can test α-stochastically efficiency associated with TCooper of(Xo*,Y˜o*)in (14) simply by solving(15)maxP(Y˜o*−∑j=1NμjY˜j<0)s.t.∑j=1nμjXj≤Xo*∑j=1nμj=1μj≥0,∀jIf the optimal value of (15) does not exceed α the DMUois “α-stochastically efficient” associated with TCooper, see Theorem 4 in Cooper et al. (1998). If we are willing to assume iid normal distributions of the random output then we can alternatively test α-stochastically efficiency associated with TCooper of(Xo*,Y˜o*)by solving (16)(16)max∑i=1msi++s−s.t.∑j=1nμjXj+s+=Xo*∑j=1nμjY¯^j+σ(μ)Φ−1(α)−s−=Y¯^o∑j=1nμj=1μ∈R+n,s+∈R+m,s−≥0whereΦ−1is the inverse standard normal distribution function,(μ′)T=[μ1,…,μo−1,μo−1,μo+1,…,μn]andσ(μ)=((μ′)T[σ1^20⋯00⋱⋯⋮⋮⋯⋱00⋯0σn^2]μ′)0.5Y¯^j,σj^2,j=1,…,nare estimators of the mean output levels and the variances from the n DMUs. For any prespecified α ∈ (0, 1), if the optimal value of (16) is less than or equal to 0 then DMUois “α -stochastically efficient” associated with TCooper (see Theorem 5 in Cooper et al., 1998). In (16) focus is on the so-called zero order decision rule and (16.2) is the certainty equivalent3131P(∑j=1NλjY˜j−Y˜o≤0)≤α⟺P((∑j=1NλjY˜j−Y˜o)−(∑j=1NλjY¯j−Y¯o)σ(λ)≤−∑j=1NλjY¯j−Y¯oσ(λ))≤α⟺Φ(−∑j=1NλjY¯j−Y¯oσ(λ))≤αthat corresponds to the chance constraintP(∑j=1NλjY˜j−Y˜o≤0)≤α.In this section we will focus on a comparison of the implicitly defined stochastic PPS in Olesen and Petersen (1995) and the corresponding random PPS TBM and its estimatorT^BMin (3) and (10). To clarify this relationship, let us compare the random PPS TBM with the implicitly defined random PPS in Olesen and Petersen (1995), but let us again simplify the comparison and focus on the situation with m deterministic inputs and one random outputy˜. We assume that we have observations from n DMUs in the form of input vectorsX1,…,Xnand n random output levelsY˜1,…,Y˜n,where we have information on the distribution of each of these n random output.Y¯j,j=1,…,nare mean output levels from the n DMUs. For comparison reasons, let us assume that the n DMUs have identical output distributions. With that assumption, the implicitly defined random PPS, here denoted TOP can be expressed as(17)TOP={(y˜,x),x∈R+m∣y˜=yeδ,y=∑j=1nμjY¯j−s+,x≥∑j=1nμjXj,∑j=1nμj=1,μj≥0,∀j,s+≥0,δisarandomdisturbance}The CCDEA approach proposed by Olesen and Petersen (1995) takes as a starting point a deterministic DEA in multiplier form. Each deterministic input and output vector is replaced by a corresponding random input or output vector. It is shown in Olesen and Petersen (1995, 2000) that this modelling structure implies that the certainty equivalent of the random PPS is expressed as the intersection of halfspaces supporting confidence regions in input output space at any prespecified set of probability levels above 0.5. In other words, observations are allowed to be positioned outside confidence regions at prespecified probability levels, and hence possibly on the wrong side of the frontier because of random noise in data.Let us use identical prespecified probability levels equal to α and follow (Olesen & Petersen, 2000, p. 18) to generateT^OP(α)as the envelopment of the n confidence intervals Dj(α) forY˜j,j=1,…,n(18)T^OP(α)={(y,x),y∈R+,x∈R+m∣∃yj∈Dj(α),j=1,…,n:y≤∑j=1nμjyj,x≥∑j=1nμjXj,∑j=1nμj=1,μj≥0,∀j}and with only one random outputT^OP(α)simplifies to(19)T^OP(α)={(y,x),y∈R+,x∈R+m∣y≤∑j=1nμjY^jf(α),x≥∑j=1nμjXj,∑j=1nμj=1,μj≥0,∀j}whereY^jf(α)=maxyy∈Dj(α).T^OP(α)is clearly not a random PPS for any fixedα∈[0,1). But without fixing α we can regardT^OP(α)as an infinite collection of deterministic PPSs, each being a realization with a specific probability. Notice that both TCooper and TOP(α) specify a random PPS where a finite sample of n distributions is used to outline these random sets.Recently, Simar (2007) has suggested a DGP for a multivariate, cross-sectional, nonparametric stochastic frontier model which is of interest here. Random but observed inputs and outputs from the i’th DMU is denoted(x˜i,y˜i)and the DGP can be written as(x˜i,y˜i)=(xi,yi)+(ɛxi,ɛyi),where (xi, yi) belongs to the support of the inputs and outputs density without noise with probability one. Hence, the DGP for (xi, yi) follows the usual sequence of conditional distributions after converting outputs to polar coordinates (assuming an output oriented inefficiency distribution). εxiand εyidenote the noise on x and on y, respectively. Simar essentially suggests a DGP where data without noise are generated in a phase one, according to the previously developed approach for bootstrapping a common distribution of radial inefficiency scores, see, e.g., Simar and Wilson (1998). In a phase two, a multidimensional random noise vector is added to the generated data. In a Stochastic DEA setting we will focus on this type of two phased DGP with one output and multiple inputs with an iid symmetric error term with unknown variance. We simplify the DGP to fit the output oriented SFA setup by introducing the noise only in the output space.With the traditional notation,ɛ=v−u,Simar (2007) argues that when the signal-ratioσvσuis small then based on the results in Hall and Simar (2002) it is possible to improve the estimator of the boundary of the support of the random inefficiency. For illustration, let us assume3232We partly follow Hall and Simar (2002) model 1 in their simulation experiment.that the frontier output levelYf≡θ=1,θ−u∼Exp(λ),λ=2and v ∼ N(0, 0.32). Hence,E(θ−u)=0.5,σθ−u=0.5,andσvσu=0.30.5=0.6. The support of the composed error termu+visR. It is shown in Hall and Simar (2002) that with a low signal-ratio it is possible to estimate the upper boundary of the support ofθ−u,i.e., obtain an estimator of the frontier output θ from an estimator of the density of the observed composed error terms. As the title indicates the authors are searching for a changepoint in the density of the composed error. It is shown that from the true convoluted density it is possible to identify this upper boundary from the largest absolute derivative of the density on the right side of the distribution. Hence, Hall and Simar (2002) suggest to obtain an estimator of the changepoint by the following procedure (i) obtain an estimator of the density of the composed error term from the estimated residuals, and (ii) estimate the output level with the largest absolute derivative on the right side of the distribution.3333Several improvements to the simplistic presentation included in this review are suggested in Hall and Simar (2002).A graphical illustration is presented in Figs. 3and 4. The two densities on noise v and the frontier output minus inefficiencyYf−u≡θ−uare depicted in Fig. 3. From the convoluted densityfθ+v−u(x)ofθ+v−uin Fig. 4 we can identify the upper end of the support ofθ−ufromargmax|dfθ+v−u(x)dx|. This arg max is of course for the true density functions depicted in Fig. 4 equal to the frontier outputYf≡θ=1.To illustrate the method, assume that we have ν DMUs using exactly the same input vector to produce output. Simar (2007) argues that in an output oriented frontier estimation3434An input oriented approach is used in Simar (2007).based on DEA/FDH (ignoring noise) we will usemaxj=1,…,νYj+(vj−uj)as the estimator of frontier output, although we know that we may overestimate the possible output for any given input mix because of noise in data. Using the observed ν output levelsYj+(vj−uj),j=1,…,νto get, e.g., a kernel estimator of the convoluted densityf^θ+v−u(x)we look for the changepointsω^=argmax|df^θ+v−u(x)dx|and useω^as the estimator of frontier output.Recently, Kuosmanen and Johnson (2010) and Kuosmanen and Kortelainen (2012) have suggested alternative versions and interpretations of the semi-parametric SFA proposed in Banker and Maindiratta (1992). Starting from a multivariate version of the nonparametric regression subject to the monotonicity and concavity constraints suggested in Hildreth (1954) a Convex Nonparametric Least Square (CNLS) estimation procedure is suggested. The CNLS is designed to findf∈F2that minimizes the sum of squared residuals:(20)min∑i=1nɛi2,s.t.yi=f(Xi)+ɛi,i=1,…,n,f∈F2where the setF2consists of all continuous, monotonically increasing and globally concave functions. This setup is well suited to estimate a production function, if we assume that all observations are affected by noise only.It is shown in Kuosmanen and Johnson (2010) that to estimate (20) we can use an equivalent representation ofF2,which provides the following least square estimation:(21)min∑iɛi2s.t.Yi−(αi+wiTXi+ɛi)=0i=1,…,n(21.1)(αi+wiTXi)−(αj+wjTXi)≤0i,j=1,…,n,i≠j(21.2)wi≥0i=1,…,n(21.3)or in the notation of Banker and Maindiratta (1992) with3535Yif−wiTXi≥Yjf−wiTXj⟺αi+wiTXi−wiTXi≥αj+wjTXj−wiTXj⟺αi≥αj+wjTXj−wiTXj⟺αi+wiTXj≥αj+wjTXjYif≡αi+wiTXi,∀i⇒Yif−wiTXi≥Yjf−wiTXj,i,j=1,…,n,i≠j(22)min∑i=1nɛi2s.t.Yi−(Yif+ɛi)=0i=1,…,n(Yjf−wiTXj)−(Yif−wiTXi)≤0i,j=1,…,n,i≠jwi≥0,Yif≥0,i=1,…,nwhich, indeed, is the structure proposed in [P4] in Banker and Maindiratta (1992) (but with a different objective function reflecting that focus is on a maximum likelihood estimation). For comparison, let us rewrite the n auxiliary functionsgi(Y1f,…,Ynf)in (4) above, but as suggested by Kuosmanen and Kortelainen (2012) we use LP duality to get equivalent expressions:gi(Y1f,…,Ynf)=min[wiTXi+α∣α≥−(wiTXj−Yjf),∀j,wi≥0],i=1,…,n=min[wiTXi−(wiTX1−Y1f)⋮wiTXi−(wiTXn−Ynf),wi≥0],i=1,…,nwhich implies that the constraints in (5)Yif−gi(Y1f,…,Ynf)≥0emerge as the equivalent Afriat inequalities:Yif−(wiTXi−wiTXj+Yjf)≥0,∀i,j,i≠jin (21.2). As argued above, to resolve the “problem” that the estimated average production function is only unique at the n observed input vectors, Kuosmanen (2008) and Kuosmanen and Kortelainen (2012) suggest to use the tightest lower bound function (9)φmin′^(x):(23)ϕmin′^CNLS(x)≡φmin′^(x)=minα,w≥0[α+wTx∣α+wTXj≥Y^jf,∀j].If we in (21) add a sign constraint εi≤ 0 then it is shown in Kuosmanen and Johnson (2010) that this slightly modified estimation will provide a simultaneous estimation of the n efficiency scores from the BCC DEA model.3636This and other modifications of the “basic” BM92 model were anticipated in Section 4 on “Concluding Remarks” in Banker and Maindiratta (1992).Hence, the estimated DEA frontier function, where we envelop the observed output levels Yj, ∀j and not the estimated frontier levelsY^jf,∀j(24)φ′^DEA(x)=minα,w≥0[α+wTx∣α+wTXj≥Yj,∀j]is a sign constrained CNLS estimator (see also Kuosmanen, Johnson, and Saatamoinen, 2014, Section 4.1).The CNLS estimation procedure provides only a consistent estimator of the production function if the DGP has the formyj=f(xj)+ɛj,where the residuals εjare iid noise terms with mean zero. Following Banker and Maindiratta (1992), by introducing a composite error termɛj=vj−uj(see Section 3), Kuosmanen and Kortelainen (2012) propose to obtain an estimator of the SF from the CNLS estimator (23) as follows:ϕ′^minStoNED=ϕ′^minCNLS+κ^whereκ^is an estimator of the expected inefficiency. Kuosmanen and Kortelainen (2012) suggest the use of either the Method of Moments (MM) or a Pseudolikelihood estimation ofκ^. The MM is often used in the Modified OLS (MOLS) estimation of SFA, see,e.g., Greene (2008). Since we assume that the residuals are distributed according to a convoluted distribution involving a symmetric noise distribution and a skew inefficiency distribution, we will expect to see the CNLS residuals being left skewed. However, it is well known that sample noise may prevent that from happening. Banker, Gadh, and Gorr (1993) report from an MC study that in approximately 10 percent of the samples a wrong skewness occurred. Similarly, Simar and Wilson (2010) report3737Simar (2007, p. 186): “Simar and Wilson (2005) have shown that in finite sample, the composite residuals may have the “wrong” skewness with nonnegligeable probabilities and that in these cases, maximum likelihood estimators do not have the usual statistical properties, making inference problematic. For instance whenn=100andσu=σvthis happens with probability ≈ 0.3; sample size n ≥ 1000 is required to reduce the probability of “wrong” skewness to less than 5 percent (see Simar and Wilson 2005,Table 1)”.that this wrong skewness occurs with non negligible probabilities in finite samples.3838“The results inTable 1show, for example, that forλ2(=σu2σv2)=1, almost 1/3 of samples of sizen=100will have skewness in the “wrong” direction. Of course, the problem goes away asymptotically, but at a rate that depends on the value of λ2. For the case whereλ2=1, even withn=1000observations, about 3.5 percent of samples will have positive skewness”, (Simar & Wilson, 2010, p. 70).In any case, if the skewness is negative, then it is possible to get estimatorsσ^uandσ^vfrom the second and the (presumably negative) third moment. It is argued in Kuosmanen and Kortelainen (2012) that given the estimatorσ^u,a consistent estimator ofϕmin′StoNED=φmin′(x)in (8) isϕ′^minStoNED=ϕ′^minCNLS+σ^u2/π. In other words, the frontier is simply a shifted version of the CNLS estimate of average performance. Notice that this procedure does not provide an estimator of uj. One approach to estimating the inefficiency is simply to use the Jondrow et al. (1985)-estimator discussed in Section 1above.The recent contributions by Hall and Simar (2002) and Simar (2007), summarized above, allow for a fully nonparametric estimation ofκ^. Kuosmanen et al. (2014) point out that the CNLS estimated residualsɛ^jCNLS,∀jare consistent estimators ofɛj+=ɛjCNLS−E(u). Using the residuals from a CNLS regression to get an estimator of the density forɛ+denotedf^ɛ+(x),the authors argue that a robust nonparametric estimator of the expected inefficiency E(u) isargmaxx∈Ldf^ɛ+(x)dx,whereLis a closed interval in the right tail of the density forɛ+.An alternative approach to this shift of the CNLS SFϕ′^minCNLSis related to an idea proposed by Olesen and Petersen (1995), discussed in the beginning of Section 3. The authors suggest that the use of multivariate normal distributions in the specification of the certainty equivalents of the involved chance constraints can be considered an approximation of a multivariate convoluted distribution of composed error terms. A similar idea is forwarded in Cooper et al. (1998, p. 69). Observations are allowed to be positioned outside confidence regions that are spanning the frontier at prespecified probability levels and may possibly be located on “the wrong side” of the frontier. Hence, this is an attempt to distinguish between (i) variation caused by inefficiency, and (ii) variation caused by noise, where the split of the total variation in data is determined by the choice of the probability levels.This approach simplifies if we focus on only one random output and multiple deterministic inputs. For this special case we will see a frontier spanned by confidence intervals, and these intervals relate to the part of the support of the composed residuals that reflects variation in inefficiency only. Hence, in relation to the approach suggested in Hall and Simar (2002) we would like to identify the probability mass related to the composed residualv−ubeing non-negative. If we have a good estimator of this probability mass then we can shift the CNLS SF upward until sufficiently few residuals are non-negative. A similar idea is suggested in Simar (1996), but this paper has focus on robust nonparametric filtering to avoid outliers: “One way to alleviate the role of possible outliers is provided by Simar (1992). It is there recommended to keep for the second step procedure all the firms achieving at least 0.90 (or 0.95) efficiency levels … This way of robust nonparametric filtering (using p-percent efficient units) could be extended in a more probabilistic setup by using new results of” ..Fan, Hu, and Truong (1994)… “They propose a nonparametric percentile curve estimation. For a given p(sayp=0.9or 0.95), the p-percentile curve fp(x)is defined asP(y≤fp(x)|x)=p. The paper proposes a nonparametric estimation of fp(x)” , (Simar, 1996, p. 183).Let us assume that we have a good estimator of the probability mass above the level Yf, whereYf+v−u≡θ+v−u≥θ,as indicated in Fig. 4 above. Let us assume thatPr[(v−u)≥0]=γ^. To allow for the use of this information, let us rewrite (21) slightly by substituting forYi−ɛi=αi+wiTXi:(25)min∑i=1nɛi2s.t.Yi−wiTXi−αi−ɛi=0i=1,…,n(25.1)Yi−whTXi−αh−ɛi≤0i,h=1,…,n,h≠i(25.2)wi≥0i=1,…,n(25.3)and let us trace the necessary additional constraints on the residual structure that follow from only allowing ε ≥ 0 in (25.1) with at most a certain probabilityγ^:(26)Pr[Yj≥wjTXj+αj]≤γ^,∀j⟺Pr[ɛj≥0]≤γ^,∀jThe estimatorγ^could be derived from an estimated density of the residuals based on the Hall and Simar (2002) approach outlined above. Let us approximate the estimator of the convoluted distribution of the residuals with a normal distribution with meanɛ¯and varianceσɛ2. We can obtain the following certainty equivalent3939Pr[ɛj≤0]≥1−γ^,∀j⟺Pr[ɛj−ɛ¯σɛ≤−ɛ¯σɛ]≥1−γ^,∀j⟺−ɛ¯σɛ≥Φ−1(1−γ^)(27)ɛ¯+Φ−1(1−γ^)σɛ≤0where Φ is the standard normal distribution function. Inserting this certainty equivalent (27) into (25) provides the following constrained CNLS:(28)min∑i=1nɛi2s.t.Yi−wiTXi−αi−ɛi=0i=1,…,N(28.1)Yi−whTXi−αh−ɛi≤0i,h=1,…,N,h≠i(28.2)wi≥0i=1,…,N(28.3)1n∑i=1nɛi≤−Φ−1(1−γ^)σɛ(28.4)Letɛ¯^andσɛ2^be the estimated mean and variance of the residuals from (25). Although the constraint (28.4) contains an endogenous standard deviation of the residuals σε one could argue that the following additional constraint should be added:σɛ=σɛ^,since this would imply that the included probabilistic constraint only affects the location of the residual distribution and not the shape.In the example outlined in Figs. 3 and 4 we have thatγ^≈1−0.828341. The mean and variance of the convoluted distribution are 0.497 and 0.33 and the probability mass aboveθ=1for N(0.497, 0.332) is equal to1−0.809379. Fig. 5illustrates the normal approximation of the convoluted distribution. Including (27) into (25) will push the CNLS-estimated frontier upward such that for increasing sample size we will approximately see an envelopment of1−γ^percent of the observations.Notice thatγ^≥0.5provides for a large sample size the original CNLS residuals, since the constraint (27) simply states that the sum of the residuals is less than or equal to a nonnegative number. We will expect to see the mean of the CNLS residuals close to zero. As mentioned above, we will also expect to see a left skewed distribution but sample noise may prevent that. If, e.g.,γ^=0.05thenɛ¯≤−1.65σɛwhich means that asymptotically we will never see more than 5 percent of the residuals being positive, IF the normal approximationN(E(u)^,σ^v−u2)is reasonably accurate in the right tail of the convoluted distribution.A similar idea was suggested in Banker (1988) in the section that presents some alternative formulations of the proposed Stochastic DEA. It is argued in this paper that it is possible to minimize the weighted sum of the absolute value of the positive part and the negative part of the composed error term. Let us denote the positive and negative part of the composed error term byɛ+andɛ−. Banker (1988) proposes the following quantile MAD regression model:(29)minγ∑j=1nɛj++(1−γ)∑j=1nɛj−s.t.(wjTXk−wjTXj)+(ɛj+−ɛj−)−(ɛk+−ɛk−)≥Yk−Yjk,j∈{1,…,n},k≠jɛj+≥0,ɛj−≥0,∀j,wj≥0,j=1,…,nand it is noted that the model “corresponds to the basic DEA formulation forγ=0, and to a MAD model forγ=0.5. Intermediate locations of the production frontier obtain for values of γ between 0 and 0.5, as in quantile regressions”.This formulation has recently been suggested in Wang, Wang, Dang, and Ge (2014), where the authors denote the approach the Convex Nonparametric Quantile Regressions (CNQR).In production economics it is often assumed that the true production frontier is continuous differentiable. However, the convex hull estimator of the frontier used, e.g., in the BCC model, or the semiparametric SFA proposed in Banker and Maindiratta (1992) are piecewise linear and non-smooth approximations to the true frontier. Partly to remedy this non-smoothness a class of smooth constrained nonparametric frontier estimators has recently been proposed based on nonparametric kernel regression with shape constraints. It is argued that there is a need for a kernel-based frontier estimation that provides continuous differentiable estimators that satisfy the traditional production economic axioms. Building on the work of Hall and Huang (2001), Du, Parmeter, and Racine (2013) and Racine et al. (2009), three kernel-based frontier estimators are suggested in Parmeter and Racine (2013). One of these estimators is indeed a smooth counterpart of the least square version of the semiparametric SFA proposed in Banker and Maindiratta (1992), i.e., the CNLS model proposed in Kuosmanen and Johnson (2010). There seems to be some evidence of a surprisingly slow convergence of this CNLS model. In general, a remarkably small number of hyperplanes are in use when the piecewise linear CNLS model is estimated. The implication of the slow convergence is that a very non-smooth estimator is provided, even for medium sized samples relative to the number of inputs included. Clearly, there seems to be a need for smoothening the CNLS estimator.Each of the kernel-based frontier estimators suggested in Parmeter and Racine (2013) is an extension of a simple basic nonparametric kernel-based regression estimator. An example of such a simple basic estimator (one input one output) is the following: With a given sample(x1,y1)…,(xn,yn)from a DGP given asyi=f(xi)+ɛi,i=1,…,nwith εibeing a noise residual, we seek an estimatorg^(x)≡y^p(x)≡E^(y|x)of the predicted output E(y|x) of the following form:(30)g^(x)≡∑j=1nAj(x)yjwhere Aj(x) is a local weighting function. Following Chapter 2 in Li and Racine (2007) an example of Aifor the univariate case isAi(x)=K(xi−xh)[∑j=1nK(xj−xh)]−1,where h is the bandwidth and K(•) is a kernel specification. Following Hall and Huang (2001) additional weights are introduced to allow for constraints on the shape of the estimatorg^(x):(31)g^(x|w)≡∑j=1nwjAj(x)yjSettingw=(1n,…,1n)corresponds to the unrestricted nonparametric kernel regression. The shape constraints (monotonicity and concavity) on the nonparametric kernel regression are introduced by including the following constraints in the regression:(32)∂g^(x|w)∂x=∑j=1nwj∂Aj(x)∂xyj≥0(33)∂2g^(x|w)∂x2=∑j=1nwj∂2Aj(x)∂x2yj≤0A restricted version follows by minimizing∑j=1n(1n−wj)2subject to the constraints in (32) and (33) and∑j=1nwj=1. The estimation procedure for the multivariate case withx∈Rnis more complicated, but as discussed in Parmeter and Racine (2013) withx∈Rn,the multivariate version of (33) is only a necessary condition for concavity. The authors suggest to include the Afriat conditions (see,e.g., (21.2) in the CNLS-model formulation (21) in Section 4) formulated using the specified product kernel version of predicted output. Specifically, in Section 4.2 in Du et al. (2013) with the somewhat misleading title “Global concavity” the Afriat conditions at a given point xiare outlined as∑j=1n{wj[∇xAj(xi)]T[xl−xi]−[Aj(xl)−Aj(xi)]}yj≥0,i,l∈{1,…,n},i≠lwhere∇xAj(xi)=[∂Aj(xi)∂x1,…,∂Aj(xi)∂xn]TThe restrictions (32) and (33) are illustrated in a numerical example in Section 5.2 in Du et al. (2013) and the unrestricted and restricted kernel estimators of the frontier are compared. To “approach” a concave estimator the authors impose negativity of the second derivative of both inputs on a grid of 250 points. Let us assume a DGPy=x0.4+ɛ,and generating 100 observation from random inputs on [0, 10] × [0, 10] with ε ∼ N(0, 0.72) . Fig. 6 illustrates the unrestricted estimator. Fig. 7illustrates the impact of imposing the Afriat conditions on the unrestricted estimator, providing an estimator that has smoothened out the non-concavities.Two different approaches have been adopted within the area of efficiency analysis, DEA and SFA. The CCR- and the BCC-model were developed within the MS tradition and SFA within the statistical or econometric tradition.The main strength of DEA compared to approaches within the statistical tradition is a non-parametric estimation of the PPS based upon fundamental axioms from production theory including convexity and monotonicity. No functional form for the frontier or the distribution of inefficiency is assumed. The outcome of an efficiency analysis based upon DEA is for these reasons easy to communicate to decision makers. Equally important, the outcome extends beyond the estimation of measures of inefficiency. The identification of a reference set of DMUs to compare with is an important piece of information for DMUs termed inefficient.Banker (1993) establishes a statistical foundation for DEA by demonstrating that DEA (with one output) provides a consistent estimator of the frontier, if the deviations from the frontier are iid with a one sided support and a monotonic decreasing density. Simar and Wilson (1998) provide a foundation for the estimation based on bootstrapping of confidence intervals for DEA efficiency measures. However, even in these models inefficiency is measured compared to a deterministic frontier, and noise in terms of measurement errors and specification errors is not modeled.The statistical or econometric tradition insists upon an axiomatic foundation for a statistical model including a specification of a DGP. Observed data are considered a sample from an underlying large population and is subject to random errors of measurement, specification errors, and other sources of noise, which in turn allows for an estimation of the impact of noise. SFA in a cross section context involves regression techniques based upon a parametric specification of the production frontier as well as the distributions of noise and inefficiency. The underlying assumptions partly reflect what is necessary to get a consistent estimator of a true frontier, and the implications are not always straightforward as seen from a decision making point of view.DEA and SFA are complementary approaches for efficiency analysis as witnessed by the developments within the area of Stochastic DEA. Stochastic DEA comprises approaches for efficiency analysis utilizing production economic axioms such as convexity or ray unboundedness of the random inputs and outputs involved and further based upon statistical axioms or distributional assumptions that allow for an estimation of stochastic inefficiency compared to a deterministic or a stochastic PPS. Stochastic DEA has established a number of bridges between DEA and SFA as well as between the MS and the statistical approach for efficiency measurement. We have designed the current review of Stochastic DEA with the aim of highlighting differences between the MS and the statistical approach, making similarities between semi-parametric SFA and CCDEA explicit.We have argued that the MS approach for efficiency analysis provides an outcome that is meaningful as seen from a decision making point of view. We have also argued that statistical inference based upon the homogeneous bootstrap approach is accompanied by a heroic assumption of a common distribution of inefficiency for all DMUs and that an aggregation of inputs and outputs may well be necessary to provide access to stable inference due to the curse of dimensionality. The outcome of an efficiency analysis based on disaggregate data are more easily understood by the involved decision makers, but the discriminating power of the corresponding confidence intervals is weak in the well known scenario with a limited number of observations in a multi-dimensional input output space.As a consequence, a statistical framework seems to prioritize aggregation possibilities, e.g., linear combinations of inputs and outputs, which best summarize the information provided by all DMUs. The statistical (or econometric) approach to estimating and testing technology characteristics typically investigates to what extent the general tendency in data questions a given hypothesis. The individual data point is not at focus, because the set of all data are regarded as a particular sample from the DGP. Using the structure of the residuals to test general technology characteristics has been an important part of econometrics for many years. But there seems to be an important difference between testing general technology characteristics and estimating confidence intervals for the inefficiency related to each of the observed data point. In the latter case we have focus on the individual observation, which in our opinion makes it more dubious to introduce an aggregation that favors some points but not others, by referring to what structure best summarizes the information in data.CCDEA and semi-parametric SFA may appear to be competing approaches within the area of Stochastic DEA. Proponents of semi-parametric SFA may argue that the approach is superior compared to CCDEA, because it is consistent with basic axioms in production theory and has a firm statistical foundation including an underlying DGP. By contrast, proponents of CCDEA may argue the superiority of this approach, because DMU-specific distributions of noise and inefficiency are easily accommodated, the case of multiple inputs and multiple outputs can be handled in a straightforward manner, and recent work by Simar (2007) establishes a DGP for a multivariate, cross-sectional, nonparametric stochastic frontier model that may be applicable in the context of CCDEA. We have shown that the estimators of a stochastic PPS in the context of semi-parametric SFA and CCDEA in spite of inherent differences regarding the modeling of noise and stochastic inefficiency share basic characteristics in the case of one output and multiple inputs.The disentangling of noise and inefficiency in models with a composite error term is subject to ongoing research. Recent developments within this area have in view of the importance of the topic been reported in this review.

@&#CONCLUSIONS@&#
