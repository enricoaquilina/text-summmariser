@&#MAIN-TITLE@&#
A probability model for key analysis in music

@&#HIGHLIGHTS@&#
A novel model to analyse the tonality in music is proposed.New key profile models are based on full probability density functions.Each pitch class for each key is assigned a full probability density function.Analytical expressions of the probability density functions of the key models are given.The new key profile models were used in detection experiments at the MIREX contest.

@&#KEYPHRASES@&#
Music information retrieval (MIR),Key probability model,Key profile model,Key analysis,Tonal behaviour,Pitch Class Profiles,

@&#ABSTRACT@&#
This paper presents a novel method to analyse the tonal behaviour of a music piece. The method is based on the development of a novel probability model of the predominant key using the well known Pitch Class Profile (PCP or chroma) descriptor. This feature represents the importance of each note of the chromatic scale within the spectral content of the audio signal.Making use of PCP feature, a main novel contribution presented is the development of new profile models for the characterization of major and minor keys. Most of the key profile models found in the literature assign a single value to each pitch class for a particular key. This value represents the salience of this pitch class in a specific key. The new key profiles, that will be described in this paper, are based on the identification of specific probability density functions (PDFs), defined after the analysis of the presence of the pitch classes of the chromatic scale in the different keys.

@&#INTRODUCTION@&#
In the last years, key or tonality analysis has become a common subject of study in the music information retrieval community. A main reason for this fact is that tonality is an important concept in musical theory since the key justifies the presence of each note within a piece of music and its relation to nearby notes. Also, the psychological perception of music is strongly determined by its tonal behaviour [1].Every key is associated with a hierarchy of the pitches of the musical scale [2,3]. This hierarchy establishes an ordering of the importance of the pitches. These relations are defined upon the concept denominated scale degree (Fig. 1). The tonic note is the centre of the key, around this note, the other degrees are organized with the dominant being the second most important degree. The dominant and the subdominant note define the tonal degrees set. On the other hand, the modal degrees set, that determines the mode of the key (mainly major or minor), is formed by the mediant, submediant, leading tone and also, although less important, the supertonic. In the minor mode, the seventh degree is called subtonic because the distance between this degree and the tonic degree is one tone, instead of a semitone as in the major mode [4].These degrees are defined for the diatonic scale (seven elements). However, a large number of studies on key estimation use the chromatic scale (12 elements), so it is common to define 12 pitch classes (C,C#,D,D#,E,F,F#,G,G#,A,A#and B) for each key mode [5]. Then, C major and c minor keys are taken as reference of major and minor keys, respectively. In this way, all the results and conclusions obtained for these pitch classes can be equally applied to each of the 24 keys (12 major and 12 minor keys) by transposition of these sets of pitch classes [4].Most studies about tonality and key analysis are based on the model defined by Krumhansl and Kessler in [6]. The major and minor key profiles defined by Krumhansl and Kessler are based on experimental data and represent the importance of the 12 pitch classes in major or minor keys. The key estimation algorithm finds the largest correlation between an input vector that represents the estimated Pitch Class Profile (PCP) and the possible key profiles. Note that the utilization of the PCP was originally proposed by Fujishima [7] and it is commonly employed for the detection of chords [8] and key profiles.Temperley [9] modified Krumhansl’s model taking into account fundamental aspects of music theory, like the importance of the dominant seventh. He also replaced the correlation score by the scalar product. Later, in 2002, Temperley developed a new tonal model using Bayesian principles to determine the key of a MIDI piece of music [10,11].In recent years, many researchers have used the key profiles defined by Krumhansl [6] and Temperley [9] as reference for key analysis. In [12], Izmirli compared the key estimation results obtained using three different chroma profiles: Krumhansl’s, Temperley’s and the flat diatonic profiles. Then, Izmirli defined a number of templates computed by a weighted combination of the spectra obtained from the recorded waveforms of monophonic notes. Later, Izmirli [13] presented two different models for key detection: the first model was based on the idea of the correlation with templates and the second one used a tonal representation of reduced dimensionality based on Principal Component Analysis (PCA).Gómez [14,15] developed another similar method for key identification based on a modified Pitch Class Profile (PCP) called Harmonic Pitch Class Profile (HPCP). The main differences with other methods are the increase of the frequency resolution to less than one semitone and the fact that the influence of the notes’ harmonics is taken into account in the analysis. Also, in this paper, Gómez considered machine learning methods for key estimation using HPCP. Other researchers focused on audio tonality mode classification using chroma features [16].In this paper, a novel concept of key profile model is presented that is able to lead to the design of new key analysis and detection systems. The new key profile models defined constitute a more detailed representation of the real contribution of each pitch class to a specific key based on probabilistic concepts. In the model developed, the conventional key profiles (major and minor) are replaced by two sets of twelve probability density functions (PDFs). One set of PDFs will characterize major keys and another one minor keys. Each PDF in these sets will account for the probability of each grade in each key. Using these probabilistic models, instead of the commonly used scalar values of the classic profile models, a procedure will be developed to estimate the most probable key of a musical piece. The experiments done and the good results obtained in the well known MIREX contest [17] show the good performance of the methods developed and validate the new profile models proposed.The paper is organized as follows: in Section 2, the new profile models for key characterization of polyphonic music are presented. In Section 3, the key detection method based on the new probability based Pitch Class Profiles is proposed. Experimental results, comparisons and a global discussion on the performance of the method are presented in Section 4. Finally, some conclusions are drawn in Section 5.In this section, we will describe the procedure followed to define a new probability model for key analysis in music. This model is based on a probabilistic analysis of the presence of each of the pitches in the different keys. This approach will lead to the definition of two sets of 12 probability density functions that will replace the commonly used key profiles, i.e. we will completely define a PDF for each degree in each key.Recall that the key of a piece of music determines the set of tones that are used in the melodic line of the piece. In the case of a monophonic piece, its monophonic nature greatly restricts the tones used in order to obtain a successful harmony induction. Since notes will have to placed sequentially, the composer will have to give enough clues of the desired harmony. This is accomplished by including enough chord tones and avoiding too many passing tones that can blur the harmonic sensation [18]. In addition, there are some dissonant intervals that should be avoided [19]. This fact makes the voicing of monophonic music predictable and restricts the number of available solutions regarding a fixed key and a harmonic progression. On the other hand, the voicing of tonal behaviour becomes considerably more flexible in polyphonic and multi-instrumental music. Since the composer has simultaneous tracks available, the harmony induction is easily achievable. However, the composition also becomes more complex since the voicing of simultaneous several instruments complicate the correct connection of the tones without infringing any harmonic rule [19]. In our analysis, we will not consider monophonic and polyphonic separately.A flowchart of the procedure developed to define our key model is shown in Fig. 2. A dataset with musical pieces labelled with the corresponding key will be analyzed. First of all, the music pieces are conveniently pre-processed. The first 30s of the audio signal are cut, in order to avoid undesired modulations of the tonality. Note that musical pieces normally start and end in the key of the song, with modulations in the middle of the piece. So, we have considered that the first 30s of a piece is representative of the key of the song without important key modulations and sufficient to perform key detection. Also note that in the audio key detection task of the MIREX contest [20], only the first 30s of the selected tracks are considered. Then, the pre-processing processes starts (Section 2.1) and, afterwards, the PCP will be obtained (Section 2.2).The key mode (major or minor) of the musical piece is extracted from the key label to determine whether the PCP data extracted should be used in the model of the major or minor keys (Section 2.3).After the whole database is analyzed, an analytical PDF function is fitted to each pitch for the major and minor keys. Thus, two sets of 12 PDFs to represent the contribution of the pitch classes to the major and minor keys will be found.Now, the different stages of the modelling procedure are described in detail. The main stage is the calculation of the two sets of PDFs that best fit the normalized histogram of the experimental data of the different pitches in the different keys.To begin with, the audio signal is windowed using a Hamming window of 3s of duration with a 50% of overlap. The window length has been chosen taking into account that the objective is to extract excerpts with musical information without losing much temporal resolution. Then, each window (frame) is pre-processed before the PCP computation.A threshold equal to the 1.5% of the maximum of the signal amplitude is used to detect the silence windows. If the mean amplitude of a certain window is under this threshold, then the windows will not be considered in the analysis process. Otherwise, the window is analyzed in the frequency domain.The frequency range selected to analyse the tonal behaviour is given by the range of piano notes (from C1−32.7Hz to B7−3951.1Hz) [21] because it comprises the notes of all the common musical instruments.The spectrum of each window is processed in order to detect the local maxima and the rest of the spectral information is deleted (Fig. 3shows an example of this process). Thus, a simplified spectrum defined as a discrete series of frequency peaks is obtained. Then, a frequency quantization process that shifts each detected peak to the corresponding theoretical frequency of the nearest note is carried out to complete the pre-processing stage (note that, in this context, the quantization process described can be seen as a frequency tuning process).In Fig. 3, the spectrum and the simplified spectrum of a musical fragment (extracted from the first piece of the Bach’s Well-Tempered Clavier) in which the C major chord is played by a clavichord, is represented. Note that the clavichord is tuned according to the baroque style:A4=415Hz, almost a half-tone flat fromA4=440Hz.The Pitch Class Profile (PCP) or chroma represents the contribution of the twelve semitone pitch classes to the whole spectrum. The PCP was originally proposed in [7], afterwards, different methods for PCP computation have been developed. In [6], the total duration of each pitch class in a musical piece is calculated and stored in its corresponding position of the PCP. Izmirli proceeds differently and obtains the PCP averaging the energy within each of the semitone regions that belong to the same pitch class [12].The PCP has been found to be useful for different applications related to the analysis of musical structure [22] and retrieval [23]. Generally speaking, the PCP can be useful to compare the main notes of two melodies regardless of the absolute pitches.The computation of the PCP in our system is based on the local maxima of the signal spectrum, selected at the pre-processing stage. In the proposed method, a 12-dimensional PCP vector (from C to B) is obtained by summing the spectral peaks selected for each tone spawning seven octaves (fromC1toB7). This process is defined by the following expression:(1)PCPt(k)=∑i=17Xs(k+(i-1)·12)whereXsrepresents the simplified spectrum (Section 2.1) andk∈{1,2,…,12}. k is used to index the twelve pitch classes. ThePCPtis computed for each frame t.In Fig. 4, an example of a PCP calculated is shown. The amplitudes are normalized so that their sum is one. This PCP corresponds to the spectrum shown in Fig. 3.In this fragment, the main tonality is B major (C major key played by a clavichord), the predominant tones areB,D#, andF#. These tones correspond to the tonic, dominant and mediant degrees [3].Since our objective is to model and identify the tonal behaviour, a novel probability model in this context is defined. The development of this model is described in the next section.Most of the schemes that deal with the key identification problem use the chroma information (PCP) of the songs to perform a comparison against a specific key profile. Usually the profile models used are modifications or optimizations of Krumhansl’s, Temperley’s or diatonic profiles [6,9,12,14,15]. In all these known profiles, a single value represents each pitch class.Making use of the selected profile model, different distance or similarity measures can be used to estimate the predominant key. This approach may give rise to different scenarios in which important information regarding the specific differences between the vectors compared is lost. This issue can be unveiled by the following questions:•Is the difference between the profiles caused only by one pitch class or by the sum of several pitch classes?Which is the degree that returned the largest difference?Is the difference between a particular pitch class of the PCP obtained and the model positive or negative? Note that depending on the pitch class of the chromatic scale, this information can be very important.Also, in music theory, the different degrees of the diatonic scale are ordered according to their importance in the key, i.e., an error in the supertonic degree (3rd pitch class) is less relevant than the same error in the tonic degree (1st pitch class) [24]. However, the relative importance of the differences between each of the estimated pitch classes and each of the model pitch classes is not weighted.A new concept of key profile is proposed to handle all these issues. Specifically, a complete probability model for each pitch class in the key profiles is defined. Recall that previously defined models use scalar values to model the contribution of each pitch class. The model proposed in this paper is different and contains more information about each pitch class in the key profiles than previous models.In order to build this model, a large number of music pieces has been analyzed to extract the probabilistic information about each pitch class for major and minor keys. Thus, a key labelled music database with different collections of music has been built. This database contains:•Well-Tempered Clavier pieces by J.S.Bach (36 pieces).Beatles collection (49 songs).String quartet pieces by W.A. Mozart (86 pieces).The key labels of the classical music pieces have been assigned manually by a group of expert musicians, analyzing the scores and the audio files of each music piece. On the other hand, the set of Beatles songs has been labelled using the key annotations available in [25]. The Beatles collection of music is commonly used to evaluate systems developed in the MIR area [26,27]. Recall that the main objective of this manuscript is to present a new model for key analysis in tonal music regardless of the genre or style. The concept of tonality is independent of the musical style, so the corpus used in the analysis contains excerpts of different types or genres.In order to avoid possible modulations of the key, only the first 30s of each of the music pieces have been analyzed using the windowing scheme and the pre-processing process described in Section 2.1 and the PCP computation method described in Section 2.2. Afterwards, the elements of the PCP vector obtained for each window are shifted the necessary number of positions to transpose the original key to C major or C minor, depending on the key mode.Finally, the shifted PCP vector is stored column-wise in the PCP matrix of the corresponding major or minor key. Thus, each row of both matrices represents the sample distribution of each pitch class in the major and minor modes (see Fig. 2). These experimental data will be used in the subsequent process to define suitable probability density functions.Because of the lack of knowledge of the nature of the phenomenon that generates the distribution of the pitches of the PCP, the type of function selected to fit the data should have a sufficiently large number of degrees of freedom. In this sense, the utilization of general parametric functions appears to be a good choice. However, we found that the family of ‘generalized Beta’ functions (Gbeta functions) proposed in [28] attains the lowest error. Note that the availability of these analytical models allows the reproducibility of the experiments or the implementation of algorithms without requiring a data set (our audio data set or any other one) to generate the new key profile models. The mathematical expression of the Gbeta PDF follows:(2)GbetaXαa,αb,βbβa(x)=betaX(αa,αb)(x)·βbβaαaβbβax-x+1αa+αb0⩽x⩽10otherwisewherebetaX(αa,αb)(x)represents the beta PDF with parametersαaandαb[29]:(3)betaX(αa,αb)(x)=(1-x)αb-1xαa-1B(αa,αb)0⩽x⩽10otherwisewithB(αa,αb)the beta function[29,30].The family of Gbeta functions is characterized by a set of three parameters that can be used to model a large range of experimental distributions. So, an optimization procedure to obtain the best fit to the sample distributions based on the minimization of the mean square error is performed. Shortened Gbeta PDFs have been fitted to each pitch class for the major and minor keys. Thus two sets of 12Gbeta PDFs are obtained. The parameters that define these two sets of PDFs are shown in Table 1.In Figs. 5 and 6, the two sets of PDFs are presented, each figure shows the 12 PDFs that correspond to each of the 12 pitch classes (from C to B) for the major and minor keys, respectively. These figures represent the normalized distribution of the pitch classes of the PCP vector in which the abscissa axis has been restricted to the range [0,0.6]. This range has been established by the minimum and assumed maximum possible values of the PCP elements according to the experimental data analyzed. Note that also the values of the Gbeta PDFs have been modified so that the functions (Shortened Gbeta) behave like proper PDFs.The experiments reveal two different types of PDFs according to their variance and the location of their mode: wide and narrow PDFs. We observed that the wide PDFs correspond to the pitch classes belonging to the diatonic scale of the each key mode. On the other hand, the narrow ones correspond to the rest of the pitch classes for each key. Also, the curves that have their mode at the largest values correspond to the pitch classes of the diatonic scale, recall that these are the more important degrees in the key. All this means that the diatonic pitch classes have a larger probability of having a relevant contribution to the specific key than the other pitch classes.In both major and minor key modes (Figs. 5 and 6, respectively), the most important pitch classes are C and G. These are the tonic and the dominant degree and their PDFs are the widest, also their mode is larger than the mode of the other pitch classes. The main difference between the two sets of PDFs is related to the mediant degree. In the major mode, the interval between the tonic and the mediant degree is a major third, so the PDF corresponding to this degree is the E pitch class. Conversely, in the minor mode, the interval is a minor third and the PDF of this degree corresponds to theD#pitch class, (note that this pitch, in the c minor context, should have been namedE♭).It is important to observe the small differences between the PDFs of the last pitch classes in the minor mode (fromG#to B). Recall that there is only one major scale but three different variations of the minor scale (Fig. 7) [4,31]: the natural minor scale (Fig. 7 a), that holds the key signature, the harmonic minor scale (Fig. 7b), that consists on the transformation of the subtonic degree into a leading tone degree raising the seventh degree by a semitone, and the melodic minor scale (Fig. 7c), in which the sixth and seventh degrees are a semitone higher. The similarity between the PDFs of the last four pitch classes (fromG#to B) is due to the fact that these pitches appear in the different minor modes (note that in the C minor context, the pitches should have been namedA♭,A,B♭and B) [32].Finally, we have performed some hypothesis tests regarding the equality of the distributions of the samples of any two different grades. Specifically, we have done exhaustive Kolmogorov–Smirnov tests [33] and we have found that the null hypothesis (the samples are drawn from the same distribution) cannot be rejected at a significance level of 5% in the following cases (recall that C major and C minor keys are taken as reference of major and minor keys, respectively):1.C and G, both of major or minor key. In this case, we are considering the distribution of the grades I and V, the tonic and the dominant, respectively. These grades define the tonality and it is reasonable that they appear equally likely, so, this finding clearly agrees with the concepts of Western tonal music.D#andG#both of major key. From the musical point of view, it is uncommon that these grades, the minor third and the augmented fifth respectively, appear in a major tonality. So, we conclude that it is equally unlikely that these grades appear in a major tonality.E andF#both of minor key. This case, regarding the major third and then diminished fifth, respectively, is equivalent to the previous one when a minor key, instead of a major key, is considered.D and E both of major key. Both are frequent notes in the main chords of a tonality [3,34]. D is the fifth of the dominant chord and E is the third of the tonic chord. The analysis indicates that these grades appear with the same frequency in a major key.C of major key and C of minor key. This grade corresponds to the tonic in both cases (C major and C minor keys are taken as reference). So, it is reasonable, from musical point of view, that the null hypothesis cannot be rejected.D of major key and D of minor key. This grade corresponds to the supertonic. This means that the assumption of the equality of the distribution of the supertonic in major keys and minor keys cannot be rejected.G of major key and G of minor key. This indicates that the distribution of the dominant in major and minor keys are not significantly different.C of major key (tonic) and G of minor key (dominant) and vice versa. Again, these grades define the tonality (see case 1). They are the most frequent ones.E of major key and D of minor key. E is the third of the tonic chord of the major key and D is the fifth of the dominant chord of the minor key. It seems reasonable, from the musical point of view, that they appear equally likely, the analysis cannot reject this idea.F of major key andA#of minor key. This means that the subdominant of the major key and the subtonic of the minor key do not appear with different distributions, according to the test.The shape, the characteristics and the relations between the two sets of PDFs have been discussed and related to music theory [3,34]. Now, a comparison between the PDFs obtained and the known profiles of Krumhansl and Temperley is presented in order to analyse the similarities among them. Recall that in order to perform such comparison, it is necessary to select the most representative scalar values from each PDF for the major and minor key modes. The parameter selected to represent each pitch class is the mode of the corresponding PDF.Krumhansl’s and Temperley’s profiles and the modes of each pitch class obtained from our new major and minor profiles have been normalized so that all the values, for each key mode, sum one. In Figs. 8 and 9, the three normalized profiles of the major and minor mode are shown, respectively.The comparison shows that in our model the dominant degree (G in C major and C minor) is given more importance than in the two other models.Also, the correlation coefficients between the profile have been calculated. In Table 2, the correlation matrices obtained [35] are presented. In this table, the label PDF-m makes reference to the profiles obtained using the normalized modes of the PDFs of the novel probabilistic profile models described in this manuscript and TEMP and KRUM correspond to Temperley’s and Krumhansl’s profiles.The correlation coefficients show that the profiles are close to one another in spite of the fact that they were obtained using very different approaches. In this paper, the key mode profiles have been computed using a probabilistic analysis of the music pieces, whereas Krumhansl and Kessler [6] carried out some psychoacoustic experiments with human listeners to define their major and minor profiles. Also, note that in all the three cases, the contribution of each pitch class to each key mode agrees with the main concepts of music theory.The comparison of the new PDF mode profiles with Temperley’s profiles and Krumhansl’s profiles (Figs. 8 and 9), the correlation coefficients between the profiles defined in this work and the classic profiles (Table 2) and the fact that the contribution of each pitch class to each key model agrees with the main concepts of music theory, indicate that the new PDF mode profiles defined comply with the fundamentals of the current musical system regarding scales and keys, with the human perception of the tonalities and with the actual audio content of Western polyphonic music pieces.In this section, we will make use of the probability based key profiles defined in the previous section to develop a key detection procedure. The main steps of this procedure are shown in Fig. 10. This procedure shares the first stages of the key characterization procedure described previously (Section 2, Fig. 2). For each frame, the PDFs previously calculated are used to estimate the probability of correspondence of the current PCP to each of the 24 keys of the Western tonal music system. The most probable key is stored in a vector and the mode of this vector will be used to define the key of the music piece. Note that this vector can also be seen as a representation of the tonal behaviour of the music piece over the time.In order to avoid possible modulations of the key, according to the observation drawn at the beginning of Section 2, only the first 30s of the pieces are taken into account.For each frame of the music piece, the two sets of PDFs (for major and minor keys) are used to calculate the pseudolikelihood of each possible key using the product of the likelihood of the pitch classes in the PCP with respect to each of the 24 possible keys in the Western musical system. To this end, the PCP vector is shifted 12 times to calculate this value for the 12 major keys (with the major set of PDFs) and for the 12 minor keys (with the minor set of PDFs). This process defines a 24-dimensional tonal score vectorSt‾, with:(4)St(k)=∏j=0j=11Mj+1(P((j+k-1)mod12+1)),1⩽k⩽12∏j=011mj+1(P((j+k-1)mod12+1)),13⩽k⩽24whereSt(k)is the score of the k-th tonality. Ifk∈{1,…,12}, then, k represents one of the 12 major keys (from C to B). Otherwise (k∈{13,…,24}),St(k)corresponds to one of the 12 minor keys (from c to b).Mj+1/mj+1represents the set of PDFs defined for the major and minor keys, respectively.P(·)represents the value of the pitch classes for the current window. Recall that the score vectorSt‾is computed for each frame t.The key of each frame corresponds to the maximum value of the score vectorSt‾. The key detected in each frame is stored in a tonal behaviour vector,T‾, for later analysis [36].The histogram of the tonal behaviour vectorT‾is calculated in order to represent the contribution of the 24 keys to the music piece. The largest value of this histogram defines the main key estimated for the music piece.In Fig. 11, the tonal behaviour histogram of the song ‘Any time at all’ by The Beatles is shown. The main key estimated is D major, which is the key of this song. Note that the other keys detected are strictly related to this one: the relative minor of D major, that is B minor, the fifth ascendant (A major) and the fifth descendant (G major).

@&#CONCLUSIONS@&#
In this paper, two new key profile models that can be used for polyphonic music analysis have been presented. The first one, uses full PDFs to define the profiles of the music keys. The second one is defined using the modes of the PDFs of the first model. This latter model can be directly compared to the classical profile models widely used in the literature. However, the new full PDF based model contains a more complete and usable description of the behaviour of the grades in the different keys. Also, a key detection procedure that makes use of the complete PDF key profile models has been defined.Analytical expressions of the PDFs that define the models are provided to assure the usability of the model and the reproducibility of the results.The performance of the detection procedures developed and the validity of the new profiles models have been evaluated using our own dataset (containing 331 annotated pieces) and in the well known MIREX contest. This contest includes a specific task to evaluate the performance of audio key detection algorithms. In this contest, that performs the evaluation of the algorithms used a database with 1252 music pieces, our proposals obtained very good scores. One of our proposals attained the best result in the ‘correct key’ detection subtask in the MIREX 2011 contest.