@&#MAIN-TITLE@&#
R3DG features: Relative 3D geometry-based skeletal representations for human action recognition

@&#HIGHLIGHTS@&#
A new family of 3D skeletal representations for human action recognition.Two new scale-invariant 3D skeletal representations.Experiments evaluating various skeletal representations on five action datasets.State-of-the-art results on various human action recognition datasets.

@&#KEYPHRASES@&#
Action recognition,Skeletal representations,Lie groups,Special orthogonal group,Special Euclidean group,Quaternions,Dual quaternions,

@&#ABSTRACT@&#
Recently introduced cost-effective depth sensors coupled with real-time skeleton extraction algorithms have generated a renewed interest in skeleton-based human action recognition. Most of the existing skeleton-based approaches use either the joint locations or the joint angles to represent the human skeleton. In this paper, we introduce and evaluate a new family of skeletal representations for human action recognition, which we refer to as R3DG features. The proposed representations explicitly model the 3D geometric relationships between various body parts using rigid body transformations, i.e., rotations and translations in 3D space. Using the proposed skeletal representations, human actions are modeled as curves in R3DG feature spaces. Finally, we perform action recognition by classifying these curves using a combination of dynamic time warping, Fourier temporal pyramid representation and support vector machines. Experimental results on five benchmark action datasets show that the proposed representations perform better than many existing skeletal representations. The proposed approach also outperforms various state-of-the-art skeleton-based human action recognition approaches.

@&#INTRODUCTION@&#
Human action recognition has been an active area of research for the past several decades due to its applications in surveillance, video games, human computer interaction, robotics, health care, etc. In the past few decades, several approaches have been proposed for recognizing human actions from monocular RGB video sequences (Aggarwal and Ryoo, 2011; Turaga et al., 2008). Unfortunately, the monocular RGB data is highly sensitive to various factors like illumination changes, variations in view-point, occlusions and background clutter. Moreover, monocular video sensors can not fully capture the human motion in a 3D space. Hence, despite significant research efforts over the past few decades, human action recognition still remains a challenging problem.Human body can be represented as an articulated system of rigid segments connected by joints, and human motion can be considered as a continuous evolution of the spatial configuration of these rigid segments (Zatsiorsky, 1997). So, if we can reliably extract the human skeleton, action recognition can be performed by classifying its temporal evolution. Using skeletal data for action recognition has several advantages such as ease of interpretability, low processing time, fast/cheap transmission and storage, etc. Skeletal data makes it easier to analyze which part of the body is playing a major role in discriminating one action against the other, and allows us to correlate this with human interpretation of motion. Interpretability is an important factor in various applications such as exercise monitoring, human computer interaction, post-surgery rehabilitation, etc. Skeletons provide a compact low-dimensional representation that can be stored easily, transmitted and processed quickly. Storage and transmission are critical in applications where the recognition module runs on a central server.Unfortunately, extracting the human skeleton from monocular RGB videos is a very difficult task (Moeslund et al., 2006). Sophisticated motion capture systems can be used to get the 3D locations of landmarks placed on the human body, but such systems are very expensive, and require the user to wear a motion capture suit with markers which can hinder natural movements. With the recent availability of cost-effective depth sensors, extracting the human skeleton has become relatively easier. These sensors provide 3D depth data of the scene, which is robust to illumination changes and offers more useful information to infer human skeletons. Recently, a quick method was proposed in Shotton et al. (2011) to accurately estimate the 3D positions of skeletal joints using a single depth image. These recent advances have generated a renewed interest in skeleton-based human action recognition.Existing skeleton-based action recognition approaches can be broadly grouped into two main categories: joint-based approaches and body part-based approaches. Inspired by the moving lights display experiment of Johansson (1973), joint-based approaches consider the human skeleton as a set of points (Fig. 1, left). These approaches try to model the motion of either individual joints or combinations of multiple joints using various features like joint positions (Hussein et al., 2013; Lv and Nevatia, 2006; Reyes et al., 2011; Sheikh et al., 2005), joint orientations with respect to a fixed root node (Xia et al., 2012; Shao and Li, 2013), pairwise relative joint positions (Wang et al., 2012b; Wei et al., 2013; Yang and Tian, 2014a), etc. On the other hand, motivated by the 3D-shape representations of Marr and Nishihara (1978), body part-based approaches consider the human skeleton as a connected set of rigid segments (Fig. 1, right). These approaches either model the temporal evolution of individual body parts (Yacoob and Black, 1998) or focus on directly-connected pairs of body parts and model the temporal evolution of joint angles (Gavrila and Davis, 1995, Ofli et al., 2014; Ohn-bar and Trivedi, 2013).In this paper, we introduce a new family of body part-based skeletal representations for recognizing human actions. Inspired by the observation that for human actions, the relative geometry between various body parts (though not directly connected by a joint) provides a more meaningful description than their absolute locations (for example, clapping is more intuitively described using the relative geometry between the two hands), we explicitly model the relative 3D geometry between different body parts in our skeletal representations.Given two rigid body parts, their relative geometry can be described using the rigid body transformation (rotation and translation) required to take one body part to the position and orientation of the other. Hence, we use the rigid body transformations between all pairs of body parts to represent the human skeleton. Rigid body transformations in 3D space can be mathematically represented in various different ways using the special orthogonal group SO(3), quaternions, the special Euclidean group SE(3), and dual quaternions. Using these mathematical representations, we introduce a family of relative 3D geometry-based skeletal representations for action recognition, which we refer to as R3DG features.One of the major issues while working with skeletal-data is scale variation. This can be handled by normalizing all the skeletons (without changing the joint angles) such that their body part lengths are equal to the corresponding lengths of a fixed reference skeleton. Note that a full 3D rigid body transformation includes both rotation and translation. Interestingly, while the relative translations between various body parts vary with this scale normalization, the relative rotations do not change (even if the body parts are not directly connected by a joint). Hence, we can get scale-invariant skeletal representations by using only the relative rotations between different body parts. In this work, we experimentally show that just by using the relative 3D rotations, we can get a classification accuracy that is close to the accuracy obtained by using the full rigid body transformation-based representations computed from scale-normalized skeletons.Using any of the proposed skeletal representations, human actions can be modeled as curves (Fig. 2) in an R3DG feature space, and action recognition can be performed by classifying these curves. Irrespective of the skeletal representation being used, classification of temporal sequences into different action categories is a difficult problem due to various issues like rate variations, temporal misalignment, noise, etc. To handle rate variations, for each action category, we compute a nominal curve using dynamic time warping (Müller, 2007), and warp all the curves to this nominal curve. Then, we represent the warped curves using the low frequency Fourier temporal pyramid (FTP) representation, which was shown to be robust to noise and temporal misalignment in Wang et al. (2012b). Finally, classification is performed using a support vector machines (SVM) classifier with the FTP representation.A preliminary version of this work appeared in Vemulapalli et al. (2014). The following are the additional contributions of this work compared to the earlier version:1.We introduce and evaluate a new family of body part-based 3D skeletal representations for human action recognition. The proposed representations explicitly model the relative geometry between various body parts using 3D rigid body transformations. While Vemulapalli et al. (2014) used SE(3) to represent 3D rigid body transformations, we consider three other alternative representations based on SO(3), quaternions and dual quaternions. To the best of our knowledge, SE(3) and dual quaternions have not been explored before in the context of skeleton-based human action recognition.We introduce scale-invariant R3DG features that use only the relative 3D rotations between various body parts.We experimentally show that the proposed representations outperform many existing skeletal representations by evaluating them on five action datasets (only three were used in Vemulapalli et al. (2014)). We also include comparisons with the skeletal quad descriptor (Evangelidis et al., 2014) and the relational pose features (Yao et al., 2012; Yun et al., 2012), which were missing in Vemulapalli et al. (2014).To make the paper self-contained, we include Sections 3 and 4 which provide relevant background information on SO(3), SE(3), quaternions, and dual quaternions.Organization: We provide a brief overview of existing skeleton-based action recognition approaches in Section 2, and briefly discuss SO(3), SE(3), quaternions, and dual quaternions in Sections 3 and 4. We introduce the proposed family of R3DG features in Section 5, and describe the temporal modeling and classification approach in Section 6. Experimental results and conclusions are presented in Sections 7 and 8, respectively.Notations: We useRnto represent the n-dimensional real vector space, and I to denote an identity matrix of appropriate size. Vectors are represented with an arrow on top. We use0→to represent the zero vector inR3. The determinant of a matrix R is represented using det(R) and the ℓ2-norm of a vectorv→is denoted by∥v→∥2. We use ⊗ to represent the direct product between groups and ⊕ to represent the direct sum between vector spaces.In this section, we provide a brief overview of various existing skeleton-based human action recognition approaches. Various depth map-based action recognition approaches (Chen et al., 2015; 2014; Hu et al., 2015; Kong and Fu, 2015; Lu et al., 2014; Oreifej and Liu, 2013; Rahmani et al., 2014; Wang et al., 2012a; Xia and Aggarwal, 2013; Yang and Tian, 2014b; Yang et al., 2012; Zhang and Tian, 2015; Zhu et al., 2014) have also been proposed in the recent past, which use various features extracted from the 3D depth data. Since the focus of this work is on skeleton-based action recognition, we refer the readers to Chen et al. (2013); Ye et al. (2013) for a review of depth map-based recognition approaches.Existing skeleton-based action recognition approaches can be broadly grouped into two main categories: joint-based approaches and body part-based approaches. While the joint-based approaches consider the human skeleton as a set of points, the body part-based approaches consider the human skeleton as a connected set of rigid segments. Approaches that use joint angles for representing the human skeleton can be classified as part-based approaches since joint angles measure the geometry between pairs of body parts that are directly connected to each other.A set of 13 joint trajectories in XYZT space was used to represent human actions in Sheikh et al. (2005), and their affine projections were compared using a subspace angles-based similarity measure. In Lv and Nevatia (2006), the trajectories of individual joints and groups of joints were modeled using hidden Markov models (HMMs). Each HMM was considered as a weak classifier, which were then combined using AdaBoost. HMMs were also used in Gu et al. (2010) to model the joint trajectories of whole body, upper body and lower body separately for performing action recognition.The 3D joint locations were combined with silhouette-based features in Chaaraoui et al. (2013), and their temporal evolutions were compared using dynamic time warping (DTW). Dynamic time warping was also used in Reyes et al. (2011) for comparing the sequences of joint positions. Instead of giving equal weight to all the joints in the DTW distance computation, a feature weighting approach was used in Reyes et al. (2011), where each joint was assigned its own weight. In Hussein et al. (2013), the temporal evolutions of joint locations were modeled using a temporal hierarchy of covariance features, and action recognition was performed using an SVM. In Gowayyed et al. (2013), the 3D trajectory of each joint was projected onto three Cartesian planes to get three 2D trajectories. Each 2D trajectory was represented using the histogram of displacements between consecutive points. The histograms from all the joints were concatenated to get the final feature representation, which was classified using an SVM. Recently, recurrent neural networks were used in Du et al. (2015) for modeling the temporal dynamics of skeletal joints.A view invariant representation of human skeleton was obtained in Xia et al. (2012) by quantizing the 3D joint locations into histograms based on their orientations with respect to a coordinate system attached to the hip center. The temporal evolutions of this representation were modeled using HMMs. In Zanfir et al. (2013), human skeletons were represented using 3D joint positions, their first and second order derivatives, i.e., joint velocities and accelerations, and a nearest neighbor-based approach was used to perform low-latency action recognition. In Shao and Li (2013), one of the joints was selected as a root joint, and all the remaining joints were represented using their orientations with respect to a coordinate system attached to the root joint. The temporal evolutions of this representation were compared using dynamic time warping.In Wang et al. (2012b); Wei et al. (2013), pairwise relative positions of the joints were used to represent the human skeleton, and the temporal evolutions of this representation were modeled using wavelets (Wei et al., 2013) and low-frequency Fourier coefficients (Wang et al., 2012b). A similar skeletal representation was also used in Wang and Wu (2013), where a discriminative learning-based temporal alignment method was used for comparing temporal sequences.In Ellis et al. (2013), the human skeleton was represented using distances between all pairs of joints in the current frame, distances between all pairs of joints in the current frame and the previous frame, distances between all pairs of joints in the current frame and the first frame of the sequence. Action recognition was then performed using a logistic regression-based approach. In Yang and Tian (2014a), the human skeleton was represented using relative joint positions, temporal displacements of the joints, and offsets of the joints with respect to the initial frame. Action classification was then performed using the Naive-Bayes nearest neighbor rule in a low-dimensional space obtained using principal component analysis (PCA). A similar representation was also used in Zhu et al. (2013) along with random forests.A local skeleton descriptor, referred to as skeletal quad, was introduced in Evangelidis et al. (2014), which encodes the relative position of joint quadruples. This descriptor represents a set of four joints using the coordinates of third and fourth joints in a coordinate system with the first joint as the origin and the second joint as (1, 1, 1). These skeletal quads were combined with Fisher vectors (Jaakkola and Haussler, 1998) and a linear SVM to perform action recognition. An interesting aspect of this descriptor is that it can be used to represent the relative 3D geometry between two body parts (since two body parts can be considered as four joints). However, the main difference between the skeletal quad descriptor and the proposed R3DG features is that while the proposed features directly use the translation and rotation between body parts, the skeletal quad descriptor encodes this information indirectly using the joint coordinates.In Wang et al. (2013), human skeleton was divided into five parts and each part was represented using the coordinates of the joints that belonged to the part. Then, a dictionary of pose templates was learned for each body part, and these templates were used to obtain a quantized representation of part poses. The authors further defined spatial-part-sets to capture the spatial configurations of multiple body parts, and temporal-part-sets to capture the joint pose evolutions of multiple body parts. Finally, the bag-of-words model was used to get the action representation, which was classified using one-vs-one intersection kernel SVM.Most of the above mentioned approaches use either the joint positions or the relative joint positions to represent the human skeleton. Different from these, Müller et al. (2005) introduced various types of relational pose features that describe geometric relations between specified joints of the skeleton, and used them successfully for indexing and retrieval of motion capture data. Similar features were later used in Yao et al. (2011); 2012); Yun et al. (2012) for skeleton-based human action recognition.In Yacoob and Black (1998), the human body was divided into five different parts, and human actions were represented using the motion parameters of individual parts like horizontal and vertical translations, in-plane rotations, etc. Principal component analysis was used to represent an action as a linear combination of a set of basis actions, and classification was performed by comparing the PCA coefficients. In Chaudhry et al. (2013), human skeletons were hierarchically divided into smaller parts and each body part was represented using certain bio-inspired shape features. The temporal evolutions of these bio-inspired features were modeled using linear dynamical systems.In Gavrila and Davis (1995), human skeletons were represented using 3D joint angles, and the temporal evolutions of this representation were compared using DTW. While Campbell and Bobick (1995) represented human actions as curves in low-dimensional phase spaces related to joint angles, Ohn-bar and Trivedi (2013) represented human actions using pairwise affinities between joint angle trajectories. In Sung et al. (2012), human skeletons were represented using joint angle quaternions. These skeletal features were augmented with RGB and depth-based HOG features, and a maximum entropy Markov model was used for action detection. In Ofli et al. (2014), a set of few informative skeletal joints was selected at each time instance based on highly interpretable measures such as mean and variance of joint angles, angular velocity of the joints, etc. Human actions were represented as sequences of these informative joints, which were compared using the normalized edit distance.In this section, we discuss two matrix Lie groups that are of interest to us, namely the special orthogonal group SO(3) and the special Euclidean group SE(3). While SO(3) is commonly used for representing 3D rotations, SE(3) is used for representing 3D rigid body transformations. We refer the readers to Hall (2003) for a detailed introduction to general Lie groups, and Murray et al. (1994) for further details on SO(3) and SE(3).A Lie group G is a group that is also a smooth manifold. The tangent spacegat the identity element e of G is referred to as the Lie algebra of G. A matrix Lie group is a Lie group of n × n invertible matrices with the usual matrix multiplication and inversion as the group multiplication and inversion operations, and the n × n identity matrix as the group identity element.The mapping from a Lie algebra to the corresponding Lie group, referred to as the Lie exponential map, is given byexpG(u→)=γu→(1),whereγu→:R→Gis the unique one-parameter subgroup of G whose tangent vector at the identity element e is equal tou→∈g. The inverse of exponential map is known as logarithm map, and is denoted by logG. Fig. 3gives an illustration of the Lie exponential and logarithms maps. In the case of matrix Lie groups, the Lie exponential and logarithm maps are given by(1)expG(u→)=eu→,logG(g)=log(g),where e and log represent the usual matrix exponential and logarithm respectively.The special orthogonal group SO(3) is a three dimensional matrix Lie group formed by the set of all 3 × 3 matrices R that satisfy the following constraints:(2)R⊤R=I,det(R)=1.The Lie algebra of SO(3), denoted byso(3),is the three dimensional vector space spanned by the set of all 3 × 3 skew symmetric matrices. For any element(3)A=[0−a3a2a30−a1−a2a10]∈so(3),its vector representation is given byvec(A)=[a1,a2,a3]. Since SO(3) is a matrix Lie group, the Lie exponential and logarithm maps between SO(3) andso(3)are given by(4)expSO(3)(A)=eA,logSO(3)(R)=log(R).The logarithm map is not unique in the case of SO(3). In this work, we use thelog(R)with the smallest norm.Elements of SO(3) are commonly used to represent 3D rotations. Letz→′be a 3D point obtained by rotatingz→∈R3by an angle θ about an axisn→. Then, we have(5)z→′=eskew(θn→)z→,whereskew(θn→)is a skew-symmetric matrix that satisfiesvec(skew(θn→))=θn→.Hence, the matrixeskew(θn→)∈SO(3)represents the 3D rotation by an angle θ about an axisn→.Interpolation: Various approaches have been proposed in the past for interpolation on SO(3) (Park and Ravani, 1997). In this paper, we use a simple piecewise geodesic interpolation scheme. GivenR1,⋯,Rm∈SO(3)at time instancest1,⋯,tmrespectively, we use the following curve for interpolation:(6)γ(t)=RiexpSO(3)(t−titi+1−tiAi)fort∈[ti,ti+1],whereAi=logSO(3)(Ri−1Ri+1)fori=1,2,⋯,m−1.SO(3)⊗⋯⊗SO(3): We can combine multiple SO(3) groups using the direct product ⊗ to form a new Lie group(7)SO(3)n:=SO(3)⊗⋯⊗SO(3)with the corresponding Lie algebra(8)so(3)n:=so(3)⊕⋯⊕so(3).The Lie exponential and logarithm maps for(A1,⋯,An)∈so(3)nand(R1,⋯,Rn)∈SO(3)nare given by(9)expSO(3)n(A1,⋯,An)=(eA1,⋯,eAn),logSO(3)n(R1,⋯,Rn)=(log(R1),⋯,log(Rn)).Interpolation on SO(3)ncan be performed by simultaneously interpolating on individual SO(3).The special Euclidean group SE(3) is a six dimensional matrix Lie group formed by the set of all 4 × 4 matrices of the form(10)P(R,d→)=[Rd→01],d→∈R3,R∈SO(3).The Lie algebra of SE(3), denoted byse(3),is the six dimensional vector space spanned by the set of all 4 × 4 matrices of the form(11)B=[0−a3a2w1a30−a1w2−a2a10w30000].The vector representation ofB∈se(3)is given by(12)vec(B)=[a1,a2,a3,w1,w2,w3].Since SE(3) is a matrix Lie group, the Lie exponential and logarithm maps between SE(3) ansse(3)are given by(13)expSE(3)(B)=eB,logSE(3)(P)=log(P).The logarithm map is not unique in the case of SE(3). In this work, we use thelog(P)with the smallest norm.Elements of SE(3) are commonly used to represent 3D rigid body transformations. Letz→′be a 3D point obtained by transformingz→∈R3using a rotation by an angle θ about an axisn→followed by a translationd→. Then, we have(14)[z→′1]=[eskew(θn→)d→01][z→1].Hence, the matrix[Rd→01]∈SE(3),whereR=eskew(θn→),represents the 3D rigid body transformation composed of a rotation by an angle θ about an axisn→and a translationd→.Interpolation: Various approaches have been proposed in the past for interpolation on SE(3) (Belta and Kumar, 2002; Zefran and Kumar, 1998). In this paper, we use a simple piecewise interpolation scheme based on screw motions (Zefran et al., 1996). GivenP1,⋯,Pm∈SE(3)at time instancest1,⋯,tmrespectively, we use the following curve for interpolation:(15)γ(t)=PiexpSE(3)(t−titi+1−tiBi)fort∈[ti,ti+1],whereBi=logSE(3)(Pi−1Pi+1)fori=1,2,⋯,m−1.SE(3)⊗⋯⊗SE(3): We can combine multiple SE(3) groups using the direct product ⊗ to form a new Lie group(16)SE(3)n:=SE(3)⊗⋯⊗SE(3)with the corresponding Lie algebra(17)se(3)n:=se(3)⊕⋯⊕se(3).The Lie exponential and logarithm maps for(B1,⋯,Bn)∈se(3)nand(P1,⋯,Pn)∈SE(3)nare given by(18)expSE(3)n(B1,⋯,Bn)=(eB1,⋯,eBn),logSE(3)n(P1,⋯,Pn)=(log(P1),⋯,log(Pn)).Interpolation on SE(3)ncan be performed by simultaneously interpolating on individual SE(3).In this section, we provide a brief introduction to quaternions and dual quaternions. While quaternions are commonly used to represent 3D rotations, dual quaternions are used to represent the full 3D rigid body transformations. We refer the readers to Broida et al. (1990); Kavan et al. (2008); Kenwright (2012); McCarthy (1990); Young and Chellappa (1990) for further details on these topics.The set of quaternionsQis equivalent to the 4-dimensional vector spaceR4equipped with the quaternion multiplication operation. Let {e0, e1, e2, e3} be the canonical basis for the vector spaceR4. The quaternion multiplication is defined by giving the following multiplication table for the basis:(19)e0e1=e1e0=e1,e1e2=−e2e1=e3,e0e2=e2e0=e2,e2e3=−e3e2=e1,e0e3=e3e0=e3,e3e1=−e1e3=e2,e0e0=e0,e1e1=e2e2=e3e3=−1.A 4-dimensional quaternionq→is commonly represented as(sq,v→q),wheresq∈Ris referred to as the scalar or real part andv→q∈R3is referred to as the vector or imaginary part. Addition of two quaternionsp→=(sp,v→p)andq→=(sq,v→q)is given by(20)p→+q→=(sp+sq,v→p+v→q).Using (19), multiplication ofp→andq→can be computed as(21)p→q→=(spsq−v→p⊙v→q,spv→q+sqv→p+v→p×v→q),wherev→p⊙v→qandv→p×v→qrespectively represent the dot product and cross product betweenv→pandv→q. Note that quaternion multiplication is not commutative.The conjugateq→¯,the norm∥q→∥,and the exponentialeq→of a quaternionq→=(sq,v→q)are given by(22)q→¯=(sq,−v→q),∥q→∥=q→q→¯=sq2+∥v→q∥22,eq→=(esqcos(∥v→q∥),esqsin(∥v→q∥)v→q∥v→q∥).The quaternions with unit norm are known as unit quaternions. The set of unit quaternions, denoted byUQ,forms a Lie group with quaternion multiplication as the group multiplication operation, andq→e=(1,0→)as the group identity element. The Lie algebra ofUQ,denoted byuq,is the three dimensional vector space spanned by the set of purely imaginary quaternions. The Lie exponential and logarithm maps forw→∈uqandq→=(sq,v→q)∈UQare given by(23)expUQ(w→)=ew→=(cos(∥w→∥),sin(∥w→∥)w→∥w→∥),logUQ(q→)=cos−1(sq)v→q1−sq2.Unit quaternions are commonly used to represent rotations in 3D space. Letz→be a 3D point, andq→z=(0,z→)be its quaternion representation. Letz→′be a 3D point obtained by rotatingz→by an angle θ about an axisn→,andq→z′=(0,z→′)be its quaternion representation. Then, we haveq→z′=r→q→zr→¯,wherer→=en→θ2. Hence, the unit quaternion(24)en→θ2=(cos(θ2),n→sin(θ2))represents the 3D rotation by an angle θ about the axisn→. We can easily convert between unit quaternion and SO(3) representations using(25)r→=expUQ(w→2),w→=vec(logSO(3)(R)),R=expSO(3)(skew(w→)),w→=2logUQ(r→).UQ⊗⋯⊗UQ: We can combine multipleUQgroups using the direct product ⊗ to form a new Lie group(26)UQn:=UQ⊗⋯⊗UQwith the corresponding Lie algebra(27)uqn:=uq⊕⋯⊕uq.The set of dual quaternionsDis the extension of quaternions using dual number theory. Each dual quaternion consists of eight elements or two quaternions:(28)ζ→=q→r+ϵq→d,whereq→r=(sr,v→r),q→d=(sd,v→d)are quaternions, and ϵ is the dual operator, i.e.,ϵ2=0,ϵ≠0. The dual quaternion addition, multiplication, conjugate and magnitude are given by(29)ζ→1+ζ→2=(q→1r+q→2r)+ϵ(q→1d+q→2d),ζ→1ζ→2=q→1rq→2r+ϵ(q→1rq→2d+q→1dq→2r),ζ→¯=q→¯r+ϵq→¯d,∥ζ→∥=ζ→ζ→¯=∥q→r∥+ϵ(srsd+v→r⊙v→d∥q→r∥).Note that the magnitude of dual quaternion is a dual number. Dual quaternions that satisfy(30)∥ζ→∥=1,i.e.,∥q→r∥=1,srsd+v→r⊙v→d=0,are called unit dual quaternions. We denote the set of all unit dual quaternions usingUD.While a unit quaternion can represent a 3D rotation, a unit dual quaternion can represent a full 3D rigid body transformation, i.e, both rotation and translation. Letz→be a 3D point, andζ→z=(1,0→)+ϵ(0,z→)be its dual quaternion representation. Letz→′be a 3D point obtained by transformingz→using a rotation by an angle θ about an axisn→followed by a translationd→,andζ→z′=(1,0→)+ϵ(0,z→′)be its dual quaternion representation. Then, we haveζ→z′=ζ→rdζ→zζ→¯rd,where(31)ζ→rd=r→+ϵ(12t→r→)∈UD,r→=en→θ2∈UQ,t→=(0,d→)∈Q.Hence, the unit dual quaternionζ→rdrepresents the 3D rigid body transformation composed of a rotation by an angle θ about an axisn→and a translationd→.LetS=(V,E)be a skeleton (Fig. 1), whereV={v1,⋯,vN}denotes the set of joints andE={e1,⋯,eM}denotes the set of oriented rigid body parts. Let em1, em2 respectively denote the starting and end points of em.Given a pair of body parts emand en, to describe their relative 3D geometry, we use the rigid body transformations required to take one body part to the position and orientation of the other. A full rigid body transformation T is composed of a rotation by an angle θ about an axisn→and a translationd→. To measure the rigid body transformationTm,n=(θm,n,n→m,n,d→m,n)required to take ento the position and orientation of em, we use a local coordinate system attached to en(Fig. 4(a)). Similarly, to measure the rigid body transformationTn,m=(θn,m,n→n,m,d→n,m)required to take emto the position and orientation of en, we use a local coordinate system attached to em(Fig. 4(b)). We obtain the local coordinate system of a body part emby rotating (with minimum rotation) and translating the global coordinate system such that em1 becomes the origin and emcoincides with the x-axis.At first glance it might appear that using only Tm, nor Tn, mwould be sufficient to represent the relative geometry between emand en. Consider the case in which enis rotating about an axis parallel to em. Though there is relative motion between the two, Tm, nwill not change. Similarly, if emis rotating about an axis parallel to en, then Tn, mwill not change. So, if we represent the relative geometry using only one of them, the representation will not change under certain kinds of relative motions, which is undesirable. Hence, we use both Tm, nand Tn, mto represent the relative geometry between emand en. Note that both Tm, nand Tn, mdo not change only when both emand enundergo same rotation and translation, i.e., only when there is no relative motion between them.Using the relative geometry between all pairs of body parts, we represent a skeleton S at time instance t using(32)C(t)=(T1,2(t),T2,1(t),⋯,TM−1,M(t),TM,M−1(t)),where M is the number of body parts. The total number of rigid body transformations used in the skeletal representation isK=M(M−1). Using the proposed representation, a skeletal sequence describing an action can be represented as a curve {C(t), t ∈ [0, T′]}, and action recognition can be performed by classifying such curves into different action categories.Note that we are using only the relative measurements Tm, n(t) in our skeletal representation. We also performed experiments by adding the absolute 3D locations of body parts to the skeletal representation. The 3D location of a body part emcan be described using its rigid body transformation Tmwith respect to global x-axis (Fig. 4(c)). But, this did not give any improvement, suggesting that the absolute measurements are redundant when the relative measurements are used.There are multiple ways to mathematically represent the rigid body transformations in 3D space. In this work, we consider the following four representations:SE(3),SO(3)⊗R3,UQ⊗R3,andUD. Using each representation we get a full rigid body transformation-based R3DG feature.SE(3):Each rigid body transformation Ti, j(t) is represented as a member of SE(3) using the 4 × 4 matrix(33)Pi,j(t)=[Ri,j(t)d→i,j(t)01],where Ri, j(t) is the SO(3) representation of 3D rotation(θi,j(t),n→i,j(t)),and the entire skeleton is represented using(34)(P1,2(t),P2,1(t),⋯,PM−1,M(t),PM,M−1(t))∈SE(3)K.Since SE(3)Kis a curved space, classification of action curves in this space is not an easy task. Standard classification approaches like SVM, which are defined for vector space representations, are not directly applicable to the non-vector space SE(3)K. Also, temporal modeling approaches like Fourier analysis are not applicable to this space. Note that the standard Fourier analysis is defined for functions whose output varies along the real line. Here, the action curve C(t) evolves in the non-Euclidean space SE(3)Kas a function of time, and the standard Fourier analysis is not defined for this case. To overcome these difficulties, we map the action curves from the Lie group SE(3)Kto its Lie algebrase(3)K,which is a6M(M−1)-dimensional vector space. The final representation of action curve C(t) is given by(35)C1(t)=[vec(log(P1,2(t))),vec(log(P2,1(t))),⋯,vec(log(PM−1,M(t))),vec(log(PM,M−1(t)))].SO(3)⊗R3:In this case, the rotations and translations are separately represented as members of SO(3) andR3respectively, and the entire skeleton is represented using(36)(R1,2(t),R2,1(t),⋯,RM−1,M(t),RM,M−1(t),d→1,2(t),d→2,1(t),⋯,d→M−1,M(t),d→M,M−1(t))∈SO(3)K⊗R3K.Similar to SE(3)K, the Lie group SO(3)Kis also a curved space. So, we map the action curves fromSO(3)K⊗R3Kto the6M(M−1)-dimensional vector spaceso(3)K⊗R3Kby mapping the rotational part from the Lie group SO(3)Kto its Lie algebraso(3)K. Note that the translational part remains the same. The final vector space representation of action curve C(t) is given by(37)C2(t)=[vec(log(R1,2(t))),vec(log(R2,1(t))),⋯,vec(log(RM−1,M(t))),vec(log(RM,M−1(t))),d→1,2(t),d→2,1(t),⋯,d→M−1,M(t),d→M,M−1(t)].UQ⊗R3:In this case, the rotations and translations are separately represented as elements ofUQandR3respectively, and the entire skeleton is represented using(38)(r→1,2(t),r→2,1(t),⋯,r→M−1,M(t),r→M,M−1(t),d→1,2(t),d→2,1(t),d→M−1,M(t),d→M,M−1(t))∈UQK⊗R3K,wherer→i,j(t)=(si,j(t),v→i,j(t))is the unit quaternion representation of 3D rotation(θi,j(t),n→i,j(t)).Similar to SO(3) and SE(3), the Lie groupUQis also a curved surface. In fact, the set of unit quaternions forms a three dimensional unit sphere inR4. Hence, to get a vector space representation, we directly use the 4-dimensional ambient space representation of unit quaternions.11Here, we could have used the Lie algebra representation instead of the ambient space representation. But, theuqrepresentation is nothing but a scaled version (a scaling factor of 1/2) ofso(3)representation (refer to Eq. (25)). Sinceso(3)representation is already being used in the case ofSO(3)⊗R3,we chose to use the ambient space representation for unit quaternions.With this, we get the following7M(M−1)-dimensional vector space representation for action curve C(t):(39)C3(t)=[s1,2(t),v→1,2(t),s2,1(t),v→2,1(t),⋯,sM−1,M(t),v→M−1,M(t),sM,M−1(t),v→M,M−1(t),d→1,2(t),d→2,1(t),⋯,d→M−1,M(t),d→M,M−1(t)].UD:Each rigid body transformation Ti, j(t) is represented using a unit dual quaternion(40)ζ→i,j(t)=(si,jr(t),v→i,jr(t))+ϵ(si,jd(t),v→i,jd(t)).The set of unit dual quaternions does not form a vector space. Hence, similar to the quaternions, we use the 8-dimensional ambient space representation for unit dual quaternions, which gives the following8M(M−1)-dimensional vector space representation for action curve C(t):(41)C4(t)=[s1,2r(t),v→1,2r(t),s1,2d(t),v→1,2d(t),s2,1r(t),v→2,1r(t),s2,1d(t),v→2,1d(t)⋯,sM−1,Mr(t),v→M−1,Mr(t),sM−1,Md(t),v→M−1,Md(t),sM,M−1r(t),v→M,M−1r(t),sM,M−1d(t),v→M,M−1d(t)].One of the standard ways to handle scale variations in skeletal data is to resize all the skeletons to a fixed size. This can be done by normalizing the skeletons (without changing the joint angles) such that their body part lengths are equal to the corresponding lengths of a reference skeleton. Interestingly, while the translations between different body parts vary with this scale normalization, the 3D rotations do not change (even if the body parts are not directly connected by a joint). So, by using only the rotations between different body parts, we can get the following two scale-invariant R3DG features based on theso(3)andUQrepresentations of rotations:(42)C5(t)=[vec(log(R1,2(t))),vec(log(R2,1(t))),⋯,vec(log(RM−1,M(t))),vec(log(RM,M−1(t)))],C6(t)=[s1,2(t),v→1,2(t),s2,1(t),v→2,1(t),⋯,sM−1,M(t),v→M−1,M(t),sM,M−1(t),v→M,M−1(t)].Note that at any time instance t,C5(t)is a3M(M−1)-dimensional vector andC6(t)is a4M(M−1)-dimensional vector. Table 1summarizes the proposed family of R3DG features.Classification of vector space curves into different action categories is not a straightforward task due to various issues like rate variations, temporal misalignment, noise, etc. Following Kulkarni et al. (2015); Veeraraghavan et al. (2009), we use DTW to handle rate variations. During training, for each action category, we compute a nominal curve using the algorithm described in Table 2, and warp all the training curves to this nominal curve. We use the squared Euclidean distance for DTW computations. Note that for computing a nominal curve all the curves should have equal number of samples. To achieve this, we re-sample the action curves using the interpolation algorithms presented for SO(3) and SE(3) in Section 3.1 and 3.2, respectively. In the case of quaternions, we first interpolate the rotations on SO(3) and then convert them to unit quaternions. In the case of dual quaternions, we first interpolate the rigid body transformations on SE(3) and then convert them to unit dual quaternions.After the DTW step, we represent the warped curves using the low-frequency FTP representation that was shown to be robust to temporal misalignment and noise in Wang et al. (2012b). We apply FTP for each dimension separately and concatenate the low-frequency Fourier coefficients to obtain the final feature vector. Action recognition is performed by classifying the final feature vectors using one-vs-all SVM. Fig. 5gives an overview of the proposed skeleton-based action recognition approach. The top row shows all the steps involved in training and the bottom row shows all the steps involved in testing.In this section, we evaluate the proposed R3DG features on five action datasets: MSRAction3D (Li et al., 2010), UTKinect-Action (Xia et al., 2012), Florence3D (Seidenari et al., 2013), MSRPairs (Oreifej and Liu, 2013) and G3D (Bloom et al., 2012). Please refer to Table 3for details about these datasets.Basic pre-processing: To make the skeletal data invariant to the absolute location of human in the scene, all the 3D joint coordinates were transformed from world coordinate system to a person-centric coordinate system by placing the hip center at the origin. For each dataset, we took one of the subjects as reference, and normalized all the other skeletons (without changing their joint angles) such that their body part lengths are equal to the corresponding lengths of the reference skeleton.22We also performed experiments by varying the reference subject, but the results did not vary much. The standard deviation in the recognition rate was around 0.2–0.3%.This normalization takes care of scale variations. We also rotated the skeletons such that the ground plane projection of the vector from left hip to right hip is parallel to the global x-axis.Alternative skeletal representations: To show the effectiveness of the proposed R3DG features, we compare them with the following alternative representations:•Joint positions (JP): Concatenation of 3D coordinates of all the jointsv1,⋯,vN(except the hip center).Relative positions of the joints (RJP): Concatenation of all the vectorsvivj→,1≤i<j≤N.Joint angles (JA): Concatenation of the quaternion representations of all the joint angles. We also tried theso(3)and Euler-angle representations for joint angles, but quaternions gave the best results. Here, we measure each joint angle in the local coordinate systems of both body parts associated with that angle.Relation pose features (RP): We use the joint distance, plane, normal plane, velocity and normal velocity features of Yun et al. (2012) computed from a single human skeleton.Individual body part locations (BPL): Each body part emis represented using its rigid body transformation Tmwith respect to global x-axis (Fig. 4(c)). Similar to R3DG features, we have six different BPL features:se(3),so(3)⊗R3,UQ⊗R3,UD,so(3),andUQ.Skeletal quads (SQ): We use the skeletal quad descriptor of Evangelidis et al. (2014) to describe the relative geometry between every pair of body parts.For a fair comparison, we use the same temporal modeling and classification approach described in Section 6 with all the representations. Table 4summarizes the alternative skeletal representations used for comparison.Parameters: For FTP representation, we used a three-level temporal pyramid with one-fourth of the segment length as the number of low-frequency coefficients. While using one or two levels for the temporal pyramid produced inferior results, going beyond three did not improve the results significantly. Changing the number of low-frequency coefficients from one-fourth of the segment length to one-third or one-fifth did not significantly change the accuracy (around 0.2%). The value of SVM parameter C was chosen using cross-validation. For each dataset, all the curves were re-sampled to have same length. The reference length was chosen to be the maximum number of samples in any curve in the dataset before re-sampling.Comparison with other skeletal representations:Table 5shows the recognition rates for various skeletal representations on five action datasets when the same temporal modeling and classification pipeline (DTW + FTP + linear SVM) is used. For all the datasets, we followed the cross-subject test setting, in which half of the subjects were used for training and the other half were used for testing. All the results reported in this table were averaged over ten different random combinations of training and test data. The best result in each column is shown in boldface style. We can see that all the proposed R3DG features perform better than all the alternative skeletal representations on all five datasets except the MSR-Pairs dataset where the RJP representation performs slightly better than some of the R3DG features. Specifically, the accuracy of the best R3DG feature is better than the accuracy of the best competing skeletal representation by 1.82% on the MSRAction3D dataset, 1.02% on the UTKinect dataset, 3.72% on the Florence3D dataset, 0.42% on the MSRPairs dataset, and 2.09% on the G3D dataset. These results clearly show the superiority of the proposed R3DG features.Contribution of the translational information: Comparing the recognition rates of rotation-based and full rigid body transformation-based R3DG features, we can see that on four out of five datasets, the contribution33By contribution, we mean the additional contribution of translational information in the presence of rotational information.of translational information is not that significant. The difference between the best recognition rates of rotation-based and full rigid body transformation-based R3DG features is 0.1% for the MSRAction3D dataset, 0.32% for the UTKinect dataset, 0.45% for the MSRPairs dataset, and 0% for the G3D dataset. Only on the Florence3D dataset, there is a significance difference of around 1.34%.Contribution of DTW and FTP modules: Our temporal modeling consists of DTW and FTP modules. To analyze the contribution of these modules to the final recognition accuracy, we performed experiments on the MSRAction3D and G3D datasets (the two largest datasets) by removing these modules from our action recognition pipeline. Table 6compares the final accuracy with and without the FTP module in the action recognition pipeline. As we can see, the FTP module contributes significantly to the final accuracy in most of the cases.Table 7compares the final accuracy with and without the DTW module in the action recognition pipeline. While the DTW module contributes significantly to the final accuracy in the case of MSRAction3D dataset, it does not change the accuracy much in the case of G3D dataset. This variation is expected since the contribution of DTW depends on the rate variations present in the dataset.Comparison with state-of-the-art approaches:Table 8compares the proposed approach with various existing skeleton-based action recognition approaches on MSRAction3D, UTKinect, Florence3D and G3D datasets.44To the best of our knowledge, all the results reported in the literature for the MSRPairs dataset are based on depth data.Since the focus of this work is on skeleton-based human action recognition, we use only skeleton-based approaches for comparison. Though combining skeletal features with depth-based features may improve the accuracy, feature fusion is beyond the scope of this work. We evaluated our approach using both linear and RBF kernel SVMs, and the kernel SVM performed slightly better on all datasets. When the RBF kernel was used, theUQR3DG feature gave the best result (among all R3DG features) for the G3D dataset and theUQ×R3R3DG feature gave the best result on all the remaining datasets. For the MSRAction3D dataset, we followed the standard protocol of using subjects 1, 3, 5, 7, 9 for training and the remaining for testing. For G3D, UTKinect and Florence3D datasets, we followed the cross-subject setting of Zhu et al. (2013) and used half of the subjects for training and the remaining for testing. Note that this is a more difficult setting compared to the leave-one-action-out scheme used for the UTKinect dataset in (Devanne et al., 2014; Presti et al., 2014; Xia et al., 2012) and the leave-one-actor-out scheme used for the Florence3D dataset in Devanne et al. (2014); Seidenari et al. (2013), where more subjects were used for training. We report the results averaged over ten random combinations of training and test data.The best accuracy on each dataset is shown in boldface style. We can see that the proposed approach gives the best classification accuracy on three out of four datasets. Specifically, it outperforms the state-of-the-art results by 2.72% on the UTKinect dataset, 3.39% on the Florence3D dataset and 5.99% on the G3D dataset. The proposed approach also outperforms many recent skeleton-based action recognition approaches on the MSRAction3D dataset. Note that the main focus of this work is on skeletal representation, and the proposed R3DG features clearly outperform various existing skeletal representations when the same classification pipeline is used with all the representations.In this paper, we introduced a family of body part-based 3D skeletal representations for human action recognition, which we refer to as R3DG features. The proposed representations explicitly model the relative 3D geometry between various body parts (though not directly connected by a joint) using rigid body transformations. We represented 3D rigid body transformations using SE(3),SO(3)⊗R3,UQ⊗R3,andUD,resulting in four different R3DG features. We also introduced scale-invariant R3DG features by using only the 3D rotations between various body parts. Using the proposed representations, we modeled the human actions as curves in R3DG feature spaces. Finally, we performed action recognition by classifying these curves using a combination of dynamic time warping, Fourier temporal pyramid representation and SVM. We experimentally showed that the proposed R3DG features perform better than various existing skeletal representations, and the proposed action recognition approach outperforms various state-of-the-art skeleton-based approaches.In our work, we used the relative geometry between all pairs of body parts. However, each action is usually characterized by the interactions of a specific set of body parts. Hence, we are planning to explore various strategies to automatically identify the set of body parts that differentiates a given action from the rest. While we focused only on actions performed by a single person in this work, we are planning to extend these representations to model multi-person interactions. We also plan to improve the results further by using the proposed skeletal representations with more complex classification approaches based on sparse representation, deep networks, etc.

@&#CONCLUSIONS@&#
