@&#MAIN-TITLE@&#
Service robot system with an informationally structured environment

@&#HIGHLIGHTS@&#
Introduction of architecture and components of the ROS–TMS.Integration of various data from distributed sensors for service robot system.Object detection system (ODS) using RGB-D camera.Motion planning for a fetch-and-give task using a wagon and a humanoid robot.Handing over an object to a human using manipulability of both a robot and a human.

@&#KEYPHRASES@&#
Informationally structured environment,Intelligent space,Distributed sensor,Service robot,Robot operating system,Motion planning,

@&#ABSTRACT@&#
Daily life assistance is one of the most important applications for service robots. For comfortable assistance, service robots must recognize the surrounding conditions correctly, including human motion, the position of objects, and obstacles. However, since the everyday environment is complex and unpredictable, it is almost impossible to sense all of the necessary information using only a robot and sensors attached to it. In order to realize a service robot for daily life assistance, we have been developing an informationally structured environment using distributed sensors embedded in the environment. The present paper introduces a service robot system with an informationally structured environment referred to the ROS–TMS. This system enables the integration of various data from distributed sensors, as well as storage of these data in an on-line database and the planning of the service motion of a robot using real-time information about the surroundings. In addition, we discuss experiments such as detection and fetch-and-give tasks using the developed real environment and robot.

@&#INTRODUCTION@&#
Aging of the population is a common problem in modern societies, and rapidly aging populations and declining birth rates have become more serious in recent years. For instance, the manpower shortage in hospitals and elderly care facilities has led to the deterioration of quality of life for elderly individuals. Robot technology is expected to play an important role in the development of a healthy and sustainable society. In particular, daily life assistance for elderly individuals in hospitals and care facilities is one of the most urgent and promising applications for service robots.For a service robot, information about its surrounding, such as the positions of objects, furniture, humans, and other robots is indispensable for safely performing proper service tasks. For example, in order to deliver an object requested by a user, a robot must know the location of the target object and which trajectory should be followed to reach the object. However, current sensing technology, especially for cases of robots equipped with external sensors, is not good enough to complete these tasks satisfactorily. For example, a vision system is susceptible to changes in lighting conditions and the appearances of objects. Moreover, the field of vision is rather narrow. Although occlusions can be partly solved by sensors on a mobile robot, background changes and unfavorable vibrations of a robot body make processes more difficult. In addition, the payload of a robot is not so high and computer resources are also limited.On the other hand, fixed sensors in an environment are more stable and can more easily gather information about the environment. If a sufficient number of sensors can be embedded in the environment in advance, occlusion is no longer a crucial problem. Information required to perform tasks is acquired by distributed sensors and transmitted to a robot on demand. The concept of making an environment smarter rather than the robot is referred to as an informationally structured environment [1–6].An informationally structured environment is a feasible solution for introducing service robots into our daily lives using current technology, and several systems that observe human behavior using distributed sensor systems and provide proper service tasks according to requests from human or emergency detection, which is triggered automatically, have been proposed [1,7–12]. Several service robots that act as companions to elderly people or as assistants to humans who require special care have been developed [13–18].We also have been developing an informationally structured environment for assisting in the daily life of elderly people in our research project, i.e., the Robot Town Project [2,19]. The goal of this project is to develop a distributed sensor network system covering a town-size environment consisting of several houses, buildings, and roads, and to manage robot services appropriately by monitoring events that occur in the environment. Events sensed by an embedded sensor system are recorded in the Town Management System (TMS), and appropriate information about the surroundings and instructions for proper services are provided to each robot [2].We also have been developing an informationally structured platform (Fig. 1) in which distributed sensors (Fig. 2a) and actuators are installed to support an indoor service robot (Fig. 2b). For example, in this platform, objects such as books, pens, pet bottles, chairs, and desks are detected by embedded sensors and RFID tags, and all of the data are stored in the TMS database. A service robot performs various service tasks according to the environmental data stored in the TMS database in collaboration with distributed sensors and actuators, for example, installed in a refrigerator to open a door.We herein introduce a new Town Management System called the ROS–TMS. In this system, the Robot Operating System (ROS [20]) is adopted as a communication framework between various modules, including distributed sensors, actuators, robots, and databases. Thanks to the ROS, we were able to develop a highly flexible and scalable system. Adding or removing modules such as sensors, actuators, and robots, to or from the system is simple and straightforward. Parallelization is also easily achievable.We herein report the followings:•Introduction of architecture and components of the ROS–TMSObject detection using a sensing system of the ROS–TMSFetch-and-give task using the motion planning system of the ROS–TMS.The detection and fetch-and-give tasks are among the most frequent tasks for elderly individuals in daily life and are a typical application task for the proposed system.The remainder of the present paper is organized as follows. After presenting related research in Section  2, we introduce the architecture and components of the ROS–TMS in Section  3. In Section  4, we describe the sensing system of the ROS–TMS for processing the data acquired from various sensors. Section  5 describes the robot motion planning system of the ROS–TMS used to design the trajectories for moving, gasping, giving, and avoiding obstacles using the information on the environment acquired by the sensing system. We present the experimental results for service tasks performed by a humanoid robot and the ROS–TMS in Section  6. Section  7 concludes the paper.A considerable number of studies have been performed in the area of informationally structured environments/spaces to provide human-centric intelligent services. Informationally structured environments are referred to variously as home automation systems, smart homes, ubiquitous robotics, kukanchi, and intelligent spaces, depending on the field of research and the professional experience of the researcher.Home automation systems or smart homes are popular systems that centralize the control of lighting, heating, air conditioning, appliances, and doors, for example, to provide convenience, comfort, and energy savings [21–23]. The informationally structured environment can be categorized in this system, but the system is designed to support not only human life but also robot activity for service tasks.Hashimoto and Lee proposed an intelligent space in 1996 [24]. Intelligent spaces (iSpace) are rooms or areas that are equipped with intelligent devices, which enable spaces to perceive and understand what is occurring within them [24–27]. These intelligent devices have sensing, processing, and networking functions and are referred to as distributed intelligent networked devices (DINDs). One DIND consists of a CCD camera to acquire spatial information and a processing computer, which performs data processing and network interfacing. These devices observe the position and behavior of both human beings and robots coexisting in the iSpace.Moreover, the concept of a physically embedded intelligent system (PEIS) has been introduced in 2005 [28]. A PEIS involves the intersection and integration of three research areas: artificial intelligence, robotics, and ubiquitous computing. Anything that consists of software components with a physical embodiment and interacts with the environment through sensors or actuators/robots is considered to be a PEIS, and a set of interconnected physically embedded intelligent systems is defined as a PEIS ecology. Inside a PEIS ecology, the software components should interact, communicate, and achieve goals together. Tasks can be achieved using either centralized or distributed approaches using the PEIS ecology [29,30].Ubiquitous robotics [31] involves the design and deployment of robots in smart network environments in which everything is interconnected. The authors define three types of Ubibots: software robots (Sobots), embedded robots (Embots), and mobile robots (Mobots), which can provide services using various devices through any network, at any place and at any time in a ubiquitous space (u-space). Embots can evaluate the current state of the environment using sensors, and convey that information to users. Mobots are designed to provide services and explicitly have the ability to manipulate u-space using robotic arms. A Sobot is a virtual robot that has the ability to move to any location through a network and to communicate with humans. The present authors have previously demonstrated the concept of a PIES using Ubibots in a simulated environment and u-space [32,33].In Europe, RoboEarth [34–36] is essentially a World Wide Web for robots, namely, a giant network and database repository in which robots can share information and learn from each other about their behavior and their environment. The goal of RoboEarth is to allow robotic systems to benefit from the experience of other robots, paving the way for rapid advances in machine cognition and behavior, and ultimately, for more subtle and sophisticated human–machine interactions.The informationally structured environment/space (also referred to as Kukanchi, a Japanese word meaning interactive human-space design and intelligence [37,38]) has received a great deal of attention in robotics research as an alternative approach to the realization of a system of intelligent robots operating in our daily environment. Human-centered systems require, in particular, sophisticated physical and information services, which are based on sensor networks, ubiquitous computing, and intelligent artifacts. Information resources and accessibility within an environment are essential for people and robots. The environment surrounding people and robots should have a structured platform for gathering, storing, transforming, and providing information. Such an environment is referred to as an informationally structured space by the IEEE Symposium on Robotic Intelligence in Informationally Structured Space (RiiSS [39]).In Section  5, we present a coordinate motion planning technique for a fetch-and-give including handing over an object to a person. The problem of handing over an object between a human and a robot has been studied in Human–Robot Interaction (HRI) [40–43]. In particular, the work that is closest to ours is the one by Dehais et al. [42]. In their study, physiological and subjective evaluation for a handing over task was presented. The performance of hand-over tasks were evaluated according to three criteria: legibility, safety and physical comfort. These criteria are represented as fields of cost functions mapped around the human to generate ergonomic hand-over motions. Although their approach is similar to our approach, we consider the additional criteria, that is, the manipulability of both a robot and a human for a comfortable and safety fetch-and-give task.The problem of pushing carts using robots has been reported in many studies so far [44–50]. The earlier studies in pushing a cart were reported using a single manipulator mounted on a mobile base [44,45]. In these systems, a single manipulator held a cart at a single point, and the planning of effector force to produce desired trajectories was discussed. The problem of towing a trailer has also been discussed as an application of a mobile manipulator and a cart [46]. This work is close to the approach in this paper, however, a pivot point using a cart is placed in front of the robot in our technique.The work that is closest to ours is the one by Scholz et al. They provided a solution for real time navigation in a cluttered indoor environment using 3D sensing [49]. Though the first attempt with the cart by Tan et al. [44,45] was limited to simple paths using an open-loop controller, Scholz et al. proposed the solution to execute smooth and arbitrary trajectories in a closed loop controller with PR2. Many previous works focus on the navigation and control problems for movable objects. On the other hand, we consider the problem including handing over an object to a human using a wagon, and propose a total motion planning technique for a fetch-and-give task with a wagon.We have been developing an informationally structured architecture referred to as the Town Management System (TMS) for assisting in the daily life of elderly people [2]. In the present paper, we extend the TMS and develop a new Town Management System called the ROS–TMS.A conceptual diagram of the ROS–TMS is shown in Fig. 3. This system has three primary components, i.e., real-world, database, and cyber-world components. Events occurring in the real world, such as user behavior or user requests, and the current situation of the real world, such as the positions of objects, humans, and robots are sensed by a distributed sensing system. The gathered information is then stored in the database. Appropriate service commands are planned using the environmental information in the database and are simulated carefully in the cyber world using simulators, such as choreonoid [51]. Finally, service tasks are assigned to service robots in the real world.In order to realize the above-described concept, the following functions are implemented in the ROS–TMS.1.Communication with sensors, robots, and databases.Storage, revision, backup, and retrieval of real-time information in an environment.Maintenance and providing information according to individual IDs assigned to each object and robot.Notification of the occurrence of particular predefined events, such as accidents.Task schedule function for multiple robots and sensors.Human–system interaction for user requests.Real-time task planning for service robots.The ROS–TMS has unique features such as high scalability and flexibility, as described below.•Modularity: The ROS–TMS consists of 73 packages categorized into 11 groups and 151 processing nodes. Re-configuration of structures, for instance adding or removing modules such as sensors, actuators, and robots, is simple and straightforward owing to the high flexibility of the ROS architecture.Scalability: The ROS–TMS is designed to have high scalability so that it can handle not only a single room but also a building and a town.Diversity: The ROS–TMS supports a variety of sensors and robots. For instance, Vicon MX (Vicon Motion Systems Ltd.), TopUrg (Hokuyo Automatic), Velodyne 32e (Velodyne Lidar), and Oculus Rift (Oculus VR) are installed in the developed informationally structured platform (Fig. 1) for positioning and human–system interaction. SmartPal-IV and V (Yaskawa Electric Corp.), AR Drone (Parrot), TurtleBot2 (Yujin Robot Co., Ltd.), a wheelchair robot, and a mobile robot with a 5-DOF manipulator are also provided in order to accomplish a variety of service tasks.Safety: Data gathered from the real world is used to perform simulations in the cyber world in order to evaluate the safety and efficiency of designed tasks. According to the simulation results, proper service plans are designed and provided by service robots in the real world.Privacy protection: One important restriction in our intelligent environment is to install a small number of sensors to avoid interfering with the daily activity of people and to reduce the invasion of their privacy as far as possible. For this reason, we do not install conventional cameras in the environment. Although the robots are equipped with several cameras, these cameras are only used in certain restricted scenarios.Economy: Sensors installed in an environment can be shared with robots and tasks, and thus we do not need to equip individual robots with numerous sensors. In addition, most sensors are processed by low-cost single-board computers in the proposed system. This concept has an advantage especially for the system consisting of multiple robots since robots can share the resources in the environment. Currently, the cost of a distributed sensor network may be higher than the cost of a single robot. However, we believe that distributed sensor network (or Internet of Things, IoT) will become a standard facility in our society in the near future, and the cost of a sensor network will be greatly reduced since all the devices will be designed to be connected each other.Some features mentioned above such as modularity, scalability, and diversity owe much to ROS’s outstanding features. On the other hand, economical or processing efficiency strongly depends on the unique features of ROS–TMS, since various information gathered by distributed sensor networks is structured and stored to the database and repeatedly utilized for planning various service tasks by robots or other systems.Fig. 4shows the overall architecture of ROM-TMS. This system is composed of five components: user, sensor, robot, task, and data. These components are also composed of sub-modules, such as the User Request sub-module for the user component, the Sensor Driver sub-module, the Sensing System and State Analyzer sub-modules for the sensor component, the Robot Controller, the Robot Motion Planning, and the Robot Service sub-modules for the robot component, the Task Scheduler sub-module for the task component, and the Database sub-module for the data component. Brief explanations of each node group are presented in Table 1. The details of these components will be described later herein.The sensing system (TMS_SS) is a component of the ROS–TMS that processes the data acquired from various environment sensors. In the current platform (Fig. 1), TMS_SS is composed of three sub-packages as described below and is described in Table 1.1.Floor sensing system (FSS)Intelligent cabinet system (ICS)Object detection system (ODS)The current platform (Fig. 1) is equipped with a floor sensing system to detect objects on the floor and people walking around. This sensing systems is composed of a laser range finder located on one side of the room and a mirror installed along another side of the room. This configuration allows a reduction of dead angles of the LRF and is more robust against occlusions [52]. An example of object detection using this system is shown in Fig. 6. People tracking is performed by first applying static background subtraction and then extracting clusters in the remainder of the measurements. Clusters are later tracked by matching profiles of cluster corresponding to legs and extending the motion using the accelerations of the legs. Moreover, this system can measure the poses of the robot and movable furniture such as a wagon using tags, which have encoded reflection patterns optically identified by the LRF [53].The cabinets installed in the room are equipped with RFID readers and load cells to detect the types and positions of the objects in the cabinet. Every object in the environment has an RFID tag containing a unique ID that identifies it. This ID is used to retrieve the attributes of the object, such as its name and location in the database. Using the RFID readers, we can detect the presence of a new object inside the cabinet. In addition, the load cell information allows us to determine its exact position inside the cabinet. An example of object detection in the intelligent cabinet is shown in Fig. 7. Additional details about the intelligent cabinets used herein can be found in [54].If neither the FSS nor the ICS is available for detecting objects such as those placed on a desk, the object detection system using a RGB-D camera on a robot is provided in this platform as shown in Fig. 8. In this system, a newly appeared object or movement of an object is detected as a change in the environment. The steps of the change detection process are as follows.1.Identification of furnitureAlignment of the furniture modelObject extraction by furniture removalSegmentation of objectsComparison with the stored informationIt is possible to identify furniture based on the position and posture of robots and furniture in the database. Using this information, robot cameras determine the range of the surrounding environment that is actually being measured. Afterwards, the system superimposes these results and the position information for furniture to create an updated furniture location model.The point cloud (Fig. 9a) acquired from the robot is superimposed with the furniture’s point cloud model (Fig. 9b). The results are shown in Fig. 9c. After merging the point cloud data (Fig. 9a) and the point cloud model (Fig. 9b), as shown in Fig. 9c, the system deletes all other points except for the point cloud model for the furniture and limits the processing range from the upcoming steps.We scan twice for gathering point cloud datasets of previous and current scene. In order to detect the change in the newly acquired information and stored information, it is necessary to align two point cloud datasets obtained at different times because these data are measured from different camera viewpoints. In this method, we do not try to directly align the point cloud data, but rather to align the data using the point cloud model for the furniture. The reason for this is that we could not determine a sufficient number of common areas by simply combining the camera viewpoints from the two point cloud datasets and can also reduce the amount of information that must be stored in memory. Using the aligned point cloud model, it is possible to use the point cloud data for objects located on the furniture, without having to use the point cloud data for furniture from the stored data. The alignment of the furniture model is performed using the ICP algorithm.After alignment, all points corresponding to furniture are removed to extract an object. When removing the furniture according to the method described in a previous study using voxelized shape and color histograms [55], it is particularly difficult to cleanly remove items of furniture such as beds, blankets, and pillows, because they are easily deformable by hand or other objects. As such, the system removes furniture according to segmentation using color information and three-dimensional positions. More precisely, the point cloud is converted to a RGB color space and then segmented using a region-growing method. Each of the resulting segments is segmented based on the XYZ space. Using this method, each of these last segments becomes a continuous area of the same color. The system then selects only those segments that overlap with the model and then removes these segments. Fig. 10shows the results of bed removal after blankets have been changed.After performing the above-mentioned processing, only the points associated with objects placed on furniture remain. These points are further segmented based on XYZ space. The resulting segments are stored in the database.Finally, the system associates each segment from the previously stored information with the newly acquired information. As a result, the system finds the unmatched segments and captures the movement of objects that has occurred since the latest data acquisition. In other words, the segments that did not match between the previous dataset and the newly acquired dataset, reflect objects that were moved, assuming that the objects were included in the previously stored dataset. On the other hand, the segments that appear in the most recent dataset, but not in the previously stored dataset, reflect objects that were recently placed on the furniture. The set of segments that are included in the association process are determined according to the center position of segments. For the segments sets from the previous dataset and the newly acquired dataset, the association is performed based on a threshold distance between their center positions, considering the shape and color of the segments as the arguments for the association. We use an elevation map (Fig. 11b) that describes the height of furniture above the reference surface level to represent the shape of the object.The reference surface level of furniture is, more concretely, the top surface of a table or shelf, the seat of a chair, or the sleeping surface of a bed. The elevation map is a grid version of the reference surface level and is a representation of the vertical height of each point with respect to the reference surface level on each grid.When various vertical height measurements occur within a single grid, the chosen measurement value corresponds to the highest point from the reference surface. Then, comparison is performed on the elevation map for each segment, taking into consideration the variations in size, the different values obtained from each grid, and the average value for the entire map. The color information used to analyze the correlation between segments is the hue (H) and saturation (S). A two-dimensional histogram for H and S is created for each segment, as shown in Fig. 11c. The number of bins for H and S is 32. Using these H–S histograms, the previous data and the newly acquired data are compared, allowing the system to determine whether it is dealing with the same objects. The Bhattacharyya distanceBC(p,q)within H–S histograms(p,q)is used for determining the similarity between histograms and is calculated according to Eq. (1).(1)BC(p,q)=∑h=031∑s=031p(h,s)q(h,s)Once distance values are calculated, the object can be assumed to be the same as for the case in which the degree of similarity is equal to or greater than the threshold value. According to the scenarios for the two tables shown in Fig. 12a and b, the objects placed on top of the tables becomes segmented, and the correlation between these segments is calculated using only H–S histograms. Fig. 12c shows the Bhattacharyya distance of the H–S histograms according to the overall combination of the previously stored dataset and the newly acquired dataset. If the distance is the highest for both sectors and is equal to or greater than the threshold, the system determines it to be the same object. The distances shown in bold represent the values for which it is regarded as the same object.Robot motion planning (TMS_RP) is the component of the ROS–TMS that calculates the movement path of the robot and the trajectories of the robot arm for moving, gasping, giving, and avoiding obstacles based on information acquired from TMS_SS and is described in Table 1.We consider the necessary planning to implement services such as fetch-and-give tasks because such tasks are among the most frequent tasks required by elderly individuals in daily life. Moreover, robot motion planning includes wagons for services that can carry and deliver a large amount of objects, for example, at tea time or handing out towels to residents in elderly care facilities as shown in Fig. 14a.Robot motion planning consists of sub-planning, integration, and evaluation of the planning described below to implement the fetch-and-give task.1.Grasp planning to grip a wagonPosition planning for goods deliveryMovement path planningPath planning for wagonsIntegration of planningEvaluation of efficiency and safetyEach planning, integration, and evaluation process uses environment data obtained from TMS_DB and TMS_SS, so that no further actions are required in order to acquire data. Moreover, actions that integrate overall planning are obtained from a combination of various individual planning threads. The integration method is considered to be not only efficient but also safe in places such as hospitals and elderly care houses. The output consists of a series of actions that can be executed efficiently and safely.In order for a robot to push a wagon, the robot needs to grasp the wagon at first. There is an infinite number of base positions that the robot can have relative to a wagon that the robot must grip. However, a robot can push a wagon in a stable manner if the robot grasps the wagon from two poles positioned on its sides. Thus, the number of base position options for the robot with respect to the wagon is reduced to four (i) as shown in Fig. 14. The position and orientation of the wagon, as well as its size, is managed using the ROS–TMS database. Using this information, it is possible to determine the correct relative position. This provides the distance, i.e., the control distance (CD), between the robot and the wagon when the robot is actually grasping the wagon. Based on the wagon direction when the robot is grasping its long side, valid candidate points can be determined using Eqs. Eq. (2) through (4) below(i=0,1,2,3). Here,Rrepresents the robot, andWrepresents the wagon. Subscriptsx,y, andθrepresent the correspondingx-coordinate,y-coordinate, and posture (rotation of thez-axis). Fig. 13shows the positional relationship between the robot and the wagon, giveni=2. Moreover, Fig. 14shows the wagon gripping position as a 3D model, givenWθ=0.(2)Rxi=Wx+(Wsizei2+CD)cos(Wθ+i2π)(3)Ryi=Wy+(Wsizei2+CD)sin(Wθ+i2π)(4)Rθi=Wθ+π+i2πWsizei={length of the wagon’s long side(i=0,2)length of the wagon’s short side(i=1,3)In order to hand over goods to a person, it is necessary to plan both the position of the goods to be delivered and the base position of the robot according to the person’s position. Using manipulability as an indicator for this planning, the system plans the position of the goods relative to the base position. Manipulability is represented by the degree to which hands/fingers can move when each joint angle is changed. When trying to deliver goods in postures with high manipulability, it is easier to modify the motion, even when small gaps exist between the robot and the person. Since it is difficult to know the exact position of the person, or to operate the robot without errors, a method that deals with these gaps is indispensable. The velocity vectorυcorresponds to the position of hands, andqis the joint angle vector. In addition, we assume the high manipulability of the arm of the person makes him more comfortable for grasping goods. Their relation is represented in Eqs. (5) and (6).(5)υ=J(q)q̇(6)ω=detJ(q)JT(q)If the arm has a redundant degree of freedom, an infinite number of joint angle vectors corresponds to just one hand position. Therefore, when solving the inverse kinematics of this issue, we calculate the posture that represents the highest manipulability within the range of possible joint angle movements. The planning procedure for the position of goods and the position of robots using manipulability is as follows:1.The system maps the manipulability that corresponds to the robots and each person on the local coordinate system.Both manipulability maps are integrated, and the position of goods is determined.Based on the position of goods, the base position of the robot is determined.We set the robot as the origin of the robot coordinate system, assuming the frontal direction as thex-axis and the lateral direction as they-axis. At each position on the XY plane, the manipulability is mapped for the situation in which objects are being carried by hand, as shown in Fig. 15a. This mapping is superimposed along thez-axis, which is the height direction, as shown in Fig. 15b. Thus, we create a three-dimensional manipulability map relative to the coordinate system of the robot. Similarly, we are also able to create a manipulability map for persons in Fig. 15b. Note that the current system uses a human model with a fixed size (1700 mm). However, we have proposed an estimation technique of a body shape using a statistical shape model of human and camera images [56], and thus we can apply the proposed technique for individual height adaptively with this technique.The next step is to determine, using the manipulability map, the position of the goods that are about to be delivered. As shown in Fig. 16a, we take the maximum manipulability value according to each height, and retain the XY coordinates of each local coordinate system. These coordinates represent the relationship between the base position and the positions of the hands. We apply the same process to the coordinates of persons, thus superimposing the manipulability maps for robots and people, as shown in Fig. 16b. In doing so, thez-axis values on the manipulability map can be compensated for by using the face of the target person as a reference and synthesizing this data to suit the corresponding conditions of the person, for example, if the person is standing or sitting. As a result, the height value z in the absolute coordinate system used when delivering goods corresponds to the height of the sum of the maximum values of the manipulability.According to the calculated height on the manipulability map for a person, the system requests the absolute coordinates of the goods to be delivered, using the previously retained relative coordinates of the hands. The position of the person that will receive the delivered goods is managed through TMS_SS and TMS_DB, and it is also possible to use this position as a reference point to request the position of the goods by fitting the relative coordinates. According to the aforementioned procedure, we can determine the unique position of the goods that are about to be delivered. As the final step, the base position of the robot is determined in order to hold out the goods to their previously calculated position. According to the manipulability map that corresponds to the height of a specific object, the system retrieves the relationship between the positions of hands and the base position. Using the position of the object as a reference point, the robot is able to hold the object out to any determined position if the base position meets the criteria of this relationship. Consequently, at the time of delivery, points on the circumference of the position of the object are determined to be candidate points on the absolute coordinate system of the base position. Considering all of the prospect points of the circumference, the following action planning, for which the system extracts multiple candidate points, is redundant. The best approach is to split the circumferencentime, fetch a representative point out of each sector after the split, and limit the number of candidate points. After that, the obtained representative points are evaluated as in Eq. (7), while placing special emphasis on safety.(7)Egive_obj_pos=View+Dhuman+DobsHere,Viewis a Boolean value that represents whether the robot enters the field of vision of the target person. If it is inside the field of vision, thenViewis 1, otherwiseViewis 0. This calculation is necessary because if the robot can enter the field of vision of the target person, then the robot can be operated more easily and the risk of unexpected contact with the robot is also reduced. In the above equation,Dhumanrepresents the distance to the target person, andDobsrepresents the distance to the nearest obstacle. In order to reduce the risk of contact with the target person or an obstacle, the positions that represent the largest distance to the target person or obstacles are valued higher.If all the candidate points on a given circumference sector result in contact with an obstacle, then the representative points of that sector are not selected. According to the aforementioned process, the base position of the robot is planned based on the position of the requested goods. The results for the case in which a person is standing still are shown in Fig. 17a through 17c, and the results for the case in which a person is sitting by a table are shown for Fig. 17d and e. Although the robot stands in front of a person in Fig. 17a and b, Fig. 17b is slightly closer to the object (a table). The corresponding evaluation results are shown in Table 2.Path planning for robots that serve in a general living environment requires a high degree of safety, which can be achieved by lowering the probability of contact with persons. However, for robots that push wagons, the parameter space that uniquely defines this state has a maximum of six dimensions, that is, position (x,y) and posture (θ) of a robot and a wagon, and planning a path that represents the highest safety values in such a space is time consuming. Thus, we require a method that produces a trajectory with a high degree of safety, but at the same time requires a short processing time. As such, we use a Voronoi map, as shown in Fig. 18.In order to be able to plan high-safety trajectories for wagons in real time, we need to reduce the dimensions of the path search space. The parameters that uniquely describe the state of a wagon pushing robot can have a maximum of six dimensions, but in reality the range in which the robot can operate the wagon is more limited. As such, for the case in which a robot is pushing a wagon, we set up a control point, as shown in Fig. 19, which fixes the relative positional relationship of the robot with the control point. The operation of the robot is assumed to change in terms of the relative orientation (Wθ) of the wagon with respect to the robot.In addition, the range of relative positions is also limited. Accordingly, wagon-pushing robots are presented in just four dimensions, which shortens the search time for the wagon path planning. Path planning for wagon-pushing robots uses the above-mentioned basic path and is executed as follows:1.The start and end points are established.The path for each robot along the basic path is planned.According to each point on the path estimated in step 2, the position of the wagon control point is determined considering the manner in which the position of the wagon control point fits the relationship with the robot position.If the wagon control point is not on the basic path (Fig. 20a), posture (Rθ) of the robot is changed so that the wagon control point passes along the basic path.If the head of the wagon is not on the basic path (Fig. 20b), the relative posture (Wθ) of the wagon is modified so that it passes along the basic path.Steps 3 through 5 are repeated until the end point is reached.Fig. 21shows the results of wagon path planning, using example start and end points. These start(Rx,Ry,Rθ)=(2380mm,1000mm,0°)and end points(Rx,Ry,Rθ)=(450mm,2300mm,−6°)use the outcomes of wagon grip position planning and position planning for goods delivery. This confirms that the movement traces of the wagon (indicated by green rectangles) are within the movement traces of the robot (indicated by the rounded gray shapes). Using this procedure we can simplify the space search without sacrificing the safety of the basic path diagram. The actual time required to calculate the path of a single robot was 1.10 (ms), and the time including the wagon path planning was 6.41 (ms).We perform operation planning for overall item-carrying action, which integrates position, path and arm motion planning. First, we perform wagon grip position planning in order for the robot to grasp a wagon loaded with goods. Next, we perform position planning for goods delivery in order to hand-deliver goods to the target person. The results of these work position planning tasks becomes the candidate movement target positions for the path planning of the robot and the wagon. Finally, we perform an action planning that combines the above-mentioned planning tasks, from the initial position of the robot to the path the robot takes until grasping the wagon, and the path the wagon takes until the robot reaches the position at which the robot can deliver the goods. For example, if there are four candidate positions for wagon gripping and four candidate positions for goods delivery around the target person, then we can plan 16 different actions, as shown in Fig. 22. The various action sequences obtained from this procedure are then evaluated to choose the optimum sequence.We evaluate each candidate action sequence based on efficiency and safety, as shown in Eq. (8).(8)Evalue=αLenminLength+βRotminRotation+γViewRatioTheα,β,γare respectively the weight values ofLength,RotationandViewRatio. TheLengthandRotationrepresent the total distance traveled and total rotation angle. TheLenminandRotminrepresent the minimum values of all the candidate action. First and second terms of Eq. (8) are the metrics for efficiency of action.ViewRatiois the number of motion planning points in the person’s visual field out of total number of motion planning point. It means the percentage for the number of motion planning points that exist in the visual field of person.

@&#CONCLUSIONS@&#
In the present paper, we have introduced a service robot system with an informationally structured environment named ROS–TMS that is designed to support daily activities of elderly individuals. The room considered herein contains several sensors to monitor the environment and a person. The person is assisted by a humanoid robot that uses information about the environment to support various activities. In the present study, we concentrated on detection and fetch-and-give tasks, which we believe will be among most commonly requested tasks by the elderly in their daily lives. We have presented the various subsystems that are necessary for completing this task and have conducted several independent short-term experiments to demonstrate the suitability of these subsystems, such as a detection task using a sensing system and a fetch-and-give task using a robot motion planning system of the ROS–TMS. Currently, we adopt a deterministic approach for choosing proper data from redundant sensory information based on the reliability pre-defined manually. Our future work will include the extension to the probabilistic approach for fusing redundant sensory information. Also, we intend to design and prepare a long-term experiment in which we can test the complete system for a longer period of time.