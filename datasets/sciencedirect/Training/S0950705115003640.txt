@&#MAIN-TITLE@&#
Robust support vector data description for outlier detection with noise or uncertain data

@&#HIGHLIGHTS@&#
We propose two new SVDD models which improve the robustness to noise.Cutoff distance-based local density can mitigate the effect of noise towards SVDD.Tolerated gap of SVDD with ε-insensitive loss can improve generalization performance.

@&#KEYPHRASES@&#
Outlier detection,Support vector data description,Local density,ε-insensitive loss,

@&#ABSTRACT@&#
As an example of one-class classification methods, support vector data description (SVDD) offers an opportunity to improve the performance of outlier detection and reduce the loss caused by outlier occurrence in many real-world applications. However, due to limited outliers, the SVDD model is built only by using the normal data. In this situation, SVDD may easily lead to over fitting when the normal data contain noise or uncertainty. This paper presents two types of new SVDD methods, named R-SVDD and εNR-SVDD, which are constructed by introducing cutoff distance-based local density of each data sample and the ε-insensitive loss function with negative samples. We have demonstrated that the proposed methods can improve the robustness of SVDD for data with noise or uncertainty by extensive experiments on ten UCI datasets. The experimental results have shown that the proposed εNR-SVDD is superior to other existing outlier detection methods in terms of the detection rate and the false alarm rate. Meanwhile, the proposed R-SVDD can also achieve a better outlier detection performance with only normal data. Finally, the proposed methods are successfully used to detect the image-based conveyor belt fault.

@&#INTRODUCTION@&#
Detecting outliers from the available data has been an important task in many diverse applications, such as fault detection, reliability analysis, disease diagnosis, hazard prediction, etc. [1,2]. The goal of outlier detection is to find the abnormal data with inconsistent characteristics that are generated by a different mechanism. In practice, the abnormal data are expensive to obtain or even not available at all, for instance, possible defect features in the fault detection or non-healthy data in the medical diagnosis; however, we can usually acquire a large number of normal ones. Consequently, one-class classification (OCC) has attracted much attention in such situations [3–5], which allows for describing the situation of positive (normal) data and identifies the negative data as outliers.Support vector data description (SVDD) is one of widely used OCC methods [6,7]. It is capable to build a flexible description boundary in the high-dimensional feature space by kernel trick [8,9]. The constructed boundary tends to enclose most of normal data in the hyper-sphere and simultaneously minimize the chance of accepting outliers. The outliers can be distinguished from normal data in the following way: the data within the hyper-sphere are considered as normal, while the data outside the hyper-sphere are outliers. Another advantage to use SVDD is the detection strategy without need of any prior knowledge about the detected object [10,11].Depending on the kernel-based distance between the hyper-sphere and the training data, SVDD may easily lead to over fitting when the training data contain noise or uncertainty [12]. The noise data may behave like normal, and be enclosed inside the hyper-sphere in the training processes [13]. In this case, the spherical boundary may not be optimal and the detection performance will become deteriorated, especially for some applied sensor data with sampling errors and transmission noise. Thus, it is necessary to develop a robust SVDD method to deal with noise or uncertain data [14].Earlier studies concentrated on adopting some distribution characteristics of the target data in the training phase of SVDD [15]. Lee et al. [16] proposed the density-induced SVDD by introducing new distance measurements based on the nearest neighborhood and Parzen-window approaches, which reflected the relative density degree for each data point. Furthermore, the kernel-based class center method was used to generate the confidence level [17] and the position-based weighting [18] respectively. Both parameters indicated the likelihood of input data belonging to the normal. Besides, there were some other methods to calculate the likelihood value, i.e. the k-nearest neighbor (k-NN) method [19], the kernel k-means clustering and kernel LOF-based method [20]. Though so much progress has been made to improve the detection performance of SVDD, most of the studies equally reflect the distribution characteristics of all data. From our observation on the training process of SVDD, taking the two dimensional space for example, the data points located on the boundary of the real space would usually be the support vectors (SVs) (i.e. boundary points of the feature space), and they have a great impact on the performance of data description. In addition, most of the above mentioned distribution characteristics were calculated by using kernel-based distance, which would be directly affected by selected kernel parameters [21].On the other hand, most of contemporary SVDD algorithms are used only with normal training data, which is similar to the unsupervised learning. But the abnormal data do exist even though the number is small. The abnormal data could refine the description boundary of SVDD if they are used in the training processes, such as SVDD with negative examples (N-SVDD) [7]. However, the margin between the normal data and the abnormal data is zero in N-SVDD, which would result in poor generalization ability. To overcome this drawback, the margin between the hyper-sphere and the abnormal (negative) data was maximized to restructure the optimization problems in [23,24]. In [12], the rough SVDD including a lower hyper-sphere and an upper hyper-sphere was constructed by using the rough set principle. Due to the class imbalance problem between two-class data, these improved methods are subject to the hyper-sphere shift and the classification deviation.In this paper, a robust SVDD is proposed with the introduction of a cutoff distance-based local density for each data point [22], which is used as the penalty weight of input data towards the noise data. Moreover, the cutoff distance-based density can effectively indicate the characteristics of those boundary data with noise. Furthermore, we investigate the margin between normal data and abnormal data, and find out that a ring-shaped band containing the inseparable data would be formed around the margin. Inspired by the ε-SVR [25], we construct another robust SVDD model with abnormal data by adding two ε bands (i.e. ε-insensitive loss) on both sides of the description boundary. The two developed models are named R-SVDD and εNR-SVDD respectively. In order to assess their detection performance, ten benchmark datasets from UCI are used for experiments. The proposed methods are also applied to detect image-based conveyor belt fault.Compared with the previous work on the robustness improvement of SVDD, the main contribution of our work can be indicated as follows. First, the cutoff distance-based local density is introduced that can mitigate the effect of noise towards SVDD, especially that of the boundary noise. Second, the ε-insensitive loss is used to refine the description boundary combing with the limited abnormal data, which can improve generalization performance and avoid the hyper-sphere shift. Finally, incorporating above two strategies to the SVDD optimization framework, two robust SVDD models are built to detect outliers.This paper is organized as follows. In Section 2, we briefly introduce the original SVDD and the cutoff distance-based local density. Section 3 presents our proposed methods and the theoretical analyses related to the methods. Section 4 demonstrates the empirical study about the robustness to noise, including UCI datasets and image-based conveyor belt fault dataset. Finally, the conclusion and further study are drawn in Section 5.As a one-class classification method, the goal of support vector data description (SVDD) is to find the minimum hyper-sphere that can enclose most of normal (target) data in the feature space. Given the target datasetX={x1,x2,…,xl}, where xi∈ Rn, the optimization problem is constructed as Eq. (1).(1)minR,a,ξR2+C∑i=1lξis.t.∥ϕ(xi)−a∥2≤R2+ξi,ξi≥0,i=1,2…,lwhere R and a are the radius and center of the hyper-sphere respectively in the feature space, ξiis the error term to allow the data point xito locate outside the hyper-sphere, C>0 is the penalty parameter of ξi, and ϕ (.) is the mapping function that makes point ximapped onto a high-dimensional feature space. We can obtain Eq. (2) by solving the Lagrange dual problem. The resolving process of the dual problem can be derived from [7] in details.(2)minα∑i=1l∑j=1lαiαj(ϕ(xi)·ϕ(xj))−∑i=1lαi(ϕ(xi)·ϕ(xi))s.t.∑i=1lαi=1,0≤αi≤C,i=1,2…,lwhere αi> 0 is the Lagrange multiplier. Generally,K(xi,xj)=(ϕ(xi)·ϕ(xj))is defined as the kernel function. Because the Gaussian radial basis function (RBF) can approximate most kernel functions if the kernel parameter is chosen appropriately [26], the Gaussian RBF kernel:K(xi,xj)=exp(−∥xi−xj∥2/2σ2)is adopted in this paper. The data with αi> 0 constitute the support vectors (SVs). And then we can obtain the following:(3)a=∑i=1lαiϕ(xi)(4)R2=1|SVs|∑xi∈SVs∥ϕ(xi)−a∥2To test an object x, the decision function f(x) is defined as Eq. (5).(5)f(x)=∥ϕ(x)−a∥2−R2When f(x)≤0, x is classified as normal; otherwise, it is classified as an outlier.As shown in Fig. 1, 50 blue star-shaped points are generated randomly with a banana shape in the two-dimensional space. The black dot 51 represents the outlier. The green dashed line is the data description boundary of SVDD. Under normal condition, the outlier can be detected by the description boundary as shown in Fig. 1(a). However, when the normal data are corrupted by noise, such as the triangular point 52 and 53, the outlier is misclassified as normal data shown in Fig. 1(b). Although decreasing the C value can reduce the interference of boundary noise, the likelihood of each data point to be an outlier is taken the same by using the same parameter C for all points. It makes most normal boundary points excluded from the data description region. Therefore, it is important to make full use of the distribution characteristic of each point, especially the boundary points. Herein, a robust modified strategy combining with the cutoff distance-based local density has been proposed to mitigate the effect of individual noise point towards SVDD.The cutoff distance-based local density was proposed to make cluster analysis in [22]. This quantity depends only on the distances dijbetween data i and data j, which indicates the number of points within the range of a cutoff distance. The local density ρiof data point i is defined as Eq. (6).(6)ρi=∑jχ(dij−dc)whereχ(x)=1ifx<0andχ(x)=0otherwise; dcis a cutoff distance determined by dij.Herein, the local density is sensitive to the choice of dc. According to [22], we can choose dcso that the average number of neighbors is about 2–5% of the total number of points. Moreover, for computing the density with the nonlinear case, we used the exponential function form in this paper, which is expressed as Eq. (7).(7)ρi=∑jexp(−(dij/dc)r)where r is the scaling factor determined by the data distribution, and here r = 2.To illustrate the characteristics of the boundary points, Fig. 2shows the values of cutoff distance-based local density for 50 normal points and 2 noise points generated in Section 2.1. We can find that the boundary points 1, 36, 44 and noise points 52, 53 have lower density values. Meanwhile, these boundary points constitute a less tight description and the description region contains blank as shown in Fig. 1(a). According to the clustering hypothesis [27], the decision boundary should cover the higher-density region and pass through the lower-density region. The points with lower densities would become less significant in the training process of SVDD.In this section, our proposed methods to improve SVDD for outlier detection will be described in detail. As mentioned above, the cutoff distance-based local density can characterize the local distribution of the data points. Thus, we will modify the optimization problem Eq. (1) by introducing the local density as a penalty weight that is defined as Eq. (8).(8)w(xi)=ρ(xi)/maxiρ(xi)The value of the penalty weight falls within the range of [0, 1]. In the following, we will apply the penalty weight to SVDD, and then develop two improved methods called R-SVDD and εNR-SVDD. The first method just considers normal data, while the other one uses both normal and abnormal data and ensures the ε-insensitive loss.By introducing the penalty weight w(xi) of each data point to Eq. (1), the newly constructed optimization problem is defined as Eq. (9).(9)minR,a,ξR2+C∑i=1lw(xi)ξis.t.∥ϕ(xi)−a∥2≤R2+ξi,ξi≥0,i=1,2…,lIn order to solve the optimization problem (9), we introduce the Lagrange function as the original SVDD, and then the solution of problem (9) can be solved by Eq. (10).(10)minα∑i=1l∑j=1lαiαjK(xi,xj)−∑i=1lαiK(xi,xi)s.t.∑i=1lαi=1,0≤αi≤w(xi)C,i=1,2…,lAfter resolving the quadratic programming problem (10), we can obtain the Lagrange multipliers αi, which would be used to calculate the radius and center of the spherical description by Eqs. (3) and (4) respectively. By applying Eq. (5), we can test an object data x. The difference from the original SVDD is that the penalty weight w(xi) for each data point is introduced here. A smaller value of w(xi) could make data xibecome less important in the training process by decreasing the penalty of the parameter ξiin Eq. (9). The robustness of R-SVDD is illustrated in Fig. 3. We can see that the noise points 52 and 53 almost have no influence on the R-SVDD method.When the abnormal samples exist, we will incorporate the generated penalty weights of both normal and abnormal data into the optimization problem of SVDD. On the other hand, the ε-insensitive loss function is used to improve the generalization performance of SVDD. For this model, the training dataset consists of two classes for the l normal data and n abnormal data shown asX={x1,x2,…,xl,xl+1,…,xl+n}, where xi∈ Rn. Meanwhile, for the penalty weight w(xi), i=1,…, l, the dijis calculated between data {xi, i=1, …, l} and data {xj, j=1, …, l}. For the penalty weight w(xi), i=l+1, …, l+n, the dijis calculated between data {xi, i=l+1, …, l+n} and data {xj, j=1, …, l}. Herein, the penalty weights of normal data indicate the distribution characteristics of normal data, and the penalty weights of abnormal data indicate the relative distances between the abnormal data and the normal data.From the above, the proposed εNR-SVDD model can be constructed as illustrated in Fig. 4. The new objective function will be reformulated as Eq. (11).(11)minR,a,ξR2+C1∑i=1lw(xi)ξi+C2∑j=l+1l+nw(xj)ξjs.t.∥ϕ(xi)−a∥2≤R2−ɛ+ξi,i=1,2…,l∥ϕ(xj)−a∥2≥R2+ɛ−ξj,j=l+1,…,l+nξi≥0,ξj≥0where C1w(xi) and C2w(xj) are the penalty parameters for each data point; ε≥0 is the insensitive loss factor determined by user, which can hold the margin between the normal and abnormal data. It can be seen that Eq. (11) will restrain the error of each data point with a different penalty weight and obtain the tolerated gap between two types of data. In addition, compared with the previous work on maximum margin, such as the SSLM approach [24], our proposed εNR-SVDD method can avoid the decision shift due to the class-imbalanced data on outlier detection.To further investigate the dual problem of Eq. (11), we introduce the Lagrange multipliers αi≥ 0, αj≥ 0, βi≥ 0, βj≥ 0 and construct a Lagrange function shown below:(12)L=R2+C1∑i=1lw(xi)ξi+C2∑j=l+1l+nw(xj)ξj+∑i=1lαi(∥ϕ(xi)−a∥2−R2+ɛ−ξi)−∑j=l+1l+nαj(∥ϕ(xj)−a∥2−R2−ɛ+ξj)−∑i=1lβiξi−∑j=l+1l+nβjξjSetting the partial derivatives of L with respect to R, a, ξi, ξjto 0 respectively, we can obtain the following:(13)∂L∂R=0→∑i=1lαi−∑j=l+1l+nαj=1(14)∂L∂a=0→a=∑i=1lαiϕ(xi)−∑j=l+1l+nαjϕ(xj)(15)∂L∂ξi=0→αi+βi=w(xi)C1(16)∂L∂ξj=0→αj+βj=w(xj)C2Introducing yi=1, yj=−1 to Eqs. (13) and (14), the new constrains can be represented as follows:(17)∑i=1l+nαiyi=1(18)a=∑i=1l+nαiyiϕ(xi)And then substituting the constrains (15)–(18) into Eq. (12), the rearranged optimization problem is as follows:(19)minα∑i=1l+n∑j=1l+nαiαjyiyjK(xi,xj)−∑i=1l+nαi(yiK(xi,xi)+ɛ)s.t.∑i=1l+nαiyi=10≤αi≤w(xi)C1,i=1,2…,l0≤αi≤w(xi)C2,i=l+1,…,l+nNote thatEq. (19) is still a quadratic programming problem, which can be easily solved withthe original SVDD solver. For the relationship between the obtained Lagrange multipliers αiand the spherical description boundary, we have the following analysis according to the Karush–Kuhn–Tucker (KKT) conditions [28].For the function (12), the KKT conditions satisfy the following:(20)αi(∥ϕ(xi)−a∥2−R2+ɛ−ξi)=0,βiξi=0,i=1,…,l(21)αj(∥ϕ(xj)−a∥2−R2−ɛ+ξj)=0,βjξj=0j=l+1,…,l+n(1)For the normal data, if 0 < αi< w(xi)C1, then βi> 0 from Eq. (15). Next, we haveξi=0and∥ϕ(xi)−a∥2−R2+ɛ−ξi=0from Eq. (20). This implies that xilies on the dotted sphere surface B1 in Fig. 4. If xilies outside the sphere surface B1, ξi> 0, thenβi=0from Eq. (20) andαi=w(xi)C1from Eq. (15). If xilies inside the sphere surface B1, we haveξi=0and∥ϕ(xi)−a∥2−R2+ɛ−ξi<0, thenαi=0from Eq. (20).For the abnormal data, if 0 < αj< w(xj)C2, then βj> 0 from Eq. (16). Next, we haveξj=0and∥ϕ(xj)−a∥2−R2−ɛ−ξj=0from Eq. (21). This implies that xjlies on the dotted sphere surface B2 in Fig. 4. If xjlies outside the sphere surface B2, we haveξj=0and∥ϕ(xj)−a∥2−R2−ɛ−ξj>0, thenαj=0from Eq. (21) andαi=w(xi)C1from Eq. (15). If xjlies inside the sphere surface B2, ξj> 0, thenβj=0from Eq. (21) andαj=w(xj)C2from Eq. (16).Therefore, the description boundary is determined by the two data sets:B1={xi|0<αi<w(xi)C1,i=1,…,l}andB2={xj|0<αj<w(xj)C2,i=l+1,⋯,l+n}, and then we have the radius of hyper-sphere as Eq. (22).(22)R2=1|B1|+|B2|∑xi∈B1,B2∥ϕ(xi)−a∥2In this section, the UCI datasets and the conveyor belt fault dataset are used to test the detection performance of our proposed methods. Besides, the Gaussian distribution noise is added to the original UCI datasets in order to show the robustness of the R-SVDD and εNR-SVDD. The experiments were performed in a PC with Intel Pentium (R) G630, 2.7 GHz CPU, 2 GB RAM, 32-bit Windows 7 operating systems and the program environment is Matlab R2010a.We first used the real-world datasets to conduct the experiments and comparative analyses with another four existing outlier detection methods, including SVDD, N-SVDD [7], DW-SVDD [19] and SSLM [24]. The datasets include Breast Cancer, Diabetes, Heart, Ionosphere, Sonar, Vehicle, Segment, Delft pump and Steel Plates Faults. They are popularly used in outlier detection problems [19,20] and available from UCI repository [29]. The detailed information about these datasets is shown in Table 1. To cope with the calculation difficulties between the features with bigger values and that with smaller values, each feature would be linearly scaled to the range [−1, +1] in Eq. (23), where v is the original value, v’ is the scaled value, max is the upper bound of the feature value, and min is the low bound of the feature value.(23)v′=2×v−minmax−min−1For SVDD, DW-SVDD and R-SVDD, 90% of the normal data were selected randomly in the training phase and the remaining data (10% of the normal data + all the outliers) were used for testing. For N-SVDD, SSLM and εNR-SVDD, 90% of the normal data and 10% of the outliers were selected randomly in the training phase, and the remaining data (10% of the normal data + 90% of the outliers) were used for testing.For all the methods, the 10-fold cross validation via grid search method was utilized to tune the parameters. The Gaussian RBF kernel function was used in all experiments and the kernel parameter σ was searched in the range from 2−3 to 23. Meanwhile, the parameter C in SVDD, DW-SVDD and R-SVDD, and C1, C2 in N-SVDD, εNR-SVDD was searched in the range from 2−3 to 24. The parameter ε in εNR-SVDD was selected from {1,10,20,30}. For the SSLM, the parameter v was selected from {1,10,20,30}, and v1, v2 from {0.001,0.01}.The performance of outlier detection methods can be evaluated with the detection rate and the false alarm rate. Based on the confusion matrix in Table 2, these two parameters are defined as follows: Detection rate=TP/(TP+FN) and False alarm rate=FP/(FP+TN). The receiver operating characteristic (ROC) curve represents the trade-off relationship between the detection rate and the false alarm rate. In general, the area under the curve (AUC) is used to measure the performance of outlier detection method, and the value of AUC for ideal detection performance is close to one. We repeat each experiment for 10 times, and present the average AUC values for performance comparison.Table 3shows the comparison of average AUC value and standard deviation for the 10 UCI datasets with different detection methods. From the table, it can be observed that the proposed εNR-SVDD performs better than other methods with high AUC values for 9 UCI datasets except the Diabetes dataset. Meanwhile, the standard deviation of AUC for the εNR-SVDD method is smaller than that of other methods in 4 datasets. In the case of only normal data, the R-SVDD method yields a higher AUC in 9 datasets except the Vehicle dataset. Therefore, by introducing the cutoff distance-based local density and the ε-insensitive loss function, the detection performance of SVDD can be greatly enhanced.Further, the Gaussian noise was added to the training datasets to investigate the robustness of the proposed methods. According to the methods used in [20,30], the Gaussian distribution of white noise was generated with the zero mean. The initial standard deviation σi° was calculated along the ith dimension of the training data, and then the standard deviation of Gaussian noise was obtained randomly from the range [0, 2×σi0]. In this way, the generated noise would have the same dimension as each dataset. Meanwhile, the percentage of the data corrupted by noise is selected from 10% to 40%. Fig. 5showsthe average AUC values achieved by the six methods with respect to different percentage of training data corrupted by noise. When noise is added to the training data, the outliers become indistinguishable from the normal data and the detection performance of six methods is degraded. But from Fig. 5, we can find that our proposed R-SVDD and εNR-SVDD can still have higher AUC values and slower performance degradation than other methods. It is suggested that our proposed methods can reduce the interference of the noise.To illustrate the complexity of the proposed methods, the average running time of six detection methods for above 10 UCI datasets is shown in Fig. 6. It can be seen that R-SVDD and εNR-SVDD take more running time than the original SVDD and N-SVDD since the proposed methods need to calculate the cutoff distance-based local density. For the training phase, given the number of samples being N, the time complexity of the quadratic programming for SVDD is O(N3) [16], and the time complexity of the calculation for distance-based local density is O(N2). So the total time complexity is O(N3) + O(N2), that is the same as that of the previous methods applied in a moderate size dataset. On the other hand, the multiplication and addition operations for the local density can be conducted quickly, so the difference is quantitated as few seconds.With the development of industry production, the application of belt conveyor is very popular, especially in modern coal mining enterprises. But the tear fault of conveyor belt occurs often due to the impact of falling sharp material in operations [31]. If the fault can be early detected, the damage of transmission devices and the hurt of operation staff caused by belts breakdown can be avoided. In this section, the conveyor belt fault dataset would be used as empirical analysis for above SVDD-based methods.The visual monitoring system of conveyor belt was established to acquire the belt fault signals and create the image-based fault feature dataset. The schematic diagram of monitoring system is shown in Fig. 7(a). A series of CCD cameras and light sources were installed at the bottom of the conveyor belt for image acquisition, and then the image information was transmitted to the industrial computer for the on-line fault detection. The fault dataset is classified into normal and abnormal (fault) situations as shown in Fig. 7(b) and (c).The dataset comprises 135 Gy belt images with the size of 640 × 480, in which 45 are normal and 90 are faulty. Due to the irregular shape of the fault, we extracted the Gray level co-occurrence matrix (GLCM) texture feature as the basis of fault detection, which can reflect the grayscale difference of the direction and adjacent interval effectively. As described in [32], the GLCM in this paper was calculated in four different directions of (0°, 45°, 90°, 135°) and the inter-pixel distance d = 4 to improve the computational efficiency. The corresponding Energy, Entropy, Contrast and Correlation were selected to describe the belt texture. Because of non-uniform illumination and dusts in the real environment, the data contain some noise or uncertainty.To perform the fault detection, the 5-fold cross validation was utilized to tune the parameters as in Section 4.1. For N-SVDD, SSLM and εNR-SVDD, we selected 25 normal data and 5 abnormal data to build the fault detection model and the remaining 105 data were used for testing. For SVDD, DW-SVDD and R-SVDD, the same 25 normal data were used in the training phase and the testing data were same as the above there methods. Fig. 8 shows the fault detection results for the 6 detection methods.In Fig. 8, the red dashed line represents the decision threshold for each method, and the data located above the dashed line are considered as abnormal. The erroneously detected points are marked by the red circle. It can be observed that the faults can be detected with a higher AUC value by using the εNR-SVDD method. For the R-SVDD method, the detection rate is good, but the false alarm rate is not low. The strategy to reduce the false alarm rate while keeping a high detection rate remains for our future work.This paper proposes two types of new outlier detection methods by introducing the cutoff distance-based local density of each data point into the SVDD training model. Moreover, the ε-insensitive loss function is utilized to construct a tolerated gap of SVDD with negative data. Our proposed methods can improve the robustness of SVDD for data with noise and uncertainty based on extensive experiments with ten UCI datasets. The experimental results have shown that the proposed εNR-SVDD is superior to other existing outlier detection methods in terms of the detection rate and the false alarm rate. Meanwhile, the proposed R-SVDD can also achieve better outlier detection performance with only normal data. Finally, the proposed methods are successfully used to detect the image-based conveyor belt fault.The future work will be extended as follows. First, we will investigate how to better evaluate the distribution characteristic of each input data point to avoid uncertainty in the real-world application. Second, we would like to improve the detection performance of spherical data description by introducing other shape-based data description, such as the strip-based description.

@&#CONCLUSIONS@&#
