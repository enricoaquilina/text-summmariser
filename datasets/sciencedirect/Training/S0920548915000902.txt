@&#MAIN-TITLE@&#
CloudBook: Implementing an E-book reader on a cloud platform by an optimized vector graphic library

@&#HIGHLIGHTS@&#
In this paper, OpenVG are implemented and integrated into an E-book application to be a standard for E-contents.An embedded Graphic Processing Unit (GPU) is used to compute the tasks of OpenVG in a parallel architecture.A Hadoop platform provides a standard interface to convert bitmap images to vector graphic format.A locality-aware resource management is implemented in the Hadoop platform to improve the efficient of resource allocation.

@&#KEYPHRASES@&#
Vector graphic,E-book,Embedded GPU,Hadoop platform,Cloud computing,

@&#ABSTRACT@&#
Problems of the E-book application include different file formats, performance inefficiency and image distortion; those may cause a slow development of E-book marketing in the industry of information technology. This paper proposes an innovative E-book system called CloudBook, which utilizes an embedded Graphics Processing Unit (GPU) and a data locality aware Hadoop system to resolve the problems. The results of the experiments show that the CloudBook system can increase the performance of the OpenVG library by 73%, reduce the execution time of file conversion by 50%–75%, and improve the data hit ratio in cloud platform by 10%.

@&#INTRODUCTION@&#
As an E-book mainly consists of text and images in digital forms, the contents of an E-book can be read on computers or other electronic devices. Many E-book applications have been available on mobile devices in the recent decade and many vendors of electronic contents also bring out some exclusive E-reader applications. However, so far, the E-book applications have not widely been used on the mobile device because E-book applications have some weaknesses for mobile devices. The greatest weakness of E-book technology is the image distortion problem. The contents and images of E-books may be distorted when user scales up the size of contents or images because most images displayed on the E-book applications are bitmap-type graphic. To solve the image distortion problem, the Scalable Vector Graphics (SVG) technology has been proposed by the W3C SVG Working Group [1]. Fig. 1shows the difference between the bitmap-type image and the vector graphic image.Some studies [2–5] implemented the vector graphic technology on mobile operating systems, such as: Android, Windows Phone and iOS. The vector graphic technology not only solves the image distortion problem, but also provides a good solution to unify the file format of E-book applications. However, two extension problems for the E-book applications have been produced after introducing the vector graphic technology. First, a vector graphic image, especially a complex one, is slower in performance than the same image rendered as a bitmap because the vector graphic technology spends a lot of time to process path calculation and drawing. Second, a vector graphic image is larger in file size compared with the same image saved as a bitmap due to the complex description of vector information. To solve the above problems, we propose an optimized E-book application called CloudBook, which integrates computational resources of an embedded GPU and cloud platforms. The CloudBook offloads some tasks of file conversion to the cloud platforms and also provides a large storage resource pool for the mobile devices to store all E-book contents in the cloud servers. The performance of CloudBook for displaying contents can also be optimized by the embedded GPU in mobile devices through the Rederscript programming interface.CloudBook features three advantages. First, it solves the image distortion problem in E-book applications through the vector graphic library implementation. Second, CloudBook can reduce the time complexity of vector graphic library from O(n2) to O(n∗log n), and thus achieve 73% performance improvement over the original library in the Android system by modifying the algorithm of graphic triangulation and offloading the path calculation to the embedded GPU. Third, CloudBook also develops and implements a locality-aware scheduler on the Hadoop platform that not only improves the performance of vector graphic converter that converts the bitmap to the vector graphic format, but also increases the data hit ratio on the Hadoop platform.The rest of this paper is organized as follows. Section 2 shows the technical background and related works. Section 3 presents the CloudBook optimization in the local system library. In Section 4, we show the whole system of CloudBook on the cloud platform. The results of the experiments conducted to evaluate the performance of CloudBook are presented in Section 5. Section 6 contains some concluding remarks.In this section, we discuss some related works of E-book development and the vector graphic technique. Then, we briefly introduce the technical background including the vector graphic technology and RenderScript.The E-book applications have been widely studied by research groups and industry manufacturers recently. Some previous studies focused on E-Book's applications [6,7], security [8,9], platform and standardization [10,11]. According to [10], many E-book file formats are different and not compatible with one another. Therefore, unifying the E-book file format is a tough issue for various kinds of E-book services. To resolve the issue, the vector graphic technology is implemented on some hand-held devices. The vector graphic is a free file format that can be installed in any mobile system. Besides, vector drawings can enlarge any image without losing quality. To take the advantages of the vector graphic technique, the CloudBook converts all contents from the bitmap format to the vector graphic format. In order to view vector graphic files on embedded platforms, there are libraries, which can support the vector graphic reader in the embedded platform [1,9,12]. We integrate the OpenVG [1] library with Android system and also optimize the vector graphic library on the Android system with embedded GPU acceleration.OpenVG [1] is a standard interface of 2D vector graphic for the cross embedded platform implementation. The OpenVG standard separates the procedures of drawing vector graphic to several steps. Fig. 2shows the detail flow of implementation in OpenVG standard. The steps of OpenVG in implementation flow can be concluded as follows: 1. Path processing, 2. Flatten, 3. Transformation, 4. Triangulation, 5. Rasterization, Clipping and Masking, 6. Boundbox finding, 7. Paint generation and 8. Blending and Antialiasing. The detail explanations of each step are as follows.Path processing: It is the first step in the OpenVG implementation. This step converts the description from input file, e.g. XML file, to the drawing information, which can be recognized by the vector graphic library.Flatten: This stage transforms the stroked path into the coordinate system as a transformation set.Transformation: In this step, OpenVG uses the transformation set to calculate the drawing path information and generates new surface coordinates.Triangulation: The major task of this step is to convert the drawing data from the OpenVG type to the OpenGL/ES format.Rasterization: This step consists of mapping the shapes on the pixel grid using some filtering rules.Boundbox finding, Clipping and Masking: This step consists of giving every pixel a coverage value. When the pixel is not within the bounds of the drawing area, then it receives the value 0.Paint generation: This step generates the parameters of colors and alpha setting to paint the image.Blending and Antialiasing: The previously processed results are taken to be blended with the corresponding parameters.Some researchers [13,14] optimized the performance of OpenVG library through modifying the procedure of rasterizer and accelerating rendering with Dual-Scanline. The above solution focused on the algorithm optimization in the rendering and rasterizer. However, the path transformation is another computing-intensive function in the OpenVG library. Furthermore, AndroidVG [2] proposes a triangulation procedure before transformation to reduce the computing complexity in the transformation step. The triangulation procedure converts a stroking path to a perspective transformation matrix. Many coordinate computations need to be completed in the triangulation phase. To improve the performance of coordinate computations, the embedded GPU is the best choice for parallel computing to improve the performance of computing-intensive procedure due to the feature of many-core architecture. Therefore, many programmers take the advantages of GPU computing to improve the performance of applications or system libraries in the embedded environment.RenderScript [15] is a new graphic rendering technique with high performance in the Android graphic framework. The technique provides not only the graphic rendering function, but also the data-parallel computation. The RenderScript parallelizes processes on cross-processors, such as: CPUs, GPUs and DSPs, allowing application developers to focus on developing algorithms, instead of low-level resource scheduling or workload balancing. RenderScript can offload some computational tasks to GPUs or DSPs, hence, this paper utilizes RenderScript to improve the performance of transformation phase in the OpenVG library.In [16], they proposed some algorithms to convert a bitmap image into a vector graph format. They only deployed the client-server architecture to execute their algorithms. However, if users need to convert a large amount of files from the bitmap type to the vector graphic type, many tasks of file conversion are produced in the server side; as the result, the server side becomes the bottleneck in the whole system. To solve the problem in the server side, our solution is a parallel implementation of the Autotrace [17] that uses multi-VM to convert multiple files simultaneously, which can achieve a near-linear speedup on the cloud computing platform. We implement the Autotrace on Hadoop platform and the performance of Hadoop platform has been improved through data-locality aware scheduler [18].Many high-efficient cloud platforms are installed with the MapReduce [19] software and Hadoop [20]. Hadoop supports the executing of parallel programs on large scale clusters of commodity hardware. Hadoop consists of two main components: a distributed file system and the MapReduce engine. The distributed file system in Hadoop is called HDFS that handles all data access and management. Due to the distributed file system, the data locality problem occurs in the MapReduce framework [18,21–23]. In this study, we develop a data locality-aware resource assignment in the Hadoop scheduler and the cloud platform of CloudBook is based on the modified Hadoop framework to raise the performance of data access. We can see that the optimized scheduler gets more data locality in the resource assignment in the experiments.CloudBook is optimized for Android system using a speed-up algorithm with GPU computing and a high performance parallelized algorithm on the Hadoop cloud platform. The details of the optimized vector graphic library of CloudBook are described in the next section.In this section, we present the details of optimized system library in the CloudBook system. The optimized library is developed based on the OpenVG specification [1] and the ShivaVG [24] source code. In order to optimize the vector graphic library, CloudBook contains a speedup triangulation algorithm to reduce the time complexity of path transformation. In addition to the time complexity, CloudBook also designs a parallel computing mechanism for improving the performance of path generation. The step of path generation is a compute-intensive process in the OpenVG standard. However, the step of path generation calculates each path curve independently, and thus the parallel computing mechanism can conserve the time consumed in path generation. A quick overview about CloudBook's library feature is presented in the next subsection.To identify the performance bottleneck and then improve it, we profile the performance of compute-intensive stages, such as: flatten, transformation, triangulation and rasterization in the OpenVG flow. In order to profile the performance, the profiling function is inserted into the OpenVG library. Three well-known sample applications, such as: TigerVG, AppleVG and StarVG, are executed on the modified vector graphic library. Fig. 3presents the profiling result of OpenVG library and the result shows the consumed time for computing between the different stages in the whole vector graphic application. After profiling the performance, we can find that the triangulation is the bottleneck of performance in the OpenVG library. Triangulation contains many graphic computation tasks, i.e. computing the coordinates of vertices and splitting the whole graph to many triangle shapes. Therefore, optimizing the triangulation process is the most important problem in the vector graphic library. To solve the problem, we propose a two-stage optimization flow shown in the rest of this section.The speedup triangulation algorithm focuses on modifying the steps of computation and reduces the time complexity of the triangulation algorithm. We simplify the steps of computation from the triangulation step to the drawing step. Fig. 4shows the comparison between ShivaVG, AndroidVG and CloudBook.In Fig. 4, there are three steps in the computation of ShivaVG. The ShivaVG first computes the drawing graph in the stencil buffer and then finds the drawing area to paint colors in the frame buffer. AndroidVG has a more complex flow between triangulation and the frame buffer. AndroidVG triangulates the planar graph and outputs the partitions of the graph to the depth buffer. After the depth buffer step, AndroidVG has the same data flow as ShivaVG. In contrast to the above vector graphic library, CloudBook has the simplest steps of computation. CloudBook follows the same step in the triangulation stage; however, CloudBook directly outputs the partitions with drawing setting to the frame buffer. The steps of computation of CloudBook reduce the time consumption that is occurred in the data movement between different buffers due to the modification of triangulation stage. In this optimization step, we modify the steps of computation to improve the overall performance for the vector graphic library. We then replaced the triangulation algorithm with a monotone triangulation algorithm to reduce the time complexity of triangulation stage.The original triangulation algorithm in vector graphic library is an ear-clipping algorithm. The time complexity of ear-clipping is O(n2) where n is the number of vertices. If the input source is a complex picture, such as: TigerVG, the consumed time of triangulation is the bottleneck of vector graphic library. In order to reduce the time complexity of triangulation stage, this study replaces the ear-clipping algorithm with a monotone triangulation. However, if the monotone triangulation algorithm is applied to a graph, the graph should be an x-monotone polygon, which is defined in Definition 1 below.Definition 1x-Monotone Polygon. A polygon P in the plane is called x-monotone, if every line orthogonal to x-axis intersects P at most twice.Unfortunately, the application of vector graphic is not possible always to be an x-monotone polygon. To address the issue, we propose a pre-processing stage before the triangulation step. The pre-processing stage focuses on converting a polygon, which is not the x-monotone type, to multiple x-monotone polygons. Before presenting the details of pre-processing steps, two terms — merging point and splitting point are defined as follows.Definition 2Merging Point. According toDefinition 1, a graph is not a monotone and then the graph must have at least one concave vertex. A point p in the plan is called merging point, if two adjacent edges of the concave vertex locate the left side of the concave vertex.Fig. 5shows an example of merging point A in a polygon.The difference between a merging point and a splitting point is that the position of two adjacent edges locates on the left side of the concave vertex, while a splitting point has two adjacent edges on its right side.Fig. 5shows an example of splitting point B in a polygon.Following the definitions, the algorithm of splitting a polygon into multiple monotone polygons is described in Algorithm 1.The time complexity of pre-processing stage is O(n∗log n), hence the pre-processing stage cannot raise the time complexity in the overall process of triangulation. After the pre-processing stage, the input polygon has been already converted into multiple pieces of monotone polygons. The monotone triangulation algorithm is applied to the multiple monotone polygons to reduce the time complexity of triangulation stage from O(n2) to O(m∗nlogn) where m is the number of monotone polygons in G and m is far less than n. The monotone triangulation algorithm is presented in Algorithm 2.Note that, the speed-up triangulation algorithm simplifies the steps of computation from the triangulation stage to the drawing stage and reduces the time complexity of triangulation from O(n2) to O(m∗nlogn). A heterogeneous computing frame is proposed to speed up the optimized triangulation algorithm in the next subsection.In addition to the time complexity, this study also proposes a high performance framework with a heterogeneous computing mechanism, which fully utilizes the computational resource on mobile devices. Since most recent mobile devices are equipped with an embedded GPU unit, the performance of applications can be raised through the assistance of GPU. A GPU contains many ALU units capable to execute the program in parallel, and thereby GPU makes a compute-intensive application executed faster than CPU does. According to the property of GPU, this study offloads the triangulation computation to an embedded GPU on the mobile platform through the Renderscript program interface. Fig. 6shows the framework of offloading mechanism in the CloudBook system.For fully utilizing the computation units on a mobile device, the offloading mechanism divides the tasks of triangulation stage into four sub-tasks and separately executes the sub-tasks on Micro Processor Unit (MPU) and GPU. The GPU executes the tasks, which are compute-intensive and high-parallel. And the MPU handles the tasks that cannot be parallelized. The mechanism takes the advantages of GPU and does not waste the computational resource of MPU.The offloading mechanism speeds up the triangulation again after the optimization of triangulation computation. The optimization of cloud platform should be carried out to enhance the overall performance of the CloudBook system due to the limited resource in mobile devices. The optimization steps of CloudBook in cloud platform are shown in the next section.The role of cloud platform in the CloudBook system is an external resource to store all materials of E-book and to convert the materials in the vector graphic format. The cloud platform is composed of three components: a Hadoop Distributed File System (HDFS), a program of vector graphic converter and a locality-aware job scheduling algorithm. The HDFS in the CloudBook system stores the files of E-book in distributed storage servers. The Cloud platform then coverts E-book files to the vector graphic format through the vector graphic converter when some users want to retrieve the E-book files. Due to the distributed environment of cloud, this study proposes a locality-aware job scheduling algorithm and applies the algorithm to the cloud platform to reduce the time consumption of data transmission between computing nodes and data nodes. The optimized cloud based CloudBook system is introduced to solve the problem of limited resource in mobile devices.The cloud-based CloudBook system is developed based on a distributed file system to keep the files of E-book in different storage servers. The distributed file system duplicates three copies of the uploaded files and stores the copies on different storage servers in order to achieve fault tolerance. The CloudBook system deploys the vector graphic converter on all data nodes; therefore, the CloudBook can dynamically dispatch the file requests from users to the data nodes. If a data node receives the job request from the job tracker, the data node loads the required files from its storage and converts the files to the vector graphic format. Fig. 7presents the architecture of cloud platform in the CloudBook system.The HDFS in the CloudBook system has built-in fault tolerance and guides the request from the user to different servers to achieve the load balance in the cloud platform. However, a new problem is generated on the cloud platform due to a poor job scheduling algorithm. If the required files and the user job are dispatched in the same server, the performance of vector graphic converter can be increased. Otherwise, the required files should be transmitted from other data nodes to the computing node that causes some overhead due to the derived network delay. Fig. 8shows the data locality problem in HDFS.In Fig. 8, if the number of computational slot equals to one, the optimal scheduler gets more data locality in the resource assignment. Hence, this study proposes a locality-aware scheduler and applies the scheduler to the JobTracker.This section presents the locality-aware scheduling algorithm for Hadoop-MapReduce. The goal of the scheduling algorithm is to achieve locality-aware resource assignment in order to reduce the bottleneck of network transmission by following the concept of data interference. Our algorithm introduces a novel notion that combines the weight of data interference with the job scheduler in the MapReduce framework. The scheduler calculates the weight of data interference on all nodes and picks up a node with a smallest weight of data interference to execute the task for locality-aware consideration. The details of the scheduling algorithm are presented as follows.The parameters in the locality-aware scheduler include data nodes, the weight of data, the replica number of input data, and the number of map slots on each node. Table 1gives a brief definition of all parameters in locality-aware scheduler.The locality-aware scheduler focuses on the “rare resources” assignment. The data nodes called rare resources contain some data, which is relatively scarce; because the most part of data nodes, which own required data, is occupied by some high priority tasks. In order to distinguish the rare resources, the parameterWnj,Tdois the major factor to measure the rarity of a data node in the locality-aware scheduler. A data node has a minimum value ofWnj,Tdowhich means that the data node is not a rare resource, hence the scheduler can dispatch the task to the data nodes for computing. The weight equation is described as Eq. (1).(1)WnjTdo=∑i=1m,i≠oSnj,di∗Fnjwdi,∞,ifnjdoesnotcontaindiornjhasnofreeslot,otherwise,in which,(2)wdi=∑j=0pSnj,di∗Fnj.In Eq. (1), the parameterWnj,Tdosums the ratio between the number of available data (di) in the node njand the total number of available data (w(di)) in the cloud platform. The value of parameterWnj,Tdothus increases when the node njcontains some data, which only exist in the node nj.This study implements the locality-aware scheduler in JobTracker. Before assigning tasks, JobTracker uses Eq. (1) to calculate the weight of data interference on each node, which has empty slots to execute the task. JobTracker then picks up a data node with a smallest weight of data interference and dispatches the task to the TaskTracker queue within the data node. Algorithm 3 shows the details of the locality-aware scheduling algorithm. The scheduler not only calculates the weight of data interference to avoid rare resource allocated in an easy way but also introduces the concept of weight of data interference to enhance the data locality in the MapReduce framework. The locality-aware scheduler thus reduces the overhead of data transmission, which is caused by the network delay, and the performance of cloud platform also is improved by the locality-aware scheduler.This study implements the vector graphic converter that converts the format of images from bitmap to vector graphic in the Hadoop cloud platform. And then, we also propose a locality-aware scheduler in the MapReduce framework to improve the overhead of data transmission and the data hit ratio. The results of the experiments are presented in the next section.

@&#CONCLUSIONS@&#
