@&#MAIN-TITLE@&#
Detection of delamination in laminated composites with limited measurements combining PCA and dynamic QPSO

@&#HIGHLIGHTS@&#
Proposed a new damage diagnostic technique for detection of delamination in laminated composite structures.Proposed algorithm is developed combining FRF, PCA and dynamic quantum PSO algorithm.Integrated an optimal sensor placement algorithm to test the proposed algorithm with limited measurements.A new dynamic quantum PSO algorithm is proposed to solve the inverse problem associated with damage assessment.

@&#KEYPHRASES@&#
Structural health monitoring,Damage detection,Delamination,Matrix cracking,Principal component analysis,Frequency response,Quantum PSO algorithm,

@&#ABSTRACT@&#
This paper presents an output only damage diagnostic algorithm based on frequency response functions and the principal components for health monitoring of laminated composite structures. The principal components evaluated from frequency response data, are employed as dynamical invariants to handle the effects of operational/environmental variability on the dynamic response of the structure. Finite element models of a laminated composite beam and plate are used to generate vibration data for healthy and damaged structures. Three numerical examples include a laminated composite beam, cantilever plate made of carbon–epoxy and a laminated composite simply supported plate. Varied levels of delamination of laminated composite plies and matrix cracking at varied locations in the plies are simulated at different spatial locations of the structure. Numerical investigations have been carried out to identify the spatial location of damage using the proposed principal component analysis (PCA) based algorithm. In order to limit the number of sensors on the structure, an optimal sensor placement algorithm based on PCA is employed in the present work and the effectiveness of the proposed algorithm with a limited number of sensors is also investigated. Finally, the inverse problem associated with the detection of delamination and matrix cracking is formulated as an optimization problem and is solved using the newly developed dynamic quantum particle swarm optimization (DQPSO) algorithm. Studies carried out and presented in this paper clearly indicate that the proposed SHM scheme can robustly identify the instant of damage, spatial location, the extent of delamination and matrix cracking even with limited sensor measurements and also with noisy data.

@&#INTRODUCTION@&#
Usage of laminated composite materials in many mechanical and aerospace engineering structures is continuously increasing basically due to their specific stiffness and strength. However, because of these material characteristics, damage can take place by inappropriate or hazardous service loads. Delamination is one of the most common and dangerous damages, caused by internal failure of the laminas interface. These internal damages can be undetected by visual inspection. Therefore, in order to assess the structural integrity, non-destructive inspection methods are needed for damage localization. The most used non-destructive inspection methods are either visual or localized experimental methods such as acoustic or ultrasonic methods, magnetic field methods, radiographs, eddy-current methods or thermal field methods [1]. These experimental methods can detect damage on or near the surface of the structure [1], and cannot detect the Delamination. Hence, structural health monitoring strategies are currently being employed to detect the hidden damages like matrix cracking and Delamination in the laminated composite structures.The principal objective of structural health monitoring is to diagnose the onset of damage in order to allow timely maintenance of the structure. Structural health monitoring (SHM) of engineering structures is a very active area of research as evidenced by the proceedings of the structural health monitoring conferences [2,3] and several books have been published recently dealing with SHM [4–6]. The task of damage diagnosis can be persecuted through the analysis of the vibration characteristics of the structure to be monitored, by deploying a network of sensors to record the structural response at several locations during vibrations. Analysis of recorded responses should allow determining the possible structural deterioration through some properly defined features related to damage. In the last decade, several vibration-based damage detection methods have been proposed based on modifications of the structural shape between the undamaged and the damaged configuration of the structure to detect the location of damage. Degradation due to delamination in laminated composites causes reduction in flexural stiffness and strength of the material and as a result, vibration parameters like natural frequency and modal responses are changed. Hence, it is possible to monitor the modal characteristics to identify the presence of delamination, and assess its size and location for structural health monitoring. The use of vibration based Delamination identification and health monitoring techniques for composite structures has been surveyed by Zou et al. [7]. These methods, based on features related to the deformed shape of the structure, can be classified as modal or non-modal methods according to whether they rely on modal or non-modal features to detect damage. Modal-dependent delamination identification can be tactically treated as an optimization problem, for which the objective function is created by relating measured modal frequencies to delamination variables [8–11]. An appropriate optimization method is utilized to optimize the objective function, giving the estimate of delamination parameters. The majority of the works reported earlier have used genetic algorithm as an optimization algorithm [12,13]. Mohebbi et al. [14] have used artificial immune algorithm combining with modal characteristics for detection of delamination. However, recently swarm based optimization algorithms like particle swarm optimization algorithms are gaining popularity over genetic algorithms. In the present work, we propose a new variant of Particle swarm optimization based on quantum principles with dynamic multiple swarms for detection of delamination in laminated composite structures.Some drawbacks related to the use of mode shapes for damage detection, mainly the need of a time consuming modal identification process, the loss of information due to the use of mode shapes alone and their inability to give reliable information about damage located close to a node, is overcome by the so-called non-modal methods based on the analysis of modifications of the deformed shape defined on a broadband frequency range. This family includes methods, such as the frequency response curvature method (FRCM) [15,16], that utilize the “operational shapes” defined for each frequency by the frequency response at the different locations of the structure. Another non-modal method based on the frequency response function called gapped smoothing method (GSM) has been proposed [17,18], where damage detection is carried out looking for deviations from a smooth behavior of the curvature: deviation is pointed out by the difference between the experimental curvature, calculated from displacements using a central difference approximation, and its polynomial interpolation as a function of curvatures in the neighboring locations. The major advantage of GSM is that it does not require base line (reference) information. However, this approach is based on the assumption that the healthy structure has a smooth deformed curvature, which may not be true always. In such instances, damage detection based solely on data measured on the structure in the inspection phase may be misleading [19].As mentioned earlier, using the non-modal (FRF based) approaches, we can avoid the tedious modal analysis phase of computations. Apart from that, the major advantage of the non-modal approach to the problem of damage detection is that the frequency response function and the corresponding deformed shapes and curvatures, are defined on the whole frequency range, contain broadband data. This is hardly achieved using mode shapes since current procedures of modal parameter extraction are usually effective only for a limited number of lower modes, hence information relevant to the higher frequency range is often lost. Furthermore, frequency response functions, if calculated in terms of acceleration, can be recovered directly from the time histories of recorded signals without resorting to displacements that are sometimes tricky to evaluate from recorded accelerations.Even though non-modal approaches discussed above have considerable merits over modal methods, they too have limitations like modal methods while handling environmental variability. In the last few years, analyses carried out on several instrumented structures [20–23] have pointed out that environmental and operational conditions (wind, temperature, mass, humidity) may induce variations of the structural characteristics, which can be misleading in the damage detection process, masking structural changes caused by damage or wrongly suggesting the existence of damage to a healthy structure. In order to avoid false or missing indications of damage, the effect of environmental variations should be properly taken into account and several papers have been published recently on this subject. Two main approaches can be followed to take into account the environmental effect. The first one is based on techniques that are able to remove the effect of the environment from the features chosen to point out the existence and the location of damage [24,25]. In this case usually the effect of the environment, specifically the temperature, should be measured and its influence on the features should be modeled. Recently methods based on the decomposition of the covariance matrix of the damage detecting features have been proposed that allow removing the environmental effects without measuring them [26,27]. The second approach is based on the use of damage detecting features that are scarcely affected by changes in environmental conditions but strongly affected by damage. The presence of damage is pointed out by the occurrence of spurious peaks in the modal filters [28]. Such peaks appear when local changes of stiffness occur and are much less influenced by global changes due to variations of temperature.In this paper, we propose a new damage diagnostic method using principal component analysis (PCA) for detection of damage in laminated composite structures due to matrix cracking and delamination. We have handled environmental variability using the parameters derived from PCA. Once the time instant and spatial location of damage is identified, we perform inverse analysis to identify the extent of matrix cracking and/or delamination by combining the PCA with a new advanced quantum PSO algorithm. In order to investigate the robustness of PCA in damage detection with a limited number of sensors, we have integrated an optimal sensor placement algorithm based on PCA for computing the responses with limited sensors. The robustness of the proposed algorithm with measurement noises is also investigated.The main contribution of the paper is the development of a robust damage diagnostic technique for laminated composite structures with limited measurement data, capable of handling environmental variability and measurement noises effectively. The proposed algorithm is developed by combining PCA with a newly developed quantum particle swarm optimization algorithm with dynamic subpopulations.The goal of PCA is to extract a set of basis functions from the dynamic response of a system that is varying both in space and time. In the context of structural dynamics, we can view PCA as a means to uncouple the spatial and temporal content contained in the dynamic response of the system. Further PCA allows a high-dimensioned process to be very accurately represented by a low-dimensioned model, thus allowing a large amount of data to be processed and modeled easily. A dynamical system can be approximated using a set of corresponding time-functions and spatial functions as follows [29].(1)z(x,t)=∑k=1Mak(t)ϕk(x)This approximation becomes exact as the number of corresponding functions, M, approaches infinity. The spatial functionsϕk(x)are termed as principal components and can be used to establish a reduced-order model of the system. The representation of a dynamical system given in Eq. (1) is not unique and the basis functions can alternatively be represented as a Fourier series, Legendre polynomials, or Chebyshev polynomials [29]. A set of corresponding time functionsak(t)can be obtained based on the choice of basis functionϕk(x)in order to provide a suitable representation ofz(x,t). However, PCA provides an optimal set of basis functions (principal components) for a given number of modes. Further PCA is applicable to nonlinear cases as well; however, optimality exists only for linear systems [30]. Alternatively, the analytical frequency response can be decomposed into principal directions and singular values, which can be directly related to the system’s energy [31,32]. In the present work, we use Frequency response rather than time history data directly to obtain Principal components.Assuming ambient vibration, the FRF of a structureHk(jω), at sensor location k, can be considered equivalent to the Fourier spectrum of the response data collected by that sensor. This spectrum can be formulated by converting measured accelerations to the frequency domain using a fast Fourier transform (FFT). The frequency response data matrix, H, can then be defined as a collection of the individual frequency response vectors(2)H=[h(ω1)h(ω2)h(ω3)…h(ωf)]Tin which f is the number of data points in the frequency range of interest, and h(ωi)is a vector of lengthns, wherensis the number of sensors.In the present work, discrete FRF data is analyzed using the Singular Value Decomposition (SVD) which is a discrete realization of PCA. Using SVD a packet of FRF data H (ana×nsmatrix of data,nadata points in the frequency range of interest ofnsdifferent measurements) can be decomposed as(3)H=UΣVTwhere U and V are orthogonal matrices (na×naandns×ns, respectively), and Σ is a diagonal matrix. The columns of the complex orthogonal matrix V are the computed principal components. The diagonal matrix Σ is termed as the singular matrix whose elements (along the diagonal) are non-negative numbers, called the singular values, arranged in decreasing order. These singular values each correspond to a single basis function, V, and represent the level of ‘energy’ present in each mode. V is a matrix with orthogonal columns containing the normalized frequency response of the principal directions. The principal directions and singular values of the frequency response data matrix are also the eigenvectors and eigenvalues of the matrix(4)HH∗=UΣ2U∗in which ∗ represents the complex conjugate transpose. It is very well documented in the literature that the singular values are related to the total energy of the system contained in the corresponding principal directions [32,33]. The singular values are arranged from largest to smallest. Most of the system’s energy is usually concentrated in the first several singular values. The corresponding principal directions show how the energy is distributed in the system [33]. There are a variety of methods for determining how many singular values to retain to properly characterize a system [34]. A common method is to retain the singular values and principal directions which correspond to the top 95% or 99% of the system’s total energy [31]. Like mode shapes, principal directions are the fundamental shapes that represent the system’s dynamics. However, principal directions automatically account for damping in the structure, effects of input locations, and out-of-band modes. Researchers have shown that principal directions converge to normal modes in symmetric linear systems if the mass matrix is diagonal and uniform, and the system is lightly damped [33,35,36]. In general, principal directions do not coincide with the mode shapes, but it can be shown that the principal directions with non-zero singular values span the same subspace as the excited modes. While there may be many vibrational normal modes in a frequency band, the response is usually dominated by a relatively small number of principal directions. Using principal directions, we can avoid the difficult task of identifying the dynamically important mode shapes. The effects from input location and damping are automatically accounted for in the principal directions.It is common practice to use only the real part of the HH∗, often referred to as the output covariance matrix, to determine principal directions and singular values (λ)for the system [32]. However, it can be shown that the sum of the k largest singular values for HH∗ will be greater than the sum of the k largest singular values of the covariance matrix. It is clear from this that more system energy can be captured using fewer singular values and principal directions from the complex covariance matrix, when compared to only its real part. In other words, for a given number, principal directions from the complex covariance matrix capture the information in the frequency response more accurately(5)∑i=1pλi(R(HH∗))⩽∑i=1pλi(HH∗)The matrices U and Σ can be multiplied to form a matrix Q and Eq. (3) can be re-written as(6)A=QVT=∑k=1mqkνkTwhereqkandνkare column matrices that represent the functionsak(t)andϕk(x)of Eq. (1), respectively. The basis functionsνkare the dynamical invariants of the system. The matrix Q, is obtained through multiplication of the left orthogonal matrix U and Σ. The singular values can be used to derive a lower order model by establishing a reduced order matrix,Σred, that contains the first p singular values with the rest of the diagonal entries set to zero. The model order p can be set to capture the desired percentage of total ‘energy’ in the system.A damage detection algorithm presented in this paper is based on the PCA by singular value decomposition of FRF data. Healthy (baseline) database of the structural responses (at specified sensor locations as shown later) is generated by using varied levels of random loads. The reference acceleration time history data at each node is partitioned into several subsets. These subsets need to be generated in order to have a large collection of data samples obtained under varying loadings. Apart from this it is also essential to have a large collection of data sets to include the effect of the statistical uncertainty. This pool of data sets should consist of data measured in as many varying environmental and operational conditions as possible. These subsets of data collected from the structure in the healthy state, are partitioned into two healthy data sets, in order to compute the residual errors due to operational and environmental variability.The FRF of the time history data in both the healthy data sets is first computed using FFT as outlined earlier. The SVD is performed (and the components are saved) on the FRF datasets of the first healthy database to compute the principal components and time functions Q and it is repeated for the second healthy data sets also. The time functions derived from the second healthy database are used to compute the residual error using the corresponding time functions from the first base line or healthy database. Once the level of residual error for healthy data is established, an ‘unknown’ case (damaged or healthy structure) can be analyzed and if the residual error is significantly higher than in the healthy case the data can be said to be anomalous and the structure damaged. The damage detection algorithm based on the above concept is detailed below.i.Arrange FRF data in ana×nsmatrix with each column representing number of data points in the frequency range of interest of an individual sensor. These data sets for first healthy database, give matricesHj, where j = 1, 2, 3, 4, 5 …. 10, denotes the number of data subsets.Compute Singular value decomposition of H matrix(B[1]denotes a first baseline or healthy database).(7)HB[1]=UB[1]ΣB[1]VB[1]T(8)QB[1]=UB[1]ΣB[1]Perform Singular value decomposition on FRF data sets in the second healthy database.(9)HB[2]=UB[2]ΣB[2]VB[2]T(10)QB[2]=UB[2]ΣB[2]In the second healthy database, for each functionQB[2], find the closest functionQBj[1]in the first healthy database. This is accomplished by minimizingηj, where j=1, .. , 10. The PCA corresponding to the closest function is denotedVBc[1].(11)ηj=QB[2]-QBj[1]2Form reduced order singular value matrixΣredby replacing all the singular values after the first p diagonal elements with zeros. The reduced model order p is set to capture the desired percentage (99% in the present case) of total ‘energy’ in the system. Using the reduced order function,Qredevaluate the corresponding reduced order model Φ.(12)Qred=UB[2]∑red(13)Φ=QredVB[2]TCompute model filterMFcusing reduced order functionQredand the closest principal component of the first baseline databaseVBc[1].(14)MFc=QredVBC[2]Compute residual error matrix R by subtracting model filterMFcfrom the model Φ. Then compute the standard deviation of the columns of R and find its average valueαkA.(15)R=Φ-MFc(16)αk=1n∑i=1nσriRepeat steps (iii)–(vii) for each data subset in a second healthy database, saving standard deviation,αk, where k ranges from 1 to the total number of FRF datasets (10 in the present case) in second healthy database.Compute average ofαkgiving a measure of residual error for healthy data files.(17)αH=1k∑k=1kαkx.Repeat steps (iii)–(ix) above using ‘unknown’ (damaged or undamaged) current FRF data subsets instead of data from the second healthy database and obtainαU(similar toαHin step ix).Evaluate Damage Index (DI) as the ratio ofαUandαH. For a healthy structure, the damage index is expected to be closer to unity. If Damage Index (DI) value is much larger than unity indicates presence of damage.(18)DI=αUαHThe basic problem of damage detection is to deduce the existence of damage in a structure from the measurements obtained from the sensors, distributed on the structure. The quality of these measurements and thus the quality of structural health monitoring achieved is to a large extent dependent upon where sensors are placed on the structure. Cost and practicality issues preclude the instrumentation of every point of interest on the structure and lead us to select a smaller set of measurement locations. A large variety of performance indices have been developed for the problem of sensor placement, but it is only comparatively recently that the problem has been considered from SHM perspective. Some methods require a single calculation to be performed, some are iterative, and many others take the form of an objective function to which an optimization technique must be applied. The majority of the current sensor placement methods are based on the mode shapes of the pretext finite element model (FEM). The major problem associated with these methods lies in identifying dynamically sensitive modes to damage. Hence, as an alternative, we employ a sensor placement technique based on the structure’s frequency response in this paper.This technique is a generalization of the Effective Independence approach [37–39]. The analytical frequency response can be calculated for finite element models based on specified frequency bands, damping, and input locations. The analytical frequency response can be decomposed (using singular value decomposition) into principal directions and singular values, which can be directly related to the system’s energy. In general, principal directions do not coincide with the mode shapes, but it can be shown that the principal directions with non-zero singular values span the same subspace as the excited modes. A system’s response is usually dominated by a relatively small number of principal directions. Using principal directions eliminates the difficult task of identifying the dynamically important mode shapes. The effects from input location and damping are automatically accounted for in the principal directions.The aim of the FRF based effective independence (FEfi) technique is to place sensors to maintain the dynamically important information contained in the frequency response data within the desired frequency band. This is accomplished by placing sensors such that the measured response is rich in the response of the active principal directions. The principal directions are analogous to mode shapes and they represent shapes that are fundamental to the structure’s dynamics. As in the case of modal based Efi, the FRF based sensor placement problem (FEfi) can be written in the form of a state estimation problem(19)H=ψV‾+NwhereV‾=SVrepresents the frequency response of the dynamically important principal directions, and N is a matrix of Gaussian white noise. The sensors are to be placed on the structure in such a way that the measured FRFs should be able to estimate the response of the dynamically important principal directions accurately. The corresponding Fisher information matrix can be written as(20)Qc=ψc∗Wψcwhereψcare the active principal directions partitioned to the candidate sensor set, and W is a weighting matrix, i.e., the inverse of the noise covariance matrix. In this paper, we choose W to be an identity matrix. Maximizing the information matrix in an appropriate norm result in minimizing the error covariance matrix and it provides the best state estimate. Sensors should be placed to provide the best estimate of the target modal response. Maximizing the determinant of the information matrix is chosen as the sensor placement criterion, because it results in maximizing signal strength and the independence of the target principal directions. Similar to the Effective Independence (Efi) method, the proposed FEfi is formulated by iteratively truncating the sensor locations that have the smallest impact on the value of the determinant of the information matrix. The FEfi value corresponding to the ith sensor is given by(21)FEfii=ψiQ-1ψiTwhereFEfii, represents the fractional reduction of the information matrix determinant, if the ith sensor is removed from the candidate set. The candidate sensors are ranked based on their FEfi values and the sensor corresponding to the lowest FEfi value is removed. The FEfi values range from 0 to 1, where a zero valued sensor can be removed without impact to the information matrix determinant, while sensors with a value of 1 are vital to the independence of the target shapes and cannot be deleted. The candidate sensor locations are deleted in an iterative fashion to finally produce the desired number of sensors. In the final sensor set, the number of sensors must be at least equal to the number of target modes to ensure independence. New information matrices and FEfi values must be computed after truncation of each sensor as the FEfi value for each sensor change as sensors are dropped from the candidate set.Delamination is a common failure mechanism in composite laminates and is one of the primary concerns in the current design with composite materials. It rarely occurs as an independent damage mode and is often associated with and triggered by other damage modes such as matrix cracking. The stiffness degradation due to matrix cracking and delamination has recently been investigated by several researchers [40–45]. Theoretical predictions reveal that transverse crack tip delaminations cause significant reduction in the shear modulus and Poisson’s ratio of cross-ply and symmetric balanced[±θm/90n]slaminates. In our present work, we have evaluated the stiffness degradation due to matrix cracking and delamination, using the theoretical approach developed by employing the equivalent constraint model of the lamina [40]. This approach helps in avoiding cumbersome consideration of the repeated laminate element defined by the intersecting pairs of transverse cracks and splits. It also uses an improved 2D shear-lag analysis [41] to determine the stress fields in the explicitly damaged lamina and the In situ Damage Effective Functions to describe its reduced stiffness properties. Reduced stiffness properties of the damaged lamina are found to depend explicitly upon the crack density and relative delamination area associated with that lamina and implicitly upon two damage parameters associated with the neighboring lamina.Kashtalyan and Soutis [44] have examined the dependence of the laminate reduced elastic properties on the orientation angle of the constraining ply and also established the contribution of each damage mode (transverse cracking, transverse crack tip delaminations, splitting and split tip delaminations) into stiffness loss. Here we present very briefly the formulation details related to stiffness degradation for the sake of completion. The complete formulation details of the related to the equivalent constraint model (ECM) can be found in Refs. [40–45].Transverse cracks and splits are assumed to be spaced uniformly and to span the full thickness and width of the 90° and 0° plies, while delaminations are assumed strip-shaped. Spacing between splits and transverse cracks are denoted respectively2s1and2s2, and the length of longitudinal and transverse delaminations is denoted2l1and2l2, respectively. These details are shown in Fig. 1.The reduced stiffness properties of the layer m; damaged by transverse cracking or splitting and delaminations, can be determined by applying the laminate plate theory to theECMμafter replacing the explicitly damaged layer with an equivalent homogeneous one.The in-plane reduced stiffness matrixQμof the homogeneous layer equivalent to theμth layer of theECMμis given by(22)Q(μ)=[Q¯(μ)]-[R(μ)](23)[R(1)]=Q‾11(1)Λ22(1)Q‾12(1)Λ22(1)0Q‾12(ply)Λ22(1)Q‾12(1)2Q‾11(1)Λ22(1)000Q‾66(1)Λ66(1)(24)[R(2)]=Q‾12(1)2Q‾22(1)Λ22(1)Q‾12(1)Λ22(1)0Q‾12(ply)Λ22(1)Q‾22(1)Λ22(1)000Q‾66(1)Λ66(1)The closed form expressions for in situ damage effective functions(IDEFs)Λ22(μ),Λ66(μ)can be written as functions of relative cracking/splitting densityDμmc=hμ/sμrelative delamination areaDμld=lμ/sμ; the layer compliancesS∧ij(μ),S∧ij(k),k≠μ; shear-lag parametersKjand the layer thickness ratio, χ as [44](25)Λqq(μ)=Λqq(μ)′Dμmc,Dμld,Sˆij(μ),Sij(k),Kj,χ(26)Λ22(μ)=1-1-Dμmcλ1(μ)1-Dμldtanhλ1(μ)1-DμldDμmc1+λ1(μ)Dμld1-Dμld+α1(μ)Dμmcλ1(μ)1-Dμldtanhλ1(μ)1-DμldDμmcμ=1,2(27)Λ66(μ)=1-1-Dμmcλ2(μ)1-Dμldtanhλ2(μ)1-DμldDμmc1+λ2(μ)Dμld1-Dμld+α2(μ)Dμmcλ2(μ)tanhλ2(μ)1-DμldDμmcμ=1,2where the constantsλi(μ)αi(μ),i=1,2; depend solely on the layer compliancesS^ij(μ),S^ij(k),k≠μ; shear-lag parametersKj,jand the layer thickness ratio χ.The modified compliancesSij(κ)k≠μ; of the equivalently constraining kth layer of theECMμare determined from the analysis of theECMkand therefore are functions of the IDEFsΛ22(k),Λ66(k)Thus, the IDEFs of theμth layer depend implicitly on the damage parametersDkmcandDkldassociated with the layer k.The IDEFs for both layers form a system of simultaneous nonlinear algebraic equations(28)Λqq(1)=Λqq(1)D1mc,D1ld,S^ij(1),Sij(2)(D2mc,D2ld,S^ij(2),Λqq(2)),χ,q=2,6(29)Λqq(2)=Λqq(2)D2mc,D2ld,S^ij(2),Sij(1)(D1mc,D1ld,S^ij(1),Λqq(1)),χ,q=2,6These system of equations are solved by a direct iterative procedure. The newly computed IDEFsΛqq(μ)are used to evaluate the reduced stiffness of the equivalently constraining kth layer repeatedly, until difference between two iterations meets the specified convergence criteria. Once the reduced stiffness matrices of the damage lamina are obtained using Eqs. (22)–(24), the reduced stiffness parameters of damaged laminate can be computed as(30)ExR=1S11R=Q11RQ22R-Q12R2Q22R;EyR=1S22R=Q11RQ22R-Q12R2Q11RGxyR=1S66L=Q66R;v12R=-S12REyR=Q12RQ11Rwhere superscript ‘R’ corresponds to reduced stiffness of lamina.In order to identify the damage distribution, we have formulated the damage quantification problem as an optimization problem and solved using a newly proposed quantum particle swarm optimization algorithm. Several earlier works have reported damage severity evaluation using genetic algorithms [46,47]. In the present work, we perform the inverse analysis to compute the exact loss of stiffness (stiffness degradation in the laminated composite) by using dynamic quantum PSO algorithm. The design variables are the stiffness reduction factors of each ply in the elements isolated as damage elements based on the damage index values computed using the proposed PCA based algorithm and using an appropriately formulated objective function. Later, we will make use of the converged values of IDEFs (design variables in the optimization) to arrive at the appropriate levels of delamination length and matrix crack spacing in both directions in the plies identified by the optimization algorithm. The objective function employed in the present formulation is based on the principal components and principal values.The reduced stiffness parameters of each ply in the already identified damaged jth element can be represented as(31)Exd,Eyd,Gxyd,v12d(j,k)T=β(j,k)IExu,Eyu,Gxyu,v12u(j,k)Twhere subscripts (j,k) refers to kth ply of jth element. The vectorβ(j,k)∈[0,1]is of size (1×4) defines the reduction in the kth lamina properties of the jth element and I is an identity matrix of size 4. The stiffness matrix K of each damaged element, j can be computed with the reduced laminate properties.The global stiffness matrix is given by(32)K=Aj=1nelKwhere ‘A’ denotes the assembly operator in the finite element method for assembling the contribution of each one of thenelelements.The objective function can be constructed by minimizing the deviation between the singular values (SV) and principal components of the current (damage) subset of data and evaluated from the finite element model constructed with a given trial set β of elemental stiffness reduction factorsβk,k=1,2,…nel. The partial objective function related to singular values can be written as:(33)fsv(β)=∑i=1nmfSVim-SVie(β)whereSVie(β)is the value of the ith singular value calculated from the finite element model constructed with a given trial set β of elemental reduction factorsβk,j=1,2,…nelandSVimcorresponds to the singular values of the current data subset. The number of vectors considered is denoted by ‘nmf’. Similarly, the partial objective function related to principal components can be written as:(34)fpc(β)=∑i=1NPC∑j=1NnodesPCijm-PCije(β)where NPC indicates the number of principal components considered and obtained from experimental measurements, Nnodes is the number of nodes where the acceleration time history measurements are compiled from the current subset of data.PCmare the vectors of principal components obtained from the measured acceleration data (current subset) andPCe(β)are principal components calculated from the finite element model constructed with a given trial set β of elemental reduction factorsβk,k=1,2,…nel. Combining Eqs. (33) and (34), the fitness can be written as:(35)Fit(β)=fsv(β)+fpc(β)The variation in singular values and principal components could potentially provide a way to identify structural modifications due to damage. However, the number of singular values used will be very small, as we limit the number of principal components based on the cutoff energy criteria. It would be reasonable to expect that the formulation given in Eq. (35) may not always be able to accurately identify the structural damage. Hence it is desirable to include time history data also in the objective function. However, including the time history data of the entire subset of all measured channels is cumbersome. In view of this, the time history data is transformed using the principal components and the reduced order data is used to construct the objective function. Let P be the reduced size sayn×m, where m is the number of principal components considered based on the energy criteria andm≪n. The reduced order time history data can be written as(36)A∼(t)=PTA(t)The partial objective function based on time history data can be constructed as(37)ftimehistory(β)=∑j=1NPC∑k=1nstepsA∼jCS(k)-A∼jFE(k)where NPC is the number of principal components considered, steps is the number of time steps in the sampling data. While the superscript ‘CS’ stands for the measured acceleration time history data of the current subset, ‘FE’ stands for acceleration time history data calculated from the finite element model constructed with a given trial set, β of elemental reduction factors,βk,k=1,2,…nel. The ∼ sign indicates reduced order data obtained using Eq. (36).However, if we algebraically add Eqs. 33, 34 and 37 to formulate the objective function, the difference in dimensionality and in the order of magnitude between them will pose problems. In absolute value, the difference between Eqs. 33, 34 and 37 for a given trial set of β is generally large. In order to make more uniform the contribution of each partial objective in the final value of the objective function, a normalization of the three functions can be achieved in the context of population based methods like Quantum PSO by writing the ith individual of the population(38)Fit(SV+PCA+timehistory)i=fsvi-fsvmin(fsvmax-fsvmin)+fpci-fpcminfpcmax-fpcmin+ftimehistoryi-ftimehistoryminftimehistorymax-ftimehistoryminThe subscripts max and min indicate the maximum and minimum values in the current population, of each principal objective function. The fitness contribution to proper orthogonal value is computed using Eq. (33), contribution from principal components is computed using Eq. (34) and finally the contribution from time history response is evaluated using Eq. (37).The objective function given in Eq. (38) is constructed to handle several principal components of the structure. When one obtains the correct response by means of the objective function given in Eq. (38), its value should be equal to zero for each term of the summation. However, one observes that the terms in the summation of the objective function present a considerable variation in magnitude which makes the search problem harder. In an attempt to circumvent this difficulty, it is proposed to gradually increase the number of principal components and singular values used along the evolutionary process. The goal is to reduce the amount of information used by the fitness function in the initial stages of evolution in order to simplify the search. To accomplish this, the number of principal components considered in the evolutionary process will be treated a variable. With the advancement of generations, the number of principal components in the fitness functions is gradually and monotonically increased until all principal components available are included. Initially, only one principal component is considered and the decision to include the additional principal component is taken when there is stagnation in the evolution of best individual. Every time the search does not produce a new best element after s specified number of generations, one additional principal component is included in the computation of the fitness function. This process is continues till all the principal components considered are included. The constraint is handled using a penalty formulation.Apart from this, a constraint value is introduced by means of subspace angle. The subspace angle between the current data (subspace with damage) and the reference (baseline subspace data) should be equal to ANGLE. With this, the proposed damage distribution algorithm involves solving the constrained optimization problem. The constraint can be written as(39)f(β)Anglei=ANGLEwheref(β)Angleiis the principal angle between the reference subspace and the subspace of data generated with the stiffness parameters of the ith population. ANGLE is the stipulated constraint value and it is arrived at by evaluating the principal angle between the current subset data and the closest reference subset data obtained from Eq. (11).The principal angle between two data subsetsS1andS2can be computed by performing a singular value decomposition ofQ‾1TQ‾2. The singular values define the q cosines of the principal anglesθibetween data subsetsS1andS2.(40)SVDQ‾1TQ‾2→Diag(cos(θi))i=1,….qThe largest angle allows to quantify how the subspacesS1andS2are globally different.Q‾1TandQ‾2Are the orthonormal bases of subsetsS1andS2respectively, and can be computed by performing QR decomposition ofS1andS2.(41)S1=Q‾1R1Q‾1∈RnsxpS2=Q‾2R2Q‾2∈RnsxqThe proposed constrained optimization problem is expected to lead to the correct damage distribution. Numerical simulation studies are presented later in this paper to demonstrate the effectiveness of the proposed damage distribution algorithm using the proposed self configurable dynamic quantum particle swarm optimization algorithm. In the next section we present the details of the proposed quantum based dynamic particle swarm optimization algorithm.Once the reduction factors β are evaluated using the proposed QPSO algorithm for the damaged elements, we can compute the IDEF of the damaged lamina using Eqs. (22)–(29) and referred to as actual IDEFs.Considering the delamination lengthL1,L2and crack spacingS1,S2as design variables, we can use the proposed DQPSO algorithm with the objective function as minimizing the error between the computed IDEF and the actual IDEF arrived earlier using the computed stiffness reduction factors.Particle swarm optimization (PSO) is a population based stochastic optimization technique developed by Kennedy and Eberhart [48] and Eberhart and Kennedy [49] inspired by social behavior of bird flocking or fish schooling.PSO is initialized with a group of random particles (solutions) and then searches for optima by updating generations. The particle is represented byPi=(pi1,pi2,…,piD)in a D-dimension search space, and its velocity isVi=(vi1,vi2,…,viD). Each particle adjusts its trajectory towards to the position of its own previous best performance(pbesti)and the best position discovered by the whole population (gbest). PSO uses a real-valued multi dimensional space as search space, defines a set of particles located in that space, and evolves the position of each particle using its current velocity, the known previous best position and the global best position. After finding the two best values, the particle updates its velocity and positions with following equations.(42)vijk+1=ωvijk+c1r1pbestij-xijk+c2r2gbestj-xijk(43)xijk+1=xijk+vijk+1vijis the particle velocity,xijKis the current particle (solution) andw,c1andc2are weight coefficients. In PSO, the existence of position data on the swarm-shared best solution gbest assures interaction among agents.The major drawback of PSO is that the swarm may prematurely converge. The underlying principle behind this problem is that, for the global best PSO, particles converge to a single point, which is on the line between the global best and the personal best positions. This point is not guaranteed for a local optimum. Another reason for this problem is the fast rate of information flow between particles, resulting in the creation of similar particles with a loss in diversity that increases the possibility of being trapped in local optima. Therefore PSO has undergone a plethora of changes since its development to improve the convergence characteristics of the classical PSO. One of the recent developments in PSO is the application of Quantum laws of mechanics to observe the behavior of PSO. Such PSO’s are called Quantum PSO (QPSO) [50,51]. In the present work, we have made an attempt to improve the convergence characteristics by building PSO algorithm with quantum search features and also by employing multiple swarms with dynamic reconfigurable features.In classical mechanics, a particle is depicted by its position vector x and velocity vector v. In quantum physics, the state of a particle with momentum and energy can be depicted by its wave functionψ(x,t)(Schro˝dinger equation) instead of the position and velocity used in traditional PSO. The dynamic behavior of the particle is widely divergent from that of the particle in traditional PSO systems. In this context, the probability of a particle appearing in a certain positionxican be obtained from a probability density function, the form of which depends on the potential field the particle lies in. Solving the Schro˝dinger equation, we can get the normalized probability distribution function F.(44)F(y)=∫-∞-∞Q(y)dy=e-2|q-x|Lwhere L determines search scope of each particle. Employing the Monte Carlo method, the particles move according to the following iterative equation(45)xijt+1=pijt±β|mbestijt-xijt|×ln(1/uij)wherembestijis the mean best of all the particles in jth dimension,uijis a random number uniformly distributed in the range [0,1]. The subscripts i and j refers to the particle and design variable respectively. This equation is implemented as(46)xijt+1=pijt+βmbestijt-xijt×ln(1/uij)ifk>0.50(47)xijt+1=pijt-βmbestijt-xijt×ln(1/uij)ifk⩽0.50where k is a random number in the range [0,1]. The most commonly used controlled strategy of β is to initially setting it to 1.0 and reducing it linearly to 0.30. In the present work, the parameter β varied linearly from 1.0 to 0.30 with the iteration as(48)βt=βmax-(βmax-βmin)t×max_iterationspijtis the local attractor and defined as:(49)pijt=φijtPbestijt+1-φijt·gbestjtwhereφijtis a random number uniformly distributed in [0,1]. β is called the contraction–expansion coefficient, which can be tuned to control the convergence speed of the algorithm. The ‘mbest’ is the mean best position and is defined as the center of the best positions of the swarm and it can be written as:(50)mbestijt=mbest1t,mbest2t,mbest3t……mbestDt=1M∑i=1MPi1t,1M∑i=1MPi2t,1M∑i=1MPi3t,……….,1M∑i=1MPiDt,where M is population size andPiis the personal best position of particle i. The details of the QPSO algorithm are given in Fig. 2.The characteristics of QPSO algorithm are reflected mainly in two ways. First of all, the introduced exponential distribution of positions makes QPSO search in a wide space. Furthermore, the introduction of Mean Best Position into QPSO is another improvement. In the standard PSO, each particle converges to the global best position independently. In the QPSO with mean best position GP, each particle cannot converge to the global best position without considering its colleagues because the distance between the current position and GP determines the position distribution of the particle for the next iteration.Although QPSO possesses better global search behavior than PSO, it may encounter premature convergence, a major problem also encountered by GA, PSO, and other evolutionary algorithms in multimodal optimization, which results in significant performance loss and suboptimal solutions. In QPSO, although the search space of an individual particle is the whole feasible solution space of the problem throughout the iterations, diversity loss of the whole population is also inevitable due to the collectiveness.In general, majority of evolutionary algorithms perform better with larger populations. While PSO family of algorithms needs a comparatively smaller population size, the size of the swarms is still pretty large. Further, in PSO, when dealing with complex optimum problems, premature convergence is still the main drawback and the solutions are most likely to be trapped into local minima. Each particle in the swarm learns from the gbest, even if the current gbest is not the global optimum. As a result, the particles are attracted to be trapped into a local optimum, if the problem is complex with numerous local optimums [52]. It has been reported in the literature [53] that, PSO with small neighborhoods performs better on complex problems.Hence, in order to bring down the size of the swarms further and to obtain a faster convergence, further modifications are made to the quantum particle swarm optimization (QPSO) algorithm resulting in what is called as the dynamic quantum particle swarm optimization algorithm (DQPSO). The dynamic quantum particle swarm optimizer is constructed based on the QPSO algorithm with a new neighborhood topology. Hence, In order to increase the diversification to achieve better results, especially on multimodal problems, the proposed DQPSO uses small neighborhoods. In the proposed algorithm, the swarms are dynamic and the size of swarms is small. The whole population is divided into many small swarms called sub-swarms and each sub-swarm uses its own members to search for better area in the search space. These sub-swarms are regrouped frequently and rather dynamically by using several regrouping schedules. Thus the information is exchanged among the swarms.Since the small sized swarms are searching using their own best historical information, they are likely to converge to a local optimum because of typical PSO’s convergence characteristics. In order to prevent the convergence to sub-optimal solution, the information has to be exchanged among the swarms. While exchanging information among sub-swarms, it is necessary to exercise sufficient care to maintain larger diversity in sub-swarms. In order to accomplish this, we have proposed a shuffling schedule to have a dynamically changing neighborhood structure for the particles. After every user defined ‘S’ generations, the population is shuffled and the search will be continued using a new configuration of small swarms. In the proposed DQPSO algorithm, the search is based on quantum principles (QPSO) in each sub- swarm and dynamic mixing of the results obtained through this parallel searches contributes to move towards a global solution. The details of the algorithm are as follows:1.In DQPSO, each possible solutionXi=(xi1,xi2,xi3,…..,xiD)where D is the number of design variables is considered as a particle. The initial population of N particles (solutions) is generated randomly and it constitutes the swarm. The fitness of each of the solution is evaluated and the particles are then sorted in descending order according to their fitness.Divide the swarm into ‘M’ sub-swarms each holding ‘K’ particles such that N=K×M. The division is done in round robin fashion i.e.. the first particle is assigned to the first sub-swarm. Second one is assigned to the second sub-swarm, the Mth particle to the Mth sub-swarm and (M+1)th particle back to the first sub-swarm. This way of distributing particles to sub-swarms preserves diversity among frogs within each sub-swarm.Each sub-swarm works independently in achieving the goal of exploring the search space for optimum solution. The various steps involved in each of the sub-swarms of DQPSO algorithm are same as QPSO algorithm discussed earlier.After a user specified number (say ‘S’) of evolutions in each of the sub-swarm, the particles are regrouped and are sorted again in descending order according to their fitness.Repeat steps (2)–(4) till the convergence criteria are satisfied. With the randomly regrouping schedule, particles from different swarms are grouped in a new configuration so that each small swarms search space is enlarged and better solutions are possible to be found by the new small swarms. Fig. 3clearly depicts the proposed dynamic QPSO implementation.Numerical experiments have been carried out to test and verify the damage diagnostic technique using the proposed PCA based damage diagnostic algorithm, discussed in this paper. It is expected that the proposed algorithm will be more reliable in finding the near accurate damage distribution in the structure. In this chapter, we present three numerical examples. They are a simply supported laminated composite beam, cantilever plate and simply supported laminated composite plate.The first problem, chosen is a simply supported laminated composite beam shown in Fig. 4. For the purpose of numerical simulation studies, the beam is discretised into 20 elements as shown in Fig. 4. The laminate consists of[0/90]Splies and the material properties are given in Table 1. The beam is excited using a random dynamic loading which is stochastic in nature. The acceleration time history response is computed using finite element analysis with Newmark’s time marching scheme. The acceleration time history response, thus obtained is used to generate FRF matrices and then using these FRF matrices, the principal components are computed as outlined in the earlier sections. Hypothetical damage scenarios are assumed by introducing matrix cracking or delamination in different plies and thus degrading the stiffness of some elements. The damage test cases considered are as shown in Table 2. The sampling rate is considered as 4000 samples per second.One of the main issues related to structural damage diagnostic techniques, when applied to real situations, is their sensitivity to noise. In view of this, white Gaussian noise is added to the acceleration time history response generated by the finite element code, before they were processed. The Gaussian white noise is added in the form of SNR (signal-to-noise ratio) that defines the amplitude of the noise with respect to that of the clean signal and is given by(51)SNR=10log10σs2σn2dBwhereσn2andσs2denote the variance of the noise and the original signal, respectively. When the noise level is given by a particular value of SNR it means that a noisy signal with such an SNR has been added to the time series of each node. Moreover the noisy sequences affecting different nodes are uncorrelated, in this way severe experimental conditions were simulated.Four different sets of healthy data are generated. The operational variability is introduced by varying the loading and environmental variability is simulated by including measurement noise using SNR values as 40, 50 and 60 respectively. Similarly the damage data set of acceleration time history data of the beam are generated by considering the test cases given in Table 2.The damage index computed for healthy data sets and damaged data sets are given in Fig. 5. It can be easily verified from the details provided in Fig. 5 that the proposed algorithm clearly identify the damage from the environmental viabilities and further it can also identify the damages at any spatial locations and is also capable of identifying the multiple damages. Further smaller level of damages simulated (Table 2) is also distinctly identified by the proposed algorithm. The algorithm is also found to be less sensitive to measurement noise as all the data sets used in the present investigation are added with Gaussian white noise as indicated earlier.In order to investigate the magnitude of damage indices with respect to the extent of damage in the structure, the damage indices for varied levels of damage in the simply supported laminated composite beam are evaluated and shown in Fig. 6. To evaluate the damage indices for this case study, damage is simulated by reducing the stiffness of elements 9 and 10 by 20% and the time history data of the structure is generated. Later, in the similar fashion, the time history data for the beam is generated and damage indices are computed by reducing the damage in element 9 and 10 to 10%. Similarly, the third time history data set is generated by confining the damage to element 10 with 20%, to evaluate the damage indices. It is clear from Fig. 6, that damage index varies considerably with the extent of damage, but certainly cannot be used as a measure to identify the exact extent of damage from these plots. In view of this, it is proposed to use an optimization procedure for precise assessment of the extent of damage.The second numerical example considered is a T300/934 carbon/epoxy cantilever composite plate of 25mm long, 12.5mm wide, and 0.21mm thick. The lay-up consists of symmetric cross-ply arrangement, using 8 plies, that is,[0/90]2s. The plate is modeled using 200 layered elements. The plate is fixed at one end (cantilever condition) and excited at the tip by a random load. To provide sensor data necessary for the damage detection algorithm, accelerations from nodes on the top surface of the plate were recorded(Fig. 8a). These nodes are numbered from left to right and then bottom to top (sensor 1 is at bottom-left, sensor 20 is bottom-right, sensor 181 is top-left, and sensor 200 is top-right). Various test cases are considered by simulating damage in the form of matrix cracking and delamination as detailed in Table 3. Fig. 7shows the details of the damage index contour plots computed using the proposed scheme. It can be clearly observed from the plots given in Fig. 7, that the proposed SHM scheme is robust enough to identify the spatial damage in laminated composites.Figs. 8–10show the damage index contour plots obtained for various test cases with a limited number of sensors. The limited number of sensors is optimally located by using an optimal sensor placement algorithm discussed in this paper. Figs. 8–10 show the number of sensors placed, their optimal locations and the damage index contour plot for each test case. It can be observed from the plots given in Figs. 8–10 that the proposed SHM scheme is capable of identifying the spatial damage even with a limited number of sensors placed optimally as dictated by the proposed optimal sensor placement algorithm.The third numerical example considered is a laminated composite graphite/epoxy plate with simply supported end conditions. The lay-up consists of symmetric cross-ply arrangement, using 8 plies, that is,[0/90]2swith both the short ends are simply supported. The properties of laminated composite plies are given in Table 1. The plate is modeled with a mesh of size 20×10. These nodes are numbered from left to right and then bottom to top (sensor 1 is at the bottom-left, sensor 20 is bottom-right, sensor 181 is top-left, and sensor 200 is top-right). Various test cases with simulated damages considered are given in Table 4. Fig. 11shows the damage index contour plot for first two test cases with and without measurement noise. The damage index is normalized and shown in the plots. It can be easily observed from these figures that the spatial location of damage can be identified precisely even with measurement noise using the proposed algorithm. In order to study the effect of the proposed damage diagnostic scheme, with the limited number of sensors, the optimal sensor placement algorithm is employed to limit the number of sensors to 64 in the first instance, 48 in the second instance and then to 36 numbers. Fig. 12shows the optimal locations of the sensors dictated by the FRF based optimal sensor placement algorithm dealt with in this paper. The damage index contour plots with the limited number of sensors for different test cases are presented in Figs. 13–15. A close look at Figs. 13–15 clearly indicates that the exact spatial location of the damage can be identified using the damage index contour plots obtained even with limited measurements.The exact delamination length and the layers effected are identified by the inverse algorithm formulated as an optimization problem and solved using the proposed dynamic quantum PSO algorithm. The design variables considered are the IDEFs evaluated based on the details of matrix cracking and delamination length in both the directions in the elements identified as damaged using the damage index computed by PCA based algorithm. The results obtained are shown in Tables 5–7for all the three numerical examples considered. It can be observed from the details presented in Tables 5–7 that the delamination identification is quite robust with the proposed DQPSO algorithm.The convergence details of the proposed algorithm with varied swarm sizes are shown in Fig. 16. The convergence studies have been carried out for all the test cases considered in this paper. However the results of test case-1 are only presented here in Fig. 16. It can be observed from the plots shown in Fig. 16, that the performance of the proposed DQPSO algorithm improves with increase in the total swarm size and swarm size of 24 found to be appropriate for the problems solved in this paper.In order to investigate the comparative performance of the proposed DQPSO algorithm, the inverse problem associated with the delamination of laminate composites discussed in this paper is solved using hybrid adaptive PSO algorithm [54], which is optimal version of the basic PSO and Quantum PSO algorithm, discussed in the earlier section. For this purpose, we have considered the simply supported laminate composite plate problem with all test cases as described earlier. The convergence plot is drawn by comparing the variation of best objective value in each evolution (generation). The details of these convergence plots for test case-1 are shown in Fig. 17. Further, Fig. 17 clearly indicates that the proposed DQPSO algorithm converges faster in less number of evolutions and also solution obtained is optimal. From this convergence study, we can conclude that DQPSO algorithm can be considered to be the fastest among all the algorithms considered in this paper.

@&#CONCLUSIONS@&#
A new damage diagnostic algorithm combining principal component analysis with a newly developed quantum PSO based meta-heuristic algorithm is presented for health monitoring of laminated composite structures with limited measurements. The principal components are employed as dynamical invariants to filter out the influence of operational/environmental variations in the dynamic response of the structure. Finite element models of beam and plate are used to generate vibration data for healthy and damaged structures, simulated with varied levels of matrix cracking and delamination. White Gaussian noise with varied SNR values is added to the vibration data, to simulate the experimental measurements. Several damage instances have been investigated employing a simply supported laminated composite beam, cantilever carbon epoxy plate and graphite/epoxy simply supported beam with measurement noise. The effect of limited sensors is also investigated by employing an optimal sensor placement algorithm. Investigations presented in this paper clearly indicate that the proposed PCA based scheme can clearly indicate the spatial damage locations even with limited number of sensors and with measurement noise.The inverse problem associated with the identification of the extent of delamination and matrix cracking in the laminated composite elements is formulated as an optimization problem and solved using a newly proposed dynamic quantum PSO algorithm. Studies presented in this paper clearly indicate that the proposed algorithm identifies the extent of delamination and matrix cracking with fair accuracy. QPSO is a promising optimization algorithm, which is superior to the standard PSO. The particles of the swarm in QPSO converge quickly. However, the proposed quantum behaved PSO with dynamic interaction of multiple swarms works much better. The studies presented in this paper clearly indicate that the proposed dynamic QPSO algorithm has superior convergence characteristics.