@&#MAIN-TITLE@&#
A cloud computing based framework for general 2D and 3D cellular automata simulation

@&#HIGHLIGHTS@&#
A solution for traditional cellular automata simulation based on cloud computing.We create a framework capable of dealing with very large matrices.We build an API for high scalability, and simple to use by non-experts.Researchers need only to be concerned with their simulation algorithm.The solution can use 3D cellular automata for special simulations.

@&#KEYPHRASES@&#
Cloud computing,Cellular automata,Simulation,Sparse matrices,Game of life,Web services,

@&#ABSTRACT@&#
Cellular automata can be applied to solve several problems in a variety of areas, such as biology, chemistry, medicine, physics, astronomy, economics, and urban planning. The automata are defined by simple rules that give rise to behavior of great complexity running on very large matrices. 2D applications may require more than 106×106 matrix cells, which are usually beyond the computational capacity of local clusters of computers.This paper presents a solution for traditional cellular automata simulations. We propose a scalable software framework, based on cloud computing technology, which is capable of dealing with very large matrices. The use of the framework facilitates the instrumentation of simulation experiments by non-computer experts, as it removes the burden related to the configuration of MapReduce jobs, so that researchers need only be concerned with their simulation algorithms.

@&#INTRODUCTION@&#
Cellular automata can be applied to solve problems in a variety of areas, such as biology, chemistry [1], medicine [2], physics [3,4], astronomy [5], economics [6], and urban planning [7]. These automata are defined by simple rules that give rise to behavior of great complexity running on very large matrices. 2D applications may require more than 106×106 matrix cells, which are usually beyond the computational capacity of local clusters of computers.Moreover, it is a huge amount of work to create a system to subdivide simulations, and process and combine the results. Cellular automata researchers put a lot of effort into networks and parallel distributed computation issues to solve their cellular automata problems.This paper shows a solution for traditional cellular automata simulation based on cloud computing technology. The objective is to create a framework capable of dealing with very large matrices, highly scalable, and simple to use by non-experts so they need only be concerned with their algorithm, leaving the subdivision and parallelization processes to the framework.Cloud computing technology is an increasingly popular trend to create dynamically scalable solutions. Rajkumar et al. [8] present a set of benefits of cloud computing, but, for the purposes of the present framework, we put our focus on scalability.The rest of this paper is divided as follows. Section 2 describes the foundations of cellular automata theory upon which our results are based. Section 3 gives a brief introduction of cloud computing concepts. Section 4 describes the proposed framework. Section 5 presents a case study and simulation results. Section 6 concludes this work.A generic framework capable of running any type of cellular automaton is not possible, given the number of possible automata variations. Nevertheless, we propose a quite flexible framework that accommodates any cellular automaton that complies with the following definition:DefinitionA d-dimensional grid cellular automaton is represented by a 4-tupleCA=(Zd, Nr, S, g)where(i) Zdis the geometry of the discrete cellular space defined as a finite grid of d-tuples of integer numbers that represent cells z. In this cellular space, C(z) maps the cell z to its coordinates and ||…|| denotes the standard Euclidean norm. Thus ||C(w)−C(z)||=1 if the cells w and z are adjacent to each other. Zdis a finite grid of size n1×n2⋯×nd.(ii) Nris a subset of Zdthat defines the neighborhood of a cell z, such thatzεNrNr={q:‖C(q)−C(z)‖⩽h(r)}where r is the neighborhood radius, and h(r) determines the topology and size of the neighborhood, which do not change during the entire simulation time. The cardinality ||Nr|| is the size of the Neighborhood. Fig. 1 illustrates the most usual neighborhood types: von Neumann (a diamond-shaped neighborhood) and Moore (a square neighborhood).(iii) S is a non-empty set of possible cell states, where a cell state is defined by a tuple of p state variables s=(ν1,ν2,…,νp), which can be continuous or discrete-valued variables. In most of the cases there is only one state variable v1, and this variable has discrete values, such as v1 as a Boolean variable (0 or 1). Another common case is v1 as a 3-value variable (−1,0,1).(iv) g is a local state transition function to be applied synchronously to every cell z taking into account the states of all cells in the neighborhood of z, at a discrete instant of time t, and producing a new state for z at time t+1 (Fig. 2), that is,g:S‖Nr‖→SThis representation supports the evolution of the cellular automaton as a sequence of configurations〈tC,t+1C,t+2C,…,t+kC〉, where the configuration tc is the set of cell states s at a discrete instant of time t.According to the definition above, a cellular automaton is a discrete dynamical system, whose behavior is governed by the transition function g. Groups of functions that lead to similar dynamics determine a class of cellular automata (CA). There are different proposals for CA classification [9,10], but we focus our attention on the following three cases, which can occur after a finite number of steps k: (a) evolution leads to a stable configuration, (b) configuration evolves to a periodic configuration (e.g.t+kc=t+k+2c andt+k+1c=t+k+3c, which corresponds to a cyclic repetition of configurations with period=1), and (c) evolution leads to a chaotic pattern.The following cellular automata attributes are implied from the above definition:•finite regular grid: Zdis represented by a matrix n1×n2⋯×nd, and we use the indices as coordinates (easily calculating Euclidean distances). Our framework can be used to deal with pixels of an image, voxels from a 3D scan or sites on a chessboard. Our definition of CA does not consider geometry as being arbitrary graphs. The adoption of a finite grid requires that boundary conditions be established (see Section 2.3).constant neighborhood: The type and size of the neighborhood do not change amongst cells and during the CA evolution.deterministic: The CA starts from an initial configuration and proceeds deterministically by successive applications of a deterministic transition function.uniform: All cell states are updated by the same transition function. Some problems can only be solved by non-uniform CA in which each cell may have different transition functions [11].history independent: The state transition does not depend on the history of the state transitions at the nodes and/or at its neighbors. However, the present framework allows that the current time t be considered by the transition function g.synchronous: All cell states are updated simultaneously, that is, all cells are synchronized with a global clock and executed at the same time. This is a serious restriction, because many real system models require asynchronous agents. However, the theoretical foundations of asynchronous CA are not yet understood satisfactorily, and few works are available in the literature [9,12].Sometimes we identify the existence of a particular stateS¯∊S, called the quiescent state, such that if all neighbors of a cell z are quiescent, then the cell will also become quiescent. That is,g(s¯,…,s¯,sz,s¯,…,s¯)=s¯,where szis the state of the cell z before the application of g. The configuration in which every cell is in the quiescent state is called the quiescent configuration.The user of the present CA framework is totally responsible for the definition of the transition function. He/she should remember that the function g determines the behavior of the CA, especially those related to convergence and invertibility. In our framework, we are mainly concerned with convergent solutions (stable or periodic). The CA is considered reversible if we can recover a configuration at any past instant of time. If there is an inverse rule g−1 that drives the cellular automaton back to previous configurations, the user can inform the framework of this and proceed backwards (please note that reverting the direction requires starting a new simulation in the framework). Reversibility is a natural problem that arises in physical systems. However, for dimensionsd⩾2, there is no general way of deciding whether a given transition function is reversible or not. A discussion on the invertibility of d-dimensional cellular automata can be found in [13].Boundary conditions are necessary to either simulate infinite grids or to represent natural boundaries of a real system model. These conditions are incorporated into our framework by including extra layers of cells, with a width equal to the neighborhood radius r. In the present framework, boundary conditions can be fixed, periodic, or reflective. We detail each one below.In the fixed boundary condition, the boundary cells have neighbors in the extra layers. These cells have a pre-specified state value that does not change during the entire simulation (value w, depicted in Fig. 3a). This type of boundary condition corresponds to a Dirichlet boundary condition in partial differential equations. The fixed boundary condition is called a null boundary condition if the fixed state value is the quiescent state, i.e.w=s¯In the periodic boundary condition, extreme opposite cells become neighbors, like a fabric being folded in one dimension. In practice, we can do this by copying the extremity cells to the opposite boundary layer at each time step (Fig. 3b). In a 2D cellular automaton, if we do this in only one of the dimensions, we shall have the topology of a cylinder (as in the left grid in Fig. 3d). If the 2D grid is folded in two dimensions, we shall have the topology of a torus (as depicted in the right grid in Fig. 3d). Periodic boundaries are used as an approximation of infinite grids.In the reflective boundary condition, the extreme cells are mirrored at the boundary. We can do this by copying the extremity cells to their adjacent boundary layers (as shown in Fig. 3c). This type of boundary corresponds to a zero-flux boundary condition that constrains the solution of partial differential equations used in continuous diffusion models.Boundary conditions can be combined, such as the long channel depicted in Fig. 3d, with a periodic condition in one dimension and a reflective condition in the other.Cloud computing is a common expression to designate the delivery of computing as a utility [14–19]. It refers both to the applications delivered as services over the Internet and the hardware and systems software in the datacenters that provide such services. Its dominant business model is the pay-as-you-go, in which customers contract and pay for a plethora of stratified computing services provided by commercial cloud computing providers [20]. Examples of the last are Amazon Web Services (from EC2 hardware resources to high-level services) [21,22], Google App Engine [23], Relational Networks LongJump [24], Salesforce Force.com [25], and Microsoft Windows Azure [26].As pointed out by Armbrust et al. [14], Microsoft Azure is an intermediate point on the spectrum of flexibility versus programmer convenience which ranges from the straightforward use of hardware virtual machines (as EC2) to the application of fully functional frameworks such as Google App Engine. On-demand software services are also located on the extreme point of this spectrum, which characterizes the business model of Software-as-a-Service (SaaS). Azure supports general-purpose computing, rather than a single category of applications. These characteristics make Azure an adequate platform for the cellular automata framework proposed by this paper. However, this does not mean that Azure is the best platform for everything, some types of problems are more conveniently served by other cloud computing services, or even by a combination of them (such as App Engine hosted on top of EC2).Microsoft Azure belongs to a class of cloud computing frameworks commonly referred to as Platform-as-a-Service (PaaS), as opposed to Software-as-a-Service (SaaS). In Azure, it is possible to opt for specific programming languages as well as development environments (IDEs); however, users cannot control the underlying operating system. Azure uses the notion of “roles” as a model of computation and networking, and that of “blobs, queues, and tables” as a model of storage.A typical Azure application consists of one or more roles, each responsible for running one or more CPUs. There are only two types of built-in roles: Web Role or Worker Role. A Web Role is comparable to a Web application, whereas a Worker Role is a background service that runs some periodic, cyclical task. When creating a new role, users need to specify the number of CPU cores (which are defined by the size of the chosen Virtual Machine, e.g. size=medium equal to 2 cores at the time this article was written) and the number of CPUs (referred to as instances). Each instance of a worker role typically takes care of a combination of services and makes use of a large number of workers (i.e. processing units) to execute them. The proposed framework makes use of one Web Role, one Worker Role, and several of their instances.The Azure storage system, a combination of blobs, queues, and tables components, may be used to implement synchronization and communication between workers. Blob (binary large object) storage is a direct way of storing large binary data, such as the cellular automata matrix of our framework. Blobs can be mapped as a device and work as a folder.Queues store small job messages and are used as a communication mechanism between applications and services. Workers regularly ping the queues to acquire a message and run a job. Queues are designed to be reliable and persistent (e.g. an acquired message is sent back to the queue if a worker fails).Tables are used to store data that needs additional structure, but that do not require being kept in relational databases.Although we have considered a general formulation for cellular automata, the proposed framework is constrained by the following simplifications:•only 2D and 3D grids are allowed (i.e. 1D cellular automata are not implemented);parts of the code and storage schema are optimized for two-state bidimensional cellular automata, i.e. the cases in which s=(ν1), and v1 is a Boolean variable (0 or 1).The present CA framework is inspired on the Map and Reduce model presented by Dean and Ghemawat [27], in which the input matrix is partitioned into separate blocks that are subsequently distributed and processed in parallel by different units (mappers), and the results are then combined, or reduced, by a separate unit. The local nature of the CA transition rules lends itself well to the use of the Map and Reduce strategy.Our architecture, in particular, makes use of two types of units, also called Azure roles:•MapReduce Worker RoleWeb RoleThe MapReduce Worker Role provides three services:•Map ServiceReduce ServiceCleaning ServiceThe Web Role (1) implements the user interface, (2) provides a means of submitting simulation data, (3) tracks results, and (4) sets up the simulation in the cloud. Fig. 4illustrates the Web Role user interface for a 2D cellular automaton with 1012 cells. This Azure role also provides the necessary tooling to prepare the simulation matrix for transmission over the Internet. The MapReduce Worker Role implements the Map and Reduce strategy, as envisioned in [27]. The remainder of Section 4 details the main activities of the proposed framework and discusses its overall architecture.Fig. 5illustrates the data stream both in the local computer and the connection with the cloud. If the simulation matrix A is sparse, then A is transformed into a compressed sparse matrix to improve the transmission process. The process for compressing sparse matrices is described in Section 4.3. The final structure of A is serialized into a file called A.ca, which is submitted to a zip compression process and then transmitted to the cloud as the file A.zip.The user should also provide the following simulation parameters and program:•grid dimension d: 2D or 3Dsize of the simulation matrix: (n1, n2) or (n1, n2, n3)file containing the simulation matrix Anumber of instances (CPUs) ncpunumber of cores nc: 1, 2, 4, 6, or 8time steps to be stored during simulation in the format range1;range2;…, where rangeiis a single instant of time or a range tinitial−tfinal(e.g. 1;50;100–120)number of state variables ptype of neighborhood: von Neumann or Mooreneighborhood radius rquiescent stateS¯(default is 0)boundary condition: fixed, periodic, reflectivelocal state transition function g in the form of a text file containing a program written in a script language. The language is automatically detected (currently, Python and Lua only). The signature of g isThere are several static and dynamic storage schemes for sparse matrices [28,29], but only dynamic ones are suitable for cellular automata, because CA matrices do not comply with any specific, standard storage optimization scheme. Most of the dynamic storage schemes are variants of the orthogonal linked-list scheme proposed by Knuth in 1968 [30]. In this paper, instead of using arrays of integers to describe the locations of the non-zero elements in the original matrix, we use circular linked lists for both rows and columns. This is far from the efficiency of the most common storage schemes found in well-known math libraries (e.g. Intel Math Kernel Library [31]), but it is an adequate scheme for the proposed system, which deals with frequent modifications of cells and requires fast node to node access. Moreover, we are not concerned with the high time complexity to compress the original matrix (which can beO(n13)for an n1×n1 matrix). Indeed, we focus on space reduction rather than rewarding processing time, because the size of the simulation matrix is a serious limitation for communicating with the cloud.Fig. 6illustrates our storage scheme for a 2D cellular automaton, that is, a matrix with dimension d=2 (Fig. 6a). In this scheme, each dimension of the matrix is represented by a circular linked list of head control nodes (shaded nodes of Fig. 6b). Each non-null element in the matrix is a node with d pointers (Fig. 6c) to link with the next non-zero element (if there is no non-null element next to it, it points to itself).A sparse matrix is a matrix with a large number of zero elements that allows us to save time and/or memory by exploiting the zeros. Therefore, this definition depends not only on the matrix itself, but on the algorithm used on the matrix [32]. In the proposed framework, a matrix is considered sparse if the number of zeros effectively permits the reduction of storage space (considered later). Therefore, we compute the stored size of the matrix to decide whether it is worthwhile to compress the matrix. The stored size of a n1×n2×n3 matrix is n1×n2×n3×sizeof(data) and a sparse matrix with m non-null element needs, at most, 6×(n1+n2+n3+m)×sizeof(data) elements (because each data needs three pointers and three dimension indices). Thus6·(n1+n2+n3+m)≪n1·n2·n3m≪n1·n2·n36-(n1+n2+n3)In our system, if m<[((n1·n2·n3)/6)−(n1+n2+n3)] we shall compress the matrix, otherwise we shall store it in serial format. It is important to emphasize that sparse matrix compression is the final memory footprint to access matrix data, because Huffman’s techniques only reduce the size for storage and transmission; we still need to decompress the matrix to access the sparse matrix information.The serialization of a sparse matrix structure into a file needs to comply with the following requirements:•access to a specific cell without loading the whole structure.data coherence for cells in the same row.The data structure of a cell (defined in Section 4.4) uses three pointers to represent a 3D matrix, so, when we write it to a file, we need to change the pointer offset value to the file offset value. We create a table with the missing cell offsets; thus, when we write a cell and we cannot put the value for the row neighbor, we create a key with the neighbor cell index (x,y,z) and put the current cell offset as the value. If the key already exists, we append the current cell offset to the value. When we write a cell, we check if it is in the table key list, and if so, we iterate over the cells offset stored on key value, changing the missing cell offset value to the correct offset value, and clear the key value data. The data structure in the memory (in the compression process) has a field to mark if a cell is written and another field to store the offset value.This process produces a fast way to find cells, even on disk, so we do not need to load the matrix to produce a submatrix. We also implement the matrix operations for sparse matrices described in [33].The system subdivides the simulation matrix into N submatrices according to the number of instances (ncpu). Each submatrix K should be properly expanded (decompression) to allow for parallel processing of the N submatrices.Fig. 7illustrates the matrix subdivision process, in which we split the matrix into cubic cells based on the following equations:ncpu=truncn1n·n2n·n3nn=truncPow2n1·n2·n3ncpu3where trunc truncates its argument to an integer value, and truncPow2 truncates its argument to a value that is a power of 2.Using this approach we can explore the process locality, because each submatrix must be generated and sent to the worker, and this consumes a lot of time. So, we send the maximum possible size for each worker. The final submatrix needs to contain its neighborhood, so we expand it based on the neighborhood radius r; thus the submatrix size is a×b×c (Fig. 7), and normally a=b=c=(n+r). Another consideration is the boundary condition, which needs to be respected in the expansion.Due to the truncation process in calculating n, some small submatrices (with dimension not equal to n) appear (n1′,n2′,andn3′) in Fig. 7), which we call leaks. Each leak is processed when a worker completes its work with a regular submatrix. We are aware that cube submatrices are more suitable for parallel execution, mainly on GPUs.After we split and expand a submatrix, it receives a registration string [k1,k2,k3,a,b,c] that describes the submatrix (Fig. 7). The first three numbers of this string are the reference cell indices of the submatrix K (this cell is not the (0,0,0) cell in the submatrix due the neighborhood radius), and the last three numbers are the size of the expanded submatrix.The split process mentioned above does not require the whole matrix to be loaded; instead, we load the submatrices in blocks with common neighborhoods.The communication architecture of the proposed CA framework in the cloud is presented in Fig. 8. Furthermore, in Fig. 8 we propose a special graphical notation, together with specially designed components, to represent roles, services, workers, storage models, and communication actions.The proposed graphical notation has the following characteristics:•A role is represented by a rectangle with an outer shadow, a worker by a rectangle, a queue by an arrow with message compartments (and the first message on the arrowhead), a table by a small empty table, and a blob by a cylinder.A service is indicated by a closed dashed line forming a rectangular shape.Communication actions are indicated by a dashed arrow for control messages, a double arrow for data streams, and a single arrow for creation/access/update processes.The Web Role performs the following tasks:•Creates the Azure tableCaJobTableand inserts the first line of information in it.Creates the Azure queueCaJobQueueand continues inserting job messages in it, like the first job to be done or a cancelation order.Creates a blob containing the file of the compressed simulation matrix A (Initial Configuration Blob) and names it with the MD5 [34] hash value of the file (〈md5〉Initial).TheCaJobTablehas the following information:•PK: “partition key” generated by a MD5 hash function of the current simulation file. PK determines the physical location (server) of the table line.RK: “row key” randomly generated as a GUID (globally unique identifier), that is, a unique reference string (a 32-character hexadecimal number) that identifies the line. Lines with the same “partition key” are stored in the same physical location.Steps: string with the time steps to be stored during simulation (see Section 4.2).StepsDone: string with progress information about each step. We use a concatenation of “[StepID|Progress];”, where StepID is the iterative step index, and Progress is a float point value of the step progress in%: [0,100].InstanceCount: number of instances used.Rule: string with the script code of the local state transition function g (see Section 4.2).Status: information about job status (Not Started, Started, Processing, Done). A job is in Started state when it starts the matrix subdivision step, and it is in Processing state when it is processing a cellular automata rule.The first job to be done is inserted in theCaJobQueueas the following string: PK + ”;” + RKThe Base64 encoding of this string is sent to the user as a key for tracking the simulation process.The MapReduce Worker Role performs the following tasks (Fig. 8):•creates the Azure queueCaTaskQueueafter communicating with the Web Role.creates two blobs: a blob for the map results (called 〈md5〉Maps), which contains files with transformed submatrices, and a blob for the reduce result (called 〈md5〉Out). These blobs are mapped as a device in the workers to simplify the implementation and accelerate I/O tasks.creates the Azure queueCaCleanQueue, which is used to clean the Map Results Blob.updates the Azure tableCaJobTable.continues communicating with the Web Role throughCaJobQueueandCaJobTable.The MapReduce Worker Role acquires the first job message and creates theCaTaskQueuewith a message for each split (i.e. each submatrix) of the simulation matrix A. The partition process that generates each submatrix is described in Section 4.3. As a Map Service, workers acquire the task messages, perform a state transition for each submatrix, and store the results in 〈md5〉Maps.The Reduce Worker combines all submatrices and creates a final file with the compressed sparse matrix on 〈md5〉Out folder, when all submatrices have been processed and stored. After this, it puts a message inCaCleanQueue. The workers of the Cleaning Service acquire messages fromCaCleanQueueand clean simulation temporary data, such as old submatrices files and temporary files.The MapReduce Worker Role puts a message inCaTaskQueuefor each generated submatrix with the following string:PK+”;”+RK+”;”STEP+”;”+I+”;”+J+”;”+K+”;”+W+”;”+H+”;”+Lwhere I, J, K, W, H, and L are the elements of the submatrix registration string (described in Section 4.4), and STEP is the simulation step id. Map Workers wait for messages in theCaTaskQueueand, when they pop a valid message, they start the state transition for each cell in the submatrix.The process starts decoding the message to load the submatrix. The sparse matrix file is available for all workers and can only be loaded as read-only, so we can have lot of workers accessing it without any problem. Each worker allocates enough memory to store the submatrix and starts to populate it loading each cell from the file in the compressed format. The load request process is an asynchronous call, which helps the loader on the server and increases the cache performance (what is done by the operation system). Each worker has more than one core, so even the load process is parallelized at the CPU level. Therefore each core loads a block in the submatrix. It is important to emphasize that the inter-block linking is thread-safe, and it is done after all cells are loaded.After loading the submatrix, the worker performs iterations over every cell, discarding neighborhood cells. The resulting matrix is temporarily stored as a linked list, with non-null cells, after simulation. That greatly simplifies the process of recreating a compressed sparse matrix. The simulation algorithm is presented in Fig. 9.In the code of Fig. 9, the variables row_0, col_0 and dep_0 are the indices of the first submatrix cell (considering the radius r), so we need to translate it to the first cell that needs to be simulated. Moreover, #row, #col, #depth are the simulation matrix size. Similarly, we decrease the size by r to get to the simulation matrix size. After we iterate over cells, we split the simulation amongst ncpuprocessing units (CPUs).Each cell creates a neighborhood matrix Nr. So, given a cell to be simulated, getMatrix creates a matrix centered in (i1,i2,i3) with radius r, based on the compressed submatrix mo. Finally, we call the transition function g, which is the code given by the user, and test if the transition produces a non-null cell. If it does produce a valid cell, we store it in the cells list. The cells list is a thread-safe list.After processing the entire matrix, we compress the list using a zip compression algorithm and save it in the temporary blob 〈md5〉Maps. We do not need to create a compressed sparse matrix, because it is a temporary result. The file name is the string 〈md5〉−〈time〉−〈submatrix reference string〉, where 〈time〉 is the simulation step.The reduction process starts after all submatrices have been processed and stored. This process uses asynchronous events for folder file changes on the Reduce Worker. When this worker starts to reduce, it combines the submatrices into a compressed sparse matrix structure. Each submatrix file is a zip compressed list of non-null cells. Thus, for each file, we sequentially insert the non-null cells into the compressed sparse matrix structure.The system can operate in two ways. A one level reduction consists of a service that processes all non-null cells and generates the final matrix file. The other way is the multi-level reduction (Fig. 10), in which we start n role services, which reduce a submatrix and create a temporary sparse matrix file. In this second way, we start n/2 role services to combine the submatrices and generate the next step. We continuously repeat this process until we have only one thread that finishes the submatrices bindings. Each reduction role service has an ID when it finishes the processing step. A reduction role service stays in an active state only if its ID belongs to a valid ID sequence, which is only if ID<Step Number/2. This process needs log2(n) steps to complete, but if the matrix is really large we can take advantage of parallel processing.Finally, we save the matrix on 〈md5〉Out folder and put a message inCaCleanQueuewith the string 〈md5〉−〈time〉. The MapReduce Worker starts to split the next simulation step when an asynchronous event detects the new file on 〈md5〉Out folder.Each file stored in a cloud server requires space and, therefore, costs money. It is desirable, therefore, to reduce the overall number of files, especially when they are used for storing intermediate results. To this end, we have devised a specific role service to clean temporary files. The cleaning process gets messages fromCaCleanQueue, decodes them, and starts removing temporary files.Even files that contain simulation results may be removed if they are not in the Steps list, but again, they can only be removed if the step result is not necessary for the final reduce step.Sometimes we notice that something is wrong with the simulation and we want to cancel it. However, a cancelation process requires that all involved workers be informed. In the proposed architecture, the user can cancel a job using the Web Role interface, forcing the Web Role to send a message toCaJobQueue, with a high priority flag assigned to the following message string: (PK + ”;” + RK + ”; CANCEL”). Due to its high priority, this message will be the next one processed.When the MapReduce Worker decodes the message, it puts the same message inCaTaskQueue, with the high priority flag set. This will force the workers to pop the cancelation message next time they try to get a message from the queue. This procedure works for the first worker only, because it is the only one that knows about the cancelation of the Job. Thus, the Map Workers must be capable of communicating amongst themselves.The Map Workers can communicate with others workers using MPI (Message Passing Interface) [35] or WCF (Windows Communication Foundation) [36] contracts. In this way, when a worker decodes a cancelation message, it sends a broadcast message to the other workers, the Job is stopped, and the Cleaning Service starts to clean the Jobs Files. Azure allows the creation of Internal Endpoints to inter-role communication, and we can select the protocol and the port number. We only use asynchronous events to listen on the port.As mentioned above, we tested the 2D case extensively. Moreover, we focused on cases that demanded the manipulation of very large matrices (106×106 matrix cells), which require both robust and efficient strategies for storage and processing. Our goal is to investigate the limits of performance when doing massive simulations. The case we describe here is an analysis of the Game of Life, a two-state bidimensional cellular automata that can simulate several phenomena. It is a very popular cellular automaton, and it is very simple to implement and understand.The Game of Life is based on the following four simple rules:•Any live cell with less than two live neighbors dies due to loneliness.Any live cell with more than three live neighbors dies due to overpopulation.Any cell with exactly three live neighbors comes alive.Any cell with two live neighbors stays in the same state for the next generation.We create an initial 106×106 matrix (1012cells) by writing a script that randomly generates a matrix of zeros and ones. The script creates a text file with each cell represented by a 0 or 1 (for instance, a 3×3 identity matrix is the string 100010001). Also, we can define the live probability of the matrix (that is, the probability that a cell is alive). Using the conversion tool integrated to the web role, we converted the matrix and uploaded it into the cloud.The tests were designed with the following objectives in mind: to determine how fast the speedup of instance numbers is, to measure the influence of the number of cores per instance, and to evaluate the overall effectiveness of the proposed solution. In particular, we would like to know the value of instance numbers from which speedup does not significantly increase due to the Map and Reduce process, and node intercommunication. Another point of interest was to estimate how effective the sparse matrix compression actually was. With this last goal in mind, we created a series of initial state matrices (100 for each pair size and life probability) with varying life probability values, and compressed each one of them to evaluate the effectiveness of the compression process.We created a series of simulations, changing the number of instances and the number of cores, and we collected time information for the Map, Simulation, and Reduce processes. Also, we registered the amount of used memory and space. Furthermore, we measured the time to transfer files to the Initial Blob.We used the following test sequence:•sparse compression efficiencyfile transfer time over the Internetspeed up analysis over instances number×cores numberinternal processing time.The objective of testing the compression scheme of the sparse matrix was to demonstrate the advantage of focusing on space reduction rather than on processing time. Another point of interest was the time to transfer the file over the Internet. Internally, cloud servers typically use a 10 GBits Ethernet protocol; so, even for internal communications, we needed to reduce transfer sizes. In the compression tests, the script runs a sequence of initial states by changing the matrix life probability and the matrix size, and submits the sequence to the compression algorithm. Each test runs ten times, and the mean value was registered. The results are summarized in Fig. 11and Table 1.Fig. 11 shows that the compression algorithm works correctly, because the sequence converges to the compression ratio limit, which is given by limsize→∞ratio=pLife. The ratio increases depending on the matrix size, but it is limited by pLife.We tested the file transfer time by sending zip files over five scenarios:•Ethernet 1GbpsEthernet 10GbpsInternet Link 100MbpsInternet Link 10MbpsInternet Link 1MbpsThe first two options represent the internal transfer inside a cluster. Azure uses Ethernet 10Gbps, but most local clusters use Ethernet 1Gbps. The last three options are Internet links. Fig. 12shows the simulation results for each option. The horizontal header is the average size of the matrix, after all compressions, and for a 25% of life probability.The results in Table 2show that it is very important to reduce the file size before sending it over the Internet. Giant matrices, e.g. 106×106, take at least 1h to upload. Due to the excessive time required to upload test files using 1Mbps Internet Link, we canceled the process after one day of transfer time. The internal transfer works well at high speed connections; therefore, the time used for internal transfer, in most cases, is not a serious problem, because we transfer only small submatrices blocks between nodes. Another observation is that the Azure storage system can work with a 10Gbps transfer rate, which is important because most storage systems use SAS connections that are limited to 6Gbps.Once we have the initial state file on the server, we can start the simulation process. After uploading the 106×106 matrix, we can test the combination of number of instances and number of cores per instance.The Map process is a very fast procedure, because it only needs to know the matrix size and the number of CPUs to split and create the messages to push on the task queue. During the Map processing tests, we measured the mean time to split and push messages. Fig. 12 shows this mean time by varying the number of CPU instances.In these tests we are not changing the number of Map workers, but only varying the number of instances necessary to subdivide and create a message. After the submatrices were mapped for each test, we collected the processing times for the process state (Table 3).The process state needs to build the submatrix and process each individual cell. Thus, it starts reading the matrix file on storage and creating the sparse matrix on local memory. This procedure is asynchronous and parallel, so each thread builds a subblock in the submatrix and combines it with the full matrix structure. This procedure is thread-safe blocking. The reading operation is made in a mapped local disk, so the operational system combines and handles the requests. In a high-speed network, it is very important to create a request buffer to reduce the request latency.The results show that the time taken to produce the results is a question of how many cores we select to run, and that they range from one to ten days to process. We spent, on average, 10min to start up the machines. The subdivision process was very efficient, and the file structure used to load the matrix was a very fast way of rebuilding the matrix without loading large bounds of memory. Also, the storage speed and network infrastructure behavior proved appropriate for this type of use.Finally, we needed to reduce the matrix. This step can be done in two ways: one-level reduce or multi-level reduction.The one-level reduction is a simple way to reduce something, in which we use a unique machine to load the cells list and build the full matrix. It is very efficient if the matrix is small, which was not the case. We tested the processing time to reduce the matrix using only one reduce worker and varying the number of cores. Fig. 13shows the results.Obviously, the time taken with one worker and one core is very large, because it needs to load the files and process each non-null cell. Thus, this method can only be used in the case of small matrices. For large matrices we must use multi-level reduction.We tested the multi-level reduction varying the number of workers from 16 to two, by sequences of power of two. Fig. 14shows the results.For big matrices, large numbers like 16 or eight workers reduce time more efficiently, but this is not true for small matrices because there are overheads related to the mapping and reducing stages of the proposed process. The results show that the split process is very fast, because it is a process that is constrained to a specific region in the map. Thus, a worker loads only the files it needs to process and then creates the next integration file to bind the sparse matrices.

@&#CONCLUSIONS@&#
