@&#MAIN-TITLE@&#
A prediction model of drug-induced ototoxicity developed by an optimal support vector machine (SVM) method

@&#HIGHLIGHTS@&#
A classification model of drug-induced ototoxicity is established by GA-CG-SVM.The naïve Bayesian method is used to develop prediction models of ototoxicity.The RP method is used to develop prediction models of ototoxicity.The established GA-CG-SVM model II outperforms models developed by other methods.

@&#KEYPHRASES@&#
Drug-induced ototoxicity,Support vector machine,Naïve Bayesian,Recursive partitioning,Classification,

@&#ABSTRACT@&#
Drug-induced ototoxicity, as a toxic side effect, is an important issue needed to be considered in drug discovery. Nevertheless, current experimental methods used to evaluate drug-induced ototoxicity are often time-consuming and expensive, indicating that they are not suitable for a large-scale evaluation of drug-induced ototoxicity in the early stage of drug discovery. We thus, in this investigation, established an effective computational prediction model of drug-induced ototoxicity using an optimal support vector machine (SVM) method, GA-CG-SVM. Three GA-CG-SVM models were developed based on three training sets containing agents bearing different risk levels of drug-induced ototoxicity. For comparison, models based on naïve Bayesian (NB) and recursive partitioning (RP) methods were also used on the same training sets. Among all the prediction models, the GA-CG-SVM model II showed the best performance, which offered prediction accuracies of 85.33% and 83.05% for two independent test sets, respectively. Overall, the good performance of the GA-CG-SVM model II indicates that it could be used for the prediction of drug-induced ototoxicity in the early stage of drug discovery.

@&#INTRODUCTION@&#
Clinical studies have shown that some drugs, such as amikacin, gentamicin and neomycin, could lead damage to the ear, specifically the cochlea or auditory nerve and sometimes the vestibular system [1–3]. Symptoms of the drug-induced ototoxicity vary considerably from drug to drug and person to person, ranging from mild imbalance to total incapacitation, and tinnitus to total hearing loss. The drug-induced ototoxicity may be either reversible and temporary, or irreversible and permanent. The bad thing is that currently there is no effective treatment to reverse the effects of ototoxicity if permanent damage happens to the ear [1,2]. Therefore, drug-induced ototoxicity, like other drug toxicities, is also an important issue needed to be considered in drug discovery.Currently, several methods based on chemical biology have been used to evaluate drug-induced ototoxicity [4]. However, these methods require a number of experiments, most of which are time-consuming and expensive, indicating that they are not suitable for a large-scale evaluation of drug-induced ototoxicity in the early stage of drug discovery. Lately emerging zebrafish-based methods seemed faster and cheaper [5,6]. However, due to the relatively large species difference between zebrafish and humans, these methods are prone to misjudge the drug-induced ototoxicity [4,7]. Therefore, it is highly needed to develop more efficient and fast methods for a large-scale evaluation of drug-induced ototoxicity in drug discovery.Computational methods have been thought as a faster and cheaper strategy and have been successfully used in the prediction of various pharmacokinetic and toxic properties of drugs. For example, Wang et al. developed prediction models of human ether-a-go-go related gene (hERG) potassium channel blockage using the naïve Bayesian (NB) and recursive partitioning (RP) methods [8]. Burton et al. used the RP method to develop prediction models of Cytochromes P450 2D6 and 1A2 inhibitors [9]. Ma et al. developed a prediction model of drug oral bioavailability by the support vector machine (SVM) method [10]. More computational prediction models of pharmacokinetic and toxic properties could be found in the literature [11–14]. However, as far as we know, there is no report of computational model for the prediction of drug-induced ototoxicity. Therefore, we shall, in this investigation, develop a computational classification prediction model of drug-induced ototoxicity using the SVM method [15,16]. Here we chose the SVM method because SVM has been demonstrated to be one of the best statistical learning methods and has shown better performances in the development of prediction models of pharmacokinetic and toxic properties than many other methods [17–21]. Specifically, we used in this study a modified version of SVM, namely GA-CG-SVM [22], in which the genetic algorithm (GA) [23]is used for the feature selection and the conjugate gradient (CG) [24] method for the parameter optimization. A main advantage of GA-CG-SVM is that it can concurrently optimize the features and SVM parameters. GA-CG-SVM has been demonstrated to outperform the common SVM method [10,17]. For comparison, NB and RP methods will also be used to build prediction models of drug-induced ototoxicity; the two methods were chosen since they are also very good and popular methods for classification modeling.SVM is a supervised machine learning method, which has been widely used for classification and regression analysis. It has shown a good performance in solving a number of biological classification problems [25–27]. Nevertheless, some issues that may have important influence on the quality of established models are often not or insufficiently considered in SVM modeling, such as feature selection and parameter optimization. Our group recently developed an optimal SVM method, termed GA-CG-SVM, in which feature selection and parameter optimization are efficiently handled in SVM modeling. Detailed algorithms for GA-CG-SVM have already been described in a previous paper [22]. Here we just make a brief summary for the basic idea of GA-CG-SVM.Supposing a given training set{(x1,y1),(x2,y2),...(xi,yi)}wherexirepresents a vector of n real numbers (features or descriptors) andyiis the class that vectorxibelongs to. The purpose of SVM classification is to find an optimal hyperplane to separate these classes with the maximum margin, which needs to solve the following optimization problem:(1)Maxw,b2||w||subjecttoyi(wxi+b)−1≥0where2/||w||is the margin.For linearly separable cases, it is easy to find an optimal separating hyperplane by a classifying determination function. For linearly non-separable cases, there is no hyperplane that can be used to perfectly separate two sets of points (See Supplementary Figure S1). Therefore, non-negative slack variablesξi≥0,i=1,....,mwere introduced. The equation to be solved becomes:(2)Maxw,b2||w||+C∑i=1mξisubjecttoyi(wxi+b)−1+ξi≥0where C is a user predetermined penalty parameter.For nonlinear (non-) separable cases, the basic idea is to project the input data setxiinto a high-dimensional feature space via a nonlinear manner using a kernel function [28]. Until now, many kernel functions have been suggested for this purpose. Among them, the radial basis function (RBF) is widely used and performed very well in most cases [29]. Thus, the RBF kernel function was also selected in this study.(3)k(xi,xj)=exp(−γ||xi−xj||2)where the parameterγdenotes the width of Gaussian kernel.As mentioned previously, the selection of penalty parameter C and kernel parameterγin SVM modeling has significant influence on the predictive accuracy of an SVM model. Thus, we used the conjugate gradient method to optimize the two parameters. Additionally, the selection of features is also of importance to the prediction ability of an SVM model [30]. The GA is a very popular optimization algorithm, which is based on the Darwinian evolutionary idea of natural selection and genetics in biological systems. GA has been widely used to solve a range of diverse problems such as data mining and optimization [31,32]. Here, we applied GA for the feature selection in SVM modeling. Finally, it is worth mentioning that an integrated scheme for the simultaneous treatment of both feature selection (by GA) and parameter optimization (by CG) was adopted. This is important since it has been shown that the feature selection and parameter setting influence each other in SVM modeling [33].A naive Bayesian classifier is a simple probabilistic classifier based on applying Bayes׳ theorem with strong (naive) independence assumptions [34]. In NB, each object is described by an n-dimensional vectorF=(f1,f2,....,fn),where(f1,f2,....,fn)represents n features. Objects belonging to the first class are each labeled a value ofCL1=1, while those in the second class are assignedCL2=−1. Based on the Bayes׳ theorem, we got(4)p(CLi|F)=p(F|CLi)p(CLi)p(F)wherep(CLi|F)denotes the posterior probability, andp(CLi)represents the prior probability.p(F|CLi)andp(F)are conditional probability and marginal probability, respectively.p(F)is constant for all classes.p(F|CLi)andp(F)can be learned from a training set.In this study, NB was used for the development of prediction models of drug-induced ototoxicity.CL1andCL2represent ototoxic drug class and non-ototoxic drug class, respectively. The naive Bayesian classifier was constructed using the Discovery Studio 3.1 software package.RP is a statistical method for multivariable analysis, which is also a popular classification method [35,36]. RP produces a decision tree that strives to correctly classify members of the population based on several dichotomous dependent variables. At each node of the decision tree, the data are split into two subsets based on a particular descriptor and corresponding splitting value, which are decided by an automated statistical analysis of the entire data set. The splitting process continues until no more significant nodes are obtained or a threshold value for stopping is reached. Let X stand for a set of independent properties (molecular properties) and Y represent a dependent property (ototoxicity class). With RP method, created decision trees can recursively partition data according to the relationship between X and Y values. In this study, 5-fold cross validation was used, splits were scored using Gini index and the minimum number of samples at each node was set to 10 to avoid excessive partitioning. Furthermore, the maximum tree depth was changed from 2 to 10 systematically in order to find a better RP model. Our recursive partitioning classifiers were built using the Discovery Studio 3.1 software package.A total of 572 small molecule compounds (positive) that have been reported to bear ototoxicity were collected from reference [37], which contains the most updated collections of ototoxic drugs. 347 drug molecules (negative) for which there is no report to bear ototoxicity were collected from DrugBank [38]. We used the “Generate Training and Test Data” module in Discovery Studio to randomly select 20% compounds (121 positives and 63 negatives) to form an independent test set (called TS1) in advance. The remaining positives and negatives were taken to construct three training sets, namely dataset I, II and III; these compounds could occur in different sets. The biggest difference of these datasets is the risk or strength of ototoxicity of positive compounds. According to Bauman, the ototoxic drugs were divided into five classes (class 1, 2, 3, 4 and 5) based on their risk or strength of ototoxicity [37]. Compounds in class 1 have the lowest risk; compounds in class 5 have the highest risk; compounds in class 2–4 have an increasing risk. Dataset I includes positive compounds of all of the risk classes (class 1, 2, 3, 4 and 5). Dataset II contains positive compounds of risk class 2, 3, 4 and 5. Dataset III includes positive compounds of higher risk classes (class 3, 4 and 5). The finally formed dataset I consists of 451 positives and 284 negatives (see Supplementary Table S1). Dataset II contains 252 positives and 284 negatives (see Supplementary Table S2). Dataset III includes 64 positives and 284 negatives (see Supplementary Table S3). Test set TS1 (121 positives and 63 negatives) is presented in Supplementary Table S4.To further evaluate the established models, we constructed another independent test set, called TS2. TS2 contains 19 positives and 40 negatives (see Supplementary Table S5). Positives in TS2 were collected from Hazardous Substances Data Bank (HSDB) of TOXNET toxicology data network [39]. Negatives in TS2 were again collected from DrugBank [37]. There is no overlap between TS2 and any one of training set I, II, III and TS1.Molecular descriptors used in this study were calculated by Discovery Studio 3.1 software package. Initially, a total of 237 molecular descriptors for each molecule were calculated; these descriptors cover a variety of molecular properties, including topological descriptors, element counts, AlogP, surface area and volume, and so on. After that, a preprocessing procedure was conducted to these calculated descriptors. The preprocessing procedure includes two steps. In the first step, some “bad” descriptors were removed. For example, descriptors with too many zero values, bearing a very small standard deviation value with others (<0.5%), or having a high correlation coefficient with others (>95%), were deleted from the descriptor list. In the second step, all the descriptor values for the remaining descriptors were scaled to a range of −1 to 1. These preprocessed descriptors were further optimized by GA (for GA-CG-SVM) or a Monte Carlo (MC) method (for NB and RP), a feature selection method similar to that used in reference [40]. Finally, these selected descriptors were used for the development of prediction models of drug-induced ototoxicity.To assess the performance of the established prediction models, the following quantities were calculated: true positives (TP, true ototoxic drugs), true negatives (TN, true non-ototoxic drugs), false positives (FP, false ototoxic drugs) and false negatives (FN, false non-ototoxic drugs). Sensitivity SE=TP/(TP+FN) and specificity SP=TN/(TN+FP) are the prediction accuracy for ototoxic drugs and non-ototoxic drugs, respectively. The overall accuracy (Q) was calculated by the equation: Q=(TP+TN)/(TP+TN+FP+FN). The Matthew׳s correlation (MCC) was calculated by the following equation:MCC=(TP×TN−FN×FP)/(TP+FN)(TP+FP)(TN+FN)(TN+FP)

@&#CONCLUSIONS@&#
