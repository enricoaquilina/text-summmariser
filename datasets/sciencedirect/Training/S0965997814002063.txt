@&#MAIN-TITLE@&#
Hybrid MPI/OpenMP cloud parallelization of harmonic coupled finite strip method applied on reinforced concrete prismatic shell structure

@&#HIGHLIGHTS@&#
Application of hybrid parallelization model coupling MPI with OpenMP.Rational usage of cloud computer resources in hybrid parallelization.Analysis of impact of cluster nodes and CPU cores number to parallelization effects.Harmonic coupled finite strip method application on shell structures.Method demonstrated on reinforced concrete prismatic shell structure.

@&#KEYPHRASES@&#
Cloud computing,MPI,OpenMP,Harmonic coupled finite strip method,Shell prismatic structures,Reinforced concrete,

@&#ABSTRACT@&#
This paper discusses the cloud computing based approach for parallelization of large displacement stability analysis of orthotropic prismatic shell structures with simply supported boundary conditions along the diaphragm-supported edges. We review the harmonic coupled finite strip method (HCFSM), and describe a software system for nonlinear analysis of reinforced concrete (RC) structures. We combine different parallelization models – MPI and OpenMP – in order to cope with the increased computational complexity, which originates from coupling of all series terms in the HCFSM formulation. We discuss the effects of parallelization from the perspective of a cloud environment. Our results show that rational usage of cloud resources can lead to significant performance improvements and monetary savings. In certain cases, the achieved performance can be very close to the maximum one.

@&#INTRODUCTION@&#
The conventional finite strip method (FSM) is based on the harmonic functions, and proved to be efficient tool for analyzing a great deal of structures for which both geometry and material properties can be considered as constants along a main direction, straight or curved, while only the loading distribution may vary. This method was pioneered by Cheung, see [1].In this work we present the semi-analytical harmonic coupled finite strip method (HCFSM) for geometric non-linear analysis of reinforced concrete plate structures under multiple loading conditions. This method takes into account the important influence of the interaction between the buckling modes [2]. In contrast, the FSM, in its usual form, ignores this interaction and therefore cannot be used in the structure stability analysis. The risk of structural instability is analyzed as realistically as possible under the influence of realistic composite material behavior.Coupling of all series terms in the HCFSM formulation increases the computation complexity, which depends on the number of used harmonics. To make this method feasible in practice, we combine different parallelization models so as to speed up the calculation time. The paper by Nikolic et al. [3] explores the effects of combining the message-passing and shared memory programming models – MPI and OpenMP – in analysis of the large deflection and post-buckling problems. We build upon this work and analyze such hybrid parallelization in a cloud computing environment. We discuss the impact of virtualization and how to rationally use cloud computing resources to achieve as good as possible price-performance ratio. We derive an upped bound on the maximal speedup that could be achieved when both parallelization strategies are simultaneously applied. This paper is based upon Hajdukovic et al. [15], but the current paper considers parallelization of an additional step in the HCFSM formulation, namely solving the system of stability equations. With this addition, we have achieved to have an almost completely parallel HCFSM workflow, with a negligible portion of the serial code. This new parallel step, however, brings into play previously unimportant aspects of parallelization – like collocation of virtual nodes or their logical organization into a processing grid – which significantly affect the communication overhead, and thus the overall running time.In this paper we focused our attention to cloud computing environment, but we believe that same approach is possible in the grid computing infrastructure [16,17] with necessary changes caused by different patterns of its resource usage.The aim of this paper is also based on the rheological–dynamical limit analysis to predict the ultimate resistance of structure. On the basis of the RDA, the working diagrams of concrete and steel are built, representing simultaneous stress–strain pairs. The RDA working diagrams are then compared with recommended diagrams from the Eurocode 2 (EC 2). The comparison of the achieved ultimate resistance of characteristic cross sections of analyzed reinforced concrete folded plate structure, according to FSM, linear HCFSM, and nonlinear HCFSM is presented on the diagrams of interaction (Nu–Mu), drawn according to Eurocode 2 and RDA.In the FSM, which combines elements of the classical Ritz and the finite-element methods, the general form of the displacement function can be written as a product of polynomials and trigonometric functions(1)f=∑m=1rYm(y)∑k=1cNk(x)qkmwhere Ym(y) are functions from the Ritz method and Nk(x) are interpolation functions from the finite-element method. We define the local Degrees Of Freedom (DOFs) as the displacements and rotation of a nodal line (DOFs=4). The DOFs are also called generalized coordinates.The nonlinear strain–displacement relations in the finite strip can be predicted by the combination of the plane elasticity and the Kirchhoff plate theory. Using this assumption in the Green–Lagrange strain tensor (2) for in-plane nonlinear strains gives Green–Lagrange HCFSM formulation. Also that, neglecting lower-order terms in a manner consistent with the usual von Karman assumptions gives HCFSM von Karman formulation.(2)εij=1/2(ui,j+uj,i+uk,iuk,j)The essential feature of geometric nonlinearity is that equilibrium equations must be written with respect to the deformed geometry – which is not known in advance. As a preliminary to tracing the equilibrium paths, it is necessary to determine the total potential energy of the structure as a function of the global DOFs. The steps in the computation are detailed discussed in [2].The total potential energy of a strip is designated Π and is expressed with respect to the local DOFs by the HCFSM.(3)Π=U+W=(Um+Ub)+W=1/2∫AquTBu1TD11Bu1qudA+1/2∫AqwTBw3TD22Bw3qwdA+1/8∫AqwTBw2TWTBw1TD11Bw1WBw2qwdA+1/4∫AqwTBw2TWTBw1TD11Bu1qudA+1/4∫AquTBu1TD11Bw1WBw2qwdA+1/8∫AquuTBu2uTUTBu1uTD11Bu1uUBu2uquudA+1/4∫AquuTBu4uTD11Bu1uUBu2uquudA+1/4∫AquuTBu2uTUTBu1uTD11Bu4uquudA+1/4∫AquvTBu5vTD11Bu1uUBu2uquudA+1/4∫AquuTBu2uTUTBu1uTD11Bu5vquvdA+1/8∫AqwTBw2TWTBw1TD11Bu1uUBu2uquudA+1/8∫AquuTBu2uTUTBu1uTD11Bw1WBw2qwdA-∫AqTATpdAThe multiplication results of the membrane and bending actions in the first bracket of Eq. (3) are uniquely defined and uncoupled, whilst those in second [von Karman assumptions] and third bracket {Green–Lagrange approach} are functions of the displacements u0, v0 and w. Consequently, the membrane and bending actions are coupled in many ways.Since the principle of the stationary potential energy states that the necessary condition of the equilibrium of any given state is that the variation of the total potential energy of the considered structure is equal to zero, we have the following relation:(4)δΠ=0Eq. (4) is satisfied for an arbitrary value of the variations of parametersδqmT. Thus we have the following conditions, which must be satisfied for any harmonic m:(5)∂Π∂qmT=0Next, we calculate derivatives of the total potential energy of a strip and finally, we get a non-homogeneous and nonlinear set of algebraic Eq. (6), which are the searched stability equations.(6)(K^uuqu+K^wwqw)+[1/2K∼wwqw+1/2K∼wuqu+1/4K∼uwqw]+1/2K^uuuuquu+3/4K∼uuuu∗quu+3/4K∼uuuu∗∗quu+1/4K∼uuvuquu+1/2K∼uuuvquv+1/4K∼wuuquu+1/4K∼uwuqw-Q=0We can visualize the construction of a strip stiffness matrix, which is composed of twelve block matrices. Assembling block matrices into conventional/geometric stiffness matrix of each strip is performed according to the scheme presented in Fig. 1, where ST1=K^uu, ST2=K^ww, ST3=K∼ww, ST4=K∼wu, ST5=K∼uw, ST6=K∼uuuu, ST7=K∼uuuu∗, ST8=K∼uuuu∗∗, ST9=K∼uuvu, ST10=K∼uuuv, ST11=K∼wuuand ST12=K∼uwu(ST5=ST4T, ST8=ST7T, ST10=ST9T, ST12=ST11T).For equilibrium, the principle of stationary potential energy requires that:(7)R=∂Π/∂qT=K^+K∼q-Q=Kq-Q=0where Π is a function of the displacements q, and R represent the gradient or residual force vector, which is generally nonzero for some approximate displacement vector q0 (the subscript 0 denotes an old value). It is assumed that a better approximation is given by:(8)qn=q0+δ0.where subscript n denotes a new value.Taylor’s expansion of Eq. (7) yields:(9)Rn=R(q0+δ0)=R(q0)+K‾0δ0+…=R0+K‾0δ0+…whereK‾0=∂R/∂qis the matrix of second partial derivatives of Π calculated at q0 (i.e. the tangent stiffness matrix or Hessian matrix). Setting Eq. (9) to zero and considering only linear terms inδ0 gives the standard expression for Newton–Raphson iteration:(10)δ0=-K‾0-1R0.Using this approach, a further iteration yields:(11)δn=K‾n-1RnwhereK‾n=∂R/∂qatqn.In addition, the blocks of the conventional and geometric tangent stiffness matrix of each strip are:(12)K‾^=K^uu00K^ww.(13)K‾∼=01/2K∼uw1/2K∼wu3/2K∼ww+3/2K∼uuuu+3/2K∼uuuu∗+3/2K∼uuuu∗∗1/2K∼uuuv1/2K∼uuvu0+01/2K∼uwu1/2K∼wuu0Comparing these expressions with Eq. (6), it is apparent that the conventional stiffness matrix remains unchanged, while the geometric tangent matrix becomes symmetrical.The loss of stability of static equilibrium states of structures subjected to conservative loads is in general known as static buckling of the structure. For conservative systems, the principle of minimum of the total potential energy can be used to test the stability of a structure (static equilibriums are extremes of the total potential energy). The Hessian with respect to the local DOFs is denoted as the tangent stiffness matrix, of each strip i.e.:(14)K‾=K‾^+K‾∼The (local) stability of equilibrium states of conservative systems by HCFSM can be assessed by looking at the eigenvalues of the tangent stiffness matrix of structure (TSMS)K‾(DOFs·r·(n+1))·(DOFs·r·(n+1))with (n+1) nodal lines, which are all real, since tangent stiffness matrix of the strip is a symmetric matrix.A typical rectangular plate is divided into (n) finite strips with (n+1) nodal lines. Let λidenote the i-th eigenvalue of(15)K‾(DOFs·r·(n+1))·(DOFs·r·(n+1))Based on theorems of Lagrange–Dirichlet and Lyapunov [4] it can be concluded that an equilibrium state is stable if all λi>0, while an equilibrium state is unstable if one or more λi<0. If along a load-path (IINCS=1, NINCS), at some equilibrium state one or more λi=0, this equilibrium state is denoted as a critical state. Static buckling refers in general to case where, starting from some stable state, a critical state is reached along the load-path. Buckling occurs where the matrix becomes singular.The integral expressions in HCFSM formulation contain the products of trigonometric functions with higher-order exponents, and therefore the orthogonality characteristics are no longer valid. To calculate the blocks of the tangent geometric stiffness matrix, it would be necessary to know values for the basic unknowns in all series terms at the moment of their computation. Therefore, the only possible way to form the tangent stiffness matrix is to take into account all series terms.Depending on the particular problem under consideration, the Green–Lagrange nonlinear contributions, Eq. (2) in a manner consistent the von Karman assumptions may be safely ignored. Otherwise the next seven blocks must be addedK∼uumnuu=∫A∑v,p=1r∑s,t=1rBu2muTUsTBu1tuTD11Bu1vuUpBu2nudA,K∼uumnuu∗=∫A∑s,p=1rBu4muTD11Bu1suUpBu2nudA,K∼uumnuu∗=∫A∑s,p=1rBu2muTUsTBu1puTD11Bu4nudA,K∼uumnvu=∫A∑s,p=1rBu5mvTD11Bu1suUpBu2nudA,K∼uumnuv=∫A∑s,p=1rBu2muTUsTBu1puTD11Bu5nvdA,K∼wumnu=∫A∑v,p=1r∑s,t=1rBw2mTWsTBw1tTD11Bu1vuUpBu2nudA,(16)K∼uwmnu=∫A∑v,p=1r∑s,t=1rBu2muTUsTBu1tuTD11Bw1vWpBw2ndA.In this section, we give a high-level overview of the program that implements the HCFSM approach to structure analysis. We pinpoint the most time consuming parts of the program, and show how to speed them up using a nested parallelization model. We also derive formulas for computing the upper bound on the maximum achievable speedup.The HCFSM program implements an iterative method for incremental computation of the values of displacements and inner forces for a given structure with predefined loading conditions. The program supports both the von Karman and Green–Lagrange formulations of the HCFSM analysis. The program input data contains information about the geometry of a given structure, material properties, description of the loading increments, and boundary conditions. It also defines the strip network configuration and the number of harmonics (series terms) used in a particular computation.The essence of the HCFSM program can be described as solving the system of stability equations, see Eq. (6). For each load increment, an iterative computation seeks for a solution that conforms to the stated criterion. In each iteration the program performs the following four steps: (1) it computes the block matrices, named ST1–12, for each strip in the network; (2) it assembles these matrices into the conventional-tangent stiffness matrix according to the scheme presented in Fig. 1; (3) it solves the system of stability equations (SSE); (4) it computes the values of displacements and inner forces. Note that the values of the tangent stiffness matrix depend on the values of displacements that were calculated in the previous iteration. The details of the algorithm can be found in [5].The integral expressions from Eq. (3) that involve trigonometric functions are independent of the values of displacements or inner forces being calculated in each iteration. When computed for the limit [0,1], the integral values are independent of particular strip lengths, too. The total execution time of the HCFSM program can be therefore significantly reduced by computing these values in advance. Different strategies for obtaining the integral values have been investigated in [5].The HCFSM program consists of two modules as shown in Fig. 2. The first module, named FSMN1, generates the integral values once and stores them on the disk for later use. The second module, named FSMNE, carries out the iterative algorithm of computing the values of displacements and inner forces, as described above. The algorithm starts by reading the integral values from the disk. These values are kept in the main memory throughout the whole program execution. The program uses them in every iteration to compute the tangent stiffness matrices of each strip. The number of integral values stored in the main memory depends only on the number of series terms used for a particular computation.The total sequential execution time of the HCFSM program, which encompasses computations of all the iterations for all the loading increments, can be represented as(17)T=Tstiff+Tgauss+Totherwhere Tstiffis the time spent on forming the stiffness matrix, Tgaussis the time needed to solve the system of stability equations using the method of Gaussian elimination, and Totherrepresents the cost of initialization and computing inner forces, solution convergence, and residual forces.The computational complexity of the HCFSM program is mainly determined by two parameters – the number of strips (NELEM) and number of series terms (NTERM). The HCFSM program computes the tangent stiffness matrix (TSMS) for each strip by taking into account the values of displacements in all series terms. The rank of the generated stiffness matrix is 4∗NELEM∗NTERM. Increasing any of these two parameters therefore incurs higher costs of matrix construction, as well as more time to solve the system of stability equations.A detailed execution profile of the HCSFM program, when the Green–Lagrange formulation is applied, reveals that the most time consuming part is the computation of stiffness matrices for every strip, whereas much less time is spent on finding the solution to SSE using the method of Gaussian elimination [3]. Fig. 3shows the cost breakdown of the sequential HCFSM execution as a function of the number of harmonics used in the Green–Lagrange formulation, when the number of strips is fixed (NELEM=20). As more series terms are being used, an increasingly larger fraction of the total execution time is spent on computing the stiffness matrices of each strip.The stiffness matrix of one strip is composed out of 12 block matrices in the Green–Lagrange formulation, respectively 5 block matrices in the von Karman formulation. These block matrices are named as STi (i=1,…,12) in Fig. 1. The total time needed for building the stiffness matrices for NELEM strips is(18)Tstiff=NELEM·Tassemble+∑i=112TSTiwhere TSTiis the time spent on computing the block matrix STi, and Tassemblerepresents the time needed for assembling the stiffness matrix according to the scheme from Fig. 1.Within one iteration, computations of stiffness matrices for every strip, as well as computations of their constituents (different STi block matrices), are mutually independent. We have already seen that these computations play the dominant role in the total execution time of the HCFSM program. Therefore they offer a natural basis for program parallelization.We employ a two-level (nested) parallelization approach. As the number of strips could be potentially large, it is reasonable to distribute independent computations of stiffness matrices across a cluster of machines. This calls for the message-passing parallelization model; we refer to it as the MPI approach [3,5]. To ensure fair load balancing, each node is in charge of computing the stiffness matrices for roughly the same number of strips. Note that the MPI approach incurs an additional communication cost due to data shipping between the master and slave nodes.The second parallelization level deals with simultaneous computation of the block matrices. The block matrices that depend solely on the geometrical properties of the strip, namely ST1 and ST2, are computed only once and used as a basis for construction of the stiffness matrices. The remaining 10 block matrices are recomputed in every iteration based on the values of displacements of all harmonics. These matrices have different computational complexities though. Fig. 4shows that four block stiffness matrices, namely ST3, ST11, ST12, and ST6 (in decreasing order of their contributions), amount to 94% of the time needed for computing the stiffness matrices of all strips (Tstiff). Notice that only a small fraction of the time (0.44%) is spent on assembling the block matrices into the stiffness matrices.Driven by the ubiquitous spread of multi-core architectures, we employed the shared-memory parallel programming model to speed up computation of the block matrices; we refer to it as the OpenMP approach [3]. This method has lower communication overhead compared to that of the MPI approach. On the other hand, a non-uniform distribution of the block matrix complexities limits the degree of achievable parallelization. The OpenMP approach also incurs higher costs of thread/process synchronization.A significant portion of the running time is spent on solving the system of stability equations, see Fig. 3. The method of Gaussian elimination, which is typically used for this step, has O(n3) complexity, where n is the dimension of the system. In our case, the system dimension is 4∗(NELEM+1)∗NTERM, where NELEM and NTERM represent the number of strips and number of series terms used in a particular computation. For structures with involved strip network configurations and large numbers of series terms, the time needed to obtain the solution becomes more pronounced. In this paper, we evaluate the effect of parallelizing this step using the ScaLAPACK library [14].To quantify the effect of parallelization, we define a speedup as the ratio of the total execution times of the serial and parallel programs. From the standpoint of Amdahl’s law [6], the maximum speedup that can be achieved is defined as(19)max_speedup=1s+1-snwhere n represents the number of processing units (nodes), and s denotes the fraction of the serial code (0<s<1). When the parameter s is negligible, i.e. s is close to zero, the maximum speedup is close to n.Fig. 5shows the sequential and parallel executions of the HCFSM program. The portion of the code that is being parallelized includes solving the system of stability equations, marked as b, and computing the stiffness matrices of all strips, marked as c. The serial part of the code is denoted as a. Note thata+b+c=1.Running the HCFSM program in a real cluster environment introduces some communication overhead between the master and slave nodes. Eq. (19) therefore provides only an upper bound on the maximum achievable speedup.Example 1. Consider execution of the HCSFM program in which solving the SSE and computing the stiffness matrices of all strips account for 20% and 75% of the total execution time, respectively. We parallelize both the b and c parts of the code by distributing the computation across 10 nodes (n=10), such that each node gets roughly the same amount of work (e.g. the same number of strips). The time fraction spent on executing the serial code is 0.05 (s=0.05). According to Eq. (19), the maximum achievable speedup is 6.9 times. Note that the upper bound on the maximum speedup is1/s– in our example 20 times – regardless of the number of used nodes.Eq. (19) allows us to derive an upper bound on the maximum achievable speedup when we parallelize only specific parts of the program. In the previous example, we considered parallelization of both the b and c parts, thuss=a. In the following sections, we will also evaluate the effect of parallelizing only the stiffness matrix computation, i.e. the c part. In that case, we haves=a+b.To exploit modern multi-core architectures, we consider a two-level approach to parallelization; rather than just distributing the computation of stiffness matrices, we also aim to parallelize computation of its constituent parts – the ST1–12 block matrices – inside single nodes using available multi-core processors. That will further reduce the c part in Fig. 5.We have also noticed the potential for applying the nested parallelization model to speed up solving the system of stability equations. The ScaLAPACK library that we used for our experiments is based on the single-threaded BLAS and LAPACK packages though. We plan to use multi-threaded linear algebra packages in our future work.To measure the effect of nested parallelization, we recursively apply Amdahl’s law, as defined in Eq. (19), to obtain the overall maximum speedup. The two-level parallelization approach exploits multi-core processors to further reduce the c part. The maximum achievable speedup is thus defined as(20)max_speedupnested=1a+bn+cnp+1-pkwhere n represents the number of processing nodes, a, b, and c are the time fractions spent on executing the serial code, solving the SSE, and computing the stiffness matrices, as in Fig. 5, k denotes the number of cores, and p (0<p<1) is the fraction of the serial code that cannot be parallelized when computing the stiffness matrices of one strip.For the later purpose, we also derive an upper bound on the maximum speedup when we parallelize only the stiffness matrix computation.(21)max_speedupnested_stiff=1a+b+cnp+1-pkFig. 6shows execution diagrams of the stiffness matrix computation for one strip using various numbers of processing units (cores). The block matrices are calculated in the decreasing order of their complexity, i.e. most expensive blocks are computed first. The OpenMP scheduler uses the dynamic scheduling policy for assigning blocks to available processing units, achieving the best possible load balance among them.Example 2. Consider our previous example of the HCFSM program execution in which solving the SSE and computing the stiffness matrices of all strips account for 20% and 75% of the total execution time (a=0.05, b=0.2, c=0.75). We distribute the parallelizable parts (b and c) across 10 nodes (n=10), as before. Using the nested parallelization approach, we also distribute the computation of the block matrices from Fig. 6 among four cores (k=4). A very tiny portion of the code, which assembles the stiffness matrix according to the scheme from Fig. 1, is executed in a serial fashion (precisely, p=0.044). According to Eq. (21), the upper bound on the maximum speedup that can be obtained is 10.96 times. The nested parallelization approach, which is based on using four-core processors, thus might give up to 58% improvement over the single-level approach from Example 1.The HCFSM program parallelization is designed for a cluster of four-core machines. For running the program in the cloud environment, we must form a virtual computer cluster with a predefined number of computing instances. Two essential issues arise here. First, we have to consider the overhead imposed by the cloud virtual machines. The literature [7] suggests that hardware-assisted virtual machines have a negligible CPU performance overhead. The second issue concerns with necessity to economically exploit the cloud resources, that is, to acquire as few as possible cloud virtual machines. The successful usage of cloud resources depends on the provider’s charging policy, which is tailored to the typical users the cloud provider expects.Cloud providers define several instance types (families) to represent available hardware configurations. Each instance type has a unique combination of resources – like CPU, memory, and disk storage – and comes with their own price tag. All instances are guaranteed to receive a minimum amount of CPU cycles with the ability to burst (with no extra charge) when excess cycles are available.Users can choose instance types that meet their computing needs. Web and database applications traditionally impose greater requirements on network bandwidth, memory capacity, and disk space, rather than on CPU cycles. High performance computing (HPC) applications, like engineering calculations or business analytics, require large amounts of computational power coupled with high network performance.A typical workflow for running the HCFSM program in a cloud environment consists of several steps presented in Table 1.Step 1 determines the optimal instance type used to build all the virtual cluster nodes. With almost all cloud providers, even the cheapest instance type satisfies our network bandwidth and storage requirements. We are left thus with optimizing on virtual CPU core performance and memory capacity. We aim for provisioning instances that have at least one four-core processor. We work under an assumption that due to CPU bursting and largely unused CPU cycles by the other servers hosted on the same physical machine, our virtual CPU core performance remains steady. Throughout our experience that has proved to be mostly correct. As for the memory capacity, we have built a realistic model of the RAM usage, based on the number of strips and harmonics used, to help us determine appropriate instance types. We aim for the cheapest instance types that meet our memory requirements so as to keep the overall cost as low as possible. Underestimating the RAM usage, however, can cause swapping, which can lead to poor virtual CPU core utilization, longer execution times, and substantially higher usage costs.Once the optimal instance type has been determined, it is necessary to verify the existence of the base node image. If the base node image does not exist, a bare server is provisioned, configured to our needs, and snapshotted to a base node image. We have found that bootstrapping every provisioned server (installing and updating required system packages, generating integrals, installing HCFSM) every time an experiment is run takes a few hours to complete, which we deemed unacceptable. We were able to cut the total bootstrap time to mere minutes by bundling the server bootstrap operations and the generated integrals into the base node image.Step 2 is responsible for creating and configuring the virtual cluster. The number of worker nodes is chosen such that the highest level of parallelism is achieved using the minimal number of nodes. To accomplish this goal, the load balance among the worker nodes has to be as perfect as possible; that is, each node should be responsible for performing calculations for the same number of strips, or in charge of roughly the same portion of the matrix representing the SSE. An UUID (Universally Unique IDentifier [9]) is used to uniquely identify the individual virtual cluster and its worker nodes, allowing for multiple independent clusters to be run at the same time. The servers are then provisioned asynchronously based on the base node image, while proclaiming the first ready server the master node. The master node starts configuring the virtual cluster for the experiment right away, as it is independent of the worker nodes for some time – thus minimizing the virtual cluster bootstrap time. All inter-node communication is performed through the private network, as it provides low latency and high-bandwidth connectivity.Cloud providers pose hourly charges on all provisioned servers, running or not. The final step thus terminates the virtual cluster as soon as the experimental results are downloaded in order to restrict the overall monetary cost.Description of the highly complex behavior of concrete is a difficult task and to date generally accepted constitutive equations do not exist. A variety of models have been proposed to characterize the stress–strain relations and failure behavior. All these models have certain inherent advantages or disadvantages that depend to a large degree on the particular application considered.Fig. 7gives comparison of working diagram of concrete C35/45 according to Eurocode 2 and RDA, under the short-term uniaxial pressure [10]. For comparison is adopted stress–strain diagram given by EC 2 for the nonlinear analysis of structures.Milašinović [11] gave a detailed description of the iterative RDA procedure for obtaining the stress–strain diagram of a steel rod, when its coefficient of linear thermal expansion αT, specific heat c, mass density ρ, modulus of elasticity EHand slenderness ratio at the point of elasticity λEare known. He experimentally verified the procedure on the prototype. On the basis of known physical parameters of material, established by the prototype, and basic mechanical parameters of steel bars, that are standard tested, Goleš in [10] theoretically obtained RDA stress–strain diagrams of reinforcement (see Fig. 8).An reinforced concrete 20m long simple supported folded plate structure, with applied vertical uniformly distributed load: self-weight of structure g, self-weight of covering on sloped planes Δg=0.5kN/m2 and snow load on sloped planes s=1.0kN/m2 (Fig. 9), is analyzed. The structure is made of concrete C35/45 and reinforcement B400. Linear FSM analysis for 10 strips and 100 series terms is performed using elasticity modulus of E=34GPa and Poisson’s ratio μ=0.Nonlinear geometric effects that are of major importance in the deformational response as well ultimate and serviceability load analysis of longer reinforced concrete folded shells are used in order to examine structure stability [12].The comparative efficiency of the von Karman and the Green–Lagrange HCFSM solutions is presented in the large-displacement stability analysis. Various span lengths (10, 15, 20, 25 and 30m) of structure and various series terms (1, 3, 5–29 and 31) are considered in the analysis. The convergence is established when the norm of the residual forces value is less or equal to 0.1 (accuracy 1/1000). The total loading was divided into 8 (0.6, 0.04, 0.04, 0.04, 0.04, 0.04, 0.1 and 0.1) increments of load. The load factor 0.8 corresponds to the service load.As shown in Fig. 10, the effect on nonlinear behavior is less pronounced in the 20m long structure. In this example the response always involves a hardening of structure.Fig. 11illustrates the convergence of the deflection w and the moment My. The convergence is non-monotonic for all predictions: Green–Lagrange (LAG), von Karman (VK), and linear (LIN). However, the convergence of the moment Myis poor, and many more series terms would be required for the moment values to converge to the exact answer.During the research, we compared HCFSM results with those obtained in ABAQUS with shell elements. The variations of central deflections w in nodal line 1 and normal force Nyin nodal line 11 with load intensity are presented in Fig. 10.Limit state design (ultimate and serviceability) of characteristic cross sections is performed according to currently valid Serbian technical regulations for concrete and reinforced concrete, using partial factor method.Diagrams of interaction (Nu–Mu) of two characteristic cross sections are drawn using various combinations of working diagrams (WD) of concrete and steel according to Eurocode 2 and according to RDA (A-both WD according to EC 2; B-both WD according to RDA, without strain limitation in concrete; C-both WD according to RDA, with limited maximum strain in concrete to εcu=3.5‰; D-RDA WD of steel and EC 2 WD of concrete; E-EC 2 WD of steel and RDA WD without strain limitation of concrete and F-EC 2 WD of steel and RDA WD with limited maximum strain in concrete to εcu=3.5‰) – Figs. 12 and 13. Partial safety factors for material are not applied. Tension force is treated as negative. Positive bending moment of the border beam stretches the bottom side of cross-section.We performed experiments in order to measure the effect of the hybrid MPI/OpenMP parallelization of the HCFSM analysis in a cloud environment. We ran all experiments on an Amazon EC2 cluster consisting of 10 computing instances (cc1.xlarge instances) [8]. Each instance runs as a hardware-assisted virtual machine with 2×Intel Xeon X5570 quad-core processors, 23GB of memory, and access to 10-Gigabit Ethernet. Instances of this family provide high CPU resources and network performance, and are well suited for parallelization of the HCFSM program. We used MPICH2 v1.5 library for communication between the master and slave nodes, and OpenMP v3.1 API for shared-memory programming. For solving the system of stability equations in a distributed manner, we used ScaLAPACK v2.0.2 and LAPACK v3.4.2 libraries [14]. The reported values are averages over 3 runs. The standard deviation was less than 2% in each experiment.We measured the overall running time of the HCFSM program when executed in a cloud environment using different numbers of virtual MPI nodes. In this particular analysis, we evaluated the effect of nested parallelization when applied only on the computation of strip stiffness matrices; that is, we solved the system of linear equations sequentially. The maximum achievable speedup is derived from Eq. (21).Fig. 14shows the achieved speedup of the HCFSM program for the discussed examples with 20 strips. The strip calculations are equally distributed among the cluster nodes. Each node performs sequential computation of the corresponding stiffness matrices. The communication overhead between the master and slave nodes is less than 5% of the total execution time in the worst case. The achieved speedup closely follows the maximum speedup calculated according to Eq. (21). Notice that the speedup potential is limited by the size of the serial portion of the code.Fig. 15presents the effects of a combined MPI/OpenMP application in our running example with 20 strips. Each cluster node in the cloud environment is equipped with two cores that are used for parallel computation of different block matrices, similarly as in Fig. 6. The OpenMP scheduler achieves almost perfect load balancing between the two cores on each machine. The overheads of communication between nodes and thread synchronization are again almost negligible. That brings the achieved speedup very close to the maximum one.Fig. 16showcases the effect of a combined MPI/OpenMP application in a cluster composed out of four-core virtual nodes. The thread synchronization overhead is more pronounced than before, but still negligible compared to the total execution time. Again the real speedup closely follows the maximum speedup calculated according to Eq. (21). However, the gap between these values is bigger compared to the previous cases. The reason is that the OpenMP scheduler is unable to distribute the load equally across four cores. This is perfectly reasonable since we know that some of the block matrices are more expensive than the others, as shown in Figs. 4 and 6. The maximum speedup defined in Eq. (21) assumes the perfect load distribution.The system of stability equations has a dimension of 4∗(NELEM+1)∗NTERM. We analyzed the structure from Section 5 using 20 strips and 51 series terms – in total the size of the matrix was 4284 x 4284. We used ScaLAPACK routines to solve the system of linear equations using multiple processing nodes.The ScaLAPACK library employs the block cyclic data distribution to spread different portions of the matrices among the processing nodes. By doing so, the ScaLAPACK algorithms achieve high-levels of data reuse and lower network traffic. The computing nodes are organized into a process grid of a fixed size, and communication between them is completely driven by the ScaLAPACK routines.The time needed to solve a system of linear equations depends on multiple factors: (1) The running time depends on the number of virtual processing nodes. More nodes might reduce the computation time, but also increase the communication cost. (2) The geometry of the process grid also affects the running time. The communication overhead of a 20×1 grid might significantly differ from that of a 5×4 grid. (3) Collocating multiple virtual nodes on the same physical server reduces the communication cost. For instance, let us consider four virtual nodes (MPI processes), which are organized in a 2×2 processing grid. The network traffic between these nodes reduces as we use fewer physical servers. That results in better performance, assuming that the physical nodes can efficiently support multiple virtual nodes.We evaluated the effect of the process grid geometry on the time needed to solve the system of linear equations. We considered different grid configurations that are possible to build using 1, 2, 4, 5, 10, and 20 virtual nodes. These cluster sizes ensure fair load balancing of the first parallelization step, i.e. computing the stiffness matrices, in our example with 20 stripes. We present the achieved speedups in Fig. 17; the speedup is measured with respect to the sequential execution time on a single node. Each virtual nodes corresponds to a different physical machine (i.e. collocation is ignored).We can conclude from Fig. 17 that the best performance is achieved with square-shaped process grids. Extremely asymmetric configurations (e.g. 20×1, 10×2) yield increased network traffic, which leads to even worse performance than that of a single node execution.Next, we exploit multi-core nodes to efficiently run multiple MPI processes on the same physical machine. The number of virtual nodes equals to the size of a given process grid. We consider only the square-shaped grid configurations.Fig. 18presents the effect of collocating virtual nodes on the running time for solving the SSE. As more physical nodes are involved in the same grid configuration, more data is being shipped over the network; this increased communication cost hurts the overall performance. We can also conclude that increasing the grid size, i.e. the number of virtual nodes, when coupled with an appropriate collocation strategy, can significantly boost the performance (e.g. grid 2×2 with one node vs. grid 4×4 with four nodes). The results also showcase the importance of load balancing between the physical servers; if violated, like in the 3×3 grid example with three nodes, the communication overhead becomes more pronounced.The parallel HCFSM program looks for performance improvements by speeding up the two most time consuming parts: computing the strip stiffness matrices and solving the system of linear equations.There is an interesting trade-off between these two parts though. The stiffness matrix computation is more amenable to distributed execution among many processing nodes, either virtual or physical. The obtained results from Section 6.1 are very close to the maximum ones. In contrast, using more resources for solving the system of linear equations might result in worse performance. We have seen from Section 6.2 that multiple factors affect the final performance. In this section we look for the setting in which the overall performance of the HCFSM program is at its best.An import aspect of parallelization is efficiency [13], which is defined as the ratio between the achieved speedup and number of used processing nodes. Fig. 19shows the maximum speedups that we achieved using certain numbers of virtual nodes, and the corresponding values of efficiency. For each node we provided the process grid configuration and number of physical servers used to obtain the best speedup.Fig. 19 shows a clear gap between the maximum and achieved speedup, which increases as we use more virtual nodes. We previously showed that the parallel computation of stiffness matrices is scalable and can be nicely distributed among nodes with negligible network overhead; as a result of this, the achieved speedups closely followed the maximum ones. The ScaLAPACK library, however, lacks the same property. The routines that we used for solving the SSE suffer from very poor parallelization efficiency. For instance, even though we used 16 virtual nodes organized in a 4×4 processing grid, the achieved speedup of this routing is barely 5 times. That limits the speedup of the whole execution. The best speedup of 11.7 times is achieved using 20 MPI processes, which are distributed over five physical machines. The process grid was 4×4 with collocated processes on four physical machines.Fig. 19 also shows one unexpected result: using more physical servers – and we experimented with 10 physical instances – does not always guarantee better performance. The reason is that we fail to use the full potential of multi-core servers. With 20 virtual nodes spread over 10 four-core physical servers, each machine is running 2 MPI processes. To form a 4×4 process grid, we need therefore 8 physical machines. In contrast, if we use only 5 servers, where each one of them is running 4 MPI processes, we need only 4 physical machines for the same grid. Obviously, the latter implies a lower communication overhead for solving the system of linear equations, and a better overall running time.This raises an interesting question. What is the minimal price that we need to invest in provisioning cloud resources in order to achieve a desired speedup?In Fig. 20we plot the best possible speedups that we could achieve with a given number of physical servers. For each result, we provide the process grid configurations and the number of running virtual node instances, i.e. MPI processes. We also show the monetary cost of provisioning the servers on the day of experiments ($1.3 per hour for cc1.xlarge instances).From the economical perspective, using more cloud resources – like virtual machines, CPU cores, or network bandwidth – incurs higher monetary cost. The above experiments have shown that there is little justification in provisioning more than one physical server, unless we look for large performance improvements (more than 10×). In that case, provisioning of five physical servers, where each one of them runs four MPI processes, has the best speedup/monetary cost ratio. This conclusion is important for the cloud environment as it leads to a more economical usage of cloud resources.The discussed examples represent typical use cases for the HCFSM approach. We expect that analyzes of different structures of approximately the same size, expressed in terms of the number of strips and harmonics, yield similar performance improvements.

@&#CONCLUSIONS@&#
The HCFSM computational model for the analysis of reinforced concrete folded plates has been described and the results of several numerical applications has been presented and discussed. Geometric nonlinear effects need to be taken into consideration in order to obtain realistic numerical solutions for longer shells because that span length has a serious influence on equilibrium state. When the lowest eigenvalue of the tangential stiffness matrix of structure becomes negative, a bifurcation state can be missed in a finite element program if this matrix is not calculated and checked. The use of various combinations of the stress–strain diagrams of concrete and reinforcement according to EC 2 and RDA has a negligible influence on the ultimate resistance of characteristic cross-sections of shorter shells. It is also shown that the ultimate load of shorter shells is almost the same for all applied analysis methods (FSM, linear HCFSM, nonlinear HCFSM, ABAQUS).To tackle the problem of increased computation time of the HCFSM formulation, we applied a hybrid parallelization approach to speed up the critical parts of the code. The approach simultaneously applies both the MPI and OpenMP parallel programming models. We focused on parallelizing the two most time consuming parts: computing the strip stiffness matrices and solving the system of stability equations. We derived an extended formulation of Amdahl’s law to come up with an upper bound on the maximum achievable speedup. We measured the performance of the parallel execution within a cluster of virtual nodes in a cloud environment. The cloud computing approach to the HCFSM program parallelization has proved to be very effective in practice. The obtained performance improvements are significant, and, in certain cases, very close to the maximum achievable speedups. We also discussed the problem of provisioning cloud resources from the perspective of performance and monetary cost. Our experiments included structures that represent typical use cases in analysis of reinforced concrete folded plates. It is therefore reasonable to expect similar performance gains in analysis of other structures of the similar problem size.