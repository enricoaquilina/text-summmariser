@&#MAIN-TITLE@&#
Semi-supervised cluster-and-label with feature based re-clustering to reduce noise in Thai document images

@&#HIGHLIGHTS@&#
We proposed a novel noise reduction method for document images.Semi-supervised learning is applied to classify noise from character components.The proposed method is suitable for Non-Latin based scripts i.e. Thai document image.We proposed an enhance labeling method of semi-supervised cluster-and-label approach.The performance of proposed methods are significantly better than comparison methods.

@&#KEYPHRASES@&#
Noise reduction,Document enhancement,Semi-supervised classification,Cluster-and-label,Thai document,

@&#ABSTRACT@&#
Noise components are a major cause of poor performance in document analysis. To reduce undesired components, most recent research works have applied an image processing technique. However, the effectiveness of these techniques is suitable only for a Latin script document but not a non-Latin script document. The characteristics of the non-Latin script document, such as Thai, are considerably more complicated than the Latin script document and include many levels of character alignment, no word or sentence separator, and variability in a character’s size. When applying an image processing technique to a Thai document, we usually remove the characters that are relatively close to noise. Hence, in this paper, we propose a novel noise reduction method by applying a machine learning technique to classify and reduce noise in document images. The proposed method uses a semi-supervised cluster-and-label approach with an improved labeling method, namely, feature selected sub-cluster labeling. Feature selected sub-cluster labeling focuses on the clusters that are incorrectly labeled by conventional labeling methods. These clusters are re-clustered into small groups with a new feature set that is selected according to class labels. The experimental results show that this method can significantly improve the accuracy of labeling examples and the performance of classification. We compared the performance of noise reduction and character preservation between the proposed method and two related noise reduction approaches, i.e., a two-phased stroke-like pattern noise (SPN) removal and a commercial noise reduction software called ScanFix Xpress 6.0. The results show that semi-supervised noise reduction is significantly better than the compared methods of which an F-measure of character and noise is 86.01 and 97.82, respectively.

@&#INTRODUCTION@&#
Normally, the document analysis process works effectively on clean document images. The clean images, however, are rarely found in real-world situations. A real-world document image usually contains noise components that can dramatically decrease a performance of document analysis e.g., accuracy of optical character recognition (OCR). Noise in a document image stems from many sources (e.g., paper background, document aging, water drop and notation) with various properties. Recent research works on noise reduction usually employ one or more image processing techniques with specific noise properties [1]. These aim to reduce a specific type of noise, e.g., salt-and-pepper noise [2], line of writing [3], double-side writing interference [4], preprinted form [5], blob noise [6], and background [7,8].Although the image processing technique is commonly applied to document image noise reduction, the effectiveness of this method is limited. First, it is only appropriated for a document image with a Latin based script (e.g., English, German and French) but not for a document with a non-Latin based script (e.g., Persian, Arabic and Thai) because the non-Latin script is ordinarily composed of many small characters whose size is similar to noise. The image processing technique typically specifies a component size threshold to separate character and noise components. If the size of character varies as in non-Latin script, some small characters may be removed when its size is less than the threshold, and a noise component will not be removed when its size is larger than the threshold. Second, many historical documents were digitized into black and white or binary images due to the limited storage space and technology during a digitization time. This black and white image contains restricted information (e.g., color depth, contrast) for image processing noise reduction methods. Re-digitizing these documents is impracticable because most of them were considerably more degraded over time, and many of them were destroyed. With the limitations above, an effective method is required to reduce noise in non-Latin script.In this paper, we propose a novel noise reduction method for reducing noise in a black and white Thai document image. A Thai document is a fascinating example of a non-Latin document because of its intricate properties, i.e., it consists of many small characters, many levels of character alignment and no word and sentence separator. In addition to using an image processing technique, we utilize a machine learning technique for reducing noise in the Thai document image. The benefit of applying a machine learning technique is that it need not specify noise criteria for distinction, but instead, a classifier is trained to distinguish noise from a character. An efficient classifier can discriminate even noise that its size is quite similar to a small character, as frequently presented in Thai script. However, to build an efficient classifier, we need a sufficient number of labeled examples. Although a large number of document images is available, the labeled components are scarce because the labeling process is effort intensive and is a time consuming task. To minimize the cost of human effort, we utilize the available unlabeled examples by applying a semi-supervised classification. The semi-supervised classification is a widely used method for the problem of limited labeled data, for example, a cross-lingual sentiment classification. Hajmohammadi [9] proposed a combination of semi-supervised and active learning methods to classify a sentiment document in the target language, which is not English and rarely finds the labeled data, by using the source language document, which is in English and the labeled sentiment documents are available.In this work, we apply the semi-supervised cluster-and-label approach to create the classifier for noise reduction. First, all examples, with or without the class labeled, are clustered by their properties. Then, a majority class of labeled examples in each cluster is specified. The unlabeled examples in each cluster are then labeled as the majority class in theirs cluster. These recently labeled examples and the prior labeled examples are used to train the final classifier. Finally, the classifier classifies all components and removes the noise classified components from the document images.When we applied the cluster-and-label procedure, another problem arose. The mislabeled examples were apparently found in a mixed-class cluster, or a cluster might contain various classes of labeled examples. Hence, we proposed a radical labeling method to improve accuracy of example labeling in the mixed-class cluster, namely, feature selected sub-cluster labeling. The idea is that if different-class examples are unintentionally grouped into the same clusters, sub-clustering will re-organize examples into a proper sub-group. Because the current feature set is ineffectual to separate examples, a new feature set for sub-clustering is then needed. A particular feature set is selected by a feature selection with information gain. This particular feature is then used to cluster examples in each mixed-class cluster into a satisfying sub-cluster. The unlabeled examples are then labeled with a class of labeled examples of their affiliated sub-cluster.The performance of feature selected sub-cluster labeling is compared with a conventional majority vote labeling. The results show that the proposed labeling method improves the accuracy of labeling in the mixed-class cluster and provides an efficient classifier. The performance of semi-supervised noise reduction with feature selected sub-cluster labeling is compared with two related noise reduction methods, i.e., a two-phased stroke-like pattern noise (SPN) removal [10] and the commercial noise reduction software, namely, ScanFix Xpress 6.0 [11]. The results show that semi-supervised noise reduction is significantly better in noise reduction and character preservation than the compared methods. We further analyzed the reason for improvement by using our method and found that for the small characters that are used frequently in Thai documents, they were clustered in the mixed-class cluster and labeled incorrectly by the standard cluster-and-label method. However, our proposed method can improve the accuracy of the small character portion of the test set.The structure of this paper is as follows. A review and discussion regarding state-of-the-art noise reduction methods in document images are presented in Section 2. The property of Thai script as opposed to Latin script is described in Section 3. An algorithm for semi-supervised cluster-and-label is described in Section 4. In Section 5, we present a methodology of the proposed semi-supervised noise reduction with feature selected sub-cluster labeling. The results and discussion are presented in Section 6. The last section provides a conclusion and future work.This work differs from our previous work [12,13] in two aspects. First, in previous work, we considered a line of a connected component as an example with an aim to reduce noise that might attach to a character. However, we found that the line-level example might contain an insignificant amount of information to distinguish a character from noise. As a result, this work uses a connected component as an example and focuses on removing particular noise, whose size is similar to a small character. Second, in previous work, we applied a traditional majority vote labeling in semi-supervised cluster-and-label classification to reduce noise but, in this work, we proposed a novel feature selected sub-cluster labeling method for semi-supervised cluster-and-label.Noise in a document image, in this work, is defined as any foreground component in the document image except a printed character. Noises can stem from many sources either intentionally (such as a water mark, rubber stamp and a signature) or accidentally (such as a paper wrinkle, a water drop and a worm hole). To the best of our knowledge, these various noises have not been categorized into a distinct category. There was one study that proposed a noise category by a source of noise, e.g., physical noise caused by damage in a document paper and digitization noise caused by an error in a conversion process of a document paper to a digital image [14]. In this work, we categorize noise by its characteristics, i.e., a specific pattern noise that explicitly diverges from a character so-called “fixed-form noise” and fuzzy pattern noise that is possibly akin to some characters so-called “free-form noise”.Several studies tried to eliminate the fixed-form noise. Examples of fixed-form noise removal methods are a salt-and-pepper noise removed by applying the k-Filled algorithm [2]; a line of writing removed by considering a threshold of the character’s length and width [3]; a pre-printed form removed by comparing it to a blank form [5]; a blob removed by considering the size and position of a large component [6]; and an interfering stroke in a double-sided handwriting document removed by considering contrast and the prior knowledge of alignment of handwritten English characters [4]. To apply any of these methods, a user must define criteria to identify noise from a character component and then apply some image processing techniques to reduce the selected component. However, these approaches seem dubious when applying them to free-form noise or noise whose attribute is quite similar to the character. For example, if line thickness of pre-printed forms is quite similar to that of the character, this noise will not be removed [5]. Although extensive studies have been performed on fixed-form noise removal, very few studies have given attention to free-from noise removal [6].Another challenge is noise reduction on a binary document image. A binary document image is a document that is digitized into a black and white image due to the limited storage space and technology during the digitization time. Re-digitizing these documents could be impracticable because most of these documents were considerably more degraded over time, and many of them were destroyed. This binary image is typically historical documents that usually consist of considerable noise. So, performing noise reduction on this historical document image is essential. However, because the binary image contains restricted information (e.g., color depth, contrast) for the image processing technique, using a machine learning technique instead could more properly reduce the noise in this binary document image.An interesting machine learning noise reduction method is the two-phased SPN removal [10] that is a script-independent noise removal for a binary document image using a supervised classification. The two-phased SPN removal divides a component in a document into a prominent text component (PTC) and a non-prominent text component (non-PTC). PTCs are composed of a character component whose characteristic obviously differs from noise components. Non-PTCs are composed of a noise and a character component whose characteristic slightly differs from noise. In the first phase of this method, the classifier for PTC and non-PTC is established. The classifier uses a connected component’s properties as an attribute set, i.e., area, perimeter, convex-area, orientation of fitted ellipse, major and minor axis lengths, eccentricity, filled area, extent, solidity, and equivalent diameter. Classified PTC components are used to calculate an average stroke-width and cohesiveness. These values are used in the second phase to divide non-PTCs into a noise or a character group by unsupervised k-means clustering. The two-phased SPN removal requires a sufficient number of labeled examples for constructing the PTC classifier; however, these labeled examples are scarce.In this work, we propose a noise reduction method using semi-supervised classification. We apply semi-supervised cluster-and-label with a novel feature selected sub-cluster labeling to reduce noise. The first step is a cluster-and-label process that increases a number of labeled examples. The second step uses the increased labeled examples to create a final classifier of noise and character. The complete algorithm of the proposed method is presented in Section 5.The additional analysis on the performance of noise reduction and character preservation by comparing the proposed method to the other two related approaches, i.e., the two-phased SPN removal and ScanFix Xpress 6.0, are presented in Section 6.The Thai language was used in Thailand by more than 65 million people in 2014 (National Statistical Office). The Thai character set [15–17] consists of forty-four consonants, twenty-one vowel letters, four tonal symbols, ten numbers, and fourteen punctuation marks. The appearance of a Thai character is shown in Table 1. Properties of Thai script are similar to that of neighboring countries, i.e., Lao [18] and Khmer [19]. The similarity of these languages is represented by an example word, which means “Thank you” in Fig. 4.Thai script differs markedly from Latin script, such as English. The first difference is the size of the characters. The size of Thai characters varies from a very small component (i.e., a tonal) to high and wide characters (i.e., consonant). Moreover, the small characters, for example the tonal or small vowels, play an important role in word meaning. If the small component is modified, the word meaning will be totally changed, as shown in Fig. 1. This small component is a troublesome component in the noise reduction process because its properties are similar to a speckle noise component in a document image. The second difference is character alignment. The position of English script is on the baseline, whereas the Thai script position is on four levels, e.g., uppermost, upper, middle and lower baseline. The position of the Thai character is illustrated in Fig. 2. The last difference is separator symbols. The English script has a space, comma, colon or other punctuation as a word separator and a full stop as a sentence separator. Thai script, on the other hand, does not have any word or sentence separator. The different properties of English and Thai scripts are indicated by an example of a Thai sentence and its translation in Fig 3.These differences are the cause of difficulty in Thai document analysis, specifically in the noise reduction process. A noise reduction approach that works properly in an English document could provide a pitiful result when applying it to a Thai document. Although many topics in Thai document analysis such as Thai OCR have been widely investigated [20–23], a study on a noise reduction method for Thai or non-Latin script has received considerably less attention [24].One problem when applying machine learning methods to the noise reduction domain is how to label a large number of training examples. Hence, the semi-supervised techniques can be useful for this problem.Semi-supervised learning is a machine learning technique between supervised and unsupervised learning. The supervised learning algorithm, such as k-nearest neighbor, neural network or decision tree, requires a sufficient number of labeled training examples to create a classifier. The unsupervised learning algorithm, such as k-means clustering, uses only unlabeled training examples to divide all examples into various groups. Semi-supervised learning utilizes both the labeled and unlabeled examples to create the classifier. This approach takes advantage of the unlabeled example to increase number of labeled examples and uses them to create a classifier [25].There are many approaches for semi-supervised classification. One efficient approach used in this work is cluster-and-label [26,27]. The cluster-and-label semi-supervised classification is appropriated to the problem in which (1) the example can be clustered; and (2) a proper clustering algorithm for current class distribution is available [25]. The cluster-and-label approach uses a clustering technique to group all examples, both labeled and unlabeled. Then, unlabeled examples in each cluster are labeled as a majority class of the labeled example in that cluster [26,28]. The increased labeled examples are combined with a prior labeled example and used to train a final classifier. This cluster-and-label approach has been successfully applied to the text classification domain [29].An overview process of our semi-supervised noise reduction process is presented in Fig. 5. The proposed semi-supervised noise reduction process consists of four steps, namely, preprocessing, feature extraction, cluster-and-label and classifier creation. A description of each step is shown in this section.The main goal of preprocessing is to extract valid information from a document paper. Initially, the document paper is scanned as a binary image. The scanned image orientation is corrected by a skew correction.The substantial information in a document image is then extracted by OpenCV [30], a well-known image processing library. First, a connected component in an image is detected by selecting a group of black pixels or adjacent pixels in eight directions. Then, the extracted component is justified by an edge smoothing technique [31]. This edge smoothing is a crucial task because an uneven edge could distort a component’s structure, as shown in Fig. 6. Finally, a component structure or a skeleton is extracted by the thinning algorithm. We use a one-pass parallel thinning algorithm [32] that is suggested for thinning a component in the Thai-OCR process [33].The feature or attribute that is broadly used in previous noise reduction studies is a noise property. A defined noise property is used to reduce a specific type of noise whose study intends to reduce. On the contrary, in our study, we aim to reduce an overall noise whose characteristic is indefinite. As a result, we use character property for distinguishing a character from a noise component. The used character property consists of a Thai character structure, i.e., width, height, ratio, density, thickness, upper leg, lower leg, junction point and loop. The feature of the Thai character structure is derived from the ones used in Thai-OCR methods [34–36]. The descriptions of each character structure feature are described in Table 2. The used feature is based on the same set used in our previous works [12,13], except the “position”. The position feature is a component position with respect to a line of writing. Because the line of writing is hardly detected in a highly degraded document image, the position feature was excluded in this research.The semi-supervised classification, cluster-and-label approach is used to create a classifier for noise and character. The cluster-and-label approach increases the number of labeled examples by clustering examples into groups and labeling each group. There is a two-step process, i.e., clustering process and labeling process.In the clustering process, we use self-organizing maps (SOM), which were invented by Teuvo Kohonen [37]. SOM is a type of unsupervised neural network with one input layer and one output layer. The number of input nodes equals to the number of attributes, which is nine in this work, and the number of output nodes is set to five by five nodes. The output nodes are connected to each other as a lattice and are fully connected to all input nodes. An example of the SOM structure is presented in Fig. 7.I is a set of input nodes, O is a set of output nodes, wijis a weight of a link between input node i and output node j, x(t) is a vector of training samples with t dimensions, N is the number of training samples, K is the number of neighborhoods, and η is a neighborhood function. The training process of SOM is presented in Algorithm 1.First, the weight of all nodes is initialized with random values. The training process considers each training sample value and calculates the similarity score of each output node with a comparison metric such as Euclidean. Then, the best match node or the winner node and its neighborhood are identified. Finally, the weight of the winner node and neighborhood are updated to close the training example with the neighborhood function such as Gaussian. A training process continues until it is met a stopping condition such as the number of rounds. The result of the training process is a meaningful SOM lattice.To cluster with SOM, the training examples, both labeled and unlabeled examples, are mapped to the output nodes. The examples that are mapped into the same node are grouped to the same cluster [26]. The cluster may be a pure-class cluster or a mixed-class cluster depending on the classes of labeled examples in that cluster. If all labeled examples in the cluster have the same class label, the cluster is the pure-class cluster. On the contrary, if labeled examples have different class labels, the cluster is the mixed-class cluster.The next step is the labeling process. The labeling process will assign a class label for an unlabeled example. In the pure-class cluster, the example labeling is straightforward. The unlabeled examples in each pure-class cluster are labeled as the class of labeled examples in this cluster. On the other hand, the example labeling in the mixed-class cluster is obscure. The two methods used to label the example in the mixed-class cluster are the conventional majority vote and the proposed method, namely, feature selected sub-cluster labeling.Majority vote labeling is a traditional method to label the examples in the mixed-class cluster [26,28]. The unlabeled example is labeled for the majority class of labeled examples in the cluster. The majority class is the most frequent class whose ratio of examples with this class is greater than a defined threshold. To determine the majority class, the number of labeled examples in each class is first counted as in Eq. 1.(1)Ncj=∑i=1nfcj(xi)(2)fcj(xi)={1ifyi=cj0ifyi≠cjwhere cjdenotes the class label,Ncjis the number of labeled examples of class cj, n is the number of examples in the cluster and yiis the class label of example xi. Then, a mixed-class ratio in each cluster is calculated by Eq. 3.(3)mixed-classratio=maxj=1C(Ncj)NlwhereNcjis the number of labeled examples of class cj, C is the number of class labels, and Nlis the number of labeled examples in the cluster.The mixed-class ratio is the number of the most frequent class examples divided by the number of all labeled examples. If the mixed-class ratio is greater than the predefined threshold, the examples will be labeled as the most frequent class in the cluster. If not, the examples are left as undefined class examples. The threshold value, which is between 0.5 and 1, determines a label assignment in the mixed-class cluster. When the threshold equals to one, no examples in the mixed-class cluster are labeled, and all examples are left as undefined class examples [26]. When the threshold equals to 0.5, all examples in the mixed-class cluster were labeled by the most frequent class [28]. Although the majority vote with threshold correctly labeled most of the unlabeled examples, many unlabeled examples were incorrectly labeled. Most of the incorrect labeling examples belonged to the mixed-class cluster because the actual class of these examples is probably different, but the majority vote with threshold labels all of these examples with the same class, i.e., a majority class. The incorrect labeling examples will be used to train a classifier and may lead to poor classifier performance.This feature selected sub-cluster labeling is a proposed labeling algorithm to improve the accuracy of labeling examples in the mixed-class cluster. The proposed method uses a feature subset that correlates to example classes to divide examples in the mixed-class cluster into sub-clusters. The overview process is presented in Fig. 8.The feature selected sub-cluster labeling algorithm is described in Algorithm 2, where x is an example, y is class label of example, C is a cluster and f is a feature set.In each cluster, the majority class or the most frequent class of the labeled example is determined. Then, it is used to label unlabeled examples in the pure-class cluster. Otherwise, the SplitCluster is performed in the mixed-class cluster. First, the new feature set is selected by a feature selection algorithm. In this work, information gain [38] is used. The information gain feature selection is performed on labeled examples in each cluster to select the best feature with respect to the classes of labeled examples. The recently selected features are then used to cluster examples of each mixed-class cluster into fine clusters. This sub-clustering process is performed by an agglomerative clustering algorithm [39]. Finally, unlabeled examples in each sub-cluster are labeled with a majority class of labeled examples in the sub-cluster.A major benefit of this labeling method is that it establishes a specific decision boundary of the classes in the mixed-class cluster. This decision boundary more accurately aids labeling for the examples in the mixed-class cluster.The increased labeled examples by cluster-and-label are combined with prior labeled training examples. These examples are used to train a final classifier for noise and character. In this experiment, we use a simple k-nearest neighbor as our classification algorithm.

@&#CONCLUSIONS@&#
