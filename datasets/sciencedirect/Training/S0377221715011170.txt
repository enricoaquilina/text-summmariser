@&#MAIN-TITLE@&#
Decomposition approaches for recoverable robust optimization problems

@&#HIGHLIGHTS@&#
We present two decomposition approaches for recoverable robust optimization.We prove that the LP-relaxation of one of the two models is stronger than the other.We present a column generation algorithm for the size-robust knapsack problem.We present a column generation algorithm for the demand robust shortest path problem.Our approach is very promising and can be generalized to many other problems.

@&#KEYPHRASES@&#
Recoverable robustness,Column generation,Branch-and-price,Knapsack,Shortest path,

@&#ABSTRACT@&#
Real-life planning problems are often complicated by the occurrence of disturbances, which imply that the original plan cannot be followed anymore and some recovery action must be taken to cope with the disturbance. In such a situation it is worthwhile to arm yourself against possible disturbances by including recourse actions in your planning strategy. Well-known approaches to create plans that take possible, common disturbances into account are robust optimization and stochastic programming. More recently, another approach has been developed that combines the best of these two: recoverable robustness. In this paper, we solve recoverable robust optimization problems by the technique of branch-and-price. We consider two types of decomposition approaches: separate recovery and combined recovery. We will show that with respect to the value of the LP-relaxation combined recovery dominates separate recovery. We investigate our approach for two example problems: the size robust knapsack problem, in which the knapsack size may get reduced, and the demand robust shortest path problem, in which the sink is uncertain and the cost of edges may increase. For each problem, we present elaborate computational experiments. We think that our approach is very promising and can be generalized to many other problems.

@&#INTRODUCTION@&#
Most optimization algorithms rely on the assumption that all input data are deterministic and known in advance. However, in many practical optimization problems, such as planning in public transportation or health care, data may be subject to changes. To deal with this uncertainty, different approaches have been developed. In case of robust optimization (see Ben-Tal, Ghaoui, and Nemirovski (2009); Bertsimas and Sim (2004)) we choose the solution with minimum cost that remains feasible for a given set of disturbances in the parameters. In case of stochastic programming (Birge & Louveaux, 1997), we take first stage decisions on basis of the current information and, after the true value of the uncertain data has been revealed, we take the second stage or recourse decisions. The objective here is to minimize the cost of the first stage decisions plus the expected cost of the recourse decisions. The recourse decision variables may be restricted to a polyhedron through the so-called technology matrix (Birge & Louveaux, 1997). Summarized, robust optimization wants the initial solution to be completely immune for a predefined set of disturbances, while stochastic programming includes a lot of options to postpone decisions to a later stage or change decisions in a later stage.Recently, the notion of recoverable robustness (Liebchen, Lübbecke, Möhring, & Stiller, 2009) has been developed, which combines robust optimization and second-stage recovery options. Recoverable robust optimization computes solutions, which for a given set of scenarios can be recovered to a feasible solution according to a set of pre-described, fast, and simple recovery algorithms. The main difference between recoverable robustness and stochastic programming is the way in which recourse actions are limited. The property of recoverable robustness that recourse actions must be achieved by applying a simple algorithm instead of being bounded by a polyhedron makes this approach very suitable for combinatorial problems. As an example, consider the planning of buses and drivers in a large city. We may expect that during rush hours buses may be delayed, and hence may be too late to perform the next trip in their schedule. In case of robust optimization, we can counter this only by making the time between two consecutive trips larger than the maximum delay that we want to take into account. This may lead to a very conservative schedule. In case of recoverable robustness, we are allowed to change, if necessary, the bus schedule, but this is limited by the choice of the recovery algorithm. For example, we may schedule a given number of stand-by drivers and buses, which can take over the trip of a delayed driver/bus combination. Especially in the area of railway optimization recoverable robust optimization methods have gained a lot of attention (see e.g. Caprara, Galli, Kroon, Maróti, and Toth (2010); Cicerone et al. (2009)).In this paper we present a Branch-and-Price approach for solving recoverable robust optimization problems. We present two new types of solution approaches: Separate Recovery Decomposition (SRD) and Combined Recovery Decomposition (CRD). These approaches can be used to model many problems; we will test them on the size robust knapsack problem and on the demand robust shortest path problem.This paper extends our conference paper (Bouman, van den Akker, Hoogeveen, Demetrescu, & Halldorsson, 2011) by presenting a general definition of our decomposition approaches, a proof that the LP-relaxation of the CRD model dominates the LP-relaxation of the SRD model, and a further study of the solution algorithm for the demand robust shortest path problem. This study includes different column generation strategies and elaborate computational experiments.To the best of our knowledge, Bouman et al. (2011) and this paper are the first ones applying column generation to recoverable robust optimization. Another decomposition approach, namely Benders decomposition, is used by Cacchiani, Caprara, Galli, Kroon, and Maróti (2008) to assess the Price of Recoverability for recoverable robust rolling stock planning in railways.The remainder of the paper is organized as follows. In Section 2, we define the concept of recoverable robustness. In Section 3, we present our two different decomposition approaches, and we show the general result that the LP-relaxation of the CRD Model is stronger than the LP-relaxation of the SRD model. In Section 4, we consider the size robust knapsack problem. We investigate the two decomposition approaches in a branch-and-price framework and we present computational experiments in which we compare different solution algorithms. Besides algorithms based on Separate and Combined Recovery Decomposition, we test hill-climbing, dynamic programming, and branch-and-bound. The experiments indicate that Separate Recovery Decomposition performs best. Section 5 is devoted to the demand robust shortest path problem. Since Separate Recovery Decomposition does not seem to be appropriate for this problem, we focus on Combined Recovery Decomposition and consider the settings of the branch-and-price algorithm in more detail. In our experiments we show that the column generation strategy has a significant influence on the computation time. Finally, Section 6 concludes the paper.In this section we formally define the concept of recoverable robustness. We are given an optimization problemP=min{f(x)|x∈F},wherex∈Rnare the decision variables, f is the objective function, and F is the set of feasible solutions.Disturbances are modeled by a set of discrete scenarios S. We use Fsto denote the set of feasible solutions for scenario s ∈ S, and we denote the decision variables for scenarios s by ys. The set of algorithms that can be used for recovery are denoted byA,whereA(x,s)∈Adetermines a feasible solution ysfrom a given initial solution x in case of scenario s. In case of planning buses and drivers a scenario corresponds to a set of bus trips that are delayed, and the algorithms inAdecide about the use of standby drivers.The recovery robust optimization problem is now defined as:RRPA=min{f(x)+g({cs(ys)|s∈S})|x∈F,A∈A,∀s∈Sys=A(x,s)}.Here, cs(ys) denotes the cost associated with the recovery variables ys, and g denotes the function to combine these cost into the objective function. There are many possible choices for g. A few examples are as follows:1.g({cs(ys)=0. This models the situation where our only concern is the feasibility of the recovered solutions.g({cs(ys)=maxs∈Scs(ys),that is, it models the maximal cost of the recovered solutions ys. This corresponds to minimizing the worst-case cost. If cs(ys) measures the deviation of the solution ysfrom x, we minimize the maximum deviation from the initial solution. Note that this deviation may also be limited by the recovery algorithms.g({cs(ys)=∑s∈Spscs(ys),where psdenotes the probability that scenarios s occurs. This corresponds to minimizing the expected value of the solution after recovery.In the remainder of the paper we will consider applications that use either a function of sum or max type for g({cs(ys).Although earlier papers on recoverable robustness (e.g. Liebchen et al. (2009)) consider the latter type of definition of g as two-stage stochastic programming, we think that the requirement of a pre-described easy recovery algorithm makes this definition fit into the framework of recoverable robustness.We discuss two decomposition approaches for recovery robust optimization problems. In both cases we reformulate the problem such that we have to select one solution for the initial problem and one for each scenario. The difference consists of the way we deal with the scenarios.In Separate Recovery Decomposition, we select an initial solution and separately we select a solution for each scenario. This means that for each feasible initial solution k ∈ F we have a decision variable xksignaling if this solution is selected; similarly for each feasible solution for each scenario q ∈ Fswe have a decision variableyqs. In the formulation we enforce that we select exactly one initial solution and one solution for each scenario. The recovery constraints enforces that for each scenario the initial solution can be transformed into a feasible solution by the given recovery algorithm. We assume that the recovery constraint and the objective function can be expressed linearly. We now obtain an Integer Linear Programming formulation which is formulated as follows (for maximization objective):max∑k∈Fckxk+∑s∈S∑q∈Fscqsyqssubject to(1)∑k∈Fxk=1(2)∑q∈Fsyqs=1foralls∈S(3)A1x+A2sys≤bsforalls∈S(4)xk∈{0,1}forallk∈F(5)yqs∈{0,1}forallq∈Fs,s∈SHere ysdenotes the vector of all entriesyqs. Constraints (1) and (2) state that exactly one solution is selected in the initial phase and for each scenario, respectively. Constraints (3) model the fact that for each scenario s ∈ S the solution ysshould be obtained from x by the given recovery algorithm. We want to solve this ILP formulation using Branch-and-Price (Barnhart, Johnson, Nemhauser, Savelsbergh, & Vance, 1998).The column generation process is depicted in Fig. 1.Since the variables xkmodel initial solutions, we have that for these variables the pricing problem boils down to an optimization problem over the set of initially feasible solutions F, i.e. a variant of the initial problem. Similarly, for variablesyqsfor scenario s the pricing problem becomes an optimization problem over Fs, i.e. a variant of the original problem in case of scenario s.In Combined Recovery Decomposition, we select for each scenario a combination of an initial solution together with the optimal recovery of a solution for that single scenario. This means that for each scenario s ∈ S we have for each combination of an initial solution k ∈ F and the corresponding solution q ∈ Fsobtained by the recovery algorithm a binary variablezkqssignaling if this combination is selected. Obviously, the combinations selected for the different scenarios must all correspond to the same initial solution. To model this we first assume that any initial solution k can be described by the parameters aik(i=1,…,l),whereaik=1if the initial solution k possesses characteristic i, andaik=0otherwise. Next, we introduce binary variables vi(i=1,…,l)to indicate whether the chosen initial solution possesses characteristic i. This leads to the following Integer Linear Programming formulation, where again we assume that we can express the functions f and g in a linear way.max∑i=1lfivi+∑s∈S∑(k,q)∈F×Fscqszkqssubject to(6)∑(k,q)∈F×Fszkqs=1foralls∈S(7)vi=∑k∈Faik(∑q∈Fszkqs)foralli∈{1,…,l},s∈S(8)zkqs∈{0,1}forallk∈F,q∈Fs,q=A(k,s),s∈S.Recall that A(k, s) denotes the result of the recovery algorithm applied to solution k for scenario s. The binary character of the v variables is guaranteed by the combination of Constraints (7) and (8). Constraint (7) ensures that the same initial solution is selected for each scenario; this initial solution can be constructed using the binary variables vi.We also solve this ILP formulation by Branch-and-Price. The column generation process is depicted in Fig. 2. The Pricing Problem now boils down to finding an optimal combination of an initial solution and a recovery solution for a given scenario.Our approach can be applied to different recoverable robust optimization problems. In the next two sections, we will demonstrate the approach for two different problems. First, we will compare the two models with respect to the strength of the LP-relaxation.Since we solve both formulations through Branch-and-Price, we solve the LP-relaxation to find an upper bound. We will show that the LP-relaxation of combined recovery decomposition gives a stronger bound than the LP-relaxation of the separate recovery decomposition. In the proof, we need the following lemma.Lemma 1For any feasible solution of the LP-relaxation of the CRD formulation we have that∑q∈Fszkqs≡zks=zkfor all k ∈ F.Let A denote the (l × |F|) matrix with entries aik(i=1,…,l;k=1,…,|F|). Then Constraints (7) can be expressed asv=Azs,for all s ∈ S, wherezs=(z1s,…,z|F|s)Tandv=(v1,…,vl)T. Using the linearity of A, we find thatA(zs1−zs2)=0for any s1, s2 ∈ S. Without loss of generality, we may assume that A has rank l, since we can remove the redundant constraints otherwise. Hence, we may conclude thatz1=z2=…=z|S|.□LetZLPSRDandZLPCRDbe the solution value of the LP-relaxation of the Separate and Combined Recovery Decomposition model, respectively. ThenZLPSRD≥ZLPCRD,i.e. the LP-relaxation of combined recovery decomposition is stronger.Let(v˜,z˜)be an optimal solution to the LP-relaxation of the Combined Recovery Decomposition model. We will give a solution(x¯,y¯)to the Separate Recovery Decomposition model, and show that it is feasible and has the same objective value as(v˜,z˜). We define:x¯k=∑q∈F1z˜kq1forallk∈Fy¯qs=∑k∈Fz˜kqsforalls∈S,q∈FsRemark that in our definition ofx¯kwe could have used the valuesz˜kqsfor any scenario, because of Lemma 1. We first check the constraints.We have that∑k∈Fx¯k=∑k∈F∑q∈F1z˜kq1=1because of Constraint (6). Hence, Constraint (1) holds. In the same way,∑q∈Fsy¯qs=∑q∈Fs∑k∈Fz˜kqs=1because of Constraint (6). Hence, Constraint (2) holds for each s ∈ S.Next, we have to show thatx¯kandy¯qssatisfy the recovery constraints, which are defined asA1x+A2sys≤bsforalls∈S.Let s be any scenario; we will show that the constraint holds for this scenario. We defineekxas the unit vector of length |F| with a 1 on position k; similarly, we defineeqsas the unit vector of length |Fs| with a 1 on position q. Since within a variablez˜kqs,we have that solution q is obtained by applying the recovery algorithm to initial solution k we must have thatA1ekx+A2seqs≤bsforallk∈F;q∈Fs.For each k ∈ F and q ∈ Fswe multiply these inequalities withz˜kqsand add them up to obtain a convex combination of the recovery constraints. This yields∑k∈F∑q∈Fsz˜kqs[A1ekx+A2seqs]≤∑k∈F∑q∈Fsz˜kqsbs=bs.Since the recovery constraints are linear, this can be rewritten asA1∑k∈F[∑q∈Fsz˜kqs]ekx+A2s∑q∈Fs[∑k∈Fz˜kqs]eqs≤bs.Using Lemma 1, we have thatx¯k=∑q∈F1z˜kq1=∑q∈Fsz˜kqs;and hence we can substitutex¯kandy¯qsto obtainA1∑k∈Fx¯kekx+A2s∑q∈Fsy¯qseqs≤bs.ConsequentlyA1x¯+A2sy¯≤bs,implying that(x¯,y¯)satisfies Constraint (3). Clearlyx¯kandy¯qsare in [0, 1], which implies that the solution is feasible. Now we consider the objective value.Remark thatck=∑i=1lfiaik. It follows that∑k∈Fckx¯k+∑s∈S∑q∈Fscqsy¯qs=∑k∈Fck∑q∈F1z˜kq1+∑s∈S∑q∈Fscqs∑k∈Fz˜kqs=∑k∈F∑i=1lfiaik(∑q∈F1z˜kq1)+∑s∈S∑k∈F∑q∈Fscqsz˜kqs=∑i=1lfi∑k∈Faik(∑q∈F1z˜kq1)+∑s∈S∑k∈F∑q∈Fscqsz˜kqs.By Constraint (7), the above expression equals∑i=1lfiv˜i+∑s∈S∑k∈F∑q∈Fscqsz˜kqs,which is equal to the cost of(v˜,z˜).□We consider the following knapsack problem. We are given n items, where item j(j=1,…,n)has revenue cjand weight aj. Each item can be selected at most once. The knapsack size is b. We define the size robust knapsack problem as the knapsack problem where the knapsack size b is subject to uncertainty. We denote by bs< b the size of the knapsack in scenario s ∈ S. We assume that the knapsack will keep its original size with probability p0 and that scenario s will occur with probability ps. Our objective is to maximize the expected revenue after recovery. We study the situation in which recovery has to be performed by removing items. As soon as it becomes clear which scenario s appears, recovery is performed by removing items in such a way that the remaining items give a knapsack with maximal revenue and size at most bs. This boils down to solving a knapsack problem where the item set is the set of items selected in the initial solution and the knapsack size is bs. Hence, our set of recovery algorithms is given by the dynamic programming algorithm for solving these knapsacks. For ease of exposition, we consider the binary knapsack problem. Our approach can directly be generalized to the integral knapsack problem.Recently, Büsing, Koster, and Kutschka (2011a) have studied a different version of our knapsack problem, where they focus on uncertainty in the weights and in the items. They showNP-hardness of several variants of the problem and develop a polyhedral approach to solve these problems. In a follow-up paper, Büsing et al. (2011b) present an integer linear programming formulation of quadratic size and evaluates the gain of recovery. With respect to approximation algorithms, Disser et al. (2014) consider policies for packing a knapsack with unknown capacity and Goerigk et al. (2014) consider the knapsack problem in which there is a limited budget to decrease item weights.In this section, we discuss our two decomposition approaches for the size robust knapsack problem and present elaborate computational experiments in which we compare our method with other algorithms.We define K(b) as the set of feasible knapsack fillings with size at most b. For k ∈ K(b), we denote its revenue byCk=∑i∈kci. In the same way, we denote the revenue of q ∈ K(bs) byCqs=∑i∈qci.We define decision variables:xk={1ifknapsackk∈K(b)isselected,0otherwise.andyqs={1ifknapsackq∈K(bs)isselectedforscenarios,0otherwise.The problem can now be formulated as follows:maxp0∑k∈K(b)Ckxk+∑s∈Sps∑q∈K(bs)Cqsyqssubject to(9)∑k∈K(b)xk=1(10)∑q∈K(bs)yqs=1foralls∈S(11)∑k∈K(b)aikxk−∑q∈K(bs)aiqsyqs≥0foralli∈{1,2,…,n},s∈S(12)xk∈{0,1}forallk∈K(b)(13)yqs∈{0,1}forallq∈K(bs),s∈S,where the parameters aikandaiqsare defined as follows:aik={1ifitemiisinknapsackk∈K(b),0otherwise.andaiqs={1ifitemiisinknapsackq∈K(bs),0otherwise.In the above model Constraint (9) states that exactly one knapsack is selected for the original situation and Constraints (10) that exactly one knapsack is selected for each scenario. Constraints (11) ensures that recovery is done by removing items.Recall that we solve this ILP formulation using Branch-and-Price; thereto, we relax the integrality Constraints (12) and (13) into xk≥ 0 andyks≥0,and solve this LP-relaxation by column generation.The pricing problemFrom the theory of linear programming it is well-known that for a maximization problem increasing the value of a variable will improve the current solution if and only if its reduced cost is positive. The pricing problem then boils down to maximizing the reduced cost.Let λ, μs, and πisbe the dual variables of Constraints (9)–(11), respectively. Now the reduced cost cred(xk) of xkis given bycred(xk)=p0∑i∈kci−λ−∑i=1n∑s∈Saikπis=∑i=1naik(p0ci−∑s∈Sπis)−λ.Observe that λ and μ are free variables, and πis≤ 0. The pricing problem is to find a feasible knapsack for the original scenario, where the revenue of item i, equals(p0ci−∑s∈Sπis). This is just the original knapsack problem with modified objective coefficients. Similarly the reduced costcred(yqs)are given bycred(yqs)=∑i=1naiqs(psci+πis)−μs.Therefore, the pricing is exactly the knapsack problem with knapsack size bsand modified objective coefficients. Note that in the pricing problem an item may have a negative revenue. Clearly such items can be discarded.To find an integral solution, we are going to apply Branch-and-Price. We branch on items that are fractional in the current solution, i.e. items i for which ∑k ∈ K(b)aikxkis fractional. This is easily combined with column generation, since enforcing that an item is taken in or omitted from the knapsack can easily be included in the pricing problem. If all values ∑k ∈ K(b)aikxkare integral, then a single initial knapsack is selected with value 1. Now consider a basic solution to the LP for scenario s. It is easy to see that this solution contains an optimal subset of the initial knapsack with total weight at most bswhich is selected with value 1. Consequently, the solution is integral.In contrast to the Separate Recovery Decomposition, we consider fillings of the initial knapsack in combination with the optimal recovery for one scenario. Consequently, we introduce decision variables:zkqs={1ifthecombinationofinitialsolutionk∈K(b)andrecoverysolutionq∈K(bs)isselectedforscenarios,0otherwise.Clearly,zkqsis only defined if q is a subset of k. The ILP model further contains the original variable xisignaling if item i is contained in the initial knapsack. We can formulate the problem as follows:maxp0∑i=1ncixi+∑s∈Sps∑(k,q)∈K(b)×K(bs)Cqszkqssubject to(14)∑(k,q)∈K(b)×K(bs)zkqs=1foralls∈S(15)xi−∑(k,q)∈K(b)×K(bs)aikzkqs=0foralli∈{1,2,…,n},s∈S(16)xi∈{0,1}foralli∈{1,2,…,n}(17)zkqs∈{0,1}forallk∈K(b),q∈K(bs),s∈S,Constraints (14) enforce that exactly one combination is selected for each scenario; Constraints (15) ensure that the same initial knapsack filling is selected for all scenarios.Again, we are going to solve the LP-relaxation by column generation. We include the variables xiin the restricted master LP and, hence pricing is only performed for the variableszkqs. We denote the dual variables of Constraints (14) and (15) by ρsand σis, respectively. The reduced cost ofzkqsis now equal to:cred(zkqs)=∑i=1naiqspsci+∑i=1naikσis−ρs.We solve the pricing problem for each scenario separately. We have to find an initial and recovery solution. This can be solved by dynamic programming. The main observation is that there are three types of items: items included in both the initial and recovery knapsack, items selected for the initial knapsack, but removed by the recovery, and non-selected items. We define state variables D(i, w0, ws) as the best value for a combination of an initial and recovery knapsack for scenario s, such that the initial knapsack is a subset of{1,2…,i},the recovery knapsack is a subset of the initial knapsack, and the initial and recovery knapsack have weight w0 and ws, respectively. The recurrence relation is as follows:D(i,0,0)=0∀iD(0,w0,ws)=−∞forw0,ws>0D(i,w0,ws)=max{D(i−1,w0,ws)D(i−1,w0−ai,ws)+σisD(i−1,w0−ai,ws−ai)+σis+psciWe performed extensive computational experiments with the knapsack problem. The algorithms were implemented in the Java Programming language and the Linear Programs were solved using ILOG CPLEX 11.0. All experiments were run on a PC with an Intel®Core™Duo 6400 @ 2.13 GHz processor with 1 GB of RAM. Our experiments were performed in three phases. Since we want to focus on difficult instances, in the first phase we tested 12 different instance types to find out which types are the most difficult. We ran two algorithms: the separate recovery branch-and-price and a hill climbing algorithm. Our instance types are based on the instance types by Pisinger (2005), where we have to add the knapsack weight bsand the probability psfor each of the scenarios. In the second phase, we tested five different algorithms on relatively small instances with up to 25 items and up to 8 scenarios. In the third phase we tested the best two algorithms from the second phase on larger instances with 50 or 100 items and up to 20 scenarios. In this section, we will present the most important observations from the second and third phase. For further details we refer to Bouman (2011).In the second phase we tested the following 5 instance classes withR=30:•The almost strongly correlated instances: pick aiuniformly from [1, R] and ciuniformly from[ai+R/10−R/500,ai+R/10+R/500].The inverse strongly correlated instances: pick ciuniformly from [1, R] and aifrom[ci+R/10].The circle(d) instances: pick aiuniformly from [1, R] andci=d4R2−(ai−2R)2,where d is set to23.The span(v, m) uncorrelated instances. Create v items withaj′andcj′from [1, R] and divideaj′andcj′bym+1. The knapsack instance is then generated by repeatedly choosing one of the v combinations(aj′,cj′)and a random value μifrom [1, m]: the created item getsai=μiaj′andci=μicj′.The subset sum instances, whereai=cidrawn from [1, R].We considered instances with 5, 10, 15 and 25 items and with 2, 4, 6 and 8 scenarios (except for the instances with 5 items, for which we only considered 2 and 4 scenarios). For each combination we generated 100 item sets (20 from each instance class). For each item set we generated 3 sets of scenarios, with large, middle, and small values of bsrelative to b, where large, middle, and small mean that the values bsare drawn uniformly from[12b,b],[38b,68b],and[14b,12b],respectively. This means that we considered 4200 ((2+3×4)×100×3) instances in total.We report results on the following algorithms:•Separate Recovery Decomposition with Branch-and-Price, where we branch on the fractional item with largestcjajratio and first evaluate the node which includes the item.Combined Recovery Decomposition with Branch-and-Price, where we branch in the same way as in Separate Recovery Decomposition.Branch-and-Bound where we branch on the fractional item with smallestcjajratio and first evaluate the node which includes the item. In every node, the optimal recovery for the current set of items is computed using dynamic programming for the regular knapsack problem. When a node adds a new item, the DP-table of the parent node is extended with only this item in order to keep the computation efficient. The upper bounds are computed by solving the LP-relaxation, which is solved by including the items in order ofcjajratio in the initial solution and in each scenario.Dynamic programming. Our DP resembles the DP for the pricing problem in case of Combined Recovery Decomposition. We use state variablesD(i,w0,w1,…,w|S|),where wj(j=1,…,|S|)corresponds to scenario j. The recurrence relation gets exponential size now, since, if item k is included in the initial solution, we must compute for each possible subset of S the value of including or omitting this item. Hence, the running time becomes O(n2|S|).Hill Climbing. We apply neighborhood search on the initial knapsack only and compute for each initial knapsack the optimal recovery by Dynamic Programming. Hill climbing performs 100 restarts.For the branching algorithms we tested different branching strategies. All our algorithms branch on whether an item should be included in the initial knapsack. The different branching strategies select which item should be branched on first and whether the item should first be included or excluded. The first item is a combination of best/worst according to weight/ratio/profit. In total this yields 12 branching strategies. In the Branch-and-Price algorithms the difference in performance turned out to be minor and we report on the strategy that performed best in Separate Recovery Decomposition. However, in the Branch-and-Bound algorithm some difference could be observed and we report on the strategy that shows the best performance for this algorithm.The results of the second phase are given in Table 1. For each instance and each algorithm, we allowed at most 3000 milliseconds of computation time. For each algorithm, we report on the number of instances (out of 4200) that could not be solved within 3000 milliseconds, and the average and maximum computation time over the successful instances. For Hill Climbing we compute for each instance the performance ratio, which is equal to the value of the obtained solution c divided by the value of the optimum solution c*; in the table we report the average and worst performance ratio. For the branching algorithms we further report the average and maximum number of evaluated nodes. For Hill Climbing ‘Failed’ is not applicable.The results indicate that for this problem Separate Recovery Decomposition outperforms Combined Recovery Decomposition. DP is inferior to Branch-and-Bound and Hill Climbing. The fact that Branch-and-Bound requires more nodes than the Separate and Combined Recovery Branch-and-Price indicated that the decomposition models have a stronger LP-relaxation than the standard ILP based on variables for each item and scenario.In the third phase we experimented with larger instances for the two best algorithms. We present results for instances with 50 and 100 items and 2, 3, 4, 10, or 20 scenarios. Again, for each combination of number of items, number of scenarios, we generated 100 item sets (20 from each instance class) with 3 scenario sets each. This results in 300 instances per combination of number of items and number of scenarios, where the maximum computation time per instance per algorithm is 4 minutes. The results are depicted in Tables 2and 3.The results suggest that the computation time of Separate Recovery Decomposition scales very well with the number of scenarios. As may be expected, Hill Climbing shows a significant increase in the computation time when the number of scenarios is increased. Moreover, the small number of nodes indicates that Separate Recovery Decomposition is well-suited for instances with a larger number of scenarios. On average the quality of the solutions from Hill Climbing is very high. However, a worst performance ratio of about 0.66 shows that there is no guarantee of quality. Hill Climbing succeeded to complete the algorithm with 100 restarts for all instances.The demand robust shortest path problem is an extension of the shortest path problem that has been introduced by Dhamdhere, Goyal, Ravi, and Singh (2005). We are given a graph (V, E) with cost ceon the edges e ∈ E, and a source node vsource ∈ V. The question is to find the cheapest path from source to the sink, but the exact location of the sink is subject to uncertainty. Moreover, the cost of the edges may change over time. More formally, there are multiple scenarios s ∈ S that each define a sinkvsinksand a factor fs> 1 by which the cost of the edges are scaled. To work with the same problem as Dhamdhere et al. (2005), we choose as objective to minimize the cost of the worst case scenario. It is not difficult to see that this problem isNP-hard, as it generalises the Steiner Tree Problem. When we pick each fshigh enough, the optimal solution is to buy a minimum cost tree that connects the source and all sinks during the first phase.In contrast to Büsing (2012), we can buy any set of edges in the initial planning phase. In the recovery phase, we have to extend the initial set such that it contains a path from the source to the sinkvsinks,while paying increased cost for the additional edges. Remark that, when the sink gets revealed, the recovery problem can be solved as a shortest path problem, where the edges already bought get zero cost. Hence, the recovery algorithm is a shortest path algorithm.Observe that the recovery problem has the constraint that the union of the edges selected during recovery and the initially selected edges contains a path from source vsource to sinkvsinks. It is quite involved to express this constraint using linear inequalities, and hence to apply Separate Recovery Decomposition. However, the constraint fits very well into Combined Recovery Decomposition.Our Combined Recovery Decomposition model contains the variable xesignaling if edge e ∈ E is selected initially. Moreover, for each scenario, it contains variables indicating which edges are selected initially and which edges are selected during the recovery:zkqs={1ifthecombinationofinitialedgesetk⊆Eandrecoveryedgesetq⊆Eisselectedforscenarios,0otherwise.Observe thatzkqsis only defined if k and q are feasible, i.e., their intersection is empty and their union contains a path from vsource tovsinks. Finally, it contains zmax defined as the maximal recovery cost.We can formulate the problem as follows:min∑e∈Ecexe+zmaxsubject to(18)∑(k,q)⊆E×Ezkqs=1foralls∈S(19)xe−∑(k,q)⊆E×Eaekzkqs=0foralle∈E,s∈S(20)zmax−∑e∈Efsce∑(k,q)⊆E×Eaeqszkqs≥0foralls∈S(21)xe∈{0,1}foralle∈E(22)zkqs∈{0,1}forallk⊆E,q⊆E,s∈S,where the binary parameters aeksignal whether edge e is in edge set k and the binary parametersaeqssignal whether edge e is in edge set q for scenario s.Constraints (18) ensure that exactly one combination of initial and recovery edges is selected for each scenario; Constraints (19) enforce that the same set of initial edges is selected for each scenario. Finally, Constraints (20) make sure that zmax represents the cost of the worst case scenario.We first relax the integrality Constraints (21) and (22) into xe≥ 0 andzkqs≥0,and solve this LP-relaxation. To deal with the huge number of variables we are going to solve the problem by column generation.The pricing problem Since we have a minimization problem, the pricing problem boils down to minimizing the reduced cost. Let λs, ρes, and πsbe the dual variables of the Constraints (18)–(20) respectively. The reduced cost ofzkqsis now equal to:cred(zkqs)=−λs+∑e∈Eρesaek+∑e∈Eπsfsceaeqs.We solve the pricing problem for each scenario separately. For a given scenario s, the pricing problem boils down to minimizingcred(zkqs)over all feasible aekandaeqs. This means that we have to select a subset of edges that contains a path from vsource tovsinks. This subset consists of edges which have been bought initially and edges which are attained during recovery. The first type corresponds toaek=1and has cost ρesand the second type toaeqs=1and has cost πsfsce. The pricing problem is close to a shortest path problem, but we have two binary decision variables for each edge. However, we can apply the following preprocessing steps:1.First, we select all edges with negative cost. From LP theory it follows that all dual variables πsare nonnegative, and hence, all recovery edges have nonnegative cost. So we only select initial phase edges with negative cost ρes. From now on, the cost of these edges is considered to be 0.The other edges can either be selected in the initial phase or in the recovery phase. To minimize the reduced cost, we have to choose the cheapest option. This means that we can set the cost of an edge equal to min (ρes, πsfsce).The pricing problem now boils down to a shortest path problem with nonnegative cost on the edges and hence can be solved by Dijkstra (1959) algorithm. We implemented the algorithm by a min heap with running time O(|E|log (|V|)).Since we solve the pricing problem for each scenario separately, the following questions arise: ‘For which scenarios do we actually solve the pricing problem?’ and ‘Which columns do we actually add to the restricted LP?’. We investigate the following strategies:•Interleaved: goes through the pricing problems of the different scenarios one by one. As soon as a variable with negative reduced cost is identified, the corresponding column is added and the master problem gets resolved. After that, it goes to the next scenario. When the pricing problem has a solution with nonnegative reduced cost for every scenario the column generation process is stopped.Best: solves the pricing problem for all scenarios, but only a columnzkqswith overall minimal reduced cost is added to the master problem. The master problem is solved again and this repeats itself until the minimal reduced cost is nonnegative.All: solves the pricing problem for all scenarios and adds a column for all scenarios for which a variablezkqswith negative reduced cost was found; after adding all those columns it resolves the master problem.Within the first few experiments it became very clear that the LP problem is very degenerate. Certainly for larger graphs with a lot of scenarios this tends to slow down the computation enormously. Observe that every solution needs at least |S| columns. To get a complete solution, because of Constraint (19), we need a collection of columns such that for each edge e the total amount by which it is selected in the initial solution∑(k,q)⊆E×Eaekzkqsis the same for every scenario s. This has the consequence that, although it is included in the basis, a promising new column often does not influence the primal solution. To deal with this problem, we use the following method: When a column is added, we always guarantee that it can be selected for the solution by generating for every scenario the best column with the same initial edges. Those columns are generated by fixing the set of initial edges and finding the best recovery edges by running Dijkstra’s algorithm for all other|S|−1scenarios.As a starting solution we take the column in which all edges are taken in the initial solution. Other strategies were tested but the differences were small and instance dependent.Moreover, we have investigated column deletion, i.e. deletion of columns with too positive reduced cost. However, this does not seem to work well in combination with including additional columns.If the solution of the LP-relaxation is integral, our problem is solved to optimality. Otherwise, we proceed by Branch-and-Price (Barnhart et al., 1998), i.e. Branch-and-Bound, where we generate additional columns in the nodes of the search tree.In a Branch-and-Price algorithm the branching strategy has to be designed in such a way that we are still able to solve the pricing problem in each node of the tree. In our algorithm we branch on the variables xe. In a node withxe=1we only generate columns where edge e is bought in the initial phase. This implies that in the first preprocessing step of the pricing we buy edge e at cost ρesand then set its cost to 0. In a node withxe=0we are not allowed to buy edge e in the initial phase. Therefore, we have to define the cost of the edge as πsfsceinstead of min (ρesπsfsce).Concerning the choice of the edge for branching, besides considering the edges in order of their index, we implemented branching on the most doubtful edge. This means that the we branch on the edge for which|xe−12|is minimal. This strongly speeds up the computation.Moreover, we investigated different node selection strategies. We considered best bound branching, i.e. branching on the node with the minimal lower bound, breadth first search, depth first search and also best depth first, which from the deepest nodes in the tree selects the one with the best lower bound. In our experiments best depth first did not improve depth first search very much. Although depth first search sometimes slightly improved best bound search, it showed a less stable behavior. The same is true for breadth first search. Therefore we chose to use best bound branching in our algorithm.To compute an upper bound three rounding heuristics were tested. The first heuristic was to select for the initial phase only edges withxe=1in the LP-solution. In the second heuristic, all edges withxe≥12were selected in the initial solution. As a third alternative we applied a randomized strategy: each edge was selected in the initial solution with a probability equal to the value of xein the optimal solution of the LP-relaxation. In all three cases, for each scenario the best recovery solution was determined by Dijkstra’s algorithm. There did not seem to be much difference in performance between the heuristics and we applied the second one since we thought it to be the most intuitive one.We have implemented our column generation and branch-and-price algorithms in Java and used ILOG CPLEX 12.4 as linear programming solver. We ran experiments on an Intel®Core™Duo 2.66 GHz processor with 4 GB of RAM.Again, our experiments were performed in three phases. We first investigated all column generation strategies, to determine the best one. Secondly, we performed a sensitivity analysis. Finally, we ran our algorithm on some larger instances.We first present results for linear programming to illustrate the effect of the different column addition strategies from Section 5.1. The strategies InterA, BestA, and AllA denote extensions of the strategies Inter, Best and All, in which, when we add a column, we also add for each scenario the best column with the same initial edges. In Table 4we give results for 4 different randomly generated relatively small instances, where Gn, ehas a graph with n nodes and e edges. The recovery factor f for these instances is fixed at 2.0 and every non source node is a possible sink and thus a scenario. For each instance, we give the number of iterations (it), the number of added columns (col), and the computation time in milliseconds (t).Our results reveal that the strategies with additional columns strongly speed up the computation. In most of our cases the number of columns is also reduced, but as may be expected, the reduction is not that strong. In some cases (G17, 31 Inter and All) the number of columns increased.We also solved the ILP for these instances, where we applied all combinations of strategies in the root and in the tree. The strategies without additional columns resulted in large running times.The improvement from the additional columns is explained by the fact that these columns enable the solver to actually use every generated ‘interesting’ column in a solution. This is especially important for the combined recovery decomposition model, where each column is a combination of an initial solution and a recovery solution for one scenario. In this model, a feasible solution requires a set of columns that constitute the same initial solution for each scenario. So the additional columns prevent a known problem in column generation: a very good column is generated but can not be used because other columns to complete the solution are lacking.Therefore, we do not feel it necessary to perform more experiments without additional columns. Moreover, it did not pay off to use a different strategy in the root than in the remainder of the tree. Consequently, from now on we only consider strategies with additional columns that use the same strategy for the complete tree.Recall that we branch on the most doubtful edge with|xe−12|minimal and select the node with the best lower bound. We first report results for a setG500of 500 random instances. They are based on graphs with 10 to 29 edges, where for each number of edges we vary the number of nodes. For every number of edges a total of 25 graphs are generated. All graphs are connected, the cost of the edges is drawn uniformly randomly from the interval [0; 100], every non-source is a possible sink, and f has a random value in the interval [1; 10].The average solution times for those instances are 53.6, 21.2, 66.7 and 34.8 seconds for InterA, BestA, and AllA and BestANoSort, where in the latter strategy we branch on the edges in order of their numbering instead of on the most doubtful edge. The BestA method performs significantly better than the other methods according the Wilcoxon signed-rank test (done with R version 3.1.1, withp=3.823e−13as the highest p). Table 5 shows results for a subset of the set of random instancesG500. For each number of edges and each strategy, we report on the average total number of iterations of column generation (it), the average number of nodes in the branch-and-bound tree (nodes), and the average computation time in milliseconds (t).In Fig. 3we plot on a logarithmic scale the computation time for each number of edges.These results suggest that especially for larger graphs BestA outperforms the other column addition strategies. Even when BestA is combined with the inferior branching strategy of branching on edges in lexicographical order, this is faster than the other column addition strategies.We also did some sensitivity experiments on the influence of the edge cost and the recovery factor f. To test the influence of the costs of the edges we used G14, 13 and G17, 31 with fixed recovery factorf=2. We created 500 random cost versions by generating the edge costs uniformly randomly from the interval [0; 50]. For the recovery factor f we generated similar instances only now we fixed the cost and varied the recovery factor in the interval [1; 10]. Because the G17,31 graph with random recovery factor f was solved relatively slowly, we only solved 25 instances. Because these experiments are about sensitivity we report the quartile points, which divide the data into four equal groups, instead of averages. We looked at the iterations, nodes and time separately, and also report the minimum and maximum. These results can be found in Table 6. All instances are solved with the BestA method.These results suggest that cost as well as recovery factor have a large influence on the iterations, nodes and solution time of the instance, this difference can be a factor of more than 100. This might be explained by the fact that some combinations of cost and recovery factor result in alternative solutions with approximately the same value, which have an impact on the size of the search tree. We consider a small example with 3 nodes: one source s and two possible sink t1 and t2 each occurring with probability12. There are two edges (s, t1) and (s, t2) with the same initial cost Q. Iff=2,then it does not make a difference if you buy all edges, one edge, or no edges in the initial phase.The experiments from now on, were performed with a better computer with an Intel®Core™i5 3.40 GHz processor and 8 GB of RAM. This computer is approximately twice as fast. Until now we considered instances were every non-source node could be the sink, which are instances with a relatively large number of scenarios. Since the size of the ILP model is linear in the number of scenarios, we may expect that instances with fewer scenarios can be solved faster. From the setG500used before we generated 42747 new instances by varying the number of scenarios.For each instance in the setG500,we generatemin{(n−1i),10}instances with i scenarios, i.e., i possible sink nodes, where n is the number of nodes of the instance. Hence, for each instance, in total∑i=1n−1min{(n−1i),10}instances are generated. The maximum of 10 instances which can be generated per instance and number of scenarios is used to limit the total number of instances. If(n−1i)>10,we randomly select 10 subsets of size i. For example, when an instance of theG500set has 4 nodes, 7 new instances are generated. Three instances, with 1 scenario are generated, three with 2 scenarios and one instances with 3 scenarios. Note that the instance which has 3 scenarios is an exact copy from theG500instance. For each instance the nodes, edges and the cost of the edges are copied from theG500instance. For each instance the corresponding recovery values fsfrom theG500instance are used for the scenarios.These instances were solved with the BestA method. Solving all 42747 instances took 56.4 hours. In Table 7we show the average computation time in milliseconds for different numbers of edges and different numbers of scenarios.Moreover, in Figs. 4and 5, we plot the computation time on a logarithmic scale, per number of edges as a function of the number of scenarios and per number of scenarios as a function of the number of edges, respectively. A larger version of the figures can be found in the appendix.Our results suggest that the number of edges has a strong impact on the computation time, which indicates exponential behavior. The impact of the number of scenarios, seems very strong in the beginning but then somewhat flattens out in the logarithmic scale. We conclude that both have a strong influence.Finally, we performed experiments for four larger instances, which are generated to get an indication of the boundaries of what can be solved. Because these instances take a long time to solve, only four instances were generated. These graphs are generated in exactly the same way as theG500graphs. Every non source node is a possible sink, the graphs are connected, the cost of the edges are uniformly random from the interval [0; 100], and f has a random value in the interval [1; 10]. In Table 8we show the number of nodes and edges, the time needed to solve the LP, the time needed to solve the ILP, the total solution time, and the number of nodes in the branch-and-bound tree, together with the node in which the best solution was found. For G4 we ran into memory problems while solving the ILP. These results indicate that instances which have 25 nodes and 50 edges or more can take a long time to solve and cannot always be solved with 8 GB of RAM available.We have presented a new approach, based on column generation, for solving recoverable robust optimization problems. The key to this approach are two decomposition methods: Separate Recovery Decomposition (SRD) and Combined Recovery Decomposition (CRD). In the SRD approach, we work with separate solutions for the initial problem and recovery solutions for the different scenarios; in the CRD approach, we work with combined solutions for the initial problem and the recovery problem for a single scenario. We have shown that the LP-relaxation of the CRD model provides a stronger bound than the LP-relaxation of the SRD model.We have tested our approach on two problems. The first one is the size robust knapsack problem, where the knapsack size can decrease, and where we can recover by removing items. Our computational experiments revealed that for this problem Separate Recovery Decomposition outperformed Combined Recovery Decomposition and that the SRD method scaled very well with the number of scenarios. Moreover, SRD outperforms the standard branch-and-bound and dynamic programming approach.We further considered the demand robust shortest path problem. Here we need to buy the edges of a shortest path from a known source to an unknown sink that will be revealed later. We buy an initial set of edges, and when the sink gets revealed we can recover by buying additional edges (at a higher cost). For this problem the CRD approach seems to be very appropriate; the SRD model is hard to implement. The computational experiments revealed that it is crucial to add columns that correspond to the same initial solution for all scenarios.There are several directions for future research. An obvious question is to find out what type of problems can be solved by this kind of approach; we are currently investigating our framework on scheduling problems. Next, it is important to formulate conditions to determine beforehand which decomposition model will work best for a certain problem. Recently, we have extended the size robust knapsack problem to multiple knapsacks (Tönissen, van den Akker, & Hoogeveen (2015)). For the single knapsack problem the SRD model outperforms CRD, but this changes if the number of knapsacks increases, and for four and more knapsacks CRD outperforms SRD. Another important question concerns the reduction of the running time; for example, in our experiments we have seen that adding the right columns or improving the primal heuristic can lead to a large improvement. Interesting issues for further research are restrictions on the recovery solution such as a limited budget for the cost of the recovery solution.

@&#CONCLUSIONS@&#
