@&#MAIN-TITLE@&#
Combining histogram-wise and pixel-wise matchings for kernel tracking through constrained optimization

@&#HIGHLIGHTS@&#
Pixel-wise matching is combined with histogram-wise matching.Weight image considers both the foreground and background.Weight matching maximizes likelihood ratio between foreground and background.Template matching computes the trade-off between accuracy and robustness.Pixel-wise similarity is optimized under the constraints of histogram-wise similarity.

@&#KEYPHRASES@&#
Object tracking,Histogram matching,Template matching,Constrained optimization,

@&#ABSTRACT@&#
In this paper, we propose a constrained optimization approach to improving both the robustness and accuracy of kernel tracking which is appropriate for real-time video surveillance due to its low computational load. Typical tracking with histogram-wise matching provides robustness but has insufficient accuracy, because it does not involve spatial information. On the other hand, tracking with pixel-wise matching achieves accurate performance but is not robust against deformation of a target object. To find the best compromise between robustness and accuracy, in our paper, we combine histogram-wise matching and pixel-wise template matching via constrained optimization problem. Firstly, we propose a novel weight image representing both the probability of foreground and the degree of similarity between the template and a candidate target image. The weight image is used to formulate an objective function for the histogram-wise weight matching. Then the pixel-wise matching is formulated as a constrained optimization problem using the result of the histogram-wise weight matching. In consequence, the proposed approach optimizes pixel-wise template similarity (for accuracy) under the constraints of histogram-wise feature similarity (for robustness). Experimental results show the combined effects, and demonstrate that our method outperforms recent tracking algorithms in terms of robustness, accuracy, and computational cost.

@&#INTRODUCTION@&#
Fast and robust object tracking has been gaining more attention as surveillance markets have grown. The object tracking technique has a wide range of applications, such as robotics, human computer interaction, video indexing and retrieval. Moreover, it can also influence the performance of high-level image interpretation procedures, such as behavior understanding and description. According to a recent survey in [1], object tracking algorithms can roughly be classified into three major categories, namely, point tracking (e.g. particle filter [2]), silhouette tracking (e.g. state space model [3]), and kernel tracking (e.g. mean-shift tracker [4], SSD (Sum of Squared Difference) tracker [5]). Among these tracking categories, the kernel tracking algorithm has relatively low computational cost, because it usually employs the gradient-based search method. The gradient-based optimization technique determines the search direction according to the objective function’s derivative information, thus it can reduce the amount of computation required by the exhaustive full search algorithm.In the kernel tracking category, there are two basic approaches to modeling spatial structure [6] between the template (or model) region and a candidate target region; one is structure-free model and the other is structure-stiff model. The structure-free model ignores the spatial relation among pixels. Typical histogram-wise matching is based on the structure-free model which is useful for non-rigid object tracking. The mean-shift tracker is one of the most influential non-rigid tracking algorithms based on the structure-free model. It employs the gradient ascent method to iteratively locate the target region where the similarity (Bhattacharya coefficient) is maximized. Although it is considered as an efficient technique for tracking 2D blobs [7], this kind of histogram-wise matching might not be quite accurate enough, since it basically loses spatial information.To enhance robust tracking performance, some extended structure-free model based trackers have been induced from the object/background likelihood ratio using the histogram-wise feature distribution [8–10]. To make high tolerance for clutter and appearance change, they exploit the spatial distributions of features that separate object from background. This scheme basically considers target tracking as a local discrimination problem with two classes: foreground object and background scene. To separate the object and the background, feature selection method [8,9] chooses discriminative features. In [8], the local object/background discrimination process is performed by computing a log likelihood ratio between the object and the neighboring background region. This likelihood gives a measure that a particular feature histogram bin belongs to the object/background. However, this log likelihood ratio can take both unbounded positive or negative value. Thus, in [9], the sigmoid kernel is used to generate a weight value from the likelihood ratio which takes a value between 0 and 1. Another discriminative approach was introduced in [10], where the target is represented by a set of spatial attention regions. This attentional region selection method chooses local distinctive sub-regions to separate the target from the background. This kind of histogram-wise matching methods even using the discriminative process still might not be accurate enough.The structure-stiff model imposes a rigidity constraint. The pixel-wise matching is a kind of structure-stiff model which is effective for rigid object tracking [5,11–13]. The classic Lucas–Kanade tracker [11] computes the motion vector which minimizes the pixel-wise difference between the template and the current image region of the target. The SSD tracker [5] is a well known traditional rigid tracking algorithm based on the structure-stiff model. It adopts the gradient descent method to iteratively find the target location where the SSD measure is minimized. The SSD tracker is extended in [12] with multiple spatially distributed kernels to estimate change in the target’s orientation and scale. Recently in [13], the optimal kernels for the target being tracked are suggested. Although it is computationally efficient, this kind of pixel-wise tracking might not be quite robust enough, since it is sensitive to background distractors, clutter, and occlusion.As for a relaxation of the structure-stiff model, posterior-based approach has been induced from the spatio-temporal relations [6,14]. The concept of posterior priority is very useful for the robust estimation of motion parameters in frames [15,16]. IVT [14] is one of the particle filtering-based affine motion trackers which represent a target image with subspaces. In particular, [6] uses a spatial context defined on its corresponding pixel, and a set of feature contexts is represented by the posterior of the associated feature class. Each individual context gives a linear constraint on the motion, and its tracker matches the contextual posteriors. Although these posterior-based approaches are very effective for accurate and robust tracking performance, they also require high computational resources in the calculation of motion parameters with temporal-dependency, and some trackers are not expected work well when target object is heavily occluded.The idea of structure-flexible model integrating the structure-free and the structure-stiff model has been studied to enhance robust and accurate tracking performance [17,18]. In [17], mean-shift and SSD algorithms are combined, which have complementary properties. The inaccuracy of mean-shift tracking is alleviated by the SSD process, while the instability of SSD tracking is overcome by the mean-shift tracker. However, this structure-flexible tracker cascaded with the SSD tracker and the mean-shift tracker has combined computational complexity of individual two trackers. The idea of using the kernel bandwidth in joint feature-spatial spaces for a continuum between rigid and non-rigid tracking approaches is introduced in [18]. As the spatial kernel bandwidth goes to 0 or ∞, this method converges to SSD tracking or histogram tracking approach, respectively. However, considering all pixel locations affected by kernel bandwidth requires a heavy computational cost which is hardly adequate for real-time applications. Thus, the existing tracking approaches based on structure-flexible model tend to be complicated.In this paper, we propose an efficient algorithm for the structure-flexible model combining both the histogram-wise feature information and the pixel-wise spatial structure information. The key idea of combining the two approaches is to use a new weight image representing the foreground probability as well as the degree of similarity between the template and the candidate target image. This is different point from the conventional approaches [9,17] calculating two weight images for the template and the candidate target separately. The combined weight image is derived from four likelihood images where two likelihood images are obtained from the template and the other two are obtained from the candidate target image. By using this weight image, a novel optimization formulation is developed to achieve a compromise between histogram-wise matching and pixel-wise template matching with low computational cost for embedded applications.The overall process has two matching steps: histogram-wise weight matching and pixel-wise template matching. The histogram-wise matching is formulated to find the region where the pixel values of the proposed weight image have high probability values, i.e., near to one. Then the pixel-wise template matching is formulated as a constrained optimization problem using the weight image and the result of weight matching. The main contribution of the paper can be summarized as a development of a novel weight image and a new formulation of a constrained optimization problem for Lucas–Kanade tracking using the weight image. As a result, the proposed approach outperforms recent tracking algorithms in terms of robustness, accuracy, and computational cost by optimizing pixel-wise template similarity (for accuracy) under the constraints of histogram-wise feature similarity (for robustness).The flow diagram of our framework is described in Fig. 1. As shown in this figure, the proposed tracking procedure mainly consists of three phases: weight image estimation, histogram-wise weight matching, and pixel-wise template matching. The first phase estimates a weight image using the object/background likelihood at the current target location. The second weight matching phase builds an objective function OW(μ) with feature distributions using the weight image. This phase finds its optimal motion parametersμWusing the Lucas–Kanade algorithm, and subsequently provides a constraint function h(δμ) for the next phase. The third pixel-wise template matching phase builds an objective function OT(δμ). This phase starts at the initial target location withμW, and formulates a constrained optimization problem by applying a constraint function h(δμ) to OT(μ). In the proposed tracking framework, our method uses the gradient projection method to determine the optimal motion parameters δμefficiently.This paper is organized as follows. Weight image estimation is described in Section 2. The proposed weight matching and template matching are detailed in Section 3. The performance results are presented in Section 4, and the conclusions are given in Section 5.We refer to the feature values of the reference image and the target region of the input image as the template and the target image, respectively. Various types of image features are applicable to the template and the target image, such as intensity, color, and image gradient. In this paper, we represent a target object being tracked by a rectangular image, and express the n1-column by n2-row pixels of the rectangular image as an n-dimensional column vector, which is an array of n=n1×n2 components. We define the template as an n-dimensional column vector,T(t), where t denotes time. Letxi= (xi,yi)Tdenote the ith pixel location in a given image, where 1⩽i⩽n, then the components of the template can be represented by T(x1,t), T(x2,t),…,T(xn,t). Briefly, we use T1, T2,…,Tnto denote the pixels of the template.For the representation of the parametric motion model, referring to [5], we defineμand f(xi;μ) to denote a motion parameter and a transformation function, respectively. Let I(f(xi;μ), t) denote the feature value at the ith pixel location of the current target image which is transformed withμ. Then we define the target image as an n×1 vectorI(μ,t) where I(f(xi;μ),t) is equivalent to the ith component. We also use I1, I2,…,Into briefly represent the ith component ofI(μ,t). To determine motion parameters in the tracking process, various motion models are used in the SSD tracker, such as translation [19], affine warp [5], 8D projective warp [20], and 3D pose [21]. However, for efficiency, we adopt the translation motion model between two consecutive frames withμ=(μx,μy)T, assuming that the modeling error would be compensated by combining pixel-wise matching with histogram-wise matching.In the tracking process,T(t) orI(μ,t) may include background clutters that apparently differ from the target object being tracked. To determine what parts ofT(t) andI(μ,t) belong to the moving object and what parts belong to the background scene, we improve the concept of object/background likelihood ratio which is introduced in [8,17]. For the discrimination process, as illustrated in Fig. 2(a), the object-window is defined as the region within the solid yellow rectangle that includes a current target image, and the background-window as the region outside the object-window but within the green rectangle. We use Hobjand Hbgto denote the joint histograms calculated over the object-window and background-window, respectively. And we use nobjand nbgto denote the number of pixels in the object-window and background-window, respectively. We definecTiandcIito denote the class label of the Tiand Iirespectively. Then fromT(t) andI(μ,t), using the empirical discrete probability distribution [8], we can define four likelihood images with feature histograms as follows,(1)P(Ti∣cTi=obj)=max{Hobj(b(Ti))/nobj,∊},P(Ti∣cTi=bg)=max{Hbg(b(Ti))/nbg,∊},P(Ii∣cIi=obj)=max{Hobj(b(Ii))/nobj,∊},P(Ii∣cIi=bg)=max{Hbg(b(Ii))/nbg,∊},where obj and bg represent object and background, respectively. b(Ti) and b(Ii) mean the histogram bins of Tiand Ii, respectively. ∊ is a small value. Note that, after the bestμis found at previous time (t−1), we measure Hobjand Hbgonce fromI(μ,t−1). Then, to find bestμat current time t, we compute the likelihood images in (1) iteratively.In the typical object/background discrimination approaches, each ofT(t) andI(μ,t) defines a weight image to extract its foreground pixels [9,17]. In the proposed framework, we define a novel weight image from the four likelihood images in (1). We consider two events with the object and background classes, O≡{cTi=obj,cIi=obj} and B≡ {cTi=obj,cIi=bgorcTi=bg,cIi=objorcTi=bg,cIi=bg} to measure the foreground-based similarity between two pixels, Tiand Ii. The first event represents both pixels, Tiand Ii, belong to the object, and the second event means that one or two pixels atxibelong to the background. Assuming that class labels are independent given Tiand Ii, we use the following function for object/background discrimination,(2)L(xi;μ)=logP(O∣(xi;μ))P(B∣(xi;μ)),whereP(O∣(xi;μ))=P(cTi=obj∣Ti)P(cIi=obj∣Ii),P(B∣(xi;μ))=P(cTi=obj∣Ti)P(cIi=bg∣Ii)+P(cTi=bg∣Ti)P(cIi=obj∣Ii)+P(cTi=bg∣Ti)P(cIi=bg∣Ii). By applying Bayes rule to (2) with priors denoted by π, and assuming same priors,π(cTi=obj)=π(cTi=bg)=π(cIi=obj)=π(cIi=bg), we can obtain the object/background likelihood ratio as following,(3)L(xi;μ)=logP((xi;μ)∣O)P((xi;μ)∣B),whereP((xi;μ)∣O)=P(Ti∣cTi=obj)P(Ii∣cIi=obj),P((xi;μ),∣B)=P(Ti∣cTi=obj)P(Ii∣cIi=bg)+P(Ti∣cTi=bg)P(Ii∣cIi=obj)+P(Ti∣cTi=bg)P(Ii∣cIi=bg).Since the value of L(xi;μ) is unbounded, we use the sigmoid kernel [9] to define a weight value between 0 and 1,(4)Wi=(1+exp(-(L(xi;μ)-a)/b))-1,where a and b controls the offset and slope of mapping function respectively. If we set (a,b)=(0,1), we can obtain the following simple function,(5)Wi=P((xi;μ)∣O)P((xi;μ)∣O)+P((xi;μ)∣B),where (5) is equivalent to the posterior of a pixel belonging to the object. Additionally, using the definitions of P((xi;μ)∣O) and P((xi;μ)∣B), (5) becomes,(6)Wi=WTiWIi,whereWTi=P(Ti∣cTi=obj)/{P(Ti∣cTi=obj)+P(Ti∣cTi=bg)},WIi=P(Ii∣cIi=obj)/{P(Ii∣cIi=obj)+P(Ii∣cIi=bg)}. Note that, when we search the bestμusing variousWIivalues,WTican be pre-computed once for each frame.Using the computed weight values, we can define a weight image as an n-dimensional column vector,W(μ)=[W1,W2,…,Wn]T. The value of Wiprovides a probabilistic measure that both the ith pixels inT(t) andI(μ,t) belong to the object. Fig. 2 shows an example of the weight image, where we chose 32 bins to make the brightness histogram. Fig. 2(b) plots Hobj/nobjand Hbg/nbg, and shows the obtained weight image.In this section, we describe the details of the proposed scheme including histogram-wise weight matching, pixel-wise template matching, and template update. Our tracking process consists of two matching steps. First, we search for the convergence point of an objective function surface using the weight image to provide robust tracking performance. This is referred to as histogram-wise weight matching. Next, we determine a convex constraint set around the obtained optimal point in the weight matching, and optimize a pixel-wise weighted matching function subject to a constraint set in order to enhance the tracking accuracy. This is referred to as pixel-wise template matching. In the proposed framework, a method for selecting a compromise solution between histogram-wise weight matching and pixel-wise template matching is presented by solving a constrained optimization problem.When cluttered pixels are included in a target image, or a target appearance undergoes severe change, it is difficult to measure the similarity betweenT(t) andI(μ,t). For robust tracking performance, we propose a histogram-wise weight matching algorithm using the weight image. We begin by observing that Wiin (5) indicates the likelihood of being a pixel of the target object, and the ideal foreground object pixel has Wi=1. Next, we determine the bestμby minimizing the squared errors between the ideal value of 1 and Wi. Given a set of n pixels atxi, for i=1,…,n, we can define the following objective function,(7)OW(μ)=∥W0-W(μ)∥2,where ∥·∥ denotes the 2-norm, andW0 denotes [1,1,…,1]T. Note that we use the reference value for each component as 1 through an image sequence, while reference values of traditional kernel trackers are typically obtained from the initial frame. In the tracking process, it is hard to make a reference value when the target undergoes severe appearance changes. In this case, our weight matching with the reference value assigned as 1 is useful to provide robust tracking performance. Basically, we assume that the object window includes target object, and the likelihood images are discriminative in image sequences enough to apply OW(μ).Referring to the approaches in [5,11], we define the displacement of a vector offset as δμ, assuming that the current motion parameter vector,μ, is known. And, to solve the nonlinear least-squared problem with OW(μ+δμ), we employ the Lucas–Kanade algorithm (also called the Gauss–Newton method) [11] which chooses the best motion parameter vector as the least-squared estimator. This method, also known as the linearization method, first uses the Taylor series expansion to obtain a linear model that approximates the original nonlinear model, then adopts ordinary least-squares to estimate the objective motion parameters.Therefore, if we consider OW(μ+δμ) as a continuous quantity, we can solve equation {EW−MWδμ=0}, where n×2 dimensional matrixMWdenotes the Jacobian matrix ofW(μ) aboutμ, andEW=(W0−W(μ+δμ)). As a result, we obtain the following solution that minimizes the convex objective function derived through linearization method.,(8)δμ=(MWTMW)-1MWTEW,whereMWTMWis supposed to have full rank.During weight matching, we iteratively determine δμby minimizing OW(μ+δμ). As a result, using the integration of δμwith (8), we can obtain the following motion vector,(9)μW=∑(MWTMW)-1MWTEW,and the best target region is calculated at the corresponding target location withμW. The proposed weight matching process is shown in Fig. 3.In the accurate tracking framework, we propose a weighted template matching algorithm using the image spatial structure information. For robust performance, we use the convergence point of histogram-wise weight matching,μW, as a starting point for pixel-wise weighted template matching. We also use a constraint boundary for the trade-off between histogram-wise weight matching and pixel-wise weighted template matching.In the proposed weighted template matching, to enhance the SSD tracking performance, background pixels inT(t) andI(μ,t) are suppressed by providing a weighting to each pixel for little influence on the objective SSD function. Since the non-object pixels appear as low values in the weight image, we can reduce the effects of background pixels in the SSD function by using Wias a weight factor for the pixel error, (Ti−Ii). That is, given a set of pixels, we determine the best target location by minimizing the sum of squared errors with Wi(Ti−Ii) for i=1,2,…,n. As a result, we can obtain the following objective function, which is formulated as a nonlinear least-square problem,(10)OT(μ)=∥ET(μ)∥2,whereET(μ) has the same dimension withW(μ), and the ith value ofET(μ) is equivalent to Wi(Ti−Ii). Note that if every component ofW(μ) is 1, then OT(μ) becomes the case of the SSD tracker.In order to efficiently find the optimal solution that minimizes OT(μ), our weighted template matching adopts the Lucas–Kanade algorithm. Referring to the weight matching procedure with δμ, we can obtain the optimal motion parameters as following,(11)μT=-∑(MTTMT)-1MTTET,where n×2 dimensional matrixMTdenotes the Jacobian matrix ofETaboutμ, andMTTMTis supposed to have full rank.In the presented objective function with (10), our scheme basically considers (Ti−Ii) as the dominant term. Thus, it is supposed that the variations with Wiare small with respect to the value of (Ti−Ii). However, in the optimization with (11), the objective function, OT(μ), could be minimized when Wigoes to zero. In this case,I(μT,t) contains a considerable amount of background pixels under the influence of Wi≈0, which is contrary to our purpose because weight matching assigns the reference value of Wias 1. To prevent this kind of false-matching case and improve tracking performance, we take the result of weight matching into account in the process of weighted template matching. We use the convergence point of weight matching,μW, as a starting point for weighted template matching, and obtain the following enhanced motion vector,(12)μT∗=μW+μT.In addition, we have experimentally found that the distance between two convergence points,μWandμT∗, generally increases when the target undergoes severe appearance change, such as abrupt rotation, object deformation, or illumination change. In this situation, spatial structure information inI(μT∗,t)is not likely to match up with that ofT(t) whereas the weight matching is more trustworthy in the sense of robustness. Hence, we can conclude that (12) is not adequate in situations whereμT∗is located much farther away fromμW. In the framework proposed here, we modify the above weighted template matching process to consider a trade-off between OWand OT, where OTis minimized while the weight matching result,μW, is well preserved. We adopt the concept of a constraint boundary to ensure that the motion parameter vector is located close toμWunder severe appearance change, which means that spatial information for weighted template matching is regarded as valid within the interior of the constraint boundary.To make a constraint boundary, we initially use the distance between the current motion parameter vector andμWwhich gives an indication of the structural instability caused by the appearance change. The key feature of our scheme is based on the assumption that ifμT∗is located close toμW, then the target undergoes small object appearance deformation. On the other hand, ifμT∗is far away fromμW, large deformation is supposed to have occurred between the template and the current target image. For this reason, we set a trade-off curve using the distance value γ between the current motion parameter vector andμW. If the weighted template matching procedure encounters the trade-off curve withμ=μB, then it satisfies ∥μB−μW∥=γ. In order to construct a constraint boundary, we use the motion parameters that have the same value of OW(μ) with OW(μB). Thus, constant γ concerns the trade-off between weight matching and weighted template matching. To set an adequate value of γ, it is useful to consider the template size and the amount of changes in a target appearance. In experiments, it was found that our tracker is insensitive to the choice of a γ value.In Fig. 4, we illustrate two cases of the proposed optimization process with OW(μ) and OT(μ). When OT(μ) has the convergence point inside the constraint boundary, as shown in Fig. 4(a), then our method selectsμT∗as the best solution. In this case, feature histogram and spatial information are incorporated into the weighted template matching procedure withμT∗. The critical case occurs whenμT∗is much farther away fromμW; that is,μT∗is located over the constraint boundary, as shown in Fig. 4(b). In this case, we combine feature histogram with spatial information by restrictingμto the constraint boundary. Therefore, among the motion parameter vectors on the boundary, our tracker locates the best motion parameter vector that minimizes the value of OT(μ). In each optimization step with δμ, we can define the constraint function as follows,(13)h(δμ)=OW(μ+δμ)-OW(μB),where h(δμ) is a scalar-valued function, and h(δμ)=0 corresponds to a constraint equation for a boundary.We can suppose that h(δμ) is convex aroundμW, because OW(μ+δμ) is assumed to be convex aroundμW. Here, we assume that h is continuously differentiable. Then, a gradient-based search method would find the optimal point as long as the initial target location is within a small neighborhood of that optimal point. If we assume that OT(μ+δμ) is locally convex about the currentμ, and δμis constrained to belong to a certain convex setΩ={δμ∈R2∣h(δμ)⩽0}, then, the search for the minimum value of OT(μ+δμ) can be represented by the following constrained minimization problem,(14)minimizeOT(μ+δμ),subject toh(δμ)⩽0.One of the most common techniques for handling the constraint is to use a descent method in which the direction of descent is chosen to reduce the objective function by remaining within the constrained region. Such a method is usually referred to as the gradient projection method [22,23]. In this paper, we exploit this method in [24–26] to solve the above nonlinear constrained optimization problem.The projection idea is as follows. We refer to α andsas the magnitude and direction of the motion parameter offsets, respectively. As illustrated in Fig. 4(b), we search the optimal motion parameter vector along the motion directions. If (δμ+αs) is within the constraint boundary, then we update the motion parameter vector as (δμ+αs). However, when the above motion offset vector αsmoves on the tangent plane, it may not exactly follow the constraint boundary. This is due to the nonlinearity of h(δμ), which typically causes the projection offset to move away from the constraint boundary. In order to bring δμback to the constraint boundary, we employ the gradient restoration method [26,27].Following the above procedure, we can obtain the closed form formula for solution as(15)μ∗=μW+μT+αs-∑{∇h(∇hT∇h)-1h(δμ)},where we denote ∇h(δμ) by ∇h, andμTis constrained by h(δμ). This formula can be easily implemented even for real-time embedded programming. Our weighted template matching process is shown in Fig. 5.Traditional gradient-based tracking algorithms exploit the initial template data to represent the latest target appearance. For example, the standard mean-shift tracker uses a fixed reference histogram model, which is obtained from the first frame, and the Hager’s SSD tracker [5] uses the initial template and updated motion model parameters to represent the current template. These strategies can preserve the originality of the target appearance, however, the current template cannot cover the large variation and clutter of the latest target appearance throughout the sequence.In order to overcome this mismatching problem, various model update algorithms have been suggested for the appearance based templates [28] and online boosting classifiers [29]. In particular, [30] adapts the online structured output support vector machine (SVM) learning method to the tracking problem which allows the stable updating of the classifier. In [31], the tracking process is combined with image matting. The scribbles for matting are automatically generated, and model updating with accurate target extracting is obtained from the matting results.Our update scheme involves regulating the reference template to represent the current object appearance. We replace the template with the current target image whenμWandμ∗ are close to each other which implies small variation of appearance, i.e.,(16)T(t+1)=I(μ∗,t)if∥μ∗-μW∥⩽θ,T(t)otherwise,where θ means a distance threshold between two convergence points of weight matching and weighted template matching.In this section, we present a set of experimental results to validate our algorithm. For simplicity and cost-efficiency, the proposed method was implemented with gray-scaled input images and 32 brightness histogram bins. All simulations have been performed on a PC platform with Intel Core2 Quad 2.5GHz CPU and 2GB RAM environment.In the experiments, we compared the results of our method with those of state-of-the-art trackers, OAB [29], MILTrack [32], CT [33], SCM [34], FragTrack [35], and IVT [14]. OAB, MILTrack, and CT involve the object-background discrimination tasks which use classifiers to separate the object from the background. OAB and MILTrack treats the tracking problem as a detection task which is knows as tracking-by-detection. OAB adopts boosting and extracts one positive image patch, while MILTrack uses multiple instance learning and updates the appearance model with a set of image patches. CT compresses image features by projecting the feature space into a low-dimensional subspace using a sparse measurement matrix. SCM uses a robust appearance model which exploits holistic templates and local visual information. It develops a sparsity-based collaborative model that integrates a discriminative classifier and a generative model in the particle filter framework. FragTrack uses a static appearance model, and breaks the template into a grid of fragment to robustly exclude background parts. It compares multiple histograms of multiple fragments of the template to measure histogram similarity. In the experimental results, our algorithm could obtain robust and accurate tracking results, where the frame rates were good enough to achieve real-time performance.In this experiment, the walking woman sequence from [35] was applied to perform the evaluation of our compromise tracking algorithm. This sequence contains a moving target, which undergoes partial occlusions in cluttered background. In Fig. 6(a), IVT, SCM,OAB, and MILTrack lost the target when the target image became partially occluded around frame 55. They did not robustly reject occluded image parts. On the other hand, our method tracked well by achieving a compromise between histogram-wise matching and pixel-wise template matching. In the kitesurf sequence, which is obtained from [33], our method showed the best performance. As shown in Fig. 6(b), the proposed algorithm robustly measured the template similarity when the computed likelihood images are discriminative. However, CT and FragTrack lost the target person around frame 38 when the partial occlusion is occurred. IVT collapsed after the background clutters appeared in the early frames.Next, we evaluated the robustness of our method when the tracking target is defined as a large-sized template. We applied the occluded face sequence from [35] and the occluded face 2 sequence. In each video sequence, the target is occluded by a book which is similar in color to the region of the target. Fig. 6(c) and 6(d) show that our method and SCM achieved stable performance when the heavy occlusion happened. In this example, we extracted the template from the initial frame, and did not update the template to preserve the original geometrical model effectively and overcome partial occlusion effects successfully. In occluded face sequence, we can see that OAB, MILTrack, and IVT yielded shifted results, and drifted away from the target being tracked.In this experiment, the david indoor sequence from [14] was applied to evaluate robust performance in the presence of background clutter and illumination change. This sequence includes a target person who moves in a complex environment, and changes his scale between frames under illumination variation. In Fig. 7(a), we can see that our tracker adaptively adjusted the target location by reducing the drift effects from outliers since the proposed algorithm efficiently updated the template with the change in target appearance. But SCM, OAB, and FragTrack lost the moving target as more background clutters were included in the target region with illumination change. Additionally, we evaluated our method using weight matching only. As shown in Fig. 7(b), it could not handle variations in scale and pose. In each frame with Fig. 7(b), the bottom image patches illustrate the template and current weight image. Note that the parts of the object being tracked have higher values in the weight images. Therefore, when the target region could contain cluttered factors and robust performance is required, our tracking method is very a useful approach.In this experiment, we demonstrate the robustness of our method when a moving target undergoes pose variation and appearance change. In the sylvester sequence, which is obtained from [14], the target rotates and changes its brightness value when it moves rapidly under a lamp. Fig. 8(a) shows that IVT and OAB lost target around frame 760, while our method was accurate at about frame 1080. The bolt sequence comes from the authors of [33]. It includes a non-rigid small target person where several similar shaped runners move with the target in background. In Fig. 8(b), our method performs the best because it can robustly track a moving person by effectively separating the target from the background. IVT, CT, SCM, and FragTrack could not locate the correct target at frame 280. We used the coupon sequence where the first page is folded after about frame 50, and a similar shaped coupon book appears in background. In Fig. 8(c), we can see that IVT, MILTrack, and FragTrack drifted away and lost the target being tracked at frame 310. But our method found the correct target, and overcame the change in appearance successfully throughout the sequence.In Table 1 and 2, we show the tracking performance of our method on a PC platform in terms of tracking error and FPS performance. Note that our method was coded in Matlab using only M-files. IVT and CT were coded in Matlab using M-files with a DLL-file and Mex-files respectively. SCM has been coded in Matlab using M-files with MEX-files and DLL-files. FragTrack was programmed with the C++ language using the OpenCV library. MILTrack and OAB were programmed with the C++ language with OpenCV and IPP. In our experiments, we used the default parameters as the authors used in their source codes. Thus we notice that other trackers may require parameter tuning for each video sequence. In Fig. 9, we plot the position errors of the state-of-the-art trackers using the experimental sequences. In our method, the average numbers of iterations are 15.3 (walking woman), 19.1 (kitesurf), 44.3 (occluded face), 47.2 (occluded face 2), 20.9 (david indoor), 15.1 (sylvester), 21.6 (bolt), and 25.3 (coupon book). Comparing the other trackers in Table 1, the proposed method shows high performance over all the test sequences in the table. In Table 2, we can see that the computational cost of our method is much lower than the other methods, and this is sufficient to achieve real-time performance in embedded applications. The FPS performance of our method is dependent on the template size. Thus, if we use a resized template, we can enhance the FPS performance for a large-sized template.In addition, we evaluated the tracking performance and computational load of our method on the latest DSP TMS320DM6437(Texas Instruments, 600MHz) platform developed by us. The extensive experimental results in DSP settings show that our tracker can maximally use a 32×64 template to achieve real-time performance (30 FPS) and achieve a successful performance under partial occlusion, rotation, and a complex background.

@&#CONCLUSIONS@&#
