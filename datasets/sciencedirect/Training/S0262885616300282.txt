@&#MAIN-TITLE@&#
Towards optimal VLAD for human action recognition from still images

@&#HIGHLIGHTS@&#
Tackle the empty cavity issue by properly selecting reference points.Reveal codeword ambiguity by imbalance assignment and enhance the reliable codeword.Incorporate GMP into VLAD.

@&#KEYPHRASES@&#
VLAD,Empty cavity,Ambiguity,Generalized max pooling,Human activity recognition,

@&#ABSTRACT@&#
Human action recognition from still image has recently drawn increasing attention in human behavior analysis and also poses great challenges due to the huge inter ambiguity and intra variability. Vector of locally aggregated descriptors (VLAD) has achieved state-of-the-art performance in many image classification tasks based on local features. The great success of VLAD is largely due to its high descriptive ability and computational efficiency. In this paper, towards optimal VLAD representations for human action recognition from still images, we improve VLAD by tackling three important issues including empty cavity, ambiguity and pooling strategies. The empty cavity limits the performance of VLAD and has long been overlooked. We investigate the empty cavity and provide an effective solution to deal with it, which improves the performance of VLAD; we enhance the codewords with middle level of assignments which are more reliable and can provide more useful information for realistic activity; we propose incorporating the generalized max pooling to replace sum pooling in VLAD, which is more reliable for the final representation. We have conducted extensive experiments on four widely-used benchmarks to validate the proposed method for human action recognition from still images. Our method produces competitive performance with state-of-the-art algorithms.

@&#INTRODUCTION@&#
Human action recognition from video can make the full advantage of structure and motion information [1–3]. Furthermore, together with temporal information, the system can be more robust to noisy background [4,5]. While the same task from still image poses even bigger challenges due to the limited useful information which deduces large intra-class variation and inter-class ambiguity.Human action recognition from still images plays an important role in human behavior analysis. The bag-of-feature (BoF) [6,7] is one of the most effective framework in image and video representations based on local features. Recently, Fisher vector (FV) [8,9] and its non-probabilistic version, i.e., vector of locally aggregated descriptor (VLAD) [10,11] have been extensively used due to their high performances in different tasks including image retrieval, scene/object classification and human activity recognition [12–15].BoF encodes local features by calculating the histogram of local feature descriptors on a pre-learned codebook, and the histogram can be viewed as an approximate of the distribution of local features [16]. The Fisher vector, which is derived from the Fisher kernel, describes a signal by gradient vectors calculated from its probability density function [9]. Normally, the probability density function is modeled by Gaussian mixture models (GMMs). In contrast to BoF using the hard assignment, FV with GMMs can be regarded as using soft assignment [17], which has shown better performance. Moreover, FV concerns not only on which codeword is activated but also on the gradient indicating the direction in which parameters should be adjusted to best fit the data. In contrary to FV, VLAD avoids the estimation of the complex GMM model which is extremely time-consuming and achieves competitive and even better performance than FV [9]. In this work, we will build our method on VLAD to take advantage of its high performance and computational efficiency.BoF, FV and VLAD can be viewed in a unified framework which describes images with local features by two main components: codebook creation and feature encoding. Most researchers focus on these two aspects to generate good codebooks and achieve discriminative encodings. However, the empty cavity issue always happens in these methods and would compromise the performance significantly, which will be thoroughly investigated in this work.As for codebook creation, apart from the traditional k-means clustering algorithm, hierarchical clustering [18] and spectral clustering [19] can also be applied, avoiding to determine the size of codebook beforehand. With recent prevalence of sparse coding [20], supervised[21–23] and unsupervised [24–26] codebook generation approaches on sparse coding are proposed. The main distinction among these approaches on sparse coding rests on the constraint conditions, such as locality-constrained linear coding (LLC) [27] limits the local descriptor only to be encoded by the codewords in its neighbor, and local coordinate coding (LCC) [25] adds the locality constraint from a different way.With regard to assignment, extensive work including soft assignment [17], triangle assignment [28], localized soft assignment [29] has been conducted to avoid the limitation of hard assignment. Hard assignment assigns each local descriptor to its closest codeword. However, soft assignment describes a local descriptor by multiple codewords using a kernel function (e.g., Gaussian function) of the distance between local descriptor and codewords. Applying the distance kernel function involved multiple codewords may be the key reasons of the accuracy enhancement of probability density estimation [30] in soft assignment. Triangle assignment and localized soft assignment are kinds of tradeoff between hard and soft assignments. The former one only counts the distance to the codeword whose distance is closer than the mean of the distances to all codewords. Yet the latter one combines the idea of localization and the soft assignment, where only the distance to the codeword is meaningful if the local descriptor belongs to the k-nearest neighbors of this codeword.However, no matter which approach is adopted to generate the codebook and how the local descriptors are assigned to codewords, the problem of empty cavity and imbalance of local descriptor assignments do exist. Although Ref. [31] discussed the negative effect of empty cavity in BoF model and Ref. [29] gave the analysis about ambiguity of codeword, the negative effects of empty cavity and the imbalance influence of assignment in VLAD have long been neglected.Towards optimal VLAD, in this paper, we improve VLAD by dealing with three key issues in VLAD: empty cavity, ambiguity and pooling. We make the following three major contributions:1)By exploiting the negative effect of empty cavity theoretically and experimentally, we propose an effective method to tackle the empty cavity issue, which can significantly improve the performance.By investigating the distribution of the number of assignment in each codeword in one image, and rebuilding the relations of the imbalance assignment and codeword ambiguity, we propose selecting the reliable codewords to enhance the weight of the pooled vector related to those codewords in VLAD.We for the first time incorporate the generalized max pooling into the construction of VLAD, which shows better performance in ordinary VLAD using the sum pooling.Human action recognition from still images has played increasing important role in human behavior analysis and it poses even more challenges than video-based action recognition. Before introducing our method, we would like to describe four challenging and widely-used image datasets that are used in this work. As shown in Fig. 1, the most difficult factor in each dataset is listed. In PPMI, confusing examples of holding and playing instruments are the hardest factor to be distinguished, as shown in sub-figure (a). Similarly, for actions from the same class in Standford 40 Action, the poses and backgrounds are different such as brushing and cooking in sub-figure (b). Moreover, sub-figure (c) in Fig. 1 gives the examples of asmall object hard to be detected in UIUC sports, and similar pose and background examples cross-class in Sports 6 are listed in sub-figure (d).•PPMI [32] contains images of humans interacting with twelve different musical instruments. They are: bassoon, cello, clarinet, erhu, flute, French horn, guitar, harp, recorder, saxophone, trumpet and violin. For each instrument, there are images that contain a person playing the instrument (PPMI+), as well as images that contain a person holding the instrument without playing (PPMI−). Here we treat it as a classification task with 24 classes. We use 100 images in each class for training and the rest 100 images for testing.Stanford 40 Action [33] contains images of 40 diverse daily human action, such as ‘brushing teeth’, ‘cleaning the floor’, ‘reading book’, and ‘throwing a frisbee’. All images are obtained from Google, Bing and Flickr. There are 9532 images in total with 180–300 images per action class. The images within each class have large variations in human pose, appearance and background clutter. We use 100 images in each class for training and all the rest images for testing as in Ref. [33].UIUC sports [34] contains 8 sports categories collected from internet: bocce, croquet, polo, rowing, snowboarding, badminton, sailing and rock climbing. The number of images in each category varies from 137 (bocce) to 250 (rowing). The background of each image is highly cluttered and human poses largely diverse. Furthermore, as shown in Fig. 1, the foreground human poses are too small to be detected in some images. We follow the experimental setting in Ref. [35] by randomly selecting 70 images as training set and 60 images as testing set, respectively.Sports 6 [36] contains images from six different sports as ‘tennis-forehand’, ‘tennis-serve’, ‘volleyball-smash’, ‘cricket-defensive-shot’, ‘cricket-bowling’ and ‘croquet-shot’ with 50 images per class. This dataset is with significant confusion due to similar pose in different action. For example, the poses in ‘volleyball-smash’ are with high similarity to those in ‘tennis-serve’ and the backgrounds from ‘tennis-forehand’ and ‘tennis-serve’ are exactly the same shown in Fig. 1. We use fixed 30 images in each class for training and the other 20 images for testing.Empty cavity means during the assignment of local descriptors in one image, there are always some codewords with no local descriptor assigned to, leaving empty in the final representation. As a result, the obtained representation tends to be less discriminative and therefore compromises the recognition performance. The phenomenon of empty cavity does widely exist in both BoF and VLAD, which severely compromises the overall performance. To show the empty cavity problem, Fig. 2gives the empty cavity rate for above four datasets, where the empty cavity rate means average number of empty cavity over training data divided by the size of codebook.It can be seen that for some datasets, the empty cavity rate is extremely high. For example, the empty cavity rate in PPMI dataset is up to 19.36% with 512 of the codebook size, which means nearly 100 codewords with no local descriptor assigned to. Additionally, the ascending trend of the empty cavity rate with increasing of the codebook size is obvious in this figure. That means, if the codebook size is further increased as 1024 or 2048, the empty cavity phenomenon is much more serious.Let {u1,u2,…,uK} be the codebook learned by k-means. For one image X, each local descriptor xt∈ RDis assigned to the nearest codeword as ui=NN (xt). Moreover, the pooled vectorvifor codeword i is computed by:(1)vi=∑xt:NNxt=uixt−uiwhich is sum pooling strategy of the residual vectors, i.e., the subtraction of local descriptor xtand its belonging codeword. Finally, K pooled vectors are concatenated as a single K×D dimensional vector.By deeply analyzing the pooled vector vi, it can be seen that the distances of local descriptors to codewords are encoded. Thus the similarities relative to the codewords during matching are all incorporated which increases the accuracy of measuring the relationship between local descriptors. While for BoF, only the number of pairs of local descriptors assigned to the same codeword in two images is counted, with no consideration about the similarity of local descriptors. This is the main reason for the better performance of VLAD.Traditional methods including the standard VLAD do not provide any way to handle empty cavity phenomenon, which means that the pooled vectorviis zero for empty cavity codeword. We will demonstrate the negative effect of this zero vector from the kernel aspect.In both classification and retrieval tasks, the essential part is to compute the similarity between two images X and Y. Kernel is one way to fulfill this aim.LetX={x1,…xD︷X1,…,xD×(K−1)+1,…xD×K︷XK}be the VLAD representations for image X andY={y1,…yD︷Y1,…,yD×(K−1)+1,…yD×K︷YK}is VLAD for image Y. Then we can kernelize the match between X and Y as:(2)K(X,Y)=∑iKiXi,Yiwhere Ki(Xi, Yi) presents thepart kernel in cluster i. The essence of Eq. (2) is to decompose the whole kernel function into several independent elements, and each element only focuses on the part generated by one cluster during VLAD.Without loss of generality, we consider a linear kernel widely used for recognition, and therefore we have(3)K(X,Y)=XTY=∑iXiTYiwhere Xiis part representation build on cluster i, namely, pooled vector above, andKiXi,Yi=XiTYi.From Eq. (3), it is obvious that if Xihappens to be a zero vector caused by empty cavity in cluster i, and no matter what value Yiis, the part kernelKiis the same. This kind of matching has misleading result under two conditions: 1) When the codeword is mutually missed in two images, i.e., both Xiand Yiare zero vectors, the codeword that can give more information will receive higher weights in the similarity measurement. 2) When the codeword is missed in only one image, that is, either Xior Yiis a zero vector, the final similarity measurement should vary with non-zero vector left, rather than fixed zero.Fig. 3is part figure of the assignments for each codeword between two randomly selected images in the PPMI dataset, which shows the above two conditions. Firstly, it can be seen that the number of points from image 1 or image on the horizontal axis occupies a certain proportion, which means that the empty cavity is not an accidental phenomena. Secondly, the pairs in green ellipses express the condition of codeword jointly missed in two images, corresponding to zero assignment for those codewords. Additionally, the pairs in black rectangles present the condition that the empty cavity codeword index in images X and Y does not happen at the same location. For example, for the highest rectangle, the assignments of local descriptors are eight for image 2 and zero for image 1.Another thing worth to notice is that the above two conditions of empty cavity do not happen occasionally.In order to solve the empty cavity problem, a non-zero reference vector should be found for pooled vector viby intuition. To obtain a better insight into reference vector selection, Fig. 4illustrates the procedure of VLAD and shows the meaning of a reference vector in our approach. In Fig. 4, the real blue arrows and black dashed arrows represent the residual vector as xt−uiin VLAD from two images. Moreover, the real red arrow and the dashed red arrows represent the pooled vectors from different images. For a certain codeword which local descriptors from both images are assigned to, as the dashed rectangles parts, the part kernel is to compute the inner product between point a and point b where the origin is the related codeword. However, there are also two other conditions for codewords as shown by dashed ellipses and dashed triangle in Fig. 4.The dashed ellipse case is to show the mismatched empty cavity condition between two images (for example, for codeword i, where only two local descriptors in image 2 are assigned and no points in image 1 is in this cavity). The other case is for co-missing codeword j in both two images, which is shown by dashed triangle part. The standard VLAD is to neglect the effect of this codeword under these two conditions. No matter how many local descriptors in image 2 are assigned to the cluster i, the part kernel has no difference, which is zero. Similar, for the dashed triangle case, the part kernel is also zero. To fix this problem, we aim to find a point as the reference to keep the pooled vector nonzero. For empty cavity from distinct image, this reference point should keep unchanged. b1, b2 and b3 are three strategies of reference point which is independent to different image itself. They are type I to type III cases to replace the pooled vector as follows.1)Type I:(4)v^i=1K∑k=1Kuk−ui.This is to treat the mean of all codewords (blue four-points star) as the reference point in Fig. 4. Then the pooled vector for image 1 is the vector starting from codeword i and ending to this blue four-points star. The main idea for this strategy is to find the point with highest probability as the reference point in RDspace, since the average of all codewords is the real center of training data points.Type II:(5)v^i=1card(S)∑k∈Suk−uiwhereSis the set that codeword is close to codeword i and at the same time, the number of points assigned to this codeword is smaller than a threshold. The nearby codewords with smaller assignments are similar to the codeword with no assignment. This is the case of the red arrow to five-points star in Fig. 4.Type III:(6)v^i=ui.For this case, it just treats the codeword itself as the pooled vector if empty cavity happens.Different from text retrieval systems, where the document vector space is discrete itself, the local descriptor space RDis a continuous one. When we use limited visual words to represent the whole continuous space, there will be an ambiguity problem. The ambiguity between codewords will be influenced by the number of words in the codebook. On the one hand, a big size of codebook allows a rich selection of codewords, while the small vocabulary essentially leads to different image parts represented by the same vocabulary element with more serious ambiguity. On the other hand, big size of vocabulary will aggravate the sparse situation and increase computational complexity.The ambiguity in codeword can be reflected in two-field. One is the misrepresentation situation, that is, the codeword could not represent the characteristic of local descriptors. The other case corresponds to the codewords with higher assignments, which is similar as the useless function word in document vector space. Although the function word as ‘is' or ‘are’ may happen in one document many times, it is still meaningless in semantic analysis for classification or recognition.In this section, we aim to build the relations between the number of assignment and ambiguity, and then give the proper solutions to enhance the codeword with less ambiguity.Fig. 5presents the variation of local descriptor assignment number in the PPMI dataset, where left sub-figure and right sub-figure are from distinct categories. It can be seen that except several high peaks in both sub-figures, others are flat among images and different codeword indexes. It is also worth to notice that for different classes, the locations of peaks in Fig. 5 are different.For the number of local descriptors assigned to different codewords, we divide the corresponding ranges into three parts: higher assignments as the peak or close to peak values in Fig. 5, lower assignments close to zero, and middle assignments which are neither higher nor lower parts.Fig. 6gives the location information of local descriptors with different number of assignments to one codeword. The blue circles in left sub-figure represent the locations of local descriptors in the cavity with highest assignments, and each red stars are the locations for those cavities with only one assignment. Moreover, stars of four different colors (pink, red, yellow and green) in right sub-figure in Fig. 6 show the locations of local descriptors in the cavity with middle level of assignments.It is clear to see that the locations in the left sub-figure could not grasp the essence of the image. In fact, the blue circles in Fig. 6 express the burstness phenomenon, i.e., the property that a given visual element appears more times in an image than a statistically independent model would predict [37]. Since the visual word with burstness mostly provides the background information, it should be attenuated during later processing. These peak or near peak assignments correspond to the ambiguity situation similar to that function words in the document vector space.However, the red stars carry similar indiscriminate information for recognition in the left sub-figure of Fig. 6, which is related to the misrepresentation of the ambiguity. It is interesting to find that this kind of one assignment appearance does not occasionally happen and may exist many times in one image (e.g., over 20 times in left sub-figure of Fig. 6). It also should be scaled down during recognition.Moreover, most stars related to the middle level assignments in the right sub-figure of Fig. 6 can represent the key information which is crucial to the recognition performance. The information grasped by the middle level assignments should be further enhanced.Burstness means that during assignment, some codewords are much more frequently selected than others. Inspired from text retrieval [38], most existing approaches use an inverse document frequency (idf) weighting scheme to alleviate the codeword effect which happens in all images. In fact, the traditional idf could not directly attenuate the burstness, especially when the size of codebook is large. In Fig. 5, it can be seen that the peaks do not happen at fixed locations along codeword index axis and image index axis, which means that the codeword with peak assignment in an image may have zero assignment in other images.Moreover, Ref. [37] proposed the modified idf with the score normalization. The normalization factor is to sum the score over the descriptors in the same codeword in the image for intra-image burstness or to sum the score over all descriptors in all images for inter-image burstness. The shortcomings of this modified idf lie in two aspects: 1) The normalization factor is computed over all local descriptors in all images, which will increase the computational burden. 2) It is more suitable for a retrieval task rather than a recognition task, and it does not consider the situation of lower assignments problem.For the unbalance assignment to each codeword, we propose to enhance the codewords with the middle level assignment in VLAD instead of penalizing the codewords with the higher or lower level assignments, since the middle level assignment has less ambiguity. In order to determine the range of middle level assignment, we assume the probability distribution function of the number of assignments as Gaussian distributions. Then the range can be determined by the mean and standard variance as follows:(7)range=maxc1,Nμ−k1Nσ,minc2,Nμ+k2Nσwhere Nμis the mean number of assignments in one image, and σ is corresponding standard variance. k1, k2c1 and c2 are constants determined by cross validation in following experiments.Then the pooled vector in VLAD is changed as:(8)vi=w∑xt:NNxt=ixt−uiifCardxt:NNxt=i∈range∑xt:NNxt=ixt−uiifCardxt:NNxt=i∉rangewhere w is bigger than one, and we vary this weight from 1 to 2.4 with 0.2 step increment.Pooling technique has played an important role in image representation based on local descriptors. Max pooling has shown its great advantages over other pooling strategies including average pooling and sum pooling. However, max pooling has not yet been investigated in VLAD due to non-trivial use in it. In this paper, we propose a new method to incorporate the newly proposed generalized max pooling strategy into VLAD.It has been indicated in Ref. [39] that max pooling is particularly well-suited to the separation of features with high sparsity. However, the conventional max pooling cannot be directly incorporated into VLAD. We investigate and adopt the recently proposed generalized max pooling (GMP) [40] into VLAD, which shows improved performance for all the different tasks.The generalized max pooling involves equalizing the similarity between local descriptors and the pooled representation, which is formulated as:(9)φtTφgmp=mwhere φtis the local descriptor and φgmpis the vector after generalized max pooling. When applying this rule into VLAD and setting m=1 since this constant has no influence for final classification, this equation is changed into:(10)xit−uiTφgmp=1.Expanding Eq. (10) into matrix form for cluster i, it can be rewritten as:(11)xi1−uixi2−ui⋯xiNiX−uiTφgmp=RTφgmp=1NiXwhere1NiXdenotes theNiXdimensional vector of all ones and R corresponds to the residual matrix. In practice, we can optimize the φgmpby following equation as:(12)φgmp=argminφRTφgmp−1NiX2+λφ2.This is a ridge regression problem whose solution is(13)φgmp=RRT+λI−1R1NiX.The regularization parameter λ can be determined during experiments. When λ is large enough, it backs to sum pooling asφgmp≈R1NiX/λ.

@&#CONCLUSIONS@&#
