@&#MAIN-TITLE@&#
Location difference of multiple distances based k-nearest neighbors algorithm

@&#HIGHLIGHTS@&#
The “location difference of multiple distances” and a method LDMDBA are proposed.LDMDBA has a time complexity of O(logdnlogn) and does not rely on tree structures.Only LDMDBA can be efficiently applied to high dimensional data.LDMDBA has a time complexity of (logdlogn) for predicting a new data point.LDMDBA has very good stability and can be applied to large databases.

@&#KEYPHRASES@&#
Location difference of multiple distances,k-nearest neighbors,Tree structure,

@&#ABSTRACT@&#
k-nearest neighbors (kNN) classifiers are commonly used in various applications due to their relative simplicity and the absence of necessary training. However, the time complexity of the basic algorithm is quadratic, which makes them inappropriate for large scale datasets. At the same time, the performance of most improved algorithms based on tree structures decreases rapidly with increase in dimensionality of dataset, and tree structures have different complexity in different datasets. In this paper, we introduce the concept of “location difference of multiple distances, and use it to measure the difference between different data points. In this way, location difference of multiple distances based nearest neighbors searching algorithm (LDMDBA) is proposed. LDMDBA has a time complexity of O(logdnlogn) and does not rely on a search tree. This makes LDMDBA the only kNN method that can be efficiently applied to high dimensional data and has very good stability on different datasets. In addition, most of the existing methods have a time complexity of O(n) to predict a data point outside the dataset. By contrast, LDMDBA has a time complexity of O(logdlogn) to predict a query point in datasets of different dimensions, and, therefore, can be applied in real systems and large scale databases. The effectiveness and efficiency of LDMDBA are demonstrated in experiments involving public and artificial datasets.

@&#INTRODUCTION@&#
kNN algorithms are used to find k-nearest neighbors of data points in a dataset. These algorithms are used in many fields including feature selection [1], pattern recognition [2–3], clustering [4], classification noise detection [5], and classification [6–10]. The basic method of finding k-nearest neighbors of a point is to compute all Euclidean distances from the query point to all other data points. This method is known as the full search algorithm (FSA). The FSA has a complexity of O(n2) so that it is very time consuming. To reduce the computation complexity, two classes of algorithms [11–27] were proposed.The first class of algorithms creates a search tree to store data points. In these algorithms, the search strategy is bounded by branches of the search tree. Fukunaga and Narendra [11] used the hierarchical clustering technique to decompose data points and represented results using Ball tree. The structure of Ball tree is highly influenced by clustering algorithms [12]. To further improve the performance, fiveBall tree construction methods were introduced by Omohundro [13]. Friedman et al. [14] used spatial decomposition to generate a balanced k-dimensional tree. A refined version of k–d tree method was introduced by Sproull [15]. The algorithm based on k–d tree method performs better than the Ball tree on datasets with a small dimensionality. Kim and Park [16] used the ordered partition method to create a multiple branch tree. Mico et al. [17] used a pre-stored distance table to eliminate more impossible nodes. McNames [18] proposed a method based on a principal axis search tree (PAT). Wang and Gan [19] combined projected clusters and the PAT algorithm to reduce the computation time. Chen et al. [20] used winner update search method and a lower-bound tree (LB tree) to speed up the algorithm.The other class of algorithms does not create a tree structure, but uses different methods. Cheng et al. [21] reduced the number of multiplication operations by using the min–max method. Bei and Gray [22] introduced the method of partial distortion to reduce the time of distance calculations. Ra and Kim [23] utilized the difference between mean values of the query point and other data points to eliminate impossible data points. Tai et al. [24] eliminated impossible data points using the projection values of data points. Nene and Nayar used a projection value to limit the distance from a query point [25]. Lu et al. [26] used the norm, mean value and variance to eliminate impossible data points. Lai et al. [27] utilized triangle inequality and projection values to accelerate the algorithm, which is now referred to as FkNNUPTI. These methods can speed up the process of finding nearest neighbors to some extent, but their time complexities have not been reduced any more so that they are still not enough efficient for different datasets.Among all available methods, the PAT algorithm [18,19,27] and algorithm based on an orthogonal search tree (OST) [28] have good performance for many types of benchmark datasets. PAT method creates a principal axis search tree according to projection values of data points onto principal axes of tree nodes. The principal axis of a node is evaluated using data points in the node and the principal component analysis (PCA) [18]. OST method chooses an orthogonal vector from the orthonormal basis for a node. The orthogonal vector selected for the node is perpendicular to orthogonal vectors chosen for ancestors of the node. As the number of orthogonal vectors is small, this method only requires little computation time to calculate projection values. Furthermore, the method uses an additional inequality to eliminate impossible data points, which cannot be deleted using the node elimination inequality. However, the performance of the algorithm deteriorates with the increase in dimensions, which is shown in [28] and our experiments. The reason is that higher dimensions lead to higher complexity of tree structures. Thus, as our experiments show, the performance of various kNN methods using various tree structures reduces significantly. In addition, as the complexity of the tree structure corresponding to various datasets is different, the stability of such methods is poor as well.In this paper, we introduce the concept of location differences and location difference based algorithm (LDMDBA) is proposed. The LDMDBA has a time complexity of O(logdnlogn) that is far less than FSA and most of other algorithms. The algorithm does not rely on any tree structure so that it can run efficiently on datasets of high dimensionality and has very good stability in various datasets. Furthermore, the algorithm has a time complexity of O(logdlogn) for predicting a data point outside datasets with different dimensionality. Therefore, unlike most existing kNN methods, the proposed algorithm can be applied in real systems and large scale databases. The performance of our method is compared with FSA and other six algorithms on datasets generated from different distributions and public benchmark datasets.The main contributions of this paper can be summarized as follows:•The concept of “location difference of multiple distances” and a new kNN algorithm LDMDBA are proposed. The LDMDBA has good prediction accuracy. In addition, its time complexity is equal to O(logdnlogn), which is no larger than existing algorithms.Unlike most existing fast kNN algorithms, our method does not rely on tree structures, so that its efficiency is not affected by dimensionality, and it can perform well on different datasets.The LDMDBA has a time complexity of (logdlogn) for predicting a new data point so that it is more suitable for large databases than existing algorithms.The effectiveness and robustness of the LDMDBA are explored and demonstrated by experiments on artificial and public real datasets.The rest of this paper is organized as follows. In Section 2, our proposed method is presented and described in detail. Experimental results are given in Sections 3 and4 and conclusions in Section 5.The proposed algorithm introduces a new measurement to measure location difference among different points. It requires only n (i.e. the number of data points) assignments of values of the measurement to each point. At the same time, the FSA usually need n2 calculations to compute the Euclidean distance between each pair of points. Therefore, by avoiding these calculations, the efficiency of the algorithm can be improved considerably.The method is based on the idea that neighbors have similar information about their location. The idea can be illustrated by the following example. Suppose, the information about the location of some people, including A, only contains the distances from these people to the North Pole. The problem is to find the people living near A. Although the spatial coordinates of these people and the Euclidean distances between them are unknown, they can be approximately estimated by the known information, i.e. the distances from those people to the North Pole. Let the distance from A to the North Pole be 100 m. The person whose distance to the North Pole is equal to 200 m is more likely to live near A than the person whose distance is equal to 10,000 m. This phenomenon can be further described using the example in Figs. 1 and 2.As is shown in Fig. 1(a), O is set as a reference point. The distance from point A to point B is denoted as AB. As the point B is a near neighbor of point A, the distance BO is close to the distance AO. On the other hand, the point C is located farther from point A compared with point B, and the distance CO is much larger than the distances AO and BO. Thus, the distances from the reference point can be used to measure the location difference instead of the Euclidean distance and find nearest neighbors. However, in some cases, the distance to a single reference point is not sufficient to find nearest neighbors accurately. As shown in Fig. 1(b), although the point A’ is located far from point A, the points are located on a same circle and they have approximately same distances to the center of the circle O (the reference point). Thus, multiple distances to some reference points can be used to measure location difference instead of the standard Euclidean distance and effectively find nearest neighbors. This method avoids computing the distances between each pair of points, and considerately improves the efficiency of the algorithm. In addition, it does not rely onany kind of tree structure. However, the neighbors found by multiple distances from some reference points are approximate. Therefore, in the proposed algorithm, the Euclidean distances between some data points are computed to further guarantee high accuracy.Based on the above analysis, the location difference of multiple distances is defined as follows:Definition 1LDMDBFGiven a database D, a point A∈D, and the norm denoted as∥·∥:Rd−>R, the distance from O1to A is denoted by dis(O1A). Higher dimensions need more reference data points to guarantee good prediction accuracy; at the same time, too many reference data points may lead to increase in computation time. Therefore, the number of reference data points is taken as log2d. The values of the first i dimensions of the ith reference point Oican be set to −1 and the other values are set to 1 (i.e. Oi= (−1, −1, −1…−1,1,1,…,1), where the number of values −1 is equal to i). The neighbors of point A found using the ith reference point are denoted by neighborsi(A), and label (D’) represents labels of all the points in D’. The label of A is determined by the sum of label (neighborsi(A)). Thus, the Location Difference of Multiple Distances Based Factor denotes the label of A that is computed using its neighbors found by the proposed method. LDMDBF(A) is equal to the sign of the sum of LDMDBFi(A):(1)LDMDBF(A)=sign(∑i=1log2dLDMDBFi(A))=sign(∑i=1log2d∑label(neighborsi(A)))where LDMDBFi(A) = ∑label(neighborsi(A)).The distancecan be denoted as:Disi(A) = ||A−Oi||Function sign is expressed as the following:sign(x)={1,x>=0−1,x<0Considering the mth point A as an example, to compute neighborsi(A), all data points are, first, sorted by the values of the LDMDBFi and a sorted sequence is obtained. The data points near A in the sorted sequence are approximate neighbors of A. In other words, the true k-nearest neighbors of point A are mostly located in a subsequence with the center point A in the sequence. The nearest neighbors will be more exact with a larger subsequence. The length of the subsequence varies with the number of neighbors, and can be denoted as 2k*ε, where k is the number of neighbors in the neighbor-searching algorithm andε is a positive value. All exact Euclidean distances between the data points in the subsequence are computed. Those points corresponding to the k smallest distances in the subsequence can be considered as the k-nearest neighbors of A.Algorithm 1LDMDBA (Location Difference of Multiple distances Based Algorithm):Given a database, D ∈ Rdlet k be the number of neighbors in the neighbor-searching algorithm. Starting with i = 1 for each pointA ∈ D, the algorithm follows the following steps.1.The ith reference point in LDMDBF is set as a vector whose values of first i dimensions are equal to −1, and the other values are set to1.Compute values of Disifor all data points.Sort data points by the values of Disiand generate a sorted sequence.For a subsequence of the points with the fixed range 2k*ε and center A, compute all exact Euclidean distances from A.Sort the distances obtained in step 4.The k points corresponding to the k smallest Euclidean distances are nearest neighbors of A.If neighbors of all data points using all reference points have been computed, compute the LDMDBF of all data points and terminate; else set i = i + 1, and go to step 1.To predict a new data point O outside D, it is not required to compute all distances from O to other points in D. The algorithm can be described as follows.Algorithm 2Location Difference of Multiple distances Based Algorithm to predict a new data point O:1.Compute the values of Disi(O).Insert O into the sorted sequence obtained as in Algorithm 1.For the subsequence with the fixed range 2k*ε and center O, compute all exact distances from O.Sort the distances obtained in step 3.The k points corresponding to the k smallest Euclidean distances are neighborsi(O).If the value of the LDMDBFi(O) of each reference point has been already computed, O is assigned the value of LDMDBF; else set i= i+1, and go to step 1.A binary search algorithm can be used to insert O into the sorted sequence in step 2. Then, the time complexity of Algorithm 2 is equal to the one of the binary search algorithm. That is O(logn) ∼1 instead of O(n), which is smaller than existing algorithms. Using the binary search algorithm, the process of predicting a new point can be significantly accelerated. This makes LDMDBA applicable to large scale databases, unlike most existing algorithms.In Algorithm 1, the time complexity is determined by step 3 and step 5. It is equal to the maximum of the time complexity of step 3 and time complexity of step 5 in the whole algorithm. The time complexity of step 5 is determined by the used sorting algorithm, and can be ensured to be within the range O(nlog2n)∼ O(n). If εis set as a constant or a small value (will be discussed inSection 2.2), the time complexity of Algorithm 1 is within the range O(log2dnlog2n)∼ O(log2dn). It can be noticed that the complexity of the proposed method is not larger than the existing fast algorithms. Furthermore, LDMDBA does not require any dimensionality dependent index structures (e.g., R-tree). Therefore, LDMDBA can be efficiently applied to various high dimensional datasets.The core part of the LDMDBA for using a single reference point is referenced as LDMDBAI algorithm. As discussed in Section 2.1, the LDMDBAI algorithm can be summarized by the following pseudo-code.Require:k: representing the number of nearest neighbors, p: the reference data point.s1: the number of data points in the dataset, s2: the dimensionality of thedataset.delta = the integral part of log(log(s1))),i1=j− delta *k.i2=j+ delta *k, j2=1, labels: predicated labels of all data points obtained bythe proposed algorithm.EnsureStep 1: Compute the Euclidean distances from each point to the referencepoint p.Fori=1 to s1dis(i) = the Euclidean distance from ith data point to pend forStep 2: Sort the vector “dis”; the resulting values are assigned to “meas”; theindexes are assigned to b.Step 3: compute the exact k-nearest neighbors for all data points.Forj=1 to s1a)  if (i1<1)or(i2>s1)Adjust values of i1 and i2 in the bound of the dataset.b)  end ifc)  fori=i1 to i2dob1(j2,:) = a matrix that consists of the index of data point b(j),distances from its LDMDBF, meas(j) and its neighbors meas(i) in theresulting sequence.j2=j2+1;d)  end fore)  fori=1to2* delta *k+1dis2(i) = the exact distance from the b(j)th point to its neighborwhose index is b1(i,1) in the sequence.f)  end forg)  b2 = the matrix consisting of b2 and the transposition of dis2.h)  if the number of labels “+1” in b2> the number of labels “−1” in b2labels(b(j)) = 1i)  elsej)  labels(b(j)) = −1k)  endend forThis section discusses a method of determining the value of parameterεthat makes the algorithm self-adaptive. Clearly, whenε becomes larger, the subsequence in step 5 of Algorithm 1 contains more points. Then, the exact neighbors are more likely to be close to the query point. However, larger values ofε may increase the time complexity of Algorithm 1. In the extreme case, if ε is equal to n, Algorithm 1 becomes a conventional algorithm whose time complexity is O(dn2). Thus,εshould be set as large as possible under the condition that the time complexity is bounded by O(log2dnlogn).It can be observed that the time complexity of Algorithm 1 is equal to O(n2) if εis a linear in n (e.g. n/2). Thus, ε should be set as an expression whose time complexity is lower than O(n). As the exponential function with the exponent smaller than 1 and logarithmic function have lower time complexity than O(n), ε may be set as any such function.If ε is set as an exponential function of n (i.e. n1/m, where m is a positive value greater than 1), the length of the subsequence in Algorithm 1 is 2*k*n1/m. Computing the distance from a data point to a reference point has time complexity of 2*k*n1/mlog2(2*k*n1/m). As the distances to a reference point need to be computed for all data points, the time complexity of step 5 in Algorithm 1 is equal to n*2*k*n1/mlog2(2*k*n1/m).limn→∞n*2*k*n1/mlog2(2*k*n1/m)nlog2n>limn→∞2*k*n1/mlog2(n1/m)log2n=limn→∞2*1/m*k*n1/mlog2nlog2n=limn→∞(2*1/m*k*n1/m)=+∞Thus, the time complexity is greater than O(nlog(n)) whenεis set as an exponential function of n. It indicates thatεshould not be set as an exponential function of n.If ε is set as a logarithmic function of n (i.e. logmn, where m is a positive value greater than 1), the time complexity of step 5 in Algorithm 1 for a reference point is equal to n*2*k*logmn*log2(2*k*logmn).limn→∞n*2*k*logmn*log2(2*k*logmn)nlog2n>limn→∞2*k*logmn*log2(2*k*logmn)log2n>limn→∞logm2*log2n*log2(logm2*log2n)log2n=limn→∞(logm2*log2(logm2*log2n))=+∞Thus, the time complexity of step 5 is greater than O(nlogn) ifεis set as logmn. Furthermore, ifεis set as logmlogmn, the time complexity of step 5 becomes n*2*k*logmlogmn log2(2*k*logmlogmn).The limitation of the quotient of time complexity of step 5 and sorting algorithm used in Algorithm 1 can be expressed as follows:(2)limn→∞n*2*k*logmlogmnlog2(2*k*logmlogmn)nlog2n>limn→∞logmlogmnlog2logmlogmnlog2n=limn→∞logmlogmnlog2logmlogmnlog2n=limn→∞logm2*log2(logm2*log2n)*log2log2(logm2*log2n)log2n=limn→∞logm2*(log2logm2+log2log2n)*log2(log2logm2+log2log2n)log2n=logm2*(limn→∞log2logm2*log2(log2logm2+log2log2n)log2n+limn→∞log2log2n*log2(log2logm2+log2log2n)log2n)=logm2*log2logm2*limn→∞log2(log2logm2+log2log2n)log2n+logm2*limn→∞log2log2n*log2(log2logm2+log2log2n)log2n=logm2*log2logm2*×limn→∞1/(log2logm2+log2log2n)*1/ln2*1/log2n*1/ln2*1/n1/n+logm2*limn→∞(1/log2n*1/ln2*1/n)*log2(log2logm2+log2log2n)1/n+logm2*×limn→∞log2log2n*1/(log2logm2+log2log2n)*1/ln2*1/log2n*1/ln2*1/n1/n=logm2*log2logm2*limn→∞(1/ln2*1/log2n*1/ln2)+logm2*limn→∞((1/log2n*1/ln2)*log2(log2logm2+log2log2n))+logm2*limn→∞(log2log2n*1/(log2logm2+log2log2n)*1/×ln2*1/log2n*1/ln2)In Eq. (2),logm2*log2logm2*limn→∞(1/ln2*1/log2n*1/ln2)=0logm2*limn→∞((1/log2n*1/ln2)*log2(log2logm2+log2log2n))=logm2ln2*limn→∞(log2(log2logm2+log2log2n)log2n)=logm2ln2*×limn→∞(1/(log2logm2+log2log2n)*1/ln2*1/log2n*1/ln2*1/n1/n)=logm2ln2*limn→∞(1/(log2logm2+log2log2n)*×1/ln2*1/log2n*1/ln2)=0logm2*limn→∞(log2log2n/(log2logm2+log2log2n)*1/ln2*1/log2n*1/ln2)=logm2ln2*ln2*limn→∞(log2log2n(log2logm2+log2log2n)*log2n)=logm2ln2*ln2*×limn→∞(1/log2n*1/ln2*1/n1/log2n*1/ln2*1/n*log2n+(log2logm2+log2log2n)*1/ln2*1/n)=logm2ln2*ln2*limn→∞(1/log2n1+(log2logm2+log2log2n))=0Thus,Equation(2)=logm2*limn→∞(1*1/ln2*1/log2n*1/ln2)=0Therefore, the time complexity of step 5 in Algorithm 1 is smaller than that of the sorting algorithm whenεis set as(3)logmlogmnThe time complexity of Algorithm 1 can be ensured to be O(log2dnlog2n)∼ O(log2dn). The smaller the m, the larger the value of expression (3) and the length of the subsequence in Algorithm 1. Therefore, m can be taken as 2.In order to intuitively explain the algorithm, artificial datasets having two dimensions and containing 60 data points are generated by Gaussian mixture model. First half of data points are identically distributed with mean 0 and variance 1 and labeled as“+1”. Second half of data points are identically distributed with mean 2 and variance 1 and labeled as“−1”. Data points in this dataset are shown in Fig. 4. Algorithm 1 is used to find nearest neighbors and classify the data points. The number of neighbors in Algorithm1 is set as 3. All the experimental tests were executed on a computer with Intel I5-2450 processor, 4GB RAM, and Window 7 operating system. Algorithms were realized using MATLAB 2014a development environment.The experimental results of the dataset in Fig. 4 are summarized in Tables 1and 2. The results for thirty data points are shown in the tables. The results of the LDMDBA are compared with the FSA. It should be noted that some data points may be generated in an area being populated by points with the opposite label drawn from the Gaussian model (e.g. the 51th data point in Fig. 4).It can be observed that about 80% of neighbors of each point founded by the LDMDBA are the same as those found by the FSA method. It indicates that some neighbors found by the proposed algorithm may not be the nearest. Take the 12th point in Table 1 as an example. Its nearest neighbors are points numbered 15, 20 and 25. However, its neighbors found by the LDMDBA are the points numbered 7, 15, 16, 20 and 25. It can be observed that, although the distances from the7th and 16th points to the 12th point are larger than the distance from the nearest points, the distances from these points are very close to the 12th point (c.f. Fig. 4). Thus, as shown in Table 1, the prediction label of the 12th point generated by the LDMDBA is the same as in the FSA. The prediction labels generated by the LDMDBA are also the same as the FSA in Table 1. In other words, neighbors found by the LDMDBA are close to query point, enough to accurately label the point. In fact, the labels computed by FSA method are the same as the labels by the LDMDBA in Table 1. Both prediction accuracies are equal to 0.95.In this section, the LDMDBA is compared with the FSA, Ball tree [11], k–d tree [14], PAT algorithm [18], LB tree [20], FkNNUPTI [27], and OST [28] algorithms. In the experiment, search trees were constructed with nc=16 for Ball tree, PAT, and OST; where nc is the number of child nodes. For k–d tree, each bucket contains 40 points. For LB tree, the LBT+HAAR method [20] is used.The proposed methods are compared with the existing algorithms using five publicly available datasets (these public datasets can be downloaded from: http://www.csie.ntu.edu.tw/∼cjlin/libSVMtools/datasets/). As shown in Table 3, the datasets are Breast cancer, Diabetes, Fourclass, Svmguide1, Svmguide3 and Cod-RNA.A summary of the datasets is shown in Table 3. As a training process is not required in the kNN algorithm, cross validation is not used in the experiments. Thus, the labels of all data points are obtained by classification without training process. Fig. 5shows the comparison of prediction accuracy of the algorithms when the parameter k is set to 3, 6 and 9. It can be seen, that the prediction accuracy of the LDMDBA is slightly lower compared with the other methods on average when k is set to different values. However, the good performance of the LDMDBA on high dimensional datasets will be shown in the next sections.In this section, the efficiency of theLDMDBA is demonstrated by comparing it with other algorithms. The number of neighbors in the neighbor-searching algorithms is set to be 3. All the experimental tests were performed on a computer with Intel I5-2450 processor, 4GB RAM, and Window7 operating system. Algorithms were realized using MATLAB 2014a development environment. As the execution time may vary due to multi-tasking environment, all values presented in the tables below are mean values of the total computation time obtained in three repeated tests.Example 1Public datasets.The comparison of computation time of all the algorithms is shown in Table 4. The computation time of the LDMDBA is far less than the one of the FSA method, and similar to other algorithms. The OST method has the least computation time on datasets Diabetes, Breast cancer, Fourclass, and Svmguide3. In the next sections, the experiments will be implemented on datasets containing more data points with higher dimensionality.Example 2Datasets generated from the uniform Markov sourceThe first dataset contains 1000 data points with dimensionality ranging from 4 to 65,536. The results are summarized in Table 5. The second dataset contains 10,000 data points, and the results are summarized in Table 6. To avoid consuming too much computational time, the dimensionality of the second dataset range from 4 to 1024. Data points in the two datasets are generated from the uniform Markov source with values in [0, 1]. Let a be the correlation coefficient of the uniform Markov sequence. The Markov sequence {yi} is generated by the following equation:yi+1+1=ayi+uwhere u is a random vector generated from the uniform distribution with values in [0, 1], and y0=0.In this example, the value of a is set as 0.9 for each dataset. Tables 5 and 6 summarize the total computation time.When the dimensionality of the dataset is 4 in Table 5, various methods perform much better than FAS, where OST shows the best performance. However, when the dimensionality increases, the computation time of other methods moves closer to FSA, with the exception for the LDMDBA. When dimensionality is equal to 65,536, the computation time of the other methods is about 93% of the FSA. However, the computation time of the LDMDBA is about 43% of the FSA.As shown in Table 5, when the dimensionality increases to 1024, the computation time of the LDMDBA is 6653 ms, about 26% of the FSA. In Table 6, when the dimensionality increases to 1024, the computation time of the LDMDBA is 156,248 ms, about 3.6% of the FSA, and much lower than in Table 6. In contrast, the minimum computation time among all the other methods is 1379221ms, shown by the FkNNUPTI; it is approximately32% of the FSA. Therefore, especially in datasets containing more data points and of higher dimension, the LDMDBA performs much better than other methods in terms of the computation time. The reason is that, the proposed method does not rely on a tree structure and is not influenced by dimensionality. However, other fast methods rely on tree structures, so that their time complexity approaches that of the FSA when dimensionality increases. Thus, on high dimensional data, with more data points, the LDMDBA has advantages compared with other methods.To see how the number of data points in the dataset affects the results, we present the corresponding graphs in Fig. 6. As shown in Fig. 6(a), the growth rate of the computation time of the proposed method is not larger than the growth rate of the other methods in low dimensional dataset (dimensionality = 16) when the number of data points increases. Thus, even for low dimensions, the LDMDBA shows a good performance. When dimensionality increases to 1000 (c.f. Fig. 6(b)), the growth rate of the proposed method is much lower than other methods when the number of data points increases. This indicates that the LDMDBA performs much better than other methods for high dimensions.Fig. 7shows the effect of the value of k. Each method was executed on a dataset with 10,000 data points with different values of k. As methods based on the index structure show a poor performance for higher dimension, it was set to a low value. As can be seen in From Fig. 7, all the methods have similar growth rate with increase in k.Example 3Datasets generated from the clustered Gaussian distribution.In this example, datasets are generated from the Gaussian data source with dimensionality = 16. Ten cluster centers are first generated from the uniform distribution with values in [0, 1]. They are: 0.5358, 0.4451, 0.1239, 0.4903, 0.8529,	0.8739, 0.2702, 0.2084, 0.5649 and 0.6403. These centers are the means of Gaussian distribution for each cluster containing 100 data points. That is, each dataset consists of 1000 data points. Table 7 shows the total computation time on the datasets with various standard deviations and dimensionality = 16. As can be seen in Table 7, the k–d tree and PAT methods are highly influenced by the standard deviation of the dataset. The variance of the computation time of the LDMDBA is smallest among the methods, which indicates that it has least sensitivity to the standard deviation and best stability. This behavior can be attributed to the variation in the complexity of tree structures corresponding to various datasets, resulting in poor stability. At the same time, the LDMDBA does not rely on any tree structure. As other kNN methods are unsuitable for very high dimensional datasets, the performance of the LDMDBA is studied for high dimensions. The results are shown in Table 8. As the size of five datasets of dimensionality= 65,536 is larger than 3GB, these datasets cannot be loaded into the memory. Therefore, the results of these datasets are excluded from Table 8. It can be observed that the changes in the computation time of the LDMDBA are very small for both low and high dimensions. It indicates that the proposed algorithm has excellent stability in datasets with various dimensions.Example 4Datasets containing different number of data points generated from the Gaussian distribution.In this example, the computation time of predicting a new data point outside the dataset is calculated to compare the performance on datasets containing different number of data points. Algorithm 2 is used in this example. Datasets are generated from Gaussian distribution with mean = 0 and variance = 1. It should be noted that the logarithmic scale is used for the computation time, as the variations are too large to show on a linear scale. Fig. 8 shows the results when dimensionality= 16. When the number of data points increases to 729000, the prediction time of the FSA is larger than 211ms, more than 1000 times larger than the other algorithms. The LDMDBA and the other tree structure based methods have much lower prediction time than the FSA. However, the performance of the algorithms based on tree structures in Fig. 9deteriorates rapidly. The reason is that the increase in dimensionality leads to much higher complexity of tree structure. This also makes most kNN algorithms unsuitable for real time systems and large scale databases with various dimensionalities. In contrast, the LDMDBA does not depend on an index structure. Therefore, it can maintain its excellent performance. The prediction of a new data point in the LDMDBA is equivalent to the problem of inserting a value in a sorted sequence, where the time complexity is O(logdlogn)∼logd. The computation time of the LDMDBA is much smaller than 1 second when the number of data points increases to 729000. Thus, in comparison to the other kNN algorithms, the proposed method is the only fast method that can be applied in real systems and various high scale databases.This paper proposes a novel kNN method “LDMDBA” based on the concept of “location difference of multiple distances”. The prediction accuracy of the LDMDBA is very close to that of the FSA, while its time complexity of O(logdnlogn) is much lower than the FSA and most other algorithms. This becomes especially clear in datasets of high dimensionality. Our experiments show that, the proposed algorithm have much better performance than other efficient kNN methods based on tree structures. Furthermore, the complexity of the tree structure corresponding to various datasets is different. Therefore, as shown in the experiments, the stability of methods based on tree structures is very poor. In contrast, the LDMDBA does not rely on any tree structure, so that it shows very good stability on different datasets. Furthermore, the proposed algorithm has the time complexity of O(logdlogn) for predicting a new data point in the datasets with different dimensions. Experimental results show that the prediction time of a new data point using the LDMDBA is much lower than other algorithms. This makes it suitable in large scale database, unlike existing kNN methods. Further, the LDMDBA can be used in many applications where kNN methods can be applied in general, such as recommender systems, face detection, signal processing, speech recognition, 3D target recognition, text classification and so on.Although the efficiency of the algorithm is improved considerably, a small loss in prediction accuracy (i.e. 0∼0.02) of the LDMDBA compared with FSA still exists on some datasets. Therefore, our future research may focus on improvements of the model to increase its prediction accuracy.

@&#CONCLUSIONS@&#
