@&#MAIN-TITLE@&#
An improved simplified swarm optimization

@&#HIGHLIGHTS@&#
A new simplified swarm optimization (iSSO) is proposed.The roles of the proposed UMs and iterative local search are also tested.Numerical examples conclude that iSSO outperforms ABC for 50 benchmark functions.

@&#KEYPHRASES@&#
Swarm Intelligence (SI),Simplified Swarm Optimization (SSO),Artificial Bee Colony Algorithm (ABC),Soft Computing (SC),Optimization problems,

@&#ABSTRACT@&#
This paper introduces an improved simplified swarm optimization (iSSO) by undertaking a major revision of the update mechanism (UM) of traditional SSO. To test its performance, the proposed iSSO is compared with another recently introduced swarm-based algorithm, the Artificial Bee Colony Algorithm (ABC), on 50 different widely used multivariable and multimodal numerical test functions. Numerical examples conclude that the proposed iSSO outperforms ABC in both solution quality and efficiency. We also test the roles of the proposed UMs and iterative local search. The proposed algorithm is thus useful to both practitioners and researchers.

@&#INTRODUCTION@&#
Since the early 1990s, Soft Computing (SC) has been utilized to obtain optimal or good-quality solutions to difficult optimization problems in fields for which exact analytical methods do not produce optimal solutions in an acceptable amount of time, especially for problems that are large in size [51]. It is well known that SC mainly involves fuzzy sets, neural networks, genetic algorithms, and rough sets. In recent years, we have seen an increasing interest in SC in creating methodologies with the aims of resembling and simulating phenomena of nature to solve larger problems in science and technology; resulting methods include Artificial Neural Network (ANN) [9,25], Simulated Annealing (SA) [23], Tabu Search (TS) [13], Genetic Algorithm (GA) [10,14], Ant Colony Optimization (ACO) [7], Particle Swarm Optimization (PSO) [21,22,28,12,40], Estimation Distribution of Algorithm (EDA) [24], Differential Evolution [31,32], Artificial Bee Colony Algorithm (ABC) [16,17,19,20], Simplified Swarm Optimization (SSO) [44–50], Firefly algorithm [41], Biogeography-based Optimization [29], Cuckoo search [42,39], Bat Algorithm [43], and Krill Herd [11,15,34–38].Swarm Intelligence (SI) is a newly developed branch of SC that belongs to the category of population-based stochastic optimization. Swarm Intelligence (SI) was introduced by Gerardo Beni and Jing Wang in 1989 in the context of cellular robotic systems [3]. Bonabeau, Dorigo and Theraulaz defined SI as “any attempt to design algorithms or distributed problem-solving devices inspired by the collective behavior of social insect colonies and other animal societies” [4]. SI has also received significant interest from researchers studying in a range of research areas and has been applied to several real-world problems.ABC is one of the most recently introduced SI algorithms and was originally designed by Karabog in 2005 [19] to simulate the intelligent foraging behavior of a honeybee swarm, based on the model proposed by Tereshko and Loengarov [33]. Research in the literature indicates that the performance of the ABC is better than or similar to that of other population-based algorithms, including GA, PSO, and EDA [16,17,19,20]. Hence, ABC is selected to compare with the proposed approach.SSO is an emerging population-based stochastic optimization method [44–48,2,1,6,5,27,30,26,8]. It is another new SI method and is also an evolutionary computation method. SSO was proposed by Yeh to overcome the drawbacks of PSO [44]. The advantages of SSO are its simplicity (easy to implement and having only a few parameters to tune), efficiency (a fast convergence rate), and flexibility. Simulation results reveal that SSO has better convergence to quality solutions than PSO, GA, EDA, and ANN [44–48]. SSO has therefore attracted considerable attention and has been widely used in a range of applications, such as redundancy allocation [44,30], data mining [45,48,27,26], health care management [6,5,8], and supply chain management [46,47,2,1].The goal of this work is to propose an improved SSO (here termed iSSO). iSSO is able to address non-discrete data. To demonstrate the efficiency of the proposed iSSO, a comprehensive comparative study is presented on the performances of the well-known swarm-based algorithms SSO and ABC and the variant iSSO for optimizing a large set of multi-variable and multi-modal numerical test functions. For iSSO, there is also consideration of using iterative local search (ILS) to the best solution, ILS to all improved solutions, or neither.This paper is organized as follows. Section 2 provides respective descriptions of ABC, which may be the best-known SC for solving numerical functions, and SSO, which is the basis of the proposed iSSO and related UM. The proposed UM and iSSO are discussed in Section 3. A comprehensive comparative study on the performances of the proposed UM with or without ILS and SSO based on 50 benchmark functions is given in Section 4. The complete comparison of the proposed iSSO and ABC, together with the proposed UM, is discussed in Section 5. Finally, the conclusion and discussion are given in Section 6.Let Nvar, Nsol, Ngen, and Nrun represent the number of variables (attributes, dimensions, genes), solutions, (populations chromosomes, particles), generations (called cycles in ABC), and independent replications. Most SCs, e.g., GA, SA, ACO, PSO, ABC, SSO, and the proposed iSSO, generate randomly distributed initial solutions with population size Nsol inside the problem space and then search for optimal solutions by updating generations. Each solution is encoded as a finite-length string with a fitness value. Note that large Nsol and Ngen will slow down the optimization process significantly, and small Nsol and Ngen will be unable to find a good solution for the requirements.LetXit=(xi1t,xi2t,…,xi,Nvart)be the ith solution (called a food source in ABC) at generationt,xijtbe the jth variable ofXit, andf(Xit)be the fitness function ofXit. The major difference among all SC methods is the UM that is used to update the current solutions. Hence, we focus on the UMs of ABC, SSO, and iSSO in the rest of the study. Sometimes, ILSs are implemented after using UMs to improve solutions by moving them to their neighbors according to neighborhood structures, but this is done at the expense of running time.In ABC, the quality of the possible solutionXitis denoted byF(Xit)and defined as follows for i=1,2,…,Nsol and t=1,2,…,Ngen [16,17,19,20].(1)F(Xit)=1f(Xit)+1,f(Xit)⩾01+f(Xit),otherwiseThere are three UMs, called the employed bee, the onlooker (bee), and the scout (bee), in ABC. The first half of the bee colony implements employed bees, while the second half implements onlookers. ABC assumes only one employed bee per food source.In the first UM, “the employed bees,”Xitis updated toXit+1based on the following equation:(2)xikt+1=xikt+ρ[-1,1]·(xikt-xjkt),where both the variable k and solutionXjtare selected randomly andρIis a uniform variable in the interval I, wherei,j=1,2,…,Nsol, k=1,2,…,Nvar, and t=0,1,2,…,Ngen−1. Ifxikt+1falls outside the acceptable range, it is set to the corresponding extreme value in that range. Moreover, ifF(Xit+1)is worse thanF(Xit), thenXit+1is replaced byXit(called a greedy selection mechanism in ABC).To guide ABC toward more highly fit positions of the search space, the second UM, “the onlooker,” is a local search operator. After a solution, say,Xit, is generated or updated, the onlooker bee is implemented Nsol times with probability Pr(Xit)defined as follows:(3)Pr(Xit)=F(Xit)∑i=1NsolF(Xit).If a solution, say,Xit, does not improve for a predetermined number of iterations(4)Nlim=Nsol×Nvar,then the third UM, “the scout,” is a special ILS that is implemented to regenerateXitrandomly and repeatedly.The proposed iSSO is based on SSO. Before discussing the proposed iSSO, basic SSO is introduced formally in this sub-section. LetpBestPi=(pi1,pi2,…,pi,Nvar)be the best fitness function value of the ith solution with its own history and gbest G=(g1,g2,…,gNvar)be the solution with the best fitness function value among all pBests, where i=1,2,…,Nsol.The fundamental concept of SSO is that each variable of any solution needs to be updated to a value related to its current value, its current pBest (as a local search), the gBest (as a global search), or a random feasible value to maintain population diversity and enhance the capacity to escape from a local optimum [44–48]. The UM of SSO is based only on the following simple mathematical modeling aftercw,cp, andcgare given:(5)xijt+1=xijtifρ[0,1]∈[0,Cw=cw)pijtifρ[0,1]∈[Cw,Cp=Cw+cp)gjifρ[0,1]∈[Cp,Cg=Cp+cg)xifρ[0,1]∈[Cg,1),where i=1,2,…,Nsol, j=1,2,…,Nvar, t=0,1,2,…,Ngen−1, and x is a random number between the lower bound and the upper bound of the jth variable. Note thatcw,cp, andcgcan be fixed or flexible.The UM of SSO is much simpler than those of other major soft computing techniques, such as PSO (which must calculate both the velocity and position functions), GA (which requires genetic operations such as crossover and mutation), EDA (which has difficulty building an appropriate probability model), and the Immune System Algorithm (which does not consider the interaction of variables) [44–48]. Using the above simple approaches, the diversity of a population can be maintained efficiently.In this section, iSSO is proposed together with the all-variable-UM (called UMa hereafter). Furthermore, one ILS is proposed to help improve solution quality.In the traditional SSO, all variables are changed simultaneously based on Eq. (5). In the proposed iSSO, Eq. (5) is revised for floating-point data listed as follows:(6)xijt+1=xijt+ρ[-.5,.5]·ujifxijt=gjorρ[0,1]∈[0,Cr=cr)gj+ρ[-.5,.5]·ujifxijt≠gjandρ[0,1]∈[Cr,Cg=cr+cg)xijt+ρ[-.5,.5]·(xijt-gj)ifxijt≠gjandρ[0,1]∈[Cg,1=cr+cg+cw],where(7)uj=xjmin-xjmax2·Nvar,xjminandxjmaxare the lower bound and upper bound of the jth variable.Eq. (6) represents a major revision of fundamental SSO to fit the continuous variables as follows:1.A variable is updated to its neighborhood using Eq. (7) if it is selected based oncr, i.e., the first item in Eq. (6).A variable is updated to the neighborhood of gbest if it is selected based oncg, i.e., the second item in Eq. (6).A variable is updated from the interval between itself and gbest if it is selected based oncw, i.e., the third item in Eq. (6).If any variable violates the boundary condition, it is set to the nearest boundary after using Eq. (6). Moreover, unlike SSO,Xit+1is set toXitafter updating in the proposed iSSO iff(Xit+1)is not better thanf(Xit),i.e.,(8)Xit+1=Xitiff(Xit+1)is worse thanf(Xit)Xit+1otherwise.There are four major stopping criteria used in SC:1.The generation number.The number to calculate the fitness function.The number of unchanged best solutions.The time limit.One population (solution/particle/chromosome) calculates the fitness function only once in each generation for any SC method not using ILS, e.g., SSO, GA, PSO, etc., but that is not always true for those algorithms using ILS, e.g., ABC. Moreover, some SC methods take a long time and use complicated methods, e.g., the second UM, “the onlooker,” in ABC, before calculating the fitness function or selecting the populations to update, e.g., the roulette wheel selection in GA. Hence, to make a fair comparison of the proposed iSSO with ABC and related SSO variants, a time limit of 1.25s is adopted here as the stopping criterion. For easy observation of the trends and changes in the entire evolution process, the 1.25s is separated into five periods, i.e., 0.∼.25, .25∼.5, .5∼.75, .75∼.1, and 1∼1.25(s).It is well-known that ILS can improve solution quality. To test whether it is necessary to add ILS in the proposed iSSO, the first item in Eq. (6) is used as the UM in ILS, i.e.,(9)xj=xj+ρ[-.5,.5]uj,whereujis defined in Eq. (7) andxjis a variable selected randomly from a solution, say, X. Eq. (9) is implemented repeatedly on X from the first variable to the last variable, i.e., Nvar consecutive times in total. The fitness function of the new X is calculated immediately after each implementation of Eq. (9). Note that X can only be updated to be a better value; otherwise, X retains its original value. After Nvar times, another Nvar consecutive iterations of ILS are implemented if X is improved. The above procedure is repeated until there is no improvement during the corresponding Nvar times.UMa is implemented in the proposed iSSO to search for the optimum. In UMa, all variables are updated one-by-one in each solution, say,Xit, as follows:STEP 1a.Let j=1.Updatexijttoxijt+1using Eq. (6).Letxijt+1=xjminifxijt+1<xjminorxijt+1=xjmaxifxijt+1>xjmax.Ifj<Nvar, letj=j+1and go to STEP 2a.LetXit+1=Xitiff(Xit+1)is not better thanf(Xit).The flowchart of the proposed UMa is shown in Fig. 1.Each solution needs to be updated based on Eq. (5) in SSO. Becausecr<cw<cp<cg, the expected number of comparisons to each solution in Eq. (5) is(10)Nvar·(cw+2cp+3cg+4cr)=Nvar·(1+cp+2cg+3cr)⩾Nvar·(1+cw+cp+cg+3cr)⩾Nvar·(2+2cr)>2·NvarIn Eq. (6), we letCr=crandCg=Cr+cginstead ofCg=cg,Cp=Cg+cp,Cw=Cp+cwand remove pBest from the traditional SSO. The expected number of comparisons to each variable is reduced to(11)cr+2cg+3cw=1+cg+2cw<2,(12)Nvar·(cr+2cg+3cw)=Nvar·(1+cg+2cw)<2·Nvar.Both Eqs. (11) and (12) are less than that of SSO listed in Eq. (10) without considering the convergent rate.Let T be the current runtime. The flowchart of the proposed iSSO is shown in Fig. 2, and the overall procedure is summarized as follows:Procedure of iSSOSTEP 1.InitializeXi0randomly, calculatef(Xi0), find gbest G, and let t=1 for i=1,2,…,Nsol.Let i=1.If1.25⩽T, halt.Implement UMa to updateXitfrom STEP 1a to STEP 5a as listed in Section 3.4.Ifi<Nsol, leti=i+1and go to STEP 3. Otherwise, lett=t+1and go to STEP 2.To evaluate the quality and performance of the proposed iSSO with or without ILS, three iSSO variants and SSO are to be tested:(1)iSSO: the proposed iSSO with no use of any ILS;gILS: the same as the proposed iSSO, but it implements Eq. (9) for eachgbest in all generations;aILS: the same as the proposed gILS, but it also implements Eq. (9) for any improved solutions and includes the gbest in the initial solutions;SSO: the traditional SSO withcg=.4,cp=.3,cw=.2, andcr=.1.These four methods are applied to 50 benchmark functions [20], which are presented in Appendix A. Appendix A also lists the corresponding initial ranges, formulations, characteristics, dimensions, and global optimum values of these problems. These 50 benchmark functions are well-known and have been extensively used in literature to evaluate alternative approaches in soft computing, such as ABC, GA, DEA, PSO and EA [20]. Moreover, these problems are large enough to include many different types of problems, such as unimodal, multimodal, regular, irregular, separable, non-separable and multidimensional issues.All proposed iSSO variants and SSO are implemented in the C programming language and run on an Intel Core i7 3.07GHz PC with 6GB memory. The runtime unit is CPU seconds. Moreover,cr=.45,cg=.40,cw=.15(in Eq. (6)), Nsol=50, and Nrun=30 (i.e., each independent replication needs to optimize 50 benchmark functions). The stopping criterion is 1.25 CPU sec as discussed in Section 3.2. The corresponding fitness function evaluation number (FE) and fitness function values (FV) are recorded at the end of each time interval, where T1=(0.,.25], T2=(.25,.5], T3=(.5,.75], T4=(.75,1.], and T5=(1.,1.25]. Let the notation AVG, MAX, MIN, and STDEV denote the average, maximal (the worst), minimal (the best), and standard deviation of the related values.The experimental results of the four methods are summarized in Table 1and ranked in Table 2.It is interesting to observe that iSSO<gILS<aILS (the smaller the better) for all statistics (AVG, MAX, MIN, and STDEV) from T1 to T5 in both FE and FV, i.e., the methods where aILS/gILS are not implemented are better than those methods where ILS is implemented. The reason may be that the proposed UMa is good enough in both global search and local search. However, aILS and Eq. (9) are only effective in local search, which can be fully replaced by Eq. (6).From Tables 1 and 2, the FE of SSO is much better than that of the rest of the methods for all time intervals and all aspects (AVG, MAX, MIN, and STDEV). The reason is that Eq. (5) is more efficient than Eq. (6). However, the FV of SSO is the worst of all four methods and shows a great difference, i.e., the proposed new UMs based on Eq. (6) are much more effective than the UM based on Eq. (5) in SSO with respect to continuous types of variables. Hence, we only focus on the comparison of the three iSSO variants in the rest of this section.Moreover, iSSO demonstrates the best results of all three methods in AVG, MAX, MIN and STDEV from T1 to T6. These observations can also be found in Table 2: iSSO is the best among all of the methods in FV.Based on Section 4, the better average FVs in each time interval are found by iSSO. Hence, the proposed iSSO is implemented using UMa without ILS. The results obtained from ABC outperform the results presented by DE, PSO and EA [16,17,19,20]. Additionally, ABC is the only one tested on these 50 benchmark problems. Hence, we only compare the proposed iSSO with ABC. Note that the ABC code is provided from the ABC website: http://mf.erciyes.edu.tr/abc/.For fair comparison, ABC is also implemented in the C programming language and run on an Intel Core i7 3.07GHz PC with 6GB memory, the runtime unit is CPU seconds, and the stopping criterion is 1.25s, as discussed in Section 3.2, just like the experiments in Section 4. The corresponding fitness function evaluation number (FE) and fitness function values (FV) are also recorded at the end of each time interval of T1=(0.,.25], T2=(.25,.5], T3=(.5,.75], T4=(.75,1.], and T5=(1.,1.25].Moreover, the colony population is 100; the number of employed bees, onlooker bees and food resources are all 50; and the number of scout bees was selected to be at most one for each cycle in ABC [16,17,19,20].In this section, we focus on comparing the average values (AVG), the worst values (MAX), the best values (MIN), and the standard deviation values (STDEV) of the obtained solutions to see the overall performance of iSSO and ABC. Table 3lists all of the average values of iSSO and ABC in AVG, MIN, MAX, and STDEV for T1–T5; Fig. 3shows all of the boxplots for these FV obtained from iSSO or ABC that are greater than10-5; Table 4lists the number of better related values obtained from ABC and iSSO for each benchmark with 30 replicates.From Tables 3 and 4 and Fig. 3, in general:1.FE: ABC is always more stable and better than iSSO in FE, and the values of iSSO/ABC are almost fixed for AVG, MAX, and MIN. The reason for this is that it takes longer to calculate Eq. (3) (the “onlooker” UM of ABC) than Eq. (6) (the UM of iSSO). Hence, the UM in iSSO is more efficient than that of ABC.FV: iSSO is superior to ABC in AVG, MAX, and STDEV, but ABC has better MIN in T2, T3, and T5. Moreover, the values of iSSO/ABC also tend to increase; hence, iSSO is more effective and robust than ABC on average, even in the worst case.Hence, the solutions obtained from iSSO are more effective and robust than those of ABC, and the UMs in iSSO are also more efficient than those in ABC.Statistical analysis is always a powerful and useful tool for analyzing experiment results. Table 5lists all relatedΔ(FV), whereΔ(FV) denotes the average fitness values of the 30 final gbest of ABC minus those of iSSO for each benchmark. From Table 5, the values ofΔ(FV) do not form a normal distribution because many values of the FV are zero. Hence, a distribution-free test called the nonparametric Mann Whitney test [18] is used to testΔ(FV) to check whether there is a significant difference between the mean FVs obtained from ABC and iSSO at a critical value of 0.05 for T1–T5.The statistical nonparametric Mann Whitney test results are shown in Table 5, where the superscript “∗” means that the mean FV for the related benchmark function obtained from iSSO is significantly better than that of ABC. In contrast, the superscript “#” means that the mean FV obtained from ABC significantly outperforms that of iSSO. The performance of each method can be analyzed using a maximum possible improvement (MPI) calculated by (the number that iSSO is significant better than that of ABC in minus the number that ABC is significant better than that of iSSO in)/(the number that ABC is significant better than that of iSSO in)×100%.Generally, the proposed iSSO is better than ABC in FV for each time period. The detailed comparisons are discussed as follows.1.In both T1 and T2: There are 17 items marked by ∗ and 12 items marked by #, i.e., MPI is (17/12–1)=41.7%. Hence, iSSO is better than ABC at the critical value 0.05 in the earlier stages.In T3: The number of items marked by ∗ is increased from 17 in T1 and T2 to 19. In contrast, the number of items marked by # is decreased from 12 in T1 and T2 to 8 at the critical value 0.05. Hence, MPI is (19/8/1)=137.5%. Thus, compared with the solutions found by ABC, the solutions found by iSSO have relatively more significant improvement and are better than those of ABC in T3.In T4: The number of items marked by ∗ is increased from 19 in T3 to 20, and the number of items marked by # is decreased from 8 in T3 to 7. In this time period, the MPI is (20/7–1)=185.7%, which is the highest among all periods. Hence, the overall FV of iSSO is much superior to that of ABC, and the solution quality is also further increased at the critical value 0.05.In T5: The number of the items marked by ∗ and # are decreased to 15 (from 20) and increased to 9 (from 7) at the critical value 0.05, respectively, i.e., the MPI is decreased to 66.7%. The reason is that ABC can finally obtain optimums for these problems that cannot be solved by ABC before T5. However, the solution quality of iSSO is still comparatively better than that found by ABC. Additionally, from the above, iSSO has a higher chance of obtaining the optimums more frequently and earlier than ABC.From the nonparametric Mann Whitney test, in general, iSSO exhibits better performance than ABC in solution quality.This work proposed a continuous version of SSO with a new UM to enhance the ability of the traditional SSO to solve continuous problems, which must use continuous variables to yield better results. The UM is always the major part of soft commuting. Hence, this paper presents significant and novel modifications to SSO.A comprehensive comparative study on the performances of the proposed iSSO and ABC together with UMa and three ILS strategies has been made. The solution quality of the proposed iSSO is clearly superior to and more robust than that of ABC, which may be the best-known SC for solving numerical functions. Thus, the proposed iSSO based on UMa has the ability to avoid local traps and can be efficiently used to solve larger-scale, multivariable, and multimodal functions. SSO has been used more than the other existing methods in supply chain management, scheduling, redundancy allocation problems, forecasting, data mining, etc. In future research, we will focus on strengthening SSO performance; we will also apply the resulting method to different optimization problems and attempt to solve real-life practical engineering problems with more variables or larger-scale benchmark problems. Additionally, in the present work, the proposed method is only compared with ABC. The iSSO method will be further compared with more state-of-the-art metaheuristic methods, such as the Firefly algorithm [41], Biogeography-based Optimization [29], Cuckoo search [42], Bat Algorithm [43], and Krill Herd [11], in the future. Furthermore, mathematical analysis using a dynamic system, such as Markov chains, is going to prove and explain the convergence of the proposed method in future research.

@&#CONCLUSIONS@&#
