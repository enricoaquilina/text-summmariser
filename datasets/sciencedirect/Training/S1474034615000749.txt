@&#MAIN-TITLE@&#
An active inference approach to on-line agent monitoring in safety–critical systems

@&#HIGHLIGHTS@&#
Active inference drives expectations about future environmental state transitions.Probabilistic characterization of a priori beliefs for an optimally behaving agent.Twin Gaussian processes are used to detect abnormal behavior.A surprise index fast pinpoints anomalous behavior.

@&#KEYPHRASES@&#
Active inference,Bayesian surprise,On-line monitoring,Twin Gaussian processes,

@&#ABSTRACT@&#
The current trend towards integrating software agents in safety–critical systems such as drones, autonomous cars and medical devices, which must operate in uncertain environments, gives rise to the need of on-line detection of an unexpected behavior. In this work, on-line monitoring is carried out by comparing environmental state transitions with prior beliefs descriptive of optimal behavior. The agent policy is computed analytically using linearly solvable Markov decision processes. Active inference using prior beliefs allows a monitor proactively rehearsing on-line future agent actions over a rolling horizon so as to generate expectations to discover surprising behaviors. A Bayesian surprise metric is proposed based on twin Gaussian processes to measure the difference between prior and posterior beliefs about state transitions in the agent environment. Using a sliding window of sampled data, beliefs are updated a posteriori by comparing a sequence of state transitions with the ones predicted using the optimal policy. An artificial pancreas for diabetic patients is used as a representative example.uncontrolled or passive dynamicsinput-gain matrixdictionary of training examplesintegral operator of the desirability functionGaussian process with meanmand covariance kcovariance functionKullback–Leibler distancetransition probability distribution for the passive dynamicstransition probability distribution for the controlled dynamicsstate transition distribution under optimal controlstate transition distribution under any implemented controlmatrix of transition probabilities for the passive dynamicscost functionenvironment internal or hidden statesurprise indexTwin Kullback-Leibler distancecurrent control action of agentoptimal cost-to-go functionobservable environmental stateeach state dimensionsequence of state transition given system observationssequence of state transition given the specificationtransition for each state dimension.desirability functionmonitor’s beliefsa given state transition distribution in the agent environmentmonitor’s belief distributions over future state transitionsdistance between belief and state transition distributionskernel width parameternoise varianceKronecker delta functionoptimal control policyan implemented control policyscaling parameter for Brownian noiseBrownian noiseblood glucose levelhepatic sensitivityinsulin sensitivityinsulin infusion levelglycemic sensor outputsensor time-lag parametersensor calibration parameter

@&#INTRODUCTION@&#
Anomaly or novelty detection refers to the process of pinpointing unusual and unexpected events, behaviors or patterns that give rise to concerns regarding system safety or performance. This is especially important for monitoring safety–critical systems in which faulty conditions need to be fast accounted for [1]. The issue of anomaly detection has generated substantial research over past years. Complete reviews of the novelty detection literature during the last decade can be found in [2,3]. It is rather clear from the existing works that the most common form of anomaly detection in use today is based on thresholds, fixed limits or crisp boundaries between what would be considered “normal” or expected and something unexpected or abnormal. Typically, when the value of a key variable is above or below of a pre-specified bound or limit, the system is considered to be in an anomalous state. For example, a performance monitor for model-based controllers was proposed using a minimum variance benchmark for a model residuals obtainable from closed-loop data [4]. Similarly in [5], the temperature measured across bridge girders was correlated to a certain state of structural performance degradation. Patterns of physiological deterioration in hospital patients (evident in the vital signs, such as heart and respiratory rate) have been classified using support vector machines [6]. Automatic detection of landmarks in the environment for topological mapping has been associated with surprising measurements, where a location is classified to be a landmark if its surprise value exceeds a given threshold [7]. In a method for detecting lane deviation of a vehicle [8], the difference between a center point of lane markers and the center point of the vehicle is computed in order to alert the driver that a dangerous deviation exists.In system monitoring, the most active research area is control loop performance monitoring based on statistics computed from plant data. The most significant development was due to the work of Harris [9], who proposed the novel concept of a minimum variance controller as the characteristic behavior of an optimal regulator. Thus, performance monitoring can be based on comparing the observed mean squared error of the controlled output against its corresponding minimum variance. This optimal behavior is based on the theoretical framework of minimum variance control previously developed [10] and has already been implemented with some success in monitoring the performance of a control loop for a diabetic patient [11]. More recently, Harrison and Qin [12] developed a minimum variance performance map that address the impact of constraints on a predictive controller operation, which highlights the inadequacy of the minimum variance criterion for more complex control tasks such as real-time optimization and intelligent control with economic objectives. Unlike the well-known minimum variance index for SISO loops, a recent approach was proposed by Srinivasan et al. [13], which does not require for performance monitoring any knowledge about time delays or other plant parameters. However, due to unavoidable uncertainties about environmental conditions, none of the existing performance monitoring algorithms can give a conclusive answer regarding normality or abnormality in a control loop operation [14,15].It is worth noting that most of the existing literature about on-line monitoring assumes that the observed system is a “passive” entity not situated in an environment. Therefore, a passive monitoring approach assumes that certain environmental variables are simply correlated to a certain system state or condition according to an immutable relationship; e.g., a temperature distribution across a structure have a strong correlation with structural response. However, agents are “situated” entities and usually perform in time-varying environments that maintain a feedback loop with them. By responding to an external stimulus, the agent carry out actions whose effects give rise to a sequence of state transitions in its environment, which is in turn affected by other agent’s actions, setting a dynamic structure as depicted in Fig. 1. An agent behavior is thus an emergence of its control policy for responding to different environmental stimulus and hence it cannot be regarded to a fixed reference, threshold or bound. Unfortunately, an agent policy cannot be directly monitored since it remains hidden to any external observer.Behavior monitoring aims at characterizing the control policy that an agent uses to react to external stimuli by inferring the state transition distributionΨ(xk)in the agent environment as a result of its actions. Besides noise and variability, uncertainty arises as a consequence of hidden states either because some variables are only partially observable or because they are related to the perception an agent has of its environment. As an example consider highway monitoring where a driver’s actions (e.g. to change lane) can only be inferred through observable changes in its immediate environment (the car steering), as well as the effect on it of other nearby affected agents (drivers). Clearly, the monitor perception change over time as sensory information from the agent environment is updated [16]. Thus, for behavior monitoring, the main consequence of the assumption of a situated agent is that any change in the environmental dynamics is mostly a consequence of the agent’s policy. Furthermore, notice that the raw readings from sensors do not usually correspond to the agent actions nor the states it perceives or the rewards it obtains, which are difficult if not impossible to measure directly by the monitor. It is thus reasonable to assume that only changes occurring in the nearby environment in which the agent performs are part of the monitor’s perception. For example, a highway monitor may recognize if a driver increases the speed by considering temporal variations in the vehicle position, as opposed to predicting car velocity based on the specific force applied on the car’s throttle.By ascribing the evolution of environmental transitions to a certain agent policy, a representation that captures much of the relevant information of an agent behavior is obtained. Since behavior in uncertain environments is a phenomenon that unfolds over time, the monitor needs to constantly infer potential outcomes of future agent actions in response to sensory information(xk)of the environmental state. To achieve this, the monitor continuously revises its hypotheses in order to update prior beliefs(μk)that include predictive distributionsΨ(μk)over future state transitions. This strategy is characterized in this work in terms of an active inference approach [17], in which the monitor perceives the nearby environment of an agent in order to contrast its prior beliefs about expected state transitions. These prior beliefs are built around a generative model of an optimally controlled stochastic process for state transitions, which involves predictions of what should be sampled in order to validate prior beliefs. Since this stochastic process has been a priori optimized, some of the state transitions are expected to be observed more frequently than others and labeled accordingly as “desirable” [18]. In particular, an optimal behavior is one that reduces the divergence between desirable state transitions and the observed ones. Therefore, any deviation between the agent behavior and the desired one can be detected in a natural way through the Kullback–Leibler distance between belief distributions and posterior distributions for state transitions perceived through sensory dataξ=D(Ψ(μk)||Ψ(xk)).For on-line monitoring, an indication of surprise should arise from a mismatch between expectations and what is actually perceived. Surprise is thus an information measure that requires an active inference of the environmental effect of future agent actions, which may change an observer beliefs as the environmental dynamics unfolds. However, it is important to figure out expectations about the different environmental situations the agent may face and how each situation may be characterized. In general, expectations are representations of the values that some perceptual features are likely to assume in the future. Because of uncertainty, expectations are naturally expressed in probabilistic terms such that a probability distribution over the range of possible observations can be considered to be a “belief state,” usually conditioned on a particular unobservable state or hidden context. This approach has been applied in [19] to support patients with dementia during hand-washing tasks. The position of certain objects (e.g., hands and towel) is evaluated by the monitor to estimate the progress of the user in the task as well as its current mental condition. This is expressed as a belief state over a range of possible sub-tasks and mental states. Therefore, if an estimated probability of an observation is available, then the certainty of a sequence of perceptions can be compared to its probability of occurrence yielding a measure of surprise.This article is structured as follows. Section 2 describes agent behavior monitoring as an active inference problem based on an optimal control policy. Section 3 briefly introduces the principles of optimal action selection to characterize the expected behavior under uncertainty. Expected behavior is formalized as a controlled stochastic process that makes the agent policy as close as possible to the desired one by describing environmental transitions in probabilistic terms. In Section 4, surprise is quantified as the Kullback–Leibler divergence between distributions about the desired and the actual state transitions in order to pinpoint any deviation from the expected agent behavior. In Section 5, an artificial pancreas is used as case study in which sensor and actuator faults which may endanger safety or optimal operation are considered. Finally, in Section 6 some remarks and future works are discussed.Perception is essential to intelligent systems since it helps cognitive agents to build their hypotheses and select an optimal course of action in the face of a changing environment. A rational agent chooses actions seeking to maximize the utility obtained despite uncertainty and unplanned events. As it is shown in Fig. 1, the agent influences the nearby environment which is mostly affected by its actions whereas the environment responds by means of rewards and state transitions. As a result, the sequence of observable state transitions in the agent environment is important for monitoring its behavior. This is fundamental, since the agent’s policy usually remains unobservable to an external observer in most cases. Bearing this in mind, and through a probabilistic characterization of the agent policy under uncertainty, it is possible to indirectly monitor the agent policy by accounting for changes in the environmental dynamics. These changes are considered as the observable responses to the agent actions, but could be influenced by actions taken by other agents and uncertainty about internal agent states.Our monitoring approach starts by differentiating the probabilistic process descriptive of the real environmental dynamics that generates new observations, from the expectation of such dynamics that is modeled by the monitor. As an example, Fig. 2highlights that by perceiving an internal statesk, the driver chooses the actionuk, which causes the environment to evolve to the statesk+1, while the driver obtains a (unobserved) rewardrk. The Markovian dependences between the driver actions affecting internal states, which subsequently yield observable responsesxkto the monitor, are highlighted in Fig. 2. Notice how the given sequence of actions performed by the agent does not have a direct influence on the monitor’s posterior beliefsμk+1; the agent policy just becomes apparent when it manipulates the sequence of environmental transitions. Therefore, the monitor perception cannot be directly related to the agent actions, but rather to the observable changes in the environmental dynamics which is affected as a result. Thus, active inference necessarily involves on-line rehearsing of future actions of agents over a rolling horizon so as to generate expectations that help discovering surprising agent behaviors.From the monitor’s perspective, each environmental statexkperceived is able to modify or reinforce the prior beliefsμk, continuously transforming them into a posterior distributionμk+1according to the Bayes rule. This constitutes a behavior monitoring approach that employs, from a Bayesian point of view, the probability distribution for the next environmental response as a reference to the monitor. It is important to remark this is a moving horizon method, since the posterior state transition density obtained corresponds to the a priori distribution for the next time interval.The active inference monitoring approach is based on an optimal control policy for the agent under uncertainty. This policy allows simulating fictitious state transitions that serve as reference to the monitor. In this sense, prior beliefs correspond to a description of how the agent is expected to act, and hence beliefs are described as a probabilistic model of expected state transitions that the monitor uses to assess the agent behavior. It is important to differentiate between the probabilistic process over which the monitor makes predictions, and the actual environmental dynamics that generates the sensor signals perceived by the monitor. Predictions about state transitions are the result of a generative model, which has been trained to infer a sequence of fictive state transitions and is described in the next section. Predicted transitions are optimal in the sense they are the result of an optimally acting agent in the face of uncertainty. However, predicted transitions are nothing more than an internal representation used by the monitor that may or may not match the future evolution of the environmental dynamics.The active inference approach to agent monitoring implies that based on prior beliefs, new predictions about environmental state transitions can be made. The monitor’s specification is built upon Gaussian distributions that embody a sequence of state transitions, which take place when the environmental dynamics is controlled by an optimal policy under uncertainty. The optimal policy is obtained using a type of Markov decision process whose control law can be framed as a linear problem with an analytical solution (see Section 3). This linearization is accomplished through an exponential transformation of the Bellman’s equation, which leads to more efficient numerical methods. Since the sequence of state transitions simulated by the generative model embeds an optimal policy, any agent that behaves properly is one that reinforces the monitor beliefs over time.Consider for instance the problem of monitoring a vehicle on a road, as in Fig. 3. The image describes the virtual environment perceived by a monitor that is observing the vehicle while the driver is about to make a left turn. There exist pedestrians, cyclists and other vehicles setting a dynamical scene which assists the driver and the monitor to characterize the situation by means of only observable information. Detection dropouts caused by noise, occlusions, and other artifacts are assumed to be hidden states since they can influence sensory data but cannot be measured directly. Based on an optimal behavior specification, the monitor infers a hypothetical trajectory for the vehicle, depending on its prior knowledge and the environment state [20]. After observing the agent environment, the monitor contrasts incoming sensory information with prior beliefs that describe the expected behavior of a proficient driver, such that the car efficiently avoids static and dynamic obstacles while obeying traffic rules. Any deviation from the expected behavior can be detected by quantifying the distance between the observed state transitions and the predicted ones.It is worth noting that the environmental states perceived by the driver are not necessarily the same states perceived by the monitor, since the driver may discern a number of activities that remain unobservable to the monitor. For instance, a mobile phone vibrating may alter the driver attention, but the monitor is clearly unaware of such situation. As a result, the monitor must infer the agent (driver) behavior just through the evolution of the state transitions in the system composed by the driver and its vehicle.As another example (left side of Fig. 3), a rolling ball on the lane should prompt -for both the driver and the monitor- the possibility of a child running out from the sidewalk. From the point of view of the monitor, a proficient driver should react by slowing down the car to a halt and looking out for someone. This is because the monitor beliefs should consider as prior knowledge both normal and abnormal situations and specify the optimal agent behavior in each scenario. In turn, a rolling ball may be an unexpected or surprising event for the driver considering its prior beliefs, yet the driver behavior to handle it should not to be surprising to the monitor. This example shows that for on-line monitoring of an agent behavior, an event is surprising not because its probability is small in an absolute sense, but rather because its probability is relatively small given the prior belief distribution of the observer (monitor) [21].A distinctive problem in monitoring situated agents is that the monitor’s perception is partially blinded, which creates uncertainty about the agent policy that explains the observed state transitions. Prior beliefs about the optimal policy are instrumental in order to predict expected responses given the history of observed state transitions over a rolling horizon. In this work, the monitor beliefs are transformed into a generative (probabilistic) model which makes possible to infer future state transitions resulting from an optimally behaving agent.In the generative model, state transition probabilities are modeled using Gaussian processes (GPs) [22] that provide information about confidence intervals for the predicted next state. A GP is a collection of random variables, any finite number of which has a joint Gaussian distribution. For Gaussian process regression, those random variables represent the value of the functionf(x)for inputsx. It is assumed thatf(x)is a zero mean stationary Gaussian process with covariance functionk(xi,xj), encoding correlations between pairs of random variables(1)cov(f(xi,xj))=k(xi,xj)One covariance function particularly useful is the Gaussian(2)k(xi,xj)=exp(-γr‖xi-xj‖2)+λrδijwithγr⩾0the kernel width parameter,λr⩾0the noise variance andδijthe Kronecker delta function, which is 1 ifi=jand 0 otherwise. This prior for the kernel function constrains input samples that are nearby to have highly correlated outputs.GPs are parameterized based on a sequence of observations resulting from on-going interactions between the agent and its environment. Given a state vectorxk, a separate GP model is trained for each state dimensionxk, in such a way the uncertainty about its change due to a (hidden) agent action is modeled statistically as(3)Δxk∼GP(m,k)wheremis the mean function andkis the covariance function. The training inputs for a Gaussian modelGPare the environmental states, whereas the targets are the differences between the successor state and the state in which the action was applied. For an inputxk, the multivariate predictive distributionp(xk+1|xk)is Gaussian distributed.To describe the monitor’s prior beliefs, the transition probabilityp∗(xk+1|xk)should correspond to an optimally controlled system dynamics. The superscript indicates that the transition probability is shifted by the optimal control policyπ∗(x). As a result, the monitor is allowed to ascribe any change in the environmental dynamics to the agent policy. On the other hand, another Gaussian model would describe the current observed system behavior in terms of a Gaussian processpg(xk+1|xk)which may deviate from the corresponding Gaussian process for optimal action selection. The superscript indicates that the transition probability is shifted by any implemented control policyπg(x). It is worth noting that the Gaussian model that characterizes the current implementation needs to be updated on-line as new data is available. This vis-à-vis comparison betweenpg(xk+1|xk)andp∗(xk+1|xk)allows the monitor to contrast the characterization of the observed agent behavior to its specification. As it will be discussed later, this comparison can be performed through Bayesian methods that can measure the divergence between two stochastic processes.As expressed above, the main challenge for monitoring an agent behavior is how the environmental response to an optimal agent policy can be characterized in the face of uncertainty. To this aim, a probabilistic representation of the desired behavior has to be built upon a stochastic process of the optimally controlled system dynamics. To obtain the expected agent policy under uncertainty a class of Markov decision process is used. Linearly-solvable Markov decision processes (LSMDPs) [23] correspond to a class of optimal control problems in which the Bellman’s equation can be converted into a linear equation by an exponential transformation of the state value function. The Bellman’s equation is fundamental to optimal control theory. This equation was first introduced as the cornerstone of the Dynamic Programming framework (DP) [24]. Later on, the Bellman’s optimality condition was instrumental for reinforcement Learning (RL) [25] algorithms, which are very general but can be inefficient [23]. The RL problem consists of learning iteratively to achieve a goal from ongoing interactions with a real or simulated system, while DP is a general method of solving sequential optimization problems using a probabilistic model of state transitions. In both cases, the idea is to predict the value or utility of future actions. Accordingly, optimal actions cannot be found by greedy optimization of the immediate cost, but instead all future costs must be taken into account. To this aim, the optimal cost-to-go functionv(x)is defined as the expected cumulative cost for starting at state x and acting optimally thereafter. Indeed, the Bellman equation characterizesv(x)only implicitly, as the solution to a dynamic optimization problem. A major advantage of using this optimization method is that it provides an explicit optimal decision policy for the agent. Therefore, based on LSMDP it is possible to build a specification of the system dynamics over time when the agent acts according to the obtained optimal policy. The latter is instrumental to detect on-line discrepancies between the beliefs and the environmental dynamics affected by the agent behavior.Stochastic processes do not have time derivatives in the conventional sense and, as a result, they cannot be manipulated using the ordinary rules of calculus. Ito [26] provided a way around this problem by defining a particular kind of uncertainty representation based on the Wiener process as a building block. An Ito process is thus a stochastic process whose state transition function is represented by the equation(4)dx=a(x)dt+B(x)(udt+σdω)whereω∈Rnu(same space as actions) andσdenote Brownian noise and its scaling parameter, respectively. The expressiona(x)describes the uncontrolled or passive dynamics [23] andB(x)is the input-gain matrix. It is important to remark that, since noise and control signals act over the same space, any state can be reached by the effect of either inherent system variability or control actions.In order to express Eq. (4) in a more convenient form, theh-step transition probability for the passive dynamicsp0is expressed as a Gaussian distributionNas(5)p0(xk+1|xk)=N(xk+1|xk+ha(x)+hB(x),hσB(x)TB(x))The controlled diffusion processpuis approximated as a deterministic function expressed as a Gaussian distribution whose mean and covariance are given as(6)puk(xk+1|xk)=N(xk+1|xk+h(a(x)+hB(x)u),Σ)One way of thinking of the net effect of control actions is noting how they change the distribution of the next state from(xk+ha(x)+hB(x),Σ)to(xk+h(a(x)+hB(x)u),Σ), where∑=hσB(x)TB(x)is the covariance. In other words, the controller shifts the probability distribution from one region of the state space to another [27]. More generally, we can think of the system under study as having a passive dynamics with a distributionp0over future states, thus the controller acts by modifying this distribution to obtain a new dynamicspuk.Thereby, the situated agent shifts the probability distribution for state transitions from one region of its state space to another by acting on its environment. A control policyπ(x)is thus defined as the probability of choosing the control actionukat statexk. For any optimal control application, the main objective is to find an optimal policyπ∗(x)which minimizes the expected cumulative cost functionv(x)as(7)v∗(x)=minuℓ(x,π(x))+Ex′∼pu(x′|x)[v(x′)]wherex′denotes the next state for a given control actionu. The minimum cumulative cost for starting at statexkand acting optimally thereafter enables greedy computation of optimal actions. Notice that Eq. (7) corresponds to the Bellman fundamental equation, which can be simplified by assuming that the immediate cost function is(8)ℓ(x,u)=hq(x)+KL(pu(xk+1|xk)||p0(xk+1|xk))The state costq(x)is an arbitrary function encoding how (un)desirable different states are, andKLis the Kullback–Leibler divergence that measures the distance between the optimally-controlled dynamics and the passive one. This distance can be understood as the price to pay for the optimal shift of the passive dynamics by actionu. TheKLdivergence between the above Gaussian processes can be proven to beh/2σ2‖u‖2which is the quadratic energy cost accumulated over intervalh. By introducing the exponential transformationz=exp(-v), the Bellman’s equation can be conveniently re-written as [23]:(9)z(x)=exp(-hp(x))G[z](x)wherez(x)is the desirability function defined as(10)z(x)=exp(-v∗(x))andG[z](x)is an integral operator defined as(11)G[f](x)=∫p0(x′|x)f(x′)dx′In contrast to the cost functionv(x), the negative exponential portrays which states are more desirable. Once the desirability function is found, the optimal control policy is computed analytically and expressed in closed form as(12)u∗(x)=-σ2B(x)Tvx(x)In this manner, optimal actions can be expressed analytically given the optimal cost-to-go. Thus, instead of finding a trajectory-based optimal solution, the goal is to find a globally optimal policy over the entire state space.The continuous problem given in Eq. (4) is solved by choosing a set of states{xn}and adjusting the matrixPk,k+1of transition probabilities fromxktoxk+1given by the passive dynamics described in Eq. (5). Since the operatorG[z](x)is linear, Eq. (9) is also linear and can be expressed in vector notation. Defining the vectorzwith elementsz(xn)and the matrixQwith elementsexp(-hq(xn))on its main diagonal Eq. (9) becomes(13)z=QPzfor the first exit formulation, namely goal-direct problems. Noteworthy, Eq. (13) can be solved by an iteration method in exponential form [18].The mountain car problem provides a challenging problem, particularly when random fluctuations affect the observable state transitions. The model dynamics simulates a point mass moving along a convex surface in the presence of gravity, see Fig. 4. The admissible range of forces is not sufficient to drive up the car greedily from the initial state to the goal position. The observable state vector isx=[x1,x2], wherex1andx2denote horizontal position and the tangential velocity of the car, respectively. The car dynamics is given by(14)dx1=x2cos(atan(s′(x1)))dtdx2=-gx2sgn(x1)sin(atan(s′(x1)))dt-βx2dt+udt+σdωwhere g=9.8m/s2 is the gravitational constant and β=0.5 is the damping coefficient. The goal for the driving agent is defined by all states such that |x1−2.5|<0.2 and |x2|<0.5. The cost model thus encodes the task of parking the car at the horizontal position 2.5 in minimal time and with minimal terminal velocity. Costs are accumulated from time 0 to infinity but accumulation stops when the car achieves the goal. Error tolerance is needed because the system dynamics is stochastic.To approximate the desirability function in Eq. (13), we use a discretization of the environment using a 101-by-101 grid spanningx1∈±3andx2∈±9for the car problem. The passive dynamics is constructed by a discretization of the time axis (with time step h=0.05) and defining probabilistic transitions among discrete states. The noise distribution is discretized at 9 points spanning±3 standard deviations in thex2direction and using a noise scale parameterσ=0.1.Once the desirability function is optimized using the passive dynamics, the control policy is derived from the computed desirability function using Eq. (12). The results are shown in Fig. 4; where (b) corresponds to the state cost functionq(x)bearing that the agent is goal-driven; (c) is the optimal cost-to-go function obtained (solid lines show stochastic trajectories resulting from the optimal behavior with different levels of variability and initial states); (d) depicts the obtained optimal control policy regarding prior beliefs about the agent behavior. In all plots blue correspond to smaller values and red to larger values.Surprise quantifies how observing new data affects the internal beliefs a monitor may have about an agent behavior and its control policy. Observations that leave the prior beliefs unaffected are not surprising and -revealing that the monitor hypotheses are confirmed by data-, whereas data observations that cause the monitor to significantly revise their prior beliefs give rise to a surprising condition. Thus, a surprising agent behavior is related to a sequence of state transitions which become suspicious whenever the stochastic process describing the agent policy through environmental state transitions deviates with respect to the monitor’s beliefs based on an optimal policy.The monitor’s beliefs are updated on-line as new sensory information arrives, transforming prior belief distributions into posterior ones. According to this, the fundamental effect of dataDon the monitor is to change its prior distributionP(M)into a posterior distributionP(M|D)via the Bayes theorem. Conveniently, Bayesian surprise is measured using the distance between the posterior and prior distributions based on the Kullback–Leibler divergenceT(D,M)=KL(P(M|D)||P(M)). TheKLdivergence, or relative entropy, should be understood as a measure of the difficulty of discriminating between two distributions.Since the environmental dynamics is affected by uncertainty, a robust metric of Bayesian surprise must be considered. To quantify the similarity between the optimal behavior and a given stochastic implementation of the agent policy, twin Gaussian processes (TGP) proposed by Bo and Sminchisescu [28] are used (see Fig. 5). A TGP provides a powerful strategy for structured prediction using GP priors on both covariates and responses [29]. Hyper-parameters for both multivariate inputs and estimated outputs are obtained through minimization of the Kullback–Leibler divergence between two GPs modeled over finite input sets of training examples, emphasizing the goal that similar inputs should produce similar responses. Instead of computing surprise point-wise for each new estimation using the GPs models, two data sets containing a sequence of the lastWstate transitions,Xg={Δxkg}k-WkandX∗={Δxk∗}k-Wk, are used to characterize the observed and the specified environmental dynamics, respectively. Then, the joint distribution of observed state differences can be modeled using a zero-mean multivariate Gaussian distribution as(15)(Xg)T∼Ng0,Rijri]riTrwhose covarianceKgis given by the kernel matrixRij=k(Δxig,Δxjg), the kernel vectorri=k(Δxig,Δxjg)and the kernel valuer=k(Δxkg,Δxkg). The kernel used here, is the one given in Eq. (2). For the implementation modeled asNg(0,Kg)based on a sampled sequence of state transitions, the offset or distance to the prior beliefs distributionN∗(0,K∗)is key to calculate a robust measure of surprise. This is achieved by computing the Kullback–Leibler divergence between Gaussian processes as(16)TKL(Ng||N∗)=-N2-12log|Kg|+12Tr{Kg(K∗)-1}+12log|K∗|The Kullback–Leibler divergence is therefore non-negative, and zero if and only if the two multivariate Gaussian distributions have the same covariance. In the latter case, the sequence of state transitions caused by a given agent policy has no surprise regarding observed environmental transitions, i.e. they can be associated to an optimal behavior. In Fig. 5,TKLis used to compute the performance of the implemented dynamicsGPgagainst the dynamicsGPpfor the optimal agent behavior. Notice that instead of computing theKLdistance point-wise for each new estimationΔxk,TKLuses a sequence of the observed state transitions{Δxk}k-Wkover a rolling horizon, which gives a more robust description of any deviant behavior.To quantify the impact of changes in the agent policy, it is necessary to introduce metrics aimed to describe in relative terms the resulting performance under uncertainty. Here, a ratio between the computed surprise value and the maximum value of surprise observed when the agent performs optimally is proposed. This is a good indicator of a deviant agent behavior, since meaningful peaks in theTKLindex produced by unexpected state transitions are many orders of magnitude larger than “background” stochastic values associated with nominal data. The surprise index(SI)is then expressed as(17)SI=TKLTKLmax∗whereTKLis the surprise value for the observed transitions (subject to anomalies) andTKLmax∗is the maximum surprise value faced when the agent behaves optimally. From Eq. (17) we can presume that anySIvalue bigger than the unit might be a symptom of an anomaly (or anomalies) affecting the agent behavior or its nearby environment.A general description of the approach for on-line monitoring using Bayesian surprise is shown in the flowchart in Fig. 6. To start, the monitor observes the environmental statexk. Based on the current state, the posterior transition distributions for prior beliefs and the implementation are computed. The associated estimationsΔxkgandΔxk∗are included in their respective data sets in order to estimateTKL. This is achieved through a moving window strategy which adds new estimations whereas the oldest one is removed. This allows maintaining the monitor’s beliefs updated so as to fast detect changes in the agent behavior. At every moment theTKLvalue obtained is compared against its maximum value allowed through theSIindex. If at any instant the index is bigger than 1, a warning signal should be activated.To fast detect any change in the agent behavior, it is clear the importance of training the corresponding GPs using a reduced and relevant set of state transitions. Hence, the lastNmax=50 transitions are used as the input set to train the GP model for the optimally controlled stochastic process (see Section 2.2). Bayesian surprise is later computed using a moving window strategy over the last W=30 state transition estimations given by Eq. (15). The number of estimations used is a tradeoff between the speed of detection of any event or disturbance and the proper characterization of the degradation in the agent behavior, and may change depending on the dynamics of each implementation.TheSIindex is measured by comparing the implemented dynamicsGPgto the desired oneGP∗in Fig. 7. This measure emphasizes the fact that similar states should produce equivalent estimates of both covariates and responses, and so, theSIvalue should be less than 1 if no anomalies or changes in the agent policy exist. In Fig. 7a, the noise scale parameterσis increased from 0.1 to 0.5 and 1.0, to simulate different degrees of variability. It is quite clear that an increase of the parameterσmight deteriorate the agent behavior. In Fig. 7b, a temporary failure in the car actuator is simulated. In the set of samples from #100 to #150, the car dynamics is governed by a random policy with control actions sampled from the intervalu∈[1,+1]. This type of actuator malfunctioning which alters the driver behavior is notably captured by the proposed Bayesian surprise index.Agent based applications have definitely entered into the challenging fields of medical decision making, where faults or errors prompt serious consequences to patients. The importance of intelligent agents in this area lays on a number of properties such as autonomy in operation, communication and cooperation with other agents or systems and their perception capabilities. An agent in a medical systems interacts with the environment by executing different actions autonomously and communicating with other agents and humans, which may allow cooperative problem solving [30]. In doing so, agent based medical systems can support physicians and patients in different medical decisions and monitoring tasks. Technological devices for supporting safety–critical systems and safe patient management require of systematic methods aimed to continuously evaluate their performance and fast pinpointing any life-threaten situation. However, effective monitoring of health care situations is complex as involves interpretation of many variables and evaluation of many patient relative parameters, a great number of which cannot be measured directly.The potential of agent based technologies can be successfully applied to aid diagnosis, treatment and prediction of many clinical problems. In this sense, recent technology breakthroughs towards a fully automated artificial pancreas (AP) give rise to the need of improved monitoring tools aimed to increase both reliability and performance of closed-loop glycemic control. Briefly, the goal of diabetes management is to mimic the basal and postprandial patterns of insulin secretion produced by normal pancreatic function [31], i.e., maintaining blood glucose (BG) levels between 80 and 140mg/dl. Successful glucose regulation in the face of unknown changes in diet, exercise, stress, medications and most important of all diminishing the risks of hypoglycemia or hyperglycemia events is a challenging problem.Poor predictability of BG dynamics is a key issue that both patients and doctors must deal with, mainly because the glucose-insulin dynamics shows great variability among different patients according to the carbohydrate content of meals, exercise level, age and stress. Because of this variability even the same insulin dose with the same meal routine may give rise to different blood glucose responses to insulin boluses on consecutive days. Closing the glycemic loop with a fully automated AP will certainly improve the quality of life for insulin-dependent patients. Such a device is made up of (i) a glucose measuring device, (ii) an automated insulin infusion pump, and (iii) an intelligent control algorithm, which calculates the optimal insulin bolus to be delivered based upon glycemic data and facilitates the communication between the components and other systems. However, besides the dynamics of human physiology, errors in glucose sensors and failures in insulin infusion pumps give rise to a number of challenges for implementing an artificial pancreas. The safety–critical condition of such an automated device makes its performance monitoring task of paramount importance in order to reduce glucose level variability and to minimize the risks of dangerous excursions outside the euglycemic range.In this work, glycemic variability and other sources of uncertainty in a diabetic patient are simulated using a stochastic process superimposed on an otherwise deterministic model of the glucose–insulin dynamics. To this aim, the Lehmann and Deutsch model [32] parameterized as described in Acikgoz and Diwekar [33] is used as the basis to describe the deterministic dynamics in a diabetic patient. All sources of variability are accounted for by adding an Ito’s stochastic process to a deterministic model of the glucose dynamics. Introducing a stochastic process ensures a heterogeneous cohort of in silico subjects that accounts sufficiently well for the observed inter- and intra-subject variability, a key aspect in characterizing an optimal control policy under uncertainty. The glycemic variability in a diabetic patient is thus modeled as(18)dBGdt=Gin+NHGB-Gout-GrenVG+σεdtwhereBGis the blood glucose concentration,Ginis the systemic appearance of glucose via glucose absorption from the gut,NHGBis net hepatic glucose balance,Goutis the overall rate of peripheral and insulin dependent glucose utilization,Grenis the renal excretion of glucose andVGis the volume of distribution of glucose. The meal routine in Table 1described in terms of the carbohydrate intakes is used.Notice that the only available measurement of glucose concentration in blood is the one obtained by the subcutaneous sensor which is an estimation of the real plasmatic concentration -and corresponds to a hidden quantity to be inferred-. As the sensor needle is placed in the subcutaneous tissue, it measures interstitial fluid glucose concentration (IG) rather than the plasma glucose concentration. In the simulation model, each IG value can be estimated through integrating a BG–IG dynamics, whereδis the static gain (considered equal to 1) andτis the time-lag constant(19)dIG(k)=-1τIG(k)+δτBG(k)Furthermore, sensor readings are corrupted by a random time-varying calibration errorξ(k)and a white Gaussian noise processv(k)so that(20)S(k)=(1+ξ(k))IG(k)+v(k)ξ(k+1)=3ξ(k)-3ξ(k-1)+ξ(k-2)+w(k)whereS(k)is the glycemic sensor output and the calibration errorξ(k)has been created using a triple integrator of a zero mean white noisew(k). For simplicity, BG is used instead of IG hereafter but recall it refers to the outcome of a continuous glucose sensor.The specification of the expected behavior when monitoring an artificial pancreas is here built upon the state transition distribution for an optimally controlled glucose dynamics. To obtain the optimal control policy using the LSMDP technique, it is required in advance a model of the passive (uncontrolled) dynamics of the glucose regulatory system. Even if different methods exist to model the passive dynamics according the each problem, most of them require of previous knowledge of the transition matrix given in Eq. (13) and immediate costs over the entire state-space [34,35]. However, since the passive dynamics describes nothing but the space of likely state transitions, it would be a valid to use an approximation of the matrix. This approximation can be obtained by means of a reduced model of the glycemic system. In this sense, the Lehmann and Deutsch model above described corresponds to an augmented representation of the two-compartments model described in Bergman et al. [36](21)dBGdt=(p1-Ia)BG+p1GbdIadt=p2Ia+p3I(t)whereIarepresents the time course of insulin infusion andGbis the basal glucose level. In this minimal model of glucose kinetics, plasma insulinI(t)enters a remote compartment where it acts by accelerating the glucose disappearance into the periphery and liver, and inhibiting hepatic glucose production. While some of the parameterspigiven in Table 2, describe important physiological responses, as the insulin sensitivitySI=-(p3/p2), others lack a meaningful interpretation. Beyond any limitation of this minimal model, it is still useful to describe the glucose–insulin state transitions in the passive dynamics.The control task corresponds to maintaining acceptable glycemic variability. For this purpose, the cost function is conveniently designed in such a way that allows guaranteeing an acceptable behavior of blood glucose dynamics within a target band (80–140mg/dl). Thereby, the state cost functionq(x)is represented by a square exponential form which saturates for great deviations from a chosen reference blood glucose value (BG=110mg/dl) and basal insulin (Ib=30mU/l). In this way, the control system will aim to keep the patient glucose level as close as possible to the reference. Note that the glycemic control problem has a two-dimensional statex=[BG,Ia]with only one control input corresponding to the insulin bolus, i.e.u=I(t). Consequently, Eq. (13) can be now solved by an iteration method in exponential form. The approximation uses a state space discretization with a 151-by-151 grid spanningBG∊[0,220] mg/dl andIa∊[0,60] mU/l. The passive dynamics is constructed by a discretization of the time axis (with time step h=0.05) and defining probabilistic transitions among discrete states so that the mean and variance of the continuous state dynamic are preserved. The noise distribution is discretized at 9 points spanning ±3 standard deviations and using a noise scale parameter σ=0.1. Thus, the real system dynamics has to be inferred by the controller. The desirability function was computed using the estimated passive dynamics matrix whereas the control policy was subsequently derived from the obtained expression. Results are displayed in Fig. 8, where (a) depicts a scheme of the Bergman’s minimal model and (b) is the state cost functionq(x)-in all plots blue corresponds to small values and red to high values-; (c) is the optimal cost-to-go function obtained -the two small paths correspond to stochastic trajectories generated by the optimal policy-. More specifically, the red one was obtained using a noise scaleσ=0.1 whereas the black withσ=0.25. Higher insulin levels are desirable (lower costs) when BG is high, whereas smaller insulin levels are needed when BG is lower. In (d) the obtained optimal control policy for agent behavior monitoring is shown.The controlled dynamics of the implemented AP is represented using a suboptimal insulin policy, assuming a certain degree of glycemic variabilityσand multiple meal consumption as in Table 1. The desired dynamics -the one controlled by the optimal insulin policy- is parameterized with: variability parameterσ=0.10; calibration error ξ=2% and time-lag τ=5min. In this way, a typical level of uncertainty in the system is considered. A set ofNmax=50 training examples are selected to set up the sequenceDof training examples, while W=30 observed state transitions are used to compute the surprise withTKL. At the 12th hour, sensor parameters are varied to simulate performance degradation in the AP components, whereasσis increased to augment patient glycemic variability. To sum up, this allows us to evaluate the clinical impact of real-life glycemic control that is affected by sensor errors as well as time-lags compounded with the effect of patient variability in diabetes management. In Fig. 9, a realization (obtained by applying the optimal control policy for a scale of variability σ=0.10) of the glucose stochastic process is shown. From the 12th hour onwards, glycemic variability is increased by changing the corresponding parameter to σ=0.50. The predicted state transitions distributions are presented in the lower part of Fig. 9; shaded areas describe the uncertainty in predictions whereas solid lines correspond to prediction means. An advantage of using GPs is that they also provide information about confidence intervals for each prediction. It is worth noting that increasing variability not only affects the predicted means but also the prediction errors, which reveals a significant degradation in the behavior of the AP controller.In Fig. 10, the ability of the optimal control policy to mitigate excessive glycemic variability is evaluated. The scale of the noise parameter of the glucose-insulin model is increased fromσ=0.25 toσ=0.50 from the 12h onwards, giving rise to a larger glycemic variability. It is noticeable how the performance of the closed-loop quickly degrades. The computedTKLmetric is irregular at the beginning while the training setDis still capturing enough information to properly describe the current system behavior. Moreover, stochastic behavior in the glycemic model gives rise toTKLvalues that are not strictly equal to 0, even if the observed and expected behaviors include the same degree of variability over the interval [0,12] hours. It is worth noting that performance degradation is quickly revealed when parameters are varied from the 12h onwards in the simulation model of a diabetic patient.Any failure resulting in an sudden change in glycemic levels should be followed by a similar increment in the surprise index. Glucose sensor readings affected by miscalibration due ξ=10% are depicted in Fig. 11for optimal control. Since the control algorithm responds to glycemic states acquired by sensory devices susceptible to noise and disturbances, the loop performance degrades if a failure in the device occurs. Similarly, a stuck fault in the sensor starting from the 12h and onwards it is also considered. Because of this, the sensor outcome freezes at the 120mg/dl reading giving rise to a significant deviation from the expected performance, even when BG levels is maintained in the desired euglycemia range. However, since the performance of the AP implementation is contrasted with its specified behavior this anomaly is fast detected using theTKLmetric.To end the analysis for this case study, Fig. 12simulates a likely scenario in which a catheter blockage occurs during the use of a continuous infusion pump. As a consequence, the insulin dose is considerably reduced such that only 80% of the level prescribed by the optimal control algorithm is in fact administrated. This mitigates the effect of the insulin bolus and leads to a poorly controlled glycemic variability. In a different, a chaotic controller is simulated by consideringu(t)=u∗+βdw, whereu∗corresponds to the value given by the optimal policy,dwis the differential of a Brownian random noise and β=3 is the standard deviation of the added noise. As a result, even if the control algorithm can still compute the optimal action, it is unable to properly adjust the amount of insulin administrated to the patient. Since a zero mean noise is used, the path still converges to the vicinity of the target levelBG=110mg/dl, despite the significant increase in the variability of the glucose dynamics.This work presents a novel probabilistic approach built upon an optimally controlled stochastic system for on-line monitoring of an agent behavior under uncertainty, which has been conceived in terms of active inference and optimal action selection. Checking if an autonomous agent behavior fulfills expectations is a key issue to guarantee safety and performance of an increasing number of autonomous agent applications such as driverless cars, drones and biomedical systems. The main problem for behavior monitoring is generating prior beliefs under the uncertainty the agent should face in its own environment. In this work, the desired behavior is modeled by a prior Gaussian distribution for state transitions, in order to verify if a given agent control policy respects its specification. The desired optimal behavior is obtained analytically using a class of Markov decision processes which are linearly solvable. Through an exponential transformation, the Bellman equation for such problems can be made linear, despite nonlinearity in the stochastic dynamical models, which facilitates applying efficient numerical methods.The availability of an optimal control policy allows simulating the desired behavior over time and comparing it with the current system performance in order to identify deviations from the desired behavior. To favor on-line monitoring, a robust metric based on surprise and twin Gaussian processes is introduced to characterize the progressive degradation in the agent behavior by quantifying the distance between the implementation and prior beliefs. A distinctive advantage of computing surprise using Gaussian processes is that the divergence from prior beliefs can be estimated not only using the expected value of state transitions but also the corresponding prediction uncertainty for optimal action selection.The proposed active inference approach to on-line monitoring of an agent behavior allows its incorporation to a number of applications in diverse domains. Typical examples include the detection of unauthorized access to computer systems [37], irregularities in vital signs and other variables in intensive care patients [38], fraud in financial services [39], detection of saccadic objects for visual applications [40] and detection of path deviation in autonomous vehicles [41].Future work, aims to extend the approach to multi-agent system monitoring by characterizing the desired collective behavior using a game-theoretic perspective. In turn, despite the efficiency of the LSMDP scheme, it is yet necessary to previously know the passive dynamics of the system, which probably is the most crucial and complex task in applying the proposed methodology. Instead of using a simplistic model of the system under study, current research work aims to include the estimation of the passive dynamics while finding the optimal cost-to-go, by using a temporal difference algorithm called z-learning.

@&#CONCLUSIONS@&#
