@&#MAIN-TITLE@&#
A model for clustering data from heterogeneous dissimilarities

@&#HIGHLIGHTS@&#
We present a new model for clustering data from heterogeneous dissimilarity matrices.The model is robust and insensitive to various types of clustering data.We propose a VNS that outperforms general purpose exact solvers in all tested cases.

@&#KEYPHRASES@&#
Data mining,Clustering,Heterogeneity,Optimization,Heuristics,

@&#ABSTRACT@&#
Clustering algorithms partition a set of n objects into p groups (called clusters), such that objects assigned to the same groups are homogeneous according to some criteria. To derive these clusters, the data input required is often a single n × n dissimilarity matrix. Yet for many applications, more than one instance of the dissimilarity matrix is available and so to conform to model requirements, it is common practice to aggregate (e.g., sum up, average) the matrices. This aggregation practice results in clustering solutions that mask the true nature of the original data. In this paper we introduce a clustering model which, to handle the heterogeneity, uses all available dissimilarity matrices and identifies for groups of individuals clustering objects in a similar way. The model is a nonconvex problem and difficult to solve exactly, and we thus introduce a Variable Neighborhood Search heuristic to provide solutions efficiently. Computational experiments and an empirical application to perception of chocolate candy show that the heuristic algorithm is efficient and that the proposed model is suited for recovering heterogeneous data. Implications for clustering researchers are discussed.

@&#INTRODUCTION@&#
Clustering algorithms determine groups of objects in such a way that objects in the same group, called clusters, are more similar to one another than to those in other groups (Hansen & Jaumard, 1997). Clustering is ubiquitous, with applications in the natural sciences, psychology, medicine, engineering, economics, marketing and other fields (e.g. Frey & Dueck, 2007; Jain, Murty, & Flynn, 1999; McLachlan & David, 2004).Among the many types of clustering models, a popular one is partitioning a setO={o1,…,on}of n objects into a set ofP={C1,…,Cp}clusters such that:(i)Cj≠∅,∀j∈{1,…,p};Ci∩Cj=∅,∀i,j∈{1,…,p}with i ≠ j; and∪j=1pCj=O.The input data for clustering algorithms is often a single matrix X of dimensions n × s, obtained by measuring s features of the objects of O. This matrix is then used to compute a n × n matrix of pairwise dissimilaritiesD=(dij)between objects of O, such that dijfori,j∈{1,…,n}(usually) satisfy: (i)dij=dji≥0,and (ii)dii=0. Such single D dissimilarity matrix does not need to satisfy triangle inequalities, i.e., to be distances.For many problems, only one dissimilarity matrix is available. For instance the Iris dataset (Fisher, 1936), one of the most popular datasets used in cluster analysis, consists of 150 samples from each of three species of Iris flowers where each flower is measured on four characteristics. Using the attributes measured, the flowers are typically cluster in the three (expected) species. The use of classical clustering algorithms (e.g. k-means, single-linkage, complete-linkage) on this dataset can provide excellent results. It is however possible that more than one dissimilarity matrix is available. In the context of the Iris data set, one could envision asking a sample of multiple experts to measure the same flowers, in case there has been significant measurement error. If there is heterogeneity in the data reported, we argue that aggregating the dissimilarity matrices might mask differences truly present in the data.There are indeed many contexts for which multiple measurements (i.e., dissimilarity matrices) are available. For instance in the social sciences, it is common to ask a sample of individuals to each provide pairwise similarity judgements between brands (e.g., how similar is Coke to Pepsi?). Such tasks, known as pairwise similarity tasks, produce one dissimilarity matrix for each participant, and have been used to study preference formation (Carpenter & Nakamoto, 1994), advertisement similarity (Schweidel, Bradlow, & Williams, 2006), comparing brands (Bijmolt, Wedel, Pieters, & DeSarbo, 1998), store positioning (Arora, 1982), variety seeking (Feinberg, Kahn, & McAlister, 1992), and substitution decisions (Hamilton et al., 2014; Ratneshwar & Shocker, 1991). The necessity to consider the multiple dissimilarity matrices stems from the fact that measurements often reflect differences in perception. Such different dissimilarity matrices can be thought of as reflecting different points of view (Brusco & Cradit, 2005; DeSarbo & Carroll, 1985; DeSarbo, Atalay, LeBaron, & Blanchard, 2008; Lee, 2001; Steinley, Hendrickson, & Brusco, 2015; Vichi, Rocci, & Kiers, 2007) which have been incorporated in a large number of perceptual models that include multidimensional scaling, three-way clustering, and mixture models.Among the various clustering models available, the p-median model has received significant attention across fields (e.g., Sáez-Aguado and Trandafir, 2012; Brusco, Steinley, Cradit, and Singh, 2012; Avella, Boccia, Salerno, and Vasilyev, 2012). The p-median model aims to partition objects into clusters such that the sum of the distances from each object to the central exemplar of its cluster (i.e., median) is minimal. Given n objects to be clustered and a known number of clusters p, the mathematical problem can be formulated as an integer linear program (ReVelle & Swain, 1970). In our notation, there is one key set of decision variables that involves the assignment of objects to clusters. First,ejj=1,which indicates if object j is chosen as the median of a cluster, and 0 otherwise, forj∈{1,…,n}. Second, on the off-diagonal elements,eij=1if object i is assigned to the cluster whose object j is the median, and 0 otherwise, fori∈{1,…,n}(object j is naturally assigned to itself if it is a median). Using this notation, the p-median model is expressed as follows:(1))min∑i=1n∑j=1ndijeij(2)subjectto∑j=1neij=1,∀i∈{1,…,n}(3)∑j=1nejj=p(4)eij≤ejj∀i,j∈{1,…,n}(5)eij∈{0,1}∀i,j∈{1,…,n}.The constraints (2) require that each object must be assigned to one and only one median. Constraint (3) imposes that the number of medians must be exactly p. The constraints (4) ensure that object i can only be assigned to object j if object j is a median and constraints (5) are domain constraints for the decision variables. Finally, the product of dijand eijin (1) captures the dissimilarity from each object i to its closest median j.One of the main characteristics of the p-median is its breadth of applicability. It can be applied to cluster metric data as well as to more general similarity/dissimilarity data, even asymmetric or rectangular data structures (i.e., when not every object can be a median) (Köhn, Steinley, & Brusco, 2010). Mladenović, Brimberg, Hansen, and Moreno-Prez (2007) present an extensive review of exact and heuristic solution methods for this problem. Despite its advantages, including excellent classification rates, robustness to outliers and attractive assumptions, an aggregate p-median formulation may still mask individual heterogeneity as is later shown in the empirical application.In this paper, we propose a mathematical programming formulation based on the p-median to cluster data collected from individuals11We use the term individuals for clarity with respect to our empirical application which refers to different consumers. We note such dissimilarity matrices could come from other sources (e.g., firms, repeated measurement for the same person, etc.), in line with the research on ”points of view” (Brusco & Cradit, 2005).who provided heterogeneous dissimilarity matrices. The model is conceived in two levels. The first identifies clusters of individuals, herein called groups for readability, with similar clustering structures. The second identifies the partitions of objects, for each of these groups.The remainder of the paper is as follows. In the next section, we present the mathematical formulation and discuss how available exact algorithms can be used to solve our model. In Section 3, we describe the Variable Neighborhood Search (VNS) (Hansen & Mladenović, 2001; Mladenović & Hansen, 1997) heuristic for the model. In Section 4, we present a Monte Carlo Simulation whereby our results illustrate the necessity for heuristic algorithms. We also show that the proposed VNS heuristic has the ability to predict heterogeneous clustering data. Section 5 provides an empirical example from a local United States retailer about perceptions of chocolate candies. This last section illustrates how the proposed methodology can be used by managers, and help discover insights based on heterogeneous perceptions.Let m individuals evaluate n objects such that a matrix dataDk=(dijk)is obtained fork∈{1,…,m},representing the dissimilarities between pairs of objects i and j as perceived by individual k, and ck, fork∈{1,…,m},the number of clusters expected by individual k. The clustering problem considered in this work involves identifying groups of individuals whose dissimilarity matrices suggest a similar clustering solution. Clusters, for each group of individuals, are organized by means of a medians-based model where each clustered object is associated to the most representative item (i.e., the median) of its cluster. The Heterogeneous Clustering Problem (HCP) can be formulated as follows:(6)min∑k=1m∑g=1Gzkg[∑i=1n∑j=1ndijkeijg](7)subjectto∑j=1neijg=1∀g∈{1,…,G},∀i∈{1,…,n}(8)eijg≤ejjg∀g∈{1,…,G},∀i,j∈{1,…,n}(9)∑g=1Gzkg=1∀k∈{1,…,m}(10)∑k=1mzkg≥1∀g∈{1,…,G}(11)∑j=1nejjg=⌊∑k=1mckzkg∑k=1mzkg⌋∀g∈{1,…,G}(12)eijg∈{0,1}∀g∈{1,…,G},∀i,j∈{1,…,n}(13)zkg∈{0,1}∀g∈{1,…,G},∀k∈{1,…,m}The m individuals are partitioned into G groups. The decision variables zkgexpress the assignment of individual k to group g. Variableseijgare equal to 1 if object i is assigned to object j in group g, andeijg=0otherwise. The objective is to minimize (6), i.e., the sum of dissimilarities between each object and its assigned median, conditional on (individual) group membership. Constraints (7) require that each object i be assigned to exactly one median, as part of each group g’s clustering solution. Constraints (8) ensure that object i can only be assigned to object j for group g if object j is a median for that group. Constraints (9) require that each individual is assigned to exactly one group, whereas constraints (10) guarantee that no empty group exist. Finally, constraints (11) impose the total number of medians for each group g equal to the floor of the average number of medians expected by the individuals in that group. As argued and shown by Blanchard, Aloise, and DeSarbo (2012a); Blanchard, DeSarbo, Atalay, and Harmancioglu (2012b), this suggestion of limiting the number of clusters to represent consumer perceptions follows numerous researchers in the behavioral literature who have shown that individuals tend to favor simple representations when forming object perceptions and preferences (Bettman, Luce, & Payne, 1998; Bettman & Park, 1980; Shugan, 1980; Simon, 1955). In our empirical application, the data reflects the fact that each individual had formed his own partitions. Doing so provided us actual data to justify the use of different number of medians for each group of individuals.The model in (6)–(13) may be further simplified. For instance, the optimization process guarantee that∑j=1nejjgis an integer value as big as possible given that more medians in a group imply lower (or equal) objective function values. Consequently, constraints (11) can be replaced by the following inequalities:(14)∑j=1nejjg≤∑k=1mckzkg∑k=1mzkg,∀g∈{1,…,G},without affecting the optimal solution. Moreover, these constraints can be modified if the user prefers that the number of medians in each group equals the closest integer to∑k=1mckzkg∑k=1mzkginstead of the floor. For that, it would suffice to add 0.5 to the right-hand side of constraints (14).The HCP is a Mixed-Integer Quadratically Constrained Quadratic Problem (MIQCQP) for which literature concerning exact and heuristic methods is vast (e.g. Anstreicher, 2012; Audet, Hansen, Jaumard, & Savard, 2000; Billionnet, Elloumi, & Lambert, 2016; Bomze & Locatelli, 2004; Galli & Letchford, 2014; Saxena, Bonami, & Lee, 2010; Zheng, Sun, & Li, 2011). Particularly for the HCP, all variables are required to be binary such that the problem is a 0–1 QCQP. In the following subsections, we present the methods employed here to solve the HCP exactly.In our attempts to solve the HCP exactly, we used Couenne (Belotti, Lee, Liberti, Margot, & Wächter, 2009), Baron (Tawarmalani & Sahinidis, 2005), and GloMIQO (Misener & Floudas, 2013). The first two are different implementations of the spatial Branch-and-Bound (sBB) algorithm (Liberti, 2006) for nonconvex mixed-integer nonlinear problems (MINLP). Much like a Branch-and-Bound (BB) algorithm for MIPs, sBB explores the feasible space exhaustively but implicitly, finding a guaranteed ε-approximate solutions for any given ε > 0 in finite (potentially exponential) time. Unlike MIPs, whose continuous relaxation is a linear program, and unlike convex MINLPs, whose continuous relaxation is a convex NLP, the continuous relaxation of a nonconvex MINLP is usually difficult to solve. To address this issue, sBB algorithms form and solve convex relaxations of the given MINLP. The convexity gap between the original MINLP and its convex relaxation therefore stems from two factors: the relaxation of the integrality constraints, as well as the relaxation of the nonconvex terms appearing in the MINLP. The third generic solver GloMIQO is a branch-and-cut algorithm based on generating tight convex relaxations from detecting special structures such as convexity and edge-concavity. The algorithm is specialized to address MIQCQPs to ε-optimality.Linearization can be achieved by means of Fortet’s inequalities (Fortet, 1960), thereby replacing the product of binary variableseijg×zkgbywijkg(wijkg∈[0,1]) fork∈{1,…,m};i,j∈{1,…,n};g∈{1,…,G},along with three additional constraints which together ensure thatmax{0,eijg+zkg−1}≤wijkg. The three sets of constraints are:(15)wijkg≤eijg∀g∈{1,…,G},∀k∈{1,…,m},∀i,j∈{1,…,n},(16)wijkg≤zkg∀g∈{1,…,G},∀k∈{1,…,m},∀i,j∈{1,…,n},(17)wijkg≥eijg+zkg−1∀g∈{1,…,G},∀k∈{1,…,m},∀i,j∈{1,…,n}.To further accelerate the optimization process of the resulting mixed-integer problem (MIP), we strengthen the formulation by adding constraints (cuts) that do not affect the optimal integer solution. In the spirit of the Reformulation- Technique (RLT) (Sherali & Adams, 1990), we obtain a set of additional cuts by multiplying the n × G constraints in (7) by zkgand(1−zkg), fork=1,…,m,then replacing the productseijg×zkgbywijkg. This yields the following constraints:(18)∑j=1nwijkg=zkg,∀g∈{1,…,G},∀k∈{1,…,m},∀i∈{1,…,n},(19)∑j=1n(eijg−wijkg)=1−zkg∀g∈{1,…,G},∀k∈{1,…,m},∀i∈{1,…,n}.Notice that constraints (18) make constraints (16) redundant. Without loss of generality, consider particular indices i, k and g associated to one of the constraints (18). In order to hold the equality, since w is non-negative, each one of the termswijkgmust be smaller than zkg, which implies that constraints (16) are redundant. Moreover, constraints (17) are redundant due to constraints (19). For allg∈{1,…,G},∀k∈{1,…,m},∀i,j∈{1,…,n},we can rewrite (17) aseijg−wijkg≤1−zkg. Now, let us take, without loss of generality, particular indices i, k and g associated to one of the constraints (19). It follows that the equality holds if and only ifeijg−wijkg≤1−zkg,since constraints (15) guarantee thateijg−wijkg≥0. Consequently, constraints (17) are also redundant to the model.The resulting MIP model is denoted HCP-R1, and was solved in our computational experiments by the generic MIP solver CPLEX version 12.6.As a 0–1 QCQP, the HCP can also be written in the following form:(20)minxTQ0x+c0xsubject to(21)xTQhx+chx≤bh∀h∈{1,…,t}(22)x∈{0,1}swhere the Qhare symmetric matrices of order s, the chare s-vectors and bhare scalars. To illustrate, consider an illustrative objective function that is as follows3z11e121+3z11e211+3z12e121+3z12e211+4z21e122+4z21e212+4z22e122+4z22e212withm=2,n=2andG=2. We can express this function in quadratic form with:x=[e121e211e122e212z11z12z21z22]andQ0=[00001.51.50000001.51.50000000022000000221.51.50000001.51.50000000022000000220000]We performed the convexification of (6) and (14) with two different methods:(i)the method proposed by Hammer and Rubin (1970), which makes matrix Q semi-definite positive by subtracting its minimum eigenvalue from the diagonal entries and by adjusting the linear term of the expression, andthe method that convexifies each product C · x · y with C > 0, by replacing it by the difference of two convex functions:12(x+y)2−12(x+y).Both reformulations preserve the cost of every feasible binary solution. In our example, the minimum eigenvalue of Q0 is−4. Thus, the objective function is replaced by (i) with the convex functionf′(x)=f(x)+4((e121)2+(e211)2+(e122)2+(e212)2+(z11)2+(z12)2+(z21)2+(z22)2)−4(e121+e211+e122+e212+z11+z12+z21+z22). In reformulation (ii), the products of the z and e variables are replaced by the difference of convex functions. For instance, the product3z11e121in the objective function of our example above is replaced by32(z11+e121)2−32(z11+e121).The resulting convex 0–1 QCQP formulation of the HCP with (i) was denoted HCP-R2 whereas that with (ii) was denoted HCP-R3. They are solved by CPLEX which automatically converts a 0–1 convex QCQP formulation into a 0–1 second order cone program for which relaxations are solved via the barrier algorithm.VNS is a metaheuristic developed to solve combinatorial and global optimization problems by changing neighborhoods in its local descent step for intensification as well as in its shaking step for diversification (see Hansen, Mladenović, and Pérez, 2010, for a survey).VNS relies on the following three observations:Observation 1:A local minimum with respect to one neighborhood structure is not necessary so for another;Observation 2:A global minimum is a local minimum with respect to all possible neighborhood structures;Observation 3:Local minima with respect to one or several neighborhoods are often relatively close to one another.In the VNS framework, the neighborhoods are defined around types of moves, or perturbations, of the best current solution x – the center of the search. When looking for a better one in a minimization problem, a solution x′ is drawn at random in an increasingly wider neighborhood, and a local descent is performed from x′ leading to another local optimum x′′. If x′′ is worse than x, then x′′ is ignored and one chooses a new neighbor solution x′ in a more distant neighborhood of x. If instead x′′ is better than x, the search is re-centered around x′′ and the local search restarts in the closest neighborhood of the newly found best current solution. Once all neighborhoods of x have been explored without success, one begins again with the closest one to x, until a stopping condition (e.g. maximum CPU time) is met.As the size of neighborhoods tends to increase with their distance from the current best solution x, close-by neighborhoods are explored more thoroughly than far away ones. This strategy takes advantage of the three observations 1–3 mentioned above, and yet can ensure with sufficient computational time that the algorithm is not stuck in a poor local optimum. We now turn to our implementation of VNS for the HCP.VNS requires an initial solution which can be either provided or constructed by the user. Algorithm 1 presents the pseudocode of our approach to construct an initial solution.Algorithm 1 first solves the problem of assigning individuals to groups, and does so by using a distance matrix of m × m individuals based on the Frobenius norm of each individual’s distance matrix between objects. Once the p-median is applied to this distance matrix between individuals, they are assigned to groups according to the partition obtained, i.e., if a pair of individuals have their distance matrices assigned to the same cluster in the p-median model then these individuals are assigned to the same group in the initial solution. Then, for each initial group, solving subproblems Mg(z), forg=1,…,Gprovides a complete initial solution for HCP:(23)Mg(z)=min∑i=1n∑j=1nd¯ijgeijgsubject to(24)∑j=1neijg=1∀j∈{1,…,n}(25)eijg≤ejjg∀i,j∈{1,…,n}(26)∑j=1nejjg=⌊Ωg⌋(27)eijg∈{0,1}∀i,j∈{1,…,n},whered¯ijg=∑k=1mdijkzkg,andΩg=∑k=1mckzkg∑k=1mzkg. We note that problem (23)–(27) corresponds to the p-median problem (1–5). Furthermore, this constructive heuristic could be easily replaced by others, such that other distance norms (e.g., L1, L∞) could be substituted just as the assignment of objects to clusters could be done via any other partitioning heuristic. For our constructive heuristic, we used the approach by Hansen and Mladenović (1997) as it ensures that each group contains at least one individual.The shaking component of our VNS is implemented by means of random moves in the swap neighborhood which encompasses all the possible ways of removing an individual from a group and adding it to a different one. Thus, if the parametert=2for shaking, then two random swap moves are performed for two individuals; ift=3,then three swap moves are performed for three individuals, and so on.Given an existing solution, we need to search the neighborhood to reach a local optima. We developed our local search following the Variable Neighborhood Descent (VND) framework, which generalizes the observations 1–3 to descent methods. Algorithm 2 presents a general VND’s algorithmic steps.Applied to HCP, VND involves the iterative optimization of the objective function via improvements based on three descent methods: (1) descent on the clustering of objects (conditional on group memberships and number of medians), (2) descent on the group memberships (conditional on objects clusterings and number of medians), and (3) descent by (perhaps) augmenting the number of medians. VND (the local search) ends when all descent methods have been consecutively explored without any improvement in the objective function. Whenever an improvement occurs, the algorithm resets s to smin. In the present section, we present each of our descent procedures.The descent methoddescent1for Algorithm 2 solves subproblem (23)–(27) for each group affected by the shaking procedure. Namely, for each group the descent method identifies the conditionally optimal clustering of objects assuming that both the number of medians and the group memberships are known. Our choice has been to perform this descent by heuristics (e.g., Hansen & Mladenović, 1997; Resende & Werneck, 2004; Hansen, Brimberg, Urošević, & Mladenović, 2009) to accelerate the whole algorithm.The second descent,descent2,temporarily assumes the clustering of objects to be known in all groups (i.e. variables e). Then, it descents by conditionally reassigning individuals to the groups that provide the best values for z. To do so, the following binary program is solved:(28)W(e)=min∑k=1m∑g=1Gzkgd˜kgsubject to(29)∑k=1mckzkg∑k=1mzkg≥ωg∀g∈{1,…,G}(30)∑g=1Gzkg=1∀k∈{1,…,m}(31)zkg∈{0,1}∀g∈{1,…,G},∀k∈{1,…,n}whered˜kg=∑i=1n∑j=1ndijkeijg,andωg=∑j=1nejjg. Problem (28)–(31) is a binary program which is usually solved at the root node of the branch-and-cut algorithm implemented by CPLEX according to our limited computational experiments. We chose to halt the second descent after root node solution. If the W(e) is not solved to optimality, the previous solution is kept.It is trivial that the objective function of a clustering algorithm is improved when one allows the clustering solution to have a greater number of medians. However in our case, the HCP restrains the number of medians in each group g, forg∈{1,…,G},by means of constraints (14), and thus pushes that number to the largest integer smaller or equal to∑k=1mckzkg∑k=1mzkg. Specifically for the HCP, increasing the number of medians for a group-level clustering solution will not necessarily always improve the objective function. The third descent thus aims at seeing if the number of medians in a group g* can be augmented by reallocating objects to a new median, thereby satisfying constraints (14) to the new number of medians.Algorithm 3 details how this procedure works. Specifically, for each groupg*(g*∈{1,…,G})we initialize a solution (zbest, ebest) with the values of the best current solution for the HCP, i.e., (zbest, ebest) ← (z, e). Then, the problemMg*(z)is solved withΩg*replaced byΩg*+1,thereby producing a new partition for the objects in g* for which the number of medians is increased by one unit. We then solve the problem W(e) and verify if this replacement ofΩg*byΩg*+1,can be accommodated by reassigning the individuals among the groups. If W(e) is infeasible or if the cost of the new yielded solution is larger than the best solution found, the solution of W(e) is ignored. Otherwise, the solution of W(e) becomes the new best incumbent solution.A critical element in VND heuristics is the order in which the descent methods should be explored (e.g., in Algorithm 2). In our implementation, this decision was based on several observations. First,descent_3is more computationally expensive than the other two and as such, it was set to be the last. Second, because our shaking step involves perturbing only the group membership variables, applying the shaking step directly afterdescent_2(about group memberships) would revert the shaking step22To illustrate, suppose a local minimum (z′, e′) with respect to the first and second descent. The shaking step applied to (z′, e′) generates a new solution (z′′, e′), perturbing only the group membership variables. Thus, if the second descent is applied just after the shaking step it will change back, yielding the same solution (z′, e′).. Thus, we useddescent_1as the first descent, followed bydescent_2anddescent_3.In the previous section, we have introduced a new model, a series of reformulations, and a heuristic to solve for the HCP. In the present section, we tackle the relative performance of the approaches. Specifically, we first generate a set of datasets for which the true solution is known. Second, we attempt to solve the HCP for each dataset via exact methods and our proposed heuristic, showing that the use of a heuristic is necessary because exact methods cannot be used for problems of moderate size. Third, we compare the performance of the proposed heuristic to that of a benchmark heuristic based on a related model and show that the performance of our heuristic is significantly better. Fourth, we investigate the circumstances under which the proposed heuristic for HCP is likely to outperform competing alternatives.To do so without providing any algorithm an unfair advantage, we needed a set of problems with known data generating mechanisms for Dk, and ck. As such, we simulated data following the fractional factorial experimental design used by Blanchard et al. (2012a). The process involves generating 27 simulated datasets (i.e., experimental trials) that have known solutions and that can be used to study the impact of different dataset characteristics on the ability of the competing algorithms to perform. The factorial design appears in Table 1.The characteristics of the datasets (experimental factors) included the total number of individuals (m = 150, 300, 450), the number of groups (G = 2, 6, 10), the number of objects (n = 18, 30), the variance in the number of medians across groups, the amount of error added to the dissimilarity matrix of each individual (using N(0, 0.05) or N(0, 0.1) before rounding), and the amount of error added to the number of medians sought by each individual (using N(0, 0.5) or N(0, 1) before rounding). Further, following the works of Blanchard et al. (2012a), Blanchard and DeSarbo (2013), Brusco and Cradit (2001) and others, we also assume that the true number of groups G is known - but not their composition.33The instances can be found at http://www.gerad.ca/~aloise/publications.html.In the present section, we wish to establish the necessity of introducing a heuristic for HCP via comparing its results to the following exact solvers:(a)Couenne version 0.5.3, Baron version 15.2.0 and GloMIQO version 2 on formulation (6)-(14),CPLEX version 12.6 on HCP-R1, HCP-R2 and HCP-R3, andthe VNS heuristic presented in the last section.To compare the performance of the generic MINLP solvers, CPLEX, and the VNS heuristic, we use each to solve the 27 simulated instances presented in Table 1. Computational experiments were performed on a Xeon(R) CPU X5650 2.67 gigahertz and 64 gigabytes of RAM memory. Couenne, Baron, GloMIQO and CPLEX were allowed to run for 24 hours with default parameters. The VNS heuristic was allowed to run for 600 seconds. The algorithm was implemented in C++ and compiled by gcc 4.4.As most of the instances could not be solved to optimality, Table 2reports the best upper bounds (ub), lower bounds (lb) and number of explored branch-and-bound nodes (#bbn) as obtained by each solver. The upper bounds correspond to feasible solutions obtained by different heuristics used within each solver. They are important in branch-and-bound algorithms to eliminate branches of the enumeration tree that do not lead to the optimal solution. Better upper bound values are usually able to cut more of these branches, which improves the overall performance of branch-and-bound methods. An empty value in column (ub) indicates that no feasible solution was reported by the solver before the time limit was attained for that instance.The results in Table 2 reveal that instance#26is the only one that could be solved to optimality by our exact approaches. It has the easiest problem characteristics of all our datasets, withm=150,n=18,G=2and without any kind of perturbation (error) added. Yet, solver GloMIQO spent 12510 seconds to solve the problem whereas CPLEX on HCP-R1 took 374 seconds. For instance#27,none of the solvers were able to find a feasible upper bound solution within 24 hours of CPU time.Across all the datasets, the lower bounds obtained by the solvers are very often equal to the trivial one (zero). The only exception is those obtained by CPLEX for the RLT-linearized formulation (HCP-R1), which is still quite difficult as revealed by the number of branch-and-bound nodes solved by CPLEX within 24 hours. In 18 of the 27 instances ( ≈ 67 percent), CPLEX was able to solve only the root node.It does indeed seem that the reformulations presented in Sections 2.2 and 2.3 helped CPLEX to tackle the problem as it performed better than Couenne, Baron and GloMIQO applied to the original HCP formulation. The lower bounds obtained by the latter are never better than those obtained by CPLEX on HCP-R1. Regarding upper bounds, Baron and Couenne together found only 4 times the best solutions (in instances#2,#3, #19 and #21).Among the convexified formulations, HCP-R2 and HCP-R3 do not allow CPLEX to obtain better lower bounds than those obtained by CPLEX on HCP-R1 despite exploring more branch-and-bound nodes. However, we note that the upper bounds obtained on these two formulations (HCP-R2 and HCP-R3) were better than those obtained for HCP-R1 in 18 out of 27 instances, i.e., ≈ 67percent.Table 3presents the results obtained by Algorithm 1 (CH) and the VNS heuristic. The second column reports the cost of the initial solution provided by the constructive heuristic. The third column presents the average upper bound solutions obtained in 10 distinct executions of the heuristic, whereas the fourth column shows their associated standard deviation. The fifth column refers to the relative difference between the VNS solutions and the best upper bound values presented in Table 2. Finally, the sixth column refers to the relative difference between the solutions obtained by VNS and the best lower bound values of Table 2.Comparing to the results obtained for the exact solvers, we find that VNS always obtained better upper bound solutions. The only exceptions are for instances#2,#3and#26,when all algorithms obtain the same objective function value. The superior performance of VNS attains its maximum for instance#13with a difference of 43.58 percent in solution quality.Our results suggest that the proposed VNS algorithm is stable, as demonstrated by the very small standard deviations, which were inflated by a few larger instances. For instance, the largest variability in objective function values for the 10 distinct VNS executions is found for instance#5,whose data contains perturbations not only in dissimilarity values and in the number of medians sought by each individual, but also had a large number of groups to solve for (i.e.,G=10).Contrasting constructive heuristic to the entire VNS algorithm, we note that the constructive heuristic provided a solution which could not be improved by VNS in 9 out of 27 instances (i.e. ≈ 33 percent). It is interesting to note that the initial solution provided by the constructive heuristic, in 20 out of 27 instances, led to better upper bound solutions than those obtained by CPLEX in 24 hours.Our VNS algorithm is composed of three descent steps in its local search step. To contrast their effectiveness, we conducted a series of experiments in which they are used in an incremental way. Its results are summarized in Table 4. The first column of the table refers to the instances for which VNS improves the solutions provided by the constructive heuristic. The other columns refer to the relative differences in cost obtained by the application of the successive descents. Thus, ivns1 reports the average improvements (in percent) obtained by VNS with respect to the solutions provided by the constructive heuristic using only the first descent as local search. Let costCHbe the cost of the solution obtained by the constructive heuristic, then ivns1 is calculated ascostCH−costvns1costCH,where costvns1 is the average cost obtained by VNS using only the first descent. Similarly,ivns1+2reports the average improvements (in percent) obtained by VNS when the second descent is added to the VND framework, and finally,ivns1+2+3when the third descent is included to complete our algorithm. All average results in the table are calculated for 10 runs of the VNS heuristic with a time limit of 600 seconds.We notice from Table 4 that the gains incurred by the incremental use of the proposed descents are non-increasing in average: ≈ 2.83 percent with only the first descent, ≈ 0.58 percent with the first and second descents, and ≈ 0.09 percent with all three. However, we note that in some cases (instances #16 and #22) the largest gains are obtained only after the third descent is used. The negative values for Instances #24 and #27 are due to the smaller number of VNS iterations within the established time limit when the third descent is applied. Nevertheless, its incorporation within VND improved the average solution values in 11 out of 18 instances.In the experimental datasets used in our comparisons, we not only know the objective function value at the global optimum but also the true assignments of individuals to groups and, for each group, the true clustering solutions. As such, we can also investigate the ability of the algorithms to recover these decision variables. In this subsection, we compare the classification provided by the heuristics for the HCP with that provided by using the heuristic for the heterogeneous p-median problem (HPM) (Blanchard et al., 2012a).The two clustering models, and the resulting VNS heuristics, share many similarities. For starters, both models aim to group individuals based on the clustering solutions that can be obtained from their perceptions of a set of objects. However, the models are distinct in that: (i) whereas the number of medians in a group is conditioned to individuals membership in HCP, it is a variable in HPM; (ii) HPM is a multi-objective model converted to a single-objective model; it weights the sum of dissimilarities between each object and its assigned cluster, conditional on group membership, and the difference between the number of medians of each individual and the estimated number of medians. This second observation is critical because HPM requires the user of the proposed algorithm to select the value for the weight parameter. Our comparison contrasts the results obtained by the VNS heuristic of Section 3 and the VNS heuristic of Blanchard et al. (2012a) for the HPM using the same aforementioned computational platform. The weight used in HPM was set to the average of all dissimilarity values, weighting both roughly equally as was done in Blanchard et al. (2012a).Given that not only the algorithms but the models also are distinct, to compare the resulting approaches we need a performance metric that does not systematically favor one over the other. As such to examine the models’ (and the algorithms) ability to recover the original data of Table 1, we use the Adjusted Rand Index (ARI; Hubert and Arabie, 1985). Doing so allows us to compare the true clustering used in generating the simulated data with the predicted clustering variables e, as well as the individuals true partition in comparison with the predicted assignment variables z. We believe that this comparison is fair given that the same VNS heuristic framework was used, that both heuristics were demonstrated to have good performance regarding the optimization of their respective models, and that the results were collected using the same computational platform.The results in the last four columns of Table 5indicate the ARI index with respect to the recovery of the objects clustering (columne¯) and the groups (columnz¯). To calculate these measures, we obtained ARI separately for each individual before averaging the results across all the individuals. For both heuristics, we allowed 600 seconds of computational time.Our results suggest that, on average, both algorithms performed very well with an average ARI of .952 for HCP and .801 for HPM. However, the results from a paired sample t-tests suggests that the within-dataset difference between the algorithms for HCP (M=.952,SD=.059) and HPM (M=.801,SD=.263) is significant (t(26)=3.20,p<.01)44t refers to the t-statistic, and p refers to the associated p-value.. In fact, excluding the 9/27 datasets where both algorithms perfectly recovered the original data, the one for HCP outperformed the one for HPM in 14 out of 18 trials. With respect to the recovery of the assignments of individuals to groups, both algorithms also performed very well. Namely whereas the heuristic for HCP obtains an average ARI of .893 (M=.893,SD=.169), the heuristic for HPM obtains .851 (M=.851,SD=.236). The within-dataset difference between the two is not significant (t(26)=1.18,p=.25).What affects the algorithms’ ability to recover the original data? To investigate this critical question, we used multiple linear regression to predict each algorithm’s ARI using dummy-coded factors for each of the data characteristics. The results are displayed in Table 6for the objects clustering variables and in Table 7for the individuals grouping variables. In both tables, the rows indicate the data structures characteristics that were manipulated in the 27 generated instances. Each row contains the main effects (beta coefficients) for the factors used as independent variables, along with the significance of the factor. Finding a significant regression coefficient suggests that the algorithm is sensitive to the data characteristic. As few significant coefficients, as possible, is desired for an algorithm to be robust.With respect to HPM, we find that the algorithm has some sensitivity to changes in data structures. Specifically whereas the algorithm is unaffected by the number of individuals or the number of clusters, partitions with numerous clusters of equal sizes are better recovered than fewer clusters or those with uneven sizes. We also find that error (even in small amounts) to both dissimilarities and to number of medians significantly affects performance. The algorithm for HCP, in contrast, is mostly unaffected by data structures. Of note, it is particularly insensitive to errors added to the distances. It is also better able to recover clustering structures with a larger number of objects, and datasets when the number of groups is smaller than 10. That said, the impact of these factors is minimal as the mean ARI is .95 – a near perfect recovery of the original pairwise data.With respect to assignments of individuals to groups, we find that error added to the number of medians is a significant predictor, and that it is also sensitive to datasets where the number of groups is a few (but large) clusters. The algorithm for HCP is mostly unaffected when it comes to recovering group memberships. It has marginally more difficulty recovering large group memberships (whenG=10) and is only impacted by large error added to the number of medians.To further demonstrate the usefulness and performance of the proposed procedure, we collected data for a real-world application and used the proposed VNS heuristic to illustrate heterogeneity in the clustering performed by different individuals. The sorting task (also known as card sorting) asks participants to allocate a set of objects into piles according to their own perception. It is common to instruct participants to (1) put objects into the same pile if they are similar in some way (there are no pre-determined labels), and (2) use as many piles as they desire (c.f., Blanchard & Banerji, 2016), and the result is a set of participants who performed their own “partitions” over the set of objects.The pairwise similarity data provided by the sorting task is, in the most simplified way,yijk=1if individual k (k ∈ {1, … , m}) places objects i and j (i, j ∈ {1, … , n}) in the same pile (high similarity), 0 otherwise (low similarity). The task is particularly suited for the generation of such pairwise similarity data because the task mirrors closely the cognitive activities involved in the categorization process individuals follow as they form similarity judgments (Coxon, 1999), and it leads to as high quality data with less fatigue and boredom from participants as compared to pairwise similarity tasks (Bijmolt & Wedel, 1995; Rao & Katz, 1971).Data was collected from an online sorting study featuringm=189undergraduate students from a large northeastern United States university who answered about their perceptions aboutn=20chocolate candies disposed at the Corp’s Vital Vittles, the first storefront for Students of Georgetown Inc. opened in 1973. Today, Vital Vittles is a full-service grocery store which sells frozen foods, meals on the go, and a variety of home supplies. Because the Georgetown Campus housing is fairly isolated, it is considered a one-stop shop: Vittles faces little competition from local grocery stores. It is also very healthy financially, with gross sales averaging over 2 million dollars a year and a 23 percent gross profit margin.As part of its most prominent checkout counter shelves, Vital Vittles offers a large selection of chocolate candy. The shelf section dedicated to chocolate snacks includes the following options: Almond Joy, Baby Ruth, Butterfinger, Hershey (Almond), Hershey (Plain), Junior Mints, Kit Kat, M & M (Peanut), M & M (Plain), Mars Bar, Milky Way, Mounds Bar, Nestle’s Crunch, Oh Henry!, Payday, Reece’s Cups, Snickers, Three musketeers, Twix, and York Mint.Do consumers perceive these brands in similar ways? The piles made by these participants provide preliminary evidence that we can expect heterogeneity in the clustering solutions obtained: the mean number of piles (partitions) made by the participants was 5.73 (min=2,max=12), and the large variation in the number of piles made is illustrated by the histogram in Fig. 1.In order to be used by the HCP algorithm, the sort provided by each individual k, fork=1,…,m,is converted to a dissimilarity matrix Dkfollowing the procedure proposed by Takane (1980), and the number of medians ckis made equal to the number of piles made by that individual in the sorting task.For both VNS heuristics, we performed 10 executions of the procedures. All executions were terminated after 600 seconds of CPU time and Table 8shows the best of the objective functions as the number of groups (G) increases. To facilitate model selection, the table also shows the percentage improvement obtained when an additional group is added. Both algorithms seem to identify a solution withG=2groups, following the traditional “elbow in the curve” approach. The selection ofG=2is particularly evident for HCP which produces minimal improvements when G > 2 (less than 1 percent).Given that in this data the true assignment of individuals to groups (and their clustering solutions) is unknown, we cannot use the ARI to assess the absolute performance of the algorithms. However, the nature of the sorting task’s original data (yijk) allows to obtain the ARI regarding the piles composition made by each individual and its predicted clustering of objects according to the HCP. In our application, an agreement exists ifyijk=1whereas the HCP predictszkg=1andeijg=1.Among the twoG=2solutions, the heuristic for HCP performed slightly better. Namely whereas the heuristic for HPM obtainedARI=.2931,the one for the HCP obtainedARI=.3143- an improvement of 7.23 percent. This difference in fit comes primarily from a difference in assignments of individuals to groups. When comparing variables z predicted by both heuristics, we find a difference ofARI=.3413.The solution for HCP is presented in Table 9. The solution is composed of two groups, Novices (group 1; #individuals = 69) and Experts (group 2; #individuals = 120). As compared to members of the expert group, members of the novice group reported (1-strongly disagree, 5-strongly agree) being less confident of their piles (MNovices=4.55;MExperts=4.92;t(184)=1.72,p=.09) and more likely to enjoy salty foods more than sweets (MNovices=1.79;MExperts=1.42;t(184)=1.68,p=.09) than experts. Furthermore, when asked about their knowledge of such snacks compared to their peers, they were less likely to agree when compared to others, that they know more about chocolate (MNovices=2.94;MExperts=3.19;t(184)=1.72,p=.09).The partition structure of the Novices group contains 5 clusters. Because members of this group are less frequent consumers of chocolates, their clusters are largely structured around either attributes (main ingredient) or awareness. For this group, Cluster 1 include the mint based candy of Junior Mints and York Mint – an attribute salient in the product names. Cluster 2 includes tablets (thinner and wider) Hershey plain (median), Kit Kat, Hershey (Almond) and Nestle’s Crunch. Cluster 3 contains the small candy of M&M Peanuts (median) M&M Regular and Reese’s cups.Both Clusters 4 and 5 involve chocolate covered candy, yet they differ based on their perceived popularity among the participants. Cluster 4 contains the more popular bars, a cluster for which all the members of group knew all the items. Snickers, the most sold chocolate candy bar in the world, is the median of the cluster. Other members include Butterfinger, Milky Way, 3 Musketeers, and Twix. On average, 99.13 percent of cases were each of these bars known to members of this novice group. This is further confirmed by the results of our survey that followed the sorting task. Namely, on average each candy in this cluster had a 16.23 percent chance of being consumed once a month or more. Cluster 5, in contrast, includes less popular bars such as Mounds (median), Almond Joy, Baby Ruth, Mars, Oh Henry! and Payday. The results of the survey suggest that, on average, 28 percent of these bars were unknown to members of this novice group. Further, in only 3 percent of the cases were candy in this cluster consumed once a month or more.Just as novices, members of the Experts group also have a cluster composed of mint based candies which includes Junior Mints and York Mint. However, their partition structure is generally more complex starting with the presence of 2 additional clusters. For the Experts, cluster 2 is composed of nougat chocolate bars including Milky Way (median), Mars, Snickers and Three Musketeers. Cluster 3 focuses on almond & coconut based chocolates, with Almond Joy (median), Hershey (Almond) and Mounds Bar as members of the cluster. Cluster 4 includes crispy candy of Kit Kat (median), Nestle’s Crunch, and Twix, all candy known for their crispiness. Cluster 5 includes Reese’s cups (median) and butterfinger, two chocolates known for their peanut butter flavors. Cluster 6 includes Payday (median) and Baby Ruth, and Oh Henry, two candy bars heavily focused on peanuts and caramel. Finally, their Cluster 7 includes popular coated chocolates M&M, Hershey Plain, and M&M Peanuts.Insofar we have argued that aggregating similarity matrices potentially masks the heterogeneity present in the data . To evaluate the impact of aggregation, we can interpret the solution obtained withG=1and not only contrast it to theG=2solution obtained, but also see if the clusters have a meaningful interpretation. The clusters obtained forG=1(one aggregate matrix where all matrices are summed up) are presented in Table 10.The solution (for G=1) identified contains 5 clusters. When there is no heterogeneity (i.e., when both novices and experts are assumed to perceive the items similarly) such that both groups have a cluster in common, such a cluster is likely to also be found under the aggregate solution (G=1). The first cluster (Mint) is identical to that shared with both the novices (Group 1) and experts groups (Group 2). The second cluster (Nougat chocolate bars) is easy to interpret, and is highly similar to that of the experts - with the addition of Baby Ruth. The third cluster (Mixed Nuts) reflects a combination of various chocolates that have almonds, coconut, and other nuts (but exclude peanuts). It is not similar to any cluster formed for the novice and expert groups, and thus represents neither groups.In the presence of heterogeneity in dissimilarities however, clusters found in an aggregate solution might reflect a mix of both cluster solutions in a way that does not necessarily make sense. For instance, the fourth and fifth clusters are difficult to interpret. The fourth cluster includes tablets, bars, small candies, crunchy, and the snacks very in popularity. The fifth cluster includes M&M (both plain and peanuts) and Hershey (plain) likely reflecting outliers. In sum, the aggregate solution provided does poorly (particularly the 4th and 5th clusters) at providing insights that are managerially useful.Comparing the partition structures of the two groups brings important implications for the retailer. First, it seems that infrequent chocolate buyers pay more attention to physical cues such as the size (small, tablet, bar) than more frequent buyers. They also tend to know fewer brands and thus distinguish between the popular and less popular candy in the first stage. As previous research has shown, consumers with low store knowledge and with significant time pressure tend to be the most likely to be lead to unplanned purchases (Park, Iyer, & Smith, 1989) and that the assortment structure chosen can influence similarity perceptions and willingness to pay (Lamberton & Diehl, 2013), we would first recommend that Vital Vittles organizes a section of its checkout counter candy display along the lines of the clusters encountered by novices (a large sample of busy students). Second, frequent buyers (experts) tend to think of chocolate as a function of their key ingredients, and the salient ones seem to be more about the filling rather than the coatings or shape. An organization that helps label these chocolates for quick identification (e.g., stickers that indicate that the chocolate contains nougat) could prove to be especially helpful in directing this audience.We presented a new model for clustering data collected from heterogeneous dissimilarity matrices (e.g., from individuals with different points of view). The model simultaneously assigns individuals to groups with similar clustering and, for each group, determines the best clustering solution. Our computational experiments show that several generic solvers are inadequate to tackle the problem, and that the proposed Variable Neighborhood Search heuristic is able to solve quickly for problems of even large sizes. It is able to do so because the local search introduced relies on three local descents embedded into the Variable Neighborhood Descent framework. Our experiments show that the algorithm is also able to recover the clustering structure from available datasets.Through a Monte Carlo Simulation, we demonstrate the algorithm’s capacity to recover heterogeneous data, confirmed by average Adjusted Rand Indices always superior to 0.89, and its (in)sensitivity to various data characteristics. Finally, through an empirical illustration that involves consumer sorts of chocolate candy, we illustrate how the model can be used to gain insights into perceptions of a set of brands for groups of consumers who vary in their expertise with the product category. Thus, the present paper contributes to a growing literature that focuses on the role of unobserved clusters in the consumer decision making processes (e.g. Noseworthy, Wang, & Islam, 2012; Swait, Brigden, & Johnson, 2014).Our heuristic involves, in the three descents, the use of generic MIP solvers to accelerate optimization. A possible direction for future research would be to investigate if the use of specialized heuristics such as local branching Fischetti and Lodi (2003); Hansen, Mladenović, and Urošević (2006) would provide additional improvements. Our computational experiments show that the VNS heuristic outperforms general purpose exact solvers widely used, providing better (or equivalent) solutions in all tested cases in much less computing time, but it could perhaps still be improved.We need to qualify that our results regarding the relative superiority of our heuristic (to VNS-HPM) reflect a difference in both the underlying mathematical models, and the implementation of the resulting heuristics. Whereas we compared the performance of the heuristics using a criterion (i.e., Adjusted Rand Index) for which neither model or heuristic had an undue advantage, it is thus remains possible that a better implementation of VNS-HPM (or a better approach to set the penalty parameter delta) would lead to VNS-HPM to outperform the HCP.Finally, we approach to modeling heterogeneity in the dissimilarity matrices has been by simultaneously identifying groups and clustering solutions for each group. Such an approach is related to the literature on consensus clustering, where the objective is to find the one (and only one) solution that best represents the data. We note however that the objective in group-level models (such as ours) is to help identify if a single clustering solution can be used, and if not, illustrate the extent to which the multiple solutions differ. For instance, future research should investigate if applying consensus clustering (e.g., Monti, Tamayo, Mesirov, & Golub, 2003) could have obtained to a single clustering solution adequately data that has been generated based on truly heterogeneous judgements.

@&#CONCLUSIONS@&#
