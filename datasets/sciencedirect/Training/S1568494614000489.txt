@&#MAIN-TITLE@&#
Genetic algorithms tuned expert model for detection of epileptic seizures from EEG signatures

@&#HIGHLIGHTS@&#
Novel scheme using wavelet packet decomposition (WPD), SVM and GA.Features extracted by WPD and computing statistical and non-statistical properties.Support vector machine acts as objective function for GA.Optimal feature sub-set selected by adaptive GA.Seizure detection rate achieved is 100%.

@&#KEYPHRASES@&#
Discrete wavelet transform (DWT),Genetic algorithm (GA),Support vector machine (SVM),Electroencephalogram (EEG),Field programmable gate arrays (FPGA),

@&#ABSTRACT@&#
The uncontrolled firing of neurons in brain leads to epileptic seizures in the patients. A novel scheme to detect epileptic seizures from back ground electroencephalogram (EEG) is proposed in this paper. This scheme is based on discrete wavelet packet transform with energy, entropy, kurtosis, skewness, mean, median and standard deviation as the properties for creating features of signals for classification. Optimal features are selected using genetic algorithm (GA) with support vector machine as a classifier for creating objective function values for the GA. Clinical EEG data from epileptic and normal subjects are used in the experiment. The knowledge of neurologist (medical expert) is utilized to train the system. To evaluate the efficacy of the proposed scheme, a 10 fold cross-validation is implemented, and the detection rate is found 100% accurate with 100% of sensitivity and specificity for the data under consideration. The proposed GA-SVM scheme is a novel technique using a hybrid approach with wavelet packet decomposition, support vector machine and GA. It is novel in terms of selection of features sub set, use of SVM classifier as objective function for GA and improved classification rate. The proposed model can be used in the developing and the third world countries where the medical facilities are in acute shortage and qualified neurologists are not available. This system can be helpful in assisting the neurologists in terms of automated observation and saving valuable human expert time.

@&#INTRODUCTION@&#
Electroencephalogram (EEG) was first measured in humans by German psychiatrist Hans Berger [6] in 1929. It gives an insight into the neural activity in brain and is a non-invasive method to study cognitive processes and the physiology of the brain. It gives very useful information related to brain dynamics and different physiological and psychological aspects of the brain. Epilepsy is a chronic disorder characterized by recurrent seizures, which may vary from a brief lapse of attention or muscle jerks, to severe and prolonged convulsions in the body [47]. The seizures are caused by sudden, usually brief, excessive electrical discharges in a group of brain cells (neurons) (Cited in WHO: http://www.who.int/topics/epilepsy/en/). More than 1% (50 million people) of the world's population is affected by epilepsy [44]. Eighty percent of the epileptic seizure activity can be controlled or treated effectively, if properly detected and diagnosed (cited in WHO: http://www.who.int/topics/epilepsy/en/) [47]. The acute shortage of medical facilities in the developing and third world countries is the major cause of deaths. Even if the basic facilities of medical help may be present in some parts of these countries, yet trained neurologists are present only in super specialty hospitals in big cities. A neurologist inspects the EEG recordings over time to assess and diagnose various ailments related to brain and thereby decides the course of treatment and evaluates the pre-surgical conditions of patient if surgery is required. In rural and remote areas, some form of automated system of diagnosis of severe illness like epilepsy is required which can be operated by skilled or semi-skilled medical persons. Other benefits of designing an automated expert system are to help the neurologist to effectively assess the condition, to reduce the requirement for storage of data and to save time of manual inspection.Many approaches towards automatic detection of this disease are reported in literature [1,3,16,18–20,23,31,32,35,37,52]. In this paper, a novel scheme is proposed, which uses the knowledge of a trained neurologist about the epileptic and non-epileptic subjects for training of the system, and once trained, it can detect epileptic seizures. The performance of an epileptic seizure detection system depends on features extraction and features selection. In this paper, the use of discrete wavelet packet decomposition with energy, entropy, kurtosis, skewness, mean, median and standard deviation, as properties for creating the feature set is proposed. A genetic algorithm with adaptive probability of mutation is used as the optimization algorithm for selecting optimal feature set to present to the classifier so as to maximize the classification accuracy. Support vector machine is used as a classifier for each set of values stored in each chromosome and the classification rate is chosen as the objective function value for the GA. The data of epileptic and non-epileptic subjects are taken from Andrzejak website as given in [40]. The proposed GA-SVM technique outperforms the results of recent approaches reported in [20,37,52]. In the present approach, optimal selection of feature sub set is employed, so it outperforms the approaches of [4] in terms of feature set and also in terms of correct classification rate. The proposed approach outperforms the reported techniques in that it gives 100% correct classification rate whereas the earlier approaches reported the same between 73% [37] and 99.33% [47]. The paper is organized in six sections starting with introductory section followed by literature survey and state of the art in Section 2. Data and methods are given in Section 3 followed by optimization of features selection in Section 4. Section 5 deals with experimental results and Section 6 presents conclusions and future directions.Richard Canton, an English physician, discovered electrical currents in the brain in 1875. Electroencephalogram (EEG) was first recorded [6] in 1929, and paved the way for studying electrical signals related to the cognitive and brain activities. EEG is a non-invasive method giving a lot of information related to the different physiological states and complex dynamical behaviour of the brain. EEG recorded over a long time span is very important for monitoring incidental disorders like epileptic seizures which are not permanently present in the recordings. EEG data for long periods of time is visually inspected by trained neurologists for detecting epileptic seizures and pre-surgical evaluation of epilepsy patients, and used for clinical diagnosis and possible treatment plans. This is a time consuming and very costly process. Efforts for predicting the epileptic seizures were first time reported in 1975 [43]; however, further attention was paid by researchers and clinical neurologists during 1990s. A recent approach (2012) presented in [52] employs a tunable support vector machine classifier for epileptic seizure detection, and gives correct classification rate of 98.72%. Tapan et al. [47] recently proposed a scheme based on discrete wavelet transform (DWT) and energy estimation at each node of the decomposition tree followed by application of probabilistic neural network (PNN) for classification, giving classification rate of 99.33%. Recently Zandi et al. reported predicting of epileptic seizures in scalp EEG based on a Variational Bayesian Gaussian mixture model of zero-crossing intervals [1]. An 8-channel scalable EEG acquisition SoC (System-on-Chip) with patient-specific seizure classification and recording processor is presented in [23]. An EEG classification system based on Wavelet-CSP for hand movements is presented in [36]. Neonatal seizure detection is addressed in [32]. The canonical correlation is used in [16], while in [35], detection of epileptic spikes by magnetoencephalography and electroencephalography (EEG) is explored. A data driven approach for identification of pre-seizure states in epilepsy is presented in [19]. Recurrent neural network based prediction of epileptic seizures in intra- and extra-cranial EEG is reported in [3].A comparative study based on the field of seizure prediction using statistical validation of event predictors is given in [18]. A comparative analysis of feature selection from EEG, using mutual information and support vector machine is presented in [7]. Prediction of epileptic seizures using accumulated energy in a multi-resolution framework is presented in [45]. Some approaches for seizure detection like Phase space topography and the Lyapunov exponent of electrocorticograms [30], computer analysis of EEG signals with parametric models [2], artificial neural networks [38], non-linear time series analysis [24,26,33], recurrence quantification analysis [51], synchronization in intracranial electroencephalogram recordings [15] have been reported in literature. Nonlinear dynamics and chaos are used in [24,26,30,51], phase coherence in [32], and time domain analysis in [4,5], for seizure detection based on EEG. Various soft computing approaches for scalp EEG processing are explained in [25]. EEG signals are used for classification of BMD and ADHD patients [27]. Seizures anticipation in human neocortical partial epilepsy is addressed in [49]. EEG signals classification using the K-means clustering and a multilayer perceptron neural network model is presented in [48]. The methods reported in [37] by Ocak use genetic algorithms and wavelet transforms but only entropy at different levels of decomposition is taken for preparation of feature sets and a correct classification rate of 73% was achieved. The same authors attempted discrete wavelet transform and approximate entropy leading to an increased classification rate of 96% [20]. In [13], the use of mixture of experts (ME) network structure to guide model selection for classification of electroencephalogram (EEG) signals is illustrated and gives classification rate of 93.17%. In this paper, a novel scheme is proposed based on discrete wavelet packet decomposition of the signals from epileptic and non-epileptic subjects (100 segments each) and then computing the energy and other statistical and non-statistical properties at each node of decomposition tree to create the feature set for classification. The feature set thus created is vast enough, so an optimal selection of the feature sub-set is done by using a GA. This technique outperforms the results of recent approaches reported in [20,37,52]. As there is no feature sub-set selected in the approaches reported in [47], but in the present approach optimal selection of feature sub set is employed, so it outperforms the approaches of [47] in terms of feature set and also in terms of correct classification rate. The proposed approach outperforms the reported techniques in that it gives 100% correct classification rate where as the earlier approaches reported the same between 73% [37] and 99.33% [47].The EEG data used for this work is taken from University of Bonn, Germany, as given in [40] by Andrzejak et al. This data [40], comprising of five sets (denoted A–E), each containing 100 EEG segments of 23.6-s duration, are used for the study. Sets A and B consist of segments taken from surface EEG recordings that are carried out on healthy volunteers using a standardized electrode placement scheme (10/20 International system of electrode placement for non-invasive recording of EEG from human scalp) as shown in Fig. 1. Volunteers were relaxed in an awakened state with eyes open (A) and eyes closed (B), respectively. While sets C and D contain only activity measured during seizure free intervals, set E contains only seizure activity. Segments are selected from all recording sites exhibiting ictal activity [40]. We take set E and set A for our experiments, as set E contains the ictal state data, so these are confirmed cases of epilepsy and set A pertains to confirmed subjects having no epilepsy. Knowledge of medical expert is available in terms of the data sets already defined to be containing data from epileptic or non-epileptic subjects. Part of this data (i.e., 50%) is used for training and 50% for testing during execution of GA-SVM and a 10-fold cross validation is used for the final optimized feature sub set given by GA-SVM, to illustrate the efficacy of the technique.Wavelet packet analysis is used for extracting features of non-stationary signals, and as such is appropriate for extracting features from EEG signals which are non-stationary in nature. Wavelet transform gives time-frequency representation of a signal. Wavelet packet analysis is a generalized form of the DWT, wherein a signal is split into approximation and detailed coefficients. The approximation is again split into a second-level approximation and detailed coefficients, and the process is repeated. There are k+1 possible ways to encode the signal for the k-level decomposition. The signal is passed through a series of low-pass and high-pass filters called quadrature mirror filters. In wavelet packet analysis, the approximation as well as detailed coefficients can be split like a complete binary tree structure. This way, of recursively applying decomposition process in both low and high frequency sub-bands to generate next level in the binary tree, yields more than 22k−1 different ways to encode the signal. For each level k, there are 2k−1 number of nodes available after wavelet packet decomposition. Fig. 2shows the wavelet packet decomposition tree obtained for 8th level of decomposition and example data for node (8, 64) means data at 8th level and 64th coefficient of decomposition. Shannon entropy-based criterion is used to select suitable decomposition of the signal. Eighth level decomposition coefficients are used to extract the features. Statistical and non-statistical properties such as energy, mean, median, standard deviation, kurtosis, skewness and Shannon entropy of the decomposition coefficients are used to create the complete feature set. All algorithms are developed in the MATLAB environment with Wavelet and Bioinformatics Toolboxes. The coefficients Cikat 8th decomposition level are used to calculate features by using following standard equations [47]:(1)Energy,EDi=∑k=1NCik2i=1,2,…,l(2)Standarddeviation,σi=1N−1∑k=1N(Cik−μi)2,i=1,2,…,l(3)Mean,μi=1N∑k=1NCik,i=1,2,…,l(4)Kurtosis,KURi=ε(Cik−μi)3σi3,i=1,2,…,l(5)Skewness,SKi=ε(Cik−μi)3σi3,i=1,2,…,l(6)Entropy,ENi=∑k=1NCik2Log(Cik2),i=1,2,…,l(7)Median,medi=∑k=1Nmedian(Cik),i=1,2,…lThe coefficient Cikis the decomposition coefficient. Here, i=1,2,…,l is the node number at 8th level of decomposition. N is the number of coefficients in each decomposed data. Energy EDi, Standard deviation σi, mean μi, Kurtosis KURi, Skewness SKi, Entropy ENi, and Median mediare computed for each node at 8th level of decomposition tree. The term E(Cik−μi)nis nth moment of the mean. Total seven properties, as defined in above equations, are used for creating the feature set. Thus at 8th level of decomposition, there are 256 (28=256) coefficients and a feature set of 1792 (256 coefficients×7 properties=1792) features is extracted.The total feature set becomes:[Features]=[ED1, ED2,…,ED256, σ1, σ2, σ256, μ1, μ2,…,μ256, KUR1, KUR2,…,KUR256, SK1SK2,…,SK256, EN1, EN2,…,EN256, med1, med2,…,med256].The total numbers of nodes at 8th level of decomposition are 256. This matrix of length 1792×1 gives the total feature vector.The proposed hybrid scheme based on wavelet packet decomposition, genetic algorithms and support vector machine is a novel scheme in terms of SVM acting as objective function for GA and the feature sub set selection by GA with stopping criterion set to 100% accurate classification rate. The present scheme starts with Wavelet Packet Decomposition of each signal up to 8th level. Two data sets namely A and E, having 100 EEG segments in each set, are taken for this experiment, as explained in Section 3.1. At 8th level of decomposition, each signal is decomposed into 256 coefficients. For each decomposition coefficient, seven properties namely energy, mean, median, standard deviation, kurtosis, skewness and Shannon entropy are calculated, giving 256×7=1792 features for each signal, making a complete feature set for a signal. These features are computed for both sets, i.e., set A and set E for creating a complete feature set for all the signals used in the experiment. In EEG signal classification, all these features are not important in the learning process of the classifier. Considering all the features for classification causes computational inefficiency and increases computation time. Sometimes, even the contrasting characteristics of some features may cause to degrade the overall performance of the classifier. So, for optimal classification accuracy and efficient computation time, optimal selection of features is carried out by a genetic algorithm with adaptive probability of mutation and SVM acting as objective function. The objective function values for GA are calculated by SVM. Out of the feature set comprising of 1792 features, the minimal feature set (50 features out of 1792) which gives best classification is selected optimally by the proposed GA-SVM scheme. The number of such features is of importance, as the exhaustive feature set (comprising of 1792 features) is computationally intensive and time consuming. A larger feature space than that used in [47] is chosen in the present study. A 6th level decomposition with lesser number of statistical and non-statistical properties and without optimal features selection scheme is presented in [47]. However, in the present work, more possibilities are explored to find optimal and relevant features for higher correct classification rate. Here optimal feature sub set means a feature sub set which will result in optimal (here maximum) classification accuracy. Fifty features out of the total feature set are selected. The number of optimal features is also an area of research, as to how many features will be sufficient for efficient and proper classification. According to the theory of large numbers, 10 is usually considered as the smallest large number, so authors started with selecting a features sub set of order 10 but this was insufficient to drive the proposed system to convergence, so the number of features increased in the steps of 10 towards the optimal features sub set and it is found that when 50 features were selected, then the system converged in reasonable time. As the number of features selected goes below 50, the number of features becomes insufficient for classification and the algorithm iterates in a futile way to search for a feature set which may give 100% classification rate. The stopping criterion is set to 100% classification rate, so the GA will stop only when it searches for a feature sub set which gives 100% classification rate. The proposed technique is tested only for the present data set, however, if we put the technique for a clinical trial and new samples at large are presented before the proposed system, it may or may not result in 100% correct classification rate. The efficacy of the proposed scheme is further validated by testing it with a 10 fold cross validation scheme used for SVM with the selected feature sub set (50 features) for same data sets and it results in 100% correct classification rate with same order of specificity and sensitivity.Genetic algorithm find optimal solution based upon the natural phenomenon of selection, reproduction and mutation. Fundamental work of Holland [21,22], Goldberg [11], Rechenberg [39], Schewefel [41], and Fogel [14] paved the way for today's genetic algorithms. When applied for practical problem solving, a GA begins with a few contending trial solutions. Variation operators are applied, and evaluation of objective function followed by a checking of stopping criterion completes the flow of GA, as shown in Fig. 3.In present application, we have used GA with roulette wheel selection, one point cross over, mutation with adaptive probability of mutation and elitism. An objective measure of performance is used to assess the fitness of each trial solution and the selection mechanism determines which solutions should be maintained as ‘parents’ for the subsequent generation. A population of 30 chromosomes, with 50 allel positions in each chromosome, is initialized. Each allel value represents the index of a feature in the features set. The support vector machine is used for generating the fitness value for each solution (chromosome). Each chromosome in the population is presented to the SVM and classification rate is calculated. The classification rate is the fitness value for the chromosome, and the objective is to maximize the classification rate. Roulette wheel selection is used for selection operator. One point crossover, with 0.8 probability of crossover is applied to perform crossover. The mutation rate is initially set as 0.009. Adaptive probability of mutation is used; as the GA reaches towards convergence, the mutation rate is decreased based upon the increase in average fitness of the population. Elitism is used to save so-far-best solutions during evolution of the GA, the best solution in each generation is saved and passed on to the next generation without applying variation operators to it. The fitness for the population is calculated again and stopping criterion is checked. The stopping criterion used in this paper is 100% correct classification rate. Initially, the GA took too much time to converge but as the probability of mutation is adaptively decreased with increase in average fitness of the successive populations, it converged earlier.The architecture of an SVM [52] is shown in Fig. 4. For the data sets {(x1,y1),…,(xm,ym)}, x are patterns of the data and y are the class labels of each data set x. In present application, there are two labels, epileptic and non-epileptic and are designated as +1 and −1. During training, the classes are presented with each data sample. The SVM tries to construct the optimal hyperplane yielding the maximum margin of separation between the members of the two classes by solving the following quadratic programming optimization problem. A soft margin can be constructed by introducing a slack variable βi, i=1,…,m, here m is the number of samples. The optimization problem becomes [52]:(8)Minimize(W,b,βi)12W2+D∑i=1mβiSubject toyi((W∗Xi)+b)≥1−βi,D>0βi≥0,i=1,2,…mW∈RN,b∈ℜ.D is a user-defined constant greater than zero. This problem can be simplified by its Lagrangian dual as follows:(9)Minimize(a∈ℜn)∑i=1maiajyiyj(XiXj)WithD≥ai≥0∑i=1maiyi=0HereW=∑i=1maiyiXii=1,2,…,m.The decision function becomesf(x)=∑i=1m(aiyi(X,Xi)+b).From this deduction, the patterns Xi(ai≠0) are the support vectors. These support vectors lie on the margin of the decision boundary and all other data points are irrelevant. The feature vector X only appears in the equations as dot products so, the decision functions can be written as:f(x)=∑i=1m(aiyiK(X,Xi)+b).where K(X,Xi) is kernel function and is simply the dot product of X and Xi. This kernel function is used in the present application. Other types of kernel functions can be tried for making SVM more efficient and less time consuming for the proposed kind of heavy computation problem where the SVM is called many times in a single run of the GA.Cross-validation, also called rotation estimation, is the statistical method of partitioning data set into subsets such that the analysis is initially performed on a single subset, while the other subsets are retained for subsequent use in confirming and validating the initial analysis [9,10,29]. The initial subset of data is called the training set; the other subsets are called testing or validation sets. The original data set is partitioned into K subsets in a K-fold cross-validation arrangement. Out of these K subsets, one subset is kept as the validation data for testing the system, and the remaining (K−1) subsets are used as training data. The cross-validation is then repeated K times (equal to the no. of folds), with each of the K subsets used exactly once as the validation data. The K results from the folds then can be averaged to produce a single estimation. The advantage of this method is that all observations are used for both training and validation, and each observation is used for validation once only. The variance of the resulting estimate is reduced as K is increased. The disadvantage of this method is that the training algorithm has to be re-run from scratch K times, which means it takes K times as much computation to make an evaluation [29]. K-fold cross validation is not used for each computation of objective function, but only for the optimized set of features after the GA gives the final subset of features. Ten-fold cross validation is used in the present application.

@&#CONCLUSIONS@&#
