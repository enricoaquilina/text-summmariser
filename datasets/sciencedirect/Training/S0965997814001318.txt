@&#MAIN-TITLE@&#
Stepwise approach for the evolution of generalized genetic programming model in prediction of surface finish of the turning process

@&#HIGHLIGHTS@&#
Importance of generalized model in understanding the turning process is addressed.M-MGGP method is proposed for the evolution of generalized model.Performance of M-MGGP is compared to those of SVR, ANN and MGGP.Among four methods, M-MGGP evolves model with better generalization ability.Parametric/sensitivity analysis explains mechanism of turning process.

@&#KEYPHRASES@&#
Surface roughness prediction,Surface property,Turning,Genetic programming,Stepwise regression,Support vector regression,

@&#ABSTRACT@&#
Due to the complexity and uncertainty in the process, the soft computing methods such as regression analysis, neural networks (ANN), support vector regression (SVR), fuzzy logic and multi-gene genetic programming (MGGP) are preferred over physics-based models for predicting the process performance. The model participating in the evolutionary stage of the MGGP method is a linear weighted sum of several genes (model trees) regressed using the least squares method. In this combination mechanism, the occurrence of gene of lower performance in the MGGP model can degrade its performance. Therefore, this paper proposes a modified-MGGP (M-MGGP) method using a stepwise regression approach such that the genes of lower performance are eliminated and only the high performing genes are combined. In this work, the M-MGGP method is applied in modelling the surface roughness in the turning of hardened AISI H11 steel. The results show that the M-MGGP model produces better performance than those of MGGP, SVR and ANN. In addition, when compared to that of MGGP method, the models formed from the M-MGGP method are of smaller size. Further, the parametric and sensitivity analysis conducted validates the robustness of our proposed model and is proved to capture the dynamics of the turning phenomenon of AISI H11 steel by unveiling dominant input process parameters and the hidden non-linear relationships.

@&#INTRODUCTION@&#
Components produced during the turning operation have critical features that require specific surface finish [1]. Surface roughness is a widely used index for the measure of product quality. Hence, achieving desired surface roughness is of prime importance for the functional behaviour of the component. Past studies reveal that surface roughness depends on process parameters such as tool geometry, cutting conditions and work piece properties. These process parameters may be optimised for obtaining minimum cost and minimum production time. However, for obtaining the optimal input process parameter settings, the surface roughness needs to be predicted accurately. Hence, the modelling of turning process has attracted a great community of researchers with the purpose of reduction of overall cost of the engineering component [2]. With the use of numerically controlled CNC machines, the need for process modelling and optimisation is strengthened.Considerable amount of research has been done in the prediction of surface roughness of the turning process [3–10]. Researchers have developed physics-based models to understand the behaviour of turning, but this may be a challenging task with the availability of partial information about the process [2,11]. The empirical modelling based only on the given data is a possible route for the modelling of the process. For this purpose, several empirical modelling methods such as regression analysis, artificial neural networks (ANN), support vector regression (SVR), fuzzy logic (FL) and genetic programming (GP) have been extensively applied in the prediction of surface roughness [12–16].The regression analysis is based on statistical assumptions, and thereby induces uncertainty in the prediction ability of the models [17–19]. These models cannot be used to generalise the process data. ANN is well known for capturing the dynamics of the process. However, the optimal architecture of ANN is determined either through trial-and-error approach or by hybridising it with heuristic optimisation methods such as genetic algorithm (GA), and particle swarm optimization. This indicates that some skills/knowledge is needed to select the optimal architecture of ANN for faster training and better accuracy of the model [20–24]. SVR based on principle of SRM, is known for injecting generalisation ability in the models. Least square-support vector machines (LS-SVM) variant of SVR has been used for predicting the performance of turning process [7,25,26]. However, it does not provide explicit formulation between the input and output process parameters, and gives output values in crisp form.FL models mostly used for modelling turning process are Mamdani and adaptive neuro-fuzzy inference system [27–30]. The formulation of FL model requires expert knowledge to formulate the fuzzy rules. Researchers have carried out additional set of experiments to test the empirical models, but it involves high labour costs and results in increase in overall cost of the product [31,32].The applications for the explicit formulation of the performance of machining process using evolutionary approach GP have been on the rise [33–38]. The main advantage of GP over the regression analysis and other statistical modelling techniques is that it has the ability to generate mathematical equations without assuming any prior form of the existing relationships. GP and its variants have been successfully applied for modelling the performance of various non-conventional and machining processes [39,40]. The performance attributes measured were cutting force, surface roughness, tool wear, etc. Other variant of GP that uses set of genes for the formulation of model is multi-gene genetic programming (MGGP).The peculiarity of MGGP method [41,42] is that each model participating in its evolutionary stage is the combination of several genes combined using the least squares method. The applications of MGGP method suggest that it performs better than the traditional GP method [43–45]. In traditional GP method, the model is a single tree/gene expression whereas in MGGP, the model formed is a linear combination of several trees/genes. Gandomi and Alavi [46–48] in his work demonstrated the usefulness of MGGP approach in designing the non-linear models based on the data obtained from the complex non-linear systems. Past studies reveal that MGGP method provides a fast and cost-effective explicit formulation of a mathematical model based on multiple variables with no existing analytical models. Despite remarkable capabilities of MGGP method [49–54], it is found that during the combination mechanism using least squares method in its evolutionary stage, the occurrence of genes of lower performance can degrade the performance of the MGGP model [55–58]. This limitation of the MGGP method has motivated us to develop modified MGGP (M-MGGP) method by sensibly selecting the relevant genes of higher performance for the combination mechanism.In the present work, a M-MGGP method is proposed and applied to the modelling of surface roughness of the turning process. In this method, a stepwise regression approach is introduced for the combination of genes. Unlike least squares method, the stepwise approach selectively eliminates redundant/poor performing genes and thus only combines the high performing ones. One objective of the present work is to compare the performance of the M-MGGP model to those of the standardized MGGP approach, SVR and ANN. Sensitivity and parametric analysis is then conducted for the proposed model to accentuate the principle behind the process and examine the dominant input parameters.In the present work, the turning phenomenon to be modelled is referred from an earlier study conducted on modelling and optimisation of hard turning of AISI H11 steel using response surface methodology [59]. The experiments were performed in dry conditions using lathe type SN40C with a spindle power of 6.6KW. The sample material used was AISI H11 hot work steel which is often used for the manufacture of diecasting moulds, dies and helicopter rotor blades. The composition of AISI H11 steel is shown in Table 1. The dimensions of the material is 80mm in diameter and is hardened to 50 HRC. The input variables considered were cutting conditions such as cutting speed, feed rate and cutting time and the output variable was surface roughness. The input variables with its low-centre-high levels used are shown in Table 2[59].Twenty-seven sets of data samples as shown in Table 3are considered in this study in the turning of AISI H11 steel. These data samples were collected based on L27 standard full factorial 3-level experimental design. For the appropriate selection of training and testing data set, the Kennard and Stone (K–S) algorithm [60] is used to divide the data set into training and testing. This algorithm selects the training and testing data in such a way that the whole data set is distributed uniformly throughout the domain. Several applications of the K-S algorithm are reported [60–62]. Table 3 shows the set of data samples after the Kennard and Stone algorithm have been applied. The first twenty-one samples form the set of training data with remainder used as the set of test samples. The test data samples are chosen for testing the generalisation ability of a model while only the training data is used for formulating the models.Genetic programming (GP) is an advanced modelling method that generates models in the form of tree structures based on the principle of Darwinian natural selection [63]. The initial population of individuals is randomly created to search for the solutions in large global space. Each member of the population is a tree structure comprising functions and terminals. The functions and terminals are chosen from the function and terminal set respectively. A function set F usually consists of basic arithmetic operations (+, −, ×, /, etc.), Boolean operators (AND, OR, etc.), or other operators as defined by the user. The Terminal set T consists of numerical constants and input decision variables of the process. The model is constructed by combining the functions and terminals randomly from the respective sets. An example of a simple tree representation of a GP model is shown in Fig. 1. The performance of every individual of the population is evaluated on the training data based on the fitness function. The fitness function generally used is the root mean square error (RMSE) given by(1)RMSE=∑i=1N|Gi-Ai|2Nwhere Giis the value predicted of the ith data sample by the MGGP model, Aiis the actual value of the ith data sample and N is the number of training samples.The GP algorithm evaluates individuals and selects them for genetic operations (reproduction, mutation and crossover). The genetic operations on the initial population of individuals results in a new generation comprising a new set of individuals. During the crossover operation (Fig. 2), a branch of tree is randomly selected from the two individuals and swapped. During the mutation operation (Fig. 3), a terminal or function is selected at random from the tree and is mutated. The evolutionary process continues as long as the termination criterion is not met. The termination criterion defined by the user is the maximum number of generations. The best model is selected based on minimum error on training data.Unlike traditional GP, the evolutionary stage of MGGP method involves the model which is a combination of several set of trees/genes. The algorithm of the MGGP method is outlined as follows:BEGINStep 1: Define problemStep 2: MGGP algorithmBegin2.a Define initial algorithm parameters such as terminal set, functional set, population size, number of generations, maximum depth of gene, maximum number of genes to be combined, probability rate of genetic operators, termination criterion, etc.2.b Randomly generate initial population of genes2.c Combine genes using least squares method to form a model2.d Evaluate performance of initial population of models based on fitness function RMSE and cross-check the termination criterion2.e If termination criterion satisfied select the best model, else apply genetic operations and GOTO Step 2.dEnd;END;In the mechanism of formulation of the MGGP model (Fig. 4), genes are combined randomly and regressed using the least squares method so as to determine the coefficients Po, P1, P2 and P3. The model predicts the output variable based on the two input decision variables (x1 and x2). A limit on the maximum number of genes and maximum depth of the gene can provide control over the complexity of the MGGP models which may results in compact models.Several parameter settings are involved in the implementation of MGGP method. The parameter settings are important since it affects the generalization ability of the formed MGGP model. The parameters selected based on trial-and-error approach are shown in Table 4. The function set F consists of basic arithmetic operators and few non-linear mathematical functions. The broader set of functions is chosen in the function set since this can provide broader variety of non-linear mathematical models. The number of models in the population is represented by population size. The number of generations is the number of genetic operations that an algorithm makes before it terminates. The population size and number of generations fairly depends on the complexity of the problem. Based on literature review by Garg and Tai [64], the population size and number of generations should be fairly large so as to find the models with minimum error. The size of search space and number of models searched within the space are directly influenced by the maximum number of genes and maximum depth of the gene. In this study, the maximum number of genes and maximum depth of gene is kept at 8 and 6 respectively.We used GPTIPS software in the current study to perform multi-gene genetic programming for the prediction of surface roughness of the turning process. This software is a new “Genetic Programming and Symbolic Regression” code written based on MGGP [41,42] for the use with MATLAB. MGGP method is applied to the data set shown in Table 3. The best MGGP model (Eq. (2), where x1, x2 and x3 are input process parameters) is selected based on minimum RMSE on training data from all runs and its performance is discussed in Section 4.(2)Surface roughnessMGGP=2841.7769+(63.304)∗(tanh((tan((psqroot(x-2))∗(tanh((0.737515)))))∗(tanh(tanh(x3)))))+(0.0025959)∗(tan((x1)-(x3)))+(0.0018065)∗(tan((((x1)+((-8.368002)))-(psqroot((x3)∗(x2))))+(((x1)+((x1)-(x3)))∗(tanh(x1)))))+(0.011418)∗(tan(tan(((tanh(x1))∗(tan(x2)))-(x3))))+(-845.6592)∗(((tan(tanh((x1)∗(x1))))∗(tan((tanh(x1))∗(tanh(x3)))))+(tanh(((psqroot(x2))+((1.583457)))-(tanh(tanh(x2))))))+(0.12505)∗(psqroot(((6.070678))-((tanh(((6.070678))∗(x2)))-(x3))))+(0.19082)∗(psqroot(((x3)∗(tan((x2)+(x2))))-(psqroot(x3))))+(0.092673)∗(tan((((x1)+((-8.368002)))-(psqroot((x1)∗(x2))))+(((x1)+((6.070678)))∗(tanh(x1)))))In this work, a modified form of MGGP method i.e. M-MGGP method is proposed. In MGGP method, during the combination mechanism, the genes of quality (lower or better performance on training data) are regressed using least squares method so as to determine the coefficients P0, P1, P2 and P3. There is a possibility that a gene of lower performance i.e. genes having poor accuracy on training data can combine with other genes of higher performance and degrades the performance of the MGGP model. If we can thought of a possible way of eliminating genes of lower performance and only allow genes of higher performance to take part in combination mechanism, the performance of the MGGP model can improve. In this proposed method, genes are combined and regressed using a stepwise regression approach. The idea of introducing a stepwise regression approach for combining the genes and eliminating gene/s of lower quality is similar to that in the statistical field for eliminating redundant decision variables and multi-collinearity in data. The performance of each gene evolved during the evolutionary stage of M-MGGP algorithm is evaluated against the surface roughness value obtained from the turning process. Based on the difference between the output value of each gene and the surface roughness value, the hypothesis is formed on, whether the difference between them is significantly different or not. The p-value is used to define the hypothesis test. In this work, the standard p-value of 0.05 is used. If the p-value obtained is lower than 0.05, this indicates that there is enough evidence to conclude that the difference between the predicted value obtained from the gene and surface roughness is significantly different, and, following which the given gene is removed during the evolutionary stage. For example, as shown in Fig. 5, the Gene 3 with p-value lower than 0.05 is eliminated with other 2 genes get combined using the stepwise regression method.The algorithm of the proposed M-MGGP method is outlined as follows:BEGINStep 1: Define problemStep 2: M-MGGP algorithmBegin2.a Define initial algorithm parameters such as terminal set, functional set, population size, number of generations, maximum depth of gene, maximum number of genes to be combined, probability rate of genetic operators, termination criterion, etc.2.b Randomly generate initial population of genes2.c Combine genes using stepwise regression approach to form a model2.d Evaluate performance of initial population of models based on fitness function RMSE and cross-check the termination criterion2.e If termination criterion satisfied select the best model, else apply genetic operations and GOTO Step 2.dEnd;END;For fair comparison with the MGGP method, the parameter settings of the M-MGGP method are kept same as discussed in Section 3.1. The M-MGGP method is applied on the data set shown in Table 3. The best M-MGGP model (Eq. (3)) is selected based on minimum RMSE on training data and its performance is discussed in Section 4.(3)Surface roughnessM-MGGP=0.028809+(-355.1803)∗(x2)+(38.1242)∗((tanh((((x2)∗(x3))-(tan(x2)))+(((x3)∗((2.778717)))+(psqroot(x1)))))∗((tan(x2))∗((x2)+(x3))))+(0.0044733)∗(tan(tan(((((-9.418193))∗((2.778717)))-(tan(x2)))+((x3)+(psqroot(x1))))))+(-38.5227)∗((x2)∗(x3))+(0.036321)∗((tanh(psqroot(tan(x1))))+(x3))+(-0.089854)∗(tan(tan(((tanh((5.836704)))-(tan(x2)))+((((-9.418193))∗((2.778717)))+(psqroot(x1))))))+(-0.0012777)∗((tan(((tanh(x1))-(tan(x2)))+((((-9.418193))∗((2.778717)))+(psqroot(x1)))))∗(x3))+(355.3322)∗((tanh(x2))∗(tanh((x1)∗(x2))))The most famous method in field of machine learning specifically for imparting generalisation ability to the models is SVR [65,66]. SVR method is derived from the support vector machine (SVMs) which is often applied to solve classification problems. SVM when applied to regression problems is named as SVR. SVR is not based on statistical assumptions (model structure, error dependency, etc.) and formulates models based on the only given data [67].SVR is originated from the community of statistical learning, and its framework is developed on structural risk minimisation (SRM) principle. The input process parameters space is transformed into the higher dimensional space T using the non-linear hyperspace function. For e.g. the regression problem of higher non-linearity is converted to a linear regression problem in a higher dimensional space. Several non-linear/hyperspace functions exist to do the transformation/conversion.The training data{(xi,yi)}i=1N∈Rm×Ris used to formulate the SVR model, where xithe input is process variable and yiis the actual value/response of the process. In present work, there are three input variables and one output variable. The SVR model is given by(4)Y=z(x)=∑i=1Nwiρi(x)+b=wTρ(x)+bwhere the function ρi(x) is a space that has been converted into higher dimensional space, andw=[w1w2…wN]Tandρ=[ρ1ρ2…ρN]TNonlinear hyper-surface given by Eq. (4) is projected in the higher dimensional space T. The regression model ρ(x) is the transformed linear model in the higher dimensional space. Given the data in Section 2, the kernel function chosen learns and minimises the regularised risk function (Lr). The optimisation of Lrdetermined the parameters w (weight) and b (bias).(5)Lr(w)=12wTw+λ∑i=1N|yi-z(x)|ewhere|yi-z(x)|e=0,if|y1-z(x)|<ε|y1-z(x)|-ε,otherwiseThe regularisation parameter (λ) is a trade-off between the weight vector norm and the approximation error. The approximation error is decreased by increasing λ or weight vector norm, but this may not ensure the high generalisation ability of the model and may cause over-fitting. The user defines the values of λ and ɛ where ɛ is the tolerance level. The definition of ɛ-insensitive loss function (|yi−z(x)|e) is given in Eq. (5). If the predicted values of the SVR model z(x) lies within the defined tolerance level ɛ, the loss function is zero, and for the points outside ɛ, the loss function is the absolute of the difference between the values predicted by the SVR model and tolerance level ɛ. The points on the margin lines (y=z(x)±ɛ) are called support vectors, whereas those outside are known as error set (Fig. 6).For the implementation of SVR method, the kernel function plays an important role in learning the hyperspace from the training data. In this work, RBF kernel function is chosen, since it is more compact and well known for its shorter training process and imparting high generalisation ability to the model. LS-SVM tool box [68] built for MATLAB is applied on the data set shown in Table 3. Several applications of this toolbox [69,70] in the field of production have been reported. The parameters λ and σ of radial basis function (RBF) are determined using a combination of coupled simulated annealing (CSA) and a grid search method. Firstly, the CSA determines the good initial values of λ and, and then, these are passed to the grid search method which uses cross-validation to fine tune the parameters. A SVR model with optimal parameters λ=629.37 and σ=0.0317 are found at the second iteration. The performance of the SVR model is discussed in Section 4.ANN is a mathematical model based on neural networks found in nervous system of living organisms. ANNs are characterised by a set of nodes and connection between the nodes. The nodes are computational units, they receive information and process it. Connections between the nodes determine information flow. The whole interaction of nodes through neurons represents the global behaviour which is different from the behaviour of the individual nodes. This makes the capabilities of the network of the network to be greater than that of its elements.A node consists of inputs multiplied by weights (strength of received signals). The input is the number of input process parameters. In present study, number of inputs is three. Therefore, there are three neurons in the input layer. A mathematical function computes the output which can be sent to other nodes through the connections. ANN combines a large number of nodes in the fashion to process information. Strength of the input signals can be manipulated by the weights, which can be positive or negative. These weights can be adjusted to produce desired behaviour from the ANN. The process of adjusting the weights is called training or learning.Artificial neurons can be arranged from input to output in one or many layers. Weighted sum of the input receives from the previous layer by the qth neuron is denoted by (network)qand given by(6)(network)q=∑pp=1nWtpqxp+swhere Wtpqis the weight between the pth neuron in the previous layer to the qth neuron, and xiis the output of the pth neuron in previous layer, and s is a fixed value. The output of the qth neuron denoted by Outputqcan be computed with a sigmoid activation function as(7)Outputq=f(network)q=11+e-α(network)qwhere α is a constant used to control the slope of the semi-linear region.Back Propagation (BP) algorithm is used to train the network. The weights are updated in such a way that the error (absolute of difference between output value of network and actual value) becomes minimum. The computation of weights is iterative and consumes time. The Levenberg–Marquardt algorithm that works on the principle of the second derivative is used to update weights [71]. The simpler form of Hessian matrix is used and the algorithm iterates weights using formulae:(8)xk+1=xk-[JTJ+μI]-1JTewhere J is the Jacobian matrix that consists of the first derivatives of the network errors, e is a vector of network errors, μ is the learning rate and I is the identity matrix.In this work, three-layer feed forward neural network is used with BP learning and implemented in MATLAB R2010b. The parameter settings used for ANN is shown in Table 5. The optimal network of ANN is determined based on the minimum value of RMSE of the model on the training data set. Fig. 7shows that for the number of neurons nine, the value of RMSE is minimum and so therefore the network of 3–9–1 (Fig. 8) for the ANN model is selected. The performance of the selected ANN model is discussed in Section 4.The results obtained from the three modelling methods, MGGP, M-MGGP and ANN, are illustrated in Figs. 9–12on the training and testing data respectively. The best prediction method that gives good generalization ability is determined by comparing these modelling methods using the six metrics: the square of the correlation coefficient (R2), the mean absolute percentage error (MAPE), the RMSE, relative error (%) and multi-objective error functions (MO1 and MO2) [72] given by:(9)R2=∑i=1nAi-Ai‾Mi-Mi‾∑i=1nAi-Ai‾2∑i=1nMi-Mi‾22(10)MAPE(%)=1n∑iAi-MiAi×100(11)RMSE=∑i=1N|Mi-Ai|2N(12)Relativeerror(%)=|Mi-Ai|Ai×100(13)Multiobjectiveerror(MO1)=MAPE+RMSER2(14)Multiobjectiveerror(MO2)=RRMSE(1+R)where Miand Aiare predicted and actual values respectively,M‾iandA‾iare the average values of predicted and actual respectively and n is the number of training samples. Two multi-objective error functions are considered because R will not change significantly by shifting the output values of a model equally, and error functions (e.g. RMSE and MAE) only shows the error not correlation. Therefore, the criteria should be combination of R, RMSE and/or MAE [72].The result of training phase shown in Figs. 9a–12a indicates that the four models have impressively learned the non-linear relationship between surface roughness and the input variables with high correlation values and relatively low error values. The result of the testing phase shown in Figs. 9b–12b indicates that the proposed M-MGGP model outperformed the standardized MGGP, SVR and ANN, and, the predictions obtained are in good agreement with the experimental data, with achieved values of R2 as high as 0.97. Between MGGP and ANN, ANN has shown better performance. Among MGGP, SVR and ANN, ANN has shown better performance. Between SVR and MGGP, MGGP have shown better performance.MO values for the four models are computed on the training and testing data as shown in Table 6. It shows that the proposed M-MGGP model outperforms the standardized MGGP, SVR and ANN. The descriptive statistics of the relative errors of the four models are shown in Table 7, which illustrates error mean, standard deviation (Std dev), Standard error of mean (SE mean), lower confidence interval (LCI) of mean at 95%, upper confidence interval (UCI) of mean at 95%, median, maximum and minimum. The lower value of range (UCI–LCI) of the confidence intervals of the M-MGGP model indicated that its performance is better than those of the other three models.Goodness of fit of the four models is computed based on the hypothesis tests and shown in Table 8. These are t-tests to determine the mean and f-tests for variance. For the t-tests and the f-tests, the p-values of all the three models is >0.05, so there is not enough evidence to conclude that the actual values and predicted values from these models differ. Therefore, the four models have statistically satisfactory goodness of fit from the modelling point of view.In addition to the comparison of MGGP and M-MGGP based on the generalization criterion, the complexity of the best model for MGGP and M-MGGP methods evolved in each run is measured by estimating the number of nodes of the model. The bar graph (see Figs. 13 and 14) plotted for the 20 best models (1 model evolved in each run based on minimum RMSE) shows that the models evolved by the M-MGGP method possesses lower complexity when compared to the MGGP models.Thus from the statistical comparison presented, it can be concluded that the proposed M-MGGP model has outperformed the standardized MGGP, SVR and ANN, and, is able to capture the dynamics of the turning process in the evaluation of surface roughness of AISI H11 steel.For the validation of the robustness of the M-MGGP model, sensitivity and parametric analysis about the mean is conducted. The sensitivity analysis (SA) percentage of the surface roughness to each input parameter is determined using the following formulas [73]:(15)Li=fmax(xi)-fmin(xi)(16)SAi=Li∑j=1nLj×100wherefmax(xi)andfmin(xi)are, respectively, the maximum and minimum of the predicted output over the ith input domain, where other variables are equal to their mean values.Table 9shows the sensitivity results of input variables in the prediction of surface roughness. From Table 9, it is clear that the process input variables, namely the cutting speed, has the highest impact on the surface roughness followed by the feed rate and cutting time. This reveals that regulating cutting speed generates the greatest amount of variation in surface roughness. The parametric analysis provides a measure of the relative importance among the inputs of the experimental data and illustrates how the surface roughness values vary in response to the variation in input variables. For this reason, the first input is varied between its mean±definite number of standard deviations, and the network outputs are computed, while the other inputs are fixed at their mean values. This analysis is then repeated for other inputs. Fig. 15displays the plots generated for each input variable and the surface roughness values over the range of the input variables. These plots reveal that, for example, the surface roughness increases non-linearly with an increase in feed rate, behaves non-linearly with an increase in cutting speed and remain almost same, with an increase in time.From these Table 9 and Fig. 15, we can then select the optimal values of the input variables, which then optimise the surface roughness. In this way, our proposed M-MGGP model can be used to reveals insights on the turning phenomenon of AISI H11 Steel by unveiling hidden non-linear relationships and dominant input parameters.

@&#CONCLUSIONS@&#
We demonstrated the use of experimental and soft computing methods for studying the mechanism of turning of AISI H11 Steel. The present work addresses issues in procedure of formulation of the MGGP model. To counter this, M-MGGP methodology is proposed. The performance of the M-MGGP model is found to be better than those of the other three models. In addition, the M-MGGP method evolves models of lower complexity when compared to the models evolved from MGGP method. This concludes that we have successfully formulated a robust form of MGGP that evolves compact and accurate models. Further, the parametric and sensitivity analysis conducted reveals insights on dominant process parameters and hidden relationships of the process.The high generalisation ability of the M-MGGP model is beneficial for production experts, who are currently looking for high fidelity models that predict the turning process in uncertain input process conditions, and therefore the cost of having to run additional experiments can be avoided. The M-MGGP model provides a functional expression (Eq. (3)) between surface roughness and the input process parameters, and thus can be used offline to predict the surface roughness. This model can also be further optimised and the optimal turning process parameter settings can be estimated to minimize the surface roughness. In this way, the desired surface roughness can be achieved by carrying out experiments based on the obtained input turning process parameter settings, and thus resulting in an increase in productivity. Although, the implementation of M-MGGP method is time consuming, the same can be compensated by higher accuracy of the model on testing data.