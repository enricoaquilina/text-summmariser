@&#MAIN-TITLE@&#
Standard Stochastic Dominance

@&#HIGHLIGHTS@&#
We propose a Stochastic Dominance criterion based on ‘standard risk aversion’.We develop linear systems of optimality conditions to implement ‘StSD’.Our analysis covers both discrete and polyhedral choice sets.We apply the method to various empirical asset pricing data sets.Accounting for standardness affects the abnormal returns of small-loser stocks.

@&#KEYPHRASES@&#
Decision theory,Stochastic Dominance,Standard risk aversion,Portfolio theory,Linear Programming,

@&#ABSTRACT@&#
We propose a new Stochastic Dominance (SD) criterion based on standard risk aversion, which assumes decreasing absolute risk aversion and decreasing absolute prudence. To implement the proposed criterion, we develop linear systems of optimality conditions for a given prospect relative to a discrete or polyhedral choice opportunity set in a general state-space model. An empirical application to historical stock market data shows that small-loser stocks are more appealing to standard risk averters than the existing mean-variance (MV) and higher-order SD criteria suggest, due to their upside potential. Depending on the assumed trading strategy and evaluation horizon, accounting for standardness increases the estimated abnormal returns of these stocks by about 50 to 200 basis points per annum relative to MV and higher-order SD criteria. An analysis of the MV tangency portfolio shows that the opportunity cost of the MV approximation to direct utility maximization can be substantial.

@&#INTRODUCTION@&#
The choice criteria of Stochastic Dominance (SD) give a partial stochastic ordering of prospects based on general regularity conditions for decision making under risk; see Bawa (1975) and Levy (2006). Recent applications of SD in OR/MS include Dupacová and Kopa (2014), Meskarian, Xu, and Fliege (2012), Podinovski (2014), Post and Kopa (2013), Roman, Mitra, and Zverovich (2013) and Hu, Homem-de Mello, and Mehrotra (2014). The most common regularity conditions are non-satiation and risk aversion, but additional assumptions about higher-order risk aversion are often required to exclude pathological preference structures and achieve an acceptable level of discriminating power. For example, third-order SD (TSD) imposes the additional assumption of prudence (or skewness love; see Menezes, Geiss, & Tressler, 1980) and fourth-order SD (FOSD) adds temperance (or kurtosis aversion).This study develops and implements a new SD criterion based on the notion of standard risk aversion of Kimball (1993). We refer to the resulting choice criterion as ‘Standard Stochastic Dominance’ (StSD). We will derive a system of linear equations for testing StSD relations in the spirit of the revealed preference approach of Samuelson (1938) and Afriat (1967). Linear Programming (LP) can identify feasible solutions or quantify possible violations of the linear system. Using this approach, we will show that imposing standardness helps to improve the economic meaning and discriminating power of SD tests in an application to investment portfolio construction.We develop StSD with an eye to financial decision support and equilibrium theory. Experimental economists have convincingly shown that real people deviate in systematic ways from Expected Utility Theory (EUT). Non-Expected Utility theories such as the Cumulative Prospect Theory describe behavioral patterns such as loss aversion and probability distortion. The utility functions that we examine in this study are generally not consistent with these theories and patterns. Nevertheless, EUT remains the standard for normative decision support in areas such as security selection and asset allocation. In addition, EUT is important in asset pricing theory, because the forces of competition and arbitrage mitigate the impact of behavioral errors by individual investors on market prices in equilibrium.Standard risk aversion combines decreasing absolute risk aversion (DARA) and decreasing absolute prudence (DAP). These higher-order risk aversion properties are motivated by certain stylized facts about the relation between wealth, risk exposure and asset allocation. Specifically, standardness ensures that introducing a zero-mean background risk makes people less willing to accept another independent risk and, in addition, an increase in the risk level of an asset reduces the demand for that asset.Despite the appeal of DARA and DAP, the common Nth order SD (NSD) rules do not impose these regularity conditions. As a case in point, SD of any order allows for a quadratic utility function (provided that the bliss point lies outside the relevant range of outcomes), despite the well-known undesirable properties of the quadratic form. The NSD rules therefore seem too permissive for analyzing prospects with a skewed outcome distribution. For example, Basso and Pianca (1997) show that NSD allows for financial option prices that are inconsistent with DARA and Post, Fang, and Kopa (2015) show that NSD underestimates the pricing errors to small-cap stocks for DARA investors. Our analysis pays special attention to the distinction between StSD and FOSD.Following Kuosmanen (2004), Post (2003), Roman, Darby-Dowman, and Mitra (2006), Shalit and Yitzhaki (1994) and Scaillet and Topaloglou (2010), we focus on a choice opportunity set of all convex combinations of a finite number of prospects—in our application, financial securities, and, in other areas, for example, agricultural cropping programs or health care treatments. The Appendix outlines how to derive solutions for the simpler case of a discrete choice opportunity set of a finite number of prospects, including the simplest case of pairwise comparison of two given prospects.Rather than imposing a particular shape for the joint probability distribution, we focus on a general state-space model. To arrive at finite optimization problems, we assume a finite number of states. This assumption is not restrictive because empirical studies generally use discrete sample distributions and experimental studies generally use a discrete population distribution. In addition, many continuous population distributions can be approximated accurately using a finite number of random draws from the population distribution.The empirical part of our study applies the proposed StSD rule and several other decision criteria to analyze the efficiency of a broad stock market portfolio using historical investment return data. The application can be viewed as an empirical test for a rational representative-investor model of capital market equilibrium or, alternatively, as a revealed-preference test for individual investors who adopt a passive strategy of broad diversification. Empirical findings about which market segments and active strategies outperform a passive strategy may also be useful for active money managers. Apart from analyzing a market portfolio, we also use our StSD efficiency test to examine the mean-variance (MV) efficient tangency portfolio in order to estimate the opportunity cost of the MV approximation to direct utility maximization.We consider M distinct prospects with risky outcomes,X1,…,XM∈R++:=(0,+∞), that are treated as random variables with a state-dependent, joint probability distribution characterized by R mutually exclusive and exhaustive scenarios with probabilities pr> 0 and realizationsx1,r,…,xM,r,r=1,⋯,R. LetX0:={X1,…,XM}. We assume that convex combinations of the individual prospects are feasible and the choice opportunity set is given by the convex hullX:=Conv(X0)={∑j=1MλjXj:∑j=1Mλj=1;λj≥0,j=1,…,M}. The Appendix analyzes the case in which mixtures are not allowed.The M prospects may be mixtures of more basic choice alternatives. To allow for general linear restrictions, the mixtures may represent the vertices of a general polyhedral choice opportunity set. To represent or approximate dynamic intertemporal choice problems, the prospects could be dynamic mixtures, such as actively managed investment portfolios with mixing weights that depend on a conditioning information set.We useY∈Xfor the evaluated prospect and yr,r=1,…,R, for its realizations. The ordering of the scenarios is inconsequential for our analysis and we are free to label the scenarios by their ranking with respect to the evaluated prospect: y1 ≤ ⋅⋅⋅ ≤ yR. In practice, this means that the scenarios may have to be re-ordered if we change the scenarios (for example, in statistical resampling procedures) or the evaluated prospect. The ranked outcomes {y1, ⋅⋅⋅, yR} represent a partition of the interval [y1, yR]. We will use ymfor the median outcome, that is,m:minr{r:P(Y≤yr)≥0.5}.Decision makers’ preferences are described by four times continuously differentiable utility functionsu(x)∈C4(0,∞). We denote the absolute risk aversion (ARA) quotient and the absolute prudence (AP) quotient by(1.1)au(x):=−(u′′(x)u′(x));(1.2)pu(x):−(u′′′(x)u′′(x)).The StSD utility functions follow:(2.1)U4*:={u∈U4:u′(x)>0;u′′(x)<0;a′u(x)≤0;p′u(x)≤0∀x∈R++};(2.2)U4:={u∈C4:u′(x)≥0;u′′(x)≤0;u′′′(x)≥0;u′′′′(x)≤0∀x∈R++}.The StSD functionsU4*are a subset of the (FOSD) functionsU4. The restrictions on the signs of the derivatives have the economic meaning of non-satiation(u′(x) ≥ 0), risk aversion (u″(x) ≤ 0), prudence (u‴(x) ≥ 0 ) and temperance (u‴′(x) ≤ 0).Standardness requires the additional restrictions of DARA (au′(x)≤0) and DAP (pu′(x)≤0). A behavioral characterization of standardness is that bearing one risk makes the decision maker less willing to bear another risk, even when the two risks are independent. We will further elaborate on the distinction between FOSD and StSD after analyzing the properties ofU4*below.Fig. 1illustrates the relevance of imposing DAP in addition to DARA. The figure shows the level u(x), slope u′(x), curvature−u′′(x)and ARA quotient au(x) for a two-piece exponential function. Although the function obeys non-satiation, risk aversion and DARA, it does not obey DAP due to a discontinuous drop of the curvature and ARA quotient atx=1. This function is thus not included inU4*.It is well-known that global DAP implies global DARA. However, our analysis imposes DAP for the rangeD:=[y1,yR]but not for its complementDC:=(0,y1)∪(yR,+∞). Since local DAP does not imply local DARA for the same range, we explicitly impose DAP in addition to DARA. Standardness also requires strict monotonicity (u′(x) > 0) and strict risk aversion (u″(x) < 0), because u′(x) and u″(x) enter as the divisor in au(x) and pu(x), respectively.Importantly, the above regularity conditions are robust to aggregation across individual utility functions, for example, to the level of a family or an economy, andU4*is a convex set. Convexity ensures that an aggregate decision maker has the same general risk aversion properties as the individual decision makers. In addition, convexity allows for representing all admissible utility functions as mixtures of basic utility functions that form the extreme rays ofU4*.We can reformulate StSD in terms of the functions(3.1)Au(x):=ln(u′(x));(3.2)Pu(x):=ln(−u′′(x)),which are the negative anti-derivatives of the ARA and AP quotients, respectively:−Au′(x)=au(x)and−Pu′(x)=pu(x).Lemma 1ARA-AP linkagesThe conditions of DARA and DAP have various equivalent formulations in terms of the ARA and AP quotients and their anti-derivatives:(4.1)au′(x)≤0⇔Au′′(x)≥0;(4.2)au′(x)≤0⇔pu(x)≥au(x);(4.3)pu′(x)≤0⇔Pu′′(x)≥0.These results allow us to characterize the differences between FOSD and StSD using higher-order moment risk. Prudence is equivalent to convexity of u′(x), or u′(E[x]) ≤ E[u′(x)] for every possible prospect. Similarly, temperance amounts to convexity of−u′′(x), or−u′′(E[x])≤E[−u′′(x)]. These conditions allow for quadratic utility, in which case we findu′(E[x])=E[u′(x)]and−u′′(E[x])=E[−u′′(x)]and indifference to skewness and kurtosis.Using (3.1) and (4.1), DARA amounts to log-convexity of u′(x), or ln(u′(E[x])) ≤ E[ln(u′(x))]. Similarly, using (3.2) and (4.3), DAP boils down to log-convexity of−u′′(x), orln(−u′′(E[x]))≤E[ln(−u′′(x))]. These conditions imply u′(E[x]) < E[u′(x)] and−u′′(E[x])<E[−u′′(x)]. StSD thus requires strict skewness love and strict kurtosis aversion. In addition, skewness love must strictly increase with risk aversion.Proposition 1Exponentiation of anti-derivativesThe StSD functionsU4*are those FOSD functionsu∈U4for which the anti-derivatives of the ARA and AP quotients belong to the SSD functionsU2:(5.1)U4*={u∈U4:u′(x)=exp(Au(x));u′′(x)=−exp(Pu(x));−Au,−Pu∈U2};(5.2)U2:={u∈C2:u′(x)≥0;u′′(x)≤0∀x∈R++}.ProofClearly, (5.1) obeys all properties of (2.1):(i)u′(x)=exp(Au(x))>0;u′′(x)=−exp(Pu(x))<0;−Au′′(x)≤0⇔au′(x)≤0;−Pu′′(x)≤0⇔pu′(x)≤0.u′(x)=exp(ln(u′(x)))=exp(Au(x));u′′(x)=−exp(ln(−u′′(x)))=−exp(Pu(x));(u′(x)>0)∧(u′′(x)<0)⇒0≤au(x)=−Au′(x);au′(x)≤0⇔−Au′(x)≤0;(u′′(x)<0)∧(a′u(x)≤0⇒u′′′(x)>0)⇒0≤pu(x)=−Pu′(x);pu′(x)≤0⇔−Pu′(x)≤0.□Proposition 2ConvexityThe standard utility functionsU4*form a convex cone in the function space:u,v∈U4*⇒au+bv∈U4*∀a,b≥0:a+b>0.ProofClearly,u,v∈U4*⇒au,bv∈U4*∀a,b>0, and therefore we need to prove onlyU4*∋u+v=:w. It also seems obvious that  w is strictly increasing and−w′′is positive and decreasing as required, and what remains to be proven is that−w′′it is log-convex. Using (3), this condition is equivalent to convexity ofPw, orPw(y)≥Pw(x)−pw(x)(y−x)for allx,y∈R++.Our strategy is to derive this inequality from the properties of u and v. Since−u′′and−v′′are log-convex,PuandPvare convex, or{Pu(y)≥Pu(x)−pu(x)(y−x)Pv(y)≥Pv(x)−pv(x)(y−x).Exponentiating both sides of both equations gives{−u′′(y)≥−u′′(x)exp(−pu(x)(y−x))−v′′(y)≥−v′′(x)exp(−pv(x)(y−x)).Adding the two equations and dividing both sides by−w′′(x)gives−w′′(y)−w′′(x)≥(−u′′(x)−w′′(x))exp(−pu(x)(y−x))+(−v′′(x)−w′′(x))exp(−pv(x)(y−x)).The LHS is a convex combination of two exponential functions. Taking the logarithm of both sides, and using the strict concavity of the logarithmic function, yieldsln(−w′′(y)−w′′(x))≥ln((−u′′(x)−w′′(x))exp(−pu(x)(y−x))+(−v′′(x)−w′′(x))exp(−pv(x)(y−x)))>(−u′′(x)−w′′(x))ln(exp(−pu(x)(y−x)))+(−v′′(x)−w′′(x))ln(exp(−pv(x)(y−x)))⇔ln(−w′′(y))−ln(−w′′(x))>(−pu(x)u′′(x)−pv(x)v′′(x)−w′′(x))(y−x)⇔Pw(y)>Pw(x)−pw(x)(y−x).□Definition 1StSD efficiencyA given prospectY∈X, is StSD efficient if it is the optimum for some admissible utility functionu∈U4*, orY=argmaxX∈X∑r=1Rpru(xr).Lemma 2KKT conditionsA given prospectY∈Xis StSD efficient if and only if it obeys the Karush–Kuhn–Tucker first-order optimality conditions for some admissible utility functionu∈U4*:(6)∑r=1Rpru′(yr)(yr−xj,r)≥0,j=1,…,M.Although the StSD functions are of infinite dimension, the optimality conditions (6) evaluate marginal utility only at the discrete outcome levelsy1,…,yR, allowing for a formulation of finite dimension. We can use a piecewise-linear (PWL) representation ofPu(x)(or a piecewise-constant AP quotient  pu(x)), the details of which are given in the next section. Repeated integration by parts ofu′′(x)=−exp(Pu(x))yields a composite utility functionu(x)=u1(x)+u2(x) that consists of a piecewise-exponential componentu1(x)=−pu(x)−2exp(Pu(x))and a PWL component  u2(x) that ensures continuity of u′(x) and u(x). Unfortunately, optimization over such functions is a non-linear and non-convex problem, which makes this approach unpractical. The next section therefore introduces a useful linearization.Our strategy is to linearize the conditions−Au∈U2,−Pu∈U2,u∈U4and the exponentiationsu′(x)=exp(Au(x))andu′′(x)=−exp(Pu(x)). We use an exact linearization for−Au∈U2,−Pu∈U2, andu∈U4using particular Riemann sums, and a (tight) local PWL approximation for the exponential function. We will also use a local linear approximation to the logarithmic function in order to normalize the levels of log marginal utility.Proposition 3Linearized log derivativesFor any utility functionu∈U4*, we can representAu(yr)andPu(yr),r=1,…,R, using decreasing and convex PWL functions that are linear in a finite number of parameters and obey a set of linear restrictions:(7.1)Au(yr)=∑k=rR−1αk(yk+1−yr)+αR,r=1,…,R;(7.2)αr≥0,r=1,…,R−1;(8.1)Pu(yr)=∑k=rR−1πk(yk+1−yr)+πR,r=1,…,R;(8.2)∑k=rR−1πk≥∑k=r+1R−1αk,r=1,…,R−2;(8.3)πr≥0,r=1,…,R−1.ProofThe mean-value theorem for integration implies that the values of the anti-derivative can be represented using a Riemann sum:Au(yr)=∑k=rR−1au(zk(a))(yk+1−yk)+ln(u′(yR)),1,…,R, where the sampling pointszr(a)∈[yr,yr+1]are selected such thatau(zr(a))=ln(u′(yr)/u′(yr+1))/(yr+1−yr),r=1,…,R−1. We can further decompose the ARA coefficients asau(zr(a))=∑k=rR−1αk, using the ARA decrementsαr=au(zr(a))−au(zr+1(a)),r=1,…,R−2, and the ARA levelαR−1=au(zR−1(a)). Consequently, the values of the anti-derivative can be built via summation by parts using (7.1), whereαR=ln(u′(yR)). The non-negativity conditions (7.2) reflect risk aversion(αR−1≥0)and DARA(αr≥0,r=1,…,R−2). Similarly, the levels ofPu(yr)in (8.1) are built from the AP decrementsπr=pu(zr(p))−pu(zr+1(p)),r=1,…,R−2, and the ARA levelπR−1=pu(zR−1(p)), for sampling pointszr(p)∈[yr,yr+1]such thatzr(p)=ln(u′′(yr)/u′′(yr+1))/(yr+1−yr),r=1,…,R−1, andπR=ln(−u′′(yR)). Inequalities (8.2) follow from (4.2),au(zr+1(a))=∑k=r+1R−1αk,pu(zr(p))=∑k=rR−1πk, andzr+1(a)≥zr(p). The non-negativity conditions (8.3) reflect prudence(πR−1≥0)and DAP(πr≥0,r=1,…,R−2). □Proposition 4Linearized derivativesFor any utility functionu∈U4,we can represent u′(yr) and−u′′(yr),r=1,⋯,R, using decreasing and convex PWL functions that are linear in a finite number of parameters and obey a set of linear restrictions:(9.1)u′(yr)=∑k=rR−1βk(yk+1−yr)+βR,r=1,…,R;(9.2)βr≥0,r=1,…,R;(10.1)−u′′(yr)=∑k=rR−1γk(yk+1−yr)+γR,r=1,…,R;(10.2)∑k=rR−1γk(yk+1−yr)+γR≤∑k=r−1R−1βr,r=2,…,R−1;(10.3)∑k=rR−1γk(yk+1−yr)+γR≥∑k=r+1R−1βr,r=1,…,R−2;(10.4)γr≥0,r=1,…,R.ProofBy analogy to Proposition 3, the derivatives u′(yr) in (9.1) are built via summation by parts from the increments of the second-order derivativeβr=u′′(zr+1(2))−u′′(zr(2)),r=1,…,R−2,βR−1=−u′′(zR−1(2)), for sampling pointszr(2)∈[yr,yr+1]such thatu′′(zr(2))=(u′(yr+1)−u′(yr))/(yr+1−yr),r=1,…,R−1,andβR=u′(yR). The non-negativity conditions (9.2) reflect non-satiation (βR≥ 0), risk aversion(βR−1≥0), and prudence (βr≥0r=1,…,R−2). Similarly, the second-order derivatives u″(yr) in (10.1) are built from the decrements of the third-order derivativeγr=u′′′(zr(3))−u′′′(zr+1(3)),r=1,…,R−2,γR−1=u′′′(zR−1(3)), for sampling pointszr(3)∈[yr,yr+1]such thatu′′′(zr(3))=(u′′(yr+1)−u′′(yr))/(yr+1−yr),r=1,…,R−1,andγR=−u′′(yR). Inequalities (10.2) follow from u‴(x) ≥ 0,−u′′(zr−1(2))=∑k=r−1R−1βr, andzr−1(2)≤yr; similarly, (10.3) follow from u‴(x) ≥ 0,−u′′(zr(2))=∑k=rR−1βr, andyr≤zr(2). The non-negativity conditions (10.4) follow from risk aversion (γR≥ 0), prudence(γR−1≥0), and temperance (γr≥0r=1,…,R−2). □Propositions 3 and 4 give an exact linearization rather than an approximation. When using fixed sampling points (for example, the midpointszr=0.5(yr+yr+1),r=1,…,R−1), a Riemann sum gives an approximation for the definite integral in question, the accuracy of which generally depends on the coarseness of the partition. By contrast, our sampling pointszr=zr(a),zr(p),zr(2),zr(3),r=1,…,R−1,are derived endogenously as the points where the tangent ofh(y)=Au(y),Pu(y), u′(y), u″(y) is parallel to the secant that joins (yr, h(yr)) and(yr+1,h(yr+1)). The mean-value theorem implies that these tangency points exist and that the Riemann sum equals the definite integral for this specification regardless of the coarseness of the partition.Proposition 4 deviates in an important way from Theorem 1 in Post and Kopa (2013). For FOSD, they model u′(yr) (u″(yr)) with a piecewise-quadratic (PWL) function that is obtained by integrating over a piecewise-constant third-order derivative u‴(yr). This approach would be inconsistent with standardness, because a piecewise-quadratic function implies locally increasing ARA and AP. For this reason, we use two separate PWL functions, based on two different sets of sampling points,zr(2)for u′(yr) andzr(3)for u″(yr), rather than a single piecewise quadratic function. The two approaches converge as the partition is refined ((wr+1−wr)→0), but only our formulation allows for a coarse partition.We have now derived two sets of linear conditions: (7) and (8) expressAu(yr)andPu(yr)as linear functions of the levels and differences of the ARA and AP quotients; (9) and (10) express u′(yr) and u″(yr) as linear functions of the levels and differences of the second-order and third-order derivatives. The relation between these sets of restrictions is non-linear:u′(yr)=exp(Au(yr))andu′′(yr)=−exp(Pu(yr)). We will develop a set of linear conditions based on a (tight) local PWL approximation to the exponentiation.Letfs∈U4*,s=1,…,S, be ‘frame functions’, or prior parametric utility functions that are fitted to the decision problem in question. Since marginal utility u′(y) is decreasing, it seems natural to assume the following minimal goodness criterion:(11)u′(ym)≥maxs=1,…,Sminr=1,…,Rfs′(yr)=maxs=1,…,Sfs′(yR).Our default choice isS=4CRRA power functionsfs(x):=ηs1−θsx1−θs,s=1,…,4,with risk aversion coefficientsθ1=0.5,θ2=1,θ3=2andθ4=4(a plausible range in many applications), and η1, η2, η3, and η4 selected to obtain the desired normalization (see below). Additional or alternative frame functions may be used depending on the application in question.Each frame function generates a set of plausible starting values for the derivatives, fs′(yr) and fs″(yr),r=1,…,R,s=1,⋯,S. We may apply a local first-order Taylor series approximation of the exponential functiong(x):=exp(x)at pointAu(yr)around pointAfs(yr),and a similar approximation at pointPu(yr)around pointPfs(yr):Lemma 3ExponentiationFor any utility functionu∈U4*and set of frame functionsfs∈U4*,s=1,…,S,(12.1)u′(yr)=g(Au(yr))≥fs′(yr)(1+Au(yr)−Afs(yr)),r=1,…,R;(12.2)−u′′(yr)=g(Pu(yr))≥−fs′′(yr)(1+Pu(yr)−Pfs(yr)),r=1,…,R.ProofSince g(x) is convex andg(x)=g′(x), we findg(x2)=g(x1)+g′(x1)(x2−x1)=g(x1)(1+x2−x1). Settingx1=Afs(yr)andx2=Au(yr)gives (12.1);x1=Pfs(yr)andx2=Pu(yr)gives (12.2). □For a given scenarior=1,…,R, the S linear inequalities (12.1) for the S frame functions define a PWL lower-envelope functionf′(yr):=maxs=1,⋯,S(fs′(yr)(1+Au(yr)−Afs(yr)))for u′(yr). We find u′(yr) ≥ f′(yr) and u′(yr) ≈ f′(yr) ifAu(yr)≈Afs(yr)for at least one of the frame functions,s=1,…,S. Similarly,−f′′(yr):=(maxs=1,⋯,S−fs′′(yr)(1+Pu(yr)−Pfs(yr)))is a lower envelope for−u′′(yr).Our frame functions are reminiscent of SD With Respect to a Function of Meyer (1977a,b) but differs from that approach in an important way. SDWRF uses given functions as exogenous lower and upper bounds for the ARA quotient au(x). By contrast, our approach uses the frame function to generate sampling points for a local PWL approximation to the exponentiation in order to build endogenous lower bounds for the values of u′(yr) and−u′′(yr),r=1,…,R, that are consistent with the values ofAu(yr)andPu(yr). StSD can however be used in combination with SDWRF.11For example, it seems appropriate in many cases to assume that the relative risk aversion (RRA) quotientru(x)=au(x)xis not smaller than 1 at the highest outcome level, or ru(yR) ≥ 1. A value of 1 seems relatively low compared with empirical estimates in many applications. In addition, Kenneth Arrow observed thatlimx→∞ru(x)>1is needed for utility to be bounded from above. To impose ru(yR) ≥ 1, one could use the restrictionαR−1≥1/yR, based onr(yR)=au(yR)yRandαR−1=au(zR−1(a))≥au(yR)(due to DARA andzR−1(a)≤yR).To avoid numerical instability, we normalize utility such that marginal utility has an average value of unity, or(13.1)1=∑r=1Rpru′(yr):=u′(Y)¯.We deviate from the median-based normalization of Post et al. (2015) (Eq. (8.3)) in order to allow for a better comparison across different choice criteria (for a given data set) and to follow the convention in the asset pricing literature (our application area). The median-based normalization inflates the average level of marginal utility and common inefficiency measures if the marginal utility function is skewed (assigns a relatively high value to the worst outcomes). Hence, this normalization can induce a bias toward utility functions with a relatively low AP, such as the negative exponential (pu(x)=au(x)). By contrast, our mean-based normalization allows for a skewed marginal utility function without inflating the levels of marginal utility and inefficiency. This alternative normalization requires a non-trivial adjustment to the normalization of log marginal utility of Post et al. (2015) (Eq. (7.2)).Lemma 4Normalizing log marginal utilityNormalization (13.1) implies the following joint condition for marginal utility and log marginal utility:(13.2)u′(ym)−(maxs=1,…,Sfs′(yR)−1maxs=1,…,SAfs(yR))Au(ym)≤1.ProofUsing convexity of u′(x) and (11), we know thatmaxsfs′(yR)≤u′(ym)≤u′(Y)¯=1. Concavity of the logarithmic function therefore impliesAu(ym)≥λ(maxs=1,…,SAfs(yR))+(1−λ)ln(1), withλ:=u′(ym)−1maxsfs′(yR)−1. It follows thatAu(ym)≥(u′(ym)−1maxsfs′(yR)−1)(maxs=1,…,SAfs(yR))or, rearranging terms, (13.2). □Whereas (12.1) bounds u′(ym) from below using a PWL lower envelope for the exponential function exp (ln (u′(ym))), (13.2) bounds u′(ym) from above using a linear inner approximation to the logarithmic function ln(u′(ym)). Despite the wide range of outcomes above the median, or [ym,  yR], the linear approximation is tight in our applications because (i) log-convexity limits the range of marginal utility below the median, or [u′(yR), u′(ym)], and (ii) the low probability mass in the left tail limits the deviation of the mean from the median, or(u′(Y)¯)−ln(u′(ym)).We can combine Lemmas 1, 3 and 4 and Propositions 1, 3 and 4 to derive the following linear conditions for StSD efficiency:Theorem 1Conditions for StSDA necessary condition for StSD efficiency of a given prospectY∈Xis that, for any given set of frame functionsfs∈U4*,s=1,…,S, there exists a solution,αr*,πr*,βr*,γr*,r=1,…,R,to the following system of linear inequalities:(14.1)∑r=1Rpr(∑k=rR−1βk(yk+1−yr)+βR)(yr−xj,r)≥0,j=1,…,M;(14.2)∑k=rR−1βk(yk+1−yr)+βR≥fs′(yr)(1+∑k=rR−1αk(yk+1−yr)+αR−Afs(yR)),r=1,…,R,s=1,…,S;(14.3)∑k=rR−1γk(yk+1−yr)+γR≥−fs′′(yr)(1+∑k=rR−1πk(yk+1−yr)+πR−Pfs(yR)),r=1,…,R,s=1,…,S;(14.4)∑r=1Rpr(∑k=rR−1βk(yk+1−yr)+βR)=1;(14.5)(∑k=mR−1βk(yk+1−ym)+βR)−(maxs=1,…,Sfs′(yR)−1maxs=1,…,SAfs(yR))(∑k=mR−1αk(yk+1−ym)+αR)≤1;(8.2)∑k=rR−1πk≥∑k=r+1R−1αk,r=1,…,R−2;(10.2)∑k=rR−1γk(yk+1−yr)+γR≤∑k=r−1R−1βr,r=2,…,R−1;(10.3)∑k=rR−1γk(yk+1−yr)+γR≥∑k=r+1R−1βr,r=1,…,R−2;(7.2)αr≥0,r=1,…,R−1;(8.3)πr≥0,r=1,…,R−1;(9.2)βr≥0,r=1,…,R;(10.4)γr≥0,r=1,…,R.ProofRestrictions (14.1) follow from (6) and (9.1, 14.2) from (12.1, 9.1) and (7.1, 14.3) from (12.2, 10.1) and (8.1, 14.4) from (13.1) and (9.1, 14.5) from (13.2, 9.1) and (7.1). To prove the necessary condition, suppose that the evaluated prospect is optimal foru∈U4*, and hence also for the normalized functionv=u/u′(Y)¯∈U4*. A solution to the linear system then isαr=av(zr(a))−av(zr+1(a)),r=1,…,R−2,αR−1=av(zR−1(a)), andαR=ln(v′(yR));πr=pv(zr(p))−pv(zr+1(p)),r=1,…,R−2,πR−1=pv(zR−1(p)), andπR=ln(−v′′(yR));βr=v′′(zr+1(2))−v′′(zr(2)),r=1,…,R−2,βR−1=−v′′(zR−1(2)), andβR=v′(yR);γr=v′′′(zr(3))−v′′′(zr+1(3)),r=1,…,R−2,γR−1=v′′′(zR−1(3)), andγR=−v′′(yR). □The linear system gives a necessary but not sufficient condition. The system cannot falsely classify an efficient prospect as inefficient but it may falsely classify an inefficient prospect as efficient. Some feasible solutions may not obey all regularity conditions, because the four sets of sampling pointszr=zr(a),zr(p),zr(2),zr(3),r=1,…,R−1, may be different and, in addition, the numerical approximations to the exponential and logarithmic functions in (14.2, 14.3) and (14.5) may be imperfect.The use of a necessary condition in Theorem 1 is consistent with the general hierarchical structure of the SD rules. Non-dominance by lower-order SD rules (FSD and SSD) is a necessary condition for non-dominance by higher-order rules (TSD, FOSD), DSD and StSD; and the latter is a necessary condition for optimality for any given admissible utility function.The strength of the necessary condition increases with the accuracy of the partition{y1,…,yR}and with the number of frame functions,fs∈U4*,s=1,…,S. As the partition is refined ((yr+1−yr)→0), the four sets of sampling points converge, aszr∈[yr,yr+1]for allzr=zr(a),zr(p),zr(2),zr(3),r=1,…,R−1.Similarly, the approximation error for the exponential and logarithmic functions shrink as we increase the number of frame functions,fs∈U4*,s=1,…,S.We can diagnose solutions a posteriori, that is, after testing the necessary condition, to detect possible false efficiency classification and determine the need for refinements. If the optimal marginal utility levelsu′(yr)=∑k=rR−1βk*(yk+1−yr)+βR*,r=1,…,R,and curvature levels−u′′(yr)=∑k=rR−1γk*(yk+1−yr)+γR*,r=1,⋯,R,do not show a log-convex pattern, then an approximation error must have occurred. It is straightforward to check for log-convexity, either by visual inspection of a log-level plot or with a small linear program.The linear system is relatively small withO(R)variables and constraints. We can test the linear system using LP. The specific formulation of the program would of course depend on the specific application area and decision problem. Our empirical section will develop a linear program for testing StSD efficiency of a stock market portfolio relative to all portfolios formed from a set of base assets.The market portfolio in our analysis is a value-weighted average of all non-financial common stocks that are listed on the NYSE, AMEX or NASDAQ exchange and covered by the CRSP and COMPUSTAT databases. It is compared with 25 actively managed stock portfolios that are formed, and periodically rebalanced, based on the market capitalization and lagged return of individual stocks. These portfolios are of particular interest because a wealth of empirical research suggests that contrarian and trend-following strategies earn a return premium over passive investment that defies rational explanation. Our analysis also includes a riskless asset with return equal to the time-series average of the T-bill yield in the relevant sample period.Weconsider three different sets of portfolios based on the lagged return in three different estimation windows: (i) the past month (window ‘1-1’), (ii) the 11 months before the past month (window ‘2-12’), (iii) the four years before the past year (window ‘13-60’). These three data sets are designed to capture the known short-term reversal effect of Jegadeesh (1990), momentum effect of Jegadeesh and Titman (1993) and long-term reversal effect of De Bondt and Thaler (1987), respectively.We use gross value-weighted portfolio returns for the entire sample period covered by the data library of Kenneth French. We analyze evaluation horizons of one month (H=1), one quarter (H=3) and one year (H=12). The sample periods are July 1926–December 2013, 1926Q2–2013Q4 and 1927–2013 for window ‘1-1’; January 1927–December 2013, 1927Q1–2013Q4 and 1927–2013 for window ‘2-12’; January 1931–December 2013, 1931Q1–2013Q4 and 1931–2013 for window ‘13-60’. The time-series are relatively long and provide an accurate partition of the observed range of market returns.Table 1 shows descriptive statistics for the nine data sets (three estimation windows times three evaluation horizons). For the sake of brevity, we tabulate only the statistics for the five portfolios in the first size quintile, where the reversal and momentum effects are strongest. Not surprisingly, the return distribution of small-cap stocks appears far from normal, witness the high levels of skewness and kurtosis. A distinct pattern is that the return distribution of small-loser (S/L) stocks tends to have high upside potential compared with small-winner (S/W) stocks. Presumably, many S/L stocks feature high levels of financial and/or operational leverage and behave as ‘out-of-the-money’ call options after large price drops. This pattern suggests that SD criteria may diverge from the basic MV criterion.To test whether the market portfolio is StSD efficient, we design a linear program for the linear system of Theorem 1. In this application, the individual prospects are the returns to the 25 stock portfolios(X1,…,X25)and the T-bill yield (XF), and the evaluated prospect is the market portfolio return (Y).The R time-series observations are interpreted as scenarios with equal probabilitiespr=R−1,r=1,…,R. This approach amounts to analyzing the empirical joint distribution function of portfolio returns. We will use a bootstrap method to perform statistical inference regarding the latent population distribution.In this application, u′(yr) can be interpreted as a stochastic discount factor, or ‘pricing kernel’. The kernel can be transformed to a risk-neutral probability distribution that can be used for pricing derivative securities. However, the SD relations in this study are based on the physical measure rather than the risk-neutral measure. Under the risk-neutral distribution, differences in expected return between securities cannot be interpreted as rational risk premiums based on risk-averse utility functions.Recall that we normalize the average marginal utility to unity (13.1). Following another convention in the asset pricing literature, we require the pricing kernel (u′(Y)) to be consistent with the equity premium by requiring a zero pricing error for the T-bill (∑r=1Rpru′(yr)(yr−xF,r)=0). We are therefore left with the task of evaluating the pricing errors for theM=25stock portfolios.Our objective is to minimize (across all admissible utility functions) the maximum of the 25 pricing errors:(15)ξ(Y)=minu∈U4*maxj=1,…,M∑r=1Rpru′(yr)(yr−xj,r).We can implement this mini-max criterion by minimizing a model variable ξ subject to the restrictionsξ≥∑r=1Rpru′(yr)(yr−xj,r),j=1,…,M,and the linear system of Theorem 1. A value ofξ*=0is required to classify the market portfolio as efficient; ξ* > 0 implies inefficiency.Our results are robust to the specification of the objective function. We prefer the mini-max criterion because it allows for a straightforward economic interpretation of the objective as the largest abnormal return that can be achieved without leverage or short selling. In addition, the linear formulation considerably reduces the computer time for our bootstrap procedure (see below). A follow-up analysis suggests that minimizing a weighted sum of squared errors in the spirit of the Generalized Method of Moments using Quadratic Programming leads to similar results and conclusions.Apart from the StSD efficiency test, we also apply efficiency tests based on the DSD, FOSD, TSD, SSD and MV criteria. The additional SD tests drop or change certain restrictions in Theorem 1: DSD drops (14.3, 8.2, 10.2) and (10.3); FOSD drops (14.2, 14.3, 14.5) and (8.2), TSD drops (14.2, 14.3, 14.5, 8.2, 10.2) and (10.3); SSD drops (14.2, 14.3, 14.5, 8.2, 10.2) and (10.3) and replaces (9.2) with∑k=rR−1βk(yk+1−yr)+βR≥0,r=1,…,R). The MV efficiency test uses a linear marginal utility function with coefficients based on the equity premium and an average value of unity, so that the MV pricing errors amount to Jensen's alphas.For statistical inference, we will use a re-centered IID bootstrap approach that repeatedly applies the LP test to random pseudo-samples that are based on the original sample. To re-center the bootstrap process, we subtract the estimated pricing errorα^j:=R−1∑r=1Ru′(yr)(yr−xj,r)from every return observation:x^j,r:=xj,r−α^jr=1,⋯,R. We implement the bootstrap by generating 10,000 pseudo-samples of the same size as the original sample through random draws with replacement from the re-centered original sample, and test portfolio efficiency in every pseudo-sample. Finally, we compute the critical value for the original test statistic from the percentiles of the bootstrap distribution. Similar results were obtained by using an empirical-likelihood bootstrap that re-centers the bootstrap distribution by weighting the time-series observations with their empirical likelihood.Table 2 shows the test statistic, bootstrap p-value and pricing errors for the TSD, DSD and StSD criteria. The SSD results are not tabulated, because they appear not economically meaningful, as discussed below. The MV and FOSD results are also omitted, because they do not show material differences from the TSD results.Fig. 2 displays the relevant pricing kernels for the data set based on estimation window ‘1-1’ and evaluation horizonH=12. The graphs for the other eight data sets look similar and are omitted for brevity. The figure also includes the unrestricted kernel with the lowest variance among all kernels that price all assets correctly in this sample. This kernel is constructed without imposing any of the utility-based regularity conditions and it is completely fitted to the data to serve as benchmark for the restricted kernels. Clearly, it does not resemble a well-behaved marginal utility function, as it is not smooth, tends to concavity in the center of the distribution and takes negative values in the right tail.Naturally, the SSD efficiency test yields the smallest pricing errors and the highest p-values. For the estimation window ‘13-60’, the p-value falls in the range of conventional significance levels (0.01–0.10) and the violations of portfolio efficiency are only marginally significant. However, the SSD pricing kernel (in the center left panel of Fig. 2) is a far cry from a well-behaved marginal utility function. The kernel is flat for the bulk of the observations and drops abruptly to zero for large gains. This pattern is not consistent with standardness and casts doubt on the economic meaning of the SSD results. Like the unrestricted kernel, the SSD kernel seems over-fitted to the data and assigns an artificially low weight to the scenarios with the highest market returns.The TSD criterion imposes prudence and avoids concave segments of the kernel. The pricing errors increase and the p-values decrease considerably for every sample. Particularly, the S/L pricing errors increase by more than two percentage points per annum in every sample. By contrast, the errors for the S/W stocks are substantially less affected by the assumption of prudence. This explains why the test statistic for the estimation window ‘2-12’, which is based on the price drift of S/W stocks, is more robust to the decision criterion than for the estimation windows ‘1-1’ and ‘13-60’, which are based on the price reversal of S/L stocks.The TSD kernel closely resembles the linear MV kernel and shows large violations of DARA on the wide return interval in our samples. The MV/TSD kernel avoids penalizing S/L stocks for their positive skewness, but it does not reward these stocks either. Almost identical results are obtained using the FOSD criterion. This finding is not surprising, given that restrictions on the fourth-order derivative generally have a minimal effect on the shape of the utility function.The DSD criterion leads to a substantial increase in the pricing errors. Depending on the assumed trading strategy and evaluation horizon, the abnormal returns to S/L stocks are some 40–180 basis point per annum larger than for the MV/TSD/FOSD criterion. By contrast, the pricing error of S/W stocks is not materially affected by imposing DARA, leading to an increase of the error for the average small-cap stock. Although the kernel is convex, it exhibits large linear segments in violation of DARA (log-convexity). These violations reflect approximation errors in the local PWL approximation of the exponentiation. Similar deviations do not surface in Post et al. (2015) because their median-based normalization biases the optimal kernel toward a negative exponential shape.The StSD criterion imposes the additional assumption of DAP. The incremental effect of imposing DAP in addition to DARA is smaller than the effect of imposing DARA in addition to risk aversion and/or prudence. Nevertheless, imposing DAP increases the pricing errors for S/L stocks by some 10–20 basis point per annum in the typical sample. In addition, the optimal StSD kernel is better behaved than the DARA kernel. The optimal StSD kernels closely resemble the negative exponential shape that obeys all regularity conditions. We arrive at this standard shape with an (unbiased) means-based normalization and by explicitly imposing standardness.Particularly large is the increase of the pricing error of S/L stocks from 12.14 percent for the SSD criterion to 18.86 percent for the StSD criterion in the data set with estimation window ‘1-1’ and evaluation horizonH=12. The increase of 6.72 percentage points clearly shows the discriminating power of standardness. Assuming that the second-order derivative is only non-positive (SSD) leads to much weaker results than assuming that the second-order derivative is negative, non-decreasing and log-concave (StSD). Imposing standardness seems an effective remedy against over-fitting the kernel to the data.As a robustness test, we repeated our analysis after excluding the first size quintile (portfolios S/L, S/2, S/3, S/4 and S/W) and the early sub-period 1927–1962, common robustness tests in the empirical asset pricing literature. The full-sample results and conclusions are robust to these exclusions. The general momentum and reversal profits decrease by several percentage points, but the effect of imposing standardness is comparable with that in the full sample. For example, the long-term reversal effect (‘13-60’ window) becomes insignificant forH=1,3when using the SSD test, but it remains significant when standardness is assumed.The above analysis evaluates a passive market portfolio. One may expect larger differences between the various decision criteria for active portfolios that use short selling and/or derivative securities. To illustrate this point, we will now evaluate the SD efficiency of the MV tangency portfolio, or the combination of the benchmark portfolios that maximizes the Sharpe ratio without weight restrictions other than a budget constraint.The tangency portfolio plays a pivotal role in asset pricing theory and tests. In the Capital Asset Pricing Model, all investors mix the tangency portfolio with a riskless asset (Tobin separation) and the market portfolio of risky assets equals the tangency portfolio. More generally, the tangency portfolio defines the unique pricing kernel in complete markets and the minimum-variance kernel in incomplete markets. The tangency portfolio also plays a pivotal role in empirical tests to gauge the MV efficiency of a given portfolio such as Gibbons, Ross, and Shanken (1989) and measures of the goodness of a given pricing kernel such as Hansen and Jagannathan (1997).In contrast to the market portfolio, the tangency portfolio generally features short positions. Naturally, unlimited short selling is not realistic, as it introduces a potential solvency risk (x < 0) to the investor (and to her clients and brokers). To avoid extreme positions and solvency risk, we scale the tangency portfolio by mixing it with the T-bill to obtain the same standard deviation as the passive market portfolio. Importantly, this approach affects neither the relative weights of the risky assets nor the portfolio's Sharpe ratio.Whereas the (scaled) tangency portfolio is MV efficient by construction, it need not be optimal for utility maximizers because it is constructed without accounting for higher-order moment risk. To analyze sub-optimality, we construct the tangency portfolio in each of our nine samples, mix it with the T-bill to adjust the risk level, and subject it to our SD efficiency tests. Since the tangency portfolio is optimized for a given return distribution, we now do not use the bootstrap; the empirical distribution now serves as the population distribution. The results for the tangency portfolio cannot be interpreted in terms of pricing kernels and pricing errors, as a portfolio with short positions cannot represent the aggregate market portfolio.Panel A of Table 3 shows the optimal weights assigned to the five small-cap portfolios and the T-bill; the (smaller) weights for the remaining 20 portfolios are omitted for the sake of brevity. Not surprisingly, the optimal portfolio takes a long position in S/L stocks and a short position in S/W stocks for estimation windows ‘1-1’ and ‘13-60’, and takes the opposite positions for ‘2-12’. Panel B shows that the tangency portfolio has a very high mean compared with its standard deviation. Unfortunately, MV optimization may replace upside potential by downside risk, reducing the meaning of standard deviation as a risk measure. Indeed, the tangency portfolio does not exhibit the high positive skewness of a typical concentrated portfolio of small-caps. For the estimation window ‘2-12’, the tangency portfolio even introduces negative skewness by shorting the positively skewed S/L stocks.The StSD test results in Panel C demonstrate that the tangency portfolio is sub-optimal for every standard risk averter in each of our nine samples. The degree of sub-optimality decreases with the length of the estimation window and increases with the evaluation horizon. For estimation window ‘1-1’ and evaluation horizonH=12, the optimization error for S/L stocks is as high as 21.52 percent per annum. Compared with the variance averter, the standard risk averter is less inclined to sell low-yielding (but positively skewed) small-cap stocks (S/L or S/W, depending on H) and more inclined to buy high-yielding small-cap stocks and sell large-cap stocks.The SSD criterion struggles to detect the sub-optimality of the tangency portfolio by allowing for implausible patterns of higher-order risk aversion. As before, imposing standardness substantially increases the test statistic. The incremental effect of imposing DAP (in addition to DARA) is now much larger than for the analysis of the market portfolio. Notably, for window ‘2-12’ and horizonH=12, the StSD test statistic is 12.23 percent per annum, some 6.03 percent points above the value for DSD. Due to the high reward-to-variability ratio, a high level of general risk aversion is needed to rationalize the tangency portfolio and the effects of higher-order risk aversion become very pronounced.StSD has more discriminating power than the existing NSD criteria by imposing DARA and DAP (in addition to non-satiation and risk aversion). Given the universal appeal of these regularity conditions, we see little risk of specification error and the additional power therefore seems a ‘free lunch’.Our analysis does not impose complete monotonicity, or mixed risk aversion of Caballe and Pomansky (1996), which requires additional restrictions on the signs of the derivatives of order five and higher. We subscribe to the theoretical motivation of mixed risk aversion, and, in addition, we can extend our analysis (notably Proposition 4) in a straightforward manner to include additional model variables and linear constraints for the higher-order derivatives. In our experience, such higher-order restrictions however have no material effect on the shape of the optimal utility function.We also do not impose increasing relative risk aversion (IRRA). It seems plausible that the relative risk aversion (RRA) quotientru(x):=au(x)xis constant or increasing slightly for individual utility functions of wealth. However, these arguments do not carry over directly to the utility of income or consumption. In addition, IRRA is not robust to aggregation across utility functions. For example, the combined utility of multiple decision makers with power utility functions with different RRA levels exhibits decreasing RRA.Our empirical application illustrates the benefits of StSD: a substantial increase in the estimated pricing errors of small-loser stocks and an economically more meaningful shape for the pricing kernel compared with the traditional MV and NSD criteria. Depending on the assumed trading strategy and evaluation horizon, we find that the abnormal returns to small-loser stocks are about 1–7 percentage points per annum larger for the StSD criterion than for the SSD criterion and about 50–200 basis points larger than for the MV and higher-order SD criteria. Whereas the optimal kernel for SSD seems pathological and the kernel for the higher-order SD criteria resembles the questionable linear MV kernel, the StSD kernel approximates a standard negative exponential shape. The StSD criterion also shows that the MV tangency portfolio is substantially sub-optimal for standard risk averters in our samples.Compared with the analysis of DSD by Post et al. (2015), we introduced additional variables for the second-order derivative and the AP quotient and additional restrictions for temperance and DAP. The empirical application illustrates the goodness of the resulting linear system. In our analysis of the market index, the incremental effect on the test statistic of imposing DAP (in addition to DARA) is some 10–20 basis point per annum in the typical sample, modest in comparison with the effect of imposing DARA. Nevertheless, the optimal StSD kernel is clearly better behaved than the DSD kernel. In addition, in our analysis of the MV tangency portfolio, the incremental effect of DAP becomes as large as multiple percentage points, and StSD substantially increases the estimated opportunity cost of the MV approximation.We also introduced several other methodological refinements (besides imposing DAP). First, by using different sets of sampling points for the derivatives of different orders, we allow for a coarse partition of the outcomes range, which is useful for low-frequency returns. By comparison, the analysis of DSD optimality in Post et al. (2015) implicitly assumes that the utility function is a piecewise-cubic function, an assumption that is restrictive if the partition is coarse. Second, our mean-based normalization avoids ‘punishing’ skewed marginal utility functions that assign a high value to the worst outcomes. The median-based normalization of Post et al. (2015) artificially increases the level of the errors for such functions by pushing the average value of marginal utility above unity. Third, using multiple frame functions allows for a more accurate, local PWL approximation to the exponentiation.Our empirical results challenge the notions that (i) the MV criterion gives a close approximation to expected utility for analyzing stock returns, (ii) higher-order SD rules properly accounts for higher-order risk aversion and (iii) accounting for higher-order moments reduces the abnormal returns to active investment strategies. Small-cap stocks seem more appealing to standard risk averters than the classical (MV and NSD) decision criteria suggest. In particular, small-loser stocks seem to entail a relatively high probability of future price reversals, introducing a high upside potential compared with their downside risk. A standard risk averter will be more inclined to buy, and less inclined to short, these stocks than a variance averter. The TSD/FOSD criteria do not detect this pattern, because the optimal TSD/FOSD utility function takes a quadratic shape and thus exhibits constant risk aversion.We now analyze the case of the discrete choice opportunity setX0:={X1,⋯,XM}rather than its convex hull,X=Conv(X0). We will need some additional notation. Collect all possible outcomes across prospects and states in{w∈R++:w=xi,ri=1,…,M;r=1,…,R}; rank the elements in ascending order, w1 ≤ ⋅⋅⋅ ≤ wS; and useqi,s:=P[Xi=ws]=∑r:xi,r=wspr,i=1,…,M,s=1,…,S.Definition 2StSD optimalityA given prospect Xi, i ∈ {1, ⋅⋅⋅, M}, is StSD optimal if it is the optimum for some admissible utility functionu∈U4*:(A.1)∑s=1Su(ws)(qi,s−qj,s)≥0,j=1,…,M.Optimality cannot be analyzed using the KKT optimality conditions (6). Our approach is to introduce variables and constraints for the utility levels u(ws),s=1,…,S,in addition to the variables and constraints for the derivatives and their logs in Propositions 3 and 4.Proposition 5Linearized utility and derivativesFor any utility functionu∈U4,we can represent−u(ws), u′(ws) and−u′′(ws),s=1,…,S, using decreasing and convex PWL functions that are linear in a finite number of parameters and obey a set of linear restrictions:(A.2.1)−u(ws)=∑k=sS−1υk(wk+1−ws)+υS,s=1,…,S;(A.2.2)υs≥0,s=1,…,S−1;(A.3.1)u′(ys)=∑k=sS−1βk(wk+1−ws)+βS,s=1,…,S;(A.3.2)∑k=sS−1βk(wk+1−ws)+βS≤∑k=s−1S−1υs,s=2,…,S−1;(A.3.3)∑k=sS−1βk(wk+1−ws)+βS≥∑k=s+1S−1υs,s=1,…,S−2;(A.3.4)βs≥0,s=1,…,S;(A.4.1)−u′′(ws)=∑k=sS−1γk(wk+1−ws)+γS,s=1,…,S;(A.4.2)∑k=sS−1γk(wk+1−ws)+γS≤∑k=s−1S−1βs,s=2,…,S−1;(A.4.3)∑k=sS−1γk(wk+1−wr)+γS≥∑k=s+1S−1βs,r=1,…,S−2;(A.4.4)γs≥0,s=1,…,S.ProofThe utility levels u(ws) in (A.2.1) are built from the decrements of the first-order derivativeυs=u′(zs(1))−u′(zs+1(1)),s=1,…,S−2,υS−1=u′(zS−1(1)), for sampling pointszs(1)∈[ws,ws+1]such thatu′(zs(1))=(u(ws+1)−u(ws))/(ws+1−ws),s=1,…,S−1,andυS=−u(wS). The non-negativity conditions (A.2.2) follow from non-satiation. Inequalities (A.3.2) follow from u′(x) ≥ 0,−u′(zs−1(1))=∑k=s−1S−1υs, andzs−1(1)≤ws; similarly, inequalities (A.3.3) follow from u′(x) ≥ 0,−u′(zs(1))=∑k=sS−1υs, andws≤zs(1). The remaining variables and constraints are as in Proposition 3, mutatis mutandis.□Our Proposition 5 deviates from Proposition 4 in Post et al. (2015) by accounting for the second-order derivative and by using two separate PWL functions, with two different sets of sampling points, for u(ws) and u′(ws). The two functions are not directly linked through integration but through the inequality constraints (A.3.2) and (A.3.3). The two approaches converge as the partition is refined ((wr+1−wr)→0), but only our formulation allows for a coarse partition.The other lemma's and propositions do not require adjustment other than an obvious change in notation from yr,r=1,…,R,to ws,s=1,…,S. We can then combine Proposition 5 with Lemmas 3 and 4 and Propositions 3 and 4 to derive a linear system for (A.1) by analogy to Theorem 1.

@&#CONCLUSIONS@&#
