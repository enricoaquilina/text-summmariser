@&#MAIN-TITLE@&#
HTS and HTSn: New shape descriptors based on Hough transform statistics

@&#HIGHLIGHTS@&#
We show that the Hough space is a good source of information for shape analysis.We developed new shape descriptors based on the Hough transform, HTS and HTSn.HTS and HTSn are more accurate than important shape descriptors of the literature.HTS and HTSn have linear complexity, being faster than other descriptors, e.g. BAS.

@&#KEYPHRASES@&#
HTS,HTSn,Image analysis,Shape analysis,Hough transform,Content-based image retrieval,Optical character recognition,

@&#ABSTRACT@&#
With the widespread proliferation of computers, many human activities entail the use of automatic image analysis. The basic features used for image analysis include color, texture, and shape. In this paper, we propose a new shape description method, called Hough Transform Statistics (HTS), which uses statistics from the Hough space to characterize the shape of objects or regions in digital images. A modified version of this method, called Hough Transform Statistics neighborhood (HTSn), is also presented. Experiments carried out on three popular public image databases showed that the HTS and HTSn descriptors are robust, since they presented precision-recall results much better than several other well-known shape description methods. When compared to Beam Angle Statistics (BAS) method, a shape description method that inspired their development, both the HTS and the HTSn methods presented inferior results regarding the precision-recall criterion, but superior results in the processing time and multiscale separability criteria. The linear complexity of the HTS and the HTSn algorithms, in contrast to BAS, make them more appropriate for shape analysis in high-resolution image retrieval tasks when very large databases are used, which are very common nowadays.

@&#INTRODUCTION@&#
Vision is a powerful and important sense for human beings. It is mainly through this sense that they can identify the elements of the world. Humans have an extremely powerful ability to deal with images and have developed, during evolution, highly sophisticated cognitive and neural systems dedicated to the task of image processing and visual pattern recognition [1].After the advent of computers, many researchers and companies started developing techniques and technology to automate the tasks of digital image capturing and processing, as well as feature extraction and visual pattern recognition processes. With automation, many activities that are typically done manually by humans can be executed faster and, in some cases, more accurately by computers.Many image analysis techniques and hardware devices are currently available. The implementation of visual pattern recognition systems is not only becoming more flexible but also more complex [1].Besides color and texture, shape is a basic feature that can be extracted from digital images in order to obtain information about their contents [1–5]. In many cases, shape is the only discriminative feature that can be used to analyze an unknown object. Silhouettes of objects, for instance, can be identified by human and computer systems based only on their shapes.According to Costa and Júnior [6], the concept of “shape” can be understood as any visual singular entity or an object as a whole defined by a single set of connected points (either in a discrete or continuous space). The shape of an object is invariant under certain transformations such as translation, rotation, and scaling. Depending on the magnitude of the transformation, i.e., in the case that relatively few and non-significant parts of the object are affected, we may also state that the shape of objects is also invariant to projection, deformation, and occlusion.Notably, according to Latecki and Lakämper [7], any method used in the shape analysis task needs to function in a manner similar to the human visual perception system; i.e., it must have the following characteristics: (i) the shape similarity metric should allow the recognition of visually similar objects even if they are not mathematically identical; (ii) the method should ignore distortions such as noise from the digitalization process and segmentation errors; (iii) the method should consider the significant visual parts of the objects; and (iv) the method should not depend on the scale, orientation, and position of the objects (or on other transformations such as projection, deformation, and occlusion). It is important to observe that, in some cases, this invariance to transformations should not exist: the shape analysis method used in a system employed to recognize handwritten digits should not be invariant to rotation since it should be able to differentiate between the digits “6” and “9”, for instance.Inspired by some shape description methods described in the literature, especially the Beam Angle Statistics (BAS) method [8,9], which is a well-referenced shape descriptor, we developed a shape analysis method based on the Hough transform [10,11], called Hough Transform Statistics (HTS). A modified version of this descriptor is also proposed, called HTSn (HTS neighborhood), which presents better results in most of the experiments.Besides presenting the HTS (which we briefly describe in [12]) and HTSn shape descriptors, we show that these new methods present good accuracy when compared with the Beam Angle Statistics (BAS) [9] and other well-known shape description methods, such as Zernike moments [13], Tensor Scale (TS) [14], Multiscale Fractal Dimension (MFD) [15], Fourier [16], and Contour Salience (CS) [17]. We also provide details of the HTS and HTSn algorithms and show that their complexities in the feature extraction and matching phases are much lower than the complexity of BAS, making them more appropriate for working with large databases and large (or high-resolution) images.Proposed by Hough [10] and improved by Duda and Hart [11], the Hough transform was developed in order to detect, at a low computational cost, complex patterns of points in binary images.Hough transform has been extensively used to detect straight line segments in many computer vision applications. In this case, the Hough transform defines a mapping transformation between the image space and theρ–θparameter space (known as Hough space) using a polar coordinate system. This parameterization specifies a straight line of the image space (the x–y space) by the angleθbetween its normal vector and the x axis, and its algebraic distanceρfrom the origin(0,0). The straight line corresponding to this geometry is given by Eq. (1)[11]:(1)ρ=xcos(θ)+ysin(θ)Consideringθ∈[0°;180°), given a straight line r in the image space with its respectiveρandθparameters, sayρ0andθ0, it is represented in theρ–θspace by a single point, with coordinates(ρ0,θ0). Each point, with coordinates(x,y)in the image space, is represented by a sinusoidal curve inρ–θspace following Eq. (1). It is easy to show that in the parameter space, the sinusoidal curves that correspond to points belonging to r have a common intersection point:(ρ0,θ0). Based on this, the problem of detecting straight lines in the image space can be converted into the problem of identifying intersection peaks in the parameter space [11].When working in a discrete space, one can represent the parameter space by an accumulator matrix. Further, to find straight lines in the image, it is necessary to find peaks (positions with high values) in the accumulator matrix.The BAS (Beam Angle Statistics) method, proposed by Arica and Vural [9], is a robust and well-referenced two-dimensional (2D) shape descriptor. It is based on the idea of representing a 2D shape as a one-dimensional (1D) function that can represent all the concavities and convexities of the object boundary.An object boundary B is represented as a connected sequence of boundary pointspi=(xi,yi),i=1,2,…,n, where n is the number of boundary points, andpi=pi+n, i.e., the sequence of the points is circular. For each pointpi, the beams ofpimay be represented as the set of vectors that connectpitopi-jandpi+jin the boundary B (with j varying from 1 ton/2).Given a boundary pointpiand considering the angleCj(i)between the two vectors originating atpiand terminating atpi-jandpi+j, with j varying from 1 ton/2, as a random variable (denoted byCi), we can associate withpithe first (mean), second (variance), and third (skewness) moments of this variable. This process is repeated for all points of B, resulting in three 1D functions: the first, second, and third moment-based functions. After the three 1D functions are obtained, they are sampled in k equally spaced positions, generating the feature vector of B.Given a query image, it is classified by comparing the similarity between its feature vector,Π, with each gallery feature vector,Γ, associated with each image stored in the database (gallery images). The assigned class will be the one associated with the gallery image whose feature vectorΓhas the minimum distance to the query feature vectorsΠ(1-NNclassification strategy), calculated by using the OCS (Optimal Correspondent Subsequence) algorithm [18], whose complexity isΘ(k2), where k is the size of the feature vectors.It is important to note that if invariance to rotation is desired, the feature vector of the query image must be rotated k times, and, then, compared k times with the database feature vectors using the OCS algorithm. The final distance (similarity measure) between the query feature vector and all gallery feature vectors is taken as the minimum distance obtained from all k matchings [19].The BAS shape descriptor can be implemented as shown in Algorithm 1.Algorithm 1BAS shape descriptor1:(*Preprocessing Phase*)2:Read the query image (silhouette);3:Binarize the query image;4:Detect n boundary points in the image;5:Find the top leftmost boundary point in the image;6:Create a sequence S with the n boundary points starting with the top point and continuing clockwise;7:(*Feature Extraction Phase*)8:forp =S(1)toS(n)do9:Calculate then/2beam angles associated with p;10:Calculate the first, second, and third moments (f,s, and t, respectively) of then/2values of the beam angles estimated for p;11:SetF(p)=(f,s,t);12:end for13:Sample F at k equally spaced positions, generating the query feature vectorΠof the silhouette (with k defined by the user);14:(*Matching Phase*)15:Compute the distance between the query feature vectorΠand each gallery feature vectorΓusing the OCS algorithm (carrying out k comparisons by shiftingΠ);16:Assume the distance between the query feature vectorΠand each gallery feature vectorΓas being the minimum distance among all k comparisons;17:Classify the query image as belonging to the class of the gallery image that provided the minimum distance between their feature vectors.Inspired by the idea of generating 1D functions associated with statistic values to represent 2D shapes, we propose a new shape descriptor called HTS (Hough Transform Statistics). Similar to the BAS method, the HTS method has the following phases: (i) preprocessing: boundary extraction; (ii) feature extraction: association of moment values with each point of the boundary, generation of 1D moment functions, sampling of the moment functions, and creation of the feature vector; and (iii) matching of the feature vectors.However, in contrast to the BAS method, in which the moment values associated with each boundary point are related to beam angles, in HTS, they are extracted from the Hough space calculated from the boundary points of the object shape being analyzed. The use of statistics from the Hough space for shape description is worthwhile, since, as shown in Fig. 1, the Hough spaces of silhouettes of objects of the same class are similar, whereas they are quite different when computed from silhouettes of distinct classes. In Fig. 1, the grayscale value associated with each space position is proportional to the value stored in the respective position of the accumulator matrix. The darker the pixel, the lower the correspondent value in the matrix.Given the digital image of an object, the first step of HTS, after segmenting the object from the background and obtaining its boundary, consists of mapping each boundary point into its corresponding sinusoidal curve in the Hough space, which is an accumulator matrix with all elements initially set to zero. The sinusoidal curves in theρ–θspace are calculated based on Eq. (1), using the coordinates x and y of the corresponding boundary points and varying theθparameter from0°to179°. During this process, the positions of the accumulator matrix corresponding to the points of each sinusoidal curve are incremented by one.As mentioned, by applying the Hough transform, a straight line in the image space is mapped into a single point in theρ–θspace, which is the intersection point of all sinusoidal curves associated with the points belonging to that straight line. By following, in the parameter space, a sinusoidal curveσthat represents a given boundary point p in the image space, it is possible to find the numberh(θ)of all sinusoidal curves that intersectσat positionθ. At the end of this process, the histogram h is associated with that boundary point p. The valueh(θ)indicates the number of boundary points that are collinear to p in the directionθ+π2. Based on this, if we consider square shapes, then the points that belong to the boundaries of the four sides will have protuberant peaks in their corresponding histograms, while in rounded shapes all boundary points will present equalized histograms. Thus, we conclude that it is worthwhile to use the number of intersections and their distribution (histogram) as a shape descriptor.Fig. 2shows the histograms of four boundary points (A, B, C, and D) of two bell silhouettes from the MPEG-7 CE-1 (Part B) database [20]. One can observe that boundary points located in different regions of the same silhouette (points A and C, and points B and D) exhibit very different histograms, while points belonging to the same silhouette regions, but from different samples of an object (points A and B, and points C and D) exhibit more similar histograms. These results confirm that the histograms of the sinusoidal curves intersections are powerful sources of information for the characterization of shapes. It is also noteworthy that points belonging to small straight line segments in the boundaries tend to present more uniform histograms, whereas points belonging to long straight line segments in the boundary tend to present histograms with protuberant peaks.Based on this understanding, in the HTS shape descriptor method, after finding the object boundary points, calculating its Hough space (accumulator matrix), and obtaining the histogram h for each boundary point p, we calculate the first and second moments of histogram h.Boundary points that belong to straight line segments are expected to exhibit high concentration of intersections (peaks) in some specific positions along the sinusoidal curveσ, and hence, the standard deviations of their histograms are expected to be higher than the standard deviations of those histograms associated with boundary points of curved regions. As in the BAS method, given an initial boundary point (the top leftmost point, for instance) and by following the boundary clockwise, two 1D functions are built, based on the mean and standard deviation values associated with the boundary points.Fig. 3shows the two 1D moment functions for a square silhouette, and for some silhouettes from the Kimia-216 [21] and MPEG-7 CE-1 (Part B) [20] databases. As one can see, both moment functions for the square silhouette exhibit four peaks. Each peak corresponds to one corner point, since corner points in a square shape belong to two large straight line segments of the boundary and, then, their histograms have high standard deviations. All other boundary points belong to only one straight line segment, so their histograms present lower standard deviations. These other points have equal moment values associated with them because all straight line segments in the boundary to which they belong have the same size.One can also observe in Fig. 3 that the two 1D functions obtained from two silhouettes of the same class are similar. On the other hand, the two 1D functions obtained from silhouettes of diverse classes are quite different.After obtaining the two 1D functions for a given silhouette, they are sampled in k equally spaced positions, generating the feature vectorΠof the shape in which each element i contains two values: the first (Π(i)f) and second (Π(i)s) moments.In order to classify an unknown silhouette, its feature vectorΠis compared with each gallery feature vectorΓ. The similarity betweenΠandΓis defined, as proposed in [9], by theL1distance:(2)L1(Π,Γ)=∑i=1k(|Π‾(i)f-Γ‾(i)f|+|Π‾(i)s-Γ‾(i)s|)where k is the size of the feature vectors, andΠ‾andΓ‾are the normalized feature vectors.If invariance to rotation is desired, the feature vectorΠmust be shifted (rotated) k times and, after each shift, be compared with the feature vectorΓ. The similarity ofΠandΓis, then, taken as the minimum distance obtained after the k comparisons.After calculating the similarity between the query feature vectorΠand each gallery feature vectorΓ, the unknown silhouette is classified as belonging to the same class of the gallery feature vectorΓthat results in a minimumL1distance.The HTS descriptor has three phases:Preprocessing: In this phase, likewise the BAS method, given an image of an unknown silhouette, it is binarized and an edge detection algorithm is applied in order to segment the boundary of the silhouette. Then, a sequence of boundary points is obtained by starting from the top leftmost boundary point and following the boundary clockwise until complete a full turn around the silhouette.Feature extraction: This phase begins by calculating the Hough transform by considering all boundary points of the silhouette. Next, for each boundary point p, its histogram is calculated by taking all the values found in the Hough space in the positions given by the correspondent sinusoidal curve. Then, the first and second order moments are computed from the histograms calculated for each boundary point, generating two 1D moment functions. Finally, the two 1D functions are sampled in equally spaced k points (where k is defined by the user), generating the feature vector associated with the silhouette being analyzed.Matching: The query feature vectorΠ, extracted from the unknown silhouette, is compared with each gallery feature vectorΓusing theL1distance. To achieve invariance to rotation, likewise the BAS method, the feature vectorΠis shifted (rotated) to the left (always copying the values in the first position to the last) k times. Each new configuration of the shifted query feature vector is compared with the gallery feature vectorΓ. The final distance between the two feature vectors is taken as the minimum distance obtained for all k comparisons. The unknown silhouette is then classified as belonging to the same class of the nearest (most similar) silhouette in the database.The HTS shape description method can be implemented as shown in Algorithm 2.Algorithm 2HTS shape description method1:(*Preprocessing Phase*)2:Read the query image (silhouette);3:Binarize the image;4:Detect n boundary points in the binary image;5:Find the starting (top leftmost) boundary point in the silhouette;6:Create a sequence S of n boundary points beginning at the starting point and continuing clockwise around the silhouette;7:(*Feature Extraction Phase*)8:Initialize the accumulator matrix (HoughSpace(:,:)=0);9:forp =S(1)toS(n)do10:forθ=0°to179°do11:Find the respectiveρparameter using the coordinates of p and applying the Eq. (1);12:SetHoughSpace(ρ,θ)=HoughSpace(ρ,θ)+1;13:end for14:end for15:forp=S(1)toS(n)do16:forθ=0°to179°do17:Find the respectiveρparameter using the coordinates of p and applying the Eq. (1);18:SetHistogram(p)(θ)=HoughSpace(ρ,θ);19:end for20:Calculate f and s, the first and second moments ofHistogram(p);21:SetF(p)=(f,s);22:end for23:(*Matching Phase*)24:Sample F into k equally spaced positions, generating the feature vectorΠof the query silhouette (where k is a parameter defined by the user);25:Compare the query feature vectorΠwith each gallery feature vectorΓusing theL1distance. The comparison is carried out k times, shifting the query feature vectorΠ. Among all k distance values obtained, the minimum one is taken as the distance between the query feature vectorΠand the gallery feature vectorΓ;26:Classify the query silhouette as belonging to the same class of the nearest database image (that is, the image associated with the gallery feature vectorΓwith the minimum distance toΠ).A modified version of the HTS descriptor is also proposed in this paper, called HTSn (HTS neighborhood).HTSn is similar to HTS except in the histogram computing step, in which ar×rneighborhood of the sinusoidal curve (σ) associated with a boundary point p is used to compute the histogram of p.Fig. 4illustrates the process of construction of the histogram for a given boundary point p in the HTS and HTSn methods. In this figure, the matrices represent a region of the Hough space at three consecutive moments in the process of construction of the histogram to a given boundary point p in the (a) HTS and (b) HTSn descriptors. For the HTSn method a3×3neighborhood is adopted in this example. The sinusoidal curveσassociated with p pass over the positions of the matrices highlighted in red. While in the HTS the histogram is calculated by taking the values stored in the matrix where the sinusoidal curveσpasses over (Histogram=[4,2,3], see Fig. 4(a)), in the HTSn method, the histogram is calculated by summing the values stored in the3×3neighborhood of the matrix centered at each position of the sinusoidal curveσthat corresponds to p (Histogram=[46,26,22], see Fig. 4(b)).After associating a histogram to each boundary point p, the HTSn method computes the two 1D functions for the object under analysis, obtains its feature vector, and classifies it likewise the HTS method.Regarding the perceptual aspects of shape description, Arica and Vural say that their intention when proposing BAS method was “to convert the closed boundary of an object to an open boundary by breaking it at one point and kindly overlaying it in a straight line without disturbing its topological properties” [8].In perceptual terms, in the BAS descriptor each valley in the 1D functions corresponds to a concave region of the object boundary, while each hill corresponds to a convex region. Boundary points from concave regions of the shape tend to present smaller moment values associated with them since their beam angles are mostly close to0°, whilst boundary points from convex regions of the shape tend to present higher moment values associated with them since their beam angles range from0°to180°.In summary, BAS preserve the human perception of the concavities and convexities of the 2D shape, while representing it in a more efficient way, by using 1D moments functions [8,9].HTS shape descriptor, whose approach was inspired by BAS, is motivated by the fact that the human visual perception system is extremely sensitive to straight line stimuli.Torsten Wiesel and David Hubel, in the early 1960s, discovered in the Brodmann area 17 (the primary visual cortex), and Brodmann areas 18 and 19, the extrastriate cortical areas, some kinds of cells that provide high responses to orientation, movement direction, size and extremities of rectilinear stimuli. These cells are called: simple cells, complex cells, and hypercomplex cells [22–24].Fig. 5(a) presents a neuronal structure that forms the receptive field of a simple cell. A simple cell in the primary visual cortex is a cell that responds primarily to oriented edges and gratings (bars of particular orientations) [25].Fig. 5(b) presents a neuronal structure that forms the receptive field of a complex cell. The complex cells, which can be found in the primary and extrastriate cortical areas, receive inputs from a number of simple cells. Therefore, likewise the simple cells, the complex cells respond primarily to oriented edges and gratings moving towards a certain direction [25].Fig. 5(c) presents a neuronal structure that forms the receptive field of a hypercomplex cell (currently called an end-stopped cell), also found in the primary and extrastriate cortical areas. The hypercomplex cells receive inputs from a number of complex cells and are defined by the property of end-stopping, which is a decrease in firing strength with increasingly larger stimuli. The sensitivity to stimulus length of these cells is accompanied by selectivity for the specific orientation, motion, and direction of stimuli [25].The presence of the simple, complex and hypercomplex cells in the primary areas of the visual cortex indicates the importance of the rectilinear stimuli, as well as their attributes, like orientation, movement and size, to the human being global visual processing, what includes shape processing. Therefore, the presence of straight lines in the objects borders in the images is an extremely important information, and imediately searched by the human biologic perceptual system.In the HTS approach, given a boundary of an object in a certain image, the 1D moments based functions tend to present higher values for border points belonging to the more rectilinear regions of the boundary, since those points will present peaks in their histograms and, consequently, will be associated with higher standard deviation values. In contrast, border points belonging to rounded regions of the boundary will not present salient peaks in their histograms and, consequently, they will be associated with lower standard deviation values.Fig. 6(a)–(c) show, respectively, the shapes of the three objects: a square, a hexagon, and a circle, and Fig. 6(d), shows their respective standard deviation functions (SDF). As one can see, the square shape SDF presents four regions with almost the same standard deviation values (around 25) and four very prominent standard deviation values (around 35), which are associated with the four corner points of the square. The hexagon SDF presents six regions with almost the same standard deviation values (around 12), associated with each one of its six sides, with each such region having lengths proportional to the hexagon sides. Finally, the circle SDF tends to present almost the same low standard deviation values (around 6). As in digital image processing the objects are represented in a discrete space, even the circles borders are formed by small straight line segments, that results in the small differences observed in its standard deviation values. In a continuous space, these values would be equal for all circle boundary points.The BAS method is a well-known shape descriptor that presents very good results [9]. The goal of the development of the HTS is to provide a robust shape description method that adopts the BAS approach for shape description, but it is faster.The BAS feature extraction algorithm has a complexityΘ(n2), where n represents the number of boundary points of the silhouette being analyzed. This algorithm calculates, for each boundary point, the three first moment values of the beam angles associated with it. To calculate these values it is necessary to measuren/2beam angles associated with each one of the n boundary points. Therefore, the BAS feature extraction phase requiresn·n/2operations, leading to aΘ(n2)complexity.In the matching phase, the BAS shape description method uses the OCS (Optimum Correspondent Subsequence) algorithm [18], which has a quadratic complexity. If the boundaries of the silhouettes being compared are previously aligned, the matching phase would have a complexityΘ(k2)(inherited from the OCS algorithm), where k indicates the size of the feature vectors. However, when no technique is used to carry out the pre-alignment of the two shapes being matched, the comparison between the two feature vectors using OCS is carried out k times (shifting k times the query feature vectorΠ). Hence, the complexity of the BAS matching phase becomesΘ(k·k2), i.e.,Θ(k3).When working with small databases and with silhouettes exhibiting small boundaries, the complexity of the BAS algorithm is not a serious problem. However, when the database and/or the images are very large (and the number of points on the boundaries of the objects grows considerably), the BAS method can become very slow.The HTS method, in contrast, has a feature extraction phase with linear complexity. To associate the two moment values with each pointpiof the boundary,i=1,2,…,n, it is necessary to calculate the Hough transform, to follow the sinusoidal curve associated withpiand to find its histogram. Therefore, HTS requires, for each boundary pointpi, a total of 360 operations: 180 operations in order to generate the sinusoidal curve associated with the pointpiin the Hough space (by varying theθparameter from0°to179°, and finding the respectiveρvalue according to Eq. (1)), and 180 operations to follow back the sinusoidal curve associated withpiin order to obtain its histogram. Hence, to associate the two moment values with all the n boundary points, HTS method requires only360·noperations. With this, the HTS feature extraction algorithm has a complexityΘ(n), being much faster than the BAS method.In the matching phase, if it is not necessary to shift the query feature vectorΠin order to correct possible rotations, HTS carries out k operations to calculate theL1distance between the query feature vectorΠand each gallery feature vectorΓ(where k is the size of the feature vectors), leading to a complexity ofΘ(k). When the objects are not previously aligned, it is necessary to shift k times the query vectorΠ, and for each shift, compare it with the gallery feature vectorΓ. Hence, the matching phase has a complexityΘ(k·k), i.e.,Θ(k2), being also much better than the BAS method.The HTSn method has an algorithm that is quite similar to the HTS method. Thus, it presents the same complexities in the feature extraction and matching phases. The modified strategy to obtain the histograms does not increase the linear complexity of the method in the feature extraction phase.Aiming to assess the performance of our shape description methods, we carried out some experiments. As the BAS shape description method [9] was the main inspiration to both the HTS and the HTSn methods, our first goal was to compare their performances. In addition, we compared our methods with other six well-referenced shape description methods: Tensor Scale (TS) [14], Multiscale Fractal Dimension (MFD) [15], Fourier [16], Contour Salience (CS) [17], Zernike moments [13], and Curvature Scale Space Descriptors (CSSD and CSSD+) [26,27].The performances of the methods were compared by means of precision–recall curves (the higher the curve, the better is the shape descriptor), which is one of the most adopted criterion. In pattern recognition and information retrieval, precision is the fraction of retrieved instances that are relevant, while recall (or sensitivity) is the fraction of relevant instances that are retrieved.Besides using different shape description methods, we also used in our experiments some public shape images databases: Kimia-216 [21], MPEG-7 CE-1 (Part B) [20], and MNIST [28].The HTS and HTSn algorithms were implemented using the C programming language. For all other shape description methods, except CSSD and CSSD+, we used algorithms implemented by third part developers (also in the C programming language and Matlab for the Zernike moments) [15,29]. For CSSD and CSSD+, we reported the precision–recall results obtained in the literature (for the MPEG-7 CE-1 (Part B) database) [30], since we could not get their executable or source codes. The experiments were conducted on a Macintosh Power Mac G5 with a PPC970 2.0GHz (two-core) processor, 4GB of RAM memory, and Debian 6.0.4 as the operating system.In the first experiment, we used the Kimia-216 database [21], which has binary images from 18 classes of objects, 12 images per class (totaling 216 images). For each image, there is a black silhouette of an object drawn on a white background. The images in this database have almost the same size (around 100 by 100 pixels), and there are no scale transformations on the silhouettes. Some silhouettes are rotated, translated or present small deformations.Besides the HTS, the HTSn and the BAS methods, we also assessed the following shape description methods: Zernike moments, Multiscale Fractal Dimension (MFD), Tensor Scale (TS), Fourier, and Contour Salience (CS).Fig. 7presents the precision–recall curves obtained. One can observe that, regarding this criterion, the best shape description methods for the Kimia-216 database were (in this order): BAS, HTSn, Zernike, and HTS.It is important to note that the precision–recall curves of the BAS, HTS, HTSn, and Zernike descriptors are relatively close to each other. For low recall values, HTS and HTSn have almost the same precision results as BAS and, for many applications, this part of the curve is much more relevant than the other. It is also important to observe that the precision–recall curve of the HTSn descriptor for high recall values is superior to the curve obtained for the BAS method.In the second experiment, we used images from the MPEG-7 CE-1 (Part B) database [20]. This database contains binary images of 70 classes of objects, 20 images per class (totaling 1,400 images). In each image, there is a white silhouette of an object drawn on a black background.The MPEG-7 CE-1 (Part B) [20] images have variable sizes (with the majority of them being large, 450 by 450 pixels); some silhouettes are rotated, translated, or distorted; and some exhibit intentional noise. In addition, there are silhouettes in which a combination of transformations were applied.In order to compare the performance of the HTS and HTSn methods on the MPEG-7 CE-1 (Part B) dataset [20], we selected the following set of shape description methods: BAS (Beam Angle Statistics), Zernike moments, Curvature Scale Space Descriptors (CSSD and CSSD+), Multiscale Fractal Dimension (MFD), Tensor Scale (TS), Fourier, and Contour Salience (CS). The precision–recall curve of each method is shown in Fig. 8. The curves of the CSSD and CSSD+ methods were obtained from the literature [30]. As can be observed, HTSn and HTS produced the third and fifth best results, respectively, and the best descriptor in terms of precision–recall for the MPEG-7 CE-1 (Part B) database [20] was BAS.It is important to note that the Zernike, CSSD and CSSD+ methods are considered “MPEG-7 descriptors”, i.e., traditional shape descriptors for experiments with the MPEG-7 CE-1 (Part B) dataset [20], well-referenced in many works. Despite of HTSn presenting, in this experiment, a worse performance in terms of precision–recall than the Zernike method, their curves are relatively close. Besides, the HTSn methods outperformed the CSSD and the CSSD+ methods. The HTS method also presented a better performance than the CSSD descriptor and better precision rates for high recall values than the CSSD+.In the third experiment, we used images from the MNIST database (Mixed National Institute of Standards and Technology database) [28], which is a public image dataset that contains 10,000 images of handwritten digits from the NIST Special Database 1 (SD-1) and Special Database 3 (SD-3), to assess our shape descriptors. The original binary images from SD-1 and SD-3 were size normalized to fit in a pixel box with 20 rows per 20 columns (preserving their aspect ratio) and centered in grayscale images with 28 rows per 28 columns (which were binarized before applying the shape analysis methods tested).Since the writers of the digits in the SD-1 and SD-3 databases belong to two very distinct groups of people (in social and cultural terms), the goal of the MNIST database is to mix images from both datasets in order to make the tests with pattern recognition algorithms more complex and realistic, i.e., more independent from the social and economic profile of the writers [28].Despite of presenting some relatively well-written digits (as shown in Fig. 9(a)), the MNIST database [28] also presents many images with handwritten digits that are difficult to be classified even by a human being due to the “bad” calligraphy of some writers (as one can see in Fig. 9(b)). Besides this, since the writers have different calligraphy, in some cases, digits from distinct classes seems very similar even for humans.In order to compare the performance of the HTS and HTSn methods on the MNIST database [28], we selected the following set of shape description methods from the literature: BAS (Beam Angle Statistics), Zernike moments, Multiscale Fractal Dimension (MFD), Tensor Scale (TS), Fourier, and Contour Salience (CS). The precision–recall curve of each method is shown in Fig. 10. The BAS method presented the best performance while the HTSn and the HTS methods presented, respectively, the second and fourth best results. The Zernike descriptor presented the fifth best performance and, as one can see, its precision–recall curve is not so close to the HTS one.For the methods that presented the best performances regarding the precision–recall criterion in the Experiments 1, 2 and 3, we also analyzed their performance regarding the search radius versus separability (multiscale separability) criterion, which has been considered by some authors as a metric to compare shape descriptors [17]. Multiscale separability indicates how clusters of different classes are distributed in the feature space (the more separated the clusters, the better is the descriptor). The higher the multiscale separability curve obtained by the method, the better is its performance.As shown in Experiment 1, for the Kimia-216 dataset [21], the best descriptors in terms of precision–recall results were: BAS, HTSn, Zernike, and HTS. Their multiscale separability curves for this same database are shown in Fig. 11(a). As one can see, HTS, HTSn and Zernike methods presented similar results, with Zernike descriptor being slightly superior to HTS. The BAS method presented a bad performance regarding the multiscale separability criterion.For the MPEG-7 CE-1 (Part B) dataset [20], the best descriptors in terms of precision–recall results were: BAS, Zernike, HTSn, CSSD+, and HTS. The multiscale separability curves of these methods, except CSSD+, for the same dataset are shown in Fig. 11(b). In this case, the curves of the HTSn, BAS and Zernike methods are close and better than the curve of the HTS method. We did not calculate the multiscale separability curve for the CSSD+ method because we do not have its executable code. As mentioned before, the precision–recall curve presented for this method was generated from results reported in the literature [30].For the MNIST dataset [28], the best descriptors in terms of precision–recall results were: BAS, HTSn, Fourier, and HTS. Because Zernike descriptor presented good precision–recall results for the Kimia-216 and MPEG-7 CE-1 (Part B) datasets, we decided to calculate its multiscale separability curve too. The multiscale separability curves of these shape descriptors for the MNIST dataset are shown in Fig. 11(c). As one can observe, the HTS and the HTSn methods outperformed the other ones regarding this criterion.The Beam Angle Statistics (BAS) [9] method was the main inspiration to the development of the HTS and the HTSn methods. Therefore, one of the main goals of this work is to compare their performances. In subsections 6.1, 6.2, 6.3, and 6.4 we presented their performances regarding the accuracy rates over three important databases: Kimia-216, MPEG-7 Part B and MNIST, respectively. In this subsection, we compare their performances regarding processing time.Table 1presents the times (in seconds) taken by the BAS, the HTS and the HTSn methods, in the feature extraction (FE) and the matching (M) phases to identify the objects in all the images of the three databases used in this work: Kimia-26, MPEG-7 and MNIST.For the Kimia-216 database, while the HTS and the HTSn algorithms spent only 9 and 11 s, respectively, extracting the features of the 216 images of the database, and 6 s in both methods performing the2162matchings, the BAS algorithm spent 11 and 72 s, respectively, on the same activities.For the MPEG-7 Part B database, as one can see in Table 1, the HTS and the HTSn algorithms achieved again much better results than the BAS algorithm regarding the processing time due to their lower complexity. The time taken by the BAS algorithm to execute all the 1,400 feature extractions (FE) of the 1,400 silhouettes in the MPEG-7 CE-1 Part B database was almost four times greater than the time required by HTS for the same task (1,157 s for BAS and 296 s for HTS). In the matching phase (M), the HTS algorithm was approximately fifteen times faster to perform the1,4002matchings (2,997s for BAS and 221s for HTS). The HTSn method presented processing times similar to the HTS ones: 315 s for the feature extraction phase (FE) and 221 s for the matching phase (M).Finally, for the MNIST database, the HTS and HTSn algorithms achieved again much better results than the BAS algorithm regarding the processing time due to their lower complexity. The time taken by the BAS method to execute all the 10,000 feature extractions (FE) of the 10,000 images in the MNIST database was approximately 2,700s, while the HTS and the HTSn algorithms spent, respectively, 83 and 152s. In the matching phase (M), the BAS algorithm spent approximately 155,000s to perform the10,0002matchings, and the HTS and the HTSn algorithms spent only 11,200 and 11,400s, respectively.In order to obtain a more detailed analysis of the processing times taken by BAS, HTS and HTSn methods in the feature extraction (FE) and matching (M) phases when applied to shapes with different sizes and different length of their feature vectors, we created a sequence of eight binary images,s1,s2,…,s8, composed by square silhouettes, so that the size of the square silhouette in the imagesi+1is the double of the size of the square silhouette in the imagesi. In the first image,s1, the square has 76 boundary points (side=20), in the second image,s2, the square has 156 boundary points (side=40), in the third image,s3, the square has 316 boundary points (side=80), and so on.Each descriptor (BAS, HTS and HTSn) was tested separately. First, we analyzed the time taken by the methods to extract the features of the boundary, i.e., to generate the feature vector for a given square silhouette. The results are shown in Fig. 12(a) and (b). As one can see, when the boundary of the silhouette is small (20 points per square side, for instance), the BAS and the HTS descriptors exhibit quite similar execution times, while the HTSn descriptor is a little slower. However, when the number of boundary points is large, the HTS and HTSn descriptors are much faster than BAS. It is important to say that, given the increasing storage capacity of the new devices, and the increasing availability of high-resolution sensors, the digital images, and, consequently, the boundaries of the objects in the images, are getting larger and larger.For the analysis of the matching time, only the feature vector length matters. Therefore, in this analysis we used only the largest square silhouette from the images8, which has 10,236 boundary points (side=2,560). From the BAS, HTS, and HTSn 1D functions, we generated the feature vectors by sampling these functions at an increasing number of samples (fromk=20tok=2,560samples). Fig. 13(a) and (b) show the results obtained in the matching phase for each value of k. Again, the HTS and HTSn descriptors were much faster than BAS descriptor. When the length of the feature vectors was 2,560, the BAS descriptor spent 126s, whereas the HTS and the HTSn descriptors spent approximately 0.29s. As one can see, even for small silhouettes, the HTS descriptor is faster than the BAS descriptor in the feature extraction and matching phases.The difference in the execution times of the HTS and the BAS descriptors quickly increases when the sizes of the boundaries or the lengths of the feature vectors grow. When this happens, the BAS descriptor becomes much slower than HTS. These results were expected because, as shown in Section 5, BAS has higher complexities than HTS in the feature extraction and matching phases.The HTSn method also presented much lower processing time than BAS, except in the feature extraction phase when applied to very small objects. This performance was also expected since HTS and HTSn present practically identical algorithms. The processing times obtained for the HTSn descriptor in the feature extraction phase are slightly worse than the HTS ones, since the HTSn descriptor analyses the neighborhood of the sinusoidal curve associated with each boundary point p in order to generate the histogram of p, in contrast to the HTS method that only uses the values of the sinusoidal curve.In order to assess the influence of the use of the OCS method in the BAS performance, we repeated the experiments on the three databases (Kimia-216 database [21], MPEG-7 CE-1 (Part B) database [20], and MNIST database [28]), but replacing the OCS method by theL1distance in the matching phase. One can see in Fig. 14(a) and (b) that, with this change, the BAS performance decreases from first to fifth position on the Kimia-216 database, and from first to fourth position on the MPEG-7 CE-1 (Part B) database. For the MNIST database, the BAS method withL1distance obtained better results than using the OCS method and maintained the first position.We have also assessed the HTS and the HTSn methods with the OCS method replacing theL1distance, and the results were quite similar to the results obtained by them using theL1distance, in terms of accuracy rates. So, we conclude that the HTS and the HTSn methods do not benefit from using the OCS method instead of theL1distance, and that, depending on the characteristics of the images in the database, BAS can get worse in terms of accuracy when replacing the OCS method by a common distance function, likeL1distance.

@&#CONCLUSIONS@&#
This paper presents a new shape description method based on the Hough transform statistics, called HTS (Hough Transform Statistics), and its modified version, called HTSn (HTS neighborhood).Experiments carried out on Kimia-216 [21], MPEG-7 CE-1 (Part B) [20] and MNIST [28] shape databases showed that the HTS and the HTSn methods present better precision-recall results than several well-referenced shape description methods described in the literature, such as Tensor Scale [14], Multiscale Fractal Dimension [15], Fourier [16], and Contour Salience [17], being also superior to the Zernike moments shape description method [13] for the MNIST database.When compared to the Beam Angle Statistics (BAS) [9] method, the shape description method that inspired their development, the HTS and the HTSn methods obtained inferior precision-recall results on the three mentioned shape databases. However, if the multiscale separability criterion is adopted, these two methods are superior for the Kimia-216 and MNIST databases, and similar to the BAS method in the MPEG-7 CE-1 (Part B) database.Since HTS and HTSn shape feature vectors are calculated from the Hough space, and the Hough space will always have the same width (θalways ranges from0°to179°), regardless of the size of the input image, the new shape description methods have linear complexities in the feature extraction and matching phases, being much better than BAS regarding the processing time criterion.Therefore, from the obtained results, we can conclude that in the shape analysis tasks using large images stored in very large databases, which is a common task nowadays, the HTS and the HTSn methods can outperform the BAS descriptor regarding the processing time, while presenting accuracy rates almost similar to the BAS method and superior to several other well-known shape descriptors found in the literature.In future work, we intend to investigate new strategies that can be used to optimize the HTS and HTSn descriptors. The use of the Hough space salience points to register the shapes can be a promising alternative. We also intend to further assess the performance of the HTS and the HTSn methods by applying them to other shape databases and comparing them with other shape description methods.