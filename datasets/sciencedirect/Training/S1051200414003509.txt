@&#MAIN-TITLE@&#
Denoising sound signals in a bioinspired non-negative spectro-temporal domain

@&#HIGHLIGHTS@&#
We address the problem of signal representation in the broad field of signal denoising.A biologically-inspired method is here adapted to a non-negative matrix factorization framework.A sparse coding is estimated of a combined dictionary from noisy and clean signals.A main denoising algorithm along its variants are presented.The method improves the quality of the signals, mainly under severe degradation.

@&#KEYPHRASES@&#
Approximate auditory cortical representation,Sound denoising,Non-negative sparse coding,Bioinspired signal processing,

@&#ABSTRACT@&#
The representation of sound signals at the cochlea and auditory cortical level has been studied as an alternative to classical analysis methods. In this work, we put forward a recently proposed feature extraction method called approximate auditory cortical representation, based on an approximation to the statistics of discharge patterns at the primary auditory cortex. The approach here proposed estimates a non-negative sparse coding with a combined dictionary of atoms. These atoms represent the spectro-temporal receptive fields of the auditory cortical neurons, and are calculated from the auditory spectrograms of clean signal and noise. The denoising is carried out on noisy signals by the reconstruction of the signal discarding the atoms corresponding to the noise. Experiments are presented using synthetic (chirps) and real data (speech), in the presence of additive noise. For the evaluation of the new method and its variants, we used two objective measures: the perceptual evaluation of speech quality and the segmental signal-to-noise ratio. Results show that the proposed method improves the quality of the signals, mainly under severe degradation.

@&#INTRODUCTION@&#
In previous years, several techniques of signal analysis have been applied to audio and speech denoising with relatively good results in controlled conditions [1]. However, it is widely known that the performance of these signal analysis techniques in adverse environments is far from that of a normal human listener [2]. On the other hand, there is an increasing number of new signal processing paradigms that promise to deal with more complex situations. This is the case with sparse coding and compressed sensing [3,4]. Their ability to efficiently solve challenging signal representation problems could be exploited in order to develop new audio and speech processing techniques.For many years, researchers in the field of signal processing have greatly benefited from the use of methods inspired by human sensory mechanisms. Some examples of this for audio and speech encoding were mel frequency cepstral coefficients (MFCC) and perceptual linear prediction (PLP) coefficients [5]. Auditory representations of sound at the cochlea have been widely studied. Different mathematical and computational models have been developed that allow the approximate estimation of the so-called early auditory spectrogram[6,7]. These investigations have enabled an accurate modeling of the discharge patterns of the auditory nerve [8,9].Although less known, the underlying mechanisms at the level of the auditory cortex have also been studied and modeled [10]. In experimental conditions – given a sound signal – a pattern of activations can be found at the primary auditory cortex that encodes a series of meaningful cues contained in the signal. This cortical representation seems to use two principles: the need for very few active elements in the representation and the statistical independence between these elements [11]. This behavior of the cortical neurons could be emulated using the fundamentals of sparse coding (SC) [12], the independent component analysis (ICA) [13] and the notion of spectro-temporal receptive fields (STRF). The STRF are defined as the optimal linear filter that convert a time-varying stimulus into the firing rate of an auditory cortical neuron, so that it responds with the largest possible activation [14]. These concepts have led to the development of a number of contemporary auditory models that incorporate different auditory phenomena, for example neural timing information [15], modeling of spectral and temporal content in the cochlear response [9]. A very complete and recent review on biologically-inspired models for speech processing is given in [16].A number of works have explored the use of auditory models for building robust speech/speaker recognition system. In [17], a model of auditory perception (PEMO [18]) is used to obtain the features in a digit recognition system, after processed with well-established algorithms for speech enhancement (for example, the Ephraim and Malah estimator [19]). In [20], authors proposed the use of the model of Li [21] as a front-end in a hidden Markov model-based speech recognizer. Here, the speech is first pre-processed with state-of-the-art enhancement algorithms ([19,22] and others). More recently, different modifications of the MFCC representation were introduced (noise suppression, temporal masking and others) and compared to standard MFCC and PLP coefficients for speech recognition [23]. As can be seen, these efforts were mainly devoted – differently from our speech enhancement point of view – to build new feature extraction schemes for the recognizers while maintaining standard techniques for the enhancement itself.In a previous work [24], the approximate auditory cortical representation (AACR) which is a set of activations computed using matching pursuit (MP) on a discrete dictionary of bidimensional atoms, was presented. These atoms represent the STRF of the auditory cortical neurons. The AACR intends to model the global statistical characteristics of the discharge patterns in the auditory cortex, in a phenomenological rather than a physiological way. This technique provides an approximated representation of the speech signal at the auditory cortical level. It has proved to be beneficial with respect to standard spectro-temporal techniques given the fact that at this higher level in the auditory path, some aspects of the acoustic signal that arrives at the eardrum have been reduced or eliminated [16]. Among these superfluous aspects are the temporal variability of the signal and the relative phase of acoustic waveforms [25]. This approach was then applied to a phoneme classification task in both clean and noisy conditions, showing the advantages of the intrinsic robustness of the sparse coding achieved.In this work, this approach is adapted to a non-negative matrix factorization (NMF) framework. A non-negative auditory cortical representation is used in order to propose a novel sound denoising algorithm. NMF is a recently developed family of techniques for finding parts-based, linear representations of non-negative data [26–29]. These models deal with the temporal continuity of the signals (which is also found in our auditory spectrograms), such as slow variation of pitch in speech and music through consecutive frames, and were applied to monaural source separation. Regarding the speech processing applications, semi-supervised/supervised approaches were reported [30–33]. In these systems, first statistical models for clean speech/noise are estimated. Then, the input signal is analyzed to obtain the denoised version, which is then applied to the recognition block. In [34] two sparse dictionaries are obtained directly from spectrograms of clean speech and noise. Then, a representation of the noisy speech is obtained by a linear combination of a small number of both type of exemplars, in order to feed a robust speech recognizer.In the biologically-inspired context, the NMF use data described by using just additive components, e.g. a weighted sum of only positive STRF atoms. This new model still retains its biological analogy, in spite of the fact that positive STRF implies only non-inhibitory behavior. Thus, positive coefficients could be interpreted as firing rates of excitatory cortical neurons. The new proposal of a non-negative auditory cortical denoising algorithm also differs from previous work in the sense that now two STRF dictionary are estimated from clean and noisy signals separately. Then, the dictionaries are combined in a mixed dictionary containing the most representative atoms for each case, obtaining a better representation of the important features of sound and noise for the denoising stage.The organization of the paper is as follows. Section 2 presents the methods that give the signal representation in the approximate auditory cortical domain. Section 3 outlines the proposed technique to perform the signal denoising in this domain. Section 4 presents the experimental framework and data used in the following experimentation. Section 5 shows the obtained results and the discussions. Finally, Section 6 summarizes the contributions of the paper and outlines future research.Mesgarani and Shamma [10] proposed a model of sound processing carried out in the auditory system based on psychoacoustic facts found in physiological experiments in mammals. The main idea behind the model is first to obtain a representation of the sound in the auditory system. Then, they further decompose this representation to its spectral and temporal content in the cochlear response.While the complete model of Shamma consists of two stages, in this work only the first stage was used. This stage produces the auditory spectrogram (AS), an internal cochlear representation of the pattern of vibrations along the basilar membrane.In the following, subscript ‘ch’ stands for cochlear, ‘an’ for auditory nerve and ‘hc’ for hair cell. The first part of the model is implemented by a bank of 128 cochlear filtersxchthat process the temporal signals(t)and yield the outputs(1)xchk(t,f)=s(t)⊗hchk(t,f),wherehchkis the impulse response of the k-th cochlear filter [10]. This is a bank of overlapping constant-Q (QERB=5.88) bandpass filters with center frequencies (CF) that are uniformly distributed along a logarithmic frequency axis, over 5.3 octaves (24 filters/octave, 0–4 kHz). The CF of the filter at location l on the logarithmic frequency axis (in octaves) is defined as(2)fl=f02l(Hz),wheref0is a reference frequency of 1 kHz [10]. The quantity and frequency distribution of the filters proved to be satisfactory for the discrimination of important acoustic clues and for an appropriate reconstruction of speech signals [9].These 128 filter outputs are transduced into auditory-nerve patternsxanusing(3)xank(t,f)=ghc(∂txchk(t,f))⊗μhc(t),where∂trepresents the velocity fluid-cilia coupling (highpass filter effect),ghcthe nonlinear compression in the ionic channels (sigmoid function of the channel activations) andμhcthe hair-cell membrane leakage modeling the phase-locking decreasing on the auditory nerve (lowpass filter effect) [10]. Finally, the lateral inhibitory network is approximated by a first-order derivative with respect to the tonotopic (frequency) axis, which is then half-wave rectified as(4)xlink(t,f)=max⁡(∂fxank(t,f),0)[10].The AS is then obtained by integrating this signal over a short window, modeling a further loss of phase locking. Fig. 1shows a scheme of the auditory model as used in this work.We now suppose that the representation of any bidimensional slide signalx∈Rm×nobtained from the early auditory model in (4) is given by a linear combination of atoms representing the STRFs, in the form(5)x=Φa,whereΦ∈Rm×n×Mis the dictionary of M bidimensional atoms anda∈RMis the target representation. The 2-D basis functions of the dictionary are vectorized asΦ=[Φ→1…Φ→M]withΦ→i∈R[mn]×1. Then, (5) can be alternatively written asx→=∑1≤i≤MΦ→iai. The desired sparsity is included when the solution is restricted to(6)mina⁡‖a‖0,where‖⋅‖0is thel0norm, which counts the number of non-zeroes entries of the vector. This is an NP-complete problem so several approximations were proposed [35].In order to find the required representation, two problems have to be jointly solved: the estimation of a sparse representation and the inference of a specialized dictionary. The coefficients found with methods based on basis pursuit (BP) or MP give both atoms and activations with positive and negative values [36,37]. However, in some applications it could be useful to work only with positive values, thus providing the method with the ability to explain the data from the controlled addition of (only positive) atoms. This is the objective of non-negative matrix factorization methods.As it was mentioned in Section 1, there are several approaches to obtain a nonnegative atomic sparse decomposition of data. Among them, in this work the method proposed in [38] is selected given its simplicity, excellent performance in other applications (for example, image classification [39]) and the possibility to explicitly set the number of sparse components to use in the approximation.Aharon et al. introduced the K-SVD as a generalization of the k-means clustering algorithm to solve the sparse representation problem given a set of signals x to be represented [38]. Moreover, they included a non-negative version of the BP algorithm, named NN-BP, for producing non-negative dictionaries. The method solves the problem(7)mina⁡‖x−ΦLa‖22s.t.a≥0,where a sub-matrixΦLthat includes only a selection of the L largest coefficients is used. In the dictionary updating, this matrix is forced to be positive by calculating(8)minϕ→k,ak⁡‖Ek−ϕ→kak‖22s.t.ϕ→k,x→k≥0,for each one of the k selected coefficients. The error matrixEkis the residual between the signal and its approximation with the k-th atomϕ→kand its respective activationakbeing updated.The dictionary itself and the activation coefficients are calculated from the SVD ofEk=UΣVT. This decomposition is then truncated to null the negative entries. Finally, the atoms and activations are obtained as the rank-one approximation with the first left and right singular vector asϕk=u1andak=v1. The complete algorithm, called NN-K-SVD for short [38], is illustrated in Appendix A.The main idea of the proposed method is that sound and noise signals can be projected to an approximate auditory cortical space, where the meaningful features of each one could easily be separated. The signals being analyzed could be decomposed into more than one (possibly overcomplete) dictionary containing a rough approach to all the features of interest. More precisely, the method here proposed is based on the decomposition of the signal into two parallel STRF dictionaries, one of them estimated from clean signals and the other one from noise. The estimation of both dictionaries is carried out after obtaining the respective two-dimensional early auditory spectrograms for each type of signals, as was explained before. Given that this type of representation is non-negative, a natural way to obtain both the dictionary and the cortical activations is to use an algorithm that obtains a representation with non-negative constraints. This is especially true in the case of denoising applications, where forcing non-negativity on both the dictionary and the coefficients may help to find the building blocks of the different type of signals [38]. Among the several NMF models reported in literature (some of then summarized in Section 1), we chose for our purposes the above outlined NN-K-SVD.Before carrying out the denoising, the dictionaries corresponding to clean signals and noise should be estimated. They are produced applying twice the NN-K-SVD algorithm described in Section 2.3, one for each type of signal. The dictionaries are then rearranged according to the activation for the training samples, in descending order. From these two sets, a combined dictionary containing atoms of signal and noise is used in our approach. This new dictionary is composed by the “most representative” atoms of each previous dictionary, by selecting those with greater activation.Fig. 2shows a diagram of the method here proposed, which consists of two stages. In the forward stage (Fig. 1a), the auditory spectrogram is firstly obtained. Then, using the combined dictionary, the auditory cortical activations that best represent the noisy signal (including both clean and noisy activations) are calculated by means of the non-negative version of the BP algorithm. In the backward stage (Fig. 1b), the auditory spectrogram is reconstructed by taking the inverse transform from only the coefficients corresponding to the signal dictionary (synthesis). In this way, the denoising of the signal is carried out in the approximate non-negative auditory cortical domain. Finally, the denoised signal in the temporal domain is obtained by the approximate inverse ear model. The proposed method is named NNCD, which stands for non-negative cortical denoising.The reconstruction of the auditory spectrogram from the cortical response is direct because it only consists of a linear transformation. However, a perfect reconstruction of the original signal from the auditory spectrogram is impossible because of the nonlinear operations in the earlier described in Section 2.1. Shamma proposed a method to approximately invert the model and showed through objective and subjective quality tests that the resulting quality of this approximate reconstruction is not degraded [9].The idea of using a cortical model for sound denoising was also proposed by Shamma in a recent work [10]. The main differences with our approach are that his cortical representation uses the concept of spectro-temporal modulation instead of STRF and non-negative sparse coding, and also the way he incorporates information about signal and noise.We propose applying the NNCD in three different scenarios for denoising speech signals degraded by uncorrelated additive noise:(a)“NNCD speech”: corresponds to the NNCD reconstruction from selected atoms of the speech dictionary, discarding the noise selected atoms.“Wiener/NNCD noise”: applies a Wiener filter to the noisy signaly(t), where the noise estimationn′(t)is given by the NNCD reconstruction from only selected atoms of the noise dictionary.“NNCD+Wiener”: applies a Wiener filter to both previously NNCD estimations of noisen′(t)and speechs′(t).In cases (b) and (c), the Wiener filter is estimated by means of the Short-Time Fourier Transform (STFT), as|S(ω,τ)|2|S(ω,τ)|2+|N(ω,τ)|2. Here,S(ω,τ)andN(ω,τ)are the STFT representations ofs(t)andn(t)respectively. Note that in case (c), the Wiener filter is estimated from the speech signals′(t)instead ofs(t)[40,41]. Fig. 3shows the block diagrams of these configurations.For comparison purposes, different filtering algorithms were also implemented and tested:•iWiener: the iterative Wiener method [42]. After preliminary experimentation, the number of iterations was fixed at 4.apWiener: the speech enhancement based on the use of the A Priori Signal to Noise ratio in a minimum mean square error estimation, as given in [43].Wavelet: sound denoising using the thresholding of wavelet coefficients. The parameters of this process were: 5 levels of a Daubechies 8 function, soft thresholding using the unbiased SURE estimator and rescaling using a single estimation of level noise based on first-level coefficients [44].mBand: Multi-band spectral subtraction, a method that takes into account the fact that colored noise affects the speech spectrum differently at various frequencies [45]. The parameters of the algorithm were fixed at 6 frequency bands with a linear spacing between bands.BNMF: a recently proposed Bayesian formulation of nonnegative matrix factorization [33]. First, a mean square error estimator for the speech signal is derived, then it learns the NMF noise model online from the noisy signal (unsupervised speech denoising).Given the nature and characteristics of the artificial/real signals, the Wavelet denoising was used in the experiments with artificial signals, where mBand and BNMF were used in the experiments with speech data.A series of experiments were carried out to demonstrate the capabilities of the proposed technique. The first of this were carried out on artificial “clean” sound signals constructed by a mixture of chirps and pure tones. Then a second series of experiments were developed to work with real data consisting of speech signals of complete sentences from a single speaker. Noises with different frequency distributions and non-stationary behaviors were additively aggregated to the signals at several signal to noise ratios (SNRs). The proposed technique was then applied to obtain the denoised signals and the performance was evaluated by two objective methods: the perceptual evaluation of speech quality (PESQ) score [46] and the classical segmental signal-to-noise ratio (SNRseg) [47].A total of 1000 artificial signals were obtained by concatenating 7 different subsignal segments of 64 ms each at a sampling frequency of 8 kHz. Each segment consisted of the random combination of up or down chirps and pure tones. In order to restrict all the possible combinations of these features so that a relatively simple dictionary was able to represent them, the spectrogram was divided in two frequency zones, below and above 1200 Hz. Inside each zone only one of the features could occur. Also, the frequency slopes of the chirps are fixed in each zone. Experiments with this type of signals were designed just to illustrate the operation of the method, also for sanity check and to show the feasibility of the method.The clean speech data was extracted from a widely-used database in the speech recognition field, the TIMIT corpus [48]. The data used in this work corresponds to the set of 10 speech sentences of the speaker FCJF0 in dialectic region number 1. Sentences have a mean length of 5 seconds.Two kinds of noise with different frequency content were used. On the one hand, the white noise, which exhibits a relatively high frequency content with a non-uniform distribution in the early auditory spectrogram (due to its logarithmic frequency scale), and on the other hand voice babble and street noises with mainly low frequency content in that representation. The white noise was generated by an HF radio channel and the babble noise was recorded in a crowded indoor ambient, both taken from the NOISEX-92 database [49]. The street noise corresponds to an outdoor recording and was taken from the Aurora database [50]. In all the experiments, the noise was first conveniently resampled to the same rate and resolution of the clean signals. The noisy signals were obtained by additively mixing the signals at different SNRs.First, the auditory spectrograms of clean signals were obtained. Then, the training data for the estimation of the dictionaries was extracted by means of a sliding time-frequency windowing using frames of 64 ms in length with an overlapping of 8 ms.The dictionaries were generated using complete dictionaries. For the artificial data, 512 atoms of size64×8were calculated. Here, the 64 coefficients correspond to a downsampled version of the original 128 coefficients representing the range 0–4 kHz, while the 8 columns correspond each to a window of 8 ms. For speech data, based on preliminary experiments, the number of columns was reduced to 4, given that with 8 windows the dictionary learning process becomes computationally very intensive. Thus, in this case, the dictionaries have 256 atoms of size64×4.For the artificial data, 1/10 of the total number of signals was used as training data (100 random selected chirp signals). For the estimation of noise dictionaries, the same ratio of 1/10 was used as the balance of training/test data. For the speech sentences, a 10-fold leave-one-out method was applied, where each partition consisted on 9 sentences for train and 1 sentence for test.From each dictionary, the most active atoms were collected. Then, they were combined to form new dictionaries with atoms containing both clean and noisy features. The reported results consist of the mean value obtained for the 10 partitions.For the speech denoising experiments, two well-known objective speech quality measures were evaluated: the PESQ score and the segmental signal-to-noise ratio (SNRseg).The PESQ score is an objective quality measure introduced by the International Telecommunication Union (ITU) as a standard for evaluation of speech quality after transmission over communication channels [46]. It uses an auditory representation based on bark scale to compare the original and distorted speech signals. It has been shown to be very well correlated with perceptual tests using mean opinion score (MOS) [51] and robust automatic speech recognition results [52]. The measure has an ideal value of 4.5 for clean signals with no distortion, and a minimum of −0.5 for the worst case of distortion.The segmental signal-to-noise ratio is another quality measure here evaluated. It was obtained as the frame-based average SNR value calculated from the original and the processed signals. Here, short segments of 15–20 ms are used (instead of the whole signals). This time domain measure was computed as in [47], using the MATLAB code provided in [53].

@&#CONCLUSIONS@&#
A new denoising method of audio signals was presented, inspired by the biological processing carried out at the primary auditory cortical level. The method obtains a sparse coding of the spectrogram at cochlea level using a non-negative approach. The atoms of the dictionary are calculated from clean signals and noise. Then, the denoising signal is obtained by inverting the model using only the atoms corresponding to the signal, discarding the noise activations.The performance of the method using synthetic and real signals with additive noise was obtained through two objective quality measures. Results showed that our proposed method and its variants can improve the quality of sound signals, specially under severe conditions.Future research will be devoted to further improve the performance and also investigate the application of this technique in the preprocessing stage of robust classification systems.