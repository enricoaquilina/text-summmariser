@&#MAIN-TITLE@&#
Predictive analytics model for healthcare planning and scheduling

@&#HIGHLIGHTS@&#
We model appointment no-show behavior based upon a patient's past history.We determine how much of a patient's past history is necessary to provide insight.Patients with a history of non-compliance tend to repeat that pattern in the future.We prove that our model produces globally optimal estimates.Our model is well-suited for Big Data applications.

@&#KEYPHRASES@&#
Analytics,OR in health services,Predictive modeling,Outpatient appointments,No-show modeling,

@&#ABSTRACT@&#
Patients who fail to attend their appointments complicate appointment scheduling systems. The accurate prediction of no-shows may assist a clinic in developing operational mitigation strategies, such as overbooking appointment slots or special management of patients who are predicted as being highly likely to not attend. We present a new model for predicting no-show behavior based solely on the binary representation of a patient's historical attendance history. Our model is a parsimonious, pure predictive analytics technique, which combines regression-like modeling and functional approximation, using the sum of exponential functions, to produce probability estimates. It estimates parameters that can give insight into the way in which past behavior affects future behavior, and is important for clinic planning and scheduling decisions to improve patient service. Additionally, our choice of exponential functions for modeling leads to tractable analysis that is proved to produce optimal and unique solutions. We illustrate our approach using data from patients’ attendance and non-attendance at Veteran Health Administration (VHA) outpatient clinics.

@&#INTRODUCTION@&#
In this paper, we present an analytical model for predicting the success of the next outcome of a binary time sequence, where the outcome, success or failure, is the result of human behavior. Our model is the result of the consideration of patients' attendance or non-attendance at a wide variety of medical and surgical outpatient clinics, where outpatient attendance – one example of such a time sequence – is of significant concern. A patient not attending an appointment, a no-show, is disruptive to a clinic, may cause access and scheduling issues because of its effect on clinic capacity, and may increase the cost of clinic operation. Healthcare facilities have the potentially conflicting objectives of providing high-quality service and reducing costs, and the identification and reduction of no-shows assists with both of those objectives (Glowacka, Henry, & May, 2009; LaGanga & Lawrence, 2007). No-show rates vary, but have been reported to range from 3 percent to 80 percent (Rust, Gallups, Clark, Jones, & Wilcox, 1995).The presence of no-shows has also impacted the healthcare scheduling literature. Outpatient clinics fall under a class of service operations that are affected by customer non-attendance (LaGanga & Lawrence, 2012). Cayirli and Veral (2003) list the prediction of no-shows as one of the three major decision levels in a scheduling system. Zeng, Turkcan, Lin, and Lawley (2010) state that a no-show model that accurately captures patient behavior is the first step in developing an overbooking scheduling schema. While the importance of identifying individual patient no-shows is recognized, scheduling models that incorporate the presence of no-shows typically use an average no-show rate for all scheduled appointments (LaGanga & Lawrence, 2012; Zacharias & Pinedo, 2014), or no-show probability based upon appointment lead time (Liu, Ziya, & Kulkarni, 2010). LaGanga and Lawrence (2012) use a no-show rate that may differ for each day, and Zacharias and Pinedo (2014) assign a high or low no-show rate for each patient. Both articles remark that the schedules produced are improved by permitting heterogeneity in no-show rates. Berg, Denton, Erdogan, Rohleder, and Huschka (2014) create an outpatient clinic scheduling model that allows for individual patient no-show probabilities, and find that their inclusion adds more volatility to the scheduling structure. The recognized importance of accurate no-show prediction, for operational planning and scheduling in healthcare and similar service environments, motivated us to build a predictive analytics model to do such predictions.Prior modeling to predict no-shows ranges from rule-based methods (Glowacka et al., 2009) to logistic regression (Daggy et al., 2010; Huang & Hanauer, 2014). The models typically include patient demographic variables, appointment characteristic variables, and a variable representing a patient's past history. Glowacka et al. (2009) use an indicator variable which represents patient attendance for the past appointment. Additional representations include prior no-show rate over a horizon (Daggy et al., 2010), or count of previous no-shows (Huang & Hanauer, 2014). In all models, the prior history variable is found to be significant. Our model focuses on modeling a patient's past history, in an effort to refine the way it is included in a prediction model.Our model uses past sequences of successes and failures, over a limited historical horizon, in a regression-like approach, to predict the probability of a success on the next occurrence. Human beings tend to repeat behavioral patterns, but those patterns may change over time. More recent behavior is likely to be more salient than prior behavior, and, after some time, past behavior may no longer be relevant for predicting the future. We show how to estimate the parameters of such a model. Because the complexity of our methodology is a function of the length of the history included, not of the size of the data set, it is particularly useful for “Big Data” applications.In this article, we focus on no-show predictions to inform planning and scheduling decisions, but our model is relevant in any service environment that is affected by customer non-attendance or non-participation. Examples of such applications are responses to charitable solicitations, such as the one considered by Fader, Hardie, and Shang (2010), changes in employment (Mehran, 1989), prediction of recessions (Startz, 2008), and airline no-show rates (Lawrence, Hong, & Cherrier, 2003), among others.We numerically demonstrate the generalizability of our approach using two real data sets: one extracted from outpatient appointment records, and the other involving charitable solicitations. Our approach provides insight into the length of historical behavior that influences future behavior, and the relative importance of each of the observed outcomes in that historical record. In general, we found that the sequence of past successes is important for recent behavior, although the importance of the particular ordering of successes and failures may decrease as outcome recency decreases.The remainder of the paper is organized as follows. Section 2 includes a review of the related research. In Section 3, we present our model. Section 4 describes the datasets used for analysis, the model results, and comparisons. Section 5 has a discussion, summary, and directions for possible further research.Predicting no-shows based upon past historical values involves the analysis of binary data sequences, so we provide a brief review of the literature on that topic. Approaches to modeling binary data include Markov models (Cox et al., 1981; Berchtold & Raftery, 2002) and moving average approaches that incorporate generalized linear models (Zeger & Qaqish, 1988; Li, 1994; Startz, 2008). The simplest Markov models consider only the current state in describing future behavior. If it is assumed, as in our model, that outcomes earlier than the present one are necessary for accurate prediction of future occurrences, a higher-order Markov chain can be developed (Cox et al., 1981), which is subject to the curse of dimensionality (Startz, 2008; Prinzie & Van den Poel, 2006).Cox et al. (1981) provides a review of the literature on time series, and proposes several examples of “observation-driven” models, in which the conditional expectation of the present depends explicitly on past data. He proposes that binary data be analyzed using an observation-driven linear logistic regression model, which Startz (2008) terms BAR(p). The BAR(p) model uses a logit model, and has p+1 parameters – a constant value and p lagged values. The BAR(p) model is attractive, as it is linear, and its parameters may be estimated using logistic regression, but Cox states that it may not be suitable for data with long-range effects. Zeger and Qaqish (1988) extend the BAR(p) technique, an extension that Startz labels the BARX(p) model, to include cross-terms for all p lagged values, and a potential for substituting covariate terms for the constant value. Startz states that, while the BARX(p) model provides a starting point for moving away from traditional Markov models, it does not perform well when transitions are on the “edge of permissible space” (Startz, 2008), that is, transition probabilities that are 0 or 1. Li (1994) proposes another variant of the BAR(p) model, the BARMA(p,q) model, which adds moving average terms. The BARMA(p,q) model is a focus of (Startz, 2008). Startz finds that the BARMA(p,q) model performs better than traditional Markov models when predicting U.S. recessions. We build on the autoregressive nature of those models, and include the use of exponential sums to enhance predictions.Our formulation differs from a typical autoregressive model in the distribution of the errors, model evaluation techniques, assumptions, and the amount of data needed for model evaluation. A BAR(p) model, as described in Startz (2008), is similar to a logistic regression where the errors are assumed to follow a logistic distribution. The parameters are estimated using techniques such as quasi-maximum likelihood, with no closed form solutions for the coefficients. The data are collected sequentially through time, and the model assumes that the data are equi-spaced. Our model is analogous with a least squares regression, where the errors are assumed to be normally distributed and the coefficients can be directly solved. We do not consider the spacing of the collected data. Additionally, a BAR(p) model requires more data to generate parameter estimates. Box, Jenkins, and Reinsel (2011) state that a minimum of fifty data points is preferred when building an autoregressive model. For applications such as outpatient appointment no-shows and charity donor solicitations, obtaining 50 historical data points for each person is highly restrictive, and thus many people would be excluded from the model. Our model requires a person to have one more data point than the lag number being modeled. This allows for people with one occurrence to still be included in the analysis. Because the use of a BAR(p) or a BARMA(p,q) model would exclude the majority of our dataset due to the length of data needed to tune the model, we do not directly compare the results of our model with these models.Two additional models that predict binary data without an exponential increase of parameters are the Mixture Transition Distribution (MTD) model, introduced in Raftery (1985), and the beta-geometric/beta-Bernoulli (BG/BB) model, from Fader et al. (2010). We refer to those models in depth because of their salience, and because of their application to service industries.The MTD model seeks to predict the next outcome of a binary variable based upon past history. It produces an m×m transition probability matrix (TPM), where m denotes the number of states, and a vector of lag parameters that allows for each lag to be weighted separately. Probabilities of success are calculated by multiplying the transition probability at each lag with the lag weight, and adding across all lags. This approach is more parsimonious than is a Markov model; the number of parameters is m(m−1)+(l−1), where l is the number of lags in the model. The MTD parameters can be solved using approaches such as maximum likelihood estimation algorithms, minimum χ2estimation, or expectation-minimization (EM) algorithms. Extensions to the MTD model are discussed in Berchtold and Raftery (2002), and allow for the use of distinct transition matrices for each past occurrence (MTDg model), and an infinite length history (Mehran, 1989). Applications of the MTD model include employment data (Mehran, 1989), financial services (Prinzie & Van den Poel, 2006), and non-Gaussian time series (Berchtold & Raftery, 2002).We seek to improve on the MTD approach in several ways. First, due to the iterative nature of the algorithms required to solve for the MTD's parameters, in some cases, an optimal solution may not be reached (Berchtold & Raftery, 2002). We believe that a guarantee of optimality is attractive in a prediction setting, especially when predictions may be used to induce operational change. In Section 3, we demonstrate the optimality and uniqueness properties of the solution to our model. Second, the MTD model is not easily implementable. A software program, MARCH, is available online (Berchtold, 2005). However, the performance of MARCH deteriorates as the dataset size increases. For example, running the program on a dataset of 473,144 records with nine lags required one hour of CPU time on a high-end desktop computer, indicating that the time involved might be prohibitive on data sets as large as our complete outpatient data file, which has over five million records. “Big Data” applications may well involve even more than five million records. We believe that it is advantageous to create an approach that can be implemented using spreadsheets software, and for which the computation time is not a function of the size of the data set.Fader et al. (2010) proposed a Bayesian technique, which they term the beta-geometric/beta-Bernoulli (BG/BB) model, for responses to solicitations by charities. Their approach assumes that the probability of a success (meaning a donation) and the probability of a “death” (a donor becoming permanently inactive) are heterogeneous, and follow a beta distribution. The model uses a binary representation of giving history to tune the model. It assumes that historical sequences with the same number of successes (frequency) and the same last success (recency) produce the same probability for the next outcome. That is, if the history of the system is written with the most recent trial on the left and the least recent on the right, the sequences 11100 and 10101 produce the same probability of success on the next trial.The BG/BB model is attractive, because the number of parameters to be estimated is the same for any value of k – two for the beta-geometric element and two for the beta-Bernoulli element – and because of its concise representation of the binary time series sequences. Our technique differs from the BG/BB model in two significant aspects. One, our model incorporates structures for capturing situations in which the impact on the future of past occurrences diminishes with increasing time, with greater impact for the more recent occurrences. The BG/BB model is not able to detect such effects. For example, in the sequences mentioned in the above paragraph, if the impact of a success on the second occurrence is large compared to the impact of a success on the fifth, our approach would predict that the sequence 11100 is more likely to be followed by a success than is the sequence 10101. The BG/BB model must assign equal follow-up-success likelihood to both. Two, our formalism incorporates structures that allow for the direct interpretation of the relative effect at each lag, which we believe is integral to a prediction model. The BG/BB model does not have such structures.Based on our review of the literature on patient attendance in healthcare applications, and on a study of our outpatient data, we anchor our model on two assumptions. While these assumptions may seem implicit in a model that predicts future behavior, we believe that building a model grounded on these assumptions allows us to tailor the model to human behavioral applications such as appointment no-shows.Assumption 1Past history is an important determinant of future no-show behavior.A plethora of literature exists on how patient demographics or appointment characteristics affect no-show behavior. Variables typically identified as being significant include age, gender, appointment lead/delay time, and the number of previous appointments (Bean & Talaga, 1995; Garuda, Javalgi, & Talluri, 1998). Although an individual's past attendance history has been found to be the most significant determinant of future no-show behavior (Goffman et al., 2016; Garuda et al., 1998; Daggy et al., 2010), past history is not usually represented explicitly. It is typically operationalized as an input variable, usually as an indicator variable for most recent appointment status (Glowacka et al., 2009), or as the fraction of appointments that have been no-shows (Daggy et al., 2010) A predictive model for outpatient no-shows, such as the one described in Goffman et al. (2016), is based on modeling components beyond those incorporated in our model.Assumption 2The sequence of past no-shows, i.e., the order in which they occurred, may be a significant factor in determining the probability of the next no-show.A more parsimonious model might assume that sequences can be grouped based upon total number of successes (no-shows) or time of last success (no-show). Current research has found that the number of previously made appointments assists in predicting no-shows (Cosgrove, 1990), with no mention of the ordering of the successes through time. From preliminary analysis of our dataset, we find that the ability of the model to allow for varying importance of at least the most recent lags is essential to model accuracy. As an example, for patients with 5 appointments and 3 successes (no-shows), with a success on the most recent occurrence (the digit on the far left of sequence), the no-show probability ranges from 0.343 for sequence 10101 to 0.449 for sequence 11100.Approximating an arbitrary function by a sum of exponential distributions is an established concept (see, for example, Beylkin & Monzon, 2005; Beylkin & Monzon, 2010). It has been shown that a finite linear combination of exponential functions constitutes a dense set in the space of continuous functions, and may be used to represent many physical processes such as exponential decay (Pereyra & Scherer, 2010) and hospital length of stay (Vasilakis & Marshall, 2005; Xie, Chaussalet, & Millard, 2005). To model k historical sequences with exponential functions, we begin with a general exponential function as in (1)(1)f(k)=∑j=0kzje−λjk,where zj∈ ℝ are decay amplitudes and the λjare decay rates. If we seek to model with an intercept term, Eq. (1) becomes(2)f(k)=z0+∑j=1kzje−λjk.Solving for z0, zj, and λjvalues leads to a nonlinear least squares problem. Several algorithms have been developed to solve for the parameters, using techniques such as singular value decomposition (SVD) and ordinary least squares (OLS), both of which lead to good approximations (Pereyra & Scherer, 2010). Because we seek to model probabilities with a model that can be solved to optimality, we work with a modified version of Eq. (2).Our objective is to predict the probability of success on the next occurrence of a Bernoulli process that has a non-constant probability of success. The prediction is based solely on a fixed window of past occurrences of the process; the width of the window is denoted by k. Because there are two possible outcomes at each time period, and there are k prior time periods, there arei=2kpossible k-period sequences of zeroes and ones. We denote a success at time period t byXt=1and a failure at time period t byXt=0. The predicted probability of a success at time t, given the history of the successes and failures over the k prior time periods is denoted by(3)p^ik=P^(Xt=1|Xt−1,Xt−2,Xt−3,...Xt−k).We want to estimatep^ikusing a sum of exponential functions, as in Eq. (2). We assign the decay amplitudes, zjof Eq. (2), as the zeroes and ones that represent the past history of the process. We denote those asxijk,j=1,...,k, which represent a success or failure on the jth past occurrence of the ith historical sequence when sequences are of length k. In Eq. (2) the decay rates at each lag, λj, are multiplied by the lag number, k, which produces λjestimates that are scaled by the lag number. We remove this relationship, to allow the λjto be on the same scale and to be directly comparable. Considering the adjustments to Eq. (2) just described, we estimatep^ikby(4)p^ik=z0k+∑j=1ke−λjkxijk.For each possible sequence in the history of length k, denote by vikthe proportion of the observations that have the historical sequence i, and by pikthe proportion of those observations that were followed by a success on the next occurrence. For a given value of k, to solve for the intercept z0kand theλjk,j=1,..,k, we use the technique of weighted least squares, and minimize Fk, where Fkis given by:(5)Fk=∑i=12kvik(pik−z0k−∑j=1ke−λjkxijk)2.The squared errors are weighted by the vikto account for the frequency of each sequence in the dataset, and in order to permit successes and failures in the population to have different likelihoods. For a look-back window of width k, there are k+1 variables to solve for in the model of Eq. (5). To ensure that the model produces values that may be interpreted as probabilities for all sequences, the minimization of (5) is performed subject to the following constraints:(6)z0k≥0(7)0≤z0k+∑j=1ke−λjk≤1.Eq. (5) allows each lag to have a unique coefficient, and, therefore has k+1 decision variables. For some datasets, it could be optimal to have fewer decision variables, and allow for the coefficients, after some point in the look-back window, to be identical. Modeling the data in this way indicates that, after some point in the past, the total number of successes is sufficient to provide insight; the ordering is not necessary. This allows for the estimation of fewer decision variables, and for the data to be divided into fewer data sequences. To account for groups of lags that have the same effect, we add an additional constraint to (5) that allows a block of the λjkvalues to be equal. We refer to the number of distinct λjkvalues as k′, where k′ is a value between 1 and k. For each k, we generate k models denoted byFk′,k. For example, when k=3, we predict the three models shown in (8).(8)F1,3=∑i=123vi3(pi3−z03−e−λ13∑j=13xij3)2F2,3=∑i=123vi3(pi3−z03−e−λ13xi13−e−λ23∑j=23xij3)2F3,3=∑i=123vi3(pi3−z03−∑j=13e−λj3xij3)2When k′ is equal to k,Fk′,kis equal to Fk, and all the λjkare distinct. When k′ is equal to one, all the λjkare equal, and a success at any lag contributes the same amount to the predicted probability of a success at time t. To determine the optimal k′ value for any value of k, we use a BIC equation tailored for regression, as suggested by Burnham and Anderson (2002), where(9)BIC=nln(SSEn)+(k+2)ln(n)The coefficient of ln(n) is k+2, to account for the estimation of the intercept and the estimation of the model variance. We use the BIC metric because the Weighted Sum of Squared Error (WSSE) is not adjusted for the number coefficients that have been estimated, so WSSE decreases monotonically as k′ increases. We refer to the model of Eqs. (5)–(7) as Sums of Exponentials for Regression (SUMER), because ourp^ikvalues are estimated using a regression model, where the coefficients of the regression are modeled by exponential functions.For any given value of k, taking the first partial derivatives of (5) with respect to z0kand to thee−λjk,j=1,..,k, and setting them equal to zero, yields a system of k+1 linear equations in k+1 unknowns. To solve for z0kande−λjk,j=1,..,k, in general, it is necessary to solve a linear system of the formAksk=bk, where skis the coefficient vector, and Akis the Hessian of the objective function Fkin (5). The matrix Akis positive definite (Theorem 3.1, below), so Akis invertible. Thus, the system of linear equations can be solved by Cramer's Rule, andsk=Ak−1bk.Because constraints (6) and (7) are linear, SUMER is a convex optimization problem. When Akis positive definite, the objective function of (5) is strictly convex, and skis a unique global minimizer (Griva, Nash, & Sofer, 2009). Theorem 3.1 below shows that when at least one sequence is represented in a dataset, Akis positive definite. Because the reduced parameter models are equivalent to adding equality constraints to the original model, proof of uniqueness of the original model holds for all reduced parameter models.Theorem 3.1If vik> 0 for at least one i, then the matrix Ak is positive definite for all values of k.ProofSee the Appendix.Corollary 3.1All sk are global minimizers of Fk.ProofFollows because the Hessian matrix Akis positive definite for all k.Because the xijkvalues are binary, SUMER estimates the probability of success at time period t by adding exponential terms for every time period at which there was a success. We chose to use the exponential distribution to model the coefficients because the parameters and model are easily interpretable. The value of z0kis the predicted probability of success on the next occurrence, when all past occurrences have been failures. Intuitively, as k increases, z0kshould decrease, and approach zero from above, i.e.,z0k→k→∞0+. Thus, as the history of all failures becomes longer, the lower the predicted probability of success should be on the next occurrence, as the person has established a more consistent pattern of failures.The termse−λjk,j=1,..,kare comparable to the typical “beta” coefficients in a regression. They represent the change in the probability of success between a success and a failure at lag j when all other lags are held constant. If we assume that more recent behavior is likely to be more salient than prior behavior, we would expecte−λjkto monotonically decrease as j increases. A monotonic decrease would indicate that more recent successes have a greater impact on the probability of success at the next occurrence. Becausee−λjkcan never be negative, a success at lag j will always increase the probability of a success on the next occurrence; failures contribute no value to the probability of success. Modeling when this assumption does not hold is an extension of the model outlined in Section 3.2.3.While we can gain historical insight from thee−λjkvalues, we can gain additional insight by interpreting the decay rates, λjk. For a given value of k, λjkrepresents the rate at which a success at time j produces a success at time t. By Eq. (7), λjkwill always be greater than or equal to zero, becausee−λjkis greater than one for λjk< 0. The greater the value of λjk, the faster the effect of a success at lag j decays and approaches zero. So, for larger λjk, lag j is less relevant to the outcome at time t. There is a different vector of rates for each look-back window. The rate vectors for any two values of k are assumed to be independent, and are modeled with separate exponential distributions. For a fixed value of j, as the length of the look-back window increases, we expect the coefficiente−λjkto decrease, because there are more historical time periods contributing to the probability estimate, that is, for a given value of j, we expect λjkto increase as k increases.SUMER, as described above, incorporates the assumption that past successes (ones) in the historical sequences will have a positive effect on the probability of success in the future. SUMER generates a probability prediction by adding the predicted probability for the sequence of all past failures, z0k, with the coefficients, which will always be positive. If, as an example, the probability of success for the all failure sequence is greater than the probability of success for the all success sequence i.e.,p1,k>p2k,k, then SUMER must try to estimatez0k>(z0k+∑j=1ke−λjkxijk), which will produce estimates for all sequences equal to z0k. Datasets for which it would be necessary to predictp1,k>p2k,kare datasets where there is a “ping-pong effect”, and a success (one) at a lag reduces the baseline probability of success, z0k, as opposed to increasing it. A donation dataset with donors who contribute sporadically, rather than consistently, might cause such an effect. For such persons, once they have contributed, they do not contribute again until, perhaps, their charitable budget has been replenished.To permit SUMER to accommodate such datasets, an additional set of parameters is applied at each lag to allow lags to have a negative effect. Eq. (10) shows the extended model with the additional parameters, αjk.(10)∑i=12kvik(pik−z0k−∑j=1kαjke−λjkxijk)2Additional constraints,−1≤αjk≤1ande−λjk>0, are required to estimate the parameters. The constraints on the αjkare added to bound the parameter space. Because they are bounded to be between −1 and 1, we can interpret them as proportions, as described below. The second constraint is analogous to Eq. (7) above. Given that an additional set of parameters, αjk, have been added, the first-order conditions are now nonlinear functions. We solve the model using an iterative algorithm.Table 1 displays a sample dataset, along with the BG/BB, MTDg, SUMER and Extended SUMER predictions. The addition of the αjkparameters noticeably improves SUMER's performance, and permits probability estimates less than z0kto be generated. The extension to SUMER is therefore important in permitting it to model “ping-pong” behavior.Table 2 lists the parameters generated from each model. The original SUMER model assigns a coefficient of zero to each of the past occurrences. If we interprete−λjkas the amount the baseline probability changes with a success at lag j, αjkis the percentage ofe−λjkthat is used to change the baseline probability. Whenαjk=−1, 100 percent of the predictede−λjkvalue decreases the baseline probability when there is a success at lag j. When−1<αjk≤0, then (100*αjk) percent of the predictede−λjkvalue decreases the baseline probability when there is a success at lag j. When 0 < αjk≤ 1, a success at lag j still increases the baseline probability at the rate λjk. Because the model is designed with this case in mind, the increase will typically be modeled with the rate parameter, λjk, and the associated αjkwill be one. For example, for datasets where the αjkparameter is not necessary, the generated solution vector for λjkusing the extension is identical to the solution vector when SUMER is utilized, and allαjk=1.SUMER is a generalization of Extended SUMER, with all αjkassumed to be one. An analyst can determine if Extended SUMER is necessary by analyzing the table pikvalues. When p1kis greater than any other probability, Extended SUMER should be implemented.If the same amount of historical data is available for all cases in the data, then, after a k′ model has been chosen within each k, it is possible to compare the chosen models across k to determine the optimal width of the look-back window. In this section, we present an approach to select that value of k. We assume that the optimal k value will be chosen based upon analysis of a training dataset. For new cases with historical sequences of length less than k, the approach of this section could be used to rank-order the attractiveness of the models that are feasible for that case. At least one historical sequence is needed to use SUMER as an analytical tool. New patients with no history could be assigned an average probability of success, or a probability of success based upon patient demographics and appointment characteristics.In Section 3.1, we showed how to choose the optimal k′ for each k. In general, we use the BIC metric of Eq. (9) to determine the optimal width of the look-back window, k. We also develop a heuristic to choose k when the BIC values do not achieve a minimum value in the interior of the range of look-back windows considered.Using a BIC criterion, the optimal value of k is the smallest k value for whichBICk≤BICk+1. Our empirical experience shows that for very large values of n, the small decrease in SSE that is achieved by increasing k, results in a small decrease in BIC, and BIC steadily decreases across all k. Thus, it is preferable to choose k based on achieving a sufficiently large decrease in SSE, rather than by requiring only a decrease in BIC. Such an approach is also used in clustering routines to determine the optimal number of clusters (Chiu, Fang, Chen, Wang, & Jeris, 2001).From Eq. (9),BICk<BICk+1, whenln(SSEk)−ln(SSEk+1)<ln(n)n, or whenSSEkSSEk+1<exp(ln(n)n).When n is large,ln(n)nbecomes very small, andexp(ln(n)n)≅ 1. In this situation, even a minimal decrease in SSE will result in a decrease in BIC, and the SSE is expected to decrease with an increase in k. To implement the heuristic, we stop increasing k when there is not a sufficient change in SSE when going from k to k+1, that is, whenSSEkSSEk+1<1+δ, where 0 ≤ δ ≤ 1 is a user-specified parameter.We used data sets from two different service operational environments to assess the performance of SUMER on real data. The first data set is the charity donor data from Fader et al. (2010) which was used to validate the BG/BB model. The second data set was extracted from the attendance of outpatients at a Veterans Health Administration (VHA) facility. We denote the charity donor data by DO and the outpatient dataset by OP.DO includes data collected from 1996–2006, and includes 11,104 records. For each solicitation, a donation is coded as 1 and a non-donation as 0. The data were split into training and test datasets. The training dataset consists of historical donation activity from 1996–2004 to predict the donation activity in 2005, so a look-back window may range from one to nine. The model was tuned on the training dataset, an optimal k value between one and nine was chosen, and the model was tested on the activity of 2006 based upon the optimal- k past occurrences.First, models were run on the training data for all k′ and k combinations to determine the preferred model. With a maximum k value of nine, there were9(10)2=45models to tune. SUMER was programmed in Wolfram Mathematica 10, and solutions for the 45 models were found in 49.67 seconds. Solutions for a single model were solved in less than 2 seconds. Table 3shows the optimal k′ value chosen at each k and the calculated BIC value. TheBICk≤BICk+1at k=8; thus, the optimal width of the look-back window for DO is eight.Table 4lists the decay rate (λjk) and the coefficient (e−λjk) values for the model when k=8 and k′=5 (see Appendix for the variable values from all models). For lags five through eight, the total number of donations, and less the ordering of the donations, is significant for predicting the probability of success for the next occurrence. As an example, the probabilities of success for sequences 10000110 and 10000011 would both be equal to 0.4286, because they have two donations in years represented by lags five through eight. The intercept value indicates that if a person has failed to donate during the eight prior years, the probability that he or she will donate in the current year is 0.0073. In contrast, a person who donated in each of the past eight years will donate in the current year with probability 0.8428. That value is calculated by adding the coefficient values across all lags.As expected, the coefficients decrease as j increases. That decrease causes the most recent lags to have more of an effect on the outcome at time t. Additionally, holding the lag value constant, the λjkincrease as k increases, up to k=8 (this can be seen in the values in the Appendix). Lag 1 is the most influential lag, with a coefficient value that is 44 percent of the largest possible probability, and almost twice as large as the coefficient value at lag 2. The probability of giving decreases from 0.8428 to 0.4690 when a person did not donate the previous year. A person is 3.33 times less likely to give in the next solicitation if he or she gave in lag years three through eight, but not in lag years one and two. Lags five through eight, collectively, contribute only 11 percent to the largest possible probability, thus soliciting people who have only donated within that timeframe would prove to be less fruitful. As a final insight, we find that, while the frequency of successes is sufficient for analysis within lags five through eight, this does not hold throughout lags two through four. This is evidenced when looking at the predicted probabilities for sequences 11100000 and 10000011, which are 0.6852 and 0.4286, respectively. While both of these sequences contain a donation in the most influential lag, knowledge of the timing of the other two donations increases the probability for sequence 11100000 by 59.8 percent.To evaluate the model's performance, we use the model at the optimal value of k, and calculate the probabilities of donation in 2006 based upon 1998–2005 historical data, and the area under the ROC curve (AUC). We compare the AUC for our model with the AUC derived from the empirical probabilities used to tune the model. We provide this comparison because the empirical probability values are easily calculated, they are typically utilized when a model is not available, and they may provide a contrast between utilizing a descriptive analytics technique and a predictive analytics technique. To compare AUCs, we use the nonparametric approach of DeLong, DeLong, and Clarke-Pearson (1988), which detects differences among two or more models based on the areas under their ROC curves. We use the DeLong et al. method because the ROC curves for the models are correlated, as they were applied to the same dataset.The AUC for SUMER equals 0.9160, and the AUC for the empirical probabilities equals 0.9018. Using the DeLong method, the SUMER AUC is statistically greater than the empirical AUC (p<0.0001), thus SUMER is the preferred prediction model for the DO dataset.As an additional evaluation of model performance, we compare the expected number of people who are predicted to make zero to eight donations, as calculated from SUMER at k=8 and k′=5, with the actual number from the test dataset. Fig. 1 illustrates the comparison. The pattern of the actual distribution indicates that, as the number of donations increases, the number of people who donate increases. Given the properties of a regression model, the total expected donations will equal the total actual donations, thus, the SUMER estimates balance across all donation levels.OP is derived from the show/no-show behavior of patients at a Veterans Health Administration (VHA) facility from Fiscal Year 2007 to Fiscal Year 2012. A maximum of sixteen past appointments for each patient were tallied, with a total of 4,760,733 appointment sequences generated. The MTDg model required more than 72 hours to estimate parameters for the model with all 4,760,733 records. Thus, a subsample of 473,144 sequences was used to train all of the models, to allow for the comparison with the MTDg model. The training dataset consists of appointments one through fourteen to predict the no-show on the fifteenth appointment. We tested the model on the no-show realization of the sixteenth appointment. For a maximum k=14, there were14(15)2=105models to run. The program took 2506 seconds in Wolfram Mathematica 10 for all 105 models and under 3 seconds for the model when k=9. Table 5lists the optimal k′ value chosen at each k and the calculated BIC value.The BIC values in Table 5 are all negative, and decrease steadily as k increases from one to fourteen. To determine the optimal value of k, we use the heuristic from Section 3.3 and calculateδ=473,144*0.0000001=.0473. At that value, we select k=9, with k′=5. Similar to the DO data, lags five through nine have the same increase on the probability of success. Table 6lists the decay rates and the coefficients for the preferred model.There appear to be similar trends in the coefficients in both datasets. In OP, appointments that are more recent have a greater effect on the probability of a no-show at the next appointment than do less recent appointments. The probability of a success following a history of all failures – a patient showing up for all past appointments – is greater in OP than it is in DO, and the probability of a success following a history of all successes sequence is less in OP than it is in DO. The most recent occurrence is also significant for OP. If it is a success, it contributes 30.7 percent to the maximum possible probability of a success on the next occurrence. The second most recent outcome has less weight; the probability of a success on the next occurrence is only 1.7 times less if a patient has no-showed for all appointments as compared with no-showing for all but the most recent two. Lags two through nine have similar coefficients, ranging from 7 percent to 10 percent of the total possible probability on the next occurrence. As a result, sequence difference in occurrences two to nine time periods previous do not result in noticeably different predicted probabilities of success on the next occurrence. For example, the increase in probability between sequences 111000000 and 100000011 is 10.3 percent, even though the timing of the successes, except for the most recent one, is as different as possible, and the sequences have the same number of successes.The AUC values for SUMER and for an empirical probability table on the sixteenth appointment, based upon behavior of the seventh through fifteenth appointments, are 0.7064 and 0.7066, respectively. The empirical table has a greater AUC by 0.000195; but that difference is not statistically significant at the α=0.5 level. Thus, we conclude that SUMER is preferred to an empirical table for this dataset also, given the insight provided by the parameter values.Fig. 2 depicts the number of people who are predicted to have a particular number of no-shows over nine periods versus the number of people who actually had that number of no-shows, for the OP dataset. The overall pattern in Fig. 2 is the opposite of the pattern in the DO dataset; the number of people who no-show is inversely related to the number of no-shows. SUMER, with k=9 and k′=5, follows this pattern and balances out the expected number no-shows across the nine periods.As expected, the decay rates and coefficients for the two datasets differ. There are several differences in the datasets that can cause these contrasts. First, the success rate is greater in the DO data set than it is in the OP data set. For DO, the success rate is 23 percent in the training set and 17 percent in the test set. For OP, the success rate is 8.8 percent in the training set and 8.9 percent in the test set. A greater overall success rate results in greater coefficient values, and can lead to a greater number of influential lags. The DO dataset has two influential lags, both of which have greater coefficient values than the most influential lag in the OP dataset. The total overall probability in the DO data set is 67.8 percent greater than the corresponding probability in the OP dataset. While the difference in the width of the optimal look-back window also contributes to differences in coefficient sizes, a similar pattern holds comparing the values for k=8 for both datasets. The greatest predicted probability of success that the model can produce using the OP coefficients is 0.5717. That value is produced for a patient who has missed all nine previous appointments. Because patients, as a whole, typically attend medical appointments, the data records show that even a person with a poor recent attendance record still has a substantial probability of showing up for his/her next appointment.For both DO and OP, the sequence with the greatest frequency is the all failure sequence. For DO, the sequence 00000000, meaning that the contacted person did not donate on any of the eight previous solicitations, contains 38 percent and 44.2 percent of the total data for the training and test sets, respectively, with a probability of donation on the ninth occurrence equal to 0.009. For OP, the sequence 000000000, meaning that the patient attended all nine previous appointments, contains 57.2 percent of the total data, with a 0.045 probability of non-attendance (success) on the tenth appointment, for both training and test. Those characteristics lead to two rich insights. First, repeated failures lead to different outcomes for the datasets. DO has a greater overall probability of past success, but a lower probability of future success, for the all-failure sequence. For this dataset, the data represent a situation in which repeated refusals to donate are a strong signal towards future refusals. OP has a lower overall probability of past success, but a greater probability of future success for the all failure sequence. OP is signaling that repeated shows still could produce a no-show on the next sequence. Such a difference might be due to the nature of medical appointments, where life circumstances could still cause even an excellent attender to no-show on the next appointment.Second, a model, such as SUMER, induces overall patterns in a data set, and therefore is more likely to be able to continue to produce accurate probability estimates as the records in a data set change from time period to time period. An empirical table is more likely to be influenced by particular idiosyncrasies that are present at the time it is constructed.For additional analysis of SUMER's predictive ability, we compared SUMER with MTDg, BG/BB, and two traditional methods used for binary data analysis, Logistic Regression (LR) and Classification and Regression Trees (CART). We again used DeLong's method of comparison, as opposed to a WSSE or the Brier score of Brier (1950), because DeLong's method allows for the analysis of statistical differences. We coded the BG/BB model in Excel as per Fader et al. (2010). We used the MARCH software (andrewberchtold.com) for the MTDg calculations. The LR and CART models were estimated in IBM SPSS Statistics 21. To do a direct comparison of SUMER and BG/BB, we ran the BG/BB model for a history of length k+1, and calculated a weighted average of the probabilities generated for the two sequences with the same first k appointment orderings. For example, to generate BG/BB predictions for the OP dataset at k=10, we ran the BG/BB model for a history of length 11, and calculated a weighted average of the probabilities generated for the two sequences with the same first 10 appointment orderings.Table 7lists the AUCs, standard errors, and the p-Value of a χ2 test to determine if SUMER's AUC is statistically greater than the AUC of the other models. SUMER is significantly superior to the BG/BB model on both datasets. Recall that DO is the dataset that was used to tune and to test the BG/BB model. The BG/BB model incorporates the concept of “death” for the failure case. “Death,” for DO, connotes that a person has become inactive, and will no longer donate. For OP, “death” implies that a patient has permanently stopped no-showing for his or her appointments, and will attend all appointments in the future. Such a permanent change in behavior might be due to actual death, or to a change in behavior brought about by a change in attitude or health status. While the OP dataset was shown to have characteristics that would be beneficial for the BG/BB model, the assumption that recency and frequency are sufficient to accurately predict outcomes did not provide adequate enough estimates for either dataset. SUMER and MTDg have AUCs that are not statistically different for both models, but both are greater than BG/BB and the Table representation. SUMER has an advantage over MTDg in its ability to handle large datasets. For OP with 4,760,733 records, all models are able to compute estimates in less than 1 minute, except for MTDg which took over 72 hours.SUMER is significantly superior to CART for both datasets. CART also lacks the interpretability of the SUMER parameters. The output of CART is a tree or association rules that can be followed to associate each lag to the next occurrence. This type of output lacks direct relatability of the weight of each lag, which is available with the SUMER parameter estimates. SUMER and LR are not statistically different for both DO and OP, but both are greater than BG/BB and the Table for DO. The primary advantage of SUMER over LR is also the interpretability of its parameters. Because we model a human behavioral process, we assume that the coefficient values will decrease as the lag value increases, so occurrences that are more recent have a greater effect. As shown in Tables 3 and 4, even though SUMER is not constrained to produce decreasing parameter estimates, it is able to produce estimates that follow the assumed behavioral trend for DO and OP. Table 10 lists the LR coefficients. Each coefficient represents the change in the log-odds of a success at time t, all other coefficients held constant. The constant value represents the log-odds of a success at time t when there are no successes in the look-back window. For both datasets, the log-odds values are not ordered, and therefore, do not fit the inherent structure of the behavioral process.As an additional evaluation of model performance, we calculated cumulative gain for each model. To calculate gain, the predictions for each model are rank ordered from the greatest probability of success to the lowest, and the data are split into equally sized groups. Gain is calculated as the percentage of the total successes represented in each group. Gain values are cumulative across groups, such that the gain of the last group is 1.From a managerial standpoint, it is preferable to reach the greatest number of successes in the fewest number of trials. Therefore, a larger gain value for the top few groups is ideal. DO was split into five groups to calculate the cumulative gain, so each group contains 20 percent of the 11,104 records. Table 8 lists the cumulative gain values for the top three groups.The greatest values in each row are highlighted in bold. SUMER's predictions for Group 1 allow 71.6 percent of the total donors in the dataset to be targeted by contacting only 20 percent of DO's test set, that is, 1386 of the 1936 donors will be targeted by contacting only 2221 people. The results for LR and CART are similar, with 1385 and 1384 donors being targeted in Group 1. For Group 2, BG/BB is the only model that has a greater cumulative gain than SUMER. For Group 3, SUMER, MTDg, BG/BB, and LR have identical values. The results in Table 8 indicate that the subset comprised of the top three groups contains 97.85 percent of the donors in DO's dataset for all three models.OP was split into ten groups to perform gain analysis. The values for the top five groups are listed in Table 9. For Group 1, the Table probabilities provide the greatest gain, followed by SUMER and MTDg. For the three models, Table, SUMER, and MTDg, targeting 10 percent of OP's test set – 47,314 patients – allows a clinic to target 31.96 percent, 31.85 percent, and 31.83 percent, respectively, of patients who no-show. Those percentages represent 13,513, 13,465, and 13,458 patients, respectively. For groups two through five, the model with the greatest cumulative gain varies. By subset five, SUMER, MTDg, BG/BB, and LR all contain 74.93 percent of the total no-shows in the dataset.

@&#CONCLUSIONS@&#
In this paper, we presented a new predictive analytics model to address binary data that evolve from human behavioral processes, by combining regression modeling with sums of exponential functions. We focus on the properties of a human behavioral process, because such data have a random component and a habitual component that is associated with the user. Those components, properly understood, may be used to inform planning and scheduling decisions. We contribute to the literature on predictive analytics for binary data sequences in several ways.One, we present a parsimonious prediction model that combines regression-like modeling and the use of the sum of exponential functions to produce probability estimates. The use of a regression-like approach allows the model to be easily understood by a practitioner, and to produce parameters that can be used in interpreting human behavior. Those characteristics are valuable, because we seek to both predict future occurrences and to explain those occurrences, based upon past realizations of a process. The coefficients of the induced model provide insight, derived from the data, as to how much of a person's past behavior should be included when predicting how he/she will behave in the immediate future. The coefficients also provide an indication as to the rate at which past behavior becomes increasingly irrelevant. The empirical differences we observed when applying the model to two real data sets, one of charitable donations and one of outpatient attendance, show the flexibility of SUMER to adapt to datasets with different underlying human behavioral patterns.Two, we established that the process for estimating SUMER's coefficients yields an optimal and unique solution. The estimated parameters minimize the weighted sum of squared errors of the model as outlined in Eqs. (5) through (7), and satisfy the Karush–Kuhn–Tucker (KKT) conditions. This result is desirable, given that our model is motivated by the need to obtain accurate no-show predictions that can be used as an input to larger prediction model or a scheduling model.Three, the computational complexity of SUMER model is a function of the length of the past history considered, not of the number of observations in the data set, so that SUMER is particularly well-suited to Big Data applications. Given the influx of large datasets in analytics, and the desire to process information quickly, a model which can perform well regardless of dataset size is beneficial. SUMER is able to achieve this, while also producing a mechanism to determine when past history can stop being considered.Four, our model acts as a valuable input to planning and scheduling decisions in a service operations environment, such as an outpatient healthcare clinic. The use of an average no-show probability, or no-show rate, for all patients does not give proper insight into patient heterogeneity, and therefore does not serve to address the uncertainty and volatility that no-shows present in a system. SUMER is a novel predictive analytics model that can be used in conjunction with patient demographics and appointment characteristics to provide a reliable estimate of patient no-show probabilities. Because the model was constructed in a way that does not directly depend on the application domain, it should generalize well to other service operations that would benefit from the reliable identification of customer behavior over time.Several extensions of the SUMER model might be addressed by future research. One, the current structure of the SUMER model does not treat the time between occurrences as a parameter. Because human behavior may be affected by time elapse as well as the number of past occurrences, incorporating the time between outcomes might enhance the quality of the models predictions, and may allow for additional insight. Intuitively, an increased lag time should decrease the effect a past incident has on the next outcome. Two, the SUMER model might be modified so that it is able to predict multiple future outcomes, not only the next outcome. Three, the current models are built solely on the basis of successes; failures do not adjust the predicted probabilities either up or down. In situations where prior failures provide information about future successes, the model's parameter space could be expanded to reflect that information.