@&#MAIN-TITLE@&#
Decision-theoretic rough set: A multicost strategy

@&#HIGHLIGHTS@&#
We propose a DTRS by a set of cost matrices.Optimistic and pessimistic cases are two special models of our approach.Two criterions based reducts are calculated by two different algorithms.

@&#KEYPHRASES@&#
Cost reduct,Decision-monotocity reduct,Decision-theoretic rough set,Multiple cost matrices,

@&#ABSTRACT@&#
By introducing the misclassification and delayed decision costs into the probabilistic approximations of the target, the model of decision-theoretic rough set is then sensitive to cost. However, traditional decision-theoretic rough set is proposed based on one and only one cost matrix, such model does not take the characteristics of multiplicity and variability of cost into consideration. To fill this gap, a multicost strategy is developed for decision-theoretic rough set. Firstly, from the viewpoint of the voting fusion mechanism, a parameterized decision-theoretic rough set is proposed. Secondly, based on the new model, the smallest possible cost and the largest possible cost are calculated in decision systems. Finally, both the decision-monotocity and cost criteria are introduced into the attribute reductions. The heuristic algorithm is used to compute decision-monotonicity reduct while the genetic algorithm is used to compute the smallest and the largest possible cost reducts. Experimental results on eight UCI data sets tell us: 1. compared with the raw data, decision-monotocity reduct can generate greater lower approximations and more decision rules; 2. the smallest possible cost reduct is much better than decision-monotocity reduct for obtaining smaller costs and more decision rules. This study suggests new research trends concerning decision-theoretic rough set theory.

@&#INTRODUCTION@&#
Cost-sensitive learning is a valuable problem. It has been addressed by many researchers from a variety of fields including decision making [22], machine learning [8,9,55,41], pattern recognition [56] and so on. Note that in recent years, cost-sensitive learning has also attracted much attention of researchers in rough set theory [32]. In broad terms, rough set is one of the most important tools of Granular Computing (GrC) [33,44,46] and then introduction of cost sensitivity into rough set is bound to bring GrC a worthwhile topic.Roughly speaking, driven by many practical applications, two main types of costs have been discussed in rough set: test cost and misclassification cost. On the one hand, each test (i.e., attribute, measurement, feature) may have an associated cost which is regarded as test cost. For example, in medical diagnosis, a blood test has a cost which may be the time or money spent on testing blood. On the other hand, misclassification cost is basically the loss when classifying data into a specific outcome. For example, it may be troublesome if a healthy subject is misclassified as a patient, but it could result in a more serious consequence if a patient is misclassified as healthy subject.In rough set theory, two crucial aspects should be covered in terms of costs. Firstly, since the basic model, i.e., lower and upper approximations are not sensitive to cost, then how to introduce cost into modeling is an interesting issue. Such aspect will provide us a new perspective to characterize the uncertainty, mainly because if lower and upper approximations are sensitive to cost, then inevitably, uncertainty [1,7] in rough set is sensitive to cost. Secondly, it is well known that attribute reduction is one of the key topics in rough set, from which we can see that finding reducts with some particular requirements of cost (e.g., to find reduct with the smallest cost) is also a challenge. In recent years, unremitting efforts have led to great progress around the two aspects we mentioned above.•As far as the modeling is concerned, both test cost and misclassification cost have been explored.For example, by considering test cost of the attributes, Yang et al. [43] introduced test cost into the construction of the multigranulation rough set [35,37,13]; by considering misclassification cost, Yao [47] proposed the concept of the Decision-Theoretic Rough Set (DTRS). Decision-theoretic rough set is actually a probabilistic rough set. Since the determination for a pair of the thresholds used in probabilistic rough set is a substantial challenge, then the pair of thresholds presented in decision-theoretic rough set is calculated by loss function. It must be noticed that for Yao’s loss function matrix, not only misclassification cost is used, but also the delayed decision cost is considered. Therefore, decision-theoretic rough set is corresponding to a three-way decision procedure [45]. For more generalizations and applications of decision-theoretic rough set, please refer to Refs. [2,4,14,15,17–21,23,24,26,27,38,39,42,53].As far as the attribute reduction is concerned, both test cost and misclassification cost have also been studied by some researchers.For example, Min et al. [28,31] studied the approaches to test cost based attribute reduction. Their goals are to find reducts with a smaller (obtained by a competition strategy) or the smallest (obtained by a backtracking approach) test costs. Following decision-theoretic rough set, Jia et al. [11,12] formulated an optimization problem. They aimed to minimize the cost of decision. With respect to different requirements, Yao and Zhao [48] studied different definitions of attribute reductions in decision-theoretic rough set.From discussions above, we can see that decision-theoretic rough set takes the cost into consideration (model is sensitive to cost) and then attribute reduction of decision-theoretic rough set is bound to close to cost. For Yao’s classical decision-theoretic rough set, the loss function is a 3×2 cost matrix which includes both misclassification and delayed decision costs. However, one and only one cost matrix may not be good enough for problem solving. We have some practical examples to illustrate the limitations of one cost matrix.1.Take for instance one medical accident, it is reasonable to assume that different regions may execute different criteria for making compensation, i.e., the costs of the same medical accident may be different in different regions. The developed regions may pay more than that paid by a developing region for economic factors.Here is a China old saying: “Everyone has a steelyard in his heart” and “It is hard to please all”, that is, for a given decision, different individuals may have different views which will inevitably generate different costs.Suppose that Mr. X wants to buy a house, if the prices show an up trend, then Mr. X faces different costs during different periods. In other words, one and only one cost is not enough to characterize the variability of cost.Based on the above examples, it is noticeable that in Yao’s decision-theoretic rough set, one and only one cost matrix does not take the characteristics of multiplicity and variability of cost into consideration. Therefore, multiple cost matrices are required. From this point of view, a voting fusion mechanism will be used and then three models are constructed when facing multiple cost matrices. They are referred to as θ (parameterized), optimistic and pessimistic decision-theoretic rough sets in this paper. Note that optimistic and pessimistic models are two limits of parameterized approach.To facilitate our discussions, we present the basic knowledge about rough set and decision-theoretic rough set in Section 2. In Section 3, not only three multicost based decision-theoretic rough sets are proposed, but also the computations of smallest and largest possible costs are discussed. In Section 4, decision-monotocity criterion based attribute reduction and cost criterion based attribute reduction are presented. The heuristic algorithm is used to compute decision-monotocity reduct while the genetic algorithm is used to compute cost reduct. In Section 5, the theoretical results shown in this paper are tested on eight UCI data sets. The paper ends with conclusions and outlooks for further research in Section 6.An information system can be considered as a pairI=<U,AT>, in which U is a non-empty finite set of the objects called the universe; AT is a non-empty finite set of the attributes. ∀a ∈ AT,Vais the domain of attribute a. ∀x ∈ U,a(x) denotes the value that x holds on a (∀a ∈ AT). Given an information system I,∀A⊆AT, an indiscernibility relation IND(A) may be defined asIND(A)={(x,y)∈U2:∀a∈A,a(x)=a(y)}.Obviously, IND(A) is an equivalence relation. ∀X⊆U, one can construct the lower and upper approximations of X by IND(A) such that(1)A̲(X)={x∈U:[x]A⊆X},(2)A¯(X)={x∈U:[x]A∩X0.25em0ex≠0.25em0ex⌀};where[x]A={y∈U:(x,y)∈IND(A)}is the equivalence class of x. The pair (A̲(X),A¯(X)) is a Pawlak’s rough set of X with respect to A. The positive region of X isPOSA(X)=A̲(X), the boundary region of X isBNDA(X)=A¯(X)−A̲(X), and the negative region of X isNEGA(X)=U−A¯(X).For a Bayesian decision procedure, a finite set of the states can be denoted byΩ={ω1,ω2,⋯,ωs}. A finite set of t possible actions can be denoted byA={a1,a2,⋯,at}. ∀x ∈ U, let Pr(ωj|x) be the conditional probability of object x being in state ωj,λ(ai|ωj) be the loss, or cost for taking action aiwhen state is ωj. Suppose that we take the action aifor object x, then the expected loss is(3)R(ai|x)=∑j=1sλ(ai|ωj)·Pr(ωj|x).For Yao’s decision-theoretic rough set model, the set of states is composed by two classes such thatΩ={X,∼X}. It indicates that an object is in class X or out of class X; the set of actions is given byA={aP,aB,aN}, in which aP,aBand aNexpresses three actions: aPindicates that x is classified into decision-theoretic positive region of X, i.e.,POSDTA(X);aBindicates that x is classified into decision-theoretic boundary region of X, i.e.,BNDDTA(X);aNindicates that x is classified into decision-theoretic negative region of X, i.e.,NEGDTA(X). The loss function regarding the costs of three actions in two different states is given in Table 1. Obviously, Table 1 is a3×2matrix. It is denoted by M in this paper.In Table 1, λPP,λBPand λNPare the losses for taking actions of aP,aBand aN, respectively, when stating x is included into X;λPN,λBNand λNNare the losses for taking actions of aP,aBand aN, respectively, when stating x is out of X. ∀x ∈ U, by using the conditional probability Pr(X|[x]A), the expected losses associated with three actions are:R(aP|[x]A)=λPP·Pr(X|[x]A)+λPN·Pr(∼X|[x]A);R(aB|[x]A)=λBP·Pr(X|[x]A)+λBN·Pr(∼X|[x]A);R(aN|[x]A)=λNP·Pr(X|[x]A)+λNN·Pr(∼X|[x]A).The Bayesian decision procedure leads to the following minimum-risk decision rules:(P)0.35em0exIf0.35em0exR(aP|[x]A)≤R(aB|[x]A)0.35em0exand0.35em0exR(aP|[x]A)≤R(aN|[x]A),0.35em0exthen0.35em0exx∈POSDTA(X);(B)0.35em0exIf0.35em0exR(aB|[x]A)≤R(aP|[x]A)0.35em0exand0.35em0exR(aB|[x]A)≤R(aN|[x]A),0.35em0exthen0.35em0exx∈BNDDTA(X);(N)0.35em0exIf0.35em0exR(aN|[x]A)≤R(aP|[x]A)0.35em0exand0.35em0exR(aN|[x]A)≤R(aB|[x]A),0.35em0exthen0.35em0exx∈NEGDTA(X).SincePr(X|[x]A)+Pr(∼X|[x]A)=1and we assume a reasonable loss function with the conditions 0 ≤ λPP≤ λBP≤ λNPand 0 ≤ λNN≤ λBN≤ λPN, then decision rules (P), (B) and (N) can be expressed as:(P)0.35em0exIf0.35em0exPr(X|[x]A)≥α0.35em0exand0.35em0exPr(X|[x]A)≥γ,0.35em0exthen0.35em0exx∈POSDTA(X);(B)0.35em0exIf0.35em0exPr(X|[x]A)<α0.35em0exand0.35em0exPr(X|[x]A)<β,0.35em0exthen0.35em0exx∈BNDDTA(X);(N)0.35em0exIf0.35em0exPr(X|[x]A)<γ0.35em0exand0.35em0exPr(X|[x]A)≤β,0.35em0exthen0.35em0exx∈NEGDTA(X);whereα=(λPN−λBN)(λPN−λBN)+(λBP−λPP),0.35em0exβ=(λBN−λNN)(λBN−λNN)+(λNP−λBP)andγ=(λPN−λNN)(λPN−λNN)+(λNP−λPP).Since 0 ≤ β < γ < α ≤ 1, then we have(P)0.35em0exIf0.35em0exPr(X|[x]A)≥α,0.35em0exthen0.35em0exx∈POSDTA(X);(B)0.35em0exIf0.35em0exβ<Pr(X|[x]A)<α,0.35em0exthen0.35em0exx∈BNDDTA(X);(N)0.35em0exIf0.35em0exPr(X|[x]A)≤β,0.35em0exthen0.35em0exx∈NEGDTA(X).From discussions above, the lower approximation, upper approximation of X areA̲DT(X)={x∈U:Pr(X|[x]A)≥α};A¯DT(X)={x∈U:Pr(X|[x]A)>β}.The pair (ADT(X) andA¯DT(X)) is referred to as a decision-theoretic rough set ofX,POSDTA(X)=A̲DT(X)is decision-theoretic positive region ofX,BNDDTA(X)=A¯DT(X)−A̲DT(X)is decision-theoretic boundary region ofX,NEGDTA(X)=U−A¯DT(X)is decision-theoretic negative region of X.For classical decision-theoretic rough set, one and only one3×2cost matrix is used. Yao assumed that such cost matrix comes from the expert’s evaluation. However, as what have been argued in Section 1, it is clear that a single cost matrix is insufficient for making a better or a more suitable decision. Therefore, multiple cost matrices and the corresponding decision-theoretic rough set approach will be explored in this section.Suppose thatM={M1,M2,⋯,Mm} is the set of m different cost matrices,∀i∈{1,⋯,m}, the ith loss function regarding the costs of three actions in two different states is given in Table 2.Similar to classical decision-theoretic rough set, for each cost matrix, we may obtain three minimum-risk decision rules. For example, by considering the ith cost matrix, the corresponding minimum-risk decision rules are:(Pi)0.35em0exIf0.35em0exPr(X|[x]A)≥αi,0.35em0exthen0.35em0exx∈POSDTA(X);(Bi)0.35em0exIf0.35em0exβi<Pr(X|[x]A)<αi,0.35em0exthen0.35em0exx∈BNDDTA(X);(Ni)0.35em0exIf0.35em0exPr(X|[x]A)≤βi,0.35em0exthen0.35em0exx∈NEGDTA(X);whereαi=(λPNi−λBNi)(λPNi−λBNi)+(λBPi−λPPi)andβi=(λBNi−λNNi)(λBNi−λNNi)+(λNPi−λBPi).Therefore, the immediate problem is how to fuse these m different (P), (B) or (N) rules when facing m cost matrices. To achieve such goal, we will present a voting strategy. Voting is a basic and the most widely used approach in ensemble learning. Each member of the ensemble casts a single vote for deciding whether a sample is in or out of the class and then the final decision is computed by the sum of these votes.Taking for instance our fusion of m different (P) rules, each (P) rule is used to judge whether x is in the positive region of decision class X. Similar to voting approach in ensemble learning, our final decision can be computed by the sum of votes. The details are:(IP)0.35em0exIf0.35em0ex|{i:Pr(X|[x]A)≥αi,1≤i≤m}|m≥θ,0.35em0exthen0.35em0exx∈POSA(X);(IB)0.35em0exIf0.35em0ex|{i:βi<Pr(X|[x]A)<αi,1≤i≤m}|m≥θ,0.35em0exthen0.35em0exx∈BNDA(X);(IN)0.35em0exIf0.35em0ex|{i:Pr(X|[x]A)≤βi,1≤i≤m}|m≥θ,0.35em0exthen0.35em0exx∈NEGA(X);where θ ∈ (0,1] is a threshold, |X| is the cardinal number of set X.Pr(X|[x]A) ≥ αitells us that x is in positive region of X based on the information given by the ith cost matrix,|{i:Pr(X|[x]A)≥αi,1≤i≤m}|mindicates the ratio of votes that x is in positive region to number of used cost matrices. By (IP), we can see that if such ratio is greater than or equal to the given threshold, then x is classified into positive region of X finally. In other words, if the number of experts (they agree that x is in positive region of X) achieves a certain amount, then x is finally decided in positive region. In particular, if θ ∈ (0.5,1], then our voting is a majority voting approach, which means the minority is subordinate to the majority. The semantic explanations of (IB) and (IN) are similar to that of (IP).Following the voting of fusion, an interesting issue is to investigate two limits of such fusion.1.The lower limit is: at least one of the experts agreePr(X|[x]A)≥αi,0.35em0exβi<Pr(X|[x]A)<αior Pr(X|[x]A) ≤ βi.The upper limit: all the experts agreePr(X|[x]A)≥αi,0.35em0exβi<Pr(X|[x]A)<αior Pr(X|[x]A) ≤ βi.On the one hand, the lower limit expresses an optimistic view because if for at least one of the cost matrices, Pr(X|[x]A) ≥ αiholds, then we obtainPr(X|[x]A)≥APTARANORMALmini=1mαiimmediately, and vice versa; on the other hand, the upper limit expresses a pessimistic view because if for all the cost matrices, Pr(X|[x]A) ≥ αiholds, then we obtainPr(X|[x]A)≥APTARANORMALmaxi=1mαiimmediately, and vice versa. Moreover, for the ith cost matrix, similar to Yao’s decision-theoretic rough set theory, we also have αi> βi, from which we can conclude thatAPTARANORMALmini=1mαi>APTARANORMALmini=1mβiandAPTARANORMALmaxi=1mαi>APTARANORMALmaxi=1mβi. To sum up, we may consider the following two limits of fusions.1.Firstly, let us present the optimistic fusion of risk decision rules such that:(IIP)0.35em0exIf0.35em0exPr(X|[x]A)≥APTARANORMALminmi=1αi,0.35em0exthen0.35em0exx∈POSA(X);(IIB)0.35em0exIf0.35em0exAPTARANORMALminmi=1βi<Pr(X|[x]A)<APTARANORMALminmi=1αi,0.35em0exthen0.35em0exx∈BNDA(X);(IIN)0.35em0exIf0.35em0exPr(X|[x]A)≤APTARANORMALminmi=1βi,0.35em0exthen0.35em0exx∈NEGA(X).Secondly, let us present the pessimistic fusion of risk decision rules such that:(IIIP)0.35em0exIf0.35em0exPr(X|[x]A)≥APTARANORMALmaxmi=1αi,0.35em0exthen0.35em0exx∈POSA(X);(IIIB)0.35em0exIf0.35em0exAPTARANORMALmaxmi=1βi<Pr(X|[x]A)<APTARANORMALmaxmi=1αi,0.35em0exthen0.35em0exx∈BNDA(X);(IIIN)0.35em0exIf0.35em0exPr(X|[x]A)≤APTARANORMALmaxmi=1βi,0.35em0exthen0.35em0exx∈NEGA(X).From the above voting strategy of fusion, we now obtain the following multicost based decision-theoretic rough sets.Definition 1LetI=〈U,AT〉be an information system.M={M1,M2,⋯,Mm} is the set of m different cost matrices, ∀A⊆AT and ∀X⊆U. The θ decision-theoretic lower approximation, upper approximation and boundary region of X are defined as(4)A̲θDTM(X)={x∈U:|{i:Pr(X|[x]A)≥αi,1≤i≤m}|m≥θ};(5)A¯θDTM(X)={x∈U:|{i:Pr(X|[x]A)>βi,1≤i≤m}|m≥θ};(6)BNDθDTM(X)={x∈U:|{i:βi<Pr(X|[x]A)<αi,1≤i≤m}|m≥θ};where θ ∈ (0,1].The pair (A̲θDTM(X),0.35em0exA¯θDTM(X))is referred to as a θ decision-theoretic rough set of X. Definition 1 is actually a parameterized decision-theoretic rough set due to the using of parameter θ. Obviously, objects inA̲MθDT(X)support rules (IP), objects inBNDθDTM(X)supports rules (IB), objects inA¯MθDT(X)support rules (IN).Similar to the limits of fusions shown in Section 3.1, we can also present the following limits of θ decision-theoretic rough set. They are referred to as optimistic and pessimistic decision-theoretic rough sets, respectively.Definition 2LetI=〈U,AT〉be an information system.M={M1,M2,⋯,Mm} is the set of m different cost matrices, ∀A⊆AT and ∀X⊆U. The optimistic decision-theoretic lower approximation, upper approximation and boundary region of X are defined as(7)A̲MODT(X)={x∈U:Pr(X|[x]A)≥APTARANORMALminmi=1αi};(8)A¯MODT(X)={x∈U:Pr(X|[x]A)>APTARANORMALminmi=1βi};(9)BNDODTM(X)={x∈U:APTARANORMALminmi=1βi<Pr(X|[x]A)<APTARANORMALminmi=1αi};the pessimistic decision-theoretic lower approximation, upper approximation and boundary region of X are defined as(10)A̲MPDT(X)={x∈U:Pr(X|[x]A)≥APTARANORMALmaxmi=1αi};(11)A¯MPDT(X)={x∈U:Pr(X|[x]A)>APTARANORMALmaxmi=1βi};(12)BNDPDTM(X)={x∈U:APTARANORMALmaxmi=1βi<Pr(X|[x]A)<APTARANORMALmaxmi=1αi}.The pair (A̲ODTM(X),A¯ODTM(X)) is referred to as an optimistic decision-theoretic rough set of X while the pair (A̲PDTM(X),A¯PDTM(X)) is referred to as a pessimistic decision-theoretic rough set of X. Obviously, objects inA̲ODTM(X)support rules (IIP), objects inBNDODTM(X)supports rules (IIB), objects inA¯ODTM(X)support rules (IIN); objects inA̲PDTM(X)support rules (IIIP), objects inBNDPDTM(X)supports rules (IIIB), objects inA¯PDTM(X)support rules (IIIN).For Yao’s decision-theoretic lower approximation, Pr(X|[x]A) ≥ α implies a x-related (P) rule: “If0.35em0exR(aP|[x]A)≤R(aB|[x]A)andR(aP|[x]A)≤R(aN|[x]A),0.35em0exthen0.35em0exx∈POSDTA(X)”. In our optimistic decision-theoretic lower approximation,x∈A̲ODTM(X)indicates that there must be a cost matrix Misupports Pr(X|[x]A) ≥ αi, from which we can conclude that the minimum-risk decision (P) rule holds for at least one of the cost matrices. In other words,x∈A̲ODTM(X)requires that the x-related (P) rule should be valid for at least one of the cost matrices. In our pessimistic decision-theoretic lower approximation,x∈A̲PDTM(X)indicates that for all the cost matrices, we have Pr(X|[x]A) ≥ αi, from which we can conclude that the minimum-risk decision (P) rule holds for all the cost matrices. In other words,x∈A̲PDTM(X)requires that the x-related (P) rule should be valid for all the cost matrices. Similar explanations between (B)/(N) rules and boundary/negative regions can also be obtained, respectively.Example 1Fig. 1 shows us an example of the difference between optimistic and pessimistic decision-theoretic lower approximations, which are based on two cost matrices.It is assumed that by two cost matrices such thatM={M1,M2}. We obtain two values of α such thatα1=0.7andα2=0.3. The left sub-figure is the result of optimistic decision-theoretic lower approximation while the right sub-figure is the result of pessimistic decision-theoretic lower approximation. Through an investigation of Fig. 1, we can see that both Pr(X|[x]A) and Pr(X|[y]A) are greater than the minimal value of α1 and α2 and then by Eq. (7),x,y∈A̲ODTM(X)holds; on the other hand, only Pr(X|[y]A) is greater than the maximal value of α1 and α2, by Eq. (10),y∈A̲PDTM(X)andx0.25em0ex∉0.25em0exA̲PDTM(X).LetI=〈U,AT〉be an information system.M={M1,M2,⋯,Mm} is the set of m different cost matrices, ∀A⊆AT and ∀X⊆U. We have(13)A̲ODTM(X)⊇A̲θDTM(X)⊇A̲PDTM(X);(14)A¯ODTM(X)⊇A¯θDTM(X)⊇A¯PDTM(X).∀x∈A̲PDTM(X), by Eq. (10), we havePr(X|[x]A)≥APTARANORMALmaxi=1mαi. Therefore, by Eq. (4), we know|{i:Pr(X|[x]A)≥αi,1≤i≤m}|m=1≥θbecause θ ∈ (0,1], it follows thatx∈A̲θDTM(X).∀x∈A̲θDTM(X), by Eq. (4), we have|{i:Pr(X|[x]A)≥αi,1≤i≤m}|m≥θ, since θ ∈ (0,1], then we know that|{i:Pr(X|[x]A)≥αi,1≤i≤m}|m>0, it follows that there must bei∈{1,⋯,m}such that Pr(X|[x]A) ≥ αi, and thenPr(X|[x]A)≥APTARANORMALmini=1mαi, by Eq. (7),x∈A̲ODTM(X)holds.Similarity, it is not difficult to prove thatA¯ODTM(X)⊇A¯θDTM(X)⊇A¯PDTM(X). □Proposition 1 shows the relationships among θ decision-theoretic lower/upper approximations, optimistic decision-theoretic lower/upper approximations and pessimistic decision-theoretic lower/upper approximations. It tells us that optimistic decision-theoretic lower and upper approximations include θ decision-theoretic lower and upper approximations, respectively; θ decision-theoretic lower and upper approximations include pessimistic decision-theoretic lower and upper approximations, respectively.Remark 1It should be noticed that there is no necessary inclusion among θ, optimistic and pessimistic decision-theoretic boundary regions. For example, Fig. 2 shows us the difference between optimistic and pessimistic decision-theoretic boundary regions. It is observed that optimistic and pessimistic decision-theoretic boundary regions may have overlap. They may also have completely different parts.LetI=〈U,AT〉be an information system.M={M1,M2,⋯,Mm} is the set of m different cost matrices, ∀A⊆AT and ∀X⊆U. We have(15)A̲ODTM(X)=∪i=1mA̲DTi(X);(16)A¯ODTM(X)=∪i=1mA¯DTi(X);(17)A̲PDTM(X)=∩i=1mA̲DTi(X);(18)A¯PDTM(X)=∩i=1mA¯DTi(X);whereA̲DTi(X)andA¯DTi(X)are classical decision-theoretic lower and upper approximations constructed by the ith cost matrix.∀x ∈ U, by Eq. (7), we havex∈A̲ODTM(X)⇔Pr(X|[x]A)≥APTARANORMALminmi=1αi⇔Pr(X|[x]A)≥α10.35em0exor0.35em0exPr(X|[x]A)≥α20.35em0exor0.35em0ex⋯or0.35em0exPr(X|[x]A)≥αm⇔x∈A̲DT1(X)0.35em0exor0.35em0exx∈A̲DT2(X)0.35em0exor0.35em0ex⋯0.35em0exor0.35em0exA̲DTm(X)⇔x∈∪i=1mA̲DTi(X).That completes the proof ofA̲ODTM(X)=∪i=1mA̲DTi(X). Similarity, it is not difficult to prove Eqs. (16)–(18). □Proposition 2 shows the relationships between optimistic/pessimistic decision-theoretic rough sets and classical decision-theoretic rough set. The details are: (1) optimistic decision-theoretic lower/upper approximation are unions of m classical decision-theoretic lower/upper approximations; (2) pessimistic decision-theoretic lower/upper approximations are intersections of m classical decision-theoretic lower/upper approximations.The end result of rough set theory is to derive decision rules from decision systems. A decision system is a special information systemI=〈U,AT∪D〉, in which AT is the set of the condition attributes, D is the set of the decision attributes. To simplify our discussions, we only consider one decision attribute d and then the decision system is denoted byI=〈U,AT∪{d}〉. Generally speaking, decision attribute d can partition the universe into a set of the equivalence classes such thatU/IND({d})={X1,X2,⋯,Xn}.A decision rule can be regarded as the relationship between descriptions on condition and decision attributes. It is known that decision-theoretic rough set is closely related to costs, then an interesting issue is to explore the costs of decision rules. In this subsection, we are mainly focusing on computation of costs with two different views: the first one is to generate the smallest possible costs, the second one is to generate the largest possible costs.For θ decision-theoretic rough set, we may consider following three smallest possible costs by the minimal operator because m different cost matrices are considered.•∀x∈A̲θDTM(Xj), it generates a (IIP) rule and then the corresponding smallest possible cost is:LCPAM(x)=Pr(Xj|[x]A)·(APTARANORMALminmi=1λPPi)+(1−Pr(Xj|[x]A))·(APTARANORMALminmi=1λPNi).∀x∈BNDθDTM(Xj), it generates a (IIB) rule and then the corresponding smallest possible cost is:LCBAM(x)=Pr(Xj|[x]A)·(APTARANORMALminmi=1λBPi)+(1−Pr(Xj|[x]A))·(APTARANORMALminmi=1λBNi).∀x0.25em0ex∉0.25em0exA¯θDTM(Xj), it generates a (IIN) rule and then the corresponding smallest possible cost is:LCNAM(x)=Pr(Xj|[x]A)·(APTARANORMALminmi=1λNPi)+(1−Pr(Xj|[x]A))·(APTARANORMALminmi=1λNNi).The above costs match the semantic explanation of “optimism” because for many practical applications, lower cost is what we want to get. Take for instanceLCPAM(x), if we want to obtain cost of positive rule as lower as possible, then a feasible strategy is to use the minimal value of costs in m different cost matrices. This is why the operator “min” is employed. Similar explanation is applicable forLCBAM(x)andLCNAM(x).Following Jia et al.’s approach [11], it is not difficult to present the following smallest possible cost in a decision system.(19)LCAM=∑j=1n(∑x∈A̲θDTM(Xj)LCPAM(x)+∑x∈BNDθDTM(Xj)LCBAM(x)+∑x∉A¯θDTM(Xj)LCNAM(x)).Similar to the smallest possible case, we may also consider following three largest possible costs of decision rules by using the maximal operator based on m different cost matrices.•∀x∈A̲θDTM(Xj), it generates a (IIIP) rule and then the corresponding largest possible cost is:HCPAM(x)=Pr(Xj|[x]A)·(APTARANORMALmaxmi=1λPPi)+(1−Pr(Xj|[x]A))·(APTARANORMALmaxmi=1λPNi).∀x∈BNDθDTM(X), it generates a (IIIB) rule and then the corresponding largest possible cost is:HCBAM(x)=Pr(Xj|[x]A)·(APTARANORMALmaxmi=1λBPi)+(1−Pr(Xj|[x]A))·(APTARANORMALmaxmi=1λBNi).∀x0.25em0ex∉0.25em0exA¯θDTM(X), it generates a (IIIN) rule and the corresponding largest possible cost is:HCNAM(x)=Pr(Xj|[x]A)·(APTARANORMALmaxmi=1λNPi)+(1−Pr(Xj|[x]A))·(APTARANORMALmaxmi=1λNNi).ForHCPAM(x),0.35em0exHCBAM(x)andHCNAM(x), a larger cost is what we want to obtain. Take for instanceHCPAM(x), if we want to obtain cost of positive rule as larger as possible, then a feasible strategy is to use the maximal value of cost in m different cost matrices. This is why the operator “max” is employed. Similar explanation is also applicable forHCBAM(x)andHCNAM(x).The largest possible cost in a decision system is(20)HCAM=∑j=1n(∑x∈A̲θDTM(Xj)HCPAM(x)+∑x∈BNDθDTM(Xj)HCBAM(x)+∑x∉A¯θDTM(Xj)HCNAM(x)).The aim of attribute reduction is to delete redundant attributes in information or decision systems. For example, if we delete redundant attributes for obtaining approximations’ preservation reduct, the certain decision rules derived from lower approximation can be shortened. That is to say, we can generate simple decision rules. This is why attribute reduction plays an important role in rough set theory. In classical rough set theory, since indiscernibility relation, positive region, approximations, conditional entropy and so on are monotonic with the increasing or decreasing of condition attributes, then many researchers have presented different definitions of attribute reductions for preserving indiscernibility relation, positive region, approximations, information measurements and so on, see Refs. [5,6,10,29,30,34,36,40,52,51,54] for more details about recent advances in attribute reduction.However, it must be noticed that in decision-theoretic rough set theory, many evaluations used in attribute reductions are not necessarily monotonic. For instance, with the decreasing of condition attributes, positive region is not necessarily shrinking. To solve such problem, Yao and Zhao [48] have presented a decision-monotocity criterion. Following their work, it is not difficult to introduce the decision-monotocity criterion into our multicost based decision-theoretic rough set. To simplify our discussions,we only consider the lower approximation based decision-monotocity criterion in this paper.Definition 3LetI=〈U,AT∪{d}〉be a decision system.M={M1,M2,⋯,Mm} is the set of m different cost matrices, ∀A⊆AT,A is referred to as a θ decision-monotocity reduct if and only if A is the minimal set of the condition attributes, which preservesAT̲θDTM(Xj)⊆A̲θDTM(Xj)for each Xj∈ U/IND({d}).For the conditionAT̲θDTM(Xj)⊆A̲θDTM(Xj)in θ decision-monotocity reduct, it is required that by reducing attributes, a (IP) rule is still a (IP) rule with the same decision. This is mainly because (IP) rules are supported by objects in lower approximations. Therefore, by using the concept of decision-monotocity reduct, the number of (IP) rules may be increased in the decision system. In other words, decision-monotocity reduct has two benefits: 1. simplify decision rules; 2. generate more decision rules.The greatest difference between decision-theoretic and classical rough sets is that the former introduces misclassification cost and delayed decision cost into the mathematical model. Therefore, it is interesting to discuss attribute reduction from the viewpoint of cost. For many practical reasons, it is required to obtain a subset of condition attributes, which can generate lower cost. That is why Yao and Zhao also have presented the cost criterion [48]. Similar to decision-monotocity criterion, it is also not difficult to introduce cost criterion into our multicost based decision-theoretic rough set. In Section 3.3, we have considered two types of costs, the corresponding two cost reducts can also be presented.Definition 4LetI=〈U,AT∪{d}〉be a decision system.M={M1,M2,⋯,Mm} is the set of m different cost matrices, ∀A⊆AT,1.A is referred to as a smallest possible cost reduct if and only ifLCAM≤LCATMandLCBM>LCAMfor each B⊂A;A is referred to as a largest possible cost reduct if and only ifHCAM≤HCATMandHCBM>HCAMfor each B⊂A.In Definition 4, smallest/largest possible costs reducts indicate that we want find a subset of condition attributes so that the smallest/largest possible costs will decrease or remain unchanged. That is to say, cost reduct will help us to obtain lower costs with fewer attributes.Presently, in many rough set literatures, two approaches have been widely adopted to find reducts [28]: exhaustive algorithm and heuristic algorithm. A typical exhaustive algorithm is to construct the set of all reducts by using the discernibility matrix. Unfortunately, such approach is NP-hard. This is why heuristic algorithm has attracted much attention. Most heuristic algorithms have the same structure and their differences are constructions of the heuristic functions. In this section, we will use the heuristic algorithm to compute decision-monotocity reducts.Moreover, it is noticeable that the goal of cost reduct is to obtain a subset of condition attributes, which can derive lower cost. Such attribute reduction can easily be formulated as an optimization problem. Therefore, we will use the genetic algorithm to compute cost reduct.Now, we will present a heuristic algorithm to compute θ decision-monotocity reduct. Firstly, we need the following new measurements to express the significance.(21)Sig(A,AT,d)=∑j=1n(AT̲θDTM(Xj)⊙A̲θDTM(Xj))where n is the number of decision classes, i.e.,n=|U/IND({d})|,∀X,Y⊆U,X⊙Yis defined as:X⊙Y={|Y−X|:X⊆Y−∞:X¬⊆YSig(A,AT,d) measures the significance of set of the attributes A. Such measurement is based on the basic condition of decision-monotocity reduct. For example, a θ decision-monotocity reduct A requirersAT̲θDTM(Xj)⊆A̲θDTM(Xj)for each Xj∈ U/IND({d}) and then Sig(A,AT,d) > 0. That is to say, if we obtain a subset A such that Sig(A,AT,d) > 0, then A may be a potential θ decision-monotocity reduct, otherwise, A does not need to be considered. It’s significance is regarded as negative infinity. For heuristic algorithm, Yao et al. [49] have summarized three strategies: the deletion strategy, the addition–deletion strategy and the addition strategy. In this paper, we will use the addition–deletion strategy to design a heuristic algorithm which includes two steps: 1. construct a super-reduct from an empty set by adding condition attributes; 2. obtain the final reduct by deleting redundant attributes from super-reduct. The details are shown in Algorithm 1.Algorithm 1Heuristic approach to compute θ decision-monotocity reduct.Input: Decision systemI=〈U,AT,∪{d}〉, cost matricesM1,M2,⋯,Mm, thresholdθ;Output: Anθdecision-monotocity reduct A.1. ComputeAT̲θDTM(Xj)for eachXj∈U/IND({d});2.A←⌀,A̲θDTM(Xj)=⌀for eachXj∈U/IND({d}),Sig(A,AT,d)=0;//Addition3.while∃Xj∈U/IND({d})such thatAT̲θDTM(Xj)¬⊆A̲θDTM(Xj)do4.for eacha∈AT−A5.ComputeSig(A∪{a},AT,d);6.end for7.Find the maximalSig(A∪{a},AT,d)and the corresponding attribute a;8.ifSig(A∪{a},AT,d)>0then9.A=A∪{a}10.end if11.end while//Deletion12.for eacha∈A13.ComputeSig(A−{a},AT,d);14.ifSig(A−{a},AT,d)>0then15.A=A−{a};16.end if17.end for18. Return A.In the following, we will present a genetic algorithm to compute smallest possible cost reduct and largest possible cost reduct. This is mainly because the objective to find cost reduct can be formulated as an optimization problem while genetic algorithm is one of the widely used approaches to solve optimization problems. In genetic algorithm, a chromosome is represented as a binary string of length |AT|, where “1” expresses that the corresponding attribute is present, and “0” shows that the corresponding attribute is not. For smallest possible cost reduct, the fitness function is defined as:(22)fALC=LCAM;while for largest possible cost reduct, the fitness function is defined as:(23)fAHC=HCAM.The goal of these fitness functions is to find a attribute subset that can generate lower smallest or largest possible costs. The selection method in our genetic approach is the roulette wheel selection. In the roulette wheel selection, individuals in the population are assigned probabilities of being selected that is directly proportionate to their fitness. For the parameters setting, in the crossover procedure, the mixing ratio is 0.7, and the mutation operation is applied with the probability 0.1. The details of genetic algorithm are described in Algorithm 2.Algorithm 2Genetic approach to compute smallest possible cost reduct.Input: Decision systemI=〈U,AT,∪{d}〉, cost matricesM1,M2,⋯,Mm, thresholdθ;Output: A smallest possible cost reduct A.1. Create an initial random population (number=70);2. Evaluation the population;3.while Number of generations < 100 do4.Select the fittest chromosomes in the population;5.Perform crossover on the selected chromosomes to create offspring;6.Perform mutation on the selected chromosomes;7.Evaluate the new population;8.end while9. Selected the fittest chromosome from current population and output it as A.In Algorithm 2, the evaluation of population is to compute smallest possible fitness for each chromosome in population. If smallest possible fitness is replaced by largest possible fitness, then Algorithm 2 can be used to compute the highest largest cost reduct.The objectives of the following experiments are: 1. comparing costs derived by different approaches when facing multiple cost matrices; 2. comparing the effectiveness of different reducts shown in Section 4. All the experiments in this section have been carried out on a personal computer with Windows 7, Intel Core 2 DuoT5800 CPU (2.00GHz) and 2.00GB memory. The programming language is Matlab 2010a. The data sets used are outlined in Table 3, which were all downloaded from UCI Repository of machine learning databases [3].In Section 3.3, we have presented smallest and largest possible costs those can be computed by objects in different regions of θ decision-theoretic rough set. Since θ decision-theoretic rough set was proposed based on multiple cost matrices, then it is interesting to analyze the differences between costs come from decision-theoretic rough sets based on multiple cost matrices and single cost matrix. To achieve such goal, we will compute four different types of costs, they are smallest possible cost, largest possible cost, maximal cost and minimal cost. The expressions of smallest and largest possible costs have been shown in Eqs. (19) and (20), respectively. The mathematical expressions of maximal and minimal costs are:(24)Maximal0.35em0excost=APTARANORMALmaxmi=1STaCOSTATi;(25)Minimal0.35em0excost=APTARANORMALminmi=1STaCOSTATi;whereSTaCOSTATiis the decision cost for the ith cost matrix, i.e., the cost derived from Yao’s decision-theoretic rough set that is constructed based on the ith cost matrix. Maximal/minimal costs are the greatest/least values of m decision costs, which are derived from m Yao’s decision-theoretic rough sets because each cost matrix supports one Yao’s decision-theoretic rough set.In this experiment, we have generated 100 cost matrices for data sets shown in Table 3 randomly. The values in each cost matrix are in [0,1] with the condition such thatλPPi=λNNi=0because it is assumed that the correct classification will not bring us cost.Fig. 3shows the detailed change trend lines of four costs with the increasing of θ (10 different values of θ). Each sub-figure is a result on one data set. In each sub-figure of Fig. 3, the x-coordinate pertains to value of θ, whereas the y-coordinate concerns the value of cost.Through a careful investigation of Fig. 3, it is not difficult to draw the following conclusions.1.Both smallest and largest possible costs are not strictly monotonic increasing or decreasing with the increasing of θ.For all the tested data sets, the largest possible costs are smaller than maximal costs with ten different values of θ, in other words, by comparing the maximal cost derived from multiple Yao’s decision-theoretic rough sets, our largest possible cost approach may be better when facing multiple cost matrices.Though for some data sets, the smallest possible costs may be same to or greater than the minimal costs for some values of θ. We noticed that the frequency of such a situation is not too high and in most cases. Smallest possible costs are well below the minimal costs, in other words, by comparing the minimal cost derived from multiple Yao’s decision-theoretic rough sets, our smallest possible cost approach may also be better when facing multiple cost matrix.In this subsection, we will compare the performances of heuristic algorithm and genetic algorithm for computing reducts. Such comparison is based on the different costs. Similar to the experiment shown in Section 5.1, we will also use 10 different values of θ in this experiment.Fig. 4shows the detailed change trend lines of heuristic algorithm (Algorithm 1) and genetic algorithm (Algorithm 2) for computing costs of reducts. Each sub-figure is also a result on one data set. In each sub-figure of Fig. 4, the x-coordinate pertains to value of θ (10 different values of θ), whereas the y-coordinate concerns the value of cost comes from reduct.Through a careful investigation of Fig. 4, it is not difficult to draw the following conclusions.1.For some data sets and some values of θ, the costs derived by heuristic algorithm (Algorithm 1) may be greater than the original costs, that is to say, heuristic algorithm cannot guarantee the decreasing of smallest and largest possible costs, mainly because the objective of heuristic algorithm is to pursue greater lower approximations instead of smaller costs.For all the data sets, the costs derived by genetic algorithm (Algorithm 2) are same or lesser than the original costs. The costs derived by genetic algorithm are also same or lesser than the costs derived by heuristic algorithm, i.e., genetic algorithm is better than heuristic algorithm for obtaining lower costs.In this subsection, we will further compare the numbers of decision rules derived by θ decision-monotocity reducts and cost reducts.Fig. 5 shows the detailed change trend lines of heuristic algorithm and genetic algorithm for computing (IP) rules. Each sub-figure is a result on one data set. In each sub-figure of Fig. 5, the x-coordinate pertains to value of θ (10 different values of θ), whereas the y-coordinate concerns the number of (IP) rules. Moreover, “Reduct 1” expresses the computation results of θ decision-monotocity reducts, “Reduct 2” expresses the computation results of smallest possible cost reduct, and “Reduct 3” expresses the computation results of largest possible cost reduct.Through a careful investigation of Fig. 5, it is not difficult to draw the following conclusions.1.For all the data sets, the numbers of rules derived by θ decision-monotocity reduct (Reduct 1) are same or greater than the original numbers of rules, mainly because θ decision-monotocity reduct is to pursue greater lower approximations and objects in lower approximations are used to generate (IP) rules.Though θ decision-monotocity reduct can increase the numbers of rules, an interesting issue to be noticed is that for many data sets, the numbers of rules derived by smallest possible cost reduct (Reduct 2) are greater than those derived by θ decision-monotocity reduct (Reduct 1). The reasons are: (1) the objective of smallest possible cost reduct is to pursue cost as lower as possible, (2) in our experimental setting,λPPi=0expresses that (IP) rules are correct classification. It will not bring us cost and then the aim of pursuing cost as lower as possible will provide us more (IP) rules.

@&#CONCLUSIONS@&#
In this paper, we have developed a general framework for the study of decision-theoretic rough sets when facing multiple cost matrices. Based on voting fusion scheme, a θ decision-theoretic rough set was proposed. Such rough set includes two special cases: optimistic and pessimistic decision-theoretic rough sets. Moreover, based on the model of θ decision-theoretic rough set, we also presented two strategies to compute costs in decision systems. One is to generate cost as smaller as possible, and the other is to generate cost as larger as possible. Finally, decision-monotocity criterion and cost criterion are introduced into θ decision-theoretic rough set and then θ decision-monotocity reduct, smallest possible cost reduct and largest possible cost reduct are proposed, respectively. To compute decision-monotocity reduct, heuristic algorithm is used. Genetic algorithm is employed to compute cost reduct. The experimental results show us: (1) through θ decision-monotocity reduct, we can obtain greater lower approximations and more rules; (2) by smallest/largest possible cost reducts, genetic algorithm can help us to generate lower smallest/largest possible costs; (3) smallest possible cost reduct is better than θ decision-monotocity reduct because it may generate lower costs and more rules.The present study is the first step to multicost based decision-theoretic rough set. The following are challenges for further research.1.Only the criterions of “smallest possible” and “largest possible” are used to compute costs in this paper. How to compute costs based on θ decision-theoretic rough sets with different values of θ is an interesting issue to be addressed.Construction of decision-theoretic rough set based on cost matrices with complicated data type, e.g., interval-valued cost [25].How to use multicost based decision-theoretic rough sets to deal with multiple categories [16,50] problem is a challenge.It is assumed that each cost matrix is given by an expert and the weights of all experts are regarded as the same in this paper. Multicost with different weights may bring us new topics.