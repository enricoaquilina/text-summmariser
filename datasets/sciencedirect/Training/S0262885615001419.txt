@&#MAIN-TITLE@&#
Event-enabled intelligent asset selection and grouping for photobook creation

@&#HIGHLIGHTS@&#
An end-to-end, metadata-driven intelligent system for automatic photobook creationA flexible intermediate story representation for creating multiple product typesAutomatic generation of representations for event-based and theme-based groupingsA new pagination algorithm for mapping selected images onto photobook pagesMetadata-aided UI enabling users to view groupings and find alternative images

@&#KEYPHRASES@&#
Metadata generation,Event detection,Asset selection,Image quality,Semantic understanding,Consumer image collection,Photobook,

@&#ABSTRACT@&#
The process of creating a photo product, such as a photobook, calendar or collage, from a large personal image collection requires intensive user effort. The primary goal of the current research was to develop an end-to-end solution to the problem of photo product generation that enables the user to complete the process with minimal edits, where the system intelligently selects assets and groups them before presenting the output to the user. The automation is driven by metadata extracted both from individual images as well as from sets of assets in a collection. In particular, we use an automatically detected event hierarchy to establish meaningful groupings in the assets, and to determine an appropriate grouping and pagination for the final product. We propose a novel intermediate construct, called a storyboard, which can be translated to different product types without recomputing the underlying metadata. In addition to chronological storyboards, we also describe a novel hybrid storyboard that joins chronological image presentation with groups of images of a common theme. A pagination algorithm uses the information in the storyboard and the product constraints to generate a product. Finally, the user is provided with a metadata-driven editing mechanism that makes it easy to change the auto-populated product. Given that the proposed system envisions user interaction in creating the final product, user studies are conducted to judge the usefulness of the system, where consumers use the system to generate a photobook with their own images.

@&#INTRODUCTION@&#
The proliferation of low cost, high quality digital capture devices such as digital cameras, smart phones, and wearable cameras has enhanced the opportunities for picture taking while expanding the availability of capture metadata, usage data and image-based social interactions. Since users are now amassing vast collections of media assets—both digital images and video—browsing these assets has become increasingly difficult due to the sheer volume of content. In addition, with collections that include thousands of still image and video files, selecting and presenting desired subsets of the collections becomes a formidable task for the average consumer interested in generating a photo product such as a collage, photobook, or calendar. Moreover, the large collections of personal and social media assets make it increasingly difficult for consumers to retrieve specific images or videos from various events or to select interesting scenes for creating multimedia stories or slideshows for sharing.There are a number of commercial offerings for photobook creation (e.g., Shutterfly, Google Auto-awesome, Flickr photobook, Kodak Moments app, CeWe) for automatically taking a set of multimedia assets and creating a photobook of a specified size making use of some metadata. A precursor [1] to the system described here used a rule-based system to select assets to automatically create different types of story albums. The problem of asset selection is also described in [2]. However, asset selection is only part of the problem; a complete solution must not only select the appropriate assets, but also group them in a product-appropriate manner. Research interest in the problem of automatically creating multimedia products from consumer images was spurred on by a challenge proposed during the 2009 ACM Conference on Multimedia. Gao et al. [3], Chu and Lin [4], and Sinha et al. [5] were among the papers that addressed this challenge to various degrees. Gao et al. [3] demonstrated a complete photobook creation system combining user interaction with some automated components. Chu and Lin [4] and Sinha et al. [5] focus on summarization using temporal characteristics and duplicate detection respectively. Much of the recent research in enabling automated or semi-automated storytelling from consumer collections has been in the context of online social networks [6–8], as these can provide a rich source of metadata and contextual information. In [7], Obrador et al. describe a system that uses a consumer's online photo albums to learn the users' social context and assist them in creating a photo album for sharing. Their work includes face and image esthetic ranking in order to identify the best images to use. In [8], Saini et al. also use information from a subset of a user's friends from their social networks to enrich the available assets and metadata for creating a meaningful story. Using a limited definition of an album (i.e., small number of images and no pagination), Sadeghi et al. [9] propose context-sensitive image selection to represent vacation photo collections. Another approach being followed recently is to focus on providing a supportive user interface to help the user find and select assets to use in the product, rather than trying to automate the asset selection process. In [10], Chen et al. describe a human-centric interface for creating story narratives. In [11], Karlsson et al. describe a multiscale timeline that helps the user view the assets in the collection efficiently, even on a mobile device with a small screen.In general, a user may spend a lot of time editing the auto-populated products generated by these systems due to the poor quality of asset selection and image placement. These systems also typically require the consumer to re-do the asset selection process all over if they wish to go from one output modality to another, such as from an 8×10 photobook to an 8×12 photobook, or from a photobook to a calendar. To address this problem, we present a novel reusable, product-independent ‘story’ representation that can be used to generate many different product types. Moreover, current solutions typically only order photos sequentially, either based on chronology or upload order. However, consumers often like to create photobooks that may only loosely follow chronological order. For example, a manually created photobook may often have one or more pages that are dedicated to a particular theme, where the multimedia assets associated with the theme were captured at various times. Our system provides theme-based image groupings that can be incorporated into a chronologically ordered image product. While much of the published research in this area focuses on the use of contextual information from social networks to produce meaningful albums, our work focuses on using derived metadata from image understanding algorithms. Therefore our method is applicable to unannotated images in the user's primary collection. While our approach benefits from the additional metadata available from social media platforms, it does not require it.In this paper, we introduce an end-to-end, metadata-driven system with an easy-to-use user interface for automatically creating and editing multi-page photobooks from a consumer image collection. The distinctive contributions of our system include a) a flexible intermediate representation of a story from which multiple product modalities may be created; b) the story features an ability to include thematic groupings in addition to the traditional event-based (chronological) groupings; c) a new pagination algorithm for mapping selected images onto photobook pages that respects boundaries between groupings; and d) a metadata-aided user interface that makes it easy to find alternative images for addition or replacement.The paper is organized as follows. In Section 2, we give an overview of the automated photobook creation system. Section 3 describes the key algorithms and component technologies employed by the system. In Sections 4 and 5, we present the storyboard generator and pagination algorithm respectively. Section 6 summarizes user testing results of the system for creating a photobook product. Concluding remarks and future work are in Section 7.Our target user is one with a large collection of images from which they would like to make a photo product such as a photobook or calendar. This collection may consist hundreds of pictures from a single event such as a vacation or a special family gathering; or it may encompass many events over time. It can include pictures gathered from their online social networks and mobile devices. The typical manual workflow requires the user to browse through their collection and select a set of images to use for the photo product. The selection step is time consuming and usually needs further refinement during the actual photo product generation step. The user must manually lay out their set of images in the photo product using a computer, web, mobile or kiosk-based software. The layout step is extremely time consuming as well, and requires decisions on the final images to use in each page. This process then needs to be repeated if a different photo product is subsequently desired.The main area of focus in this paper is to provide an end-to-end solution to the problem of photo product generation that presents an automatically generated photo product to the user, as well as a metadata-enhanced editing mechanism that enables them to create a final product with minimal effort.Fig. 1shows an overview of the system with its components. The key to making the problem manageable is metadata. Some metadata is simply extracted from the camera-generated EXIF data. Other metadata is algorithmically derived, either on an individual image basis, or from sets of images. For example, [12] face detection and color analysis are performed on individual images; event clustering, near duplicate detection, and facial clustering/people recognition are performed on sets of images. The metadata is stored in a metadata repository using the flexible RDF (Resource Description Framework) model [13].We propose a new intermediate construct, called a storyboard, that maps the asset metadata into a form that can be translated efficiently into any product. A storyboard represents a particular way of grouping, ordering, and prioritizing the media assets in a multimedia collection. The behavior of the storyboard generator is specific to the type of storyboard that is being generated. Our storyboard generator supports multiple grouping and ordering paradigms, each with its own way for prioritizing objects. For example, the hierarchical grouping may reflect the event structure of the collection with the importance of individual images encoded in a priority score. The storyboard representation is expressed in an XML format and does not contain product-specific layout information. A given storyboard representation can be easily mapped to any number of output products, e.g., photobooks of different sizes, a collage print or a DVD.This paper also describes the generation of a new story format called a hybrid story. This story type combines a chronological presentation of images following the underlying event structure with a thematic presentation that gathers images of a common theme together to produce a more creative output product. The themes are automatically detected based on commonality of features derived from metadata.Each photo product provides constraints specifying the number of pages, pictures per page, etc. A pagination algorithm creates a product from the storyboard representation by selecting an appropriate subset of images using the priority scores, generating page breaks based upon the story hierarchy and image emphasis scores.The layout of images on the pages and the selection of background images are not described in this paper, but also form a part of the overall system. The photo viewer presents the user with images and video keyframes laid out on virtual pages and presents a metadata-assisted editing capability.The temporal event clustering algorithm [12] is a fundamental image organization tool for efficiently clustering a user's images into separate events and sub-events. The algorithm leverages capture date/time metadata and color histogram information of the images for clustering. Events are detected by performing a two-means clustering on a histogram of time differences between chronologically adjacent images or videos. The output of the algorithm determines whether the separation between two adjacent images belongs to one of two classes: 1) an event boundary that corresponds to a relatively large time difference between two chronologically adjacent images, or 2) a non-event boundary that corresponds to a relatively small time difference between two chronologically adjacent images. Then within each detected event, visual similarity based on global and block-based color histograms is used to detect possible sub-event groupings.For a potentially large event like a vacation, the algorithm may produce multiple event groupings. In those cases, it may further group these events into super-events. This is accomplished by extending the algorithm in [12] to group nearby events into super-events using a density-based clustering method. In addition, the algorithm in [12] was also extended to further group images within a subevent that were taken at approximately the same time and having a high degree of visual similarity into near-duplicate clusters. Hence the current hierarchy for event clustering contains four levels: 1) super-event, 2) event, 3) sub-event, and 4) near-duplicates.Metadata includes data recorded by the capture device—e.g., EXIF data that includes various capture settings associated with an image such as f-stop, speed, and flash information; as well as the capture timestamp and, increasingly, location. Metadata also encompasses user-provided metadata, such as that provided via a user interface located on the capture device or an image editing application interface on a computer. Another source of human-generated metadata comes from the social contacts of the user when the image is shared on a social network. Such metadata can include the number of people “liking” the image as well as any comments. Lastly, metadata encompasses derived metadata, such as metadata computed by face detection or event classification algorithms that are applied to media assets post-capture. (Table 1lists some examples of different types of metadata.)Derived metadata can be broadly categorized into low-level features and semantic features. Low-level image features are computed directly from the pixel content of the image, such as color, texture or edge information. Low-level image features provide an indication of the visual content of an image and may be useful for determining the similarity of appearance between two images. We include colors present in an image as a low-level feature. Typically, color is represented by a color histogram of the image. To achieve a fixed number of levels, we translate the native RGB color space to thirteen named colors using the ISCC-NBS color naming system. To ensure that the detected colors match the perceived color regions in an image, we determine spatially coherent color regions in an image before translating the colors from coherent regions into the color name space.Because pictures of people form an important segment of consumer collections, it is important to capture as many aspects of faces in an image as possible. Some relevant facial features have been studied in [9]. Both commercial and open-source software products (such as OpenCV, Intel Perceptual Computing and bob) are available for face detection and characterization. These products can be used for obtaining face-based metadata features such as a count of the number of faces in the image, approximate sizes of faces and locations of faces. In addition, faces can be grouped by similarity into face clusters. Face clustering uses data generated from facial detection and feature extraction algorithms to group faces that appear to be similar. In our system, the output of the face clustering algorithm is new metadata, namely, a new object representing the face cluster is created; each media asset containing a face that is part of the face cluster receives a metadata item indicating that the asset contains a person as represented by the face cluster object.The current implementation uses low-level image quality and color classifiers, along with algorithms for face detection, facial feature analysis and people clustering. There has been extensive promising work towards determining the semantic content of images, resulting in scene classifiers such as city, nature, beach, and sunset [14,15]. The database described by Xiao et al. [16] features close to four hundred scene categories. Some of these scene classifiers use information from the EXIF header (such as flash fired or focal length), in addition to pixel-based information. Specific detectors such as flower, text, pet etc. can be developed using bag-of-words features that combine low-level color and texture features, while more complex semantic classification requires the incorporation additional cues. Juneja et al. [17] use a parts-based approach.In [18], Bhattacharya et al. describe a method for recognizing complex events using time-series modeling of concatenated concept classifier scores. Lazebnik et al. [19] describe a method based on spatial pyramid matching. Scene and object classifiers have been combined to describe the event class in [20]. While the current prototype does not employ such classifiers, its RDF data model enables them to be easily added in the future.Das et al. [21] describe identifying recurring patterns in a metadata database using a frequent pattern mining algorithm on the set of discretized metadata from an image collection. The frequent pattern mining step identifies a set of frequent itemsets, where each of the frequent itemsets is a co-occurring feature descriptor group that occurs in at least a predefined fraction of the digital images. The images corresponding to each frequent itemset generally represent a theme in the image collection. For example, a frequent itemset corresponding to the color features “orange” and “red,” and time-based features “evening” and “summer,” may produce a group of images of sunsets. The frequent itemsets detected depends on the images in the image collection. Groupings based upon common features are used as part of the hybrid storyboard, described further in Section 4.2. The general algorithm for frequent itemset mining treats all features equally; however, we use our domain knowledge to prioritize and limit the set of possible features.The ability to automatically determine the quality of an image is an important function of our system. Given the fact that a large majority of consumer images contain people or faces, we have developed a facial attribute assessment algorithm that uses face detection along with facial feature extraction algorithms to assess the subjective quality of each face in an image. Faces that are larger and more centrally located are weighted higher than smaller faces off to the side. A vector of weighted facial attributes including face size, location, expression, blink, and pose is passed through a dimensionality reduction step that enables a computationally more efficient and more discriminative algorithm [22].The dimensionality reduced weighted facial features feed a multiclass support vector machine classifier. The output of the classifier is a class score. These scores are tied to human preference through a large psychophysical study on facial preference and facial composition in an image as described in [22]. The proposed method also includes a fitness evaluator, which takes the output of the classifier along with the output from the facial fusion step, to produce a single fitness score per image. This final fitness score can then be used to objectively rank the input image. If all images in a collection are assigned a fitness score, the images can be sorted from high to low, where higher scoring images are expected to be more pleasing to the average consumer.This face-based quality ranking can be combined with other image quality metrics (e.g., sharpness, contrast, tonescale) or fitness scores. The asset selection system uses an efficient image quality measure based in part on sharpness, contrast and colorfulness; a predecessor of the algorithm is described in [23]. In our algorithm, a four-layer neural network with 20 and 10 nodes in the internal layers is used to generate a quality score at the output layer given six image features in the input layer. The features used are brightness, contrast (standard deviation of luminance), two measures of colorfulness, and two measures indicating sharpness of edges. The network is trained using a large collection of consumer images that has been rated by image quality judges. These two algorithms are used together in the asset selection system for providing ranking related to the overall image quality. There is new research on determining the memorability of images [24] and prediction of the potential for generating ‘likes’ from friends [25] that go beyond measures of low-level image quality. These features could also be incorporated into the image ranking process.The goal of the storyboard is to represent a particular way of organizing a collection and to specify an ordering and prioritization of the assets in the collection, all in a product-agnostic way. For example, the storyboard representation may capture a chronological paradigm, in which media assets belonging to an event or super-event are summarized, ordered, grouped, and prioritized in a hierarchical event structure. Another example is the hybrid story proposed in this paper that inserts the time-independent thematic paradigm within a primarily chronological storyboard. In general, the storyboard groups media assets into a tree-like segmented structure, where the semantics of the grouping depends upon the story type.The event story structure is generated using the temporal event clustering algorithm described previously. Fig. 2shows a graphical representation of a story structure for a set of media assets. In this tree structure, the second-level sequential segment(s) correspond to super-events, and the next level to events as shown. The ordering of assets is captured by classifying each segment as one of three types: sequential, parallel or alternate. Sequential segments or groupings mean that their children represent content that should be presented in sequential order. A parallel segment is one in which the child content may be displayed in parallel. Media assets in parallel segments can be displayed in any arrangement, order, or sequence. In the context of a storyboard, sequential segments indicate content where order is important, i.e., where content needs to be shown in chronological order. Parallel segments represent content where the order is not important. An alternate segment type indicates that the children are related such that only one representative is to be selected from them, as would be the case with near-duplicate images.In some cases, respecting chronological order at a fine level, such as level of sub-events, is not important. For example, consider a family vacation to Washington, D.C., with trips to two different Smithsonian museums on a single day, one in the morning and one in the afternoon. Users would expect a chronological ordering of the captured media assets to order the morning images before the evening images, and may expect even that the different exhibit areas to be ordered sequentially. However, it is often not necessary that every picture be in strict sequential order; if several pictures were taken at a given exhibit, the user may desire to have the pictures ordered in whatever arrangement produces the most esthetically pleasing result. This gives the system more flexibility in determining the final view-based representation because there is no constraint on the order of presentation of the media assets in a parallel segment. Non-sequential presentation of vacation images has also been explored in [9]. Thus, media assets in such segments may be arranged in a manner that makes the best use of spatial constraints of the particular output modality. In the case where a storyboard is mapped to a physical, hardcopy output such as a photobook, images grouped in a parallel segment can be arranged in the manner that is visually most appealing because no one image has to strictly follow or precede another.We propose a novel storyboard type that combines a predominantly chronological presentation of the assets based on the event structure, with time-independent theme-based groupings that are based on commonality of features. The hybrid story aims to emulate a creative approach towards photobook generation that goes beyond a simply chronological ordering. Consumers often like to create photobooks where one or more pages are dedicated to a particular theme, e.g., a person or a backdrop. For example, in a vacation photobook covering one week, the images may be laid out in a loosely chronological ordering, but there may be a page dedicated to all the sunset images that were taken over the week; and there may be a page containing the photographs of all the family members who went on the vacation taken against different backdrops. The “sunset” theme may be captured by a thematic group that has common elements on color and time of day. The “family” theme may be captured by a thematic group that has the number of faces and the face identities in common.Fig. 3illustrates the process of incorporating thematic groups into the event-based grouping. From the thematic groups generated from a collection by thematic clustering, the smallest event grouping containing all the pictures in the thematic group is identified for each of the top ranking thematic groups. This smallest event grouping could be at the level of a super-event, event or even sub-event. In some cases, the smallest grouping could be the collection itself when the thematic group is a summary across the whole collection.For each event grouping, a set of thematic groupings are identified that are above a threshold score. Each detected frequent itemset (a thematic group) is assigned a thematic group score which is computed as a sum of the category-level weights for each of the features in the itemset.11Some of the example features identified in Table 1 are not used in the current prototype due to performance level of the available classifier or the rarity of the category, but may be added in the future.This method of computing thematic group scores will generally result in thematic groups that include face-based features receiving a higher score than thematic groups that include only time-based and content-based features. An itemset that is a combination of multiple features will generally have a higher score than one comprised of a single feature. In the absence of face-based features, multiple other features may be needed to exceed the minimum threshold score for inclusion in the final list.For thematic groups matching the threshold criterion, a segment is created containing the images from the group, and the resulting segment is inserted as a sibling of the event group in the storyboard structure. The thematic group would then appear as a summary page before or after an event grouping in the final product layout.Within each segment in the storyboard structure, a priority score and an emphasis score are computed for each asset. The image priority score is computed using a multi-phase scoring heuristic that uses two components: 1) the scene score that captures the technical quality of the image, including features such as sharpness, contrast, and colorfulness; and, for images with detected faces, 2) the facial score that indicates the quality of the faces in the image, including features such as size, location, sharpness of facial region, and facial expression. The scene scores for images within a containing (sub)event are normalized to ensure that if a group of images all have consistently low scene scores, which may be due to particularly challenging capture conditions, images can still be selected from the group. The facial score is adjusted based upon both the number of detected faces as well as the frequency at which a detected person appears within the collection of interest. As part of the metadata generation process, detected faces are clustered based upon similarity, with faces that appear to belong to the same person grouped together. If an image contains detected faces that appear to belong to people frequently appearing in the collection, then that image may be more important than other images. Of course, in the real world, exceptions to this heuristic will occur; one may have captured only one picture of a special relative or perhaps the primary photographer appears in only one picture. Berg et al. [26] propose an importance score based on a set of human-centric factors.The computed image priority score is adjusted based on various factors, such as whether or not the image is one of several near duplicates, or if the user had explicitly given the image a high rating using, for example, a five star rating system. The scores of images belonging to near duplicate sets are increased under the assumption that they represent an important moment—one important enough that the photographer attempted to capture the moment multiple times.The story generation algorithm also flags certain images as being emphasis images—candidates for receiving more area on a page when the product is laid out and rendered. In determining emphasis, the system considers image-specific criteria, such as the relative importance of people portrayed in the image, any user-provided image rating, and whether the image's priority score is above average.The computed priority and emphasis scores become part of the storyboard. Storyboards are serialized to an XML-based format, which is then passed to the part of the system responsible for mapping storyboards to specific products. Separating the story representation from product representation provides some useful engineering and end-user benefits. In particular, the same storyboard may be used to drive the creation of multiple product representations for potentially different output modalities. This can save both the consumer and the system considerable effort.The pagination algorithm takes as inputs the storyboard structure along with product-specific parameters such as the maximum number of pictures per page and the desired number of pages, and produces a mapping of selected images onto pages of a given output product. Typically only a small subset of the original collection is actually used in making a photobook; the pagination algorithm determines which pictures are to be used based upon the previously computed image scores and the product size constraints. The pagination algorithm is reminiscent of, and indeed is based on, a dynamic programming algorithm for determining how to break paragraphs into lines of text [27]. Unlike a line breaking algorithm, our algorithm has the ability to vary the number of included assets in order to come up with an optimal pagination relative to the specified target number of pages and average picture density per page.The assets to be paginated are organized into a hierarchical group structure in the storyboard. For the purposes of this algorithm, a group is defined as a set of images sharing some common characteristic. The currently supported groupings are temporal groupings, where the groupings correspond to near-duplicates, sub-events, events and super-events; and thematic groupings, where the groupings correspond to algorithmically determined commonly held characteristics. The two groups' types are paginated separately; content from different group types may not appear on the same page. A page is defined as a single side canvas typically corresponding to the surface of one physical page in a photobook, and a spread as two facing pages in a photobook. The algorithm may be run in two phases, first to find the optimal breaks between spreads, and then, for each spread, to determine the optimal page break within that spread.For each asset ai, we have the following data:•priority score,emphasis score,group identifier and type for all enclosing groups, and thetimestamp or sequence number (if content is to be grouped sequentially).In a user-guided creation workflow, the user selects the product (e.g., a 11×14 inch photobook). However, one could also imagine a workflow where the product type is automatically determined by the system. The pagination algorithm requires the following parameters:•Pmax, the maximum number of pages;Pdesired, the desired number of pages; if unspecified the number of pages is determined by the content groupings;Pmin, the minimum number of pages;Emax(g), the maximum number of assets (elements) per page, as a function of the group type;Edesired(g), the average desired number of assets (elements) per page, as a function of the group type; andEmin(g), the minimum number of assets (elements) per page, as a function of the group type.In a typical situation, where the number of assets is considerably more assets than can fit on a reasonable number of pages, the algorithm uses the asset priorities to determine which assets are to be included. The main steps in the algorithm are:1)Compute the expected number of pages (or spreads):a)Compute the number PG, the number of pages needed for thematic content, by computing the number of thematic groups present in the story.Compute the number PC, the number of pages needed for chronological content, as PC=Pdesired−PG.Compute the target priority threshold:a)Order all the assets in priority order.Set the priority threshold pthresh to be the priority of the asset at position PC×Edesired.i)If applying emphasis, then the priority threshold is instead set to the smallest value j such that∑i=0j+1Weightai>PC×Edesired.Let Aselbe the set of all assets aisuch that priority(ai)≤pthreshordered by time (or sequence number), numbered a1…an.Given the above, the algorithm can now paginate assets Aselusing a dynamic programming approach. We construct a virtual network, where each node represents a feasible breakpoint, and where the edges between nodes represent the cost or weight of that breakpoint. For a node (breakpoint) aj, there is a conceptual link to the previous breakpoint aisuch that the best or lowest cost pagination that includes a break at (after) aj, also includes a breakpoint at (after) node ai.Each edge Ei,jrepresents the cost of having assets ai+1, …, ajall on the same page. The cost function is based upon several factors, including the difference between the resulting number of images on the page and the desired number of pictures on the page, and the cost of having a break after image aj. The cost of a break between image ajand aj+1 is based upon the degree of commonality in group memberships for the two images. For example, if the two images belong to different events, then introducing a break between the images is much more desirable than if the two images belong to the same subevent. The total cost may be expressed formally asCostij=Costi+Edesired−∑k=i+1jWeightak+Penaltytogetherij+Penaltyafterj.The value of function Penaltytogether(i,j) is dependant upon whether ai+1 and ajbelong to the same subevent (no penalty), same event but different subevents, same superevent but different events, or different superevents (highest penalty). Similarly, Penaltyafter(j) measures the cost of having a page break after aj; it has the value of zero if ajand aj+1 belong to different subevents, and the highest value if ajand aj+1 belong to the same subevents.The optimal pagination as determined by dynamic programming may differ from the target number of pages. Consequently the complete algorithm uses a binary search strategy, varying the number of included assets until the pagination sequence that best satisfies the initial constraints has been found.The success of an automated photo creation system is determined by the user's perception of the performance of the system. We assessed the performance of our system by conducting two first-party user tests of our system, along with a separate qualitative user study and a detailed comparative study of publicly available third-party photobook applications.Before describing the study results, we briefly describe the user experience implemented in our prototype.The workflow of the asset selection and pagination application has five steps as depicted in Fig. 4. In the first step, the user selects a collection of pictures from which the photobook will be made. This selection made by the user is not intended to be discriminating at the level of individual pictures. Rather, the user is expected to simply select a large superset of images that will appear in the final photobook. These pictures can come from the user's local collection, or from an online social media source such as Facebook. Next, the user selects the type of storyboard to create. The type of storyboard may be a chronologically based event story, or a hybrid story that includes thematically based structures. The user then selects the type of product that is desired. For these studies, the type of product was restricted to photobooks of different sizes and aspect ratios. Within this step, the user also sets parameters specific to the type of product. For example, the user may specify the number of pages, the average number of images per page, and the layout style. With this information, the system automatically generates and populates the photobook and presents a digital representation to the user for editing and approval. Finally, when the user is satisfied, a final output version is generated. This output version may be in the form of a physical photobook, or it may take the form of rendered pages that are uploaded to a social media site such as Facebook.Within the editing phase, the user is provided with a view of the virtual photobook and a suite of editing tools with which to make changes. Many of these editing tools are standard and common to any photobook creation process. However, the behavior of several editing operations is enhanced by using knowledge of the storyboard.Fig. 5illustrates an example of one of these storyboard-aware editing operations. In this screen shot, the user is attempting to find a better picture of the girl in the flowered sundress. When the user clicks on the picture of the girl, an Image Operations panel appears from the right. When the user selects “Replace Image”, an image tray appears from below. The left side of the tray is occupied by a set of buttons reflecting the storyboard grouping hierarchy. The lowest level grouping contains near duplicates of the selected image, while the higher level groupings contain other images within the same sub-event or event of the selected image. In this case, the lowest level is selected and the remainder of the tray is occupied by images that might be used to replace the one on the page. The image on the left is the one currently placed on the page; the user has selected the image on the right to replace it. If the user did not find an appropriate replacement image at this level of the hierarchy, more images would be progressively be made available by selecting the corresponding hierarchy button. This approach has the advantage of guiding the user to an appropriate image quickly, while not limiting access to the entire collection if so desired.Other operations can similarly be made storyboard-aware. In adding an image to a page, the system looks at the existing contents of the page to assess which hierarchical portion of the storyboard to display. Similarly, when inserting a page, the system can examine the preceding or following page to determine an appropriate hierarchy.Editing operations that are related to the processes of creating and automatically populating the photobook are recorded for use during user studies. The number of edits performed by users is a measure of how closely the algorithms came to creating a product deemed acceptable by the user.This user study focused on obtaining a quantitative estimate of the level of effort a user had to expend to get an acceptable final product starting from the system-generated version. In the first part of the study, authors used their own personal image collections to create photobooks covering events such as vacations, family gatherings and a wedding. In the second part of the study, eight participants who were not familiar with the system were recruited to work on a photobook using their own images. The number of images that needed to be touched during the editing process was counted, including the number of additions, deletions and replacements from the original selection provided by the system. Tables 2 and 3show the data collected from the two parts of this study; including the number of images in the system-generated photobook (n), the user-performed deletions and additions (d, a), and the number of images from the system generated set that were retained by the user (r).Precision, P, is defined as the fraction of the system-selected image set that were retained by the user in the final product:P=r/r+d.This number captures the effort made by the user in deleting unwanted images, and in many cases replacing them with other images from the collection.Recall, R, is defined as the fraction of the set of images in the final product that were also in the system-selected image set:R=r/r+aThis number captures the effort made by the user in adding new images from the collection that the system-selected set had missed.Since the purpose of the study was to evaluate automatic asset selection, swapping images either within a page or between pages was not taken into account. Other edits that primarily improve the subjective appearance of a page such as image cropping, tilting, overlaps and image enhancements were also not considered.Both the average precision and average recall is 80% over all photobooks in Table 2 (authors' collections), and 87% in Table 3 (novice users' collections). The combined average recall and precision are both 83% for this study. Since both sets of photobooks were created by the owner of the photos, no significant difference in performance was expected. The authors made slightly more edits to their photobooks showing that novice users may be less inclined to make the effort required to edit the system-generated photobook. The average fraction of images used is 0.32 in Table 2 and 0.15 in Table 3, and this varies based on the size of the initial collection. For small collections, this fraction could be as high as half, whereas for larger collections it was less than one-fifth. Although the recall and precision scores do not appear to be affected by the fraction of images selected in this limited sample size, in general the error would be expected to increase as the fraction chosen decreases.For this study, eight consumers were recruited from the general population who fit the profile of a typical photobook maker. They were all women in the age group of 28–35, who had young children and regularly went on vacations. They were asked to bring a collection of 150–800 photographs from a vacation, without pre-selecting or deleting any images. They were asked to review the auto-populated photobook created by our system. Table 4shows a list of questions they were asked after the process, along with a count of the responses. It is noted that most users did not have any prior experience with automated photobook generation, and so were unable to comment on whether this system was an improvement over another automated workflow; but there was strong agreement about its improvement over their typical manual workflow.As can be seen from Table 4, the qualitative reactions to our intelligent photobook generation were overwhelmingly positive.For this study, photobooks were generated using a single set of vacation photographs (218 images) using three commercially available web-based photobook making sites.22The names are withheld in the paper due to our corporate disclosure restrictions.Table 5provides a comparison of each software with our system in terms of the intelligent features offered. The features listed in the column heading of Table 5 are all implemented in our system.Based on our comparisons, the existing systems that generate photobooks automatically lack some of the critical features that reduce the work needed from the user to correct or edit the auto-generated photobook. Part of the novelty of our system lies in the use of automation in every step of the photobook making process and in providing the user with an interface that is also metadata-driven, allowing them to easily locate images for replacement. Also, the absence of a product-independent story representation, the user needs to start the process from the beginning (selection step) if they decide to change the final product.Fig. 6gives examples of album pages that are generated by our system showing the main features listed in Table 5. The first and second spread show examples of page boundary selection that respects sub-event boundaries, as well as auto-cropping of images that is face-aware. Each page has images from a single sub-event in these cases. The third spread gives an example of a spread with no detected faces. The near duplicate detector is not able to group the similar sunset images due to the changing lighting. However, the pages still have interest generated by the variety of layouts used. The last two spreads show the effect of the emphasis score where people images are given larger area on the page when possible. The background selection is also highlighted where the system chooses non-people images for the background, when possible.We have described a flexible end-to-end solution to metadata-assisted photobook generation that can save a consumer time and effort. We use an automatically generated event structure to create a product-agnostic storyboard that provides a hierarchical grouping of the assets in the collection, along with a prioritization based on image characteristics. The storyboard is mapped to a product when the consumer selects the desired product type; the same storyboard can be used to generate different products without re-computation. The pagination algorithm uses product-specific information to select an appropriate number of assets based on priority score, and assigns them to pages. The virtual photobook generated is presented to the user to edit. The photobook editor also assists the user by presenting images from the event hierarchy as possible alternatives to replace the selected images. The underlying RDF data model and storyboard abstraction provide our system with great flexibility, enabling new types of metadata as well as new story types to be readily incorporated into the system.Using our system prototype, we collected user feedback about the photobook creation experience from users working with a collection of their personal images from a large event such as a vacation. A numerical count of edits needed to finalize the system-generated photobook shows a high percentage of images remaining untouched by the user, demonstrating good system performance in both asset selection and the placement of the images on pages. In a separate study, qualitative feedback from the target demographic group of active photobook makers was extremely positive.We plan to explore other storyboard types such as weddings, graduations, year-in-review and tributes. Future work also includes tighter integration between multi-modal elements in the photobook such as text, image sequences from video and background elements. Finally, the quality and availability of media analysis algorithms continues to improve; we hope to incorporate additional types of metadata generators into the asset selection algorithms. In particular, there have been great strides in scene classification and image tagging using convolutional neural networks and advanced deep learning approaches [28,29]. These semantic tags can enable event identification [30] for improving grouping, layout decisions and styling of the photobook. Incorporating automatically generated text may also become an interesting area for the future with the availability of captioning algorithms [31,32].

@&#CONCLUSIONS@&#
