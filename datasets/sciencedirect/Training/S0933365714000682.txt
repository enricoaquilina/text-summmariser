@&#MAIN-TITLE@&#
Recommendations for the ethical use and design of artificial intelligent care providers

@&#HIGHLIGHTS@&#
Ethical, moral, and legal issues associated with AI care providers are reviewed.The risks and benefits of AI care providers are evaluated.Professional ethics codes and guidelines need to be updated to address risks.Recommendations for ethics codes and the design of AI care providers are presented.Ethical use and design of these systems must be an imperative for all involved.

@&#KEYPHRASES@&#
Artificial intelligent agents,Ethics,Ethical codes,Practice guidelines,Care providers,Mental health,

@&#ABSTRACT@&#
ObjectiveThis paper identifies and reviews ethical issues associated with artificial intelligent care providers (AICPs) in mental health care and other helping professions. Specific recommendations are made for the development of ethical codes, guidelines, and the design of AICPs.MethodsCurrent developments in the application of AICPs and associated technologies are reviewed and a foundational overview of applicable ethical principles in mental health care is provided. Emerging ethical issues regarding the use of AICPs are then reviewed in detail. Recommendations for ethical codes and guidelines as well as for the development of semi-autonomous and autonomous AICP systems are described. The benefits of AICPs and implications for the helping professions are discussed in order to weigh the pros and cons of their use.ResultsExisting ethics codes and practice guidelines do not presently consider the current or the future use of interactive artificial intelligent agents to assist and to potentially replace mental health care professionals. AICPs present new ethical issues that will have significant ramifications for the mental health care and other helping professions. Primary issues involve the therapeutic relationship, competence, liability, trust, privacy, and patient safety. Many of the same ethical and philosophical considerations are applicable to use and design of AICPs in medicine, nursing, social work, education, and ministry.ConclusionThe ethical and moral aspects regarding the use of AICP systems must be well thought-out today as this will help to guide the use and development of these systems in the future. Topics presented are relevant to end users, AI developers, and researchers, as well as policy makers and regulatory boards.

@&#INTRODUCTION@&#
Nearly half a century ago Joseph Weizenbaum introduced ELIZA, the first simulation of a psychotherapist [1]. ELIZA, also known as DOCTOR, was a simple computer program that was capable of mimicking the question and response conversation of a psychotherapeutic interview. A few years later, psychiatrist Kenneth Colby developed a program called PARRY that simulated a person with paranoid schizophrenia [2]. Advancements in artificial intelligence (AI) and associated technologies, such as virtual reality, natural language processing, and affective computing have enabled the creation of artificial intelligent agents in the form of highly realistic simulated psychotherapists, counselors, and therapeutic coaches. These modern systems, which may be considered to be the conceptual evolution of primitive “chatterbot” systems such as ELIZA and PARRY, are capable of carrying on highly interactive and intelligent conversations and can be used to provide counseling, training, clinical assessment, and other therapeutic functions [3].The practice of mental health care entails significant ethical responsibilities that involve consideration of complex legal, moral, cultural, and personal factors. The professions of psychology, counseling, and psychiatry, for example, all have ethical codes of conduct that help guide ethical decision making and behavior of care providers; however, existing professional ethics codes and practice guidelines do not presently consider the current or the future use of artificial intelligent agents to assist or potentially replace humans in these professions. As history has demonstrated, rapidly changing technology can get ahead of the awareness of the greater population and thus laws and guidelines have to catch up with technology. My goal with this paper is thus to discuss emerging ethical issues associated with artificial intelligent agents that are designed to provide interactive mental health care services (i.e., psychotherapy, counseling, clinical assessment, etc.). Many of the same ethical and philosophical considerations are also applicable to the use of this technology in other helping professions such as medicine, nursing, social work, education, and ministry.I begin by providing an overview of current developments in artificial intelligent care providers (AICPs) in order to illustrate current capabilities and future directions of the technology. I also present a brief overview of professional ethics codes in order to orient the reader to the overarching values and ethical principles of the mental health care disciplines. I do not survey the depth of ethical theory or the technical aspects of the design of artificial moral agents as these are covered elsewhere [4–7]. Rather, I focus on practical ethical issues and what may be needed in future professional ethics codes, laws, and practice guidelines. I also discuss the pros and cons regarding the use of AICPs in the mental health care and other helping professions. The topics that I present are not only important for being ethical users of these technologies, but for the design of these technologies in order to make them effective at what they are intended to do. Thus, what I discuss here should be of interest to end users, developers, and researchers as well as professional organizations and regulatory boards in the years ahead.AICPs can be designed in various forms to interact with users including virtual reality simulations (avatars), robots (humanoid or non-humanoid), or non-embodied systems that consist of only voice simulation and environmental sensors. A leading area of development of AICPs is the creation of virtual human avatars that make use of advancements in virtual reality simulation, natural language processing, and knowledge-based artificial intelligence. Life-like virtual humans have been developed and tested for use in clinical training and skill acquisition and to provide care seekers with information about mental health resources and support [8,9]. SimCoach (www.simcoach.org), for example, is an online avatar-based virtual intelligent agent system that is designed to interact with and connect military service members and their families to health care and other helping resources [9]. Virtual intelligent agent systems have also been developed and tested to help medication adherence among patients with schizophrenia [10] and to provide patients with hospital discharge planning [11].Current developments in affective sensing and processing are providing artificial intelligent systems with capabilities to detect, interpret, and express emotions as well as detect other behavioral signals of humans that they interact with. For example, the Defense Advanced Research Projects Agency (DARPA) Detection and Computational Analysis of Psychological Signals (DCAPS) system uses machine learning, natural language processing, and computer vision to analyze language, physical gestures, and social signals to detect psychological distress cues in humans [12]. The intent of the system is to help improve the psychological health of military personnel as well as to develop and test algorithms for detecting distress markers in humans from various inputs including sleep patterns, voice and data communications, social interactions, Internet use behaviors, and non-verbal cues (e.g., facial gestures and body posture and movement).Robotics and other intelligent technologies are also advancing at an incredible pace and are finding very practical applications in the field of medical care. Robots that can interact with patients and medical staff, such as RP-VITA [13], are being tested and deployed in hospitals. IBM has developed an expanded, commercially available version of the natural language processing DeepQA system Watson that has learned the medical literature, therefore allowing it to serve as an interactive medical knowledge expert and consultant [14,15]. These emerging technologies have the potential to greatly expand the capabilities of AICPs through integration of them. In the future, it may be possible to build what I call the “super clinician” [3], an AI system that integrates optical and auditory sensors, natural language processing, knowledge-based AI, and non-verbal behavior detection that could be used to conduct psychotherapy and other clinical functions. Computer vision processing could be used to observe facial expressions, speech analysis technologies could assess inflection and tone of voice, and natural language processing could be used to detect semantic representation of emotional states. The use of infra-red sensors, for example, could enable such a system to detect physiological processes that are undetectable by humans and will make it far superior to the capabilities of human clinicians.The technological capabilities of AICPs are rapidly expanding and further public and private investment in AICP development and application can be expected. The practical applications of AICPs include screening, assessments, and counseling for self-care as well as for traditional clinical care in both government and private health care institutions. Other practical applications include use in corporate employee assistance programs, in prisons for forensic assessments and evaluations, and in austere or remote environments where human care providers are few, such as in submarines or during orbital or interplanetary space travel. Just as with so many other new technologies (e.g., Apple's Siri), we can expect AICPs to become a ubiquitous aspect of our society in the years ahead.The American Psychiatric Association (APA), American Psychological Association (APA), and the American Counseling Association (ACA) are examples of the largest mental healthcare professional organizations in the United States that have published ethical codes for their respective disciplines. There are also several national certification boards, state regulatory boards, and specialty areas that have their own ethics or professional practice guidelines [16]. The specific guidelines of these professional organizations and boards cover the range of areas applicable to mental health care practice including providing treatments, clinical assessments, research, training and consultation. In general, ethical codes and guidelines are intended to help guide the behavior of health care professionals and organizations toward the benefit and protection of patients. Ethical codes and guidelines also help practitioners to resolve ethical dilemmas and to justify their decisions and courses of action. Further, they protect care providers and their institutions by setting standards of conduct that ultimately promote the trust of patients, professional colleagues, and the general public.While there are differences between the ethical codes and guidelines of the various mental healthcare organizations, there are several key common themes. As noted by Koocher and Keith-Speigel [17], these include:1.Promoting the welfare of consumers (patients)Practicing within scope of one's competenceDoing no harm (non-maleficence)Protecting the patients’ confidentiality and privacyActing ethically and responsiblyAvoiding exploitationUpholding integrity of the profession by striving for aspirational practiceProfessional ethics codes typically include both aspirational principles (e.g., beneficence, non-maleficence, autonomy, dignity and integrity) as well as standards that are enforceable rules. Guidelines are not ethical codes or legal standards; however, provisions within may reflect the spirit of the overarching medical ethical principles and help to assure competence of practitioners. Laws are enforceable obligations and in some cases, laws may be in conflict with ethical responsibilities. The American Psychological Association addresses this with the following provision: “If psychologists’ ethical responsibilities conflict with law, regulations, or other governing legal authority, psychologists make known their commitment to the Ethics Code and take steps to resolve the conflict. If the conflict is unresolvable via such means, psychologists may adhere to the requirements of the law, regulations, or other governing authority” [18]. Several ethical codes, such as that of the American Psychiatric Association, specify that care providers (physicians) should work to change the law if they believe that the law is unjust [19].In recognizing the need to address ethical issues and best practices that arise with new technological advancements, several mental health care professional organizations have included provisions in their ethical guidelines or as supplements that address topics such as electronic data security, use of the Internet for providing care services, or the use of electronic communication with patients. While some of the existing mental health care ethics codes (e.g., the American Psychological Association) provide general provisions for competence of care in emerging areas, these provisions are entirely general and are not sufficient to address the emerging ethical implications associated with artificial intelligent systems as care providers. Other organizations, such as the National Board of Certified Counselors, The International Society for Mental Health, the American Medical Association, and the American Telemedicine Association have established specific guidelines regarding the use of the Internet to provide care; however, these do not yet consider the application of AICPs to provide care services.Topics discussed in the field of machine ethics [4] and associated fields are of high relevance to the ethical design and use of AICPs. Machine ethics is concerned with the moral behavior of artificial intelligent agents. Artificial moral agents (AMAs) are robots or artificial intelligent systems that behave toward human users or other machines in an ethical and moral fashion [4]. Roboethics [20,21] is also an emerging field that is concerned with the ethical behavior of humans when it comes to designing, creating, and interacting with artificial intelligent entities (i.e., robots). Science fiction has also approached the topic of ethics with intelligent machines, most notably Isaac Asimov's Three laws (and one additional law) of Robotics [22,23]. In 2011, the Engineering and Physical Sciences Research Council (EPRSC) and the Arts and Humanities Research Council (AHRC) (Great Britain) published a set of ethical principles for designers, builders and users of robots. These are shown in Table 1. Riek and Howard [24] have also discussed emerging ethical, design, and legal considerations associated with human–robot interaction.While ethics for humans users of artificial intelligent agents can be conceptualized as distinct from the ethics of intelligent machines [4], both are highly relevant to the use of AICPs, especially as these systems are designed to be increasingly autonomous. According to Moor [25], implicit ethical agents are machines constrained by what programmers will have them do whereas explicit ethical agents are machines that calculate what is ethical on their own by applying ethical principles to diverse and varying degrees of complex situations. Current virtual avatar systems, for example, can be considered to be implicit ethical agents; however, as they become more autonomous, they will need to function as explicit ethical agents. Consider an artificial intelligent agent that is capable of independently performing diagnostic assessments, providing treatment, and monitoring the symptoms of a depressed patient. The system will need to be able to make decisions and select courses of action that are consistent with applicable ethics codes during its interaction with patients and also be capable of resolving complex ethical dilemmas, just as a human psychotherapist is required to do. Thus, the relevance of professional ethics and machine ethics can be expected to increase as AICPs are put to use in the mental health care and other helping fields.The therapeutic relationship, also called the working alliance, is a term used to describe the professional relationship between a provider of care (i.e., a psychotherapist) and a patient whereby the care provider hopes to engage with and achieve therapeutic goals with a patient [26]. The therapeutic relationship is a key common factor associated with desirable treatment outcomes that is independent of the specific type of treatment. In other words, the quality of the therapeutic relationship accounts for why clients improve (or fail to improve) at least as much as the particular treatment method [27,28].The relationship between professional care providers and patients is not an ordinary social relationship; there are important legal and ethical obligations because care providers are in a position of power in their relationship with patients and there is potential for harm and exploitation of patients. The ethical obligations of the relationship between care providers and patients are central to the ethical codes of the mental health care professions. The American Psychiatric Association Ethical Principles states:“The psychiatrist shall be ever vigilant about the impact that his or her conduct has upon the boundaries of the doctor–patient relationship, and thus upon the well-being of the patient. These requirements become particularly important because of the essentially private, highly personal, and sometimes intensely emotional nature of the relationship established with the psychiatrist.” [29]Central to the therapeutic relationship is trust as this forms the basis for the care provider–patient relationship. In order to make accurate diagnoses and provide optimal treatments, the patient must be able to feel comfortable to communicate all relevant clinical information to care providers. Just as between human care providers and their patients, trust will be a critical variable for patients who interact with AICPs.The issue of trust raises an important philosophical question; is it unethical to simulate a human so much that people believe that the simulation is human? Weizenbaum [1] notes that ELIZA did indeed deceive people:“Nevertheless, ELIZA created the most remarkable illusion of having understood in the minds of the many people who conversed with it. People who knew very well that they were conversing with a machine soon forgot that fact, just as theatergoers, in the grip of suspended disbelief, soon forget that the action they are witnessing is not “real”. This illusion was especially strong and most tenaciously clung to among people who knew little or nothing about computers. They would often demand to be permitted to converse with the system in private, and would, after conversing with it for a time, insist, in spite of my explanations, that the machine really understood them” (p. 189).Weizenbaum observed that people conversing with ELIZA were more than simply suspending belief. He noted that even while people know that they are conversing with a software program, they still believe that the simulated psychotherapist is real because the person assigns meaning and interpretation to what ELIZA says, which “confirms the person's hypothesis” that the system understands them. We also know from research that people can form strong emotional attachments to technological devices and simulations, such as virtual avatars [30]. We can thus expect persons to confide in AICPs, to experience them as “real persons”, and to form strong attachments to them.Key to forming and maintaining the therapeutic relationship is empathetic understanding[31]. Empathetic understanding conveys to the patient that the care provider understands and cares what the patient is feeling and experiencing. Reflection describes the process whereby the care provider communicates that she/he is feeling and responding to the feelings of the patient [31]. ELIZA demonstrated with its basic text interface and few hundred lines of computer code that it is easy to reflect the feelings of a person, espouse empathy, and “pull in” a person interacting with it. Current systems are much more advanced in this regard; simulated care providers can verbally echo important statements expressed by the patient, say “uh ha”, and make facial or body movements that reflect attentiveness and understanding of what a patient is saying or expressing with body movement. Thus, modern simulations have the appearance of empathetic concern that is incredibly interactive and human-like.Even when a patient is consciously aware that a care provider simulation is a machine, the patient can be expected to experience intense emotions during the interaction and toward the simulation. Emotional engagement (and even intense emotional interaction) may be desirable in the appropriate therapeutic context. For example, transference is the process in which patients unconsciously redirect feelings toward one object (a significant person in the patient's past or current life) to another (i.e., the therapist) [27]. While transference may be desirable and have therapeutic benefits in some contexts, it is important to consider how this may be undesirable as well. For example, some patients may become overly attached or even “attracted” to an AICP in a non-therapeutic manner. A potential for iatrogenic effects of AICPs can be paralleled to the same types of ethical issues and risks identified when using clinical virtual reality including exacerbating or inducing mental health symptoms that were previously not present [32]. Even when disclosure is made that the system is “just a machine”, some patients may believe that the machine is “alive” or that there is a person, or some other malevolent force behind the simulation. Miller [33] and Riek and Watson [34] have discussed the concerns with “Turing Deceptions”, whereby a person is unable to determine if they are interacting with a machine or not. This issue poses an ethical problem, especially when working with vulnerable persons. For example, simulations may be contraindicated for use with persons who have delusional or psychotic psychopathologies in the absence of careful patient screening and monitoring.It is a necessary ethical obligation to establish a safe therapeutic environment for patients to experience emotions and for psychotherapists to monitor emotional reactions and attenuate emotional interactions in a clinically appropriate manner. Psychotherapists must also pay special attention to the effects that the end of the therapeutic relationship will have on their patients. AICP systems must be designed to appropriately end its relationship with patients in a manner that does not cause distress or is harmful to patients (i.e., not making appropriate referrals for additional care or addressing the emotional impact of the closure of the therapeutic relationship, etc.). Unsupervised AICPs that are unable to detect worsening of symptoms would present particular risk.Mental health care professionals are responsible for many tasks that require general and specific training and skill. Competence refers to their ability to appropriately perform these tasks. Competence is an ethical obligation because providing services outside of the boundaries of one's trained skill or expertise could put patients at risk of harm. Moreover, incompetence of professional care providers is damaging to the perception of the mental health care profession when examples of inappropriate or harmful behavior are made public. The ethical codes of the American Psychological Association, American Psychiatric Association and others have specific provisions that address professional competence. The American Psychiatric Association code states “A psychiatrist who regularly practices outside his or her area of professional competence should be considered unethical. Determination of professional competence should be made by peer review boards or other appropriate bodies.” [29]. In the mental healthcare fields, competence is generally maintained through formal education, licensure, specialty training, and continuing education.The topic of competence is germane to both the design and ethical use of AICPs. Just as with the use of any other technology or technique, mental healthcare professionals (and health care organizations) must be competent in their application of AICP systems. This may include knowing when they are appropriate for use, and what the limitations are, and what the risks are. Moreover, AICP systems must be designed and capable of performing tasks in a competent manner. This includes core competence in application of interpersonal techniques, treatment protocols, safety protocols (how to manage situations where care seekers indicate intent to self-harm or harm another person), and cultural competencies.A central ethical problem may be the application of AICP systems when they are not adequately controlled based on the scope of their tested capabilities. Systems deployed for access on the Internet that claim to provide particular clinical services or benefits to patients when they are not adequate or appropriate to do the stated services is one example. This is already known to be a problem with unregulated online mental health services and has led to lawsuits in the past [35]. This issue raises a related question; what should be the requirements for showing the credentials of AICPs? The American Psychological Association Ethics Code explicitly states: “Psychologists make available their name, qualifications, registration number, and an indication of where these details can be verified” [16]. The lack of display of the credentials and qualifications of AICP systems and their operators could be damaging to end users and the mental health care profession.The logical extension of discussions regarding scope of use and competence of AICPs and their operators is the topic of liability. Some of the key questions that arise are: Who should be responsible for the actions, decisions, and recommendations that AICPs make? What if a care seeker dies by suicide or engages in homicide after disclosing intent to an AICP? What is the appropriate responsibility of end users, developers and others as these systems increase in autonomy? Ethics codes, guidelines, and laws will need to consider these types of questions.A central ethical responsibly of professional health care providers is to assure the safety of patients while they are under care. Take, for example, a patient who discloses intent to end their own life while under care. It is the responsibility of the care provider to take appropriate actions (e.g., in some cases hospitalize the patient, notify emergency services) to assure the safety of the patient. Duty-to-warn statutes require healthcare professionals to notify authorities when a patient makes a threat to another person. In either of these examples, this requires monitoring of content of what the patient discloses, assessing risk factors, and applying clinical judgment (e.g., assessing intent, weighing clinical and other factors) to make decisions regarding the best course of action. The laws regarding duty-to-warn and other requirements may also vary from one jurisdiction to another (i.e., state to state in the United States). Thus, processes must be in place in order to assure that patient safety and that other legal and ethical obligations are met. However, current systems, such as virtual avatars that are accessible via the Internet, may provide services across state lines and for that matter, international boundaries. The issue of practice across jurisdictional boundaries is a major discussion in the field of telemedicine (providing health services over a distance using communication technologies) [35]. Unregulated deployment of these systems, such as on the Internet, will undoubtedly present significant ethical and legal implications if expectations for competence and patient safety are not enforced.The complexity of liability increases with the use of highly autonomous AICP systems. As noted by Sullins [36], one theoretical model to address liability is the standard user, tool, and victim model. In this model, technology mediates the moral situation between the actor (person who uses the technology) and the victim. When something goes awry, the user is seen as at fault, not the tool. However, this simplistic model does not describe the complexity of when the tool (i.e., an AICP) has a level of moral agency (moral decision making) or when there are other key entities involved, such as the programmers of the technology. Sullins proposes that intelligent machines (i.e., robots) are indeed moral agents when there is reasonable level of abstraction under which the machine has autonomous intentions and responsibilities. If the machine is seen as autonomous, then the machine can be considered a moral agent. If this is the case then the machine may also be seen as responsible, at least in part, for the actions it makes. Furthermore, as the computational complexity of AICPs increase, it will become increasingly difficult to predict how an AICP system will behave in all situations. If in a court of law, human mental health care professionals (or their employers) must demonstrate that their actions were reasonable and consistent with what is typically expected by others in the same profession (i.e., a group of peers), ethical codes, laws and guidelines. If an AICP system exists as a mysterious black box, it may be considered unethical to use it without the capability to demonstrate how the system derived its decisions for a particular course of action.Respect of privacy reflects the right to autonomy and dignity and it is seen as essential to individual wellbeing [37]. Betrayal of trust due to privacy invasion or unauthorized misuse of information damages the trust of both individual care providers and their profession. Threats to privacy can come in the form of inappropriate use of private data as well as data security issues including unauthorized access to electronic data.While threats to patient privacy (electronic data security) are common to many types of technologies used in the practice and management of healthcare [35], current and emerging technological capabilities, such as psychological signal detection (e.g., via visual, voice, psychological data collection), access to “big data”, as well as conversation logs between patients and AICPs create the potential for much more data to be collected about individuals, and without individuals being aware of the collection. Patients can be expected to reveal deeply personal information to AICP systems, of which can cause significant harm to individuals if used inappropriately. The issue of privacy during use of a simulated psychotherapist was highlighted by Weizenbaum [1] who noted that when he informed his secretary (who had been using ELIZA) that he had access to the logs of all the conversations, she reacted with outrage at this invasion of her privacy. Weizenbaum was surprised to find that such a simple program could so easily deceive a naive user into revealing personal information.Information AICPs may share with human care providers (e.g., to assure continuity of care), as well as potential inconsistency between the information provided by human care providers and AICPs, could result in patient distrust in care providers and be damaging to the therapeutic relationship. Furthermore, the dual-use aspects of the technology may be a threat to the trust that people have of these systems and the helping professions. The fact that the same technologies may be used for multiple purposes other than their stated use in health care may influence the public's perception of the use of those technologies and thus increase apprehension to participate in activities in which their data are being collected. For example, it is feasible that the same psychological signal detections systems with applicability for mental health care purposes can be used for prisoner interrogation purposes. Also, the possibility that data collected while interacting with AICPs may also be used by corporations, governments or other entities for making predictions about a person's behavior that have implications, such as for future employment, may also increase distrust and apprehension regarding their use.The implications regarding data privacy and trust extend beyond the helping fields and have much grander moral ramifications. These technologies present an inherent risk of the perception of and the actual loss of private thoughts. It is already a known fact that current technologies that make use of artificial intelligence are used to track and analyze phone calls, observe and identify people through cameras in public places, and all form of electronic communication in the world [38]. Abuse of this type of personal data that could be collected by AICPs could allow governments or other entities to control individuals or suppress dissent. Hackers also pose a risk to privacy and could potentially exploit very sensitive data of individuals who interact with AICPs. Both the actual and perception of loss of private information are inimical to the fields of mental health practice and to a free society as a whole.I provide a summarized list of recommendations for ethical codes and guidelines in Table 2. To begin, existing professional ethical codes and guidelines will need to be expanded to address the therapeutic relationship between AICPs and patients. Provisions that include requirements for human supervision of AICPs whereby clinicians are responsible for monitoring and addressing therapeutic relationship issues, emotional reactions, and adverse patient reactions that may pose a risk to patient safety should be considered. Requirements for supervision or monitoring should depend on the context and clinical application of AICP systems. For example, AICPs that are used for simple assessment of symptoms, educational coaching, and training purposes may not require the level of supervision or monitoring that more intensive clinical interactions and treatments would require. Provisions that address the competence of users of these systems (i.e., mental health care providers, companies that provide AICP services) are also recommended. These should include requirements to demonstrate that system users understand the capabilities, scope of use, and limitations of AICP systems. For example, psychologists who use these systems in the context of psychological work should have training and knowledge regarding the use of AICPs and in the domain that the AICP is to be used (i.e., psychotherapy, clinical assessment, etc.). There should also be a requirement for AICP systems to be updated to keep current with the latest clinical best practices just as human providers must do.For AICP systems that are accessed remotely (i.e., via the Internet), we should expect adherence to the same guidelines for assuring patient safety and overall clinical best practices that are used in the field of telemedicine. These include; assessment of whether remote care is appropriate to begin with, assessment of safety risks, the gathering of information regarding emergency services at the patient's location, and involvement of third parties who may assist during emergencies [39]. For instance, if a system is designed to conduct clinical diagnostic assessments, it will need to demonstrate the ability to detect and assess risk for suicide and then take appropriate steps to alert supervisors or administrators to become involved to assure the safety of the patient. Specifications that use of AICP systems must be consistent with applicable laws (e.g., practice jurisdictions, etc.) and established best practices for remote clinical services should be enforced. Legal restrictions on the use of AICPs when presented to the public in untested and uncontrolled manner (such as on the Internet) may also be required. Furthermore, the honest and obvious display of credentials and representation of qualifications and competencies of AICPs and their operators should help consumers make informed decisions and to help protect them from harm resulting from incompetence or unauthorized practice. Patients should also have a way to voice concerns regarding patient safety or quality of services provided by AICPs and to have those issues reviewed and resolved.Requirements for full disclosure of what the system(s) does, its purpose, its scope, and how people use the information from it needs to be provided to end users. This disclosure should also describe whether the system(s) meets requirements of applicable privacy laws. Rules that specify requirements for maintenance and destruction of AICP data records, including conversation logs, should be developed and disclosed to end users. The disclosure process (i.e., informed consent) should also include clear description of the system and intentions of its use.Given the potential for fully autonomous intelligent agents in the helping professions, it is conceivable that ethical codes, guidelines, and laws that hold autonomous intelligent systems accountable will be needed. However, AICPs are fundamentally different from human care providers because they cannot accept appropriate responsibility for their actions nor do they have the moral consequences that humans do. Machines will not experience embarrassment, stress, or the pain associated with loss of clinical privileges at a care institution, expulsion from professional organizations, loss of their license to practice, or other legal sanctions. While this topic is open to philosophical debate, it may make most practical sense for legal liability to be explicitly linked to humans.A list of considerations and recommendations for the design of AICPs are shown in Table 3. AICP systems could be built to do work in nearly any combination of domains (psychological work, medicine, counseling, social work, etc.), thus, systems will need to meet the ethical requirements of those respective professional domains based on what activity they are providing. For example, an AICP that functions as a psychiatrist by performing clinical diagnostics and monitoring medication use of patients should follow the ethical guidelines applicable to the field of psychiatry. The developers of AICP systems will need to be able to specify the capabilities and limitations of these systems based on what services they are designed to provide.How AICPs are designed to recall information when interacting with care seekers is another important issue. Richards and Bransky [40] found that virtual intelligent agents (in the form of real estate agent characters) that were programmed to be poor at recalling information were frustrating to end users, whereas virtual characters that exhibited forgetting by either explicitly acknowledging that they have forgotten something or by not stating it at all were perceived to have more natural memory by end users. While simulation of forgetting and recall facilitated a more believable and natural interaction between the virtual intelligent agent and the end user, it also influenced how much the end user experienced trust in the virtual intelligent agent. This design characteristic is important to consider because of how it may influence the ability for AICPs to effectively interact and engage with patients to achieve therapeutic goals.Autonomous AICP systems that are intended to fully replace human care providers will have to have the capabilities to assess and monitor emotions just as human care providers do, including the observation of very subtle characteristics of emotional states. Technology is rapidly advancing in the area of affective sensing and thus the capabilities required for this are becoming feasible. However, ethical codes and guidelines will need to specify requirements for end users to know what the limitations are and to address them in a competent manner. It will also be necessary to demonstrate that autonomous AICP systems can expeditiously resolve the complexity of ethical issues that involve the safety of patients or other third parties (e.g., duty-to-warn). As I discussed previously, how AICPs derive courses of action will have increasing complexity that may pose a legal problem when needing to justify the actions of AICPs. Requirements for an audit trail with a minimum level of detail to describe decision processes are one way to help address this issue. Specifications of limits of use as well as limits of autonomy of the systems are also needed. Developers may wish to consider the need for built-in safeguards to assure that systems are only able to provide services within the boundaries of competence for the intended domain of use.With a move toward autonomous systems, a challenge for developers of these systems is and will be how to design systems to perform ethically in diverse situations, especially when facing complex ethical dilemmas. Ethics codes are broad and general and as noted by Pope and Vasquez [41] formal ethical principles can never be substituted for an active, deliberative, and creative approach to meeting ethical responsibilities. In other words, ethics codes cannot be applied in a rote manner to autonomous artificial agents because patient's situations are unique and may call for different solutions. While intelligent machines can be designed to be highly predictable, accurate, and reliable when approaching many situations, scripted decision paths may not be sufficient in situations that entail complex ethical decision making. Thus, for intelligent machines to ever fully function unsupervised and in place of humans, these systems will need to comply with the ethical codes of conduct just as human healthcare providers currently do but also be capable of selecting the best course of action that would be considered to be reasonable based on current standards. AICP systems will also need to be able to apply theoretical approaches and expert knowledge that is current and evidence-supported.Testing and evaluation of AICPs before they are ready for market should be a requirement. The same phases of clinical research that are used for evaluating medical devices may be one applicable model. This testing may also include a requirement to demonstrate the ethical decision making of AICPs. Ethical Turing Tests have been proposed as one way to test the ethical and moral behavior of artificial intelligent agents [42]. For example, Allen, Varner and Zinser [42] proposed the “comparative moral Turing test” (cMTT) that entails comparing the ethical decision making of an artificial intelligent agent to a person. One way to implement such a test for the mental health professions would be to compare the decisions made by an AICP to the opinions of a panel of professional mental healthcare providers.

@&#CONCLUSIONS@&#
