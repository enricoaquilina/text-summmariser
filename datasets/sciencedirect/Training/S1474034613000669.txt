@&#MAIN-TITLE@&#
Input variable selection in time-critical knowledge integration applications: A review, analysis, and recommendation paper

@&#HIGHLIGHTS@&#
Input Variable Selection (IVS) helps when processing system inputs is computationally heavy.A framework to accommodate high level perspective of different approaches to IVS is provided.Sensitivity analysis (SA) helps IVS in heterogeneous input variables with time constraint.Event-based SA proves fit by its application in an industrial drilling disaster prevention problem.

@&#KEYPHRASES@&#


@&#ABSTRACT@&#
The purpose of this research is twofold: first, to undertake a thorough appraisal of existing Input Variable Selection (IVS) methods within the context of time-critical and computation resource-limited dimensionality reduction problems; second, to demonstrate improvements to, and the application of, a recently proposed time-critical sensitivity analysis method called EventTracker to an environment science industrial use-case, i.e., sub-surface drilling.Producing time-critical accurate knowledge about the state of a system (effect) under computational and data acquisition (cause) constraints is a major challenge, especially if the knowledge required is critical to the system operation where the safety of operators or integrity of costly equipment is at stake. Understanding and interpreting, a chain of interrelated events, predicted or unpredicted, that may or may not result in a specific state of the system, is the core challenge of this research. The main objective is then to identify which set of input data signals has a significant impact on the set of system state information (i.e. output). Through a cause-effect analysis technique, the proposed technique supports the filtering of unsolicited data that can otherwise clog up the communication and computational capabilities of a standard supervisory control and data acquisition system.The paper analyzes the performance of input variable selection techniques from a series of perspectives. It then expands the categorization and assessment of sensitivity analysis methods in a structured framework that takes into account the relationship between inputs and outputs, the nature of their time series, and the computational effort required. The outcome of this analysis is that established methods have a limited suitability for use by time-critical variable selection applications. By way of a geological drilling monitoring scenario, the suitability of the proposed EventTracker Sensitivity Analysis method for use in high volume and time critical input variable selection problems is demonstrated.

@&#INTRODUCTION@&#
In a complex interrelated world, industry is faced with ever changing performance metrics. This complexity and relentlessness of change both in substance and in presentation is forcing companies to make ever larger investments in data acquisition and processing technologies. The dilemma of “usefulness” and “relevance” for the investor still prevails [8,38]. In addition, there is a direct relationship between identifying useful and relevant input data and the levels of investment on data acquisition, communication and computational capabilities required for performance measurement. The identification of useful and relevant input data that affects the performance measurement relies on the speed and the quality of the process that separates the useful and relevant data from the non-useful non-relevant input data.This paper reports on the existing techniques that have been developed in recent years for measuring the degree of usefulness and relevancy of input data in describing the state of system or how sensitivity the state is to specified parameters. These techniques include the traditional Input Variable Selection (IVS), Feature Selection (FS), and Sensitivity Analysis (SA). The authors approach for analyzing and comparing the applicability and practicability of such techniques is to determine if such techniques are capable of dealing with extremely volatile cases in industry which require tracking of real-time events and their importance as they occur. We also briefly refer to the computational restrictions that traditional IVS, FS and SA techniques have that make them irrelevant for the type of systems we deal with.A case study in the Deep Drilling Industry (Deep Drilling Disaster Prevention) is presented in this article. It demonstrates the ability of a recently proposed real-time sensitivity analysis technique EventTracker [65] in generating accurate disaster knowledge support system for the purpose of Disaster Prevention as a unique solution for dealing with time-critical and unaware situations. The emphasis on “unaware” is to insist that in such cases there is no historical knowledge of the system behavior. In this particular case a drilling machine drilling deep into a geophysical unknown or volatile terrain.Note that the purpose of the literature review and analysis is reporting on the existing IVS, FS and SA techniques and to demonstrate the impracticality of these techniques in dealing with such situations, because they normally rely on historical knowledge (e.g. statistics and probability), postulation or heuristics to manage and interpret input data and assemble system state information. The example offered in this article demonstrates a real industrial case that no traditional input variable analysis technique can be employed without major compromises to the quality of data or decision making whilst drilling.The remainder of this article is structured as follows. First, a cross-conceptual survey of system ‘variable’ and ‘feature’ extraction is given in Section 2. In Section 3, a comprehensive spectrum of perspectives is provided to summarize IVS methodologies. In Section 4, the authors focus on how sensitivity analysis supports IVS. This is followed in Section 5 by a description of application of EventTracker sensitivity analysis in a detailed case study and the demonstration of its feasibility based upon the results. Finally, the conclusions of the research project are provided in Section 6.Although the concepts of Input Variable Selection (IVS) and feature selection [49] appear similar, there are key differences between them. Here an analysis of these key aspects such as dimensionality reduction and the associated computational overhead is carried out.Feature Selection (FS) is a well-known problem addressed by much research [13,16,24,26,27,30,53,71,72]. The objective of this paper is to propose a technique to decide which input data sources are useful and relevant to determine the state of a given system, e.g. output. For this purpose we distinguish between FS and IVS with respect to two aspects; first the nature or substance of the input variables and features, and second, how they are selected. Both aspects are explained as follows.Although the terms “variable” and “feature” are sometimes regarded to be quite similar, input variables differ in nature from features. Input variables and raw input data series generally provide information about the system, whereas features are used to represent a model of the system. A feature may be locally and temporarily created to support the understanding of some specific behavior in the system. Based on the problem in hand, non-sequential data, i.e. snapshots, may be sufficient for a given data mining task. However, continuous information of input variables is time-critical in certain applications when the system state changes rapidly in time, i.e. in Volatile Systems [28]. This conceptual difference is shown in Fig. 1.Input variables represent knowledge about the direct result of aggregation – and pre-filtering - of raw input data from data sources. Variable construction resembles data fusion as an overlay task in terms of aggregating raw input data sources. Features, however, are meant to add to the aggregated knowledge when input variables do not provide the required knowledge with adequate certainty, efficiency or based upon other domain-specific objectives.Features are created in two ways:•They are created either by application of specific transfer functions based on the input variables, raw input data, or a combination of both. Feature construction extends the knowledgebase.Or they are created through finding patterns in a data series.With the help of an example, we describe feature construction and feature extraction as follows.When a system is monitored, the key features that describe the system status are separated from the initial input variables that are generated by situated sensors. Each constitutes a distinct layer in the data acquisition architecture, i.e. the sensor level is taken to be the primary layer and the feature level occurs at an intermediate layer [1]. For example, the input variables for a typical production line are specified to assemble the initial feature candidates that in turn are interpreted as indicators of the shop floor status and the job characteristics [41]. Transformations and combinations of the primary data and intermediate features are then used to extract the key performance indicators [41].In another example, in order to detect faults in rechargeable batteries i.e. state, two performance indicators are required, capacity and life cycle [52]. These two indicators are measured based upon the amount of charge and the number of completed charge/discharge cycles (i.e. input variables) prior to the nominal capacity falling below a specified value. In order to reduce the measurement cycles and to facilitate the detection process, the paper reports on devising a new set of variables (features) by combining the derivatives of the two variables with different orders.Fig. 2illustrates the feature construction from input variables. The curved arrows symbolize the extra effort spent in converting input variable type knowledge into feature type knowledge. Depending on the type of process, one can expect increases in the computational overhead. The cost of the computational overhead may affect subsequent control, monitoring, and decision making processes, particularly from a time-critical perspective.The key challenge in any data mining and analysis process is to decrease the uncertainty associated with the relationship between input variables and the interpretation of system behavior. It is not always the case that input variables can provide a sufficiently clear view of a system’s behavior [12,20]. Neither can input variables always lead to an adequate model that can be built and evaluated. An analysis of the time series of the acquired data (and input variables) can enable features of data sources, that are not previously known and actionable, to be revealed. For example, in order to understand the complex characteristics of bioprocesses and enhance production robustness, both descriptive (e.g., frequent pattern discovery, clustering) and predictive (e.g. classification, regression) pattern recognition methods have been applied [15]. As a result, significant trends in processing data sourced from archived temporal records of physical parameters and production scale data were discovered [15].Furthermore, a Virtual Metrology system capable of predicting each wafer’s metrology measurements based upon production equipment data and metrology results collected for statistics, such as mean, variance, minimum, and maximum values from each sensor during two etching processes as used by a Korean semiconductor manufacturing company is presented [36].Data mining and feature extraction require an additional effort (see Fig. 3) for both data warehousing (as represented by the database element) and historical data processing (as represented by blue arrows) because they are heavily reliant on historical data. This increases the computational costs and may affect the time-critical aspects of monitoring.There is evidence from research and practice conducted for measurement of computational efficiency of IVS methods that they used derivations of the original inputs to select and build new variable subsets [11,29,37,43]. The term “derived variable” has been used for the same purpose by the data mining community, suggesting that techniques such as Projection Pursuit Regression (PPR) and Principal Component Analysis (PCA) can be used efficiently for variable transformation [28]. PPR technique is a common multivariate regression technique and PCA is a multivariate orthogonal transformation cluster analyzes [20]. Regression [67] and cluster analysis are two well-known bases for many dimensionality reduction techniques [32,48]. The pros and cons of both regression and cluster analysis methodologies for IVS are discussed in the following two sections.The task of estimating relationships between a number of independent variables and a dependent variable is called regression [28]. In this paper, independent variables are regarded as input variables to a system model. Dependent variables are interpreted as the model’s performance indicators or system outputs. A regression method is used when:1.There is a requirement to predict the value of a dependent variable from new values of independent variables and when no predictive model between the independent variables and dependent variable exists.A new set (but usually fewer in number) of independent variables used to replace the original set of independent variables are expected to lead to the same effect.The heterogeneous nature of the data distributions that represent input variables can invalidate assumptions about the nature of the relationship between independent and dependent variables. This leads to a consideration of nonlinear and non-parametric regression methods. In [6] there is an explanation that by growing the dimension of input variables, the number of possible regression structures increases more than exponentially. This makes regression methods far less reliable for input data analysis.Banks et al. [6] compares the performance of regression techniques by conducting simulation experiments on ten prominent regression methods. They considered the effect of six factors in their experiments: the type of regression method; the embedded functional relationship between the data of independent and dependent variables; the number of variables (dimension); the sample size; the added noise to the sample data; the portion of involved variables in the function (model sparseness). Of the ten regression methods examined, none is applicable to all conditions [6]. They report that Recursive Partitioning Regression (RPR) is able to cope with high numbers of dimensions, e.g. 12 variables, when all variables were involved (explanatory). They however, recommend analysts to engage in the process of selection of the regression method by trying portions of data on each method until they seem to be a reasonable fit.From the above analysis, one can conclude that no particular regression method is capable of covering the scale and heterogeneity of variables involved in volatile situated industrial systems whilst keeping the computational cost low. Although some guidance is given concerning the selection of a suitable IVS method, e.g. to be incremental rather than memory-based, it is still encouraging to explore other dimensionality reduction methods, such as clustering.Cluster analysis methods use “similarity criteria” so that a group of data values that are “similar” can either be replaced by a new value representing the group (clumping) or assigned a unique type of label (partitioning) [32]. As a basic example, K-Means clustering [48] divides a set of data entities into K non-overlapping clusters of similar data where each cluster is represented by the mean value of its data (the centroid). The choice of the number of clusters (K) and similarity criteria are the two main challenges in this approach.As a type of clustering method with specific similarity criteria and automatic selection of number of clusters, Principal Component Analysis (PCA) replaces a number of input variables that are correlated by a smaller number of variables which are not correlated (principal components) and which at the same time keep the same variability of the original input variables [35]. Given a fixed set of input variables PCA always produces a unique set of new variables independent of the analysis of model performance factors. A key issue here is at the choice of using one of the actual (genuine) entities in each group to represent the group instead of generating an artificial one, e.g., a mean value [48].Fundamental challenges associated with clustering are data representation, the purpose of grouping, cluster validity, and cluster algorithm comparison [32]. It is understood that although there is generally no universally good data representation, domain knowledge aids the clustering process. In the general case where input variables are assumed to adhere to a heterogeneous data series in the time domain, there is no clear choice for changing and selecting a particular type of data representation. In terms of grouping, the added dimension in the time domain, as is the general assumption of this paper, raises extra computational issues; First, one common indicator or variable is required to represent the time series of input variables or alternatively, a similarity criteria could be applied between each of two input variable time series. This on one hand is computationally exhaustive, but on the other hand imposes a limitation on the choice of similarity criteria between the time series due to the different sampling rates of different input variables, and therefore, a different size of collected samples.Jain concludes that due to the unknown prior knowledge about the structure of data, no single clustering algorithm exists and a diverse set of approaches often need be tried before determining an appropriate algorithm for the clustering problem at hand [32].In summary, neither regression-based and nor cluster analysis-based methods are able to support a single best solution to the problem of input variable selection in its most heterogeneous form. Moreover, both these methodologies are computationally exhaustive when the number of input variables grows.In general, IVS finds the most appropriate set of input variables according to some criteria that may or may not consider the output performance of the system. [25,68] introduced a variable selection criteria based on numeric data based fuzzy modeling, which was explored by [62]. Gaweda et al. [25] performed an input–output sensitivity analysis to remove input variables holding the least maximum normalized sensitivity index in a backward elimination fashion. [68] proposed an Input Variable Selection Criterion (IVSC) function, which estimates the importance of each input variable numerically. The most important input variables are then kept using a forward selection scheme.In Derived Variable-based IVS (DVIVS) methods that are used to evaluate newly (artificial) created variables, the values of the original variables need to be acquired when the new variables are evaluated. This adds a burden to time-critical data integration systems that wish to employ an IVS method with a low computational overhead. Original variable-based (OVIVS), in contrast to DVIVS, avoids this burden.OVIVS tends to reduce the computational cost of data integration by avoiding a high sampling rate and by processing the least important portion of the input data. It is however important to invalidate this tendency by exploring OVIVS methods.Variable ranking has proved to be a sound approach in many variable selection algorithms due to its simplicity and scalability [26]. In variable ranking, a scoring (or scaling) function is used to compute and assign a quantitative score for each input variable in relation to a target class [48], e.g. a correlation coefficient [54], and then variables are sorted based upon their score [26].Quevedo et al. [54] present a simple ranking algorithm that seems to be superior to more complex state-of-the-art ranking algorithms in terms of computation efficiency. The powerful computation time performance of their ranking method mainly comes from the explicit and sequential implementation of a cycle of two selection and elimination tasks [54]. First, a correlation-based criterion examines and ranks input variables with respect to output variables. Then an orthogonalization module applies redundancy detection and variable elimination by normalizing input variables according to the top ranked variable at the same cycle. This sequence is shown in Fig. 4.The loop stops when either all input variables are processed, or when the number of training data values is smaller than the number of input variables.In [54] a Simplified Polynomial Expansion (SPE) is used as a sufficiently good approximation of general nonlinear models that map input variables to output variables. Therefore, an initial computational effort is required at the beginning of each cycle embedded in the variable ranking module in order to accomplish the SPE between output variables and the remaining input variables. They also experimented using an SPE-ranker algorithm on artificially generated data sets. The stoppage criterion, however safe, leads to long runs for systems with a high number of input variables, since for example, for a system with several input variables, the algorithm needs at least 100 execution cycles for processing 100 samples. With a typical sampling rate of up to 5 samples per second, this takes at least 20s before a decision about the elimination or change of acquisition settings of input variables could be taken.The type of approach to measure the performance of a selected set of variables could affect the accuracy of the resulting selected variable set as well as the computational cost. As seen in Fig. 5, approaches differ in the way variable selection module and selection validation modules interact, i.e. if the loops are embedded, or are they a one-off incident. They also differ in terms of the number of data sets involved from input or output variables i.e. none, once, or multiple times that data sets are used for training or test purposes.Any of the above variable selection methods may adopt a wrapper, embedded or filter approach.A wrapper approach [38] measures and compares the usefulness of different subsets of input variables with respect to parameters used for decision making. To achieve this, the approach selects the input variable and measures their influence on the performance model (Fig. 6).Therefore, similar to a learning machine, execution of an iterative runs of a performance model uses different variables (training set) at each run [54]. Finally an examination of selected variables with a different data set, a test set, contributes to the computational overhead [63]. Lemaire et al. [44] introduces a wrapper method for measuring the importance of input variables based upon predictive models, and probability distribution of input variables. In a wrapper method, when computation is deemed to be too exhaustive for large number of variables, heuristics such as backward elimination and forward selection [50] may reduce but not necessarily eliminate the overall computational cost [68].In an embedded method, similar to wrapper method, an iterative learning mechanism is applied to determine the importance of the subsets of input variables. However, in contrast to a wrapper method, an embedded method does not use a closed learning mechanism which works independently from the actual variable selection function. Instead an embedded method incorporates variable selection in the learning mechanism (Fig. 7). This accelerates the discovery of solutions as well as leading to a more accurate outcome [26].Nevertheless, both embedded and wrapper methods require iterative executions of the performance model as well as historical variable datasets for the training and test phases. Hence, the associated computational overhead is still a concern.Filtering of irrelevant and redundant variables, if accurate enough, could help to lower the computational complexity for data acquisition [54] and the search space [55]. In a filter method, data relevance is assessed using an independent criterion function. Therefore, the details of algorithms and models that govern the output variables (system performance data) have no effect on the variable selection. Since a filter method incorporates a one-step learning-based wrapper method, as shown in Fig. 8, it is a low computationally complex process [63]. However, for the same reason, of not implementing a multi-step learning process, filter methods do not necessarily find the most accurate and useful subset of variables [26].The selection of an IVS method encompasses an assessment of performance as well as an understanding of the type and amount of computational effort that must be spent to implement and execute it. Questions like ‘Should exclusive knowledge of the variables be incorporated?’, ‘Should there be an automatic learning approach?’, ‘Should a simulation support observation of performance with different sets of variables?’ and so on, arise from different points of view. This leads us to encompass the aforementioned methodologies as a set of viewpoints or perspectives.This section provides an overview of IVS methodologies from several computation perspectives. The various computation perspectives explore the initial assumptions about the nature of dependent and independent variables within a search space. The efforts made by others to describe the deterministic or stochastic nature of the relationships between input variables and output performance measures are also covered. It is intended that such summarization supports the first step required to select an IVS methodology.The scale of the problem and the time constraints on IVS as well as the discrete nature of the search space seems predisposed to a heuristic approach [4]. Heuristics refers to the search strategies that use problem-specific knowledge to find a solution [50]. The way heuristic can help with solving complex problems is through ignoring some aspects of the problem avoiding further computational processes and therefore complexity. Therefore, in using a heuristic approach for IVS, some input variables to a problem are known to be less important and more computationally expensive. These could be ignored in the process of ranking or in redundancy detection. Thus, a heuristic function that evaluates the cost of the input variables acquires extra information about the computational method that is applied to the input variables of system.Coding and maintaining heuristic rules are easier to compute than optimization procedures [4]. However expert knowledge of the factors that determine the rules or heuristics is required. This limits the application of this approach to well-defined and known problems [4].It may be argued that input variable selection is similar to an optimization problem [66]. Hence the objective of IVS could be rephrased to “minimizing the computational overhead caused by sampling, processing, and storage of unnecessary values of inputs to a system that does not suffer a loss in accuracy of the system state interpretation”. The computational overhead is directly proportional (linear or non-linear) to the number of input variables [54]. Therefore, this leads to strategies to minimize the number of input variables, or actually, to minimize their sampling rate while maintaining the accuracy of performance variables of the model.The difficulty in adopting this approach for use with IVS arises out of the stochastic nature of the values of objective i.e. model’s performance variables. The accuracy in these values cannot be evaluated exactly due to its non-deterministic nature. Banks [5] introduced some remedies to the issues of estimated values of stochastic parameters in a simulation based on the execution of multiple replications and/or longer runs of the simulation. Such a solution does not comply with the time constraints of time-critical analysis situations.In other words, optimization via simulation seeks to build a prescriptive model (answering how to set the input variables) from an existing descriptive model (system simulation) [4]. Prescriptive models can grow in size and become non-linear very quickly when details are incorporated, making them virtually impossible to solve optimality [4].Statistic approaches support extraction of probability distributions and their associated properties from data. For input variables, the approach replaces each input variable with the probabilistic distribution that can represent the time series of the associated input variable with a close enough accuracy (using an error estimation method). For the selection of input variables, the allocated probability distributions may be clustered or otherwise shortlisted according to the result of the analysis against the degree of their influence on the performance of the model.Statistic perspective is erected based on two features that input variables hold in the generic view that this paper commits to; one is the heterogeneity of data types and therefore, the expected uncertainty about the data series of the input variables. Nothing is assumed to be known about the distribution of these data sets. The other is the unlimited nature of the data series that enables a statistic approach to look at them as a sample of a larger distribution [48].The difficulty with using statistic approaches to IVS arises from limiting the number of distributions that could be applied to fit the sampled input variable data. Therefore, the suitability of statistical techniques to assess the influence of an input variable on system state within the limited time (real-time) is doubtful. This unsuitability is compounded when a known distribution cannot be ascertained [65].Application of machine learning necessitates a repetitive use of consecutive values of data in order to set some prediction parameters which could later be used to categorize newly created data [75]. Sufficient samples from each input variable should be tried against a sufficient number of samples of output performance parameters. This requires an exhaustive number of executions.Supervised machine learning that uses performance parameters in the analysis plays an important role in wrapper and embedded-based variable selection methods. The major disadvantage of using a learning mechanism is the requirement to accumulate data about the input variables using repetitive performance model executions making the process computationally expensive [76].The unsupervised learning machine approach has the same on-by-one, repetitive comparison process similar to a supervised learning algorithm. An unsupervised learning algorithm also relies on historical input variable data and conducts goodness-of-fit experiments to verify the relationship between system input and output. One can deduce that such technique would be unsuitable for time-critical applications.Unlike statistical approaches, data mining methods search for patterns and regularities within the available data [48] that naturally occur and are unknown [32]. Data mining methods do not seek to provide a consistent statistical distribution as in statistical analysis or as in machining learning methods. Instead, they focus on producing a more compact representation of the given data [73].In general, data mining-based input variable selection methods focus on the similarity of the input variables independent of their importance or influence on the model performance. Therefore, although a group of similar input variables can be identified as a result of data mining, further effort is needed to determine their importance with respect to the output variables and to decide on the acquisition attributes.Advocates of data mining methods consider data classification as part of data mining. However, there seems to be subtle differences in the essence of the knowledge acquired from the two methods. Within the context of IVS, the cause-effect relationship between the input variables and performance variables could be considered as knowledge about the system. Input variables with different levels of cause-effect relationships could be classified into different categories. In a classification, in contrast to data mining, categories are pre-defined or synthesized to support finding and grouping of data [74]. From a discriminative viewpoint, some function is required to maximize a measure of separation between the variables [28]. Such a discriminant function is explored through studying the level of impact that each independent variable has on the dependent variable. The following section opens up a discussion on this issue and about the available options, with respect to sensitivity analysis.Sensitivity analysis (SA) has been presented as a technique to minimize the computational overhead by eliminating the input variables that have the least impact on the system [9,17,18,22,58]. Sensitivity analysis techniques can help by focusing only on the most valuable information that has a significant impact on the behavior of systems. The measurement of the true impact of an input on the output of a system becomes challenging due to the epistemic uncertainty of the relationship between two variables [39]. The selection of an appropriate method for sensitivity analysis therefore, depends on a set of factors and assumptions. A framework is structured to allow us to analyze and compare the performance of SA methods. This framework takes into account the efforts needed to explore the type of relationship between inputs and outputs. The framework also covers the nature of the input and output data series. Last but not least, the framework provides a means to exhibit the computational effort that each SA method requires.The majority of sensitivity analysis methods tend to demonstrate the impact of a change in one variable on another one by means of a mathematical equation that describes the relationship between them. Methods such as differential analysis [31], Green’s function [70], and coupled/decoupled direct [23] are classified as analytical sensitivity analysis methods by [31]. However, the non-linear and non-monotonic relationship between inputs and outputs for a given system may not necessarily lend themselves to the use of such analytical methods [39]. The reasons for this are as follows.In differential analysis, the impact of an independent variable on the dependent variable is assessed by identification of the perturbation behavior of the dependent variable due to changes of the independent variable [2]. This is achieved by finding the coefficients of the differential equation that explain the relationship between the independent and dependent variables [2]. Methods such as Neumann expansion [42] and perturbation methods [14] could help to extract these coefficients by approximating differential equations. However, it cannot be guaranteed that on occasion the complex and nonlinear relationships between system variables can be adequately approximated with differential equations with sufficiently low error margins [31].Challenges for differentiating model equations encourage use of Green’s function as a catalyst to help achieve the sensitivity equations [31,45]. Effectively, in this method, the differentiation operation is replaced by a sequence of finding the impulse response of the model and by subsequent integration operations [7,21]. This introduces an auxiliary function proposing that a linear and time-invariant system only benefits from this solution. Another disadvantage of the application of Green’s function is its ability to work only with ordinary differential equations that govern dependent variables with respect to independent variables [33]. In real applications it is often difficult to separate independent variables and dependent variables. Additionally, working one variable at a time for high dimensional systems could be computationally expensive.In a coupled direct method, after differentiation of the model equations, subsequent sensitivity equations are solved together with the original model equations [31]. In a decoupled direct method, such equations are solved separately. This gives the impression that a decoupled direct method is advantageous in terms of its reduced computational cost. Although a decoupled direct method is reported to be more efficient than use of Green’s function, as are other analytical methods, knowledge of the model equations is required. This contributes to the disadvantages of analytical methods being model-oriented and expertise-hungry, compared to sensitivity analysis methods that do not require such model equations.There are occasions when effective work on the model equations is not feasible. This may be due to the non-existence of analytical relationship between model variables, the lack of expertise to identify such a relationship, or due to changes on the configuration of inputs and outputs every so often. Consequently, the effort required by expertise is costly. On these occasions, sampling-based SA methods tend to establish a model equation. They do so by identification of some statistical features in the distribution of the data series of the two variables. The general shortcoming of these methods such as Fourier Amplitude Sensitivity Test (FAST) [18,47], Morris [34,10], Monte Carlo [61] and Latin Hypercube [19] is their heavy reliance on historical data. This dismisses them from being good candidates for time-constrained applications.For example, [17] applied their model to 1280 sample values of 20 input parameters. Each cycle of sample generation and model execution took between 2 and 52h per set of samples. The overall execution cycles took almost 46days. This example illustrates the significant impact of the sampling based analysis on the computational overhead. The following subsections analyze the major sampling based methods.Random sample generation is a key characteristic of a Monte Carlo method and provides values for independent variables from which dependent variables are produced, based upon the execution of the model on sampled input data [57]. The random sampling scheme occurs either in no particular way, or with respect to some criteria that could help with the efficiency of computation [31]. For example, in the Latin Hypercube Sampling (LHS) method [19], the range of each input parameter is divided into intervals of equal probability. In each set of samples of input parameters, each input parameter takes a random value from one of its intervals with no repeat of the same interval for one full sampling cycle [19]. This way, it would be more likely to work on all segments of data in the distribution. Therefore, more informative distribution parameters for a generated output could be achieved in a shorter period [31].An overview of Monte Carlo methods is given in Fig. 9. First the probability distribution of input variables is estimated from the available data stream, i.e. using curve fitting blocks. Then based on these distributions, random sample generation blocks are produced. After the model is applied to the generated samples, the output values generated are processed in order to estimate and extract the distribution attributes [60].An important issue with the use of Monte Carlo methods in time-sensitive applications is the effort required to estimate the distribution of the input variables prior to sample generation.For sensitivity analysis purposes based on Monte Carlo sampling method, in order to infer the impact of each input variable based upon the output variable, data samples for one input variable (the checked box in Fig. 9) are generated one at a time while the other input variables are set to a fixed, e.g. average, value (the cross-marked boxes in Fig. 9). This cycle repeats itself for each input variable. Krzykacz-Hausmann [40] called this feature a ‘double-loop nested sampling procedure’. This can be computationally very expensive, particularly with high dimension input variables.In Morris method, based on parameter screening, changes in the value of output variable are measured for each change in the input variable [19]. An elementary effect is calculated based on the changes to only one input variable [34]. The resulting set of elementary effects is then processed to estimate the distribution. This implies that for each cycle of the output distribution, the estimation takes M=2rn model executions if r is the number of required output values for an estimation of a stable distribution and n is the number of input variables [19]. Even though more economical extensions of Morris method could reduce the total number of cycles – for example by using each generated model output in more than one calculation [10] – a typical low value for M is as high as 21000 executions for 1000 output values and 20 inputs applied to M=r(n+1) in such an improved Morris method [19]. The Morris method thus cannot support sensitivity analysis in time-constrained applications.One-At-a-Time (OAT) identification of the dependency of the output variables to inputs does not support the detection of higher order interactions between multiple input variables, i.e. second order and higher, and outputs. To overcome this issue, a series of sensitivity analysis methods are used to measure and decompose the variance of output distribution to some elements each of which could separately represent these input–output interactions [58]. ANOVA based sensitivity analysis methods are in general computationally more efficient for this reason [56].Variance decomposition is defined as [61]:(1)V(y)=∑iVi+∑i∑j>iVij+…+v12…nin which, the left hand side shows the total variance of model output y and the right hand side is a sequence of summation terms of the first order influences of input variables, second order influences, and so on. V12…krepresents the portion of variance of the output for the interaction of all n input variables together. Based on this variance decomposition, a sensitivity index for the output with respect to each input variable is defined as:(2)Si=Vi/V(y)To decompose the elements with respect to Eq. (1) and from there the sensitivity indices, when no explicit relationship exists between inputs and output, i.e. when an analytical approach is not possible, a numerical approach based upon sample generation, e.g., Monte Carlo, could be adopted [9]. The amount of computational overhead, in terms of the number of model runs (for producing output values per each input sample set) can be derived using the following equation:(3)M=N×∑i=0nn!(n-1)!1!where N is the sample size, and n is the number of input variables. For example, with 10 input variables and 1000 samples, the number of model executions would make 1024000. This is high. Thus, the computational cost for time-constrained applications needs to be improved. The following subsections cover this improvement and the overall drawback of all sampling based methods.Fourier Amplitude Sensitivity Test (FAST) [18] and its extended version [69] are examples of improvements in computational efficiency for ANOVA-based sensitivity analysis methods. FAST (and extended FAST) can be distinguished from other ANOVA methods by means of its input data sample generation scheme, in which, samples for each input variable are generated according to a periodic function within the limits of the input variable [19]. In other words, in a FAST method, the data distribution of input variables cannot be estimated from the acquired historical data. Instead, all distributions of input variables are considered to be uniform and within a range which should be specified. The subsequent generated samples in this range follow a periodical function [59]. The periodic nature of a sample generation scheme causes the model output values to be periodic. Therefore, by using a numerical Fourier analysis on the values of output, the magnitude of the Fourier spectrum at each frequency represents the sensitivity index of the corresponding input variable. Components of this process are shown in Fig. 10.As shown in Fig. 10, some aspects of computational cost that existed in the Monte Carlo method, i.e. distribution estimation, are omitted by FAST and replaced with simple tasks for boundary detection and frequency association. Furthermore, the output value distribution estimation is also replaced by a numerical Fourier Transform (FT) method for finding Fourier spectrum. In order to explicitly identify the power coefficient associated with the frequency of each input variable, a proper choice of the unique frequencies is required. For this, the range of frequencies is divided into high and low ranges. A high frequency is assigned to the input variable subject to power spectrum coefficient identification. The remaining input variables are assigned a frequency from the lower range. This way the distance between the high frequency and other low frequencies in the spectrum facilitates clear identification of the coefficient, or sensitivity index. In Fig. 10, the checked box in the frequency association module shows that the input variable number 1 is assigned a very different (high) frequency for the sample generation compared to the frequency of others (those with crossed boxes). As a result the power coefficient of the first frequency can be detected with sufficient confidence.This type of frequency association adds a new loop for the sample generation and model execution process to the analysis. The number of model runs using FAST is obtained as follows:(4)M=Nn(8wmax+1)where N is the sample size, n the number of input variables and wmax is the largest one among the assigned frequencies [59].The number of model executions in FAST does not seem low compared to alternative sampling-based SA methods, as it still features the ‘double-loop nested sampling procedure’ [40]. However, the computational overhead could be lower due to the simpler tasks included in the nested loops. The sample generation and Fourier transform in FAST is usually less computationally costly than sample generation, distribution estimation, and distribution-based function fitting (model search).A major drawback of sampling-based SA methods is in the concurrent generation and concurrent use of generated sample data. In other words, sample time series are generated at equal time intervals with respect to their own time series as well as the other sample time series. They are also taken into the model to generate outputs at equal times. This is not always the case for actual data in a system where all data values may not be generated and used in the model at the same time. For example, a thermostat generates an input event once the temperature passes a certain threshold. The input event to warning dissemination system then generates an alarm output. In the same system, a water level-gauge generates a trigger input event if a water tanks is filled up above a certain level. The two event inputs are not guaranteed to be simultaneous. The simultaneous generation of values of data and their connection to the model to produce the corresponding next output value is a rare event in systems with stochastic events and non-deterministic responses [46].As another example, consider the application of stochastic simulation for statistical analysis and characterization of Resin Transfer Molding processes by Guyon and. Elisseeff [27]. When calculating the mold pressure profile and resin advancement progress, they consider the measurement of time as a source of uncertainty with respect to measurements of viscosity, pressure, displacement, surface density, compressing variation, stacking sequence, race tracking, and human error.In sample generation for measured data series, the magnitude of the input variables follows a fit distribution. The timestamp when each new data is generated obeys a fixed frequency. When no value of a data series has actually entered the system, a data entity of zero or a low value is generated based on the probability distribution fitted. This concept is shown in Fig. 11.In the example shown in Fig. 11, four data time series from four sensors are acquired with sampling rates of 10, 4, 6, and 2 samples per second. Therefore, in one second, 22 data entities enter the system. However, sample generation with generation rate of 10 samples per second, generates 40 data entities in one second. Hence, it seems very likely that a portion of computational overhead of sampling-based sensitivity analysis methods results from the generation of extra data. To avoid this, one may consider the randomness or frequency of the data entering the system, then simulating this by applying a realized frequency or random function to the rate of sample generation. This additional stochastic modeling effort [51] has not been reported in sampling-based sensitivity analysis methods. Even though if applied, it increases the computational efforts of distribution estimation and sample generation tasks, leaving a trade-off between the added computational overhead and the reduction of generated sample data.Krzykacz-Hausmann [39,40] tackled the issue of the computational cost for the ‘double loop sample generation strategy’ and restrictive conditions for the evaluation of dependent variables based upon independent variables in sampling-based SA methods by proposing an approximation approach that measures the entropy of variable distributions in the original data samples. This method used the same decomposition equation as in Eq. (1) in Section 4.1.4.3 but uses entropy instead of the variance of sample data distributions.The method avoids the time consuming sample generation of independent variable and the evaluation of dependent variable by ‘Simple-Random Sampling (SRS)’ using piecewise uniform density function estimations. The approach is shown in Fig. 12.Only one sample size execution is sufficient to obtain the samples and to approximate the sensitivity indices (see Fig. 12). [40] demonstrated the feasibility of the estimation approach in a test case with fifteen independent and two dependent variables. Reasonable results were provided with far less computational cost. However, obtaining the appropriate indicator functions for each independent variable requires knowledge of their distribution probabilities [40].The second factor in the selection of sensitivity analysis method involves the characteristics of the data distributions of the input and output variables. The sensitivity indices are normally influenced by the distribution of the corresponding data series. For example, nonlinear relationships between input and output series of a model cannot be recognized by correlation-based sensitivity analysis methods [3]. Variance-based and Entropy-based indices are expected to be more sensitive to heteroscedastic data [39], whilst the homoscedasticity of a data series can be higher among discrete signals and much higher between binary signals.In this paper, assumptions concerning the generality of type and characteristics of data, discourage the use of such SA methods, e.g. correlation-base SA cannot be considered as a suitable approach. However, variance-based or entropy-based SA methods look promising from the point of view of data heterogeneity.Sensitivity analysis is a computationally heavy process. In domain-wide (global) sensitivity analysis methods, large batches of input variables are captured at each time interval and levels of sensitivity are measured based on historical data analysis. For example, sampling-based methods need to generate new and rather large sizes of sample values of both output and input data regardless of the original sample sizes.The amount of resources needed by the SA algorithm and its associated data can be compared with the amount of savings that may occur as a result of the applied algorithm. Correlation-based methods [3] need equal sizes of data batches for the input and output series of the model. Therefore, the sampled data series needs to use either interpolation or extrapolation to maintain an equal size, subsequently adding an extra computational load onto the system. ANOVA based SA methods save the computational effort on the analytical analysis side, but instead contribute to the computational cost through the sample generation.According to Section 4.1, FAST performs better than other ANOVA-based SA methods because it is independent of the detection of input variable distribution characteristics. Ravalico et al. [56] argues that FAST performance is the most efficient of all SA methods. However, having investigated the ability of several global SA methods, including Morris, correlation, and FAST, they also stress that none of the existing SA methods are scalable with increasing numbers of input variables.The summary of analysis of different approaches and techniques that may support IVS is reflected in Table 1. The main criteria that IVS requires are (1) Based on the derived or original variables, (2) Support heterogeneity of data, (3) Accurate results, and (4) Scalability, i.e. computational overhead with growth of the number of input variables. It is obvious that there is ample opportunity to explore methodologies for sensitivity analysis that can perform more efficiently, for use in time-constrained applications.An event-based sensitivity analysis method (EventTracker) is feasible for use with variable selection in time-constrained applications [65]. The EventTracker SA method is applied in this paper to the sensor-based data system in a well-drilling system.EventTracker is an input variable selection method that assembles together information about changes in system inputs and outputs using sensitivity analysis. For this to happen, an input/output trigger/event matrix is produced for all inputs and outputs from the system. This matrix is designed to map the relationships between input sensor data as causes that trigger events and the output data that describes the actual events. The cause-effect relationship between the causes of state change i.e. input variables and the effect system output parameters determines which set of inputs have a genuine impact on a given output. In this way the ‘EventTracker’ method is able to construct a discrete event framework [64] where events are loosely coupled with respect to their triggers for the purpose of sensitivity analysis. The method has a clear advantage over analytical and computational IVS method since it tries to understand and interpret system state change in the shortest possible time with minimum computational overhead. The processes for input variable organization and the creation of performance measures (system output) are explained in [65].During the drilling of a well, a driller needs continuous knowledge about the borehole stability and even more urgently about any significant borehole instability. The actual state of the borehole is used for any immediate, eventually urgent, counteractions, or to make revisions to improve the drilling plan respectively. The state is typically evaluated using a number of different information sources that incorporate a geological model. Such information sources usually supply real-time data, measurements at regular intervals and drilling reports. Real-time data originates typically from sensors mounted at the surface as well as in some special cases from down hole; these sensor inputs are introduced in Table 3 in the Appendix A. The data are sampled and provided with an interval of the magnitude of seconds. Fig. 13shows a portion of such data together with the associated information about the state of the drilling rig.Each decision support function has to meet response time requirements according to estimations of these (Table 2).The required response time depends mainly on the critical event to be detected. The earlier a counteraction is initialized; the better is the chance to manage an emerging crisis. Thus the question for the response time is not permissible per se, answering ‘the response time should be as short as possible’ is of course insufficient. Nevertheless it can be stated that the overall magnitude of the response time is of the order of seconds. Especially for a kick event the response time is strongly dependent upon the ground formation parameters such as permeability, pressure conditions and fluid properties and thus the required response time may be extended to an order of minutes. In contrast, for stuck pipe prevention, ream and wash operations are usually applied after connecting a new stand to the drill string. Thus, waiting for recommendation decisions to perform these, or not, makes little sense if the response time is the magnitude of the duration of that operation.In terms of real world tasks, stuck pipe detection is an actual but rather simple problem. The real and more sophisticated task is stuck pipe prediction and as a consequence, stuck pipe prevention, through the use of some precautionary counteractions. The assumption that hook-load and torque contain information about emerging stuck pipe is still valid, how that information is covered is actually unknown and therefore a challenge whose importance should not be underestimated.Fig. 14sketches a typical borehole drilled nearly vertical at its beginning that then changes direction to nearly horizontal. The main components of the hook load FHook are the acceleration force FA(FA=m.a), the weight of the whole drill string FW, the friction forces FFand some other non-quantifiable forces denoted as ε. In case of creating a deterministic model, the mass influx of the drill string, the borehole trajectory, especially the inclination and friction factors, amongst others, need to be known.Since it appears unpromising to identify and estimate all factors with a reasonable certainty and accuracy, and to predict and thus to prevent a stuck pipe (as well as other crises), a heuristic approach incorporating deterministic know-how seems to be a more feasible solution. To incorporate as much deterministic knowledge as possible, a systematic approach in building variables based on some laws of physics appears to be an appropriate solution here. Consider the case for each of the ten sensors installed in each well, that 100 variables are generated. For instance the simple rule for the acceleration force FA, i.e. FA=m.a leads to the resulting heuristics that FAcan be expressed asFA=c1(dḣa+dsa)+c2ain which dhdenotes the length of the drill string above a rotary table and thus out of the borehole, dsis the length of the drill string in the borehole and a is the acceleration applied to the total drill string. The constants c1 and c2 need to be evaluated. Fig. 15shows a portion of the variables generated for one of the sensor data sources for duration of 10min at a 0.1Hz generation rate.The Time series from these 100 variable data sources were collected for ten sensors in four drilling wells. Also the generated six system states (defined in Fig. 13) of each of the four drilling processes (TDL-S075, TDL-S085, TDL-S140, and TDL-S240) that provided the instantaneous state of the drilling scenario were collected. The EventTracker SA method was executed on the collected samples. It created a matrix of sensitivity indexes to associate each output to each of the 100 input time series for each of the ten sensors. A similar task was performed for a 100 variables for 10 sensors in the other three drilling platforms.Fig. 16shows the sensitivity indexes of the Operation Code (CodeOS) system state with respect to all 100 variables in each of the four well datasets. Correlations in the chart mean that some variables, e.g. variables 1–3, 10–15, 18, 20–25, 41, 50–55, and so on, have a less significant role than some others, e.g. variables 4–9, 16–17, 19, 26–40, 42–49, and so on in determining the state of the well.By grouping and sorting the normalized sensitivity indexes of all Operation Codes with respect to all variables for each sensor data in separate diagrams, we obtain significance plots as shown in Fig. 17. Obviously, although the 100 variables are the same in Figs. 16 and 17, their order in any of the four plots in Fig. 17 are not the same and are not the same as their order in Fig. 16 as variables are sorted based on their normalized sensitivity index. For example, variable indexed 1 in Fig. 16 maybe variables indexed 5 in Fig. 17 C0108, and that maybe variable index 28 in Fig. 20 C0110.These plots indicate that a large proportion of variables have low normalized sensitivity indices. The low index could be interpreted as not important for all of the Operation Codes with respect to the associated sensor data. Eventually, if a significant threshold, for example 0.45 is considered (dashed horizontal line in Fig. 17), and by inspection of the data presented in Fig. 17 and for similar results for other variables in the Appendix A, it is realized that more than 50% of the generated knowledge could be ignored, i.e. does not need to be generated, saving computation.In Fig. 17, data plots are sorted by significance values. The plots show only the significance of variables generated from sensor data C108, C110, C114, and C116. Similar plots for the variables from the other sensors of the drilling system are shown in the Appendix A. Dashed horizontal line shows the 0.45 significance threshold level.In terms of time efficiency, the execution of the proposed SA method took a few seconds in comparison with the sampling window (10min). Scoring each search slot took a fraction of a second making it feasible for time-constrained applications that generate data with order of seconds or higher to use.(5)TScoreei⩽(Search slot)NTriggerNEvent(6)TScorei⩽160ms,whereNTrigger=100,NEvent=6,SS=100s

@&#CONCLUSIONS@&#
