@&#MAIN-TITLE@&#
Robust PCA via Principal Component Pursuit: A review for a comparative evaluation in video surveillance

@&#HIGHLIGHTS@&#
We initiate a rigorous and comprehensive review of RPCA-PCP based methods.We investigate how these methods are solved.We investigate if incremental algorithms can be achieved.We investigate if real-time implementations can be achieved.A comparative evaluation with the BMC dataset. Shows the performance of 13 recent RPCA methods.

@&#KEYPHRASES@&#
Foreground detection,Robust principal component analysis,Principal Component Pursuit,

@&#ABSTRACT@&#
Foreground detection is the first step in video surveillance system to detect moving objects. Recent research on subspace estimation by sparse representation and rank minimization represents a nice framework to separate moving objects from the background. Robust Principal Component Analysis (RPCA) solved via Principal Component Pursuit decomposes a data matrix A in two components such thatA=L+S, where L is a low-rank matrix and S is a sparse noise matrix. The background sequence is then modeled by a low-rank subspace that can gradually change over time, while the moving foreground objects constitute the correlated sparse outliers. To date, many efforts have been made to develop Principal Component Pursuit (PCP) methods with reduced computational cost that perform visually well in foreground detection. However, no current algorithm seems to emerge and to be able to simultaneously address all the key challenges that accompany real-world videos. This is due, in part, to the absence of a rigorous quantitative evaluation with synthetic and realistic large-scale dataset with accurate ground truth providing a balanced coverage of the range of challenges present in the real world. In this context, this work aims to initiate a rigorous and comprehensive review of RPCA-PCP based methods for testing and ranking existing algorithms for foreground detection. For this, we first review the recent developments in the field of RPCA solved via Principal Component Pursuit. Furthermore, we investigate how these methods are solved and if incremental algorithms and real-time implementations can be achieved for foreground detection. Finally, experimental results on the Background Models Challenge (BMC) dataset which contains different synthetic and real datasets show the comparative performance of these recent methods.

@&#INTRODUCTION@&#
The detection of moving objects is the basic low-level operations in video analysis. This detection is usually done using foreground detection. This basic operation consists of separating the moving objects called “foreground” from the static information called “background”. Many foreground detection methods have been developed [8,48,52] and the main resources can be found at the Background Subtraction Web Site.1https://sites.google.com/site/backgroundsubtraction/Home.1In 1999, Oliver et al. [39] are the first authors to model the background by Principal Component Analysis (PCA). Foreground detection is then achieved by thresholding the difference between the generated background image and the current image. PCA provides a robust model of the probability distribution function of the background, but not of the moving objects while they do not have a significant contribution to the model. Although there are several PCA improvements [56,57] that addressed the limitation of classical PCA with respect to outlier and noise, yielding the field of robust PCA, these methods do not possess the strong performance guarantees provided by very recent works based on the idea that the data matrix A can be decomposed into two components such thatA=L+S, where L is a low-rank matrix and S is a matrix that can be sparse or not. This decomposition can be obtained by Robust Principal Component Analysis (RPCA) solved via Principal Component Pursuit (PCP) [10,64]. The background sequence is then modeled by a low-rank subspace that can gradually change over time, while the moving foreground objects constitute the correlated sparse outliers. For example, Fig. 1shows the original frame 309 of the sequence from [49] and its decomposition into the low-rank matrix L and sparse matrix S. We can see that L corresponds to the background whereas S corresponds to the foreground. The fourth image shows the foreground mask obtained by thresholding the matrix S. So, these recent advances in RPCA are fundamental and can be applied to background modeling and foreground detection for video surveillance.Recent advances on subspace estimation by sparse representation and rank minimization show a nice framework to separate moving objects from the background. These advances concern robust subspace tracking and robust PCA models. A representative robust subspace tracking is the online algorithm GRASTA (Grassmannian Robust Adaptive Subspace Tracking Algorithm) [25,26] for low rank subspace tracking, which is robust to both highly incomplete information and sparse corruption by outliers. GRASTA incorporates the augmented Lagrangian ofl1-norm loss function into the Grassmannian optimization framework to alleviate the corruption by outliers in the subspace update at each gradient step. So, GRASTA can estimate and track non-stationary subspaces when the streaming data vectors are corrupted with outliers. GRASTA [26] was evaluated quantitatively with success on background modeling and foreground detection. While GRASTA used thel1-norm as a convex relaxation of the ideal sparsifying function, Seidel et al. [23,14] recently approach the problem with a smoothedlp-norm in their algorithm called pROST (lp-norm Robust Subspace Tracking). Experimental results [14] show that pROST outperforms GRASTA in the case of dynamic backgrounds. For Robust PCA via low-rank and sparse matrix decomposition, several approaches have been developed and can be classified as follows:•RPCA via Principal Component Pursuit (PCP) [10].RPCA via Outlier Pursuit [66].RPCA via Iteratively Reweighted Least Squares [20,21,19].Bayesian RPCA [13], variational BRPCA [6].Approximated RPCA (GoDec [71]).In this paper, we focus only on the review for RPCA-PCP for a systematic evaluation and comparative analysis with the BMC dataset [59].The first work on RPCA-PCP developed by Candes et al. [10,64] proposed a convex optimization to address the robust PCA problem. Under minimal assumptions, this approach called Principal Component Pursuit (PCP) perfectly recovers the low-rank and the sparse matrices. The background sequence is then modeled by a low-rank subspace that can gradually change over time, while the moving foreground objects constitute the correlated sparse outliers. Candes et al. [10] showed visual results on foreground detection that demonstrated encouraging performance but PCP present several limitations for foreground detection.The first limitation is that the algorithms to be solved are computational expensive. The second limitation is that PCP is a batch method that stacked a number of training frames in the input observation matrix. In real-time application such as foreground detection, it would be more useful to quickly estimate the low-rank matrix and the sparse matrix in an incremental way for each new frame, rather than as a batch. The third limitation is that the spatial and temporal features are lost, as each frame is represented as a column vector. The fourth limitation is that PCP imposed the low-rank component being exactly low-rank and the sparse component being exactly sparse but the observations such as in video surveillance are often corrupted by noise affecting every entry of the data matrix. The fifth limitation is that PCP assumed that all entries of the matrix to be recovered are exactly known via the observation and that the distribution of corruption should be sparse and random enough without noise. These assumptions are rarely verified in the case in real applications because (1) only a fraction of entries of the matrix can be observed in some environments, (2) the observation can be corrupted by both impulsive and Gaussian noise (3) the outliers i.e., moving objects are spatially localized.Many efforts have been recently concentrated to develop low-computational algorithms for solving PCP [31,32,9,70,68,50,64,38,35,33,36,15], to develop incremental algorithms of PCP to update the low-rank and sparse matrix when a new data arrives [41,43,42,44] and real-time implementations [2,3,54]. Moreover, other efforts have addressed problems that appear specifically in real application such as: (1) Presence of noise, (2) Quantization of the pixels, (3) Spatial constraints of the foreground pixels and (4) Local variations in the background. To address (1), Zhou et al. [72] proposed a stable PCP (SPCP) that guarantee stable and accurate recovery in the presence of entry-wise noise. Becker et al. [7] proposed a inequality constrained version of PCP to take into account the quantization error of the pixel’s value (2). To address (3), Tang and Nehorai [54] proposed a block-based PCP (BPCP) method via a decomposition that enforces the low-rankness of one part and the block sparsity of the other part. Wohlberg et al. [63] used a decomposition corresponding to a more general underlying model consisting of a union of low-dimensional subspaces for local variation in the background (4). Table 1shows an overview of the different versions of RPCA-PCP in term of minimization, constraints and convexity. For each method, we investigated in their respective section its robustness, its spatial and temporal constraints, the existence or not of an incremental version, and of a real-time implementation, and of a quantitative evaluation. Furthermore, RPCA-PCP can been extended to the measurement domain, rather than the pixel domain, for use in conjunction with compressive sensing [61,62,27]. Although experiments show that moving objects can be reliably extracted by using a small amount of measurements, we have limited the investigation and the comparative evaluation in this paper to the pixel domain to permit the comparison with the classical background subtraction methods with the BMC dataset [59].These recent advances in RPCA via PCP are fundamental and can be applied to foreground detection. However, no algorithm today seems to emerge and to be able to simultaneously address all the key challenges that accompany real-world videos. This is due, in part, to the absence of a rigorous quantitative evaluation with synthetic and realistic large-scale dataset with accurate ground truth providing a balanced coverage of the range of challenges present in the real world. Indeed, the authors usually compared qualitatively their method to RSL [56] or PCP [10]. Some recent quantitative evaluations in foreground detection using the performance metrics have been made but they are limited to one algorithm. For example, Wang et al. [60] studied only RPCA-PCP solved by APG [32]. Xue et al. [67] and Yang [69] evaluated the RPCA-PCP solved by IALM [31] and Guyon et al. [18] adapted the RPCA-PCP with LADMAP [33]. Guyon et al. [17] validated BPCP for foreground detection too. Results show that BPCP gives better performance than PCP. In a recent comparative analysis and evaluation, Guyon et al. [22] compared five algorithms RSL [56], RPCA-PCP solved via EALM [31], RPCA-PCP solved via IALM [31], QPCP [7] and BRPCA [13] with the Wallflower dataset [58], the I2R dataset [30] and Shah dataset [49]. Experimental results show that BRPCA that address spatial and temporal constraints outperforms the other methods. However, this evaluation is limited to five methods and it is not made on large datasets that present a coverage of the range of challenges present in the real world. When considering all of this, we propose to initiate a comprehensive review of RPCA methods solved by PCP for testing and ranking existing algorithms for foreground detection. Contributions of this paper can be summarized as follows:•A review regarding RPCA methods solved via PCP: The original PCP, the Stable PCP, the quantization based version of PCP, the block based version of PCP and the local PCP are reviewed in Sections 3–7. For each method, we investigate how they are solved, and if incremental and real-time versions are available for foreground detection. Furthermore, their advantages and drawbacks are discussed in the case of outliers due to dynamic backgrounds or illumination changes.A systematic evaluation and comparative analysis: We compare and evaluate nine RPCA methods solved via PCP on a recent dataset which contains various synthetic and realistic videos in Section 8. This datasets is the Background Models Challenge (BMC) dataset2http://bmc.univ-bpclermont.fr/.2[59]. For the evaluation, we have used the quantitative evaluation framework provided by BMC which allows comparison with the state-of-the-art methods.The rest of this paper is organized as follows. First, we remind in Section 2 the key challenges which concern background and foreground separation in video-surveillance. Then, we review each original method in its section (Sections 3–7). In each case, we survey how each method are solved, and if incremental and real-time versions have been developed for background and foreground separation. Then, the performance evaluation using quantitative metrics over the five datasets is given in Section 8. Finally, we conclude with promising research directions in Section 9.Three main conditions assures a good functioning of the background subtraction in video surveillance: the camera is fixed, the illumination is constant and the background is static. In practice, several challenges appear and perturb this process. They are the following ones:•Noise image: It is due to a poor quality image source such as images acquired by a web cam or images after compression.Camera jitter: In some conditions, the wind may cause the camera to sway back and so it cause nominal motion in the sequence. Foreground mask show false detections due to the motion without a robust maintenance mechanism.Camera automatic adjustments: Many modern cameras have auto focus, automatic gain control, automatic white balance and auto brightness control. These adjustments modify the dynamic in the color levels between different frames in the sequence.Illumination changes: They can be gradual such as ones in a day in an outdoor scene or sudden such as a light switch in an indoor scene.Bootstrapping: During the training period, the background is not available in some environments. Then, it is impossible to compute a representative background image.Camouflage: A foreground object’s pixel characteristics may be subsumed by the modeled background. Then, the foreground and the background can be distinguished.Foreground aperture: When a moved object has uniform colored regions, changes inside these regions may not be detected. Thus, the entire object may not appear as foreground. Foreground masks contain false negative detections.Moved background objects: Background objects can be moved. These objects should not be considered part of the foreground. Generally, both the initial and the new position of the object are detected without a robust maintenance mechanism.Inserted background objects: A new background object can be inserted. These objects should not be considered part of the foreground. Generally, the inserted background object is detected without a robust maintenance mechanism.Dynamic backgrounds: Backgrounds can vacillate and this requires models which can represent disjoint sets of pixel values.Beginning moving object: When an object initially in the background moves, both it and the newly revealed parts of the background called “ghost” are detected.Sleeping foreground object: Foreground object that becomes motionless cannot be distinguished from a background object and then it will be incorporated in the background. How to manage this situation depends on the context. Indeed, in some applications, motionless foreground objects must be incorporated and in others it is not the case.Shadows: Shadows can be detected as foreground and can come from background objects or moving objects [1].These challenges have different spatial and temporal properties. In the following sections, we will see how some of these challenges can be tackled by the RPCA models.Candes et al. [10,64] have proposed a convex optimization to address the robust PCA problem. The observation matrix A is assumed represented as:(1)A=L+Swhere L is a low-rank matrix and S must be sparse matrix with a small fraction of nonzero entries. The straightforward formulation is to usel0-norm to minimize the energy function:(2)minL,Srank(L)+λ‖S‖0subjA-L-S=0whereλ>0is an arbitrary balanced parameter. But since this problem is NP-hard, a typical solution would involve a search with combinatorial complexity. This research seeks to solve for L with the following optimization problem:(3)minL,S‖L‖∗+λ‖S‖1subjA-L-S=0where‖·‖∗and‖·‖1are the nuclear norm (which is thel1-norm of singular value) andl1-norm, respectively, andλ>0is an arbitrary balanced parameter. Usually,λ=1max(m,n). Under these minimal assumptions, this approach called Principal Component Pursuit (PCP) solution perfectly recovers the low-rank and the sparse matrices. Candes et al. [72] showed results on face images and background modeling that demonstrated encouraging performance.Background modeling and RPCA-PCP: The low-rank minimization concerning L offers a suitable framework for background modeling due to the correlation between frames. So, minimizing L and S implies that the background is approximated by a low-rank subspace that can gradually change over time, while the moving foreground objects constitute the correlated sparse outliers which are contained in S. To obtain the foreground mask, S needs to be thresholded (see Fig. 1). The threshold is determined experimentally.rank(L)influences the number of modes of the background that can be represented by L: Ifrank(L)is to high, the model will incorporate the moving objects in its representation; if therank(L)is to low, the model tends to be uni-modal and then the multi-modality which appears in dynamic backgrounds will be not captured. The quality of the background/foreground separation is directly related to the assumption of the low-rank and sparsity of the background and foreground, respectively. In this case, the best separation is then obtained only when the optimization algorithm has converged. Practically, RPCA-PCP present several limitations developed in the Section 1.1 and an overview of their solutions is given in the following sections.Several algorithms complexity have been proposed for solving PCP. An overview of these algorithms as well as their complexity can be seen in Table 2. All these algorithms require solving the following type of subproblem in each iteration:(4)minL,Sη‖L‖∗+λ‖S‖F2The above problem can have a closed form solution or not following the application.•In the first case, PCP can be reformulated as a semidefinite program and then be solved by standard interior point methods [11]. However, interior point methods have difficulty in handling large matrices because the complexity of computing the step direction isO((mnmin(m,n))2), wherem×nis the size of the data matrix. Ifm=n, then the complexity isO(n6). So the generic interior point solvers are too limited for many real applications where the number of data are very large. To overcome the scalability issue, only the first-order information can be used. Cai et al. [9] showed that this technique, called Singular Value Thresholding (SVT), can be used to minimize the nuclear norm for matrix completion. As the matrix recovery problem in Eq. (3) needs minimizing a combination of both thel1-norm and the nuclear norm, Wright et al. [64] adopted a iterative thresholding technique (IT) to solve it and obtained similar convergence and scalability properties than interior point methods. However, the iterative thresholding scheme converges extremely slowly. To alleviate this slow convergence, Lin et al. [32] proposed two algorithms: the accelerated proximal gradient (APG) algorithm and the gradient-ascent algorithm applied to the dual of the problem in Eq. (3). However, these algorithms are all the same to slow for real application. More recently, Lin et al. [31] proposed two algorithms based on augmented Lagrange multipliers (ALM). The first algorithm is called Exact ALM (EALM) method that has a Q-linear convergence speed, while the APG is in theory only sub-linear. The second algorithm is an improvement of the EALM that is called Inexact ALM (IALM) method, which converges practically as fast as the EALM, but the required number of partial SVDs is significantly less. The IALM is at least five times faster than APG, and its precision is also higher [31]. However, the direct application of ALM treats the Eq. (3) as a generic minimization problem and ignores its separable structure emerging in both the objective function and the constraint [31]. Hence, the variables S and L are minimized simultaneously. Yuan and Yang [70] proposed to alleviate this ignorance by the Alternating Direction Method (ADM) which minimizes the variables L and S serially. ADM achieves it with less computation cost than ALM. Recently, Chartrand [12] proposed a non-convex splitting version of the ADM [31] called NCSADM. This non-convex generalization of [31] produces a sparser model that is better able to separate moving objects and stationary objects. Furthermore, this splitting algorithm maintains the background model while removing substantial noise, more so than the convex regularization does.In the second case when the resulting subproblems do not have closed-form solutions, Yang and Yuan [68] proposed to linearize these subproblems such that closed-form solutions of these linearized subproblems can be easily derived. Global convergence of these Linearized ALM (LALM) and ADM (LADM) algorithms are established under standard assumptions. Recently, Lin et al. [35] improved the convergence for the Linearized Alternating Direction Method with an Adaptive Penalty (LADMAP). They proved the global convergence of LADM and applied it to solve Low-Rank Representation (LRR). Furthermore, the fast version of LADMAP reduces the complexityO(mnmin(m,n))of the original LADM based method toO(rmn), where r is the rank of the matrix to recover, which is supposed to be smaller than m and n. In a similar way, Ma [36] and Goldfarb et al. [15] proposed a Linearized Symmetric Alternating Direction Method (LSADM) for minimizing the sum of two convex functions. This method requires at mostO(1/∊)iterations to obtain an∊-optimal solution, and its fast version called Fast-LSADM requires at mostO(1/∊)with little change in the computational effort required at each iteration.All these methods require computing SVDs for some matrices, resulting inO(mnmin(m,n))complexity. Although partial SVDs are used to reduce the complexity toO(rmn)such a complexity is still high for large data sets. Therefore, recent researches focus on the reduction of the complexity by avoiding computation of SVD. Shen et al. [50] presented a method where the low-rank matrix is decomposed in a product of two-low rank matrices and then minimized over the two matrices alternatively. Although, they do not require nuclear norm minimization and so the computation of SVD, the convergence of the algorithm is not guaranteed as the problem is non-convex. Furthermore, both the matrix–matrix multiplication and QR decomposition based rank estimation technique requireO(rmn)complexity. So, this method does not essentially reduce the complexity. In another way, Mu et al. [38] reduced the problem scale by random projections but different random projections may lead to radically different results. Furthermore, additional constraint to the problem slows down the convergence. The complexity of this method is alsoO(pmn)wherep×mis the size of the random projection matrix. So, this method is still nor linear complexity with respect to the matrix size. Recently, Liu et al. [46] proposed a novel algorithm, calledl1-filtering for exactly solving PCP with anO(r2(m+n))complexity. This method is a truly linear cost method to solve PCP problem when the date size is very large while the target rank is small. Moreover,l1-filtering is highly parallelizable. It is the first algorithm that can exactly solve a nuclear norm minimization problem in linear time. Numerical experiments [46] show the great performance ofl1-filtering in speed compared to the previous algorithms for solving PCP.PCP is an offline method which treats each image frame as a column vector of the matrix A. In real-time application such as foreground detection, it would be more useful to estimate the sparse matrix in an incremental way quickly as new frame comes rather than in batch way. Furthermore, the sparsity structure may change slowly or in a correlated way, which may result in a low-rank sparse matrix. In this case, PCP assumption will not get satisfied and S cannot be separated from L. Moreover, the principal directions can change over time. So, the rank of the matrix L will keep increasing over time thus making PCP infeasible to do after time. This last issue can be solved by not using all frames of the sequences. To address the two first issues, Qiu and Vaswani [41] proposed an online approach called Recursive Robust PCP (RR-PCP3http://www2.i.e.psu.edu/aybat/codes.html.3) [41] and Recursive Projected Compressive Sensing (ReProCS3) [42]. The aim of RR-PCP (ReProCS) is to causally keep updating the sparse matrixStat each time, and keep updating the principal directions. The tth column ofA,At, is the data acquired at time t and can be decomposed as follows:(5)At=Lt+St=[UI][xtSt]twherext=UTLtand the matrix U is an unknownm×morthonormal matrix. The support ofStchanges slowly over time. LetNtdenote the support ofxtwhich is assumed piecewise constant over time and given an initial estimate ofPt=(U)Nt=P^t, Qiu and Vaswani [41] solved for the sparse componentStby finding the orthogonal complement matrixP^t,⊥, and then using the projectionMtontoP^t,⊥, denoted byyt:(6)yt=P^t,⊥TMt=P^t,⊥TLt+P^t,⊥TStto solveSt. The low-rank component is close to zero ifPt^≈Pt, otherwise new directions are added. Furthermore, recent estimates ofLt=At-Stare stored and used to updatePt. RR-PCP requires the supportxtto be fixed and quite small for a given support sizeSt, but this does often not hold. So, RR-PCP could not handle large outliers support sizes. Qiu and Vaswani [43] address this problem by using time-correlation of the outliers. This method called Support-Predicted Modified-CS RR-PCP4http://home.engineering.iastate.edu/chenlu/ReProCS/.4[43] and Support-Predicted Modified-CS4[42] is also an incremental algorithm and outperforms the RR-PCP. However, this algorithm is only adapted for specific situation where there are only one or two moving objects that remain in scene. But, this is not applicable to real videos where multiple and time-varying number of objects can enter of leave the scene. Moreover, it requires knowledge of foreground motion. Recently, Qiu and Vaswani [44] proposed a method called automated Recursive Projected CS (A-ReProCS4) that ensures robustness when there are many nonzero foreground pixels, that is, there are many moving objects or large moving objects. Furthermore, A-ReProCS outperforms the previous incremental algorithms when foreground pixels are correlated spatially or temporally and when the foreground intensity is quite similar to the background one.Despite the efforts to reduce the time complexity, the corresponding algorithms have prohibitive computational time for real application such as foreground detection. The main computation in PCP is the singular value decomposition of the large matrix A. Instead of computing a large SVD on the CPU, Anderson et al. [2,3] proposed an implementation of the communication-avoiding QR (CAQR) factorization that runs entirely on the GPU. This implementation achieved30×speedup over the implementation on CPU using MKL.Recently, Pope et al. [40] proposed a variety of methods that significantly reduce the computational time of the ALM algorithm. These methods can be classified as follows:•Reducing the computation time of SVD: The computation of the SVD is reduced using the Power method [40] that enables to compute the singular values in a sequential manner, and to stop the procedure when a singular value is smaller than a threshold. The use of the Power method by itself results in4.32×lower runtime. Furthermore, the gain is improved by a factor of2.02×speedup if the procedure is stopped when the singular value is smaller than the threshold. If the rank of L is fixed and the Power SVD is stopped when the number of singular value is equal torank(L), the additional speedup is 17.35. On the results, the threshold fixed torank(L)influences the number of modes of the background that can be represented by L.Seeding the PCP algorithm: PCP operates on matrices consisting of blocks of contiguous frames acquired with a fixed camera. So, the low-rank matrix does not change significantly from one block to the next. Thus, Pope et al. [40] use the low-rank component obtained by the ALM algorithm from the previous block as a starting point for the next block. This method allows an additional speedup of 7.73.Partioning into subproblems: Pope et al. [40] proposed to partition the matrix A into P smaller submatrices. The idea is to combine the solutions of the P corresponding PCP subproblems to recover the solution of the full matrix A at lower computational complexity.These methods showed a speedup of the ALM algorithm by more than 365 times compared to a C implementation.PCP recovers the true underlying low-rank matrix when a large portion of the measured matrix is either missing or arbitrarily corrupted. However, in the absence of a true underlying signal L and the deviation S, it is not clear how to choose a value ofλthat produces a good approximation of the given data A for a given application. A typical approach would involve some cross-validation step to selectλto maximize the final results of the application. The issue with cross-validation in this situation is that the best model is selected indirectly in terms of the final results, which can depend in unexpected ways on later stages in the data processing chain of the application. Instead, Ramirez and Sapiro [45,46] addressed this issue via the Minimum Description Length (MDL) principle [24] and so proposed a MDL-based low-rank model selection. The principle is to select the best low-rank approximation by means of a direct measure on the intrinsic ability of the resulting model to capture the desired regularity from the data. To obtain the family of modelsMcorresponding to all possible low-rank approximation of A, Ramirez and Sapiro [45,46] applied the RPCA decomposition for a decreasing sequence of values ofλ,λt:t=1,2,3,…obtaining a corresponding sequence of decomposition(Lt,St),t=1,2,3,…. This sequence is obtained via a simple modification of the ALM algorithm [31] to allow warm restarts, that is, where the initial ALM iterate for computing(Lt,St)is(Lt-1,St-1). Finally, Ramirez and Sapiro [45,46] select the pair(Ltˆ,Stˆ),tˆ=argmintMDL(Lt)+MDL(St)whereMDL(Lt)+MDL(St)=MDL(A|M)denoted the description length in bits of A under the description provided by a given modelM∈M. Experimental results show that the bestλis not the one determined by the theory in Candes et al. [10].PCP is limited to the low-rank component being exactly low-rank and the sparse component being exactly sparse but the observations in real applications are often corrupted by noise affecting every entry of the data matrix. Therefore, Zhou et al. [72] proposed a stable PCP (SPCP) that guarantee stable and accurate recovery in the presence of entry-wise noise. So, this measurement model assumes that the observation matrix A is represented as follows:(7)A=L+S+Ewhere E is a noise term (say i.i.d. noise on each entry of the matrix) and‖E‖F<δfor someδ>0. To recover L and S, Zhou et al. [72] proposed to solve the following optimization problem, as a relaxed version to PCP:(8)minL,S‖L‖∗+λ‖S‖1subj‖A-L-S‖F<δwhere‖·‖Fis the Frobenius norm andλ=1n.Just as in Eq. (2) for PCP, Tao and Huan [55] showed that an easy reformulation of the constrained convex programming for Eq. (8) falls perfectly in the applicable scope of the classical ALM. Moreover, the favorable separable structure emerging in both the objective function and the constraints entails the idea of splitting the corresponding augmented Lagrangian function to derive efficient numerical algorithms. So, Tao and Huan [55] developed the alternating splitting augmented Lagrangian method (ASALM) and its variant (VASALM), and the parallel splitting augmented Lagrangian method (PSALM) for solving Eq. (8). ASALM and its variants converge to an optimal solution. However, ASALM iterations are too slow for real time application and its complexity is not known. To address this problem, Aybat et al. [4] studied how fast first-order methods can be applied to SPCP with low complexity iterations and showed that the subproblems that arise when applying optimal gradient methods of Nesterov, alternating linearization methods and alternating direction augmented Lagrangian methods to the SPCP problem either have closed-form solutions or have solutions that can be obtained with very modest effort. Furthermore, Aybat et al. [4] developed a new first order algorithm called Non-Smooth augmented Lagrangian Algorithm (NSA), based on partial variable splitting. All but one of the methods analyzed require at least one of the non-smooth terms in the objective function to be smoothed and obtain an∊-optimal solution to the SPCP problem inO(∊)iterations. NSA, which works directly with the fully non-smooth objective function, is proved to be convergent under mild conditions on the sequence of parameters it uses. NSA, although its complexity is not known, is the fastest among the optimal gradient methods, alternating linearization methods and alternating direction augmented Lagrangian methods algorithms and substantially outperforms ASALM. Recently, Aybat et al. [5] proposed a proximal gradient algorithm called Partially Smooth Proximal Gradient (PSPG). Experimental results show that both the number of partial SVDs and the CPU time of PSPG are significantly less than those for NSA and ASALM. An overview of these algorithms as well as their complexity can be seen in Table 3.Mackey et al. [37] proposed a real time implementation framework, entitled Divide-Factor-Combine (DFC). DFC randomly divides the original matrix factorization task into cheaper subproblems in term of complexity, solves those subproblems in parallel using any base matrix factorization (MF) algorithm, and combines the solutions to the subproblem using an efficient technique from randomized matrix approximation. The inherent parallelism of DFC allows for near-linear to superlinear speedups in practice, while the theory provides high-probability recovery guarantees for DFC comparable to those provided by its base algorithm. So, Mackey et al. [37] proposed two algorithms called DFC-PROJ and DFC-NYS, that differ from the method used to divide the original matrix. DFC-PROJ randomly partitions the orthogonal projection of the matrix A onto the t l-column submatricesC1,…,Ctby using a column sampling method, while DFC-NYS selects an l-column submatrix C and an d-row submatrix R using the generalized Nyström method. DFC significantly reduces the per-iteration complexity toO(mlrC1)whererC1is the rank of the matrixC1for the DFC-PROJ. The cost of combining the submatrix estimates is even smaller, since the outputs of standard MF algorithms are returned in factored form. Indeed, the column projection step of DFC-PROJ requires onlyO(mr2+lr2)time forr=maxikCi,O(mr2+lr2)time for the pseudoinversion ofCiandO(mr2+lr2)time for matrix multiplication with eachCiin parallel. For the DFC-NYS, the per-iteration complexityO(mlrC)whererCis the rank of the matrix C andO(mlrR)whererRis the rank of the matrix R. The cost of combining requiresO(lr¯2+dr¯2+min(m,n)r¯2)time wherer¯=max(rC,rR). Mackey et al. [37] improved these real time implementations by using ensemble methods that have been shown to improve performance of matrix approximation algorithms, while straightforwardly leveraging the parallelism of modern many-core and distributed architectures [28]. As such, an ensemble variants of the DFC algorithms have been developed reducing recovery error while introducing a negligible cost to the parallel running time. For DFC-PROJ-ENS, rather than projecting only onto the column space ofC1, the projection ofC1,…,Ctis done onto the column space of eachCiin parallel and then average the t resulting low-rank approximations. For DFC-NYS-ENS, a random d-row submatrix is chosen as in DFC-NYS and independently partition the columns of the matrix in l as in DFC-PROJ. After running the base MF algorithm on each submatrix, the generalized Nystrom method is applied to each pair of matrices in parallel and then the t resulting low-rank approximations is obtained by average.Becker et al. [7] proposed a inequality constrained version of RPCA proposed by Candes et al. [10] to take into account the quantization error of the pixel’s value. Indeed, each pixel has a value between0,1,2,…,255. This value is the quantized version of the real value which is between[0,255]. So, the idea is to apply RPCA to the real observations instead of applying it to the quantized ones. Indeed, it is unlikely that the quantized observation can be split nicely into a low-rank and sparse component. So, Becker et al. [7] supposed thatL+Sis not exactly equal to A, but rather thatL+Sagrees with A up to the precision of the quantization. The quantization can induce at most an error of 0.5 in the pixel value. This measurement model assumes that the observation matrix A is represented as follows:(9)A=L+S+Qwhere Q is the error of the quantization. Then, the objective function is the same than the equality version in Eq. (3), but instead of the constraintsL+S=A, the constraints are‖A-L-S‖∞⩽0.5. So, the quantization based PCP (QPCP) is formulated as follows:(10)minL,S‖L‖∗+λ‖S‖1subj‖A-L-S‖∞⩽0.5Theℓ∞-norm allows to capture the quantization error of the observed value of the pixel.Algorithms for solving QPCP: Becker et al. [7] used a general framework for solving this convex cone problem called Templates for First-Order Conic Solvers (TFOCS). First, this approach determines a conic formulation of the problem and then its dual. Then, Becker et al. [7] applied smoothing and solved the problem using an optimal first-order method. This approach allows to solve the problem in compressed sensing.Tang and Nehorai [54] proposed a block-based PCP (BPCP) that enforces the low-rankness of one part and the block sparsity of the other part. This decomposition involves the same model than PCP in Eq. (1), that isA=L+S, where L is the low-rank component but S is a block-sparse component. The low-rank matrix L and the block-sparsity matrix S can be recovered by the following optimization problem [71]:(11)minL,S‖L‖∗+κ(1-λ)‖L‖2,1+κλ‖S‖2,1subjA-L-S=0where‖·‖∗and‖·‖2,1are the nuclear norm and thel2,1-norm, respectively. Thel2,1-norm corresponds to thel1-norm of the vector formed by taking thel2-norms of the columns of the underlying matrix. The termκ(1-λ)‖L‖2,1ensures that the recovered matrix L has exact zero columns corresponding to the outliers. In order to eliminate ambiguity, the columns of the low-rank matrix L corresponding to the outlier columns are assumed to be zeros.Algorithm for solving BPCP: Tang and Nehorai [60] designed an efficient algorithm to solve the convex problem in Eq. (11) based on the ALM method. This algorithm allow to decompose the matrix A in a low-rank and block-sparse matrices in respect to the‖·‖2,1and the extra termκ(1-λ)‖L‖2,1.PCP is highly effective but the underlying model is not appropriate when the data are not modeled well by a single low-dimensional subspace. Wohlberg et al. [63] proposed a decomposition corresponding to a more general underlying model consisting of a union of low-dimensional subspaces.(12)A=AU+SThis idea can be implemented as the following problem:(13)minU,Sα‖U‖∗+β‖U‖2,1+γ‖S‖1subjA-AU-S=0The explicit notion of low rank, and its nuclear norm proxy, is replaced by representability of a matrix as a sparse representation on itself. Thel2,1-norm encourages rows of U to be zero, but does not discourage nonzero values among the entries of a nonzero row. Thel1-norm encourages zero values within each nonzero row of U.To better handle noisy data, Wohlberg et al. [63] modified the Eq. (13) with a penalized form and add a Total Variation penalty on the sparse deviations for contiguous regions as follows:(14)minU,S12‖A-DU-S‖∗+α‖U‖∗+β‖U‖2,1+β‖S‖1+δ‖grad(S)‖1subjA-DU-S=0where the dictionary D is derived from the data A by mean subtraction and scaling, andgrad(S)is a vector valued discretization of the 3D gradient of S. An appropriate sparse U can be viewed as generating a locally low-dimensional approximation DU ofA-S. When the dictionary is simply the data (i.e.,D=A), the sparse deviations (or outliers) S are also the deviations of the dictionary D, so constructing the locally low-dimensional approximation as(D-S)U, implying an adaptive dictionaryD-S, should allow U to be even sparser.Algorithm for solving LPCP: Wohlberg et al. [63] proposed to solve the Eq. (13) using the Split Bregman Algorithm (SBA) [16]. Adding terms relaxing the equality constraints of each quantity and its auxiliary variable, Wohlberg et al. [63] introduced Bregman variables in the Eq. (13). So, the problem is split into an alternating minimization of five subproblems. Two subproblems arel2problems that are solved by techniques for solving linear systems such as conjugate gradient. The other three subproblems are solved very cheaply using shrinkage, i.e., generalized shrinkage and soft shrinkage.We have evaluated and compared nine RPCA algorithms based on PCP with the dataset provided for the Background Models Challenge (BMC). We have chosen the most representative algorithms: (1) PCP solved via seven different algorithms (EALM [31], IALM [31], ADM (LRSD) [70], LADMAP [33], LSADM [15], LADM (LMaFit) [50], BLWS [34]), (2) SPCP solved via NSA [4] and (3) QPCP solved via TFOCS [7].The BMC (Background Models Challenge) dataset consists of both synthetic and real videos to permit a rigorous comparison of background subtraction techniques for the corresponding workshop organized within Asian Conference in Computer Vision (ACCV). This dataset consists of the following sequences:•Synthetic Sequences: A set of 20 urban video sequences rendered with the SiVIC simulator. With this tool, the associate ground truth was rendered frame by frame for each video at 25fps. Several complex scenarios are simulated such as fog, sun and acquisition noise for two environments (a rotary and a street). A first part of 10 synthetic videos are devoted to the learning phase, while the 10 others are used for the evaluation.Real Sequences: The BMC dataset also contains 9 real videos acquired from static cameras in video-surveillance contexts for evaluation. This real dataset has been built in order test the algorithms reliability during time and in difficult situations such as outdoor scenes. So, real long videos about one hour and up to four hours are available, and they may present long time change in luminosity with small density of objects in time compared to previous synthetic ones. Moreover, this real dataset allows to test the influence of some difficulties encountered during the object extraction phase, as the presence of vegetation, cast shadows or sudden light changes in the scene.All the videos were processed with a unique sets of parameters which are tuned based on the synthetic videos for the learning phase.To evaluate the effectiveness of the nine RPCA approaches based on PCP, we compared their performance with the following approaches: (1) original PCA [39], (2) two Bayesian RPCA algorithms (BRPCA [13], VBRPCA [6]), (3) two approximated RPCA algorithms (GoDec [71], SemiSoft GoDec [71]) and (4) the adaptive MOG [53].Tables 6 and 7show the evaluation results using the synthetic videos and the ones using the real videos for evaluation phase, respectively. These tables are divided in four parts which correspond to a type of algorithm: (1) RPCA based on PCP (RPCA-PCP, RPCA-SPCP, RPCA-QPCP and RPCA-BPCP), (2) Bayesian RPCA, (3) Approximated RPCA and (4) Adaptive MOG. First, we provide a short qualitative analysis in presence of illumination changes and dynamic backgrounds. Then, we give a full quantitative evaluation.Due the page limitation, we present visual results on three video sequences: sequence Rotary 122 (Fig. 2), sequence “Wandering students” (Fig. 3) and sequence “Traffic during windy day” (Fig. 4). More visual results are available in the supplementary materials. For the sequence “Wandering students” (Fig. 3), some gradual illumination changes occur and we can note that the results are visually acceptable. From Fig. 4, we can see that the RPCA model is robust to waving trees but less to the presence of the clouds. As the visual results seem very similar, we have done a full quantitative evaluation.From Table 6, we can see that the algorithm LSADM gives the best performance for the sequences 112, 212, 312 and 412. TFOCS seems to emerge by giving the best results for the sequence 512, 122, 222 and 522. Furthermore, TFOCS obtains the best average F-Measure against all the other methods. From Table 7, the algorithm NSA outperforms the other PCP algorithms for the sequence 002, 003, 004 and 006. LSADM confirms here its good robustness by obtaining the second average F-Measure after the BLWS. Table 8groups the F-measure for the best algorithm in each category. The LSADM gives the best F-measure for the synthetic videos. For the real videos, the best performance is obtained by the TFOCS followed by the LSADM. In conclusion, the LSADM and TFOCS are the most robust algorithms follows by the BLWS and NSA. However, these two algorithms are neither real-time and incremental. Another main conclusion is that most of the RPCA algorithms outperforms the adaptive MOG for the synthetic videos as for real videos too.All the algorithms are implemented in matlab. The computational cost of the RPCA-PCP algorithms is mainly related to the singular value decomposition (SVD). It can be reduced significantly by using a partial SVD because only the first largest few singular values are needed. Practically, the implementation available in PROPACK5http://soi.stanford.edu/rmunk/PROPACK/.5are used for the IALM, LADMAP, LSADM and LADM. The SVDs and CPU time of each algorithm were computed for each sequence. Tables 4 and 5 summarize the average times. The CPU times are reported in the form hh:mm:ss for images of size144×176and with 200 frames for the training to allow easy comparison with other RPCA algorithms as the previous publications in this field present these performances on the I2R dataset [29] in this data format. In this paper, the results for the ASALM [55] and the PSPG [5] come from their authors. We can see that EALM and ADM are very computational expensive due to the fact that theses algorithms compute full SVDs. On these problems of extremely low ranks, the partial SVD technique used in IALM, LADMAP, LSADM and LADM become quite effective and reduce significantly the computation time. For the SPCP, the PSPG solver is the most efficient follows by the NSA and the ASALM. The variational BRPCA is less computational expensive than the BRPCA. Finally, the GoDec algorithm is the one which requires less time computation time, and then it makes large videos applications reachable in real-time.

@&#CONCLUSIONS@&#
