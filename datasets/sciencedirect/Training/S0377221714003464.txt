@&#MAIN-TITLE@&#
A block-based layer building approach for the 2D guillotine strip packing problem

@&#HIGHLIGHTS@&#
We apply three successful ideas for the 2D strip packing with guillotine-cut.We pack a block of rectangles instead of a single rectangle in each step.We divide the strip into horizontal layers and pack layer by layer.We unload and repack the last portion of a solution repeated to improve it.We outperform all existing approaches on standard benchmark instances.

@&#KEYPHRASES@&#
Cutting,Packing,Guillotine-cut,Best-fit heuristic,Layer-building,

@&#ABSTRACT@&#
We examine the 2D strip packing problems with guillotine-cut constraint, where the objective is to pack all rectangles into a strip with fixed width and minimize the total height of the strip. We combine three most successful ideas for the orthogonal rectangular packing problems into a single coherent algorithm: (1) packing a block of rectangles instead of a single rectangle in each step; (2) dividing the strip into layers and pack layer by layer; and (3) unrolling and repacking the top portion of the solutions where usually wasted space occurs. Computational experiments on benchmark test sets suggest that our approach rivals existing approaches.

@&#INTRODUCTION@&#
The 2D strip packing problem (2DSP) is a fundamental problem in cutting and packing literature. We are given a set of n rectangles with dimensionswi×hi,i=1,…,nand a rectangular strip of width W. The task is to orthogonally arrange all the rectangles onto the strip such that the height of the used strip is minimized.The 2DSP models the problem of cutting small rectangular pieces from a large stock. A common consideration is the orientation of the rectangles. For example, when the material has texture patterns, the rectangles must be arranged with a fixed orientation; whereas in other applications the rectangles can be rotated by90°. In many cutting applications, due to the physical constraint of the cutting blade, each cut is parallel to the sides of the sheet and will separate the stock sheet into two completely separated sheets. Such a cut is called guillotine-cut. A packing pattern observes the guillotine-cut constraint, if all rectangles can be cut off the stock sheet by a series of guillotine-cuts.Therefore, there are four combinations when considering rectangle orientations and cutting constraint. They are denoted as: OF, RF, OG and RG following the notations of Lodi, Martello, and Vigo (1999). Where O denotes the rectangles are orientated and not rotatable, R denotes the rectangles are rotatable, F denotes that guillotine-cut is not required, and G denotes that the guillotine-cut constraint is required. In this study, we investigate two variants of the 2D strip packing problem: 2DSP-OG and 2DSP-RG. Note that we do not limit the number of stages (set of cuts with the same orientation) in which a strip can be cut, which is often called non-staged guillotine cutting. For the staged cutting, the reader could refer to Silva, Alvelos, and de Carvalho (2010).2DSP is closely related to the well-studied two-dimensional rectangular packing problem (2DRP), where the height of the strip is limited and the objective is to maximize the total area of packed rectangles. A natural approach to handle the open dimension in the strip packing problems is to impose a limit on the open dimension to transform it to the corresponding rectangular packing problems with fixed dimensions. Hifi (1998) adopts this strategy in their exact algorithm for the 2DSP with guillotine-cut constraint. Wei, Oon, Zhu, and Lim (2011) use binary search to decide the height in their algorithm for the 2DSP and developed an algorithm to solve the 2DRP problem. Bortfeldt and Jungmann (2012) iteratively decrease the height in their algorithm for the 3DSP with guillotine-cut constraint. For each height a single container loading problem with guillotine-cut constraint are solved. Since 2DSP can be seen as a special case of 3DSP by adding a third dimension to each rectangle and the stock sheet, their algorithm can also solve 2DSP instances.A generalization of 2DRP is the constrained two-dimensional single large object placement problem (CSLOPP), where the height of the strip is limited and there is a value associated with each rectangle and the objective is to maximize the total value of packed rectangles. The frequency of each rectangle should not exceed a given upper bound. This problem generalizes the 2DRP when the value of each rectangle is equal to its area.Two general techniques have been widely used to solve the CSLOPP with guillotine-cut constraint exactly: the top-down approach and the bottom-up approach. The top-down approach generates all the feasible packing patterns by repeatedly cutting a sheet into two smaller sheets. The first top-down approach is introduced by Christofides and Whitlock (1977), where all possible packing patterns are enumerated by a tree search. In the search tree, branches correspond to all possible cuts on some sheet. The bottom-up approach generates all the feasible cutting patterns by repeatedly combining two patterns horizontally or vertically. Wang (1983) first proposes the bottom-up approach. Besides the exact approaches, a few heuristic algorithms are also introduced by Chen (2008), Cui and Chen (2012) and Hifi (2004). The heuristics by Chen (2008) and Cui and Chen (2012) using top-down approaches. The heuristic by Hifi (2004) combines a depth-first search using hill-climbing strategies and dynamic programming techniques. Recently, a special case of CSLOPP are studied by Cui and Huang (2012) and Cui (2012) where only the T-shape cutting patterns are considered.There are a few exact algorithms for the 2DSP and 2DRP with guillotine-cut constraint. Hifi (1998) present an exact algorithm based on branch and bound procedures. Ben Messaoud, Chu, and Espinouse (2008) shows that guillotine-cut constraint can be modeled using linear inequalities and proposed a mixed integer programming model for 2DSP. Recently, Bak et al. (2010) propose a parallel branch-and-bound algorithm for the 2DSP with guillotine-cut. Clautiaux, Jouglet, and Moukrim (2011) introduce a graph-theoretical model for 2DRP with guillotine-cut constraint.For the 2DSP with guillotine-cut constraint, despite the theoretical advances and the improved computing hardwares, exact algorithms are only capable of finding optimal solutions for instances of moderate size. Heuristic algorithms are much more successful in dealing with large instances. Among the heuristics, the most popular and successful strategy is to partition the strip into horizontal layers and fill the layers. Examples includes the next-fit decreasing height (NFDH) algorithm (Coffman, Garey, Johnson, & Tarjan, 1980), first-fit decreasing height (FFDH) algorithm (Coffman et al., 1980), best-fit decreasing height (BFDH) algorithm (Valenzuela, Vick, & Wang, 2004), split fit (SF) algorithm (Coffman et al., 1980), floor-ceiling no rotation (FCNR) algorithm (Lodi, Martello, & Vigo, 1998), and knapsack (KP01) algorithm (Lodi et al., 1999). Ntene and van Vuuren (2009) collectively calls them level algorithms and compares the performance of these level algorithms based on standard benchmark test sets. The layer-building algorithms by Zhang, Liu, Chen, and Xie (2005, 2006), Cui, Yang, Cheng, and Song (2008), and Cui, Gu, and Zhong (2008, 2013), which pack the rectangles layer by layer until all the rectangles are packed in the strip, also fall into this category.The main motivation behind layer-building is that it greatly reduces the search space and therefore search effort. This is because, the number of possible feasible packing patterns grow exponentially as the number of rectangles increases. The number of rectangles we can pack in each layer is much smaller than the total number of rectangles. The downside is that we may miss optimal solutions. For large instances, the benefit of reduced search space outweighs its downside and the net effect of layer building is more effective use of limited computation time.We can also improve effective use of computation time from bottom-up. Instead of packing one rectangle at a time, we first combine rectangles into blocks and pack a block of rectangles in each step. As a result, we can pack all rectangles in much fewer steps, which essentially compresses the height of the search tree. Such compression effectively reduces the size of the search tree which grows exponentially as the height of the search tree. The algorithms for CSLOPP (Hifi & Roucairol, 2001; Cui, 2008) and recent algorithms for 3D rectangular packing problems (Fanslau & Bortfeldt, 2009; Wei, Oon, Zhu, & Lim, 2012; Zhu, Oon, Lim, & Weng, 2012; Zhu, Huang, & Lim, 2012; Zhu & Lim, 2012; Bortfeldt & Jungmann, 2012) have demonstrated the effectiveness of using blocks. Although using blocks reduces the nodes in a search tree, it does not reduce the search space associated with a problem instance, for a detailed analysis see Zhu and Oon, et al. (2012).We combine the layer-building with blocks in a single algorithm to harness the benefit of both sides in Section 2. Where we also illustrate the additional benefit of such combination – the waste in layers is reduced on average. We will show how we mitigate the downside of layer-building in Section 2.2.It is worth noting that Bortfeldt and Jungmann (2012) also use block building approach to solve the 2DSP-OG. Our approach differs from their approach in two aspects. First, Bortfeldt and Jungmann (2012) transform 2DSP-OG into a series of 2DRP-OG by iteratively decreasing the height of the strip, whereas, we solves the 2DSP directly by constructing a solution from bottom up layer by layer. Second, we propose a new best-fit heuristic to fill a bounded space.When a large number of rectangles are packed into a strip using a heuristic, the bottom part of the strip is usually well utilized with very little waste, the top part of the strip however leave many holes. Therefore arranging the rectangles in the top part of the strip is the key to reduce the overall height of the packing. Burke, Kendall, and Whitwell (2009) use a simulated annealing algorithm to rearrange the top part of packings found by a best-fit heuristic. We propose to repeatedly unload and repack the last few layers (Section 2.2) in our approach, which will achieve similar goal as the simulated annealing approach by Burke et al. (2009) – pay more attention to the top portion of a packing.Last but not least, we propose a new best-fit heuristic to fill a layer in Section 2.1. Extensive computational experiments are conducted in Section 4 to demonstrate the superiority of our approach over existing algorithms for both the 2DSP-OG and the 2DSP-RG.The rest of the paper is organized as follows. In Section 2, we present our block-based recursive layer-building algorithm, we then describe how this procedure is used as a subroutine in an iterative application. Our computational experiments are reported in Section 4. Finally, we conclude our study in Section 5 with some closing remarks.We can solve 2DSP-OG and 2DSP-RG as follows. We first divided the sheet into a bottom layer and a smaller sheet on top (Fig. 1) of it. We try to fill up the bottom layer with a subset of items and pack the remaining items into the smaller sheet on top. Filling up the smaller sheet is exactly the same as the original problem but of smaller scale. Therefore, it can be solved recursively. To obtain a good solution, we need to decide an appropriate height for the bottom layer and to utilize the space in the bottom layer well. To determine the best height for the bottom layer, we try a few heights and evaluate their potentials in leading to good solutions. For each height, we create a bottom layer of that height and fill it with rectangles using the best-fit heuristic PackBoundedRegion which will be presented in Section 2.1. We then invoke another heuristic PackOpenSheet to pack remaining rectangles into the smaller sheet on the top and return a best packing. The quality of the best packing is used as an indication of the potential of the bottom layer. The details of PackOpenSheet will be described in Section 2.2.Existing level algorithms often pack one rectangle at a time, whereas we pack a block of rectangles at a time until all rectangles have been packed. A block is a set of rectangles that is packed compactly inside its minimum bounding rectangle (MBR), which defines the size of the block. Fig. 2illustrates an example block that consists of three rectangles and its MBR is highlighted by dashed lines. We will explain how blocks are generated in Section 2.3.Using the height of a block instead of an individual rectangle to determine the height of a layer increases the chance of better utilizing the space in the layer. This is because, to well utilize the space in a layer, there must exist subsets of rectangles whose total height are very close to the height of the layer. Such subsets are rare when the height of the layer is determined by a rectangle, unless the rectangle is very tall compared to the other rectangles or there are many rectangles with similar but smaller heights. Fig. 3compares these two approaches using a concrete example consisting of 8 rectangles. In Fig. 3(a), height of layers is determined by rectangles. Two layers are required to pack all 8 rectangles and the resulting height of the solution is 17. In Fig. 3(b) one layer is created using the height of a block consisting two rectangles, the height of the resulting solution is 14.Since our goal is to minimize the height, once a solution is found, there is no need to consider any solutions with greater or equal height. We therefore keep track of the best known solution in a global variableS∗, update it whenever a better solution is found and use it to reduce future search space.RecursiveLayerBuilding in Algorithm 1 implements our ideas. A partial packing is represented as a state S including the following attributes:•rectList: the set of rectangles to be packed,blockList: a list of candidate blocks we can use,pckedRectList: a list of rectangles already packed with their positions recorded,packedArea: the total area of packed rectangles,height: the height of current packing,A rectangular region R in the stock sheet is described the following four attributes:•x, y: the coordinate of bottom left corner of R,width, height: the width and height of R.Block-based Recursive Layer-Building AlgorithmRecursiveLayerBuilding(S)// Input: S, the current state of packing1if no blockb∈S.blockListwithS.height+b.height<S∗.heightexists2returnS// determine the bottom layer by considering a few blocks3bestState=NULL;bestFillRatio=04for each blockb∈S.blockListwithS.height+b.height<S∗.height5nextState=S// create a bottom layer with height b.height in two steps// 1. place block b at(0,S.height)and create a layer of heightb.height6  PlaceBlock(nextState,b,0,S.height)// 2. fill the remaining space R in the bottom layer using best-fit7R.x=b.width;R.y=S.height8R.width=W-b.width;R.height=b.height9 PackBoundedRegion(nextState,R)// fill the smaller sheet on the top and update S∗10(TS,layerNo)=PackOpenSheet(nextStat)11fillRatio=TS.packedArea/(TS.height×TS.width)12iffillRatio>bestFillRatio13bestState=nextState14bestFillRatio=fillRatio// recursively fill the top sheet15return RecursiveLayerBuilding(bestState)PlaceBlock(S,b,x,y)// place a block b with bottom left corner at(x,y)// update the state accordingly1 for each rectangle i in b2  Compute the coordinate(x1,y1)of the bottom left corner of i3  Compute the coordinate(x2,y2)of the top right corner of i4  Add(i,x1,y1,x2,y2)to S.packedRectList5  Delete i from S.rectList6S.packedArea=S.packedArea+(y2-y1)×(x2-x1)7  Delete all blocks from S.blockList that contains rectangles not in S.rectListAlgorithm 1 works as follows. If no block can be packed without exceeding the height of best known solution, we can stop the search and return the best partial solution found so far (line 2). Otherwise, we try to create bottom layers using heights of blocks and select the best one (line 5), where blocks that are too tall are ignored. Once the best bottom layer is selected, we recursively pack the remaining rectangles into the smaller sheet on top of it (line 17). We evaluate the potential of a bottom layer by creating a best possible packing TS using a heuristic PackOpenSheet (line 11). Since we imposed a height limit, the best possible packing TS may not include all rectangles in the input. Therefore instead of using the height to measure the quality of the packing TS, we used fill ratio (line 12, where fill ratio of a packing is defined as the total area of packed rectangles divided by the total area of the used stock sheet). Using fill ratio to measure the quality of a packing is consistent with our objective, as feasible solution with smaller height has larger fill ratio.The procedure PlaceBlock(S,b,x,y)place a block b with its bottom left corner at the coordinate(x,y)and update the state S accordingly. All rectangles in the block are placed at the appropriate position (lines 2–3), appended to the end of S.packedRectList (line 4) and removed from the list of unpacked rectangles S.rectList (line 5). Since there are less unpacked rectangles in S.rectList, some of the blocks become invalid because they consit of more rectangles than available. Such blocks are deleted from S.blockList (line 7).Given a list of candidate blocks, we fill up a bounded region R greedily as follows. We try to find the first block that occupies the entire region. If such a block exists, we fill the region with it and we are done. Otherwise, we try to find the first block that spans either the entire width or height of the region as shown in Fig. 4. If such a block exists, we place it at the bottom left corner of the region and recursively fill the remaining region using best-fit. Otherwise, all blocks that can fit into the region are smaller than the given region in both width and height dimensions. In this case we place the first block that can fit into the region at the bottom left corner of the given region. There are two possible ways to divide the remaining space in the given region as illustrated in Fig. 5. If the remaining widthΔwis too small to accommodate any unpacked rectangles, we will divided the remaining space horizontally (Fig. 5(a)) and recursively fill the remaining spaceR1. Otherwise, we will divide the remaining space vertically (Fig. 5(b)) into two new rectangular regionsR1andR2. We will recursively fill the region with smaller area first followed by that with larger area. The best-fit heuristic is implemented as PackBoundedRegion in Algorithm 2.Algorithm 2Pack bounded region recursively using best-fitPackBoundedRegion(S,R)// Input: S: a state representing a partial packing//R: a rectangular region to be filled with rectangles1if no blocks in S.blockList can fit into R2return3b=the first block in S.blockList with same height and width as R4ifb≠NULL5  PlaceBlock(S,b,R.x,R.y)6return7b=the first block in S.blockList with same height or width as R and fits into R8ifb≠NULL9  PlaceBlock(S,b,R.x,R.y)10ifb.width==R.width11R1.x=R.x;R1.y=R.y+b.height12R1.width=R.width;R1.height=R.height-b.height13PackBoundedRegion(S,R1)14elseR2.x=R.x+b.width;R2.y=R.y15R2.width=R.width-b.width;R2.height=R.height16PackBoundedRegion(S,R2)17return18b=the first block in S.blockList that fits into R19  PlaceBlock(S,b,R.x,R.y)20Δw=R.width-b.width21ifΔw<the minimum width of unpacked rectangles22R1.x=R.x;R1.y=R.y+b.height23R1.width=R.width;R1.height=R.height-b.height24  PackBoundedRegion(S,R1)25elseR1.x=R.x;R1.y=R.y+b.height26R1.width=b.width;R1.height=R.height-b.height27R2.x=R.x+b.width;R2.y=R.y28R2.width=Δw;R2.height=R.height29ifR1.width×R1.height⩽R2.width×R2.height30PackBoundedRegion(S,R1)31PackBoundedRegion(S,R2)32else PackBoundedRegion(S,R2)33PackBoundedRegion(S,R1)Given a state S representing a partial packing of height S.height, the free space on top of it is a strip with width W whose bottom is at height S.height, we call it the top sheet for short. Our PackOpenSheet will try to fill as many unpacked rectangles as possible into the top sheet subject to a height limit S∗.height−S.height, which is the height of the best known solution. If all unpacked rectangles S.rectList can be packed, we would like to minimize the height of the final packing. Otherwise, we would like to maximize the fill ratio, where fill ratio of a packing is defined as the total area of packed rectangles divided by the total area of the packing.Our PackOpenSheet (Algorithm 3) works in two phases. In phase 1, we try to pack rectangles layer by layer in a greedy manner. In phase 2, we try to unroll the last few layers and repack them as an attempt to reduce the height of the packing. Phase 1 is implemented as a recursion (lines 2–9), each recursive call to PackOpenSheet except the last will add a new layer on top of existing partial packing. Assuming a total of k new layers are created in phase 1, the first activation of phase 2 occurs right after k layers are added by phase 1. It essentially unloads all rectangles in the last layer and tries to pack them back into the strip with reduced height. If we successfully repack all rectangles, we will reduce the height and repeat. The second activation of phase 2 happens afterk-1layers are added by phase 1. It will unload all rectangles in the last two layers added by phase 1 and attempts to pack them back into the strip with reduced height. The third activation of phase 2 will repack the last three layers created by phase 1 and so on and so forth. The number of times phase 2 is activated is controlled by a user specified parameter RepackLayerCount.The static structure of PackOpenSheet may look similar to RecursiveLayerBuilding. The runtime dynamics of these two procedures differ greatly. First, we use the first suitable block to determine a layer in PackOpenSheet, whereas, we try a few suitable blocks and select the best block using our evaluation method in RecursiveLayerBuilding. The former results in a linear structure whereas the later results in a tree structure at runtime. Second, there is a phase in which we try to unroll the last few layers and repack them as an attempt to reduce the height of the packing in PackOpenSheet; no such phase exists in RecursiveLayerBuilding.Algorithm 3Pack open sheet using greedy layer-building with progress unrollingPackOpenSheet(S)// Input: S, a state representing a partial packing// Parameter: RepackLayerCount, the number of layers to be repacked1δh=1for integral inputs and 0.1 for real inputs2b=the first block in S.blockList withS.height+b.height<S∗.height3ifb==NULL then return(S,0)// Phase 1: Fill the open space on top of S layer by layer greedily//  a. create a layer with height b.height4S′=S5  PlaceBlock(S′,b,0,S.height)6R.x=b.width;R.y=S.height7R.width=W-b.width;R.height=b.height//  b. fill the layer using best-fit8  PackBoundedRegion(S′,R)//  c. recursively fill the remaining space on top9(bestState,curLayerId)=PackOpenSheet(S′)10ifbestState.rectListis empty thenS∗=bestState// Phase 2: Iteratively reduce the height of packing11ifcurLayerId<RepackLayerCount12repeat13oldBest=S∗.height14S′=S// create a single layer whose top side is lower than S∗.height byδh// which is equivalent to unrolling all new layers in bestState and repack them15R.x=0;R.y=S.height16R.width=W;R.height=S∗.height-δh-S.height// fill the layer using best-fit17PackBoundedRegion(S′,R)18if S′rectList is empty thenS∗=S′19if fill ratio ofS′is higher than bestStatethenbestState=S′20untiloldBest==S∗.height21return(bestState,curLayerId+1)It is well known that when a packing is created from bottom up using a greedy heuristic, such as phase 1 of our PackOpenSheet, the bottom portion of the packing tends to be very neat with little wasted space and holes appear more often towards the top end of the packing. Therefore the top end of the packing is the bottleneck and needs more attention. Our arrangement of the k activations of phase 2 ensures that the space towards the top end of the packing are repacked more often.Blocks are recursively defined and generated. Every individual rectangle is a block, two blocks can be joined either along x-axis as in Fig. 6(a) or along y-axis as in Fig. 6(b) to form a larger block. A block has the following attributes:•MBR: the minimum bounding rectangle that encloses all rectangles inside the block. It defines the dimensions of the block.packedRectList: a list of packed rectangles inside the MBR of the block.fillRatio: total area of packed rectangles divided by the area of MBR.We first generate blocks that consist of one rectangle. For each rectangle in the input, we generated one block for each allowed orientation. That is, if a rectangle can be rotated by 90 degree, we will create two block one for each orientation of the rectangle. We then iteratively combine smaller blocks along x- and y-axis to generate larger blocks, until sufficient number of blocks are generated. The maximum number of blocks generated is controlled by a user specified parameter maxBlockCount. Since, our objective is minimize the waste in the strip so as to minimize the total height of the strip, we will only generate blocks whose fillRatio is not below a user defined threshold minFillRatio. Furthermore, blocks that are too wide to fit into the strip are useless and therefore not generated. Blocks that are too tall will not be generated either, where the maximum allowed height of generated blocks is specified by a parameter maxH. Since we successively join two blocks to form larger blocks, some larger blocks may consists of more rectangles than available in the input for certain type of rectangles, such blocks will not be generated. Finally, two blocks with same MBR and consisting of the same set of rectangles, even if the positions of rectangles differ, are equivalent for the purpose of minimizing packing height. Therefore, if a generated block is equivalent to an existing block, we will discard it. The pseudocode of our block generation procedure GenerateBlocks is given in Algorithm 4.Algorithm 4Generating blocksGenerateBlocks(rectList,maxBlockCount,minFillRatio,W,maxH)// Input: rectList, list of input rectangles//maxBlockCount, controls the total number of generated blocks//minFillRatio, controls the fill ratio of generated blocks//W, width of stock sheet//maxH, controls the height of generated blocks// Generate blocks consisting of single rectangles1blockList=∅2 for each rectangle r in rectList3  Create blocks for each orientation of r and append them to blockList// iteratively join blocks into larger blocks4prevList=blockList// blocks generated in previous iteration5while block count in blockList<maxBlockCountand prevList not empty6curList=∅// blocks generated in current iteration7for each blockb1∈blockListandb2∈prevList8if total rectangles inb1andb2exceeds rectList for some type9Continue for-loop10b=joinb1andb2along x-axis if total width not exceed W11ifb.fillRatio⩾minFillRatio12if no equivalent blocks exists in blockList and curList13Append b to curList14b=joinb1andb2along y-axis if total height not exceed maxH15ifb.fillRatio⩾minFillRatio16if no equivalent blocks exists in blockList and curList17Append b to curList18  Append all blocks in curList to blockList19prevList=curList20  Keep only the first maxBlockCount blocks in blockList21returnblockListThe solution produced by RecursiveLayerBuilding(S)is fully determined by the list of candidate blocksS.blockListand the height of best known solution S∗. Our approach calls this procedure several times using different lists of candidate blocks and report the best solutions found. Our iterative block-based recursive layer-building (IBRL) algorithm is shown as Algorithm 5. Initially we will only consider blocks whose areas are fully utilized, that is, with fill ratio 1.0. In subsequent iteration, we will consider blocks that consist of more wasted area. We reduce the minimum fill ratiominFillRatioby0.01in each iteration (line 7). The number of all possible blocks grows exponentially as the number of input rectangles, it is simply too huge for any input of reasonable size. We therefore limit the total number of blocks generated by a user defined parameter maxBlockCount.Algorithm 5Iterative block-based recursive layer-building algorithmIBRL(R,W)// Input: R, list of rectangles to be packed//W, width of strip// Parameters: timeLimit, CPU time limit//maxBlockCount, maximum number of generated blocks1δh=1for integral inputs and 0.1 for real inputs2totalArea=total area of rectangles in R// We can easily find a packing with fill ratio⩾90%for most inputs// We use it to estimate the maximum height of generated blocks3LB=totalArea/W4maxH=totalArea/0.9/W5  Create a dummy solutionS∗withS∗.height=∞6whileS∗.height>LBand total execution time<timeLimit7forminFillRatio=1.0downto0.9by0.01blockList=GenerateBlocks(R,maxBlockCount,minFillRatio,W,maxH)8sort blocks inS.blockListby decreasing area of packed rectanglethen by increasing height of their MBR,then by increasing width of their MBR// create an initial state representing the empty stock sheet9S.rectList=R;S.packedRectList=∅10S.packedArea=0;S.height=011S.blockList=blockList12S=RecursiveLayerBuilding(S)13ifS is a better solution thanS∗thenS∗=S14maxH=min{maxH,S∗.height-δh}15returnS∗For most problem instances, we can easily find a packing with fill ratio more than 90%. We therefore limit the candidate blocks we will consider based on this assumption as follows. Firstly, the height of a packing with fill ratio 90% will not exceedmaxH=totalArea/0.9/W, wheretotalAreais the total area of input rectangles and W is the width of the strip. Any block with height exceeding maxH is unlikely to be useful. Thus, we will only generate blocks with height not exceedingmaxH(line 4). Secondly, we will only generate candidate blocks whose fill ratio is not less than 90% (line 7). The choice of 90% here is based on experiences, for other application context alternative choices say 80% may be more appropriate. Using a smaller stopping value in the for-loop in line 7 usually produces better solutions at the cost of more computing time. The best value for an application can be determined by a calibration process as follows. First use 90% as the default, run our algorithm on historically test data. Observe the average fill ratio in the best solutions. We can then use a value that is a few percentages below the observed average.Six sets of test data have been widely used by existing approaches to compare performances. We will first summarize the benchmark test data in Section 4.1. We selected a subset of test data as training set to calibrate our approach. The calibration process includes selecting the best strategy for various components of our approach and deciding the best value for the parameters used in our approach. We presented our calibration process in Section 4.2. Finally our calibrated approach is compared with existing approaches on the entire sets of test data in Section 4.3.For all the computational experiments conducted in this section, our algorithm IBRL were implemented as sequential algorithms in C++ and compiled by GCC 4.1.2, and no multi-threading was explicitly utilized. It was executed on an Intel Xeon E5430 clocked at 2.66gigahertz (Quad Core) with 8gigabytes RAM running the CentOS 5 linux operating system. Our approach involves three user specified parameters. Unless otherwise stated, they are set as follows throughout all experiments. The parameter maxBlockCount that controls the maximum number of blocks generated is set to 10,000. The time limit for all instances was set to 60 CPU seconds. The parameter RepackLayerCount is set to 1.Throughout this section, unless otherwise stated, we will use relative gap to measure the performance of an algorithm on a test instance. For an instance with known optimal solution, the height of the known optimal solution is a valid lower bound. For an instance without known optimal solution, a simple valid lower bound is given by the total area of input rectangles divided by the width of the strip. The difference between the height of the best solution found by an algorithm and the valid lower bound divided by the lower bound gives the relative gap of the algorithm on that instance.There are six sets of test data employed by existing approaches. They can be classified into two categories by how they are generated. The first category is usually called zero-waste in the literature related to 2DSP without guillotine-cut constraint, where instances are generated by successively cutting a large rectangle into smaller rectangles. A perfect packing that utilizes 100% of the packing area is known. However, the known perfect packing may not follow the guillotine-cut constraint. Here, we still name the first category zero-waste although the perfect packing with guillotine-cut constraint may not exist for some instances. The second category is called non-zero-waste, where the input rectangles are randomly generated and optimal solution is not known.The first category includes four test sets: C, Burke, N and T.•C: consisting of 21 instances generated by Hopper and Turton (2001). They are divided into seven groups. Each group contains three instances, where the known perfect packing of first two instances follows guillotine-cut constraint. For the third instance in the first, second, third and fourth group, a perfect packing following guillotine-cut constraint has been reported (Cui & Gu, et al., 2008; Bortfeldt & Jungmann, 2012; Cui et al., 2013). For the remaining instances, the known optimal solution does not follow guillotine-cut constraint.Burke: consisting of 13 instances generated by Burke, Kendall, and Whitwell (2004). The known perfect packing follows guillotine-cut constraint.N and T: each consisting of 70 instances generated by Hopper (2000). The set N consists of 70 instances whose known perfect packing does not follow guillotine-cut constraint. The set T consists of 70 instances whose known perfect packing follow guillotine-cut constraint. However, when we follow the method described by Hopper (2000) to restore the known perfect packing for T instances, we found the known perfect packing clearly does not follow the guillotine-cut constraint. For example, Fig. 7shows the solution we restored for the instance T1b in test set T.Nice and Path: each consisting of 210 instances. The original instances are generated by Valenzuela et al. (2004). Unlike the previous test sets, the original dimensions of all input rectangles in these two sets are real numbers. Cui et al. (2013) multiple the dimensions of all rectangles by 20 and round them up to the nearest integer. For the dimensions of a sheet they are already integers so they simply multiply them by 20. Once we find a feasible solution to the scaled instance, we can scale down the entire layout by a factor of 20 to obtain a feasible solution to the original instance. This is because the dimensions of the sheet in the scaled-down layout are the same as the original sheet, while the dimensions of any rectangle in the scaled-down layout are larger than or equal to the original rectangle since⌈20x⌉/20⩾xfor all positive real number x.The second category includes two test sets: KR and bwmv:•KR: consisting of 12 instances from Kröger (1995).bwmv: consisting of 500 instances. They were designed for the bin packing problem. They are divided into 10 classes, each with 5 groups of 10 instances. The first six classes were created by Berkey and Wang (1987), and the remaining four by Martello and Vigo (1998). Iori, Martello, and Monaci (2003) adapted these instances to 2DSP instances by disregarding the bin heights.The characteristics of all the test data are summarized in Appendix B in the online supplements.For the purpose of calibrating our approach we selected a subset of instances from the six benchmark test sets as training set. For every test set, we select one instance for every three instances. Since instances are randomly generated, our selection criterion is roughly equivalent to randomly selected 1/3 of the instances. The main motivation to consider only 1/3 of the instances as training set is to avoid overfitting our algorithms to the benchmark test data. So that if our algorithm works well for the instances outside the training set, it is also likely to work well for unknown instances (outside the benchmark test data) that share similar characteristics as the benchmark test data.Recall that RepackLayerCount controls the number of layers to be unloaded and repacked in phase 2 of PackOpenSheet. To demonstrate the effect of repacking last few layers, for each instance in the training set, we executed our IBRL three times with RepackLayerCount set to 0, 1, 2. We divided the training set by the number of input rectangles n into three groups: small(n<50), medium(50⩽n<100)and large(n⩾100)and aggregate the results by group. The results for the 2DSP-OG and 2DSP-RG are presented in Fig. 8(a and b) respectively. For each group of training instances, the left, middle and right bar corresponding to the results obtained by setting RepackLayerCount to 0, 1, and 2 respectively.We can see from these figures that repacking last layer (RepackLayerCount=1) or repacking last two layers produce better solutions than no repacking (RepackLayerCount=0). However, repacking last two layers produces slightly worse solutions than repacking only the last layer. This is because increasing RepackLayerCount will increase the execution time of PackOpenSheet; thus reduce the number of iterations as we stop our IBRL once 60seconds is reached. Since repack last layer produces best overall results on the training set, we decided to set RepackLayerCount=1 when compare our algorithm with exiting approaches.To analyze the convergence behavior of our approach, for each instance in the training set, we execute our IBRL for 1024 CPU seconds and records the solutions whenever a better solution is found. After the experiment is done, we find the best solution at the end of 1seconds, 2seconds, 4seconds, …, 512seconds and 1024seconds. We then aggregate the results by input size of test instances. We divided the training set by the number of input rectangles n into three groups: small(n<50), medium(50⩽n<100)and large(n⩾100). The results for the 2DSP-OG and 2DSP-RG are summarized in Fig. 9(a and b) respectively. Each line in the figures represents the average results of one group of training instances. The CPU time in seconds are shown in x-axis, the average relative gap to lower bound is shown in y-axis.For both 2DSP-OG and 2DSP-RG, our IBRL converges faster on the smaller instances than the larger instances. Take the 2DSP-OG for example. For small instances, our IBRL algorithm converged after 8seconds, that is, the quality of solutions found after 8 CPU seconds is not much worse than that after 1024seconds. For medium instances, our IBRL algorithm converged after 32seconds. For large instances, our IBRL continue to improve when given more CPU time. However, after 64seconds the curve become flatter, which indicate diminishing return on addition CPU time. One possible reason is that the search space of the larger instances are much larger, hence the possibility of finding better solutions in the additional time are increased.For each group of test instances, our IBRL converges faster on the 2DSP-OG variant than on the 2DSP-RG variants. For example, for the groupn≤50, our IBRL converges after 8seconds for the 2DSP-OG, whereas it takes 64seconds to converge for the 2DSP-RG. Since the search space for 2DSP-RG is about2ntimes larger than that of 2DSP-OG, the slow convergence for 2DSP-RG may due to the increased size in search space.We have decided to set the time limit to 60seconds when report our final results in the following section because 60seconds is sufficient for IBRL to converge on most small and medium instances. For large instances, although allocating more time will improves the quality of solution find, the improvements are achieved at a diminishing rate.Recall that our approach reduces the minimum fill ratio minFillRatio of the block iteratively; the block list will be extended by including blocks with more waste as time increases. To analyze what minFillRatio is achieved at different time, for each instance in the training set, we record the value of minFillRatio at the end of 1seconds, 2seconds, 4seconds, …, 512seconds and 1024seconds. As with the previous analysis, we divided the training set by the number of input rectangles n into three groups: small(n<50), medium(50⩽n<100)and large(n⩾100)and aggregate the results by group. The results for the 2DSP-OG and 2DSP-RG are summarized in Fig. 10(a and b) respectively. Each line in the figures represents the average waste limit (1−minFillRatio) of one group of training instances. All small and medium instances (n<100) reached waste limit 0.1 at 1024seconds. At the time limit of 60seconds, the average waste limit reached for instances in small, medium and large group is about 0.07, 0.06 and 0.06 respectively.In order to test the effect of block waste limit to the final solution, we run each instance without setting a time limit and record the block waste limit (1−minFillRatio) at which the best solution is found. Then, for each block waste limit, we calculate the percentage of instances at which the best solution is found. The results are shown as Fig. 11. We can see from the figure that for both of the 2DSP-OG and 2DSP-RG, almost half of the instances find their best solution when the block waste limit is set to 0. The percentage of instances whose best solution is found at a given block waste limit decrease as this limit increase. However, there are still some instances whose best solution is found when the block waste limit is set to 0.1 for the 2DSP-OG.We report the performance of our approach on the six sets of benchmark data and compare our approach with the leading algorithms in the literature:•HR: a heuristic recursive algorithm by Zhang et al. (2006).SA: a simulated annealing enhancement to the HR algorithm by Zhang et al. (2005).GA: a genetic algorithm by Bortfeldt (2006).B&B: a heuristic recursive branch-and-bound algorithm by Cui and Yang, et al. (2008).TRS: a tree search algorithm by Bortfeldt and Jungmann (2012).SGVCP: a heuristic approach by Cui et al. (2013).In our IBRL the timeLimit is set to 60seconds for each instance. The computational environments for these approaches are summarized in Appendix A in the online supplements. The detailed computational results presented in this section and the test data are available online at www.computational-logistics.org/orlib/2dsp-gcut.In Table 1, we first compare our approach with existing algorithms on test set C, which is most frequently employed for comparing the performance of 2D strip packing algorithms. Algorithms that solve the 2DSP-OG and 2DSP-RG variants are grouped under the heading 2DSP-OG and 2DSP-RG respectively. For each existing algorithm, we report its performance in one column using the name of the algorithm as column heading. The performance of an algorithm is measured by the percentage gap between the height of the best solution found and the lower bound. The performance of our own approach is reported under the column IBRL. We also record the time when the best solution is found by our approach and reported in the columns TTB (seconds). Since C instances are divided into seven groups, the values reported in Table 1 are averaged over all instances in a group. For each group, the algorithm that outperforms others on a test group is highlighted in bold. Our approach achieved best results for all seven groups for the 2DSP-OG. For the 2DSP-RG, our approach achieved best results for all groups except C3.Table 2compares our approaches against existing approach on test set KR. For each algorithm the height of the best solution found is reported. For algorithm GA every instance is solved 10 times, therefore, the average of the 10 executions is reported. For our own approach, we also report the time when the best solution is found in the columns TTB (seconds).For the 2DSP-OG, TRS is the only existing algorithm that reports results on KR instances. Our approach finds better solutions for five instances than TRS and obtained better overall results. For 2DSP-RG, four existing algorithms reported their results. Our approach could match the best known solution for 7 instances. The overall performance of our approach is better than existing algorithms except SGVCP.Table 3compares our approach against existing approach on test set Nice and Path for 2DSP-RG. The test set is group based on the number of input rectangles, the average height for each group is reported.To best of our knowledge, TRS is the current best algorithm for 2DSP with guillotine-cut constraint and it is the only algorithm that reports results on all six benchmark test sets. Table 4compares our approach against TRS. All figures reported in the table are averaged over all instances in a test set. The columns gap report the average relative gap. The total execution time in seconds for each instance is recorded; the average over a test set is reported in the columns t (seconds). For our approach, the time-to-best solution is also recorded and the average over a test set is reported in the columns TTB (seconds). The gap reported for test set bwmv is computed differently from other test sets. The set bwmv consists of 10 classes of instances; each class consists of 5 groups of instances and each group consists of 10 instances. Bortfeldt and Jungmann (2012) only reported the average height of solutions found for each group without giving the height of individual test instances. We converted their results as follows. First, we compute the gap between the average height of a group and the average lower bound of the group. We then divided it by the average lower bound of the group to convert it into a percentage gap. For the 50 groups in bwmv we take the average of the percentage gap as the average for the test set bwmv. The results of our approach are converted in exactly the same manner for fair comparison.For the 2DSP-OG variant, our approach produces better results than TRS for five test sets, for the remaining test set T our approach produces slightly worse results than TRS. For 2DSP-RG, our approach produces better results for all six test sets. The detailed comparison on test sets C, Burke, N, T and bwmv are included in Appendix E in the online supplements.

@&#CONCLUSIONS@&#
Layer-building is a common technique employed by existing algorithms for 2D orthogonal rectangular packing problem. It attempts to reduce search space using divide and conquer. Block-building on the other hand tries to reduce the search space from bottom-up. We showed that by combining these two techniques into a single algorithm we can get the benefit of both sides. Simple heuristic such as best-fit remains the most effective tool for handling large scale instances. However, it is well known that packing obtained by simple heuristics leave much wasted space toward the end of the packing. Therefore, it is natural to pay more attention to the last portion of the packing instead of treating all parts equally. We show that these three ideas can be combined into a coherent algorithm to harness all of their benefit.Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.ejor.2014.04.020.Supplementary material