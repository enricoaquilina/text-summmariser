@&#MAIN-TITLE@&#
Simulated annealing and tabu search approaches for the Corridor Allocation Problem

@&#HIGHLIGHTS@&#
The Corridor Allocation Problem (CAP) is considered.Applications of the CAP include the arrangement of rooms in office buildings, hospitals, shopping centers or schools.Tabu search and simulated annealing algorithms are presented to minimize the total flow cost among facilities.Large CAP instances with up to 70 facilities are tested.

@&#KEYPHRASES@&#
Facilities planning and design,Tabu search,Simulated annealing,Combinatorial optimization,

@&#ABSTRACT@&#
In the Corridor Allocation Problem, we are given n facilities to be arranged along a corridor. The arrangements on either side of the corridor should start from a common point on the left end of the corridor. In addition, no space is allowed between two adjacent facilities. The problem is motivated by applications such as the arrangement of rooms in office buildings, hospitals, shopping centers or schools. Tabu search and simulated annealing algorithms are presented to minimize the sum of weighted distances between every pair of facilities. The algorithms are evaluated on several instances of different sizes either randomly generated or available in the literature. Both algorithms reached the optimal (when available) or best-known solutions of the instances with n⩽30. For larger instances with size 42⩽n⩽70, the simulated annealing implementation obtained smaller objective values, while requiring a smaller number of function evaluations.

@&#INTRODUCTION@&#
The Corridor Allocation Problem (Amaral, 2012), hereafter abbreviated as CAP, seeks to arrange n facilities without overlap along a corridor subject to two constraints: the arrangement on both sides of the corridor should start from a common point on the left end of the corridor; and no space is allowed between two adjacent facilities. We are given the length liof each facility i and the nonnegative amount of flow cijbetween facility i and j. The width of the corridor is considered to be negligible, thus the distance between two facilities i and j with respect to a certain CAP layout is the x-distance between their centers. The total flow cost is a weighted sum of the distances between each pair of facilities using the parameters cijas weights. The objective of the CAP is to find a layout that minimizes the total flow cost over all possible layouts.The CAP has close relationships with other problems in the literature. For example, in the single row facility layout problem (SRFLP) we wish to find an arrangement of the facilities of known lengths, all placed at the same side of the corridor, so as to minimize the total flow cost. Surveys (Anjos and Liers, 2012; Kothari and Ghosh, 2012) and several methods are presented in the SRFLP literature: exact methods (e.g. Amaral, 2006, 2008, 2009b, 2009a; Anjos and Vannelli, 2008; Simmons, 1969), heuristic methods (e.g. Datta et al., 2011; de Alvarenga et al., 2000; Kothari and Ghosh, 2013), and lower bounding methods (e.g. Amaral and Letchford, 2012; Anjos et al., 2005; Anjos and Yen, 2009; Hungerländer and Rendl, 2012).The CAP is also close to the double row layout problem (DRLP) (e.g. Amaral, 2013; Chung and Tanchoco, 2010; Heragu and Kusiak, 1988; Zhang and Murray, 2012), which is originally motivated by the arrangement of machines in a manufacturing system. In the DRLP, the facilities are placed on both sides of the corridor but the upper and lower arrangements do not have to start from a common point; and some space may be allowed between adjacent facilities.Amaral (2012) presented a mixed integer programming (MIP) formulation of the CAP. However, the CAP is NP-Hard, which limits the efficiency of an exact approach. For example, a CAP instance of size of n=15 could not be solved to optimality by cplex after 8.6hours of execution time. Therefore, there is a quest for heuristic approaches that can efficiently deal with large instances of the problem. In this regard, Amaral (2012) proposed a metaheuristic algorithm for the CAP and tested instances of size n=30 facilities.A recent working paper by Ghosh and Kothari (2012) develops a hybrid genetic algorithm and a scatter search algorithm with path-relinking for the CAP. Another recent working paper by Hungerländer and Anjos (2012) studies the CAP and presents lower bounds using semidefinite optimization and a heuristic to obtain feasible layouts from the solutions of the semidefinite programming (SDP) relaxations.The CAP is important from a practical point of view. It has applications such as the arrangement of rooms in office buildings, hospitals, shopping centers or schools (Amaral, 2012). Other applications include the layout design problem where the facility is divided into bays, which are arranged along a central spine. This spine configuration is a common design for the layout in semiconductor wafer fabrication facilities (Yang and Peters, 1997).In this paper, tabu search and simulated annealing algorithms are implemented for the CAP. An adequate neighborhood structure and a local search procedure, which are exploited by the two algorithms, are explained in the next section. Then, Section 3 presents the elements of the tabu search implementation such as the tabu status mechanism, aspiration criterion and diversification. Section 4 describes the simulated annealing implementation and its constituents: the classical simulated annealing procedure and the reversed simulated annealing procedure, which starts with a low temperature and increases it. In Section 5, the tabu search and simulated annealing algorithms are evaluated on some instances with n⩽15 taken from Simmons (1969), Amaral (2006) and Amaral (2012); and on some others having n=30 facilities from Anjos and Vannelli (2008). In addition, the algorithms are evaluated on even larger instances: thirty-six randomly generated instances having n=60 and a selected set of “sko” instances with sizes 42, 49, 56 and 64 of Anjos and Yen (2009); and two AKV instances of sizes 60 and 70 of Anjos et al. (2005).The tabu search and simulated annealing implementations both exploit the same neighborhood structure described in Section 2.1. Similarly, both implementations contain calls to the local search procedure explained in Section 2.2.The neighborhood structure used in this work is defined by two sets of swap moves: set C of column swaps and set E of row element swaps. The swaps are applied to a 2×n matrix representation of an n-facility arrangement, in which a non-zero value of a row element indicates the position of a facility on the corresponding side of the corridor. A zero value means that the facility is not present at that side of the corridor. That is, an arrangement is a matrix A=(aij) with elements(1)aij=k,iffacilityjisatpositionkonsidei,0,iffacilityjisnotonsidei,where i=1,2; j=1,2,…,n; and k∈{1,2,…,n}.For example, if n=5, the matrix(2)A=0201030102represents an arrangement, in which facilities 2 and 4 (cf. the elements of the first row at columns 2 and 4) are on one side of the corridor in the order 4 and 2, while the order of facilities on the other side of the corridor is 3, 5 and 1.A column swap is defined as an exchange of two columns in the matrix. This kind of swap may result in changing facility order only on one side of the corridor or in moving a facility from one side of the corridor to the other. For example, the swap of columns 2 and 4 in (2) puts facility 2 as the first and facility 4 as the second facility on the upper side of the corridor, and the swap of columns 1 and 2 modifies the arrangements on both sides of the corridor placing facility 1 as the second facility on the upper side of the corridor and facility 2 as the third facility on the lower side of the corridor.A row element swap exchanges elements of a given column and associates a given position to the new non-zero element. For example in (2), a swap of the elements in the third column with a given new position, say 3, for the non-zero element changes facility 3 to be the third facility on the upper side of the corridor. If the new given position of the non-zero element happens to be before other positions on the new side, an update of the succeeding positions may be needed. For example, a swap of the elements in the third column with the given new position for the non-zero element being 1 means that facility 3 should become the first facility on the upper side of the corridor; then, the positions of facilities 4 and 2 need to be updated to be 2 and 3, respectively. Similarly the positions on the side from which a facility was removed must be adjusted. In our example this means assigning facility 5 to position 1 and facility 1 to position 2.The number of column swaps is equal to n(n−1)/2. The number of row element swaps will depend on the current matrix, if all possible destination positions are allowed to the swap element. In our implementation only the last position is allowed. The number of row element swaps is, in this case, equal to n.Another common element to the tabu search and simulated annealing implementations consists of calls to a local search procedure called Procedure 1.By which the current solution S with cost c(S) is iteratively improved by application of both column and row element swaps as long as this is possible.Procedure 1localSearchinput: Solution S, array Swapsoutput: Best solution SBestNow1STest←S; SNow←S2SBestNow←S; minVal←c(S);3repeat4improved←FALSE;5fori=0 toSwaps.lengthdo6STest← applySwap(Swaps[i], SNow);7ifc(STest)<minValthen8SNow←STest;9SBestNow←SNow;10minVal←c(SNow);11improved←TRUE;12end13end14untilimproved=FALSE;The array Swaps in the input of Procedure 1 consists of the two types of swaps (column swaps and row element swaps). A column swap is represented by a pair of integers indicating the relevant columns to be swapped; and a row element swap by a single integer giving the index of the column, the elements of which are to be interchanged. The array Swaps is exemplified in Fig. 1for n=4.In Procedure 1, each swap is applied at line 6 in the inner loop spanning lines 5–12. If a swap produces a solution with a lower cost, this solution is accepted as a new current solution at line 8 and the boolean value improved is set to TRUE at line 11. The inner loop is repeated until no better solution is found.The use of Procedure 1 can be considered as an intensification strategy within the tabu search and simulated annealing implementations.In the following, we describe our implementation of the tabu search algorithm. The reader not familiar with tabu search is referred to Glover and Laguna (1997). The central part of our tabu search implementation consists of generating new solutions in the current solution’s neighborhood determined by the swaps as defined in Section 2.1. Usually, only swaps declared as “non tabu” may be applied. The tabu status of a swap is simply determined on the basis of the recency of its former use as shown in Procedure 2.Procedure 2isNonTabuinput: integers swapRecency, iIter, tenureoutput: boolean nonTabu1nonTabu←iIter−swapRecency>tenureIn Procedure 2, the test at line 1 declares a swap with recency value swapRecency as “non tabu”, if the number of iterations elapsed between the last use of the swap and the current iteration is greater than the given tabu tenure value tenure.The neighborhood of the current solution is explored by Procedure 3, which returns a new candidate for updating the current solution. The number of available swaps tends to be large in case of larger problems and, thus, applying all of them will make the search very slow. Therefore, at a given iteration, it is reasonable to consider only swaps belonging to a limited subset. For this, an array of vectors called IndexVectors was created. Each vector IndexVectors[r] pointed out by IndexVectors consists of two parts: the first part is a subset Srof column swap indices and the second part contains all row element swap indices. The subsets Srare obtained as follows: Set the number of index vectors (i.e. the size of array IndexVectors) equal to a parameter value NIndexVectors; Set the first subset (S0) equal to {0, NIndexVectors, 2*NIndexVectors, 3*NIndexVectors, … }, the second subset (S1) equal to {1, NIndexVectors+1, 2*NIndexVectors+1, 3*NIndexVectors+1, … }, the third subset (S2) equal to {2, NIndexVectors+2, 2*NIndexVectors+2, 3*NIndexVectors+2, … } and so on. Clearly, {Sr}0⩽r<NIndexVectorsform a partition of the set of all column swap indices. The array of vectors IndexVectors holds NIndexVectors vectors of indices and each vector of indices can hold n+⌈n(n−1)/(2*NIndexVectors)⌉ indices.As an example, suppose that n=6. Then, there are n(n−1)/2=15 column swap indices: 0,1,…,14; and there are n=6 row element swap indices: 15,16,…,20; totalling the n+n(n−1)/2 swap indices exemplified in Fig. 1. If NIndexVectors=4, then S1={0,4,8,12}, S2={1,5,9,13}, S3={2,6,10,14}, S4={3,7,11} (see Fig. 2).The construction of IndexVectors makes it possible that only swaps whose indices belong to IndexVectors[r] be considered at a given iteration.Procedure 3pickNextSwapinput: Current solution S, arrays IndexVectorNow, Swaps, SwapFrequencies and SwapRecencies, integers iIter, tenure, minValueoutput: Swap index iResult1SBest←null;2iBest←−1;3minValueNow←minValue;4minModifValueNow←Integer.MAX_VALUE;5foriinIndexVectorNowdo6STest←applySwap (Swaps[i], S);7ifc(STest)<minValueNowthen8SBest←STest;9minValueNow←c(STest);10iBest←i;11else ifisNonTabu(SwapRecencies[i], iIter, tenure) then12f←SwapFrequencies[i];13modifCost←(1+f/(1+f))*c(STest);14ifmodifCost<minModifValueNowthen15minModifValueNow←modifCost;16SResult←STest;17iResult←i;18end19end20ifiBest⩾0 then21iResult←iBest;22endProcedure 3 takes as an input the array Swaps, which consists of two parts: all possible column swaps and all possible row element swaps (Fig. 1). As discussed in Section 2.1, the size of this array is equal to n(n−1)/2+n. Also taken as an input is an array of swap indices called IndexVectorNow. Each swap with index in the current IndexVectorNow is applied to solution S at line 6. If its application leads to the smallest cost found until so far, the result is exceptionally accepted even in the case of a tabu swap (lines 8–10). This implements the well-known aspiration criterion of tabu search. If no aspiration takes place, the swap’s tabu status is tested at line 11 and the modified cost of the swap result is calculated at lines 12 and 13. The result is accepted, if the modified cost is smaller than the minimum modified cost found by any swap until so far (line 14). The modified cost instead of the real cost is used here for encouraging less frequently applied swaps. For example, at line 13, if swap frequency for swap i is 2 then modifCost is 1.67*c(STest), whereas if the frequency is 20 then modifCost=1.95*c(STest). Thus, the smaller value of modifCost of the swap with frequency 2 makes that swap more interesting than the swap with frequency 20. If aspiration has really taken place, then the index of the corresponding swap is set to be the output of the procedure at line 21.In addition to the proper tabu status mechanism, simple diversification steps described in Procedure 4 are applied for trying to escape from local minima.Procedure 4diversifyinput: Solution SNow, array Swaps, boolean useRecency, integer NDivStepsoutput: Solution S1S←SNow;2ifuseRecencythen3SwapsNow←select NDivSteps least recent swaps from Swaps;4else5SwapsNow←select NDivSteps least frequent swaps from Swaps;6end7fori=0 toNDivStepsdo8S← applySwap (SwapsNow[i], S);9endThe input variable useRecency determines whether to use the least recent or the least frequent swaps for diversification (lines 2–4). In both cases, NDivSteps swaps are applied to the given solution as shown in the loop at lines 7–9. The result of this sequential application is then given as the output of the procedure.Algorithm 1 gives the TS algorithm implemented in this work.Algorithm 1tabuSearchinput: Initial solution S0, array IndexVectors, double TabuFactor, integers MinTabuTenure, MaxTabuTenure, DivInterval, NMaxIters, NMaxNotImproved, NDivStepsoutput: Best solution SBest1Create array Swaps;2fori=0 toSwaps.lengthdo3SwapFrequencies[i]←0;4SwapRecencies[i]←−MaxTabuTenure;5end6NIndexVectors← array length of IndexVectors;7iIndexVector←−1; tenure←MaxTabuTenure; minValue←c(S0);8S←S0;9S′← localSearch (S);10ifc(S′)<minValuethen11S←S′minValue←c(S′); SBest←S′;12end13iIter←0; iNotImproved←0; useRecency ← TRUE;14whileiIter<NMaxItersandiNotImproved<NMaxNotImproveddo15iIter←iIter+1;16iIndexVector←iIndexVector+1mod NIndexVectors IndexVectorNow←IndexVectors[iIndexVector];17i← pickNextSwap (S, IndexVectorNow, …);18SwapFrequencies[i]←SwapFrequencies[i]+1;19SwapRecencies[i]←iIter;20S←applySwap (Swaps[i], S);21ifc(S)<minValue minValue←c(S);22SBest←S; iNotImproved←0;23S′← localSearch (S);24ifc(S′)<minValuethen25S←S′; minValue←c(S′); SBest←S′;26end27else28iNotImproved←iNotImproved+1;29ifiNotImproved mod DivInterval=0 then30S← diversify (S, Swaps, useRecency, NDivSteps);31useRecency←notuseRecency;32end33end34tenure← round (TabuFactor∗tenure);35iftenure<MinTabuTenurethen36tenure←MaxTabuTenure;37end38endIn Algorithm 1, the array of swaps is created at line 1 and the arrays of swap frequencies and recencies are initialized at lines 3 and 4. The current solution S is set to the initial solution S0 and the current tabu tenure value is selected to be its maximal value at line 7. Before the main iteration loop at lines 14–38 starts, an improvement of the current solution is tried with the help of Procedure 1 (lines 9–12). Similarly, Procedure 1 is called each time a new globally best solution has been found at line 23. At each iteration, a swap is selected via a call to Procedure 3 and applied to the current solution S. If the number of iterations without improvement reaches the value of parameter DivInterval, the diversification Procedure 4 is called at line 30. The value of tabu tenure is decreased at each iteration at line 34. If the value of tabu tenure becomes smaller than the given minimum value, it is shifted back to its maximum value (lines 35 and 36).Like our tabu search implementation, the simulated annealing algorithm (SA) implemented here, given as Algorithm 2, uses the neighborhood structure explained in Section 2.1. The heart of our simulated annealing implementation is Procedure 5, which, in fact, is the classical simulated annealing procedure presented in the literature (see, for example, Laarhoven and Aarts, 1987).Procedure 5basicSAinput: Initial solution S0, array Swaps, doubles T0, TempFactor, FinalTemp, integer NInnerLoopItersoutput: Solution S1S←S0, T←T0;2whileT>FinalTempdo3iIter←0;4whileiIter<NInnerLoopItersdo5iRandom←floor(random(0,1)∗Swaps.length);6S′←applySwap (Swaps[iRandom], S);7Δ←c(S′)−c(S);8prob←min(1, e−Δ/T);9ifrandom(0,1)⩽probthen10S←S′;11end12iIter←iIter+1;13end14T←TempFactor*T;15endThe probabilistic acceptance criterion at line 8 distinguishes the SA process from the pure local search, since also solutions with lower quality may be accepted. The acceptance probability is controlled by the temperature parameter T, the value of which is slowly decreased during the iteration. In our implementation this takes place at line 14 in the form of multiplication by the parameter value TempFactor.The initial temperature value, T0, is determined by Procedure 6, which starts with a low temperature and increases it until all solutions generated in the inner loop will be accepted.Procedure 6reversedSAinput: Initial solution S0, array Swaps, doubles T0, ReversedTempFactor, integer NInnerLoopItersoutput: Temperature T1S←S0, T←T0;2nAccepted←0;3whilenAccepted<NInnerLoopItersdo4iIter←0, nAccepted←0;5whileiIter<NInnerLoopItersdo6iRandom←floor(random(0,1)∗Swaps.length);7S′←applySwap (Swaps[iRandom], S);8Δ←c(S′)−c(S);9prob←min(1, e−Δ/T);10ifrandom(0,1)⩽probthen11S←S′;12nAccepted←nAccepted+1;13end14iIter←iIter+1;15end16T←ReversedTempFactor∗T;17endProcedure 6 differs from Procedure 5 in the outer loop stopping criterion (line 3) and in the use of temperature growth factor ReversedTempFactor, which has a value greater than one. In addition, the number of accepted solutions is counted at line 12 and the result returned is the current temperature instead of the new current solution.Algorithm 2Simulated Annealing with Restartsinput: Initial solution S0, integer NRestarts, double InitialReversedTempoutput: Best solution SBest1Create array Swaps;2SBest←S0, minValue←c(S0);3S′←localSearch (S0);4ifc(S′)<minValuethen5SBest←S′, minValue←c(S′);6S0←S′;7end 8T0←reversedSA(S0, InitialReversedTemp, …);9fori=0 toNRestartsdo10S←S0;11S′←basicSA(S, Swaps, T0, …);12S′←localSearch (S′);13ifc(S′)<minValuethen14SBest←S′, minValue←c(S′);15end16T0←T0/2;17endThe SA algorithm implemented here is given as Algorithm 2. Again, the local search procedure is called at the beginning of the search (line 3). It is also called after each application of Procedure 5 at line 12. The initial temperature is determined by Procedure 6 at line 8 and decreased after each restart at line 16. The initial solution S0 (possibly updated at line 6) is reused at each restart (line 10).The tabu search and simulated annealing algorithms were implemented in Java and tested on JVM 1.6. The algorithms were run on an Intel(R) Core(TM)2 Quad CPU (P9550) with 2.83gigahertz and 4gigabytes RAM under the Linux operating system.The performance of the algorithms was evaluated on several benchmark instances. (The majority of the instances used here to test the CAP were originally used in the literature to test the SRFLP and are available from the SRFLP benchmark instances at http://www.gerad.ca/files/Sites/Anjos/flplib.html.) For each instance each algorithm is run 30 times. Throughout the experiments, the following parameter values were used for TS and SA, respectively:TS: NMaxIters=100,000, NMaxNotImproved=50,000; DivInterval=12,500, NDivSteps=50, MinTabuTenure=n/8, MaxTabuTenure=2×MinTabuTenure, TabuFactor=0.995; NIndexVectors=max{1, ⌊ln(n(n−1)/2)⌋−4}.The expression of NIndexVectors consists of a slowly growing logarithmic function and an adjustment value equal to 4. Thus, the values of NIndexVectors are equal to 1 for 1⩽n⩽28, 2 for 29⩽n⩽47, and 3 for 48⩽n⩽77.SA: ReversedTempFactor=1−1/n2, TempFactor=1−1/(n2log(n)), FinalTemp=0.1/n, NInnerLoopIters=n, NRestarts=4.Initially, the algorithms were tested on a number of instances of the literature having n⩽15:•the largest instances of Simmons (1969) with 9⩽n⩽11;two instances with n=12 and two with n=13 of Amaral (2012);one instance of Amaral (2006) with n=15;Optimal solutions are known only for the instances with n⩽13 (Amaral, 2012).Table 1shows, for each problem instance, the best (Min.), average (Avg.), worst (Max.) and standard deviation (SD) for objective values, number of function evaluations and CPU times relative to SA, over 30 runs. Table 2shows similar figures for TS. For a given instance, the average CPU time provided in either table estimates how long a single run of the corresponding algorithm takes on average. If desired one could determine the total CPU time for all 30 runs together by multiplying the average CPU time by 30.From Tables 1 and 2 it can be observed that the tabu search and simulated annealing algorithms easily found the optimal or best-known solution of the instances in every run out of 30 runs. The best layouts found by SA for these instances are shown in Table C.1 in Appendix C.Next the performance of the algorithms was evaluated using five instances with n=30 of Anjos and Vannelli (2008). The best-known CAP solutions for these instances are given by Amaral (2012).The results of these experiments are shown in Tables 3,4. For each problem instance, the best solution value reached by TS matches that by SA. The average and the worst values are close to the best values for either SA or TS, with a little advantage for TS in this aspect. However, the average CPU time required by TS is larger than that required by SA.For each instance, the best solution values of SA and TS agree with the best solution values obtained by the heuristic in Amaral (2012). Table C.2 in Appendix C displays the best layouts found by SA for the instances with n=30.In this subsection, we experiment with a selected set of sko instances with sizes 42, 49, 56 and 64 of Anjos and Yen (2009); and with two AKV instances of sizes 60 and 70 of Anjos et al. (2005). Since the two recent working papers by Hungerländer and Anjos (2012) and Ghosh and Kothari (2012) report computational experience with these instances, we can make comparisons on the same instances allowing a better evaluation of SA and TS.The method of Hungerländer and Anjos (2012) is referred to here as SDP’. As for Ghosh and Kothari (2012), their genetic algorithm incorporating local search is referred to as CAP-GA and their scatter search algorithm with path relinking as CAP-SS.Table 5shows a comparison of solution values obtained by the different methods. Values in bold-face indicate the best found value for an instance. It can be seen that the first four methods in the table reach the same value for instance sko42_01. TS matches the best found value for instance sko49_01 and CAP-SS matches the best found value for instance sko49_03. However, for all of the instances, SA gives the best found value.Hungerländer and Anjos (2012) report that the time of SDP’ is the average computational time over 10 runs using a SDP relaxation, which they call (SDPcheap), and the bundle method on an Intel Xeon E5160 (Dual-Core) with 24gigabytes RAM, running Debian 5.0 in 64-bit mode. The algorithm was implemented in Matlab 7.7. Ghosh and Kothari (2012) report that the algorithms CAP-GA and CAP-SS were coded in C, and compiled using the gcc 4.5.1 compiler on a machine with an Intel Core i5 2400 processor at 3.1gigahertz running Window 7. The time of CAP-GA is obtained by running the genetic Algorithm 200 times. The time of CAP-SS is from a single scatter search run incorporating path relinking.Due to differences in machines, it is difficult to compare the computational times of the different algorithms. However, computational times are presented in Table 6just to give an idea of their magnitude.Detailed results for TS and SA on AKV/sko instances are presented in Tables A.1 and A.2 in Appendix A. The best layouts found by SA for these AKV/sko instances are presented in Table C.3 in Appendix C.In addition, the performance of the tabu search and simulated annealing algorithms was evaluated on new randomly generated instances with n=60. The new instances were generated as follows. The set of facilities contains s facilities with short lengths, i.e. lengths uniformly distributed in [1,10]; and (n−s) facilities with long lengths, i.e lengths uniformly distributed in [10,20]. Each cijhas a probabilityp100to be positive (i.e. a probability1-p100to be zero), with each positive cijuniformly distributed in [1,10].Then, for each s∈{30,40,50,60} and for each p∈{30,60,90}, we generate three instances with the characteristic {n,s, p}. In this way we obtain a total of 36 instances.In these experiments it was observed that, for each instance, the best solution value produced by SA (SAMIN) was smaller than that produced by TS (TSMIN). The percent deviation (% dev.) between TSMINand SAMINis given by (100(TSMIN−SAMIN)/SAMIN). Table 7shows the best solution value obtained for SA and TS as well as the percent deviation (% dev.) averaged over the 3 instances with the same characteristic {n, s, p}. According to the table, the averaged percent deviations seem to decrease as p increases. The best solution values for TS do not deviate much from the ones obtained for SA and the largest of the average deviations is only 0.08%. The best layouts found by the SA algorithm are given in Appendix C (Tables C.4–C.6).Detailed results for TS and SA are presented in Tables B.1–B.6 in Appendix B. In each of these tables, the standard deviation of objective values are relatively small. For p=30 their average is 101.8 for SA and 191 for TS; for p=60 their average is 100.2 for SA and 201.2 for TS; and for p=90 their average is 122.8 for SA and 136.9 for TS.It can be seen that, for each instance, the average number of function evaluations for TS is larger than for SA. Note that within 30 runs of either algorithm on a given instance, the number of function evaluations required is not the same. Grossly speaking, in 30 runs relative to some instance, SA used between 18,000 and 20,000 function evaluations with the average around 18,500; while, TS used between 32,000 and 65,000 with the average around 43,000. Tables B.1–B.6 show that the values for standard deviations of number of function evaluations for TS are much higher than for SA.The SA implementation presented a better performance than the TS implementation. Although TS presented a competitive performance, SA obtained slightly better solutions in a less amount of time. Thus, the SA implementation presented here should be a very good option for solving large instances of the CAP.

@&#CONCLUSIONS@&#
In this paper, both tabu search and simulated annealing algorithms were implemented for the Corridor Allocation Problem (CAP). An adequate neighborhood structure was presented to be exploited by the two algorithms. The algorithms were evaluated on several instances from the literature with n⩽30, reaching the optimal (when available) or best-known solutions for these instances. Moreover, the algorithms were further evaluated on even larger instances with size n=60 facilities. The simulated annealing implementation presented a better performance obtaining smaller objective function values and requiring a smaller number of function evaluations.The good performance of SA was also evident in a comparison with other methods on additional instances from the literature (selected AKV/sko instances). In this comparison, SA obtained the best found value for every instance.Future research should evaluate other metaheuristics for the problem in order to improve the results presented here.