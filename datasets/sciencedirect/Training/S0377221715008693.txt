@&#MAIN-TITLE@&#
Experimental behavioural research in operational research: What we know and what we might come to know

@&#HIGHLIGHTS@&#
A selective review of findings from research experiments in behavioural OR is presented.This is used to determine current knowledge about the creation and use of models.Methods for future experiments are suggested.

@&#KEYPHRASES@&#
Behavioural OR,Decision processes,Decision Support Systems,

@&#ABSTRACT@&#
There is a long standing, but thin, stream of experimental behavioural research into understanding how modellers within operational research (OR) behave when constructing models, and how individuals use such models to make decisions. Such research aims to better understand the modelling process, using empirical studies to construct a body of knowledge. Drawing upon this research, and experimental behavioural research in associated research areas, this paper aims to summarise the current body of knowledge. It suggests that we have some experimentally generated findings concerning the construction of models, model usage, the impact of model visualisation, and the effect (or lack thereof) of cognitive style on decision quality. The paper also considers how experiments have been operationalised, and particularly the problem of the dependent variable in such research (that is, what beneficial outputs can be measured in an experiment). It concludes with a consideration of what we might come to know through future experimental behavioural research, suggesting a more inclusive approach to experimental design.

@&#INTRODUCTION@&#
Behavioural research is the empirical study of the formation of decisions, choices and habits in human behaviour. Typically empirical, most behavioural research involves the identification of variables associated with behaviours, their measurement, and conclusions regarding the interaction between the variables and specific behaviours. Subjects, typically under experimental (normally laboratory) conditions, or some form of quasi-experimental conditions, perform one or more tasks, which give rise to the measured variables.The use of experimental behavioural research to better understand how modellers within operational research (OR) behave when constructing models, and how individuals use such models to make decisions, has a reasonably long history. Nevertheless, it is still a fairly thin and underdeveloped area of research. The first use of the phrase “behavioural research” to describe experimental research activities in OR is probably Bell and O'Keefe's (1992) work on the use of visual simulation models. However, experimental research in OR tends not to use the phrase “behavioural”. This is perhaps because such research has focused on specifically contributing to what we, the OR research community, knows about model creation and use, rather than contributing to a broader understanding of behaviour. Experimental methods and measurements have been appropriated as convenient so as to progress our knowledge of OR. In a search for actionable knowledge, experimental research in OR has been less rigorous than elsewhere, but more concerned with external validity. Thus much of the research in OR can be considered as quasi-experimental, giving up experimental laboratory control so as to use realistic (perhaps expert) subjects (e.g., Willemain, 1995) or actual decision situations as tasks (see Adelman, 1991).Within Information Systems (IS), there is a much better developed stream of experimental behavioural research dating back to Mason and Mitroff (1973). Their definition of an IS (“a person of a certain psychological type who faces a problem within some organizational context for which he needs evidence to arrive at a solution, where the evidence is made available through some mode of presentation”) placed individual behaviour at the heart of IS research. With the emergence of Decision Support Systems (DSS), researchers (e.g., Benbasat & Todd, 1996) have investigated the role of individual behaviours quite extensively, and much of this research is relevant to OR. Similarly, researchers in Decision Analysis (DA) have for a long time been aware of the behavioural aspects of decision making, and DA research has been intertwined with behavioural research since Simon (1959). An experimental behavioural tradition has also emerged in Operations Management (OM) (Moritz, Hill, & Donohue, 2013).The objectives of this paper are threefold. First, to better understand the current body of knowledge that emerges from experimental behaviour work, including our understanding of how to operationalise experiments. Second, to use this knowledge, and other knowledge collected from adjacent academic areas such as IS, DA and OM, to suggest what knowledge could in the future be generated from experimental work. Third, to suggest how to better operationalise our future experiments.This paper specifically covers experiments concerned with the creation and use of models by individuals. It does not initially address studies that focus on just problem structuring (such as Massey & O'Keefe, 1993, or Franco & Meadows, 2007). However, the subsequent discussion broadens the paper to include experimental work with problem structuring, the role of groups, and approaches to operationalisation such as natural experiments, as appropriate. This allows for a range of conclusions that go beyond the scope of model creation and use by individuals, and beyond common experimental methods.The next section gives a background to experimental methods in OR; that is, how researchers have to this point addressed experimenting with model creation and use. Some key concepts concerning experimentation are introduced, followed by an introduction to both cognitive style and cognitive biases. The difficulties in determining the dependent variable are considered. There follows a selective review of the results of experimental behavioural research in OR, and associated areas, which allows for a summary of what is now known. What we might come to know, and how we should approach experimentation in the future to so as to make this happen, is then discussed.Experimental behavioural research aims to uncover and describe causal relationships between variables (Shadish, Cook, & Campbell, 2002). (Ideally it should explain such relationships, but the ability of experimental methods and statistics to explain why things happen, rather than how, is limited.) Researchers typically measure a number of variables, and their causal link to one or more dependent variables (DV). Measured variables can either be manipulated (such as exposure to a particular graphics display, or variations in the data being used) or non-manipulatable (such as the age or gender of subject) (Shadish et al., 2002). Both manipulated and non-manipulatable variables can be chosen as independent variables (IVs) for analysis. However, they might be confounding variables that are causally linked to one or more DVs and other manipulated or non-manipulatable variables.Most experimental research has reasonably straightforward research objectives. Bell and O'Keefe (1995) is a useful example, stating that the research questions for their study of visual interactive simulation are:1.“How good (or bad) is the performance of users when experimenting with a visual interactive simulation?What is the value of animation and graphics?Is the process by which the user experiments with the visual interactive simulation important?”Question 1 sets-up a very typical experimental design: users will experiment with a model. Their performance will somehow be evaluated as “good (or bad)”. Thus “performance” is established as a dependent variable (DV), and characteristics of the users might be established as non-manipulatable variables. Question 2 suggests that “animation and graphics” will lead to other variables. Potentially all subjects could use the same animation, and in some way evaluate its “value”; this gives rise to a DV concerned with “value”. Alternatively, the experiment could include a treatment (e.g., an animation versus a graphical display) such that DVs can be related to a manipulated variable representing the treatment. Question 3 is an example of a process question. It may lead to one or more DVs that capture the process (for example, the time to complete a task), but might also lead to some detailed protocol analysis of subject behaviour. (In actuality Bell and O'Keefe (1995) used a non-manipulatable confounding variable as a surrogate measure for “process”.) Thus, these three questions suggest an experimental design, and cover both output and process measures.Our research questions may lead us to establish experiments where manipulated variables have an influence on one of more DVs through one or more moderating variables, which allows us to observe an interaction effect. For example, user modelling experience (a non-manipulatable variable) might produce a performance (a DV) that is moderated by (say) the time allowed to complete a task. Typically, however, and certainly in many of the studies considered below, variables are considered as simply manipulative, non-manipulatable or dependent, with limited or no moderating effects.In typical randomised experiments, subjects are assigned to treatments at random. Such subjects are selected from a realistic population so as to aid external validity. In quasi-experiments, subjects are not always selected or assigned at random. The researcher may choose to do this so as to introduce variables such as prior experience with modelling into the experimental design, comparing (say) an experienced group versus a non-experienced group. In this way, what might appear a non-manipulatable variable can become artificially manipulated. However, the potential for confounding variables coming into play then becomes considerable – for example, if the experienced group is older, or more educated, are observed differences in the DV due to experience with modelling, or age, or education?Research designs often can include antecedent measures. These are non-manipulatable variables such as age, gender or education level that can be collected for all subjects (and not artificially manipulated as in the example above). They can be established to be IVs as part of the formal statistical analysis, but are often used simply to demonstrate that the subjects present as a sample that is realistic for the experiment. For example, an experiment in model creation that used subjects with limited education and no prior knowledge of modelling would not have much external validity.What makes experimental behavioural research in OR different from much experimental work elsewhere is the creation and use of a model. The model is typically at the heart of any experimental research endeavour in OR. The model is something that can be manipulated, i.e., altered so that different tasks and/or different subjects are matched to variations of the model under differing experimental design conditions.Simplistically, experimental researchers in OR have concerned themselves with either the creation of a model (e.g., Willemain, 1994, 1995), or the use of an existing model (e.g., Bell & O'Keefe, 1995), or occasionally both (Monks, Robinson, & Kotiadis, 2014). With model creation, experiments have investigated the creation of a model by different subjects. The quality of the model produced, however measured, can be operationalised as the DV. Alternatively, with model use, the model can be provided to users so that variations in the quality of performance with the model can be observed and measured. Under a design with treatments (where the researcher manipulates an IV), users might be given access to variations of a model with differing structures (even possibly using different techniques), or different outputs (such as different animations).As will be seen later, researchers looking at model use within the OR, IS and DA communities have tended to leave the underlying structure of a model intact, but have manipulated either the visualisation emanating from the model, or the data used by the model. Thus, for example, a single model might be constructed with n different displays (the display is thus the treatment), and users acting as decision makers interact with one of the n displays (each display is a factor). Researchers then look for significant differences in DVs across the n factors. Alternatively, all subjects could have access to all n displays, and researchers look for the relationship between subjects’ preferences for each display, and performance with the model.Much experimental research aims to explore the role of individual differences across subjects, or the varied biases and decision styles that individuals bring to the experiment. This is where cognitive style and cognitive biases come to be operationalised.There is some confusion between cognitive style and cognitive biases, but the two are distinct. Cognitive style “refers to differences that can be observed in individuals owing to their inherent cognitive structures” (O'Keefe, 1989); it represents “consistent individual differences in how individuals perceive, think, solve problems, learn, take decisions and relate to others” (Armstrong, Cools, & Sadler‐Smith, 2012). These are assumed to exist irrespective of context, and to be largely unchanged within individuals over time. The Myers-Brigs Type Indicator (MBTI) is the dominant measurement instrument for cognitive style, although it must be noted that this is only one instrument, and is based upon a particular view of individual differences. Nevertheless, research has consistently shown that decision making processes can differ by MBTI types, for example, Hunt, Krzystofiak, Meindl, and Yousry (1989) and Hough and Ogilvie (2005).Cognitive biases are “cognitions or mental behaviours that prejudice decision quality in a significant number of decisions for a significant number of people; they are inherent in human reasoning” (Arnott, 2006, who provides an excellent review of cognitive biases within the context of decision support). Biases result in individuals deviating from fully rational decision making. Since the seminal work of Tversky and Kahneman (1974), numerous biases have been researched, identified, and recorded. Arnott (2006) lists 37, of which many are relevant to creators and users of OR models. (For example, sampling bias, where the size of a sample is ignored when considering its predictive power; order bias, where presentation order is given weight in making judgements; and attenuation bias, where situations are simplified by discounting uncertainty.) Simply put, cognitive style is part of what determines why and how we behave differently; cognitive bias is what makes many of us behaviour the same.From a behavioural perspective, the cognitive style of a modeller or user may have a direct effect on model creation (Franco & Meadows, 2007), or how the model is used within decision making (Henderson & Nutt, 1980). Alternatively, the effect may be moderated through model creation or model usage. Thus, strictly speaking, cognitive style is not only a non-manipulative variable, it is a confounding variable related to both IVs and DVs. That is, it can confuse the analysis between manipulated variables and DVs, as it differentiates across subjects. Most studies, however, simply treat it as an IV (see Monks et al., 2014, for an example).Within experimental research we thus typically measure the cognitive style of all subjects, and see how this relates to other variables. Similarly, propensity to a cognitive bias can be measured, but more interestingly we might vary the model or task such that we understand how a bias can be moderated or even removed from the processes of modelling or decision making (for example, Hämäläinen, Luoma, & Saarinen, 2013).What is a desired output from model creation, or use, or both? And how should this be measured within experiments? The identification of appropriate DVs can be challenging, and has been an active and important part of research in IS for some years (Delone, 2003).It must be recognised that a DV is typically a surrogate for a specific behaviour. Within OR, we are typically concerned with improving user decision making with a model, improving user learning or understanding through the model, improving the understanding of modellers, or perhaps increasing the efficiency of modellers and users. Thus, in broad categorical terms, DVs relate to either decision-making, learning/understanding, or efficiency. Within studies related to this paper and discussed later, example DVs are shown in Table 1, and categorised appropriately.Generally, across all three categories, DVs can be one of three types. The first is where absolute measures can be made to provide fully objective measures. For example, where researchers use a task for which a known or optimal solution exists, then a simple measurement of user accuracy can be constructed, and the variance of the mean accuracy can be calculated. Where no known solutions exist, relative performance between subjects can be measured. Time to perform a task can be easily measured, since this can be computer monitored and recorded as part of the experiment. A number of studies attempt to objectively measure the extent to which users engage with a model, which is listed in Table 1 as scope. A common absolute surrogate measure for scope is the number of alternatives investigated.The second type of DVs is where outcomes are judged against criteria constructed by researchers, and hence are subjective. So, for example, researchers might measure the quality of a solution by drawing upon assessments of completeness, scope, options considered, etc. This can be similarly done for learning, model understanding and model transparency. Robinson (2002) provides a discussion where various quality measures sit within overall quality concepts within simulation modelling.A third type is where measures are self-reported by subjects, a common device in much behavioural research. These are quite obviously very subjective, especially where subjects are given little guidance in how to construct their measure. Subjects could be asked to self-report their perceptions of learning or model understanding rather than researchers trying to assess it. One particularly useful and interesting self-reportable measure is confidence in the solution. It is also worth noting that the validity of self-reported measures can sometimes be improved by pre- and post-test measurement, i.e. we might aim to measure the increase in confidence in the solution from pre- to post-model usage.Our review of experimental research in OR is selective. It covers experimental work around model creation and use that has appeared in Management Science, Operations Research and the European Journal of Operational Research. Citations to work in these journals was then traced to other recognised OR journals, such as the Journal of the OR Society. Studies were included if they could be identified as experimental behavioural research, within an obvious experiment run under at least quasi-experimental conditions.The result is the studies identified in Table 2. Although a fairly thin set of papers emerges, it should be remembered that such research is time consuming, requires considerable numbers of subjects that have to be recruited, and is typically preceded by pilot studies that are often incomplete or flawed. Experimental research is not easy or quick, which perhaps accounts for the relatively slow stream of research over the years.Table 2, for each study, identifies if it is concerned with model creation, model use, or (in one case) both. The number of experiments reported is listed, as some papers cover multiple studies, sometimes with different treatments. IVs and moderating variables are listed, along with the treatments employed by the researchers, and the DVs. Note however, that these are presented using a common terminology (rather than the specific language in the papers), which in the case of the DVs draws upon Table 1.The majority of the experimental work in Table 2 is, unexpectedly, concerned with model use. Much of this is research into the visualisation of simulation models; visualisation is a key aspect of the model and task, and the visual display might be varied across treatments. An example of the later is O'Keefe and Pitt (1991), who provide subjects with multiple displays: a listing, a graphic and an animation. There is a marked preference for displays, with users' having a strong preference for either the animation or the graphic. Users' cognitive style is measured, using the MBTI, and there is some weak evidence that preference for display type can be partially explained by cognitive style. User accuracy was mixed, but users that performed well generally had higher confidence in their solution. Chau and Bell (1995) similarly present an experiment with three different simulation displays: a graphic, an animation, and a paired animation where users can run two simulations in parallel and view an animation of each. There is some weak evidence that user performance (measured by comparing solutions across users) and confidence are better when using the paired animation, and that the variance in solutions is reduced.Both O'Keefe and Pitt (1991) and Chau and Bell (1995) noted that investigation of more alternatives (called a scope measure in Table 1) by users does not necessarily produce a better solution. It appears that users with poor understanding of the problem attempt, although not always successfully, to improve their performance by undertaking lots of simulation runs. However, Bell and O'Keefe (1995) further investigated the behaviour of users through a detailed analysis of model interactions and found that users who obtained a correct solution investigated fewer alternatives and used fewer interactions than those obtaining incorrect solutions. This can be explained in part by process, with correct solutions more associated with an alternative-based approach, as opposed to an attribute-based approach (see Payne, 1976). Users, who look at various attributes of the model within each alternative, and move across alternatives, do better than those who look at alternatives within attributes. (In Bell & O'Keefe, users had to determine the number of trucks allocated to a task in a mining operation; those who considered multiple attributes such as time and cost within an alternative, and then changed their alternative, did better than those who focused on an attribute and considered measures of that single attribute for various alternatives, and then changed attribute.) This is consistent with findings in DA and IS, for example Jarvenpaa (1989).It is worth noting that in all of these three simulation studies (Bell & O'Keefe, 1995, Chau & Bell, 1995, O'Keefe & Pitt, 1991) user accuracy is relatively poor compared to either an optimal or a known solution. Bell and O'Keefe (1995), however, does demonstrate significant improvement by users over a solution they suggest prior to using the model. In a more recent study, Akpan and Brooks (2014) show that users of a 3D simulation animation achieve better understanding of a model than users of a 2D animation. In their study a between groups factorial design is used (so that different displays are made available to different subject groups) rather than measuring preference across all subjects. Cognitive style is measured, although not using the common MBTI, but it is shown to have no significant impact on performance.Willemain, Wallace, Fleischmann, Waisel, and Ganaway (2003) is a rare model use study in that the data used by the model is manipulated. In the second experiment they report, subjects interact with a model where the quality of the data available is varied within the experiment. The study demonstrates the ability of users to compensate for poorer quality data, and to achieve solutions of a similar quality irrespective of the data. (In a similar experiment using forecasting models, Willemain (1989) demonstrates the ability of users to make appropriate visual adjustments to less than optimal forecasts.)Hämäläinen et al. (2013) is a more recent set of experiments, and is unusual within OR in addressing a cognitive bias. Building on Cronin, Gonzalez, and Sterman (2009) they consider the bias of accumulation, where users have to understand how in-flow (or entrances) and out-flow (or exits) into a simple systems dynamics model can accumulate to give total numbers at a point in time. Whereas Cronin et al. (2009) present poor user performance with accumulation tasks, and suggest this as a cognitive bias, Hämäläinen et al. (2013) show that changes in the model usage (due to different prompting and questions to consider) can improve user performance. This is an extensive study which shows how modelling can help overcome cognitive biases. Sternman (1989) is similar and early study that uses systems dynamics more akin to recent research in OM. Compared to an estimated solution, it is shown across multiple experiments and subjects that users cannot perform better than a simple heuristic for a stock management problem.Monks et al. (2014) is a fascinating, although complex, study in that it combines the effects of model creation and model use. Subjects in three treatments either use an existing model, create a new model and experiment with it for a set time, or create a new model and experiment with it extensively. It is found that the latter group “identify and explore more new variables in experimentation” than do the other subjects. This group also exhibit a higher “attitude” measure towards understanding “resource utilisation in an emergency department”, i.e., the given task. This appears to be a variation of a confidence measure. Overall the results suggest that model usage alone may be problematic and limiting as the experience gained through model creation is not available for the user to draw upon.Experimental studies focusing entirely on modellers and model creation are even rarer than those based on model use. The work of Willemain (1994, 1995) is now a classic reference point for such work. Using protocol analysis, Willemain looked at how expert OR modellers create models. Transcripts of modelling tasks were coded to reflect five topics corresponding to stages in the OR process: problem context, model structure, realization, assessment and implementation. The study concludes that expert modellers spend a significant amount of time on topics other than model structure, while switching between structure and other topics such as assessment. Willemain (1995) “concludes that there was limited support for the notion of modeller-specific and problem-specific effects influencing the attention given to each topic; and there was support for the idea of individual modelling styles.” Tako (2014) repeats and extends the work of Willemain, working specifically with experienced simulation modellers.Tako and Robinson (2010) is a more recent experiment into expert modelling using protocol analysis, comparing discrete-event modellers versus system dynamics modellers. (In effect, subjects are assigned to a discrete-event or systems dynamics treatment, although comparisons are made difficult due to the variation in the task set to each treatment.) There is no attempt to define the overall quality of the model (more an implicit assumption that good modellers will produce models above a threshold such that analysing model construction is a valid research activity). The authors conclude that “[simulation] modellers focus significantly more on model coding and verification & validation, whereas [systems dynamics] modellers [focus] on conceptual modelling.”Across all of these studies (those focused on model creation and decision making) there are few efforts to draw in antecedent variables. Hämäläinen et al. (2013) and Akpan and Brooks (2014) collect data regarding age, gender, etc., but use it to assess the appropriateness of the subject group, rather than as part of the causal analysis.References within, and citations to, the papers in Table 2 were used to search out papers outside of OR that have similarly addressed model creation and use. It was found that researchers in DA and Organisation Behaviour have similarly used experimental methods to explore the use of decision aids (to use their prevailing terminology). Some such studies are concerned with model usage, where experiments typically compare the use of a decision-aid versus no aid, using a theoretical basis of cost-benefit, and are normally framed around a preferential choice problem (e.g., choosing a new car, choosing a project to invest in, etc.). This stream of research owes much to the work of Payne (1976; also see Payne, Bettman, & Johnson, 1993), who has consistently argued that users chose a decision strategy (such as elimination of alternatives, ordering by alternative or by attributes, or implicit utility measures) so as to minimise their cognitive effort (“costs”) while maximizing decision quality (“benefit”).Within this research stream, costs to the user include measures of time and effort; benefits include improved decision making, often measured as the number of alternatives investigated, the quality of the solution, etc. (using DVs similar to those listed in Table 1). Research often adds in constraints on the decision maker, such as time or limited data. Chu and Spires’ (2000, 2001) investigation of time constraints is a good example of such research, showing that (not surprisingly) “time-constrained decision makers process information faster, process less information, and use less rigorous decision strategies”. Note, however, that the model aspect in these experiments is often limited; typically a choice set is presented, where alternatives have associated attributes. Decision makers have the ability to (say) order or sort the choice set, or to eliminate alternatives. These are certainly not sophisticated OR models.Within IS, researchers have similarly explored the decision strategies adopted by users of Decision Support Systems (DSS). Todd and Benbasat (1991) is the classic work in this area; Benbasat and Todd (1996) is a good starting point for OR researchers. They have shown over multiple experiments that decision makers do not just attempt to maximise their decision quality: decision makers tend to adapt their strategy selection to the type of decision aids available in such a way as to reduce effort. From an OR perspective, time, effort and efficiency in decision making will have an impact on the decision made, and the quality of the decision might be moderated based upon these factors. Researchers in Marketing have run similar experiments, investigating the effect that decision aids have on the screening of alternatives, or the selection of alternatives. For example, Häubl and Trifts (2000) produce findings that are very similar to Benbasat and Todd's (1996) work, showing that consumers balance cost and effort. Focused and well-designed decision aids can support consumers in making better decisions with less effort.Operations Management (OM), in many ways a cousin of OR, has pursued an approach to behavioural research that is similar to behavioural economics. That is, experiments have investigated behaviour in fairly standard constrained contexts, typically looking at cognitive biases in decisions made with choice problems, and comparing performance with known optimal solutions or fully rational choices (Bendoly, Donohue, & Schultz, 2006). A good example is Moritz et al. (2013) on the newsvendor problem, where subjects “play” at being newsvendors that have to stock and sell newspapers. Results are compared to optimum solutions and also across subjects. The cognitive reflection construct of Frederick (2005) is used to measure subjects’ ability to quickly move their thinking beyond intuitive solutions, and it is found that “cognitive reflection is a better predictor of performance than college major, years of experience, and managerial position”.So given Table 2 and the above discussion, plus research in other areas such as IS, DA and OM, what do we know? Table 3 suggests a possible body of knowledge generated by experimental behavioural work in OR, and supported by research elsewhere. This is suggested, perhaps even just hypothesised, because the body of work is such that generalisation is difficult. As can be seen above, studies have not always built upon each other (except for authors building on their own work), and the lack of a conceptual model within most individual pieces of research, plus the selection of differing DVs, makes comparing research results difficult. Nevertheless, some insights do emerge; as well as many limitations to what we know.As Table 3 identifies, model users produce poorer solutions relative to known or optimal answers, but better solutions than those prior to their interaction with a model. This is consistent across studies in OR, IS (Benbasat & Todd, 1996) and OM (Moritz et al., 2013). This suggests that OR as a producer of optimal solutions, where possible, is of great value – but so is OR where solutions are not measureable as optimal, or perhaps even easily identifiable as “better”. It is tempting to conclude that we need both “hard” and “soft” OR, and that each can support decision makers. While probably fairly obvious to many practitioners, we can now present experimental work that very simply justifies this.Those who are typically confident in their solution have good reason to be so, as this indicates a better solution than where confidence is low. However, it should be noted that consistent overconfidence by decision makers (sometimes called post-decisional confidence) is itself a well-known cognitive bias (see Sniezek, Paese, & Switzer III, 1990). Moreover, decision makers are over confident when dealing with a complex issue – explaining why this effect is possibly more visible with research into OR models than into simple choice problems. This suggests a very strong role for experimental behavioural OR in understanding confidence and over-confidence when faced with complex problems.As found within studies in DA and IS (e.g., Jarvenpaa, 1989), an alternative-based approach to considering solutions appears to be more effective than other decision strategies. There is thus strong evidence that OR and DSS should provide systems that help users take an alternative-based approach to decision-making – considering attributes within alternatives rather than vice-versa.Perhaps the most interesting suggested finding with regard to model use concerns the efficiency of decision making with a model, and its link to effectiveness. DA research consistently demonstrates the cost-benefit effect, where decision makers balance the cost of using a decision aid versus their perceived quality of the solution (Payne et al., 1993). Within OR experiments (e.g., Bell & O'Keefe, 1995), subjects appear to be expending more time than others with the model because they do not understand the model structure as well as they might. They are, in effect, pursuing what is referred to as a compensatory strategy (Payne et al., 1993). Subjects use time with the model, such as more simulation runs, to overcome their limited knowledge. In effect, subject experience (whether measured or not) becomes a confounding variable, making the relationship between efficiency measures such as time, and various DVs, difficult to interpret.As to model creation, research paints a picture where expert modellers are different from novices, different from each other, and different across techniques. They can be good at recognising and dealing with data issues (Willemain et al., 2003). However, studies of model creation have been much looser in their design and execution than those concerned with model use. There is little attempt to bring in antecedent measures, and no consideration of confounding variables.While the knowledge suggested here is speculative, two findings are very consistent across studies. First, more appropriate visual displays lead to better decision making. There is clear evidence that animations, and possibly 3D animations, can aid model users more than less technically advanced displays (such as graphics). Studies such as Akpan and Brooks (2014) have modernised older work by Bell and his co-authors (e.g., Chau & Bell, 1995), showing that this finding holds for more modern higher quality and more detailed displays.Second, cognitive style appears to have no direct impact on decision quality in any of the experimental studies that have measured it. Where cognitive style has been measured, and typically treated as an IV, there is no significant causal relationship with any DV. This finding is consistent with current thinking in behavioural IS research (Davern, Shaft, & Te'eni, 2012), despite the continued research into cognitive style and decision making elsewhere (e.g., Hough & Ogilvie, 2005).In effect, it could probably be concluded that experimental research in OR has provided little strong evidence about model creation and use. Much of what is presented above is confirmed by experimental work in IS and OM, which could have been better used as a starting point for OR researchers. While Table 2 provides some causal descriptions, little has been provided to consider the underlying reasons for such description. For example, why are model users with better decisions more confident in their solution? Is this simply a post-decisional cognitive bias? Or is it a positive result of their decision strategies? Could it emanate from antecedent measures, such as education or experience? Experimental behavioural research in OR has provided few clues to the important why questions of causal relationships.So how can experimental research methods be used to better understand model creation and use? And can they be extended to research other aspects of the OR process? This section covers four things. Starting with suggestions on where Table 3 could be extended through current methods, research on groups, specifically problem structuring, is then considered. There follows a quick look at what is happening in IS, DA and OM, and then a consideration of how experimental methods in OR should be progressed.Few experiments in OR have looked at how modellers and model users deal with the vagaries of data, and the resulting model structure. Yet this is what dominates some descriptive work on the process of OR (for example, see Mitchell, 1993). OR is an interaction between models and data – and yet the assumption that data is always available and in the correct form can permeate OR research. More experiments such as Willemain et al. (2003) can shed light on how both modellers and model users deal with varied, confusing and missing data. Such experiments can draw upon, and learn from, work in DA.Few of the studies identified in Table 2 have considered the role of cognitive biases. Yet surely the role of the OR process is in part to overcome such biases, as shown in Hämäläinen et al. (2013)? This is a potentially fruitful area of study, and the linkages between biases, modelling and model use can be explored through experimentation. Hämäläinen et al. (2013) is an important paper because it manipulates the user interaction with the model to reduce the cognitive bias. If we knew how to do this across a range of models and context, then guidance could be given in how to make OR modelling more productive. And as noted above, post-decisional confidence is a particular interesting cognitive bias that could be better explored.While cognitive style has fallen out of favour in IS and DA research into decision strategies, dual processing theories have emerged as a vibrant area of research (see Witteman, van den Bercken, Claes, & Godoy, 2009). Such theories hypothesise that decision making can be intuitive or rational, and that individuals switch from “fast” intuitive decision making into “slow” logical and rational modes as and when necessary. Frederick's (2005) cognitive reflection is a simple measure of how well people can do this. As noted above, Moritz et al. (2013) have demonstrated dominance of this measure over education, experience and career position when evaluating performance against antecedent measures in the newsvendor's problem. Surely OR is designed in part to tap into the logical and rational? Hence we might expect that subjects who can more easily switch into slow thinking are better able to create and use models. But this needs testing in experiments, and experience with cognitive style does not lead us to believe that dual processing measures will provide the significant causal relationships that cognitive style has not.Many readers will have realised that the convenient limitation to focus on “the creation and use of models by individuals” excluded experimental work specifically concerned with problem structuring alone. Whereas some experimental research into problem structuring is concerned with the individual (e.g., Massey & O'Keefe, 1993), increasingly such work is concerned with how groups define problems and progress to these models (Tavella and Franco, 2015). Both the systems dynamics and soft-system communities have conducted studies into group problem formulation and model creation. Many of these studies are a long way removed from traditional quantitative experimental work, being based around actual interventions, and using extensive amounts of qualitative data collection and protocol analysis. For example, Tavella and Franco (2014) analyse the use of facilitated collaborative modelling in a specific workshop, identifying how new knowledge is created by the group. Rouwette, Größler, and Vennix (2004)) is a quantitative study, but again based upon modelling workshops. A variation of the theory of planned behaviour is used to capture attitudes and intentions from workshop participants.Clearly, given the pervasiveness of group model creation, experimental researchers in OR could attempt to tie such studies to actual models. For example, consider the Tako and Robinson (2010), where individuals use a model, create and use a model under time constraints, or create and use a model under no time constraints. An interesting variation might be to have groups, rather than individuals, work with each factor. This would allow an investigation of group performance, which may (or may not) exceed that of individuals.While experimental research in DA and IS was useful in backing up findings in experimental OR, since the turn of the century such research has moved in three directions that have perhaps taken it away from OR. Davern et al. (2012) provides a good review of where such research is currently positioned.First, IS has produced an extensive stream of research on group decision support, investigating how tools can be used to support negotiations, shared decision making, and conflict between decision makers. Dating back to seminal work by Desanctis and Gallupe (1987), this stream of research obviously crosses paths with the group and problem structuring research identified above. Early research, as in DA, tended to compare use of a group decision aid versus group activities without a decision aid. Recently, experiments have focused more on distributed decision activities across time zones and cultures.Second, there has been a serious attempt to move experiments beyond the very dominant use of subjects situated in North America. It has become recognised that cross-cultural studies have a role to play in experimental work. For example, within OM, Cui, Chen, Chen, Gavirneni, and Wang (2013) suggest that Chinese subjects behave differently in newsvendor experiments, being more cautious in their decision making and behaviours. This contrast nicely with the American subjects in Moritz et al. (2013).Finally, and perhaps most importantly, experimental researchers have moved to focus on consumer decision making. Given that there are far more consumers making decisions with technology (e.g., on-line shopping) than managerial decision makers in organisations, this has provided a very fertile and well-trodden research area. Moreover, the findings of Payne (1976) and others are now visible on many shopping Web sites: consumers are invited to view a choice set by alternatives (known to be more efficient than by attribute), to order alternatives by attributes, and to filter on pre-determined or user generated criteria. Karimi, Papamichail, and Holland (2015) is a good way into this literature for OR people, using many of the concepts and measures discussed in this paper. A useful experiment for OR might be to explore how a model, such as a simple additive multi-criteria model, performs for shoppers relative to current methods.Selection of appropriate experimental tasks is part of the methods determined by the researcher. Using known problems, or tasks used by others, increases the external validity of the research. As noted in Section 1, experimental OR researchers have typically traded rigour for better external validity, but there is far more that can be done.Visiting known problems, as has been done in behavioural OM (Bendoly et al., 2006), would allow a comparison with that body of knowledge. For example, now that OM has demonstrated how poorly individuals may behave with the newsvendor problem (or any other variants of inventory problems), experiments that show how OR can increase performance and confidence in solutions would build on this knowledge. The approach in OM (e.g., Sternman, 1989) has often been to work with scenarios where an optimal solution can be determined, or at least approximated. When an optimal exists, experimental studies can produce DVs for accuracy and variance in an identical way.Selection of differing DVs across studies makes comparisons difficult; consistent use of DVs will aid external validity. Yet a debate on what actually are the preferred DVs for OR, in all its forms, is in itself a useful activity. DeLone and McLean's (1992) paper on the DV for IS is a highly cited paper that led to a vibrant debate about what success means when building an Information System. Robinson (2002) does a similar job for simulation, but this is a rare piece of deep consideration of success measures for OR.Within a debate on DVs, it may also be worth considering some standard approach to measuring user and modeller understanding across studies, perhaps drawing upon work in education, such as that on how serious games can enhance learning (Girard, Ecalle, & Magnan, 2013). Further, approaches to measuring cognitive biases and cognitive style (or perhaps now dual processing measures) should be standardised. In the same way that using MBTI has allowed researchers to make valid comparisons with other studies using the construct, the prevalence of Frederick's (2005) cognitive reflection similarly allows for such comparison.A number of studies have used students as subjects (e.g., Hämäläinen et al., 2013, O'Keefe & Pitt, 1991) or even colleagues (Akpan & Brooks, 2014). Better external validity is achieved with actual decision makers, or at least professionals. This is becoming prevalent in IS and OM (e.g., Moritz et al., 2013). Further, as discussed above, there is a need to be aware of cross-cultural differences, and to avoid inappropriate generalisations beyond the subject pool.However, such tinkering with experimental behavioural methods might not add too much value to research. At the end of the day, experiments are artificial, and design and sampling problems will persist. Within IS, interest in experimental research has declined following some damning criticisms, such as Introna and Whitley (2000).What might be necessary is a move towards what are sometimes termed natural experiments. These are experiments that are conducted in natural settings with actual model creators or users, whereby measures of behaviour variables and even treatments are developed alongside the “real” work of. Some of the research on soft OR, especially that with groups, could be easily formulated as experimental work. For example, Tavella and Papadopoulos (2014) present a study that compares facilitation in a modelling workshop as performed by expert and novice facilitators. While presented as “action research”, the effort could easily be presented as a natural experiment with an IV (novice versus expert facilitators) and DVs that represent the performance of the facilitators. Framing such research as experimental would allow it to be more easily related to other experimental research. However, natural experiments bring their own problems – not least in that variables that the researcher might want to investigate cannot be easily manipulated (Shadish et al., 2002).

@&#CONCLUSIONS@&#
The majority of research in OR is concerned with advancing techniques; yet academics and practitioners alike understand that advancing the modelling process is equally important. The use of experimental behavioural research might not provide a mainstream approach to furthering research, but alongside case studies, action research and other narratives it can provide insights, supported by experimental evidence, that progresses our knowledge.In conclusion, some avenues for further experimental research were identified in Section 4. Here three are emphasised. First, experiments where the provision of data is varied. This will allow for findings pertaining to the interaction between data and models, and allow researchers to draw upon findings related to data from DA and IS. Second, experiments that show how the use of models can help users overcome cognitive biases. This has the capacity to progress OR and make it more productive. Third, consideration to the role of groups, and the differences between the behaviour of groups and individuals. Using groups in experiments concerned with the use of models will allow for findings regarding the differences (perhaps advantages) that groups bring to the creation and use of models.With regard to experimental methods, progress in experimental behavioural OR will require common measures and DVs. Researchers need to build on the work of each other rather than to create and execute one-off experiments that employ measures that are at variance with previous studies. Further, there is a growing role for natural experiments executed in real situations with active modellers and model users. Combining natural experiments (or at least quasi-experiment where some aspect of the task and/or users exist naturally) with common measures would allow experimental behaviour OR to possibly become a driving force in how we generate our body of knowledge.