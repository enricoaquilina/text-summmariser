@&#MAIN-TITLE@&#
SpiNNaker: Fault tolerance in a power- and area- constrained large-scale neuromimetic architecture

@&#HIGHLIGHTS@&#
Discussion of chip-level fault tolerance of SpiNNaker’s design.The implemented software improves fault tolerance by providing diagnostics and reconfiguration.Exploration of communication-level fault tolerance and its effects on system scalability.Wide range of experiments showing that SpiNNaker is highly resilient to failures.

@&#KEYPHRASES@&#
68M15,94C12,68M10,68R10,Fault tolerance,Globally asynchronous locally synchronous,Low power system,Massively-parallel architecture,Spiking neural networks,System-on-chip,

@&#ABSTRACT@&#
SpiNNaker is a biologically-inspired massively-parallel computer designed to model up to a billion spiking neurons in real-time. A full-fledged implementation of a SpiNNaker system will comprise more than 105 integrated circuits (half of which are SDRAMs and half multi-core systems-on-chip). Given this scale, it is unavoidable that some components fail and, in consequence, fault-tolerance is a foundation of the system design. Although the target application can tolerate a certain, low level of failures, important efforts have been devoted to incorporate different techniques for fault tolerance. This paper is devoted to discussing how hardware and software mechanisms collaborate to make SpiNNaker operate properly even in the very likely scenario of component failures and how it can tolerate system-degradation levels well above those expected.

@&#INTRODUCTION@&#
SpiNNaker is an application specific design intended to model large biological neural networks – the name “SpiNNaker” being derived from ‘Spiking Neural Network architecture’. It consists of a toroidal arrangement of processing nodes, each incorporating a purpose-built, multi-core System-on-Chip (SoC) and an SDRAM memory (Fig. 1). Neurons are modelled in software running on embedded ARM968 processors; each core is intended to model a nominal 1000 neurons. Small-scale SpiNNaker systems have successfully been used as control systems in embedded applications [1], providing robots with real-time stimulus-response behaviour as described in [2]. However the ultimate aiming of the project is to construct a machine able to simulate up to109neurons in real time. To put this number in context some small primates have brains with slightly lower neuron counts whereas the human brain has roughly 86 times this number [3]. To reach this number of neurons more than one hundred thousand integrated circuits will be needed (half of which are SpiNNaker chips and the other half SDRAMs).A system of this scale may be expected to suffer component failures and many features of its design are included to provide a certain degree of fault tolerance. These features can sometimes be justified on cost alone: the overall yield for the 100mm2 SpiNNaker SoC was estimated, using public domain yield statistics on a 20-core, at 50% fault-free chips, 25% single-fault chips, 10% two-fault chips and the remaining 15% will be unusable due to critical failures. Early test on the production chip (in Fig. 2) show similar, if rather better, yield characteristics (see Section 4). The 35% of chips having one or two faults would not be usable without fault tolerance features. Fault tolerance is addressed at a number of levels, not least the application itself, which is intrinsically fault-tolerant. SpiNNaker incorporates measures to enable continued function in the presence of faults; in fact it has been designed as a power- and cost-effective fault-tolerant platform.The major defence against faults in such a system is the massive processing resource. Processors are almost free and dedicating a small proportion of the processing power for system management and reconfiguration yields significant distributed ‘intelligence’ without much impact on the application. From the outset the intention has been to allocate one core on each SoC entirely to system management; if this eventually proves insufficient it is simple to delegate a second core to this task. Cores devoted to system management can identify and map around failed devices at run time.Particular attention has been paid to inter-chip communications where link failures or transient congestion may be routed around rapidly without software intervention. Finally some more conventional techniques – such as automatic CRC generation and checking and watchdog timers – are employed in each processing node. As a large-scale system has not yet been built the full possibilities of software reconfiguration have yet to be explored. However statistical models of the architecture have been developed and used to verify the principles, and the hardware mechanisms themselves have been tested in silicon in small-scale (4 chip) systems. The construction of a larger machine is in progress.

@&#CONCLUSIONS@&#
