@&#MAIN-TITLE@&#
Optical flow modeling and computation: A survey

@&#HIGHLIGHTS@&#
We propose a survey of optical flow estimation focusing on recent developments.We adopt a classification approach organizing methods in a comprehensive framework.The paper is conceived as a tutorial introducing and explaining the main concepts.

@&#KEYPHRASES@&#
Optical flow,Motion estimation,Regularization,Parametric models,Optimization,Feature matching,Occlusions,

@&#ABSTRACT@&#
Optical flow estimation is one of the oldest and still most active research domains in computer vision. In 35years, many methodological concepts have been introduced and have progressively improved performances, while opening the way to new challenges. In the last decade, the growing interest in evaluation benchmarks has stimulated a great amount of work. In this paper, we propose a survey of optical flow estimation classifying the main principles elaborated during this evolution, with a particular concern given to recent developments. It is conceived as a tutorial organizing in a comprehensive framework current approaches and practices. We give insights on the motivations, interests and limitations of modeling and optimization techniques, and we highlight similarities between methods to allow for a clear understanding of their behavior.

@&#INTRODUCTION@&#
Motion analysis is one of the main tasks of computer vision. From an applicative viewpoint, the information brought by the dynamical behavior of observed objects or by the movement of the camera itself is a decisive element for the interpretation of observed phenomena. The motion characterizations can be extremely variable among the large number of application domains. Indeed, one can be interested in tracking objects, quantifying deformations, retrieving dominant motion, detecting abnormal behaviors, and so on. The most low-level characterization is the estimation of a dense motion field, corresponding to the displacement of each pixel, which is called optical flow. Most high-level motion analysis tasks employ optical flow as a fundamental basis upon which more semantic interpretation is built.Optical flow estimation has given rise to a tremendous quantity of works for 35years. If a certain continuity can be found since the seminal works of [120,170], a number of methodological innovations have progressively changed the field and improved performances. Evaluation benchmarks and applicative domains have followed this progress by proposing new challenges allowing methods to face more and more difficult situations in terms of motion discontinuities, large displacements, illumination changes or computational costs. Despite great advances, handling these issues in a unique method still remains an open problem. Comprehensive surveys of optical flow literature were carried out in the nineties [21,178,228]. More recently, reviewing works have focused on variational approaches [264], benchmark results [13], specific applications [115], or tutorials restricted to a certain subset of methods [177,260]. However, covering all the main estimation approaches and including recent developments in a comprehensive classification is still lacking in the optical flow field.This survey paper is conceived as a tutorial aiming at organizing in a comprehensive framework the main approaches and practices for optical flow estimation. We will not try to tell the whole story of optical flow evolution, neither we give an exhaustive list of existing methods. We rather propose a synthetic classification of the main methodological principles of existing methods, with a particular concern given to recent developments. We will insist on the modeling aspects, practical interests and limitations of each introduced methodological element, with the aim of providing useful cues for a better understanding of optical flow. We adopt a classifying approach, decomposing the optical flow estimation problem in sub-parts. This viewpoint has the advantage of being didactic and facilitates the identification of similarities between methods in a global unique framework. However, this ambition of clarity should not hide that individual methods are often conceived as coherent approaches, and cannot be reduced to a mere assembly of isolated parts.The remainder of this paper is organized as follows. In Section 2, we present general and basis concepts on optical flow used throughout the paper. Section 3 is devoted to a review of data constancy assumptions for optical flow and their usage for the design of data terms. In Section 4, the parametric approach, its specific issues and existing solutions are described. Section 5 presents the modeling and optimization aspects of globally regularized methods. Section 6 focuses on the occlusion issue. Section 7 is dedicated to the recent trend of integrating feature matching in the optical flow estimation process. Finally, the general conclusions of this survey are reported in Section 8.The notion of optical flow literally refers to the displacements of intensity patterns. This definition originates from a physiological description of the visual perception of the world through image formation on the retina. In that sense, while optical flow is necessarily caused by relative motion between the observer and the objects of the observed scene, it only represents motion of intensities in the image plane, and not necessarily accounts for the actual 3D motion in the physical scene [251]. This definition of optical flow states that intensity of moving pixels remains constant during motion, and the main difficulty is to cope with the uncertainty due to the ill-posedness of the problem, called aperture problem (described in Section 2.4).However, motion of interest in computer vision is in practice the real displacements of the objects. Thus, the variable that we actually want to retrieve is the projection on the image plane of the 3D motion in the scene, usually called motion field. The problem of recovering the 3D motion itself from several synchronized cameras is also an active research domain known as scene flow estimation [18,248]. The new problem arising is then that intensity changes are not necessarily caused by the displacements of objects of the scene, but can also be due to other disturbing phenomena like lighting changes, reflection effects or modifications of the properties of the objects affecting their light emission or reflectance.The terms optical flow and motion field are usually mixed up to both designate the above definition of the motion field, and we will carry on this practice in the sequel of this survey.Optical flow provides fundamental information at the basis of various computer vision systems, in a wide range of applicative domains. We cite here some of the main fields where it is currently exploited.The increasing use of multimedia devices involving video displays accentuates the importance of several video analysis problems. Action recognition is for instance an essential task for semantic interpretation of video content [1,130,256]. Video compression standards like MPEG exploit motion estimation to predict intermediate frames [131]. Optical flow can also be a determinant feature in video indexing and retrieval [123,202,230]. Video restoration of aged films is another field of interest, and can be improved by taking optical flow estimation into account [92,270].In a biomedical context, dynamic properties of tissues or cellular objects are also of upmost importance. The deformation of organs [109,275] or the estimation of blood flow [124] are examples of medical applications that can require optical flow computation. Many conceptual and methodological links can also be found between medical image registration and optical flow [100,211,212,219,224]. In microscopy, dense motion can inform about cell deformation [3,90,139,203], motion of cellular structures [73,89], or help for individual cell tracking [167].Robots or vehicle navigation also exploit optical flow as input of control systems for automatic guidance. Autonomous car driving has received particular attention in recent years [94,98,235]. Obstacle detection and avoidance are the main tasks investigated for general robot control in real environment exploiting optical flow [57,67,82].Automated video surveillance is another growing research field requiring motion analysis. In particular, facial expression and gesture recognition [31,70], crowd motion and pedestrian behavior analysis [19,143] often involve dense motion analysis.Among the other fields of interest, fluid flow analysis has applications in meteorology, oceanography aerodynamics or fluid mechanics and requires to consider specific physical constraints [65,113,115,168]. Several typical image deformations characterized by the conservation of texture properties during motion can be described as dynamic textures, and receive a specific treatment [69,85,84].As in other computer vision domains, much attention has been paid to the design of appropriate evaluation procedures for optical flow.The visualization of motion fields provides a qualitative insight on the accuracy of the estimation. The two main visualization techniques are presented in Fig. 1. The arrow visualization directly represents motion vector and offers a good intuitive perception of physical motion. On the counterpart, a clean display requires to under-sample the motion field to prevent overlapping of arrows. The color code visualization associates a color hue to a direction and a saturation to the magnitude of the vector. It allows for a dense visualization of the flow field and a better visual perception of subtle differences between neighbor motion vectors.On the other hand, objective quantitative evaluation based on error metrics is necessary for an accurate comparison of method performances. When ground truth is available, two error measures are commonly used, namely the Angular Error (AE) and the Endpoint Error (EPE). The AE of an estimated motion vectorwest=(uest,vest)⊤w.r.t. the reference vectorwref=(uref,vref)⊤is defined by the 3D angle created by the extended vectors(uest,vest,1)⊤and(uref,vref,1)⊤:(1)AE=cos-1uesturef+vestvref+1uest2+vest2+1uref2+vref2+1.The EPE is defined as the Euclidean distance between the two vectors:(2)EPE=(uest-uref)2+(vest-vref)2.These two metrics are complementary regarding motion magnitude. AE is very sensitive to small estimation errors, occurring for small displacement, whereas EPE hardly discriminates between close motion vectors. On the contrary, AE tends to under-evaluate distances between motion vectors in some cases of large motion magnitudes, while EPE strongly penalizes large estimation errors.The design of challenging benchmarks with ground truth on which to apply these error measures has motivated a substantial amount of work, which main steps are illustrated in Fig. 2. The first optical flow benchmark with ground truth established by [17] dealt either with simple parametric transformations applied to real images, like translation or rotation, or with synthetic sequences for which the true motion is available by construction. The resulting motion fields were characterized by small displacements and absence of discontinuities. More recent benchmarks have been designed to address new types of problems or specific applicative issues. The Middlebury benchmark1http://vision.middlebury.edu/flow/eval/.1[11,13] is composed of more challenging sequences, partly made of smooth deformations similar to the sequences described in [17], but also involving motion discontinuities and motion details. While some sequences are synthetic, several others were acquired in a strictly controlled environment allowing to produce ground truth for real scenes. Issues raised by Middlebury being almost solved by modern methods, the MPI Sintel benchmark2http://sintel.is.tue.mpg.de/.2[52] has been recently proposed. It is extracted from a synthetic movie and opens new issues mostly related to very large displacements, occlusions, illumination changes, and effects like blur or defocus. In parallel, dedicated datasets have been designed to solve specific problems related to applicative contexts, the most successful example being the KITTI benchmark3http://www.cvlibs.net/datasets/kitti/index.php.3[94] devoted to assisted driving applications.Let us denote an image sequence byI:Ω×T→R, whereΩ⊂R2is the image domain and T is the sampled time interval of the sequence. Every optical flow estimation method is based on an assumption on the relationship between the searched motion fieldw:Ω→R2at time t and the imageI(·,t). The most natural and widely used assumption is that pixel intensity remains constant during displacement. The brightness constancy constraint equation (BCCE) is then defined by:(3)dIdt(x(t),t)=0.Other feature constancy can be chosen, each encoding specific image properties, which will be discussed in Section 3. The discrete approximation of (3) at a given pixelx∈Ωand time t yields:(4)I(x+w(x),t+1)-I(x,t)=0.However, constraint (4) usually generates particularly difficult optimization problems. It can be much more tractable to consider the expanded version of (3) with partial derivatives, resulting in a linear version of (4):(5)∂I∂x1(x)u(x)+∂I∂x2(x)v(x)+∂I∂t(x)=0,wherew(x)=(u(x),v(x))⊤andx=(x1,x2)⊤. For the sake of clarity, we have dropped the dependency of I over t. We can also write (5) as(6)∇I(x)·w(x)+It(x)=0,where∇·=∂·∂x1,∂·∂x2Tis the spatial gradient operator,It=∂I/∂tis the partial temporal derivative and the dot “·” denotes the inner product.The linearized brightness constancy constraint (5) provides only one equation to recover the two unknown components ofw(x). From this single constraint, the component of the motion vectorw(x)in the direction of the image gradient can be computed, but the two-dimensional problem remains under-constrained. This is known as the aperture problem, stating that motion of linear structures, as it is assumed by (5), is by nature ambiguous if the neighboring context is not taken into account.To make the problem well-posed, it is necessary to introduce an additional constraint encoding a priori information onw. The a priori will take the form of spatial coherency imposed by either local or global constraints, described in Sections 4 and 5.It is important to mention that while (4) holds for motion of arbitrary magnitude, the continuous motion constraint (5) restricts its validity domain to the linear region of I, which usually corresponds to small displacements or very smooth images. The linearization is nevertheless necessary for methods relying on differential computations. The standard technique to cope with large displacements is to embed the estimation in a coarse-to-fine scheme [27,81,174]. The idea is to create a pyramid of coarse-to-fine downsampled versions of the original image. At the coarsest level, the linearity domain of the image encompasses large displacements and the estimation can be based on (5). The estimations at coarser levels serves to warp the image at subsequent finer levels, where the estimation then reduces to a search for small motion increments. The solution is iteratively refined at each level until reaching the full image resolution. It is possible to continue the process in still higher resolutions created by interpolation, as proposed in the coarse-to-overfine approach [5]. The solution at each level of the multi-resolution pyramid can be interpreted as a fixed point in the direct optimization of the non-linearized constancy Eq. (4)[45]. Almost all differential methods concerned with large displacements resort to the multiscale approach, possibly with some additional strategies to avoid its drawbacks.The main undesirable effect produced by smoothing at coarse levels is the loss of small and rapidly moving objects in the final estimated flow field. If the object extent is smaller than its displacement, it is likely to be smoothed out at coarse levels and then “forgotten”. Avoiding this drawback has been a very active topic in recent years. It has been accomplished mainly by integrating feature matching in the estimation process, as will be discussed in Section 7, or by resorting to discrete optimization methods, as detailed in Section 5.2.2.The deviations from the data constancy assumption (3) are penalized at every pixelx∈Ωvia a data potentialρdata(x,I1,I2,w), defined in the case of brightness constancy (4) as(7)ρdata(x,I1,I2,w)=ϕ(I2(x+w(x))-I1(x)),whereI1=I(·,t)andI2=I(·,t+1)denote two successive frames, andϕ(·)is the penalty function.The brightness constancy assumption is in practice an imperfect photometric expression of the real physical motion in the scene. A common counter-example consists in moving the light source of an immobile scene, producing brightness variations without motion of any objects. In general, while it is possible to create synthetic sequences for which the constraint strictly holds [52], it is often violated in practice in case of changes in the illumination sources of the scene, shadows, noise in the acquisition process, specular reflections or large and complex deformation.Choosing a quadratic penalty functionϕ(z)=z2, as in early works [120,170], makes optimization much easier, but assumes that the residual of the brightness constancy constraint Eq. (3) is normally distributed and thus gives a strong influence to large localized violations mentioned above. It is then common to resort to robust statistics [125] to reduce the impact of local errors considered as outliers. Common robust alternatives to quadratic penalty are theL1norm [45], the Tukey function [193], the Lorentzian norm [27] or the Leclerc’s function [174]. Adapted optimization schemes must then be adopted to cope with non-linearity or non-convexity induced by the robust terms, as will be discussed in Sections 4.2 and 5.2. A priori smoothness assumption based on parametric constraint (Section 4) or explicit regularization (Section 5) also counterbalances local invalidity of data constancy.Robust statistics and regularization treat the problem of violations of the constancy assumption by considering it as noise, with underlying distribution assumptions [155,221]. The considered distributions may not suitably model the possibly large localized violations implied by the above listed causes. Therefore, a large number of alternatives to brightness constancy have been proposed, aiming at more stable invariance properties. A few experimental studies have compared performances of different data costs given fixed optimization and regularization contexts [227,252].Let us notice as a preamble to this section that it is difficult in practice to design a data term independently from the spatial coherence constraint and the optimization strategy to which it will be associated. For example, sophisticated feature conservation usually involves specific optimization difficulties, and thus requires careful design of the optimization solution. We will dedicate this section to a review of the main classes of data terms for optical flow estimation, emphasizing their validity domains and their limitations, independently from the estimation context in which they were elaborated.We explore several matching costs aiming at overcoming the drawback of the brightness constancy, in particular its sensitivity to noise and illumination changes.A first class of data potentials exploits the same pixel-wise form as (7), but operates on a transformed versionf(I)of the original image sequence:(8)ρdata(x,I1,I2,w)=ϕ(f(I2)(x+w(x))-f(I1)(x))Image smoothing. We can first notice that Gaussian smoothing is applied as a pre-processing step by most methods [45,286], in order to reduce the influence of noise. It can be viewed as a modified version of brightness constancy, setting f as a Gaussian filtering operator.High-order constancy. Image derivatives possess illumination invariance properties that are well suited for motion estimation. The constancy of spatial image gradient, defined byf(I)=∇I, has been introduced in [245] for its ability to overcome the aperture problem when the determinant of the Hessian is non-zero. However, when applied on the directional derivative vectors, the gradient conservation only holds for translational or divergence motions. To achieve rotational invariance, the penalty should rather be applied on the magnitude of the derivatives, that isf(I)=‖∇I‖[45]. It was subsequently used in the context of the local approach (see Section 4) in [240] and integrated in global variational methods (see Section 5) in [45].Despite a demonstrated performance gain in the case of additive illumination changes compared to brightness constancy, gradient information is also much more sensitive to noise, and disappears in poorly textured regions. Therefore, it is always used in addition to brightness constancy. A large number of methods rely on this combination and achieve good results [45,181,279]. Finally [198] investigated higher-order data constancy like Laplacian of the imagef(I)=ΔI, or norm and determinant of the Hessianf(I)=|HI|,f(I)=det(HI).Texture. Another way to obtain robustness against illumination changes is to work with the cartoon and texture components of the image, as initiated in [263]. The decomposition proposed in [8] consists in first obtaining the structure partISby discontinuity-preserving smoothing (using the ROF model [210] in [263]), and then deriving texture partITby subtractingISto the original image. Additive illumination changes only affect the cartoon image while the texture image is less impacted. However, even ifITcaptures most image information, it is still insufficiently discriminative in some regions, and it is also more sensitive to noise thanIS. To limit this drawback, the texture image used to compute optical flow is usually blended with the cartoon part by a parameter γ[154,231,261]:f(I)=IT-γIS.Color spaces. When dealing with color images, several photometric-invariant color spaces can be exploited. In particular, multiplicative illumination invariance is essential for realistic illumination models [246] and is achieved in the HSV space by the hue channel (local and global changes) and the saturation channel (only global changes) [176]. As for previously mentioned image transformations, the benefit in illumination change regions coincide with a loss of information in other parts, and the color channels are in practice combined with the intensity valued channel [286]. Other color spaces like normalized RGB [103] or spherical space [246] have been investigated.Combination of linear filters. As mentioned before, it is often necessary to combine several constancy assumptions. It has been achieved in [259] by considering K different linear filtersfkproviding a system of constancy equations to be solved at each pixel. In [232], a learning approach is proposed for finding the best combination of constraints. The data constraints induced by the linear filtersfkare imposed through a Gaussian Scale Mixture (GSM)ϕGSM.(9)ρdata(x,I1,I2,w)=∑k=1KϕGSM(fk∗I2(x+w(x))-fk∗I1(x),Ξk),where∗denotes the convolution operator andΞkare the parameters of the GSM associated to each filter. By learning the parameters of the GSM from a set of ground truth sequences, the “weights” of each filter in (9) are automatically determined. These weights are constant in the whole image. Spatially adaptive combination is also of upmost importance and will be addressed in Section 3.2.1.Rather than pre-filtering images, neighborhood information can be integrated directly in the data term by patch-based similarity measures. Let us point out that a major issue with patch-based measure is the determination of the size an possibly shape of the patch.Filtering the data term. In addition to pre-smoothing the images (8), Bruhn et al. [49] proposed to filter the data potential as follows:(10)ρ(x,I1,I2,w)=fϕ(I2(x+w(x))-I1(x)),where f is chosen as a Gaussian filter. While it was demonstrated beneficial for very noisy sequences, this choice also significantly blurs motion discontinuities and degrades the overall performance for low amount of noise, compared to pixel-wise data term, as emphasized in [286]. This limitation is addressed in [78,206] by replacing the Gaussian filtering with anisotropic discontinuity-preserving filtering (e.g. bilateral filtering in [78] and tensor voting in [206]). The relation between adaptive filtering and non-linear diffusion has also been exploited in [48].Correlation-based measures. Similarity measures based on cross-correlation have been extensively used for various correspondence problems [44]. Normalized Cross Correlation (NCC) is usually preferred for its invariance to linear illumination changes. The NCC for a windowW(x)centered at pixelxis defined as(11)NCC(x,I1,I2,w)=∑y∈W(x)(I2(y+w(x))-μ2(x+w(x)))(I1(y)-μ1(x))υ1(x)υ2(x+w(x))where fori={1,2},μi(x)is the mean andυi(x)the standard deviation ofIiin the windowW(x). The associated data potential is(12)ρdata(x,I1,I2,w)=1-NCC(x,I1,I2,w).NCC is actually discriminative enough to be used in a matching procedure without additional regularization, and produces coarse but reasonably robust motion fields. It is used in several applications like stereovision [72], fluid flow analysis [22] or biological imaging [146] where it also enables direct physical measures for diffusion processes.The computational cost of (11) is a major limitation. Unlike simple cross-correlation which can be efficiently computed with Fast Fourier Transform (FFT), the computation of NCC for matching purpose cannot be easily performed in the frequency domain. In [163] only the numerator is computed with FFT and the denominator is rewritten as a product of sums, independent of the position of the pixel and thus efficiently computable with integral images [83]. In [171] this idea is generalized and the numerator is also computed with integral images, dramatically reducing the computation time and making it invariant to the patch size.The challenge of integrating NCC in a variational optimization scheme is to cope with its non-differentiability. Indeed, Taylor expansion on the terms containingw(x)in (11) still yields a highly non-linear potential. The approach of [180,272] has been applied to NCC but is able to handle arbitrary data terms as well. The authors directly linearize the data term and compute its spatial derivatives with finite differences. Werlberger et al. [272] keeps a second-order approximation to ensure the convexity of the energy, necessary in the primal–dual scheme used. Another recent technique allowing fast computation of NCC relies on the fact that NCC is actually equivalent to the Sum of Square Differences (SSD) when the images are filtered with the cheaply computed correlation transform[79].Census. Census transform [283] recently regained interest and was promoted in [226] for optical flow estimation [108,118,179,182,183,205,252]. The Census signature is a bit string reflecting relative value of pixels of a patch with respect to the center pixel. By discarding the absolute intensity values, only the structure of the neighborhood is encoded in the signature, which makes it robust to illumination changes. It has shown robust behavior in outdoor scenes and vehicle driving scenarios [205,226,252]. Integrating the Census transform in variational optical flow is not trivial since it cannot be easily linearized. Solutions to remedy this problem are convex approximation [252], reformulation as a generalization of the gradient constancy conservation [108] or linearization of the data term [182,205] as previously mentioned for NCC [272].The validity of each of matching costs is limited to a given range of visual situations. In a single frame, regions can coexist satisfying a given constancy assumption, and violating others. One solution could be to linearly combine them to take advantage of their complementary invariance properties. Softly selecting the best constancy constraint at each pixel is usually devoted to the robust penalty function, limiting the influence of local constraint violations. However, the data term should ideally be spatially adapted.We distinguish two classes of methods achieving the spatial adaptivity: (i) optimization of the weights of a linear combination of data potentials, and (ii) estimation of the spatial distribution of the errors attached to a single data potential. The normalization of the data term used in [214,221,286] could fall in this category since a spatially varying weight is applied to the data term. It is derived from the linearized brightness constancy equation to prevent too strong constraints in regions of high image gradient (see a detailed interpretation in [286]).The combination of P data constraints can be expressed as the weighted sum of their associated potentialsρp(x,I1,I2,w):(13)ρdata(x,I1,I2,w)=∑p=1Pλp(x)ρp(x,I1,I2,w).Weightsλp(x)are spatially variant and have to be optimized to locally favor different data terms.The idea of combining several data constraints has already been explored twenty years ago in [116]. In addition to the classical brightness constancy, the authors exploited a complementary sparse edge-based constraint. Weightsλp(x)are binary confidence measures derived from hypothesis testing providing evidence on each constraint.In [279], intensity and gradient conservation are combined and their complementarity is experimentally demonstrated. The weights are defined to operate a binary selection between the two constraints and are obtained by considering a mean field approximation of (13), which intuitively amounts to selecting the constraint having the lower (normalized) potential. This idea has been used subsequently in [181].The work of [140] addresses the problem in its most general form (13), allowing the combination of an arbitrary number and type of data conservation assumptions. A confidence measure for arbitrary data term is designed as an extension of the feature discriminability [218] to data discriminability. The confidence measures are used as local constraints on the weightsλp(x)in (13), and a regularization onλp(x)is also imposed. The weights are then optimized jointly with the motion field.Another way to handle errors related to the constancy constraint is to design more elaborated models of intensity variations. The goal is to explicitly model the intensity changes due to environmental effects so as to isolate more accurately motion-induced changes.The most common approach introduces an additional variable representing deviations from constancy assumption by a functione(x,I1,I2,ξ)parametrized by the vector ξ:(14)ρdata(x,I1,I2,w,ξ)=ϕ(I2(x+w(x))-I1(x)-e(x,I1,I2,ξ)).The model proposed by [188] is composed of an offset changeξo:Ω→R, accounting, e.g., for moving shadows or highlights, and a multiplicative changeξm:Ω→Rencoding linear illumination variations. The error function can then be expressed as(15)e(x,I1,I2,ξ)=ξm(x)I1(x)+ξo(x).This general formulation has been exploited in a number of works, considering either the offset parameter alone [55,89,193], the multiplier alone [284] or both parameters [142,159,239]. They may differ on the type of spatial coherency, the penalty function, or the optimization strategy. A smoothness constraint is assumed onξoandξm, either with a local parametric form [188,193] or a global regularization [55,89,142,159,239]. The offset formulation is also used in [10,88], but is associated to a sparsity constraint aiming at retrieving violations due to occlusions. The more general approach of [74] parametrizes intensity changes in terms of Brightness Transfer Function [106], which coefficients are learned from training data.The model (15) is based on a general polynomial approximation. If specific knowledge about the observed physical process is available, dedicated models can be designed. A number of physical constraints are explored in [110], where a generic local estimation framework is designed based on a Taylor expansion of arbitrary data constraints similar to the subsequent methods [180,272].As explained in Section 2.4, a spatial coherence constraint on the flow field must be added to the previously described data terms. To this end, one can impose the flow field to follow a parametric model in regionR⊆Ω. The motion fieldwα:R→R2is then fully characterized by the associated parameter vectorα. When the regionRis a small sub-domain of the image, these methods are referred to as local approaches. The objective energy to be minimized is the weighted sum of the potentials provided by each pixel ofR:(16)α^=argminα∑x∈Rg(x)ρdata(x,I1,I2,wα)whereg(x)is a spatial weighting function controlling the influence of pixelxin the estimation.It is crucial to determine the local estimation domain where the parametric form of the motion model is a valid approximation of the true motion. Low-order polynomial motion models like translation or affine deformation can usually represent motion in small neighborhoods, whereas more complex models like deviations from affine constraint or combination of basis functions can deal with larger regions.We first give an overview of the mostly used motion models and their associated optimization strategies. Secondly, we discuss about the different ways to define appropriate local estimation supports.The choice of the motion model is driven by a trade-off between efficiency and representativeness. Complex nonlinear and physical-based models can be exploited to model deformations for image registration [224]. These models are particularly well adapted to physically constrained situations as they can be encountered in medical imaging, and in particular to capture smooth deformations. In contrast, optical flow is dealing with temporal sequences of arbitrary content, usually involving motions of several objects with unrelated behaviors, generating motion discontinuities as well as smooth motion parts. As a result, it is difficult to capture the whole complexity of motion fields with a single unifying and computationally tractable parametric model. Therefore, attempts in this direction are not frequent and not among the best performing methods in optical flow benchmarks. The approach of most parametric methods for optical flow is rather to rely on much simpler motion models, mostly polynomial models, and to restrict their application to local domains, where they can represent accurate approximations.We will restrict ourselves to linear models of the form(17)wα(x)=∑k=1Kbk(x)αk(x),whereb={bk(x)}k∈[1,K],bk(x)∈R2are basis functions andα(x)={αk(x)}k∈[1,K]are the parameters to be optimized. Other parametric models than those described here can be found, like planar surfaces or rigid body [23], or wavelet bases [75,76,217,274]. It can also be noted that parametric models are sometimes completed with explicit regularization terms (see Section 5) imposed on the parameters themselves [75,133,175,191].Polynomial models are among the most compact parametric representations of motion fields and are also remarkably well suited to retrieve local physical motion of individual objects and even deformable motion. The polynomial model can be seen as a special case of (17), which we write for clarity as:(18)wθ(x)=B(x)θ,whereB(x)is a matrix determining the form of the model and θ is the parameter vector. Apart from the exception of [191] where the parameters are spatially variant and regularized, the parameter θ is kept constant over the estimation domain. The matrixB(x)depends on the pixel coordinatesx∈R, and the order of the polynomial determines the complexity of the motion field. Low-order polynomials are usually sufficient to model smooth motion fields, and their small number of parameters allows for efficient computation. The two mostly used polynomial models correspond to the translation and the affine motion:Translational:θ=(a1,a2)⊤;B(x)=1001Affine:θ=(a1,a2,a3,a4,a5,a6)⊤;B(x)=1x1x20000001x1x2withx=(x1,x2)⊤.The translation assumption is very restrictive and must be applied to very small regions [170]. The physical assumption underlying the affine model is a rigid motion of 3D planar surfaces projected orthogonally on the image plane. Higher-order polynomials can model more complex situations, but are still too smooth to allow for motion discontinuities. For example, the 8-parameter quadratic model represents rigid motion of a planar surface in perspective projection. The small number of parameters of the affine model and its realistic local assumption make it often considered as the best trade-off between complexity and descriptiveness [27,88,175,193]. The accuracy of local affine motion estimation in appropriate estimation support was experimentally demonstrated in [88].The basis functions can also be learned from a set of training flow fields. As for polynomial models, the resulting learned motion models cannot describe motion of complex scenes, but they are able to retrieve a larger diversity of local motion patterns, including discontinuities.The design of the training set reflects the assumption on the form of the flow. In a generic point of view, Black et al. [32] used synthetic motion fields representing simple motion patterns. The method of [190] relies on a large number of patches of ground truth motion fields. The training set can be dedicated to a specific application, as in [87] where the aim is to estimate mouth motion. To avoid resorting to external ground truth, [93] defines the training set on the processed sequence itself. The basis set is composed of trajectories constructed by feature tracking on large temporal scales, in regions ensuring reliable tracks. In all these works, an orthogonal basis of flow fields is generated by PCA decomposition, conserving only the first K components containing most of the variance of the training set.The free-form deformation model (FFD) [211] has been originally introduced for image registration and has demonstrated great efficiency to retrieve smooth deformations. The displacements are defined on a coarse regular subgrid of the image and is interpolated on the final resolution with B-splines. The motion basisbk(x)is thus formed with the displacements of the K control points and coefficientsαk(x)are B-spline influence functions. The dimensionality reduction induced by the subsampling of the image grid makes the computation much easier, and the spatial coherence of the deformation is ensured by the B-spline interpolation. On the counterpart, the framework cannot retrieve sharp motion discontinuities, while it is necessary for optical flow applications. Image-adaptive non-regular control points distribution [212] or coarse-to-fine spacing strategies [211] are possibilities to address this issue.The work of [236] was the first to apply this idea to optical flow, with non-uniform control points defined on image-driven quadtrees. In [201], B-Splines defined on a uniform grid are used to retrieve smooth deformations. In [101] the problem is turned in a discrete setting. The range of motion labels is iteratively adapted by estimating a local uncertainty covariance. Despite good results, the method is still limited by over-smoothing. The method of [219] addresses the discontinuity problem with a sparsity constraint on the B-spline coefficients, allowing to modulate the influence of the control points. Recent approaches [100,101,219] also add an explicit regularization on the motion field, overlapping with the methodology described in Section 5.Parametric models are usually associated with the penalty of a pixel-wise data constancy constraint (7). In case of intensity constancy (5), energy (16) then writes:(19)E(α)=∑x∈Rg(x)ϕ∇I(x)·wα(x)+It(x)where∇Iis the image gradient andIt=∂I/∂t. The special case of a quadratic function ϕ and a translational model as in [170] leads to a very simple optimization problem, since the cancelling of the derivatives of (19) amounts to solving the linear systemMα=b, withM=∑xg(x)Ix12(x)∑xg(x)Ix1(x)Ix2(x)∑xg(x)Ix1(x)Ix2(x)∑xg(x)Ix22(x)b=-∑xg(x)Ix1(x)It(x)∑xg(x)Ix2(x)It(x)whereIx1andIx2are the partial derivatives of I respectively along the horizontal axisx1and the vertical axisx2. The rank ofMallows one to decide if a unique solution of the linear system exists, and can be used to adapt the size of the local domainR(see Section 4.3.2). Despite the limitations of the quadratic penalty, this approach has become very popular for its implementation simplicity, low computational cost and available code in the OpenCV library [36,41].However, robust estimation is often advocated [27,32,76,193,215] as mentioned in Section 3, especially for polynomial models, to deal with the frequent case of multiple motions in the estimation domain. Among the variety of optimization methods used in case of robust penalty function, the Iterative Reweighted Least Squares (IRLS) [119] and gradient descent approaches have mostly been used. IRLS proceeds by successive optimizations of quadratic problems weighted by a function of the current estimate and is implemented in the Motion2D software [193,215] with. Gradient descent approaches are often coupled with Graduated Non-Convexity (GNC) [27,33] to cope with non-convexity of (19). Regarding the slow convergence of steepest descent, it is preferable to use second order approximations and Newton methods, or quasi Newton methods like L-BFGS or Levenberg–Marquardt, approximating the Hessian for large dimension problems.As previously mentioned, the spatial adaptation of the parametersα(x)is a way to cope with complex and discontinuous flow fields [93,191,219]. This approach often involves a priori constraints on the spatial distribution ofα(x)and is thus strongly related to the methods that will be presented in Section 5. Such a dense parameter map moves away from the compactness of parametric models.On the other hand, when parameters are constant over the estimation domainR, the resulting motion field is smooth and can constitute a valid approximation only in regions of coherent motion. The regionRmust be large enough to enable motion estimation, while small enough to keep valid the parametric approximation (generalized aperture problem[27]). We will describe below the strategies for definingRin the case of constant parametersα(x)=αoverR, for polynomial models.Despite their inability to retrieve realistic motion in arbitrary scenes, polynomial models combined with robust estimation are well adapted to capture the dominant image motion. Applied in the whole image domain Ω, they become particularly useful to separate or compensate the camera motion [193]. Applications to moving object detection [194] or to action recognition [202] are among the most common ones.The approach initiated by [170] performs independent estimations in small square or circular patches. Most of the related methods use fixed patch size, and conserve the velocity vector deduced from the estimated motion model at the square center [12,26,141,284]. This choice is very popular for its simplicity of implementation and can be naturally parallelized [222,284]. It is still extensively used for numerous applications. However, following the generalized aperture problem, patches centered at each pixel with a fixed size are likely to contain either multiple motions or no image gradient.Multiple motions in a single patch can be partially handled with robust estimation by rejecting secondary motions, considered as outliers [27,95,193,215].The second option is to adapt the size or the position of the patch so that it contains an unimodal motion distribution. The size of the patch can be adapted with a bias-variance criterion [173], or based on a confidence measure on the reliability of the local domain for parametric estimation. Starting from a small patch size, it is thus possible to increase the size of the patch until the condition for a reliable domain is violated [173,215]. In the translational model case, one can analyze the singularity of the matrixMof the linear system (20) and, for example, impose a minimum threshold to the maximum eigenvalue ofM[17]. Rather than adapting the size, [132] adapts the position of the patches. Patches corrupted by strong intensity edges are displaced by a mean-shift procedure to reach more homogeneous regions. The method described in [88] adapts both size and position of the square patches. This adaptation is performed thanks to the aggregation framework decoupling the estimation of candidates with predefined sizes and positions, and their selection achieved with a global model (see Section 5).Finally, local adaptation of the weightsg(x)in (16) is another way to adapt the estimation support. It can be done for instance with bilateral filter weights as in [78], or using the non-linear structure tensor [48].The optimal supports to perform polynomial motion estimation ideally correspond to a segmentation of the image in coherently moving regions. We briefly describe two types of approaches: independent image segmentation and joint estimation of motion and region supports or frontiers.Image segmentation. While the ultimate goal is to segment the unknown motion field, color-based image segmentation is a much simpler alternative which can help motion estimation. It can be reasonably assumed that motion discontinuities coincide with image discontinuities (but the inverse is far from being true). It implies that an image segmentation is a motion field over-segmentation, and obtained regions are thus guaranteed to contain no motion discontinuity. However, merely estimating motion in the resulting regions is problematic for two reasons.The first limitation is that the segmented regions may not contain enough information for motion estimation. Parametric estimations in these regions must be performed by circumvented ways. The very fine over-segmentation of [287] imposes for instance to perform region matching. Generally, an independent coarse and cheap motion estimation is fused with the color image segmentation to overcome the lack of information [28,34,278]. In [278], hybrid regions are found by applying mean-shift segmentation in the extended space of color and motion. Differently, [28,34] fit a parametric flow field on the coarse initial motion field, obtained with a global regularized method [30] for [28], and with the sparse KLT tracker [218] for [34].The second problem is that spatial coherence between estimated motion in neighboring segments is not ensured. Global regularization (see Section 5) can here be imposed, either on the motion parameters associated to each region [278] (similarly to [133], not resorting to image segmentation), on the coarse motion field completing color information [28] or, in a layered approach, on the layer assignment function [34].Joint estimation and segmentation. Color segmentation is usually too dependent on the image content to make it the basis of a robust motion estimation method. Rather than considering segmentation and estimation as two independent tasks, most methods have a coupled approach of the problem. Motion parameters and region supports are jointly estimated by minimizing a global energy imposing a coupling between them. This approach has first been addressed as a labeling problem [37,195] where the label fieldl:Ω→{l1,…,lN}associated to the N regions is estimated jointly with the motion parameters in each regionα={α1,…,αN}, in a discrete Markov Random Field framework:(20)E(α,l)=∑x∈Ωdρdatax,I1,I2,wαl(x)+∑〈x,y〉ρregMRF(l(x),l(y)),whereΩdis the discrete image domain andρregMRF(l(x),l(y))is a regularization prior on the label field, typically chosen asρregMRF(l(x),l(y))=1-δ(l(x),l(y)), withδ(·)the Kronecker function. The optimization is done alternatively between motion and regions. Another viewpoint in a variational framework extends the Mumford–Shah formulation of image segmentation [184] to motion segmentation [66,199,247]. In addition to the data fitting potential inside each regionRi, a constraint restricting the lengthL(C)of the set of region boundaries C is imposed globally, resulting in the energy(21)E(α,C)=∑i=1N∫Riρdata(x,I1,I2,wαi)dx+νL(C).The minimization is performed alternatively on the flow and the boundaries. Minimizing (21) with respect to C requires a differentiable approximation of the contour lengthL(C). It is common to implicitly represent the partitioning of the image with level sets, which allows to represent the interior of the regions by the sign of the function, as well as the total length of the boundaries by their level lines. One level set function can only represent two regions. For an arbitrary number of N regions, it is possible to define N corresponding levels sets, at the price of a high computational cost and a more complex energy to prevent vacuums in the partitioning. Other strategies can be employed, as the one of [56] re-used in [66], for more sophisticated combinations between functions. In [199], this level set framework is augmented with an edge-driven tracking and background detection. In [207], the two images are jointly segmented, and they influence each other through a dynamic prior term determined by the similarity of the two evolving shapes. Segmentation of static and dynamic textures is performed in [84] by assigning different data conservation assumptions to each class. A graph-cut optimization scheme has been proposed in [80]. These two formulations (20) and (21) can be found in numerous other works [137,175,213,244]. Similarly, layered approaches [9,223,231,234,257,277] aim at decomposing the observed scene into overlapping layers and introduce information about depth ordering of regions. An important interest of such methods is to provide a natural occlusion handling framework.Two main drawbacks affect the joint motion estimation and segmentation approach. First, alternating minimization w.r.t. regions and flow fields is computationally expensive. The related layered approach [233,234] achieving state-of-the-art results requires several hours to process a pair of640×480pixels, and even GPU-based implementation [244] can need up to an hour.Second, the initialization of motion and segmentation parameters usually has a substantial impact on the evolution of the region contours throughout the minimization procedure. Therefore, paradoxically, the best performing methods in terms of accuracy of optical flow fields resort to pre-calculated motion fields obtained with independent methods to initialize their algorithm. For example, the results reported in [233,244] are obtained with an initialization by the result of the methods of [231,271] respectively. Nevertheless it is shown in [244] that the initialization affects more the speed of convergence than the final result.Some works exploit more complex flow field representations than polynomial approximation, by authorizing deformations from an initial affine model [28,175,233] or an explicit regularized model [4,46]. Allowing such complex and discontinuous motion fields in segmented regions actually tends too produce larger regions, and ultimately leads to global approaches where motion discontinuities are handled by the global modeling itself. Consequently, regions may not represent coherent motion, but rather a delineation adapted to the specific estimation method. Finally, imposing prior on shapes and contours has been investigated in medical imaging [138], but a limited number of regions can be handled at the same time.In an alternating manner to the parametric representation of spatial coherence, mostly adapted to smooth deformations, the global form of the motion field can be imposed by an explicit regularization term. Motion discontinuities are then no more represented by the boundaries of the regions delimiting parametric motion fields, but they are involved in the global model, often considered as outliers w.r.t. smoothness assumptions. The variational approach has been initially proposed by [120] and is usually referred to as the global approach, since the regularization term interconnects all the pixels of the image and thus requires the optimization of the objective energy to be performed globally. In this subsection, we review current versions of the regularization model and optimization strategies.In its most general form, the energy minimized by global regularization methods can be written as:(22)Eglobal(w)=∫Ωρdata(x,I1,I2,w)+λρreg(x,w)dxwhereρdata(x,I1,I2,w)is the data potential, as discussed in Section 3,ρreg(x,w)is the regularization potential encoding an a priori assumption on the fieldw, and λ is a parameter tuning the balance between the two terms. Broadly speaking, the regularization potential aims at smoothing the motion field in regions of coherent motion while preserving motion discontinuities at the boundaries of moving objects. Finding the trade-off can also be partially addressed in the adaptation of the balance parameter λ[112,156,189,286].A major interest of the global variational framework is its versatility, allowing one to model different forms of flow fields by combining different data and regularization terms. One must nevertheless keep in mind that minimizing (22) is often a tricky task. The potentially unlimited combinations of data terms and regularization terms is restricted in practice to those compatible with efficient minimization. Besides, advances in optical flow have often been correlated with new possibilities offered by optimization techniques. For example, efficient Primal–Dual minimization for Total Variation regularization [55] have motivated a number of optical flow models [244,269,285]. The development of efficient discrete optimization techniques based on graph cuts [40] or message passing [147] also inspired various works [59,161,181]. Another consequence of the close intricacy between energy model and optimization technique is the difficulty to compare performances of different models, as global optimum is in general not guaranteed for sophisticated energies and the quality of the local optimum depends on the type of optimization method.We will detail in Section 5.1 existing regularization models independently from optimization techniques, for the sake of clarity. Section 5.2 focuses on the dependency between specific energy models and optimization methods.The most natural and widely used way to impose smoothness of the motion field is to penalize the magnitude of the flow gradient:(23)ρreg(x,w)=h(x,I1)ϕ(‖∇w(x)‖2)whereϕ(·)is the penalty function andh(x,I1)is a weighting function.A taxonomy of optical flow regularizers has been proposed in [266]. The authors focus on convex and rotational invariance properties, and prove uniqueness of the solution in each case. For each regularization of type (23), they show the equivalence between the resolution of the Euler–Lagrange equations associated with energy (22) and diffusion filtering. In addition, a diffusion tensor is derived for each particular variation of (23). We will give a more succinct overview, taking only some elements from this classification and integrating more recent approaches.Flow-driven regularization. In flow-driven approaches, no relation between the form of the flow field and the structure of the image is assumed. The weighting function is thush(x,I1)=1,∀x∈Ω. The seminal formulation of [120] adopts a quadratic penalization function:(24)ρreg(x,w)=∇u(x)2+∇v(x)2,withw(x)=(u(x),v(x))⊤. The quadratic penalization being unable to capture motion discontinuities, the introduction of a complementary line process to handle motion discontinuities in the MRF framework [116,153], and the use of robust sub-quadratic penalties [27,77,174] have soon been employed to overcome the problem. Among the wide panel of robust functions, the popular parameter-free Total Variation (TV) prior, has interesting and useful properties [45,63,279,285]. Contrary to most other robust norms, the TV yields a convex constraint facilitating optimization. The non-differentiability in 0 is generally alleviated by using the regularized versionϕ(z)=z2+∊2, where ∊ is a small constant. Associated to proximal splitting minimization, TV involves solving several ROF (Rudin–Osher–Fatemi) models [210], for which very efficient algorithms exist [54]. A series of optical flow estimation methods have exploited this idea for fast and accurate minimization [55,263,272,285].TV regularization actually favors piecewise constant flow fields. This framework is known to transform a smoothly varying variable to a succession of small discontinuous constant steps (staircasing artifacts). This undesirable effect can be reduced by replacing theL1penalization by a quadratic one for small gradient magnitude, which is the behavior of the Huber norm [220,271]. Another possibility is to penalize higher-order derivatives of the flow, as done in [241] for the second derivative, to favor piecewise affine flow fields. The Total Generalized Variation (TGV) [43], generalizesL1penalization to arbitrarily high order derivatives. The performance gain of the second order TGV has been experimentally shown for smooth deformation conditions, for which the staircasing effect is prominent with TV regularization [42,205,252].Despite the demonstrated performance of TV due to its algorithmic attractiveness, the real distribution of optical flow derivatives has been shown to follow a more heavy-tailed and concave distribution [208]. Finding good approximate solutions for non-convex priors has motivated a number of works and will be discussed in Section 5.2. When appropriate minimization strategy is available, like graph cuts or the recent work of [192], this kind of penalty functions has proven to yield improvements compared to the TV model. Efficient methods to minimize Potts model have also recently been investigated [53].Inclusion of image gradient information. It is natural to assume a link between the motion field and its source imageI1. As already stated in Section 4.3.3 about the relationship between motion and image segmentation, it is reasonable to consider that motion discontinuities coincide with image discontinuities delineating moving objects. This information can be incorporated in the regularization through the weighting functionh(x,I1)taken as a smooth decreasing function of‖∇I‖2[2,10,181,263,279], often defined as(25)h(x,I1)=e-‖∇I(x)‖2/ς2,where ς is a parameter setting the influence of the image gradient on the regularization. Despite the risk of over-segmentation, this simple weighting strategy usually improves experimental results.The weighting function (25) is isotropic since the smoothing is modulated by the same value in all the directions. This is suboptimal since we would ideally like to prevent the smoothing only across the boundaries, and allow it along them. This can be achieved by defining the regularization axes differently from the horizontal and vertical axes. The eigenvectorss1ands2of the structure tensorRρ(x)=Kρ∗[∇I(x)∇I(x)⊤]are well adapted sinces1is oriented across local image edges ands2is orthogonal tos1. This idea has been first introduced in [186], which regularizer has been rewritten in [286] as:(26)ρreg(x,w)=1‖∇I(x)‖2+2κ2κ2us12(x)+vs12(x)+(‖∇I(x)‖2+κ2)us22(x)+vs22(x)whereusi,{i=1,2}, are the derivatives of u along thesiaxis and κ is a regularization parameter. When κ is small, the regularization is reduced in the direction of the image gradients1and strengthened along image edgess2depending on the image gradient magnitude‖∇I(x)‖2. In [186], the eigenvectorss1ands2are obtained with a radiusρ=0for the Gaussian filtering of the structure tensorRρ(x).The classical artifact produced by purely image-driven regularization is an over-fitting of the flow field on image boundaries, creating artificial motion discontinuities. To reduce the impact of image gradient in the regularization, Sun et al. proposed in [232] to keep thes1ands2directions, while suppressing the weighting on‖∇I(x)‖2in (26) and employing a robust penalty function ϕ:(27)ρreg(x,w)=ϕ(us12(x))+ϕ(vs12(x))+ϕ(us22(x))+ϕ(vs22(x)).The work of [286] proposed a generalized computation of the regularization axes, oriented to follow the data constraint rather than the image edges. In analogy with the previous approach definings1,s2from the structure tensor, they compute the eigenvectors of a so-called regularization tensor, designed to be complementary with the data term. The approach of [286] can be generalized to data potentials built from the combination of L linear constancy constraints, that is,(28)ρdata(x,I1,I2,w)=∑ℓ=1Lϕ(Aℓ(x)⊤w(x)+Bℓ(x)).For this kind of data potential, the regularisation tensor is defined by(29)Rρ(x)=∑ℓ=1LKρ∗Aℓ(x)Aℓ(x)⊤,whereKρis a Gaussian kernel ands1,s2are the eigenvectors ofRρ(x). TakingL=1,A1(x)=∇I(x)andB1(x)=It(x)yields the brightness constancy constraint, and the regularization tensor reduces to the structure tensor. For more elaborated data terms, as the combination of normalized brightness and gradient constancy used in [286], the resulting axes are more consistent with the data constraints.The gradient of the flow can only provide a local constraint on the interaction between pixels. Assuming long range interactions can capture more precisely the form of the motion field. Such non-local regularization has been recently investigated in [79,154,231,269] by describing the structure of the flow in an extended neighborhoodN(x)in a discrete setting as:(30)ρreg(x,w)=∑y∈N(x)k(x,y,I1)ϕ‖w(x)-w(y)‖2.The weightsk(x,y,I1)indicate which pixely∈N(x)should share a similar motion with pixelx. They are derived from the bilateral filter, favoring small distances in the spatial and color spaces [281]:(31)k(x,y,I1)=exp-‖x-y‖2σs2-‖I1(x)-I1(y)‖2σc2,whereσsandσccontrol the influence of spatial distance and color similarity. This approach is image-driven in a similar way to local weighting (25), in the sense that the smoothness is weighted by the image edges. Nevertheless, the integration on a large neighborhood mitigates the influence of local gradients and more globally exhibits the structure of the objects. It is implemented as an alternate weighted median filtering in [231] and interpreted as a low-level soft segmentation in [272].The high-order regularization causes severe optimization difficulties discussed in Section 5.2, in particular in terms of computational cost, increasing with the size of the neighborhoodN(x).A natural idea is to extend the spatial regularization described above to the temporal dimension, assuming that motion varies smoothly across consecutive frames. Similarly with the spatial case, smoothness on the time axis can be achieved either locally, based on the temporal gradient, or taking into account a longer interval by working on trajectories.Constraint on the temporal flow gradient. The most straightforward way to model temporal smoothness is to penalize the temporal flow gradient, analogously with the spatial flow gradient in Section 5.1.1[185]. This simple extension is however unrealistic since motion of objects necessarily implies a temporal change of flow fields. Thus, regularization on the spatio-temporal direction of the motion field is more appropriate, and is achieved in [60,187,267,286] by extension of the spatial gradient penalties described in Section 5.1.1 to the spatio-temporal dimension. However, the performance of local temporal regularization is deceiving in most cases.Constraint on the trajectory. Regularization based on temporal derivatives is unable to model large displacements. In this case, temporal coherence is more adequately modeled along the trajectories of objects. It was done in [29], who does not model explicitly the trajectories but estimates temporal changes on warped images to alleviate the problem of large displacements. However, the warping is done sequentially in the forward direction and is thus prone to propagate errors. In the same spirit, Volz et al. [253] considers the same coordinate system for groups of five frames, which implies an implicit natural registration. The estimation is done jointly in all frames of the sequence, which overcomes the lack of feedback with previous frames of [29]. The trajectory constraint in [93] is explicitly imposed by modeling the flow field as linear combination of long-term trajectory bases, obtained from few reliable tracks.As previously mentioned the optimization strategy employed to minimize (22) has a decisive influence on the final result. We give an overview of the main continuous and discrete optimization methods and point out their adaptability to specific energy terms.Resolution of the Euler–Lagrange equations. The Euler–Lagrange equations give necessary conditions for minimizing energy of the form(32)E(w)=∫ΩF(x,w,∇w)dx,which is the case of (22) withF(x,w,∇w)=ρdata(x,I1,I2,w)+λρreg(x,∇w)when the regularization is a function of the flow gradient (23). They provide the following system of partial differential equations:(33)∂ρdata(x,I1,I2,w)∂u-λdiv∂ρreg(x,∇w)∂∇u=0∂ρdata(x,I1,I2,w)∂v-λdiv∂ρreg(x,∇w)∂∇v=0,where∂·∂zis the partial derivative operator with respect toz,div(·)is the divergence operator. The system (33) can be rewritten by introducing the diffusion tensorDaccounting for the relation with diffusion equations [266]:(34)∂ρdata(x,I1,I2,w)∂u-λdivD(x)∇u(x)=0∂ρdata(x,I1,I2,w)∂v-λdivD(x)∇v(x)=0.The analogy with diffusion equations makes explicit the direction and magnitude of the smoothing, which correspond respectively to the eigenvectors and eigenvalues ofD(x).The discretization of the gradient and divergence operators yields a large system of equations to be solved. If the system is linear, its sparsity makes it well suited for iterative solvers like Gauss–Seidel or successive over-relaxation (SOR) [45]. Nevertheless, the linear case is in practice only encountered in the model of [120] using quadratic penalties and a linearized data constraint. To cope with non-linearity, the typical approach [45,265] is to resort to time-lagged schemes [62] by handling each source of non-linearity with fixed point iterations, turning the problem into a succession of linear systems, and updating iteratively the non-linear parts. Convergence of the scheme is ensured if the linear systems are solved exactly, but approximations and frequent updates often yield in practice good results and much faster convergence.Fast computational schemes have been employed for achieving near real-time performance. A multigrid framework has been proposed in [50]. The scheme is very efficient, but it is problem-specific and requires a substantial implementation effort. Another approach is to consider the solution of the Euler–Lagrange equation as the steady state of the corresponding diffusion process (34), and use the Fast Explicit Diffusion (FED) principle [105] to accelerate convergence by adapting the time steps. The implementation of [107] exploits the natural parallelization of explicit schemes to achieve a quasi real-time version of the variational method of [286] on GPU, based on FED.The approach of [45] has become standard because of its simplicity, the wide range of models it can handle, and its good experimental performances even for non-convex energies [45,164,253].Early discretization. The computation of the Euler–Lagrange equations can be complex or impossible for some forms of the energy (22). Moreover, one can argue that the discretization of the Euler–Lagrange equations can generate numerical errors with respect to the original energy [204]. A way to alleviate these shortcomings is to avoid computing the Euler–Lagrange equations and instead to directly discretize the energy (22). The equation to solve is then simply the cancellation of the differentiated discretized energy:(35)∂E(w)∂w=0.Solving (35) amounts to inverting a large linear system, similar to the one obtained by Euler–Lagrange equation discretization for energies of the form (32). Employing a fixed-point iterations scheme to cope with non-linearity amounts to an IRLS approach, which is shown by [165] to be equivalent to the above described resolution of the Euler–Lagrange equations with fixed-point.Contrary to the Euler–Lagrange scheme, the regularization is not imposed to be a derivative ofw, and non-local regularization terms (30) can be handled. However, such an approach yields a dense linear system, not solvable with standard iterative methods. In [154], a linear-time method is proposed to compute matrix product by successive Gaussian filtering, allowing to efficiently inverse the dense linear system with a conjugate gradient solver. The method of [232] also exploited the ability of handling more general energies to optimize learned data and regularization terms. The general experimental study of [231] and the complex method of [233] also follows this approach, with Graduated Non Convexity (GNC) to cope with multimodality of the energy [27,33].Instead of solving (35), a gradient descent method can be applied to minimize the discretized energy. Due to the large scale of the problem, Newton methods requiring the inversion of the Hessian of the energy are not applicable, and only quasi-Newton methods are computationally tractable. A few works have explored this direction, with Truncated Newton [134,135] or L-BFGS [75].Half-quadratic minimization. Instead of solving directly the energy (22), a number of optimization methods proceed to the addition of an auxiliary variable splitting the original problem into easier sub-problems. The half-quadratic minimization falls in this class. It can be shown that under non-restrictive conditions [58,96], a function ϕ can be rewritten as the following function Φ introducing the additional variableγ∈]0,1]:(36)ϕ(z)=infγΦ(z,γ)=infγγz2+ψ(γ),where the function ψ can be explicitly derived from ϕ. The robust non-convex penaltyϕ(z)in data and regularization potentials can be replaced byΦ(z,γ)in the data and regularization terms, so that the optimization inzbecomes easy since it involves only quadratic terms. Moreover, the minimization w.r.t.γhas a closed-form solution:(37)γ^=argminγΦ(z,γ)=ϕ′(z)2z.The original minimization problem inzis thus turned into alternate simple optimizations onzandγ. This approach also leads to the IRLS algorithm described in Section 4.2.The introduction of this approach for optical flow estimation coincided with the use of robust penalties for discontinuity preserving regularization [7,32,77,174] and was more recently exploited in [65,112].Proximal splitting. Another successful optimization method based on alternate minimization of simple sub-problems has been proposed by [8] and used for optical flow in [285]. The data and regularization terms are splitted and associated to separate variables, which are quadratically coupled by a third term:(38)Esplit(w,v)=∫Ωρdata(x,I1,I2,w)dx︸Edata+12ε∫Ω‖w(x)-v(x)‖2dx+λ∫Ωρreg(x,v)dx︸Ereg,wherevis an auxiliary variable. The parameter ε sets the intensity of the coupling. If ε is small, (38) tends to the original univariate problem (22). The minimization w.r.t. each variable is the computation of the proximal operator ofEdataandEreg:(39)argminwEsplit(w,v^)=proxEdata(v^)argminvEsplit(w^,v)=proxEreg(w^),where the proximity operator of a function f is defined by(40)proxf(z)=argminuf(u)+12‖u-z‖2.The minimization problems (39) can be viewed as alternating coarse pixel-wise data matching and denoising of the flow field. A large number of variants and generalizations of this proximal splitting idea exist, among which the alternating direction method of multipliers (ADMM) [38] or the formalization in a primal–dual framework described in [55].This approach has initially been designed forL1penalty on the data and regularization terms, a.k.a. TV-L1model [8,285], for the simplicity of the resulting optimization sub-problems. The optimization of the data term withvfixed is efficiently performed by a thresholding scheme [280], and fixingwyields the Rudin–Osher–Fatemi model [210], optimized with the duality based algorithm of [54]. An advantage is that a differentiable approximation of theL1norm is not required, as in [45], and one can solve for the exact TV–L1model.In a general point of view, the independence of the optimization of data and regularization parts allows one to design dedicated minimization schemes in a variety of cases. The restriction is to be able to compute the proximal operators, and the convergence is ensured only for convex energies. For the data part, the thresholding scheme of [285] is applicable in some cases for theL1norm, and efficient solutions have been found to handle an additional fundamental matrix constraint [262], a truncatedL1norm of normalized cross correlation [269], or mutual information [196]. Another advantage of the pixel-wise nature of the data part minimization is that it enables parallel implementation strategies which can dramatically speed up the algorithm and reach real-time [285]. Based on the pixel-wise property, [227] proposes a discrete exhaustive matching for optimization inw, which opens the usage of arbitrary data terms, only limited by the computational cost of the matching. Patch-based similarity measures have also recently been implemented with the fast PatchMatch algorithm [15] by [114].Concerning the regularization part, it is possible to minimize non-local regularization terms [79,272]. From an evaluation viewpoint, the decoupling in the minimization process also allows for a fair comparison of the effects of different data and regularization terms [252].In a discrete setting, the solution of the minimization of (22) is searched in a set of discrete labelsLcorresponding to a quantization of the continuous motion vector space or the selection of a finite subset of motion vectors. We give a fast overview of basic principles of optimization methods for this combinatorial problem. For a more complete analysis, see the recent review of [255]. The spatial discretization of (22) yields an energy in the Markov Random Field (MRF) framework:(41)ED(w)=∑x∈ΩDρdata(x,I1,I2,w)+∑y∈N(x)ρMRF(x,y,w),whereΩDis the discrete image domain,N(x)denotes the pixels interacting withxin the model andρMRFis the discrete version ofρreg(x,y,I1,I2,w)which explicitly takes into account the interaction between two neighboring pixelsxandy.An important advantage of discrete optimization over the continuous approach is that it does not require differentiation of the energy and can thus handle a wider variety of data and regularization terms. On the counterpart, a trade-off has to be found between the accuracy of the motion labeling and the size of the search spaceL. Indeed, discrete optimization methods are usually severely limited in terms of accuracy and speed by the number of labels, particularly for optical flow where subpixel accuracy is necessary, and where the two-dimensional motion space becomes more rapidly intractable than the one-dimensional stereo case for instance. Therefore, the design of the label spaceLis a crucial component of discrete optimization methods for optical flow.Among early methods for minimizing (41), simulated annealing [97] offers proved convergence towards the global minimum based on stochastic relaxation. However, the optimal convergence is guaranteed only under prohibitive computational cost. An approximate solution can be obtained much faster with the Iterated Conditional Modes [25] or its High Confidence First variant [61], operating by iterative local minimizations of the energy, but this local optimum often yields poor results compared to modern methods [237], especially for non-convex functions. A multi-scale MRF approach was designed in [117] where the energy function is minimized over nested subspaces of the original discrete label space. This constrained optimization scheme offered simple implementation and fast convergence towards high quality estimates for non-linear problems such as motion computation.Graph cut. The work of [39] gave rise to rapidly growing research interest and advances on graph cut approaches for MRF-based energy minimization. The basis of graph cuts is the max-flow/min-cut problem consisting in finding the optimal path between two nodes in a directed graph, solvable by many algorithm in polynomial time [91,102]. It is possible to model an undirected MRF structure as a directed graph by introducing two additional source and sink nodes, and then interpret the min-cut partition as a binary label segmentation of the MRF energy. The global minimum of the binary optimization can be guaranteed for pairwise interactions and submodular functions. In summary, it is possible to find the global optimum for the energy (41) under the following conditions:–submodularity ofρMRF(x,y,w),pairwise interactions,binary labels.Research on graph cuts has attempted to overcome these three constraints, based on the original max-flow/min-cut algorithm.Submodularity of the pairwise term is a required property for convergence of the algorithm. Finding good approximate solution can be achieved with the Quadratic Pseudo Boolean Optimization (QPBO) [35,148,209], leading to an optimal but partial labeling. Dealing with higher-order local interactions between pixels has also been addressed in several recent works [86,128,144,150].Submodularity and pairwise interactions are not too restrictive constraints for a large range of computer vision problems and let room for a number of applications. On the contrary, the cardinality of the label space satisfies|L|>2in most cases, so that the binary label requirement is a much harder limitation. The extension of binary graph cuts to multi-label have mostly been realized through iterative move-making techniques. The idea is to create at each iteration a binary-labeled space composed of the current solution and a new proposal label. The label setLis thus explored progressively by each new proposal. If each binary minimization performed with the max-flow/min-cut method is ensured to be optimal, a decreasing of the energy is guaranteed at each iteration, and the method converges to a local minimum. It can be noticed that the class of move-making methods is not restricted to graph cuts and the moves can be achieved by other techniques such as the variational approach of [242], optimizing in the continuous space by relaxing the binary variable.The elements of a move-making method are the move-space, specifying the set of new labeling proposals at each iteration, and the way the moves themselves are performed. The most popular move-space is the expansion-move, where the proposal labeling is defined as a constant label map. Another common alternative is theαβ-swap move based on a label exchange at pixels having labels α or β. The range-move[250] extends binary proposal choice to a larger range of labels, in the case of ordered label spaces. Pre-computed labelings computed with independent and possibly continuous methods, can also serve as proposals [161].Additionally, computational efficiency of graph-cut approaches have been addressed by several works. The most representative ones are [152], operating in a Primal–Dual framework and speeding up convergence by minimizing the Primal–Dual gap, and on the other hand, [145] working on dynamic MRF and exploiting previous iterations as initializations. For more details about existing move-spaces, see [249,255]. Applications for optical flow have increased rapidly in recent years [59,64,88,101,99,161,164,234,279].Message passing. Belief propagation is based on the max-product algorithm, which is able to find the MAP of a probability distribution expressed as a product of factors, and thus representable in a factor graph (see [157] for a detailed introduction). Taking the negative logarithmic version of such distribution amounts to considering MRFs of the form (41), which can motivate to rename the algorithm min-sum in this case. In a nutshell, the minimization is done by iteratively updating local messages reflecting influence of local label configurations on the energy. After convergence of the messages, they can be used to define the probability of assigning a given label to a node, and the label with the maximum probability is chosen. The max-product has originally been designed for tree structures and is guaranteed to find the global optimum in this case. Nevertheless, it can also be used for MRFs exhibiting cycles (it is referred to as loopy belief propagation in this case [200]), without convergence guarantees but showing good experimental results in a large number of computer vision problems [136,236]. The Tree Reweighted message passing approach [254] deals with similar concepts, but with a particular message passing strategy based on tree representations. The sequential approach of [147] has proven to yield good results and computational performance compared to other discrete methods in [237]. It has been applied for optical flow estimation in [104,121,160,181]. It is also exploited for image matching involving large deformation models to cope with high computational complexity, as described in [216] or in the SIFT-Flow algorithm [166].The data costs described in Section 3 have been defined as measures of some data constancy between two corresponding points. However, they do not take into account the case of pixels without correspondence, namely occlusions. The occlusion issue in motion estimation, stereovision or image correspondence has motivated different types of investigation [10,116,34,129,149,162,225,273,276]. Occluded regions in the current image of the video sequence are defined as the set of pixels which become hidden by occluding (moving) objects in the next image, as illustrated in Fig. 3. These pixels have no corresponding points in the next image and motion is not observable at their locations. Points on (static or moving) objects which leave the camera field of view to the next image are part of a similar situation. Hence, the occlusion issue must be carefully addressed to ensure a reliable and accurate computed optical flow. This is particularly the case for large displacements [229], as in the MPI Sintel dataset [52]. Most methods implicitly deal with occlusion, together with other sources of violation of the data constancy assumption, by using robust norms in the data term, as emphasized in Section 3. To describe explicit occlusion handling approaches, we must further distinguish between occlusion detection, i.e., segmenting the image into occluded and non-occluded regions, and occlusion filling, i.e., recovering the missing flow subfields in occluded regions.Occlusion detection has been mostly investigated as a subsequent operation to motion computation, by thresholding a consistency measure issued from the estimated motion field. Several criteria have been proposed as geometric forward–backward motion mismatch [127,181], mapping unicity [279], divergence of the flow field [14] or data constancy violation [276]. The main limitation of the sequential approach is that accuracy of occlusion detection is highly dependent on the quality of the motion estimation. Several flow and image criteria have also been combined in a learning framework [126].Other approaches estimate the occlusion map jointly with the motion field in an alternate optimization scheme [10,88,127,197,244]. In the continuous setting of [10] involving a sparsity constraint for occlusion detection, the data constancy deviation is balanced by an estimated continuous residual intensity field, from which occluded points are retrieved by thresholding. The usual drawback of alternate optimization schemes is the convergence towards local minima due to the strong coupling between motion and occlusions. This problem is addressed in [88] by introducing an occlusion confidence map in the model, guiding occlusion estimation at each iteration independently from the current motion estimate.Finally, occlusion map can also be retrieved from the decomposition of the motion field in layers [223,233,234]. Indeed, occluded parts of each layers can be naturally derived from their depth ordering and the way they overlap with each other.When occluded regions are known, occlusion filling with motion vectors is conceptually closely related to image inpainting, since it recovers motion in regions where it is by definition not observable. Inpainting methods can be coarsely divided into two classes, diffusion-based methods [24,56] and exemplar-based methods [6,68,151]. A synthesis of these two approaches has been investigated in [51] in a variational framework.Let us denoteO⊂Ωthe set of occluded pixels ofI1. Classical methods for occlusion filling with motion vectors operate in a global regularization framework by cancelling the data term at occluded points, that isρdata(xo,w,I1,I2)=0,∀xo∈O. Thus, this class of diffusion-based (or geometry-oriented) occlusion filling relies on the diffusion process of the regularization to propagate motion from non-occluded regions to occluded regions via partial derivative equation (PDE) resolution [10,197,279].It is known that diffusion-based inpainting methods perform well in case of thin missing areas or cartoon-like images, but they are usually unable to handle for large missing regions. Rather than simply cancelling the data term, it is then necessary to design valid data costs for occluded pixels when dealing with large occlusion regions. This is achieved in [88], following the inpainting analogy and inspired by the superior performances of exemplar-based approach for large missing regions. At occluded pixelsx∈O, the data cost imposes proximity with the motion vector of a matched non-occluded pixelm(xo)∈Ω⧹O, assumed to belong to the same object asxoand thus to have similar motion:(42)ρdata(xo,w,I1,I2)=‖w(xo)-w(m(xo))‖2.Another possible definition of data cost at occluded pixels can be found in [14], under the assumption of temporal constancy of motion. Assuming that a pixelxo∈OofI1, occluded in the next frameI2, is non-occluded in the previous frameI0, backward data constancy is imposed at occluded pixels:(43)ρdata(xo,w,I1,I2)=ϕ(I0-I1(xo-w(xo))).An increasing challenge for optical flow estimation is to handle very large displacements and deformations, as it is reflected in the recent MPI Sintel benchmarks [52], where is it not rare to find displacements of more than 100pixels. As a consequence, the gap between feature matching and optical flow tends to vanish, and several methods have tried to combine density and accuracy of optical flow with the ability to capture large displacements of feature matching.Finding correspondences by matching image features can be considered as a local parametric approach since a given neighborhood of the pixel to match is assumed to translate or undergo another parametric transformation towards its correspondence location. The similarity measures can be patch-based distances (see Section 3.1.2), or more complex and sparse feature descriptions often based on histogram of oriented gradients [20,71,169,282], or segment matching [258]. The fundamental difference lies in the optimization process, since the parametric formulation has a differential optimization process imposing linearization, whereas feature matching explores a discrete space of admissible correspondences. Although being integer displacements and prone to errors, feature matching can thus handle large displacements without coarse-to-fine-schemes, with arbitrarily complex similarity measures. Regarding their complementarity of advantages, the combination of feature matching with regularization approaches is therefore of upmost interest.We consider three approaches to obtain dense and accurate motion fields with feature matching: local filtering of correspondences, integration in a variational framework, and generation of coarse initialization for variational refinement.Pure feature matching has long been considered unable to produce dense flow fields with competitive accuracy with the previously described local and global approaches. There are three reasons for this:1.The optimization of the similarity measure is often performed with exhaustive search, which induces prohibitive computational cost.Repetitive textures or uninformative regions are sources of ambiguities for the matching process and generate large errors.The correspondence process usually limits the accuracy to integer displacements, contrary to local and global approaches working in the continuousR2space.Several recent advances attempted to overcome these three seemingly inherent limitations.Research on speeding up block matching include multi-scale search strategies [243], integral images [83] or search in trees [158], but the recent most spectacular contribution was achieved in [15,16] with the PatchMatch algorithm. The method scans the image in the lexicographic and inverse order, and alternates two simple steps at each pixel: the propagation step minimizes locally the data cost in the space composed by the current pixel and its two predecessors in the scanning order, the second step proposes a small set of new candidates randomly chosen in the neighborhood of the current correspondence. It is easy to extend the matching to more complex transformations than translation, like rotation or scale factor [16] by increasing the degree of the search space. The method was originally designed for image editing and was applied to several other applications [16], with impressive results regarding the low computation time. For motion estimation, the interesting property is that the computational cost is not affected by the spatial extension of the search space, so that no trade-off has to be found between speed and displacement range.The problem of matching ambiguities comes from the lack of discriminative power of the data cost. Without resorting to explicit regularization (Section 5), the coherency induced by simple local filtering of patch correspondences [122] has been shown to be sufficient in practice to reduce most ambiguities and provide satisfactory dense results. The filtering is achieved in [122] by guided filtering [111] and in [172] by weighted median.Subpixel accuracy is usually reached by upscaling image resolution or by locally approximating the matching surface by a polynomial function. The induced computational cost is reduced in [122,227] by GPU implementation, and can also be handled by iterative refinement [160].The combination of the three ingredients has led to the development of competitive optical flow estimation methods based on pure feature matching locally filtered [122,172,238].As discussed in Section 2.4, a major limitation of the global variational framework is the loss of small and fast objects due to the use of a coarse-to-fine estimation scheme. One recently investigated approach to overcome this problem is to integrate an information from feature matching into the variational framework, thus combining advantages of both methods.The approach of [47], inspired by [113], made the first step in this way by adding to the classical data potential a new constraint taking into account an off-line computation of sparse feature correspondences. Let us denotewcthe displacement field obtained with a possibly sparse feature matching process. The new combined data potential is then(44)ρdata+(x,I1,I2,w,wc)=ρdata(x,I1,I2,w)+βρcorresp(x,w,wc),where β is a trade-off parameter and the matching potential is defined as:(45)ρcorresp(x,w,wc)=δ(x,wc)c(x,wc)ϕ(‖w(x)-wc(x)‖2).The binary functionδ(x,wc)returns 1 ifwc(x)is defined atxand 0 otherwise, and the weightsc(x,wc)correspond to the matching cost. The third term of (45) imposes the motion field to be close to the motion vectors obtained by feature matching.The advantage of this approach is that the termρcorresp(x,w,wc)is both differentiable and valid for large displacements. Problems related to the use of coarse-to-fine schemes are thus avoided. The main drawback is that the importance of the matching termρcorresp(x,wc,w)relatively to the data termρdata(x,I1,I2,w)is mostly determined by the value of the matching costc(x,wc). Consequently the final variational estimation is extremely dependent on the reliability of the confidence measure, which is very difficult to guarantee as emphasized in [42]. Matching errors are thus easily driven byc(x,wc)and are likely to have a high impact on the final result. Reducing the impact of local feature matching errors by the regularization and the robust penalizationϕ(·)is insufficient in practice in a lot of cases.Recently, the authors of [268] based their method on [47] by taking the model (45) and improving the feature matching stage. They demonstrated a significant performance improvement due to the increased reliability of the matching. The method of [42] is also built upon [47] with a modified matching component by using segment matching [258]. The matching term is generalized to handle weakly localized line features. It is done through a point-to-line distance in addition to the point-to-point distance of (45), combined with a confidence measure for segment matching based on the assumption of a linear mapping between segments. Moreover, considering the unreliability of confidence measure, [42] gets rid of the matching costc(x,wc)and relies only on the regularization for robustness to matching errors. To sum up, the tendency is to take into account the great dependency of the estimation on the quality of the feature matching, and thus to concentrate most efforts on the design of robust matching algorithms.Another recently investigated approach taking advantage of feature matching consists in exploiting the (possibly sparse) field of integer displacements to provide a coarse but relevant initialization for a variational refinement [59,181,279]. These methods are composed of three steps:1.A feature matching step provides a limited number of candidates at each pixel.These candidates serve as labels for the discrete optimization (see Section 5.2) of a global regularized model.The resulting coarse optical flow field is refined with one of the variational optimization techniques described in Section 5.2.The idea is that steps 1 and 2 handle indifferently small and large displacements of objects at any scale, and the initialization is assumed to be good enough to avoid the coarse-to-fine scheme in step 3. The specificity of [279] is that the three steps are repeated at each level of a multiresolution pyramid. Since steps 2 and 3 have already been discussed in previous subsections, we focus on the specificity of step 1, that is, the production of candidates from feature matching.In [279], the feature matching is only performed at a restricted number of keypoints. After pruning of similar vectors, only a few displacement vectors are retained. Each of these vectors is expanded to produce a global constant flow field, used as candidates for step 2.The approach of [181] is to consider an integer discretization of the two-dimensional motion field. Based on the observation that correlation-based patch matching is able to reproduce coarsely the motion distribution pattern of ground truth motion fields, the discretized motion space is delineated by the matching vectors.The approach of [59] is close to [181] since the idea is to rely on the dominant motion patterns of dense patch matching. In [59], the candidates to feed discrete optimization are obtained by explicitly clustering PatchMatch motion fields [16] to keep only dominant patterns.Another strategy has recently been investigated in [88], exploiting the aggregation paradigm. Feature matching is used as a part of a generation procedure of continuous motion candidates, and the best candidates are then selected in the aggregation step with a global model, without need of subsequent variational refinement.Through the analysis of optical flow literature, we have introduced principles of optical flow modeling and computation, and classified the main methodological aspects of existing methods. From this taxonomy, interests and limitations of each class have been put forward.This study also exhibits the main research directions for improving current state-of-the-art. In terms of performance, it can be considered from results of the Middlebury benchmark [13] that for a range of sequences with small displacements, moderate intensity changes and piecewise smooth displacements of large structures, globally regularized models and joint estimation and segmentation methods are able to obtain satisfying results. However, moving away from these dataset characteristics, as it is the case in recent benchmarks [52,94] or applicative domains [89], unveils the shortcomings of current methods in challenging situations, and it opens the way to potentially large improvements.In particular, some of the main difficulties are caused by large displacements. To avoid the drawbacks of coarse-to-fine schemes of variational methods, an active research direction focuses on the design of computationally tractable discrete optimization methods for the large scale problem of optical flow. We have noticed that dealing with larger displacements brings optical flow closer to feature matching. However, attempts to combine these two worlds still consider them separately and try to fuse their independent results, often leading to high sensitivity to matching errors. Another essential problem lies in large occlusions that are inevitably associated to large motion. Large intensity changes appearing in real environments or in specific applications, or caused by large deformations, is another main problem of current methods. The most promising direction seems to be the spatial adaptation of the data term.Finally, another remaining challenge is to reduce the computational cost of the best performing methods.We observe an evolution of the scope of optical flow, which was originally devoted to image sequences with high frame-rate, for which deviations from small displacements and brightness conservation stayed reasonably low. As this well identified problem becomes better handled, optical flow estimation tends to apply on larger class of sequences. Thus, the gap between optical flow and other correspondence problems like registration or feature matching progressively reduces and opens the way to unified correspondence methods.

@&#CONCLUSIONS@&#
