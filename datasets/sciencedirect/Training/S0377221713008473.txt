@&#MAIN-TITLE@&#
A unified ant colony optimization algorithm for continuous optimization

@&#HIGHLIGHTS@&#
We propose UACOR, a unified ACO algorithm for continuous optimization.UACOR integrates algorithmic components from three previous ACO algorithms.Automatic algorithm configuration automatically derives new ACO algorithms.The high potential of ACO algorithms for continuous optimization is shown.Automatic algorithm configuration is viable for designing continuous optimizers.

@&#KEYPHRASES@&#
Ant colony optimization,Continuous optimization,Automatic algorithm configuration,

@&#ABSTRACT@&#
In this article, we propose UACOR, a unified ant colony optimization (ACO) algorithm for continuous optimization. UACOR includes algorithmic components fromACOR,DACORandIACOR-LS, three ACO algorithms for continuous optimization that have been proposed previously. Thus, it can be used to instantiate each of these three earlier algorithms; in addition, from UACOR we can also generate new continuous ACO algorithms that have not been considered before in the literature. In fact, UACOR allows the usage of automatic algorithm configuration techniques to automatically derive new ACO algorithms. To show the benefits of UACOR’s flexibility, we automatically configure two new ACO algorithms, UACOR-s and UACOR-c, and evaluate them on two sets of benchmark functions from a recent special issue of the Soft Computing (SOCO) journal and the IEEE 2005 Congress on Evolutionary Computation (CEC’05), respectively. We show that UACOR-s is competitive with the best of the 19 algorithms benchmarked on the SOCO benchmark set and that UACOR-c performs superior to IPOP-CMA-ES and statistically significantly better than five other algorithms benchmarked on the CEC’05 set. These results show the high potential ACO algorithms have for continuous optimization and suggest that automatic algorithm configuration is a viable approach for designing state-of-the-art continuous optimizers.

@&#INTRODUCTION@&#
Metaheuristics are a family of optimization techniques that have seen increasingly rapid development and have been applied to numerous problems over the past few years. A prominent metaheuristic is ant colony optimization (ACO). ACO is inspired by the ants’ foraging behavior and it was first applied to solve discrete optimization problems (Dorigo & Stützle, 2004; Dorigo, Maniezzo, & Colorni, 1991, 1996). Only much later, adaptations of ACO to continuous optimization problems were introduced. Socha and Dorigo (Socha & Dorigo, 2008) proposed one of the now most popular ACO algorithms for continuous domains, calledACOR. It uses a solution archive as a form of pheromone model for the derivation of a probability distribution over the search space. Leguizamón and Coello (2010) proposed an extension ofACOR, called DACOR, that had the goal of better maintaining diversity during the search. Subsequently, Liao, Montes de Oca, Aydın, Stützle, and Dorigo (2011) proposed IACOR-LS, an incremental ant colony algorithm with local search for continuous optimization. IACOR-LS uses a growing solution archive as an extra search diversification mechanism and a local search to intensify the search. IACOR-LS was benchmarked on two prominent sets of benchmark functions for continuous optimization, obtaining very good results. These benchmark function sets are the ones proposed for a recent special issue of the Soft Computing journal (Herrera, Lozano, & Molina, 2010; Lozano, Molina, & Herrera, 2011) (we refer to this special issue as SOCO) and the special session on real parameter optimization of the 2005 IEEE Congress on Evolutionary Computation (CEC’05) (Suganthan et al., 2005).In this article, we propose a ACO algorithm for continuous optimization that combines algorithmic components fromACOR, DACORand IACOR-LS. We call this algorithm Unified ACO for continuous optimization (UACOR). It is unified, because from UACOR, we can instantiate the originalACOR, DACORand IACOR-LS algorithms by using specific combinations of the available algorithmic components and parameter settings. However, we can also obtain combinations of algorithm components that are different from any of the already proposed combinations; in other words, from UACOR we can instantiate new continuous ACO algorithms that have not been proposed or tested before.The flexibility of UACOR makes possible the use of automatic algorithm configuration tools to generate new, high-performing continuous ACO algorithms. Here, we follow such an approach and use Iterated F-race (Birattari, Yuan, Balaprakash, & Stützle, 2010), an automatic algorithm configuration tool, as implemented in the irace package (López-Ibáñez, Dubois-Lacoste, Stützle, & Birattari, 2011) for configuring new high-performing ACO algorithms for continuous optimization from UACOR. With automatic configuration tools, algorithm parameters are defined using a kind of machine learning approach in which an algorithm is first trained on a set of problem instances and later deployed. We use as training sets low dimensional versions of the functions in the SOCO and CEC’05 benchmark sets and configure two new ACO variants: UACOR-s is configured on the SOCO benchmark (the -s suffix stands for SOCO) set and UACOR-c on the CEC’05 benchmark set (the -c suffix stands for CEC). UACOR-s and UACOR-c are then tested on higher dimensional versions of the SOCO and CEC’05 benchmark functions. The results show that (i) UACOR-s is competitive or superior to all the 19 algorithms benchmarked on the SOCO function set and that (ii) UACOR-c is superior to IPOP-CMA-ES (Auger & Hansen, 2005) and statistically significantly better than other five recent state-of-the-art algorithms benchmarked on the CEC’05 function set. These experimental results show (i) the high potential of ACO algorithms for continuous optimization and (ii) the high potential of an algorithm design approach that is based on the combination of algorithm frameworks and automatic algorithm configuration. In fact, there are few researches that give evidence for the latter point. For instance, KhudaBukhsh, Xu, Hoos, and Leyton-Brown (2009) proposed SATenstein and instantiated a new state-of-the-art local search algorithm for the SAT problem; López-Ibáñez and Stützle (2010) configured a multi-objective ACO algorithm that outperformed previously proposed multi-objective ACO algorithms for the bi-objective traveling salesman problem; Dubois-Lacoste, López-Ibáñez, and Stützle (2011) configured new state-of-the-art algorithms for five variants of multi-objective flow-shop problems. More recently, the ideas behind the combination of algorithm frameworks and automatic algorithm configuration techniques have been extended to the programming by optimization paradigm (Hoos, 2012). This article is the first to automatically configure a continuous optimizer framework.The article is organized as follows. Section 2 introduces ACO for continuous domains, reviews the three continuous ACO algorithms underlying UACOR, and identifies their algorithmic components in a component-wise view. Section 3 describes UACOR. In Section 4, we automatically configure UACOR to instantiate UACOR-s and UACOR-c and in Section 5, we evaluate their performance. We conclude and give directions for future work in Section 6.The Ant Colony Optimization (ACO) metaheuristic (Dorigo & Stützle, 2004) defines a class of optimization algorithms inspired by the foraging behavior of real ants. In ACO algorithms, artificial ants are stochastic procedure for constructing candidate solution that exploit a pheromone model and possibly available heuristic information on the problem being tackled. The pheromone model consists of a set of numerical values, called pheromones, that are modified at each iteration in order to bias ants toward the most promising regions of the search space; the heuristic information, if available, captures a priori knowledge on the particular problem instance being solved.The main algorithmic components of the ACO metaheuristic are the ants’ solution construction and the update of the pheromone information. “Daemon actions” are procedures that carry out tasks that cannot be performed by single ants. A common example is the activation of a local search procedure to improve an ant’s solution or the application of additional pheromone modifications derived from globally available information about, for example, the best solutions constructed so far. Although daemon actions are optional, they can greatly improve the performance of ACO algorithms.After the initial proposals of ACO algorithms for combinatorial optimization problems (Dorigo & Stützle, 2004; Dorigo et al., 1991, Dorigo, Maniezzo, & Colorni, 1996), several ant-inspired algorithms for continuous optimization problems were proposed (Bilchev & Parmee, 1995; Dréo & Siarry, 2004; Hu, Zhang, & Li, 2008; Hu, Zhang, Chung, Li, & Liu, 2010; Monmarché, Venturini, & Slimane, 2000). However, as explained in Socha and Dorigo (2008), most of these algorithms use search mechanisms different from those used in the ACO metaheuristic. The first algorithm that can be classified as an ACO algorithm for continuous domains isACOR(Socha & Dorigo, 2008). InACOR, the discrete probability distributions used in the solution construction by ACO algorithms for combinatorial optimization are substituted by probability density functions (PDFs) (i.e., continuous probability distributions).ACORuses a solution archive (Guntsch & Middendorf, 2002) for the derivation of these PDFs over the search space. Additionally,ACORuses sums of weighted Gaussian functions to generate multimodal PDFs.Fig. 1shows a sketch of a solution archive and the Gaussian functions that form the PDFs from whichACORsamples values to generate candidate solutions. The solution archive keeps track of a number of complete candidate solutions for a problem, and, thus, it can be seen as an explicit memory of the search history.DACOR(Leguizamón & Coello, 2010) and IACOR-LS (Liao et al., 2011) are two more recent ACO algorithms for continuous optimization, which also use a solution archive and generate PDFs using sums of weighted Gaussian functions. Since the algorithmic components of UACOR are derived from theACOR, DACORand IACOR-LS, the next sections describe their operation.ACORinitializes the solution archive with k solutions that are generated uniformly at random. Each solution is a D-dimensional vector with real-valued componentsxi∈[xmin,xmax], withi=1,…,D. In this paper, we assume that the optimization problems are unconstrained except possibly for bound constraints of the D real-valued variablesxi. The k solutions of the archive are kept sorted according to their quality (from best to worst) and each solutionSjhas associated a weightωj. This weight is calculated using a Gaussian function as:(1)ωj=1qk2πe-(rank(j)-1)22q2k2,whererank(j)is the rank of solutionSjin the sorted archive, and q is a parameter of the algorithm. By computingrank(j)-1, the best solution receives the highest weight.The weights are used to choose probabilistically a guiding solution around which a new candidate solution is generated. The probability of choosing solutionSjas guiding solution is given byωj/∑a=1kωaso that the better the solution, the higher are the chances of choosing it. Once a guiding solutionSguideis chosen, the algorithm samples the neighborhood of the i-th real-valued component of the guiding solutionsguideiusing a Gaussian PDF withμguidei=sguidei, andσguideiequal to(2)σguidei=ξ∑r=1k|sri-sguidei|k-1,which is the average distance between the value of the i-th component ofSguideand the values of the i-th components of the other solutions in the archive, multiplied by a parameterξ. The process of choosing a guiding solution and generating a candidate solution is repeated a total of Na times (corresponding to the number of “ants”) per iteration. Before the next iteration, the algorithm updates the solution archive keeping only the best k of the k+Na solutions that are available after the solution construction process.Different fromACOR, DACORkeeps the number of ants (Na) equal to the solution archive size k and each of the Na ants constructs at each algorithm iteration a new solution. A further difference of DACORwith respect toACORis the specific choice rule for the guiding solutionSguide. With a probabilityQbest∈[0,1], ant j chooses asSguidethe best solution,Sbest, in the archive; with a probability1-Qbest, it chooses asSguidethe solutionSj. A new solution is generated in the same way as inACOR, and then compared toSj(independently of whetherSbestorSjwas chosen as guiding solution). If the newly generated solution is better thanSj, it replacesSjin the archive; otherwise it is discarded. This replacement strategy is different from the one used inACORin which all the solutions in the archive and all the newly generated ones compete.IACOR-LS’s main distinctive features are a solution archive whose size increases over time to enhance the algorithm’s search diversification, and a local search procedure to enhance its search intensification. Additionally, IACOR-LS uses a different rule thanACORfor choosing a guiding solution. At each algorithm iteration of IACOR-LS, the best solution in the archiveSbestis chosen as the guiding solutionSguidewith a probability equal to the value of a parameter EliteQbest∈[0,1]; with a probability of1-EliteQbest, each solution in the archive is used asSguideto generate a new solution. With this choice rule, either only one new solution is constructed by an “elite” guiding solution or k new solutions are constructed by k ants at each algorithm iteration. Each new solution is constructed in the same way as inACOR. Finally,Sguideand the newly generated solution are compared. If the newly generated solution is better thanSguide, it replaces it in the archive; otherwise it is discarded.IACOR-LS initializes the archive with InitAS solutions. Every GrowthIter iterations a new solution is added to the archive until a maximum archive size is reached. The new solution is initialized as follows:(3)Snew=Srand+rand(0,1)(Sbest-Srand),whereSrandis a random solution andrand(0,1)is a random number uniformly distributed in[0,1).IACOR-LS applies at each iteration a local search procedure for LsIter iterations. If the local search improves upon its initial solution, the improved solution replaces the original solution in the archive. The maximum number of times the local search procedure is called from a same initial solution is limited to LsFailures calls. The initial solution for the local search is chosen as follows. The best solution is chosen deterministically if it has been called less than LsFailures times. Otherwise, a random solution from the archive is chosen as the initial solution, excluding all those that already served as initial solutions LsFailures times.The initial step size for the local search procedure is set as follows. First, a solution different from the best one is chosen uniformly at random in the archive. The step size is then set to the maximum norm (||·||∞) of the vector that separates this random solution from the best solution. As a result, step sizes tend to decrease upon convergence of the algorithm and, in this sense, the step sizes are chosen adaptively to focus the local search around the best-so-far solution. In our previous experiments, Powell’s conjugate directions set (Powell, 1964) and Lin-Yu Tseng’s Mtsls1 (Tseng & Chen, 2008) local search methods have shown very good performance.IACOR-LS uses a default restart mechanism that restarts the algorithm and re-initializes the archive of size InitAS with the best-so-far solutionSbestand InitAS-1random solutions. The restart criterion is the number of consecutive iterations, StagIter, with a relative solution improvement lower than a threshold∊. IACOR-LS also integrates a second restart mechanism, which consists in restarting and initializing a new initial archive of size RestartAS (RestartAS is a parameter different from InitAS) withSbestin the current archive and RestartAS-1solutions that are initialized at positions biased aroundSbest; these positions are defined bySbest+10Shakefactor*(Sbest-Srand). The restart criterion is the number of consecutive iterations, StagIter, with a relative solution improvement percentage lower than a certain threshold10StagThresh.We define several algorithmic components for UACOR by abstracting the particular design alternatives taken inACOR, DACORand IACOR-LS. This results in seven main groups of algorithmic components, which are described next, before detailing the outline of UACOR.1.Mode. Two alternative UACOR modes, called DefaultMode and EliteMode, are identified. DefaultMode consists in deploying a number of ants in each algorithm iteration to construct solutions. EliteMode allows in each algorithm iteration to deploy only one “elite” ant with a probability of EliteQbest∈[0,1]. The “elite” ant selectsSbestin the archive asSguideto construct a new solution.Number of ants. Two design choices for defining the number of ants deployed are identified. Na defines the number of ants as an independent parameter (Na⩽k) while NaIsAS defines the number of ants to be equal to k, the size of the solution archive.Choice of guiding solution. This algorithmic component chooses how to selectSguideto sample new solutions. Three design choices are identified: (i)Sbestis selected asSguidewith a probabilityQbest∈[0,1]; (ii)Sguideis probabilistically selected from the solutions in the archive depending on their weight; (iii) solutionSlis selected asSguide, where l is the index of the currently deployed ant.Update of solution archive. The update of the solution archive concerns the replacement of solutions in the archive. We identified three design choices. A parameter RmLocalWorse defines whether UACOR globally removes the Na worst solutions among all k+Na solutions, or whether UACOR makes the decision about the acceptance ofSllocally. In the latter case, we use a parameter SnewvsGsol to decide whether the solution generated by ant l is compared withSguideor with the previous l-th solution to remove the worse one.Local search. We consider four options for the use of a local search procedure. If parameter LsType is set to F (for false), no local search procedure is used. Otherwise, LsType invokes one of three local search methods. As local search methods we considered Powell’s conjugate directions set and Mtsls1, which were already used by IACOR-LS. In addition, in UACOR we also consider the usage of CMA-ES (Hansen & Ostermeier, 1996; Hansen & Ostermeier, 2001; Hansen, Muller, & Koumoutsakos, 2003), which is an evolutionary strategy that also has been considered as a local search method in other algorithm (Molina, Lozano, García-Martínez, & Herrera, 2010).1For inclusion in UACOR, we set the initial population size of CMA-ES to a random size betweenλ=4+⌊3ln(D)⌋and23×λ=4+⌊3ln(D)⌋. The CMA-ES local search procedure is run until one of three stopping criteria (Auger & Hansen, 2005) is triggered. The three stopping criteria use three parameters stopTolFunHist(=10-20), stopTolFun(=10-12) and stopTolX(=10-12); they refer to the improvement of the best objective function value in the last10+⌈30D/λ⌉generations, the function values of the recent generation, and the standard deviation of the normal distribution in all coordinates, respectively.1All three local search procedures use a dynamic calling strategy and an adaptive step size, which follow the choices taken for IACOR-LS.Incremental archive size. The possibility of incrementing the archive size is considered. If parameter IsIncrement is set to F, the incremental archive mechanism is not used. Otherwise, if IsIncrement is set to T (for true), UACOR invokes the incremental archive mechanism.Restart mechanism. Three options for the restart mechanism are identified. If parameter RestartType is set to F, the restart mechanism is not used. Otherwise, RestartType invokes either of the two restart mechanisms, which are introduced in IACOR-LS. They are labeled as 1st and 2nd, respectively.Table 1summarizes the algorithmic components defined above and their options. Some algorithmic components are only significant for specific values of other components. We discuss the connection between these algorithmic components in Section 3.The three ACO algorithms described in the previous section as well as many others that may result from the combination of their components are subsumed under the general algorithmic structure provided by UACOR. In this section, we describe the connections of the algorithmic components of UACOR by a flowchart and show how from UACOR we can instantiate the algorithmsACOR, DACORand IACOR-LS. The flowchart of UACOR is given in Fig. 2. The related parameters are given in Table 2. Some settings take effect in the context of certain values of other settings.UACOR starts by randomly initializing and evaluating the solution archive of size InitAS. Next, UACOR selects a mode, which can be either the default or the elite mode.We first describe the default mode, which is invoked if parameter DefaultMode is set to T (true). At each iteration, Na new solutions are probabilistically constructed by Na ants (recall that and ant in our case is the process through which a solution is generated). If the parameter NaIsAS is set to T, the number of ants is kept equal to the size of the solution archive. If the parameter NaIsAS is set to F (false), a parameter Na,Na⩽k, is activated. Each ant uses a choice rule for the guiding solution. The parameterQbest∈[0,1]controls the probability of usingSbestasSguide. With a probability1-Qbest,Sguideis selected in one of two different ways. If parameter WeightGsol is T,Sguideis probabilistically selected from the solutions in the archive by their weights as defined by Eq. (1). Otherwise, solutionSl(l is associated with the index of the current ant to be deployed) is chosen asSguide. OnceSguideis selected, a new solution is generated. This process is repeated for each of the Na ants. Next, UACOR updates the solution archive by removing Na solutions. If parameter RmLocalWorse is F, UACOR removes the Na worst solutions among all thek+Nasolutions as inACOR. If parameter RmLocalWorse is T, one of two possibilities is considered. If parameter SnewvsGsol is T, each newly generated solution is compared to the correspondingSguideto remove the worse one; otherwise, it is compared to the correspondingSlto remove the worse one. Finally, a new solution archive is generated.The elite mode is invoked if parameter DefaultMode is set to F. The elite mode at each algorithm iteration deploys only one “elite” ant. With a probability EliteQbest,0⩽EliteQbest⩽1, it selectsSbestin the archive asSguide. If the newly generated solution is better than thisSbest, it replaces it in the solution archive; with a probability1-EliteQbestthe solution construction follows the default mode.After updating the solution archive, UACOR sequentially considers three procedures. These are a local search procedure, a mechanism for increasing the archive size and a restart mechanism, respectively. The details of these procedures were described in Section 2.2.3.We use a simple penalty mechanism to handle bound constraints for UACOR. We use(4)P(x)=fes·∑i=1DBound(xi),whereBound(xi)is defined as(5)Bound(xi)=0,ifxmin⩽xi⩽xmax(xmin-xi)2,ifxi<xmin(xmax-xi)2,ifxi>xmaxxminandxmaxare the minimum and maximum limits of the search range, respectively, andfesis the number of function evaluations that have been used so far. For avoiding that the final solution is outside the bounds, the bound constraints are enforced by clamping the final solutionSto the nearest solution on the bounds, resulting in solutionS′ifSviolates some bound constraints. IfS′is worse than the best feasible solution found in the optimization process,S′is replaced by it.We automatically configure UACOR before evaluating its performance on benchmark functions. As the benchmark functions, we employ the 19 functions from the SOCO benchmark set (Herrera et al., 2010) (fsoco1-fsoco19) and the 25 functions from the CEC05 benchmark set (Suganthan et al., 2005) (fcec1-fcec25). Note that in both benchmark sets, the functions allow for different dimensionalities. These two benchmark sets have been chosen as they have become standard benchmark sets for testing continuous optimizers. The SOCO benchmark set was used in a special issue of the journal Soft Computing and it extends the benchmark sets of earlier benchmarking studies on the scaling behavior of continuous optimizers such as the one held at the CEC’08 conference. The CEC’05 benchmark set was introduced in 2005 for a comparison of evolutionary optimizers; its central role is exemplified by the more than 600 citations in google scholar (as of April 2013) to the technical report describing this set of functions (Suganthan et al., 2005). Classified by function characteristics, the SOCO benchmark set consists of seven unimodal and 12 multimodal functions, or, four separable and 15 non-separable functions. The CEC’05 benchmark set consists of five unimodal and 20 multimodal functions, or, two separable and 23 non-separable functions. For a detailed description of the benchmark functions, we refer the reader to (Herrera et al., 2010; Suganthan et al., 2005).In our experiments, we follow the termination conditions suggested for the SOCO and CEC benchmarks (Herrera et al., 2010; Suganthan et al., 2005) to make our results comparable to those of other papers. In particular, we use a maximum of5000×Dfunction evaluations for the SOCO functions, and10,000×Dfor the CEC’05 functions, where D is the dimensionality of a function.For automatically configuring UACOR, we employ Iterated F-Race (Birattari et al., 2010), a method for automatic algorithm configuration that is included in the irace package (López-Ibáñez et al., 2011). Iterated F-Race repeatedly applies F-Race to a set of candidate configurations. F-Race is a racing method that at each iteration applies all surviving candidate configurations to an instance of a combinatorial problem or a function in the continuous optimization case. If a candidate configuration is found to perform statistically worse than others (as determined by the Friedman two-way analysis of variance by ranks and its associated post-tests), it is eliminated from the race. F-race finishes when only one candidate survives or the allocated computation budget to the race is used. Iterated F-Race then samples new candidate configurations around the best candidate configurations found so far. The whole process is repeated for a number of iterations (hence the name Iterated F-Race).The automatic configuration tool handles all parameter types of UACOR: continuous (r), integer (i) and categorical (c). The performance measure used for tuning is the error of the objective function value obtained by the tuned algorithm after a certain number of function evaluations. The error value is defined asf(x)-f(x∗), wherexis a candidate solution andx∗is the optimal solution. In the automatic tuning process, the maximum budget is set to 5000 runs of UACOR. The number of function evaluations of each run is equal to5000×Dfor the SOCO functions, and10,000×Dfor the CEC’05 functions, where D is the dimensionality of a function. The settings of Iterated F-Race that we used in our experiments are the default (Birattari et al., 2010; López-Ibáñez et al., 2011). We apply the automatic configuration process for UACOR two times: once using the SOCO training instances to instantiate UACOR-s, and once using the CEC training instances to instantiate UACOR-c. The input for the training consisted of 19 SOCO benchmark functions of dimension ten sampled in a random order and 25 CEC benchmark functions of dimension ten sampled in a random order.The tuned configurations for UACOR-s and UACOR-c are presented in the central and right part of Table 2. This table also gives the parameter settings for the UACOR’s instantiations ofACOR, DACORand IACOR-Mtsls1. Their parameters were also automatically tuned as mentioned above for the SOCO and CEC’05 benchmark sets, respectively, and for these specific parameter configurations we again use the extensions ’-s’ and ’-c’ depending on the benchmark set used for automatic configuration. Considering that UACOR-s does not use the restart mechanisms of UACOR after tuning and UACOR-c does, when tuning these three ACO algorithms on the SOCO training instances, we deploy them as proposed in the original literature; when tuning them on CEC’05 training instances, we extend them to use the restart mechanisms of UACOR to improve performance.As a further illustration of the respective algorithm structures, we highlight UACOR-s and UACOR-c in the flowchart of UACOR in Fig. 3. Both use DefaultMode, selectSbestasSguidewith a probabilityQbest∈[0,1], use the incremental archive mechanism. UACOR-s uses Mtsls1 local search and UACOR-c uses CMA-ES local search. The parameter settings in which they differ, imply a more explorative search behavior of UACOR-c than that of UACOR-s. In fact, (i) UACOR-c sets the number of ants equal to the size of the solution archive while UACOR-s defines it as an independent parameter (Na⩽k); (ii) UACOR-c frequently chooses all solutions of the archive asSguide(as in DACOR), while UACOR-s probabilistically selectsSguidebased on its weight; (iii) UACOR-c makes a local acceptance decision comparingSltoSguide, while UACOR-s globally removes the Na worst solutions among all k+Na solutions; (iv) UACOR-c uses a restart mechanism for diversifying the search while UACOR-s does not. Considering parameter values, UACOR-c has larger initial archive size, which is consistent with the idea of a stronger exploration than UACOR-s; the larger values ofQbestand GrowthIter would imply UACOR-c and UACOR-s differ. Similar remarks hold also for the settings of the ‘-c’ and ‘-s’ variants ofACOR, DACORand IACOR-Mtsls1. Note that the more explorative settings on the CEC’05 benchmark set are somehow in accordance with the perceived higher difficulty of this benchmark set than the SOCO set. In fact, in the CEC’05 benchmark set the best available algorithms fail to find quasi-optimal solutions much more frequently than in the SOCO benchmark function set.In this section, we evaluate UACOR-s and UACOR-c on the 19 SOCO benchmark functions of dimension 100 and 25 CEC’05 benchmark functions of dimensions 30 and 50. Each algorithm was independently run 25 times on each function. Whenever a run obtains a new best error value, we record the number of function evaluations used, and the new best error value. Following the rules of the SOCO algorithm comparison, error values lower than10-14are approximated to10-14(10-14is the optimum threshold for SOCO functions). For CEC’05 functions, error values lower than10-8are approximated to10-8(10-8is the optimum threshold for CEC’05 functions). We compute the average error obtained by an algorithm on each benchmark function of each dimensionality. These average errors on all test functions in each benchmark set (SOCO or CEC’05) are then used to compare the algorithms’ performance. To analyze the results, we first use a Friedman test at the 0.05α-level to determine whether there are significant differences among the algorithms compared (Conover, 1999). In fact, in all cases the null hypothesis of equal performance is rejected and we then determine the significance of the difference between the algorithms of interest based on the computed minimum difference between the sum of the ranks that is statistically significant.First, we compare UACOR-s with the three ACO algorithms,ACOR-s, DACOR-s and IACOR-Mtsls1-s. The left plot of Fig. 4shows that UACOR-s statistically significantly improves upon the three ACO algorithms on the distribution of average errors across the 19 SOCO benchmark functions. This test is based on the average error values that are reported in Table 3. In fact, on 14 of the 19 functions the average error obtained by UACOR-s is below the optimum threshold, while forACOR-s, DACOR-s and IACOR-Mtsls1-s such low average error values are only obtained 0, 1, and 8 times, respectively. (The main responsible for the large differences between the performance ofACOR-s and DACOR-s on one side and UACOR-s and IACOR-Mtsls1-s on the other side is due to the usage or not of a local search procedure to improve candidate solutions.) The larger number of optimum thresholds reached also is the reason why UACOR-s performs statistically significantly better thanACOR-Mtsls1-s. Only on one function, on which UACOR-s does not reach the optimum threshold, it obtains slightly worse average errors than IACOR-Mtsls1-s.As a next step, we investigate the benefit of the incremental archive mechanism used by UACOR-s when compared to a fixed archive size. The right boxplot of Fig. 4 shows that UACOR-s performs more effective than with archive sizes fixed to 1, 50 and 100, respectively. (Note that for an archive size one, the resulting algorithm is actually an iterated Mtsls1 local search algorithm (Tseng & Chen, 2008).) The differences are statistically significant for the archive sizes 1 and 50, and the average errors of UACOR-s obtain the largest number of times the optimum threshold (14 versus 6, 7 and 9, respectively).Finally, we compare UACOR-s with all 13 candidate algorithms published in the SOCO special issue and with the three algorithms that were chosen as reference algorithms in this special issue.2Information about these 16 algorithms is available at http://sci2s.ugr.es/eamhco/CFP.php.2Recall that IPOP-CMA-ES (Auger & Hansen, 2005) is considered to be a representative of the state-of-the-art for continuous optimization and MA-SSW (Molina, Lozano, & Herrera, 2010; Molina, Lozano, Snchez, & Herrera, 2011) was the best performing algorithm at the CEC’2010 competition on high-dimensional numerical optimization. UACOR-s performs statistically significantly better than these two algorithms and other ten algorithms, as shown in Fig. 5. The best performing algorithm from the SOCO competition is MOS-DE (LaTorre, Muelas, & Pea, 2011), an algorithm that combines differential evolution and the Mtsls1 local search algorithm. It is noteworthy that UACOR-s performs competitive to MOS-DE. Although UACOR-s does not obtain on more functions lower average errors than MOS-DE than vice versa, UACOR-s reaches on more functions the zero threshold (14 versus 13).We next evaluate UACOR-c on the CEC’05 benchmark set of dimension 30 and 50. Tables 4 and 5show the average error values across the 25 CEC’05 benchmark functions obtained by UACOR-c,ACOR-c, DACOR-c, IACOR-Mtsls1-c, IPOP-CMA-ES (Auger & Hansen, 2005) and other five recent state-of-the-art algorithms.Table 4 shows that UACOR-c gives across the 30 and 50 dimensional problems, on more functions lower average errors thanACOR-c, DACOR-c and IACOR-Mtsls1-c than vice versa. Considering the average error values across all these CEC’05 benchmark functions, UACOR-c performs statistically significantly better thanACOR-c, DACOR-c and IACOR-Mtsls1-c.Of particular interest is the comparison between UACOR-c and IPOP-CMA-ES, the data of which are taken from the literature (Auger & Hansen, 2005). The latter is an acknowledged state-of-the-art algorithm on the CEC’05 benchmark set. UACOR-c shows superior performance to IPOP-CMA-ES and it gives on more functions lower average errors than IPOP-CMA-ES than vice versa. The average error values that correspond to a better result between UACOR-c and IPOP-CMA-ES are highlighted in Table 4.As a final step, we compare UACOR-c with five recent state-of-the-art continuous optimization algorithms published since 2011. These reference algorithms include HDDE (Dorronsoro & Bouvry, 2011), Pro-JADE (Epitropakis, Tasoulis, Pavlidis, Plagianakos, & Vrahatis, 2011), Pro-SaDE (Epitropakis et al., 2011), Pro-DEGL (Epitropakis et al., 2011) and ABC-MR (Akay & Karaboga, 2012). In the original literature, these algorithms were tested on the CEC’05 benchmark set for which the parameter values of the algorithms were either set by experience or they were manually tuned. We directly obtain the data of the five algorithms on the CEC’05 benchmark set from the original papers. Table 5 shows that UACOR-c gives on the 30 and 50 dimensional problems on more functions lower average errors than each of these five state-of-the-art algorithms. For each algorithm, Table 6summarizes the average ranking, the number of times the optimum thresholds is reached and the number of lowest average error values obtained across all six algorithms that are compared. UACOR-c obtains the best average ranking, the highest number of optimum thresholds and it is the best performing algorithm for most functions. The differences between the best ranked algorithm UACOR-c and the other five state-of-the-art algorithms are found to be statistically significant.Finally, one may be interested how UACOR-s and UACOR-c compare on the SOCO and CEC’05 sets, respectively. Fig. 6illustrates these results using correlation plots, where each point corresponds to the average error measured for UACOR-c (x-axis) and UACOR-s (y-axis), respectively. A point below (above) the diagonal indicates better performance for the algorithm on the y-axis (x-axis). From these correlation plots, we can clearly observe that UACOR-s performs statistically significantly better than UACOR-c on the SOCO benchmark set and that UACOR-c performs statistically significantly better than UACOR-s on the CEC’05 benchmark set. Clearly, there is no best algorithm across the two benchmark sets. The main underlying reason is probably that the CEC’05 benchmark set contains many rotated functions on which the CMAES local search excels, while CMAES performs poorly on the SOCO benchmark functions (see also poor performance of IPOP-CMA-ES in Fig. 5; IPOP-CMA-ES couples CMA-ES with a simple restart mechanism that increases the initial population size to be used in the CMA-ES local search). However, our goal is not to propose one specific algorithm, but rather a framework that in combination with an automatic parameter tuning method enables the automatic synthesis of high-performance ACO algorithms for a particular class of problems. In the article we have shown that this approach obtains state-of-the-art results on two very different benchmark function sets, which is something no other algorithm of the more than 20 used as a reference in this article is able to do.

@&#CONCLUSIONS@&#
In this article, we proposed UACOR, a unified ant colony optimization algorithm that integrates components from three previous ACO algorithms for continuous optimization problems,ACOR(Socha & Dorigo, 2008), DACOR(Leguizamón & Coello, 2010) and IACOR-LS (Liao et al., 2011). UACOR is flexible and it allows the instantiation of new ACO algorithms for continuous optimization through the exploitation of automatic algorithm configuration techniques. In this way, we can generate from the available algorithmic components new ACO algorithms that have not been considered or tested before. In the experimental part of this article, we have shown that by instantiating UACOR by automatic algorithm configuration tools we can effectively obtain new, very high performing ACO algorithms for continuous optimization. In particular, the computational results showed that the automatically configured UACOR algorithms obtain statistically significantly better performance than the tuned variants of the three ACO algorithms that underly UACOR, namelyACOR, DACORand IACOR-LS on each of the benchmark sets we considered. When UACOR is automatically configured for the SOCO benchmark set, it is competitive or statistically significantly better performing than all recent 19 algorithms benchmarked on this benchmark set; when configured for the CEC’05 benchmark set, it performs superior to IPOP-CMA-ES, the acknowledged state-of-the-art algorithm on this benchmark set and statistically significantly better than other five recent high-performance continuous optimizers that were evaluated on this benchmark set. In a nutshell, in this paper we have proven the high potential ACO algorithms have for continuous optimization and the high potential automatic algorithm configuration has to develop continuous optimizers from algorithmic components.The work presented here can be extended along several directions. A first direction is to extend further the available components in UACOR. Examples are to synthesize other probability density functions for the generation of candidate solutions, to include alternative ways of handling the archive and constraint handling techniques for tackling constrained continuous optimization problems, and the consideration of also other local search algorithms. Another promising direction would be to design a more general algorithm framework from which different types of continuous optimizers other than ACO algorithms can be automatically configured. This may lead to ultimately more powerful continuous optimization techniques. Another direction would be to consider the automatic configuration of continuous optimizers for more specific classes of functions and to combine these optimizers in the form of algorithm portfolios or through the exploitation of techniques for algorithm selection. Finally, for the case of very expensive functions, where the evaluation of a single solution may take many hours or more, it would be useful to include surrogate modeling techniques into UACOR.