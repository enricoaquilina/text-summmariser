@&#MAIN-TITLE@&#
Improved medical image modality classification using a combination of visual and textual features

@&#HIGHLIGHTS@&#
Medical modality classification using visual and textual features.Best performing features are the SIFT and opponentSIFT features.Low-level fusion of the visual features slightly improves the predictive performance.Visual and textual features combined with high-level fusion further improve the predictive performance.

@&#KEYPHRASES@&#
Image modality classification,Visual image descriptors,Feature fusion,

@&#ABSTRACT@&#
In this paper, we present the approach that we applied to the medical modality classification tasks at the ImageCLEF evaluation forum. More specifically, we used the modality classification databases from the ImageCLEF competitions in 2011, 2012 and 2013, described by four visual and one textual types of features, and combinations thereof. We used local binary patterns, color and edge directivity descriptors, fuzzy color and texture histogram and scale-invariant feature transform (and its variant opponentSIFT) as visual features and the standard bag-of-words textual representation coupled with TF-IDF weighting. The results from the extensive experimental evaluation identify the SIFT and opponentSIFT features as the best performing features for modality classification. Next, the low-level fusion of the visual features improves the predictive performance of the classifiers. This is because the different features are able to capture different aspects of an image, their combination offering a more complete representation of the visual content in an image. Moreover, adding textual features further increases the predictive performance. Finally, the results obtained with our approach are the best results reported on these databases so far.

@&#INTRODUCTION@&#
Large collections of medical images have become a valuable source of knowledge, taking an important role in education, medical research and clinical decision making. One of the main problems is that the size of the collections is in constant growth due to the increasing availability of imaging equipment in hospitals. Average-sized radiology departments now produce several tera-bytes of data annually. This generates huge repositories of valuable information, which in many cases is difficult to process and manage appropriately. This prompts for development of tools for efficient and effective access to this type of information.Medical image databases are typically accessed via textual information through the standard Picture Archiving and Communication System (PACS) [1,2]. PACS integrates imaging modalities and interfaces with hospital and departmental information systems to manage storage and distribution of images to medical personnel, researchers, clinics, and imaging centers. The task of indexing and cataloging these collections has been traditionally performed manually. This is an expensive and time-consuming procedure, and it is also prone to errors. Consequently, there is a strong need for automated indexing of medical image collections in order to improve the ability to search for and retrieve relevant images [3].Medical image retrieval systems have traditionally been text-based, relying on the annotation or captions associated with the images as the input to the retrieval system. In the last few decades, several advancements in the area of content-based image retrieval (CBIR) have been made [4,5]. CBIR systems have had some success in fairly constrained medical domains, such as pathology [6], head MRIs [7], lung CTs [8], and mammograms [9]. Furthermore, combining both textual and visual techniques improves the retrieval performance over using them individually [3,10]. The queries, in that case, consist of a textual part (i.e., textual sub-query) and/or sample images (i.e., visual sub-query). For example, the queries could contain information about patients’ demographics, a limited set of symptoms and medical examination results including imaging studies.Medical image databases used for retrieval or for teaching purposes often contain images of many different modalities, such as X-ray, CT scan, ultrasound, etc. An additional complication is that these images are typically taken under different conditions and the accuracy of their annotations is variable and inconsistent [11]. This is especially true for images found in various on-line resources, including those that access the on-line content of journals.Image modality is a fundamental visual characteristic of an image and can be exploited for improving retrieval performance. However, the annotations or captions associated with images often do not capture information about the modality. The DICOM header contains tags to decode the body part examined, the patient position and the modality [12]. Some of the tags are automatically set by the digital imaging system according to the imaging protocol used to capture the pixel data. Other tags are set manually by the physicians or radiologists during routine documentation. This procedure cannot always be considered very reliable, since it frequently happens that some entries are either missing, false, or do not describe the anatomic region precisely [13].The medical retrieval task in ImageCLEF has provided both a forum as well as test image collections to benchmark image retrieval techniques. Over the years, our group has participated in different subtasks of this medical task including the subtasks of automatic medical image annotation [14], medical modality classification, compound figure separation, ad-hoc image-based retrieval and case-based retrieval [3,10]. In this paper, we present in detail the results of applying our approach to modality classification for the competitions organized in 2011, 2012 and 2013.In the modality classification task at the ImageCLEF competition, the examples are images from medical articles. The goal is to correctly classify the modality of the images using the visual information from the images and the text from the article where this image is encountered. In this work, we extensively compare the performance of 4 different techniques for feature extraction from images: local binary patterns (LBP) [15], the color and edge directivity descriptor (CEDD) [16], fuzzy color and texture histograms (FCTH) [17] and the scale-invariant feature transform (SIFT) with its variant opponentSIFT (OSIFT) [18,19]. Next, we evaluate the performance of textual features extracted from the text surrounding the images by using the standard bag-of-words representation together with the TF-IDF weighting [20].The main focus of this work is to first explore which visual features extraction technique captures the most relevant information about the medical image modality. Second, we investigate whether the combination of the different visual features improves the predictive performance. Next, we compare the performance of visual and textual features in the context of medical image modality classification. Finally, we investigate whether combining visual and textual features improves the performance and yields state-of-the-art performance.The reminder of this paper is organized as follows. Section 2 presents the task of modality classification at ImageCLEF competitions. The visual and textual feature extraction techniques are described in Section 3. The specific experimental setup used to evaluate the feature extraction techniques is outlined in Section 4. Section 5 discusses the results from the experiments and compares the different evaluation scenarios. Finally, Section 6 states our conclusions.For medical retrieval purposes, imaging modality is an important aspect of the image. In user studies, clinicians have indicated that modality is one of the most important filters that they would like to be able to limit their search by [3]. The usage of modality information often significantly improves the retrieval results.The ImageCLEF medical modality classification task is a standardized benchmark for systems that automatically classify medical image modality from PubMed journal articles. This task was first introduced in ImageCLEF 2010, when the total number of modalities was eight. In ImageCLEF 2011, the number of modalities was expanded to 18. The ImageCLEF 2012 and 2013 dataset have 31 classes (same number of classes and same classification hierarchy): However, in ImageCLEF 2013, a larger number of compound/multipane images (i.e., images that contain figures of several types) is present. This makes the task significantly harder, but corresponds much more closely to the reality of biomedical journals [3].In this work, we focus on the databases used for the competitions from 2011 until 2013. We made this choice primarily because of the data availability. Namely, our research groups participated at the competitions held in these years. In addition, the task for 2010 is relatively simple: the accuracy of the top ranked system was very high 94%. Nevertheless, the methods we present here can also be easily applied to the database from 2010.The modality classes used for the competitions in 2012 [10] and 2013 [3] are given in Fig. 1. The competition in 2011 [21] contained in total 18 classes: 3D reconstruction, angiography, compound figure (more than one type of image), computed tomography, dermatology, drawing, electron microscopy, endoscopic imaging, fluoresence, gel, graphs, gross pathology, histopathology, magnetic resonance imaging, general photo, retinography, ultrasound and X-ray. In this work, we did not exploit the hierarchy of the classes, but rather treated the problem as a multi-class classification task.The benchmark databases consist of different numbers of training images and different numbers of testing images, distributed differently across the modalities. These databases are briefly summarized in Table 1. Finally, Fig. 2shows sample images from the ImageCLEF 2013 database used in our experiments.Collections of medical images typically contain various images obtained using different imaging techniques. In order to properly represent the images, different feature extraction techniques that are able to capture the different aspects of an image (e.g., texture, shapes, color distribution…) need to be used [14]. Texture is especially important, because it is difficult to classify medical images using shape or gray level information. Effective representation of texture is needed to distinguish between images with equal modality and layout. Furthermore, local image characteristics are fundamental for image interpretation: while global features retain information on the whole image, the local features capture the details. They are thus more discriminative concerning the problem of inter and intra-class variability [22].Our approach to medical modality classification uses different visual features combined with textual features extracted from the surrounding text content of the images. More specifically, as visual features, we used local binary patterns, color and edge directivity descriptors, fuzzy color and texture histograms and (opponent) scale-invariant feature transform descriptors. For the textual information, we used the text surrounding the image and its bag-of-words representation. We made the obtained descriptors publicly available for download at http://kt.ijs.si/DragiKocev/imageclef/descriptors.zip. In the remainder, we briefly describe these feature extraction techniques.Local binary patterns (LBP) are one of the best representations of texture content in images [15]. They are invariant to monotonic changes in gray-scale images and fast to compute. Furthermore, they are able to detect different micro patterns, such as edges, points and constant areas.The basic idea behind the LBP approach is to use the information about the texture from a local neighborhood. First, we define the radius R of the local neighborhood under consideration. The LBP operator then builds a binary code that describes the local texture pattern in the neighborhood set of P pixels. The binary code is obtained by applying the gray value of the neighborhood center as a threshold. The binary code is then converted to a decimal number which represents the LBP code. Formally, given a pixel at position (xc, yc) the resulting LBP code can be expressed as follows:(1)LPB(P,R)(xc,yc)=∑n=0P−1S(in−ic)2nwhere n ranges over the P neighbors of the central pixel (xc, yc), icand inare the gray-level values of the central pixel and the neighbor pixel, and S(x) is defined as:(2)S(x)=1,ifx ≥ 00,otherwiseThe image is traversed with the LBP operator pixel by pixel and the outputs are accumulated into a discrete histogram.Not all LBP codes are informative. Certain LBP codes capture fundamental properties of the texture and are called uniform patterns because they constitute the vast majority, sometimes over 90 percent, of all patterns present in the observed textures [15]. These patterns have one thing in common, namely, a uniform circular structure that contains very few spatial transitions. They function as templates for micro-structures such as bright spots, flat areas or dark spots.To spatially enhance the descriptors and improve the performance, it has been suggested to repeatedly sample predefined sub-regions/sub-images of an image (e.g., 1×1, 2×2, 4×4, etc.) [3]. The descriptors from the different sub-images are then aggregated/concatenated into one spatially enhanced descriptor. Following this suggestion, we divide the images into a number of non-overlapping sub-images and concatenate the LBP histograms extracted for each sub-image into a single, spatially enhanced feature histogram. This approach aims at obtaining more local description of the images. Fig. 3shows how we build the LBP histogram with 16×243=3888 bins in total for each image. The optimal parameters of the LBP approach, such as neighborhood size, neighborhood radius and spatial pyramids, were determined with a set of extensive experiments. The results of the experiments are outlined in Section 4.2.1.The color and edge directivity descriptor (CEDD) includes color and texture information into a single histogram [16]. The CEDD histogram consists of 6 regions/bins, determined by the texture information. Each region further contains 24 individual regions/bins, emanating from the color information. Consequently, the final histogram includes 6×24=144 regions/bins.In order to compute the histogram, the image is divided into image blocks [16], and for each image block the texture and color information is calculated as follows. The texture information is extracted by using the five digital filters (i.e., edges present in a given region) proposed by the MPEG-7 edge histogram descriptor (EHD) [23,24]: vertical, horizontal, 45-degree diagonal, 135-degree diagonal and non-directional edges. Hence, the texture information is represented with a histogram of six bins, five of them corresponding to the types of edges found in the image plus one for no edges of any type found.The color information is extracted by processing the image blocks in the HSV color space. First, a fuzzy system produces a fuzzy linking histogram that takes the three HSV channels as inputs and creates a histogram with 10 bins as output. Each of the bins represents a preset color: black, gray, white, red, orange, yellow, green, cyan, blue and magenta.Next, the histogram is expanded into a color histogram with 24 bins by using coordinate logic filters for vertical edge detection as follows. The Hue channel is divided into 8 regions: red to orange, orange, yellow, green, cyan, blue, magenta, and blue to red; Saturation is divided into two fuzzy regions defining the shade of a color based on white; The value channel is divided into three areas: one defines when the block will be black and the other two, in combination with Saturation, when it will be gray. Finally, considering these divisions, a set of 4 fuzzy-like rules are applied transforming the previous 10 color histograms into a 24 color bin histogram comprising black, gray, white, dark red, red, light red, dark orange, orange, light orange, dark yellow, yellow, light yellow, dark green, green, light green, dark cyan, cyan, light cyan, dark blue, blue, light blue, dark magenta, magenta, and light magenta.Each image block interacts successively with all the fuzzy systems. Defining the bin produced by the texture information fuzzy system as n and the bin produced by the 24-bin fuzzy-linking system as m, each image block is placed in the bin position: n×24+m.Fuzzy color and texture histogram (FCTH) includes the texture information produced in the eight-bin histogram of the fuzzy system that uses the high frequency bands of the Haar wavelet transform [17]. For color information, the descriptor uses the 24-bin color histogram produced by the 24-bin fuzzy-linking system [17]. Overall, the final histogram includes 8×24=192 regions/bins.The image is first segmented into a predefined number of image blocks. Each image block is transformed into the YIQ color space, and then transformed with the Haar Wavelet transform. The fLH, fHLand fHHvalues are calculated and with the use of the fuzzy system that classifies the f coefficients, this image block is classified into one of the eight output bins.Next, the same image block is transformed into the HSV color space and the mean H, S and V block values are calculated. These values become inputs to the fuzzy system that forms the ten-bin fuzzy color histogram. Then, the next fuzzy system uses the mean values of S and V as well as the position number of the bin (or bins) resulting from the previous fuzzy ten-bin unit, to calculate the hue of the color and create the fuzzy 24-bin histogram. The process is repeated for all the blocks of the image. This descriptor is quite similar to the CEDD, but it captures the texture information through the Haar Wavelet transform.The scale-invariant feature transform (SIFT) image descriptors employ the bag of features approach commonly used in many state-of-the-art approaches in image classification [3,25]. The basic idea of this approach is to sample a set of local image patches using some method (densely, randomly or using a key-point detector) and calculate a visual descriptor on each patch (SIFT descriptor, normalized pixel values). The resulting set of descriptors is then matched against a pre-specified visual codebook, which converts it to a histogram. The main issues that need to be considered when applying this approach are: sampling of patches, selection of visual patch descriptors and building a visual codebook.We use dense sampling of the patches, which samples an image grid in a uniform fashion using a fixed pixel interval between patches. We use an interval distance of 6 pixels and sample at multiple scales (σ=1.2 and σ=2.0 for the Gaussian filter [19]). Due to the low contrast of some of the medical images (e.g., radiographs), it would be difficult to use any detector for points of interest. We calculate SIFT and opponentSIFT (OSIFT) descriptors for each image patch [18,19,26]. OpponentSIFT describes all the channels in the opponent color space using SIFT descriptors. The information in the O3 channel is equal to the intensity information, while the other channels describe the color information in the image. These other channels do contain some intensity information, but due to the normalization of the SIFT descriptor they are invariant to changes in light intensity [19].The crucial aspects of a codebook representation are the codebook construction and assignment. An extensive comparison of codebook representation variables is given by van Gemert et al. [27]. We employ k-means clustering (a custom implementation in the C programming language was used) on 250K randomly chosen descriptors from the set of images available for training. k-means partitions the visual feature space by minimizing the variance between a predefined number of k clusters. Here, we set k to 1000 and thus define a codebook with 1000 codewords [22].The images from the ImageCLEF 2013 database are taken from medical articles and can be indexed using the surrounding text content (article title, article abstract, full text or image captions). The text representation adopted in this work includes information from the title of the article and the image caption, which can be found in the XML representation of the corresponding article. This constitutes the text corpus for the image collection. Next, standard text processing operations are applied, including tokenization, stemming, and stop-word removal using Terrier IR[20], which is a high performance and scalable information retrieval platform. We calculate the weight for each term in each medical article using the standard TF-IDF weighting model. The calculated weights were L2 normalized and used as textual features for the corresponding images.In this section, we present the experimental setup we used to evaluate the proposed feature extraction methods for medical image modality classification. First, we present the learning algorithm/classifier that was used in our experiments. Next, we give the procedure for the selection of the optimal spatial resolution of the visual descriptors. We then define the feature fusion schemes used to improve the predictive performance of the image descriptions. Finally, we state the experimental questions that we investigate in this study.For classification, we used the libSVM implementation of support vector machines (SVMs) [28] with probabilistic output [29]. To solve the multi-class classification problems, we employ the one-vs-all approach. Namely, we build a binary classifier for each modality/class: the examples associated with that class are labeled positive and the remaining examples are labeled negative. This results in an imbalanced ratio of positive versus negative training examples. We resolve this issue by adjusting the weights of the positive and negative class [19]. In particular, we set the weight of the positive class to ((#pos+#neg)/#pos) and the weight of the negative class to ((#pos+#neg)/#neg), with #pos the number of positive instances and #neg the number of negative instances in the train set.The nature of the visual descriptors and the textual descriptors differs significantly: The textual features are very sparse. This requires that a different classifier is used to compensate for this. Consequently, SVMs for the visual features as examples were trained with a χ2 kernel. For the textual features, we used SVMs with a precomputed kernel obtained using the cosine similarity as a distance measure over the L2 normalized tf-idf weights. We optimize the cost parameter C of the SVMs using an automated parameter search procedure. For the parameter optimization, we separate 20% of the training set and use it as validation set. After finding the optimal value for C, the SVM is finally trained on the whole set of training images and evaluated on the test images.To assess the performance of the classifiers, we use the overall recognition rate/accuracy. This is a very common and widely used evaluation measure. It is calculated as the fraction of the test images whose class/modality was predicted correctly.Preliminary experiments were performed to select the optimal parameters for the feature extraction methods. These include the parameters P (number of neighbors) and R (radius of neighborhood) in LBPs and number of subimages/spatial pyramids for the other descriptors. The experiments were performed in the following fashion.We generated visual descriptors for the train and test images using different number of sub-images (grid layout) and different number of neighbors and different values for the radius of the neighborhood for LBPs. As in the other experiments, we optimized the C parameter in the SVMs by separating 20% of the training set and use it as validation set. After finding the optimal value for C, the SVM was finally trained on the whole set of training images and evaluated on the test images. The optimal number of sub-images for each visual descriptor was selected using the evaluation results. The descriptors with the selected optimal spatial pyramids were included in the concatenated visual descriptor. In the remainder, we briefly outline the results of the preliminary experiments for each feature extraction method.In our experiments, we used the patternsLBP16,2u2, where the superscript u2 reflects the use of uniform patterns that have a U value of at most 2 on a neighborhood of size 16 and radius 2. The U value is the number of spatial transitions (bitwise 0/1 changes) in the pattern. The non-uniform patterns (patterns that have U value larger than 2) are grouped under one bin in the resulting histogram. With theLBP16,2u2operator, the number of bins in the histogram is 243 (242 bins for uniform patterns and one bin for non-uniform/noisy patterns). TheLBP16,2u2patterns were selected based on preliminary experiments involving the use of several different types of LBP patterns. The results from these experiments are presented in Table 2.Increasing the number of neighborhood pixels increases the performance of the LBP descriptor: For example, increasing the number from 8 to 16 increases the performance by 3% to 8% over the three different databases. Further increase of the number of local neighbors in the pattern to 24 increases the performance slightly (on average 1%) at the price of a significant increase of the dimensionality of the descriptors (up to 555). More importantly, the efficiency of the procedure for calculation of the descriptor is severely affected (the processing time per image is increased slightly more than 4 times). Because all of this, we have selected theLBP16,2u2pattern for use in our further experiments.To spatially enhance the descriptors and improve the performance, it has been suggested to repeatedly sample predefined sub-regions/sub-images of an image (e.g., 1×1, 2×2, 4×4, etc.). The optimal number of sub-images was found by extensive experiments using different numbers of sub-images. The results are presented in Table 3. From the results, we can see that increasing the number of sub-images over 16 results in a slight decrease of predictive performance. Considering all of this, we selected to useLBP16,2u2pattern coupled with 4×4 spatial pyramid.To further spatially enhance the descriptors and improve the performance, we applied the same technique as for the LBP descriptor: the images are divided into non-overlapping sub-images and the CEDDs for each sub-image are concatenated into a single feature histogram (see Fig. 3). To select the optimal number, we have conducted experiments with different numbers of sub-images, starting from one (entire image) and going to 4, 9, 16, 25, 36, 49, 64 sub-images. The results are presented in Table 4. The optimal number of sub-images is 36 (grid layout 6×6). The total number of bins is therefore 6×6×144=5184.For this descriptor, the images are also divided into 6×6 non-overlapping sub-images to obtain a spatially enhanced feature histogram (as for LBP and CEDD, illustrated in Fig. 3). Hence, the total number of bins in this case is 6×6×192=6912 (36 sub-images with 192 bins each). For this descriptor, we have also conducted extensive experiments with different numbers of sub-images in order to select the optimal number. The results are presented in Table 5.Dense sampling gives an equal weight to all key-points, irrespective of their spatial location in the image. To overcome this limitation, we follow the spatial pyramid approach proposed by Lazebnik et al. [30]. The image is repeatedly sampled into fixed sub-regions, e.g., 1×1, 2×2, 4×4, etc., and the different resolutions are aggregated into a so called spatial pyramid. Since every region is an image, the spatial pyramid can easily be used in combination with dense sampling. For the ideal spatial pyramid configuration, Lazebnik et al. [30] claim that 2×2 is sufficient, while Marszałek et al. [31] suggest to also include 1×3. We investigate multiple divisions of the image in our experiments. The results from the experiments are presented in Table 6. For the Image CLEF 2011 dataset, the best results are obtained by using the histogram from the entire image. Inclusion of spatial pyramids results in lower predictive performance for both the SIFT and OSIFT descriptors. For the Image CLEF 2012 and 2013 datasets, the inclusion of the spatial pyramids results in increased predictive performance of the classifiers for both local descriptors. Here, we have chosen to use the histogram that includes the spatial pyramids. The resulting vector in this case is with 8000 bins ((1×1 + 2×2 + 1×3)×1000), and was obtained by concatenation of the eight histograms. Fig. 4shows an example of the histograms extracted from an image for the spatial pyramids of 1×1, 2×2 and 1×3.Several studies have shown that, taken together, various visual features bringing different information about the visual content of the images clearly outperform single feature approaches [32,22,14]. Furthermore, the textual features also bring information that can be exploited by the classifiers. Following these findings, we combine the different visual and textual features described above.We use the two different feature fusion schemes depicted in Fig. 5: low level (LL) and high level (HL). For the low level feature fusion scheme, the descriptors are concatenated into a single feature vector and a classifier is trained on the joint feature vector. The high level fusion scheme averages the predictions from the individual classifiers trained on the separate descriptors.The low-level feature fusion scheme is used for the different visual features because it performs slightly better than high-level feature fusion for medical image annotation [14]. Because of the different nature of the visual and the textual descriptors, we learn classifiers for the concatenated visual features and the textual features separately, the final predictions are obtained by averaging the individual predictions from the two classifiers. The weight for the predictions of the classifier which is using the visual features was set to 0.5 and the weight for the predictions of the classifier learned with textual features was set also to 0.5. These weights were determined by using the optimization procedure described above.The goal of the experimental evaluation is to investigate the following research questions:1.Which visual descriptor is most suitable for medical modality classification?Does combining multiple visual features improve the predictive performance?Which features are better for medical modality classification: visual features or text features?Does the use of text features in fusion with visual features increase the predictive performance?To find out which feature extraction technique is most suitable for medical modality classification (question 1), we compare the predictive performance figures of the classifiers constructed using each type of visual descriptor separately. We then investigate whether the combination of feature extraction techniques can increase the predictive performance of the constructed classifiers (question 2). For the third question, we compare the performance of the classifier constructed using the best visual descriptor to the performance of the classifier constructed using the textual features. Finally, for the last question, we compare the predictive performance of the classifiers constructed without and with text features.

@&#CONCLUSIONS@&#
In this paper, we present in detail the approach that we used for the medical modality classification task at the ImageCLEF evaluation forum for the cross-language annotation and retrieval of images.We evaluated several types of visual and textual features, and combinations thereof. More specifically, we used the modality classification databases from the ImageCLEF competitions in 2011, 2012 and 2013. We used LBP, FCTH, CEDD, SIFT and opponentSIFT as visual features and a standard bag-of-words textual representation coupled with TF-IDF weighting.The results from the experiments reveal that the best performing features for modality classification are the SIFT and opponentSIFT features. Next, the low-level fusion of the visual features slightly improves the predictive performance of the classifiers. This is because the different features are able to capture different aspects of an image, and thus, their combination offers a more complete representation of the visual content of an image. Furthermore, we investigate adding textual features: Due to the different nature of the visual and textual features and the sparsity of the textual features, these are combined with high-level fusion. This further increases the predictive performance by a significant amount. Finally, the results obtained with our approach are the best results reported thus far on these challenging databases.