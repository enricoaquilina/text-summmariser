@&#MAIN-TITLE@&#
A uniform approach for programming distributed heterogeneous computing systems

@&#HIGHLIGHTS@&#
libWater programming model, which extends OpenCL with a simplified interface.A lightweight distributed runtime system based on asynchronous command execution.A powerful representation that collects and arranges dependencies between commands.Dynamic Collective Replacement and Device-Host-Device Copy Removal optimizations.A study of the performance of the library on three compute clusters.

@&#KEYPHRASES@&#
OpenCL,MPI,Distributed computing,Heterogeneous computing,Programming model,Runtime system,

@&#ABSTRACT@&#
Large-scale compute clusters of heterogeneous nodes equipped with multi-core CPUs and GPUs are getting increasingly popular in the scientific community. However, such systems require a combination of different programming paradigms making application development very challenging.In this article we introduce libWater, a library-based extension of the OpenCL programming model that simplifies the development of heterogeneous distributed applications. libWater consists of a simple interface, which is a transparent abstraction of the underlying distributed architecture, offering advanced features such as inter-context and inter-node device synchronization. It provides a runtime system which tracks dependency information enforced by event synchronization to dynamically build a DAG of commands, on which we automatically apply two optimizations: collective communication pattern detection and device-host-device copy removal.We assess libWater’s performance in three compute clusters available from the Vienna Scientific Cluster, the Barcelona Supercomputing Center and the University of Innsbruck, demonstrating improved performance and scaling with different test applications and configurations.

@&#INTRODUCTION@&#
Ease of programming and best performance exploitation are two conflicting goals while designing programming models and abstractions for high performance computing (HPC). For instance, when programming a compute cluster, better performance can be obtained directly using low level and error prone communication layers like MPI  [27]. Alternatively, high level models like domain specific languages and frameworks can be employed to simplify the programmability and portability of the code. This simplification however brings also a loss of performance due to the level of abstraction that is too far away from the underlying hardware.The recent arise of multi- and many-core CPUs, next to special purpose hardware and accelerators such as GPUs, made this trade-off even more challenging. In fact, heterogeneous architectures require an intricate and complex mix of programming models such as CUDA, OpenMP and pthreads, in order to handle the diversity of execution environments and programming models.The Open Computing Language (OpenCL—[21]) is a partial solution to the problem. It introduces an open standard for general-purpose parallel programming of heterogeneous systems, which has been implemented by many vendors such as Adapteva, Altera, AMD, ARM, Intel, Imagination Technologies, NVIDIA, Qualcomm, Vivante and Xilinx. An OpenCL program comprises a host program and a set of kernels intended to run on a compute device. It also includes a language for kernel programming, and an API for transferring data between host and device memory and for executing kernels. Therefore, OpenCL is a big leap forward in order to assure portability between different hardware, potentially replacing standards like OpenMP and CUDA, but it also presents some limitations. A first problem is that it does not allow interactions between different platforms; for example, it is not possible to use event synchronization between devices from different vendors. Secondly, the semantics of OpenCL host applications is somewhat too verbose, as it includes different levels of abstraction (platform, device and context). Moreover, while writing an application targeting e.g. a cluster of heterogeneous nodes, we still require an intricate mix of OpenCL with a communication layer like MPI. Despite OpenCL can be easily extended in order to support remote, distributed devices (attempts in this direction are  [22,1,20,11]), the host-device paradigm forces the use of a centralized communication pattern, which is a strong limitation for scaling on large-scale compute clusters. In this article, we introduce libWater, a library-based extension of the OpenCL programming paradigm that simplifies the development of applications for distributed heterogeneous architectures. libWater  aims to improve both productivity and implementation efficiency addressing all the problems listed above. libWater  does not alter the kernel logic of OpenCL kernels, but replaces the host-side API with a new, simpler and transparent interface which abstracts the underlying distributed architecture.The main contributions of this article are:•The libWater  programming model, which extends the OpenCL standard by replacing the host code with a simplified and concise interface. It defines a novel device query language (DQL) for OpenCL device management and discovery, and introduces new features such as inter- and intra-context synchronization.A lightweight distributed runtime environment, which dispatches the work between remote devices, based on asynchronous execution of both communications and OpenCL commands. libWater  runtime also collects and arranges dependencies between commands in the form of a powerful representation called command DAG.Two effective uses of the command DAG in order to improve scalability: (a) a Dynamic Collective Replacement (DCR) optimization, which identify collective communication patterns and replaces them with MPI collective operations; (b) a Device-Host-Device Copy Removal (DHDCR), where device-device communications supersedes device-host-device ones. Both optimizations overcome the limitation of the OpenCL host-device semantic, improving scalability on large-scale compute clusters.A study of the scalability of libWater  on two real production clusters using up to 64 devices. Results show high efficiency and demonstrate the suitability of the presented command DAG optimizations for seven computational application codes. Finally we demonstrate the suitability of libWater  for a heterogeneous cluster for two codes.Our approach expands on previous work  [13] by adding a new optimization (the DHDCR, in Section  6), new test cases (Section  6), new scalability studies on an additional target architecture, the MinoTauro GPU cluster (Section  7.2) of the Barcelona Supercomputing Center and new studies to test the suitability of libWater to exploit the computational capabilities of a heterogeneous cluster configuration (Section  7.3). With a wider range of applications, test platforms and optimizations, we show how libWater  effectively improves the overall performance and scalability on large-scale compute clusters while easing the programmability.The rest of the article is organized as follows. Sections  2 and 3 provide an introduction to OpenCL and libWater  programming model. Section  4 describes the distributed runtime system and the underlying command DAG representation. The runtime optimizations are treated in Sections  5 and 6. The experimental evaluation is presented in Section  7. Sections  8 and 9 discuss related work and conclusions.OpenCL is an open industry standard for programming heterogeneous systems. The language is designed to support devices with different capabilities such as CPUs, GPUs and accelerators. The platform model comprises a host connected to one or more compute devices. Each device logically consists of one or more compute units (CUs) which are further divided into processing elements (PEs). Within a program, the computation is expressed through the use of special functions called kernels that are, for portability reason, compiled at runtime by an OpenCL driver. Interaction with the devices is possible by means of command-queues which are defined within a particular OpenCL context. Once enqueued, commands–such as the execution of a kernel or the movement of data between host and device memory–are managed by the OpenCL driver which schedules them on the actual physical device.Commands can be enqueued in a blocking or non-blocking way. A non-blocking call places a command on a command-queue and returns immediately to the host, while a blocking-mode call does not return to the host until the command has been executed on the device. For synchronization purpose, within a context, event objects are generated when kernel and memory commands are submitted to a queue. These objects are used to coordinate execution between commands and enable decoupling between host and devices control flows.Despite being a well designed language that allows the access to the compute power of heterogeneous devices from a single, multi-platform source code base, OpenCL has some drawbacks and limitations. One of the major drawbacks is that, because being created as a low-level API, a significant amount of boilerplate code is required even for the execution of simple programs. Developers have to be familiar with numerous concepts (i.e.  platform, device, context, queue, buffer and kernel) which make the language less attractive to novice programmers. Another important limitation is that, although it was designed to address heterogeneous systems, in case of devices from different vendors, objects belonging to the context of one vendor are not valid for other vendors. This limitation clearly becomes a problem when synchronization of command queues across different contexts is needed.libWater  is a C/C++ library-based extension of the OpenCL programming paradigm that simplifies the development of distributed heterogeneous applications. It inherits the main principles from the OpenCL programming model trying to overcome its limitations. While maintaining the notion of host and device code, libWater  exposes a very simple programming interface based on four key concepts: device, buffer, kernel and event. A device represents a compute device, but differently from the original paradigm this single object is an abstraction of the OpenCL platform, device, queue and context concepts. Such simplification reduces the number of source code lines necessary for the initialization of the devices, and thus avoids the boilerplate configuration code that is usually present in every OpenCL program. Furthermore, the library is not restricted to a single node but, taking internally advantage of the message passing model, it provides access to devices on remote nodes as if they were locally available.Since libWater  can grant access to a large number of distinct devices, the selection of a particular one can be cumbersome. In order to simplify this important aspect, libWater  introduces a novel domain specific language for querying devices. A device query language (DQL) query statement follows an SQL-like structure, that is composed of 4 basic clauses with the following syntax:The SELECT clause (the only one which is mandatory) respectively allows the selection of all the devices, the first topk, or a particular device from the device list generated under the restrictions on the following clauses. With FROM NODE a single node or a list of nodes can be specified narrowing the range of selectable devices to those particular nodes. The clauses WHERE and ORDER BY allow the control of the device restrictions on attribute values and the order in which the devices will be returned. The possible attribute values are currently those exposed by the OpenCL clGetDeviceInfo function. A DQL use case is shown and discussed in Section  4.2. DQL queries can be used for both device initialization and device selection. The latter must be a subset of the former and since libWater’s device concept represents a single device only, the function wtr_get_device only accepts queries that make use of the POS clause.Once a device is created, it is possible to allocate data and execute computation on it. In libWater, this is done through the use of the buffer and the kernel concepts. These two objects are similar to their respective OpenCL versions, with the main difference that, during their creation, they are bound to a specific device. For this reason no device must be specified for buffer and kernel related functions. The fourth concept in libWater  is the event object. Most of kernel and buffer functions have one or two parameters called wait_evt and evt. The latter is an output argument which is used by the invoked command to generate an event object. If not specified, libWater  assumes blocking semantics for the routine. The former specifies the event object on which the execution of the command depends. If not present, the command has no dependencies and thus it can be immediately executed.The last major difference between libWater  and the OpenCL model is the fact that initialization and release of buffers and kernels can be invoked using a non-blocking semantics. The main reason for this is to increase the amount of operations that the runtime system can overlap. Due to space constraint, we omit the complete libWater  API, which can be found in  [13]. In the next section we explain how dependency information enforced by events are then exploited by libWater’s runtime system.While the main focus of the programming interface of libWater  is on simplicity and productivity, the underlying runtime system aims at low resource utilization and high scalability. Calls to libWater  routines are forwarded to a distributed runtime system which is responsible for dispatching the OpenCL commands to the addressed devices and for transparently and efficiently moving data across the cluster nodes. The libWater  distributed runtime is written in C++ and internally uses several paradigms, such as pthreads, OpenMP and MPI for parallelization.Fig. 1shows the organization of the libWater  distributed runtime system. The host code, which directly interacts with libWater’s routines, runs on the so-called root node, which by default is the cluster node with rank 0. This thread will be referred to as the host thread. In the background, a second thread, i.e. the scheduler thread, is allocated to execute an instance of the WTRScheduler. On the remaining cluster nodes, a single scheduler thread is spawned independently of the number of available devices (only one MPI process is allocated per node). This thread executes an instance of the WTRScheduler which represents the backbone of libWater’s distributed runtime system.Each WTRScheduler continuously dequeues wtr_commands from the local command queue. wtr_commands in the system are generated in two ways, either by (i) libWater’s routines (step 1), or (ii) by delegation from the root scheduler (step 3). Calls to the libWater’s interface are converted into command descriptors (i.e. command design pattern) and immediately enqueued into the root node local command queue (step 1) of Fig. 1. Since all wtr_commands are generated by the root node itself, we refer to its queue as the runtime global command queue.wtr_commands are either wrappers for OpenCL commands or data transfer jobs (i.e.  send_job or recv_job) which are generated by the library routines whenever the device addressed by a read or write buffer operation is located in a remote (i.e.rank≠0) compute node. The descriptor of a wtr_command is self-contained since it carries all the information necessary for its execution. To be portable across cluster nodes, OpenCL objects such as kernels, buffers and events are identified, within the wtr_command object, by a unique ID. The root scheduler continuously fetches the wtr_commands from the global command queue, decodes its content and–depending on the targeted device–dispatches the command to the correct node. When the wtr_command addresses one of the local OpenCL devices, the corresponding OpenCL command is created and enqueued into the device command queue (step 2). When a remote OpenCL device is addressed, an MPI message is generated–serializing the content of the wtr_command descriptor–and dispatched to the cluster node hosting the requested device. The WTRScheduler of the target node then de-serializes the wtr_command and, instead of immediately executing it, enqueues the wtr_command instance into the local command queue (step 3). The same WTRScheduler is then responsible to dispatch the corresponding OpenCL command into one of its local device queues (step 2).The heartbeat of the WTRScheduler is an advanced event system which allows the management of an entire compute node–hosting multiple OpenCL devices–using only a single application thread. Indeed, because one instance of the WTRScheduler runs on every cluster node, trying to keep the resource usage as low as possible is of paramount importance in order to avoid wasting CPU cycles which can be used to run an OpenCL kernel. Different from related work, e.g. the SnuCL runtime system  [22], which exclusively reserves an entire cluster node and a physical CPU core in each compute node only for scheduling purposes, our system does not exclusively reserve any user resources for scheduling. Furthermore, using a single thread, for both executing local wtr_commands and for performing scheduling decisions, reduces the amount of synchronization since accesses to event and the command queues do not need to be synchronized.Relying on a single thread can however easily become a performance bottleneck. An interesting example is the interaction with MPI routines. By default many MPI implementations implement blocking behavior with a spin-lock mechanism in order to minimize latency. This means for example that a blocking receive, waiting for a message from the communication channel, continuously checks for incoming data usually saturating the cycles of a CPU core. In an environment like ours, where CPU cores may be used to run OpenCL kernels, this behavior must be avoided. Our solution is to avoid in every event handler routine any call to blocking MPI or OpenCL routines and always use the non-blocking semantics. The main idea is the creation of periodic events, handled by the event system using a priority queue based on timestamps, to check for the completion of pending operations. For OpenCL routines, we exploit the OpenCL event system and the associated callback mechanism. In this way, the WTRScheduler is able to dispatch several commands on the OpenCL devices, or MPI data transfers, which although being issued sequentially (by the single flow of the execution) are concurrently executed by the available resources (i.e. OpenCL devices and the network controller). The same event-based technique utilized to manage multiple OpenCL devices in a single node is also exploited on the large scale across cluster nodes.As already explained in the previous section, libWater  puts a strong emphasis on events. Following the semantics of OpenCL, dependency information enforced by programmers are used to select wtr_commands, which can be safely enqueued into one of the cluster nodes. libWater  provides an event object, i.e.  wtr_event. Internally, wtr_events are mapped either to an OpenCL cl_event object, or to a wtr_command identifier which is automatically generated for each wtr_command enqueued into the system. These dependencies allow the runtime system to organize enqueued wtr_commands into a DAG.A complete multi-device libWater-based host program is shown in Listing 1. This code initializes all the available NVidia GPU devices. It then selects two devices belonging respectively to node rank 0 and 1, with a global memory larger than 1024 MB. For each device the code in Listing 1 does the following: create a kernel (i.e.  kern, in line 10) and a read/write buffer (i.e.  buff, line 11). Then the contents from the host memory is written into the device buffer by the wtr_write_buffer command (line 12) and the wtr_run_kernel command is issued providing buff as an input argument (lines 14–16). The computed result is then retrieved by the wtr_read_buffer command (line 17) which moves data from the device memory back to the host memory. From the runtime system point of view, the execution of the previous code generates a set of dependent commands structured as the DAG depicted in Fig. 2. The DAGG(V,E)is composed of vertices, i.e.wtr_commands∈V, interconnected through directed edges(a,b)∈E∣a,b∈V, or events, which guarantee that the correct order of execution, and therefore the semantics of the input program, is maintained. The set of dependencies associated with a commandc∈Vis defined asc.deps={v∈V∣(v,c)∈E}. It is worth mentioning that not all libWater  library routines generate a corresponding wtr_command. For example, creation, merging and release of events are only meaningful in the root node, therefore there is no need for serializing them. In Fig. 2, each wtr_command carries a descriptor in the formx|ywherexrepresents the node rank,c.node_id, on which the targeted device,c.dev_id, is hosted andyis the unique command identifier assigned by the runtime system. As already mentioned, for buffer operations on remote devices (i.e. device on node 1) explicit data transfers are automatically inserted by the libWater  library (e.g.  wtr_commands 10 and 14).Events determine when a wtr_command can be scheduled for execution. The scheduler uses a just-in-time strategy to select the next wtr_command from the local command queue. The logic works as follows: enqueued wtr_commands are analyzed in a FIFO fashion and, for each ready command, the scheduler checks whether dependencies–explicitly specified by event objects–are satisfied. If a command has no dependencies, it can be executed. Since the host program generates all the commands solely on the root node, scheduling is done at this node. However, a centralized scheduler on a single node is not an effective strategy since it limits command throughput and thus the overall scalability of the system.In order to solve this problem, we rely on the fact that the OpenCL runtime system already has the capability of scheduling commands and handling dependencies by using events. It is worth noting that in OpenCL this mechanism is limited since events cannot be used to perform command synchronization across different contexts. libWater  unifies event handling through WTRScheduler instances which manage inter-context synchronization and offload intra-context synchronization to the OpenCL driver.We implemented a three-level hierarchical scheduling approach as described in Algorithm 1. At the top level, the root node of the libWater  runtime system pro-actively schedules wtr_commands from the global queue to the targeted cluster nodes.cmd, fetched from the command queue, is sent to the target node (i.e.cmd.node_id) only if each of its dependent commands (i.e. the setcmd.deps) is to be executed on the same remote node (lines 6–9). The second level scheduling is local to each node (lines 11–14). The scheduler checks whethercmdonly depends on wtr_commands addressing the same OpenCL device. In such case, the command is enqueued into the corresponding device queue (i.e.dev.dev_id) and dependencies are mapped to local OpenCL events. Alternatively, if a wtr_commandC1depends on a second wtr_commandC2, scheduled in another context (of the same node), the local WTRScheduler ensures thatC1is not enqueued into the OpenCL device queue beforeC2is completed. The third-level scheduling is implemented by the OpenCL runtime system itself which is responsible of managing single device queues. Ifcmdcannot be scheduled, due to unsatisfied dependencies, then it is pushed back in the command queue.Command dependencies are automatically updated when a wtr_commandccompletes. Locally, a command completion event is generated. The associated callback function removes, for every command in the local queue, any dependence onc. Additionally, nodes notify the root scheduler with a message triggering a similar completion event internally at node 0. In such a way, commands in the global queue waiting for the completion ofccan be scheduled–depending on the targeted device–either to a local device or to a remote node. The detailed algorithm can be found in  [13].This multi-level scheduling allows the runtime system to hide the costs of the scheduling, as well as data transfers, with the actual work being done by the devices in the background. The main idea is to use non-blocking semantics when OpenCL commands are scheduled in the corresponding devices. In this way, the WTRScheduler can continuously dispatch commands to other devices or move data from and to the root node. In the example in Fig. 2, commands 0|1 and 0|2 can be executed in parallel. Events at addressese+0ande+1are handled by the root WTRScheduler since the OpenCL standard does not allow non-blocking semantics for these operations. The remaining commands (i.e.  0|3, 0|4 and 0|5) are inserted asynchronously into the OpenCL device queue of node 0, upon completion of commands 0|1 and 0|2. Events e+2 and e+3 are therefore handled directly by the OpenCL runtime system. Following the same logic, wtr_commands addressing the second OpenCL device (i.e.1|∗) are sent to the node with rank 1. The blocking function wtr_wait_for_events stops the execution of the host until the release operations on both nodes have completed.The underlying architecture of the libWater  runtime system and the emphasis on events, promoted by its interface, enables several runtime optimizations which are transparent to the user. This capability is a direct consequence of adhering to the OpenCL queuing semantics. Indeed, while commands are being enqueued into the system, a command DAG (as shown in Fig. 2) is internally created. Since OpenCL issues commands to the appropriate device only when an explicit flush is invoked by the programmer, the runtime system can analyze large portions of the application DAG and optimize it for improving scalability.An optimization which has been implemented in the libWater  runtime system is the dynamic detection and replacement of collective communication patterns (DCR). Whenever the addressed device is not hosted in the root node, a call to wtr_write_buffer and wtr_read_buffer respectively generates an MPI send and receive operation. When an OpenCL application is distributed among all available devices, input buffers are usually either split or replicated between compute nodes. This parallelization strategy is common and it results in a DAG containing several send/receive transfer operations for every device of the cluster. An example is depicted in Fig. 3(a) which represents a realistic DAG resulting from the splitting of an input and output buffer among a set ofNOpenCL devices.Point-to-point data transfers performed by the libWater  runtime system imply an increased latency when compared with the native MPI send or receive routines. The reason for that is the polling mechanism implemented by the libWater  runtime system–mainly employed to save node resources–which replaces the spin-lock mechanism commonly used by MPI libraries. Additionally, the number of required data transfers is directly proportional to the cluster nodes (and thus devices). This results in a large number of commands being dispatched by the runtime system and consecutively negatively impacts the overall scalability. MPI offers a large set of communication patterns called collective operations   [27]. These routines are highly efficient since nearly all modern supercomputers and high-performance networks provide specialized hardware support for collective operations  [25]. Additionally, the implementation of such collective operations employs dynamic runtime tuning techniques which choose, among a set of semantically equivalent algorithms, which best fit the underlying network topology and architecture  [7,32,33].Related work analyzed the problem of automatic detection of collective patterns from a set of point-to-point communications. This technique is common in MPI performance tools which are capable of detecting such patterns via post-mortem analysis of program traces  [23]. The general problem of collective communication pattern detection is NP-hard, however, under particular restrictions the problem can be solved in polynomial time. A more recent work  [16] proposed a fast solution, with a complexity ofO(nlogn), which makes the approach more suitable for runtime systems.The goal of our DCR optimization algorithm is to analyze the command DAG isolating point-to-point data transfers and detect whether a subset of those resembles one of the collective patterns supported by MPI. This is possible since–if the application is carefully written using events for command synchronization–the command DAG will be available to the runtime system scheduler before the first blocking command is invoked (e.g.  wtr_wait_for_event(s)). Since data transfers in our environment have all the same root (the node 0), the analysis for patterns is simplified.The optimization algorithm is composed by two phases. First, the command DAG is traversed and all the transfer commands are collected intoNseparate lists, one per device. Second, on the extractedNlists, pattern analysis is performed. The collective pattern check is done by considering elements having the same position within the transfer job lists. Furthermore, the check is simplified by the fact that every send and receive wtr_command carries information of the buffer location (buf) and the amount of bytes being transferred (size). The pattern analysis starts by taking the first transfer wtr_command from theNlists and by checking against a supported pattern, i.e.  broadcast, scatter or gather. For instance, in a broadcastNsend operations are expected where∀i∣0≤i<N−1,bufi=bufi+1∨sizei=sizei+1. If the check fails, the transfer jobs are tested against a scatter or gather pattern∀i∣0≤i<N−1,bufi+sizei=bufi+1.Once a pattern is recognized, single point-to-point transfers are removed from the command DAG and replaced by the corresponding collective communication operation. A visual example of this optimization is depicted in Fig. 3(a), where multiple send operations are collapsed into a single scatter operation and correspondingly, receives are rewritten as a gather operation. By doing so, dependencies between successive commands are updated in order to keep the semantics of the input program unchanged.Since collective operations must involve all the processes in a communicator, the current implementation of the DCR optimization works when all the initialized devices participate in the computation. Therefore, the analysis is limited to regular applications which must involve all OpenCL devices in data transfers. This is important to keep the pattern recognition algorithm simple and fast, since this optimization is applied during runtime.Another optimization which has been implemented as part of the libWater  runtime systems is the detection and optimization of device-host-device copy patterns. As the libWater  API closely matches the OpenCL host-device model, it does not include any device-device communication. This limitation is based on the OpenCL platform model which does not include functions operating on contexts belonging to different platforms. However, on distributed computing environments, this limitation imposes the use of centralized host-device instead of more efficient device-device distributed communication.An example of this problem arises when a buffer which has been distributed overNdevices to be used as the output in a first kernel, is later used as input of one or multiple devices of a second kernel. For instance, let us consider the matrix chain multiplicationABCD. As matrix multiplication is associative, we can compute firstAB, thenCD, and finally the product(AB)(CD). While the first two multiplications work normally, the latter requires device-host-device communications that drastically affects scalability.To address this issue, we implemented a new optimization which attempts to replace similar device-host-device communications with direct device-device data transfers. This optimization, called device-host-device copy removal (DHDCR) is implemented as follows. Whenever an application contains call to wtr_write_buffer and wtr_read_buffer involving devices not belonging to the root node, libWater  generates MPI send and receive operations. If a sequence of write, read, write occurs on the same buffer (or on part of the same buffer) then this sequence is a candidate for optimization. Once the pattern is recognized, the two consecutive device-to-host and host-to-device transfers are removed from the command DAG and replaced by a single device-to-device transfer. A visual example of this optimization is depicted in Fig. 3(b). The TransJob, generated by the DHDCR optimization, is a wtr_command which the root scheduler dispatches on both nodes involved in the data transfer (nodes 1 and 2 in the example), the other nodes are not involved. However, in order to maintain the host semantics of the program unchanged, the updated value of the buffer (generated by node 1) must also be copied back on the host node. Therefore a RecvJob command is generated to collect the buffer. The main difference with the original code is that this operation can be completely overlapped with the execution of the second kernel on the node rank 2.Note that simple applications such as the ones listed in Table 1, only show a simple pattern (write, run kernel, read) and do not show any possibility to apply DHDCR. However, more complex applications are usually consisting of several kernels, with non trivial inter-node data transfers, and are more suitable for this optimization (e.g. matrix chain multiplication).We used libWater  to encode 6 computational kernels, some of them taken from various OpenCL benchmarking suites (i.e. AMD and IBM), and studied their scalability. In four of them, the kernels were optimized for local memory, i.e.  PerlinNoise (from IBM), Nbody (from AMD), Floyd and kNN manually written by us. For the remaining two codes, MatrixMul and LinReg we used a naive implementation unoptimized for what concern local memory. Table 1 shows, for each kernel, the number of input and output buffers used by the kernel. We define a buffer as splittable when its content can be distributed among the devices. The nature of a buffer is strictly related to the algorithm being implemented within the OpenCL kernel, and thus the application. Non splittable buffers are always replicated on every device. All six applications utilized for our study do not contain unsplittable output buffers. In the presence of such buffers, the merge of the result coming from different devices would generate memory consistency issues that libWater  is currently not able to handle. Table 1 also shows the reduction, in terms of lines of code, achieved when the application is written using our library. It is worth mentioning that while the original OpenCL applications were single device codes, the libWater  based implementation is instead multi-device code. On average, we were able to reduce the lines of the host code by approximately a factor of 2 due to the higher level abstractions provided by libWater.For the scalability analysis we used two large-scale production clusters, the Vienna Scientific Cluster VSC2  [38] and the MinoTauro Barcelona Supercomputing Center GPU Cluster  [37]. A second study was conducted to test the suitability of libWater  to exploit the computational capabilities of a heterogeneous cluster configuration. For this purpose we used the Ortler Cluster at the University of Innsbruck, composed of three heterogeneous compute nodes (i.e. mc1, mc2 and mc3). The hardware details of the clusters are depicted in Table 2.The applications shown in Table 1 were executed on the VSC2 CPU cluster. We were able to access up to 64 nodes with a total of 1024 CPU cores. Since the 2 AMD CPUs which are hosted per node are considered by the OpenCL driver as a single device, the speedup was computed based on the number of compute nodes (and thus OpenCL devices) instead of single CPU cores. The workload partitioning is implemented, for each test case, by assigning to each OpenCL device an equal amount of work.The scalability tests were performed in the following way: the original OpenCL version of the applications were executed in a single node and their execution times used as a reference measurement. libWater  was then used for node numbers ranging from 2 to 64. The main differences between the original version of the application codes and the one written using libWater  are mainly in the host code. The kernel code was slightly modified only to forward the offset value used by the workload partitioning (as shown in Listing 1). We computed the ideal scaling for each application using the reference execution time and dividing it by the number of nodes. We conducted experiments with libWater  by using two different settings: the first, named baseline, uses the runtime system without dynamic optimizations enabled; the second, DCR, uses the collective pattern replacement mechanism as described in Section  5. The results of our experiments are depicted in Fig. 4.For each of the six applications, we show the execution time (in seconds) for up to 64 nodes and the corresponding speedup with respect to a single node. Overall, we observe that our approach scales almost linearly, especially for those codes using few input/output buffers. PerlinNoise, Fig. 4(a), is an example of those, since it has no dependencies on input buffers and the data produced by the kernel is distributed between the devices. For such code, the baseline configuration of our runtime system achieves a speedup of 53 for 64 nodes, and thus an efficiency of 83%. When the number and size of the input/output buffers increases, the efficiency of our system decreases. The worst case is represented by the LinReg application, Fig. 4(f), which stops scaling after 32 nodes. This kernel has 4 input buffers, 2 of them are not splittable (because of dependencies within the kernel code) and therefore must be replicated on every node. The remaining 2 input and output buffers are instead splittable. For such code we have an immediate decrease (75% on two nodes) of the efficiency. This is because the kernel execution is delayed due to the fact that several wtr_commands are executed (and transferred to the target nodes) to create and initialize the input/output buffers. However this delay is a constant and system efficiency remains almost unvaried up to 16 nodes. On 32 and 64 nodes the efficiency of the baseline runtime system starts decreasing significantly.This problem is largely addressed by the dynamic collective pattern replacement, i.e.  DCR, optimization which was introduced in Section  5. This optimization reduces the load on the scheduler since it replaces several single transfer jobs with one collective operation. In LinReg this optimization improves the scalability of the system by a factor of 2 achieving an efficiency of 55%. A small effect of this optimization can be observed for smaller node configurations because collective operations are optimized for a large number of nodes. An interesting result is the effect of the DCR optimization on the PerlinNoise test case. In such a case, the DCR optimization fails to improve performance over the baseline. The reason is that collective operations are blocking while point-to-point communications in the runtime system are non-blocking thereby allowing overlapping of multiple transfers. The synchronization costs introduced by the gather operation are therefore not properly compensated by the amount of exchanged data. We believe that this problem can be eliminated by using non-blocking collective routines which have been introduced in the latest MPI standard  [27] and will soon be available in mainstream MPI libraries. Additionally, since this optimization is done dynamically, and therefore the amount of data being transferred is known by the scheduler, heuristics can be integrated to decide when such optimization should be applied.On average, libWater  achieves an efficiency of 80% on 32 nodes and 64% when 64 nodes are used. Without the DCR optimization the system has an efficiency of 47% on 64 nodes. This means that the DCR optimization improves the system efficiency by 17% on 64 nodes and we expect this value to increase proportionally with the number of nodes.To show the effectiveness of the device-host-device copy removal optimization (DHDCR) we conducted another experiment on the VSC2 Cluster. Using libWater  library, we manually coded a multi-device version of the matrix chain multiplicationABCD. We run the experiment using two different settings: the first (baseline), uses the runtime system without the optimization while the second (DHDCROpt), uses the device-host-device copy removal mechanism as described in Section  6. Notice that in both cases the DCR optimization is also performed. When both runtime optimizations are enabled, the optimizer first tries to rewrite indirect data transfers to direct ones (using the TransJob command). Then, in a second pass DCR is applied. In order to optimize the execution even further, the DCR analysis has been updated to also take into account TransJob commands during the collective pattern analysis phase.The results of our experiments are depicted in Fig. 5. For this application, we show the execution time (in seconds) for up to 16 nodes and the corresponding speedup with respect to a single node. The baseline approach scales almost linearly up to 8 nodes with an efficiency of 87%. For 16 nodes the runtime system efficiency decreases significantly reaching 48%. The main reason is the high communication overhead caused by the unnecessary copies of intermediate buffers to the root node. Before proceeding with the(AB)(CD)operation, the results ofABandCDhave to be gathered by the root scheduler and then distributed again on the remaining nodes. While the buffer containingABcan be directly reused, the result ofCDcan be copied to remaining nodes using a more efficient collective pattern, the MPI_Allgather. In this paper, only the former redundant copy is automatically detected and removed, the latter is replaced by an MPI_Gater and MPI_Bcast by the DCR optimization.The benefits of this optimization starts to show with a large number of nodes because of the increased pressure on the root scheduler. For smaller node counts, the data movement ofABis completely overlapped with computation, so that by the timeABis distributed to the nodes alsoCDis available and the final computation can start without any delay. For larger nodes, the execution of the last kernel is delayed since there is not enough computation (kernel execution becomes shorter since more devices are used) to overlap the communication overhead. This causes a sensible decrease in the efficiency. By avoiding this communication, the DHDCR optimization improves the speedup from 7.6 to 10 achieving an efficiency improvement of 15%.Another scalability study was conducted executing theN-body simulation described in Table 1, line 2, in a GPU cluster. We were able to access up to 32 nodes of the MinoTauro cluster with a total of 64 GPU devices. In all the experiments, the workload was equally partitioned between the available devices. The optimization of theN-body simulation on the GPU processor is an active research problem  [39,6,15,18]. The problem is well known to be suitable for the GPU architecture and in case of a high number of particles for cluster of GPUs.We ran the NBody test case using 3 different input sizes that show the benefit of using a high number of GPUs in case of large number of bodies. The results of our experiments are depicted in Fig. 6. The 3 tests were conducted respectively with an input size of 2 (Fig. 6(a)), 5 (Fig. 6(b)), 10 (Fig. 6(c)) Million bodies. With the smallest input size the application scales almost linearly up to 16 GPUs and stops scaling after 32 GPUs. Increasing the input size by a factor of 2 increases the execution time by a factor of 4, due to the quadratic complexity of the implemented algorithm. With an input size of 5 and 10 million bodies the application becomes more suitable for a GPU cluster and with the biggest tested input size achieves a speedup of around 49 on 64 GPUs with an efficiency of 77%. It is worth mentioning that in such environment it is important from a user prospective to find a trade-off between the number devices and the desired efficiency.Since OpenCL allows access to heterogeneous devices we conducted a second experiment which demonstrates libWater  on a heterogeneous cluster as described in Table 2. In order to run applications on such environment, the input code was rewritten so that the workload distribution was controllable via command line arguments. It is worth mentioning that workload partitioning for heterogeneous architectures is still an active research problem [12,24,19,14]. However, this aspect is completely orthogonal to our library and for the sake of this experiment, we derive workload partitionings in an empirical way. We ran the NBody and the LinReg test cases using different combinations of devices. For each device configuration, several different workload splittings were tested and the fastest one was chosen. The partitionings and their corresponding execution times, are shown in Table 3. For example, in NBody, configuration C1 assigns all the workload to the first GPU of node mc1. The execution time for this configuration is 42.2 s. By equally splitting the workload between the two GPUs on the same node, i.e.  C2, we double the performance. Between the devices, the NVIDIA Tesla k20m is the fastest device requiring 35.9 s to complete the work. However libWater  can be used to improve the execution time even further. The overall execution time can be reduced by 70% by using the workload partition as described by configuration C8 which assigns 22.5% to each GPU in mc1, 26.5% to each GPU in mc2 and the remaining 1% to each accelerator in mc3. For LinReg results are different, since the execution times for the different devices are more balanced. The best performance can be achieved in this case splitting the workload between the nodes by assigning 11% to each GPU in mc1, 14% to each GPU in mc2 and 25% to each accelerator in mc3.In this section, we analyzed the performance of libWater  in three different compute clusters. On the VSC2 CPU cluster, the library achieves on average an efficiency of 80% on 32 nodes and 64% on 64 nodes. These results include the DCR optimization that in case of 64 nodes is capable of improving the system efficiency by 17%. In the same cluster, we also tested the DHDCR optimization showing an efficiency improvement of 15% over the baseline matrix chain multiplication implementation. On the MinoTauro GPU cluster we executed theN-Body application with different number of bodies achieving a speedup of 49 on 64 GPUs with an efficiency of 77% for the biggest tested input size. This result shows that the hierarchical scheduling approach described in Algorithm 1 is able to handle multiple devices per node without compromising the overall scalability of the system. Finally, we executed the NBody and the LinReg applications using different combinations of devices on the Ortler Heterogeneous cluster. The results of the experiment demonstrate, despite higher latencies caused by additional data transfers between host and device memory, non-blocking communication yields good scalability behavior even for heterogeneous architectures.

@&#CONCLUSIONS@&#
In this paper, we introduced libWater, a library for simplifying the programming of heterogeneous distributed systems.The proposed interface demonstrates that raising the abstraction level of the OpenCL programming model is possible without losing control over performance. We showed, with an example, how a multi-device distributed host program can be written using approximately 25 lines of code. By defining a simple, but powerful, device query language (DQL), libWater  simplifies the management and discovery of a large number of OpenCL devices. The simple API makes the library a perfect target for automatic code generation tools, thus it can be easily integrated in compilers.libWater’s interface is tightly bound to a lightweight distributed runtime system which is designed from scratch for high scalability and low resource usage. Because of the non-blocking semantics promoted by the library interface, commands can be organized by the runtime system into a DAG to be used for dynamic analysis and optimizations.We studied the performance of the library on three compute clusters, demonstrating the high efficiency that the system can achieve.libWater  will be released as an open-source project with the goal of becoming a research platform to investigate performance aspects of heterogeneous and distributed HPC architectures.