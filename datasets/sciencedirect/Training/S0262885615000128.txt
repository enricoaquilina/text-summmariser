@&#MAIN-TITLE@&#
Incorporating higher order models for occlusion resilient motion segmentation in streaming videos

@&#HIGHLIGHTS@&#
Each video is divided in sub-sequences.Motion models are extracted for each video sub-sequence.The model error distribution and the model ranking for each trajectory are used to correlate different trajectories.Occlusions cause segmentation leakages when sub-sequences are merged.To avoid this effect we model segmentation as a graph coloring problem.

@&#KEYPHRASES@&#
Motion segmentation,Trajectories,Occlusion,Object leakage,Graph coloring,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Nowadays, the widespread use of video recording and storing devices has led to the creation of large video databases. Hence, there is an imperative demand for automated algorithms that can analyze video content. Video segmentation has a vital role in this pursuit, as its goal is to extract separate entities from a video. There are immediate applications in various areas of computer vision such as object recognition, activity analysis and video retrieval. Research in video segmentation has been deeply affected by the study of the human visual system (HVS), because of its unparalleled inference ability compared to the state of the art in computer vision.Starting from the principle of common fate of Gestalt psychology [1] and continuing with the seminal works of Gibson [2], Julesz [3] and Marr [4], it is well established that motion perception is a dominant factor in how the HVS reasons about the content of a scene. This has influenced computer vision researchers and a number of segmentation algorithms use the motion of tracked points as expressed by a set of trajectories throughout a video sequence.Many motion segmentation algorithms adopt an affine camera model [5], since under this assumption the trajectories of a rigid object span a subspace of dimension at most 4 regardless of their duration [6]. These methods formulate motion segmentation as a subspace clustering problem and some perform very well on the Hopkins155 dataset [7]. One such method is [8] where local sampling is employed to estimate the subspace that a trajectory belongs. The authors of [9] use Agglomerative Lossy Compression (ALC) [10] for motion segmentation. This method, based on the principles of sparse representation and lossy compression, can to a certain extent deal with noisy and incomplete trajectories. Sparse Subspace Clustering (SSC) [11] uses a sparse linear combination of each trajectory to form an affinity matrix and apply spectral clustering. In [12] the Ordered Residual Kernel (ORK) is proposed which uses the ranking of a set of motion models to correlate different trajectories. Low Rank Representation (LRR) [13] finds the lowest rank representation of all trajectories to define an affinity matrix and obtain a final segmentation through spectral clustering. All these methods are in essence generic subspace clustering methods and the Hopkins155 dataset that they are evaluated on, while has certainly fostered progress in the field, has some shortcomings.First of all the videos in the dataset have small duration while the precomputed trajectories are sparse and manually corrected. Moreover, all trajectories have the same length with no occlusions, while the number of objects is constant throughout each video. The authors of [14] identified these issues and introduced the Berkeley Motion Segmentation Dataset (BMS-26) which contains more realistic videos of longer duration where occlusions are a common phenomenon. They used the tracking algorithm of [15] to produce dense trajectories that can be partially erroneous. An algorithm was proposed that uses a translational motion model to define pairwise affinities between trajectories, followed by spectral clustering with spatial regularization to obtain a final segmentation. In the case of no temporal overlap between trajectories, the algorithm relies on the transitivity of spectral clustering. In [16] a variational method computes a dense segmentation using the segmented trajectories. Improving the segmentation framework presented in [14], the authors of [17] introduced the more challenging Freiburg-Berkeley Motion Segmentation Dataset (FBMS-59) and also proposed novel evaluation metrics for video segmentation.The authors of [18] acknowledged the problem of object leakage due to occlusions and trajectories asymmetry. The solution they proposed, models segmentation as a graph partitioning problem. It uses the longest trajectories that are more reliable to recover an initial sketch of a scene and sets repulsive weights between trajectories that belong to different connected component during their common time span. In [19] the discontinuities in the spectral embedding of spatially neighboring trajectories have been successfully used to perform video segmentation while accounting for the problem of over-segmentation. Moreover, the algorithm proposed in [20] has shown impressive results using a non translational motion model and hypergraph partitioning in conjunction with spectral clustering.The authors of [21] raised the issue of scalability in motion segmentation. As the duration of a dynamic video increases, so does the number of trajectories. Methods that treat the whole set of trajectories as nodes of a graph to be partitioned, will inevitably reach a point where the number of trajectories becomes unmanageable. Therefore, in [21] a video is divided in smaller subsequences that are segmented separately and then the individual segmentation results are merged. In [22], using the same framework of subsequences, a method is proposed that is fast while deals with the problem of occlusion. Another paper that focuses on scalability in motion segmentation is presented in [23]. Similar to our work, the authors of [23] advocate the use of superpixels to transform the initial full graph to a reduced one, in order to decrease computational and memory requirements. Under certain assumptions, the resulting segmentation from the reduced graph is equivalent in terms of the normalized cut criterion [24] to the segmentation from the full graph. Furthermore, another common characteristic of [23] with our framework is that both are suitable for video segmentation in a streaming environment.In this paper, we aim for the benefits of all previous methods and make an attempt to address some of the major issues in motion segmentation, namely non translational motion, over-segmentation, cross-object leakage and scalability. A major contribution of our work is the employment of higher order motion models in a way that is both efficient and computationally tractable. The affinity metric that we propose, leads to strong connections for trajectories of the same object even if they are spatially distant. Simultaneously, trajectories following different motions remain unconnected regardless of their spatial distance. This aspect of our work is presented in Sections 3 and 4. Section 5 describes minutely the dissimilarity metric we use to correlate segments. We have also briefly presented this metric in [22]. In Section 6 we extend our previous work in [22] and expose in detail the problem of object leakage due to occlusion. We propose a novel solution, formulating segmentation as a graph coloring problem, which allows us to significantly decrease over-segmentation. We will also see that by dividing a video in finer time slots, we get a natural setup where occlusions can be resolved as they arise. Moreover, by adopting the subsequence architecture of [21,22] we endow our method with scalability and test it under a streaming setting in Section 7. Through a series of experiments we demonstrate the efficacy and robustness of our method. Finally, in Section 8 we draw the conclusions of our work.The input of our algorithm is a set of trajectories T={t1,…,tM} corresponding to tracked feature points in a video sequence. In particular a trajectory ti, spanning frames f1 up to fV, is a 2V vector:(1)ti=tiTf1…tiTfVTwheretifj=xifjyifjTdenotes the image plane coordinates of trajectory tiat frame fj. Our method does not assume that the trajectories have a constant time span as most factorization based methods. Instead trajectories can be incomplete having various lengths due to occlusions and the limited camera field of view. Initially, the video sequence following [21,22], is divided in N overlapping subsequences {W1,…,WN} as is illustrated in Fig. 1. Subsequences are processed separately and the individual results are later merged to a final segmentation.The initial step in processing a subsequence is to extract a number of motion models as representatives of the moving objects in the video. Following the work of [20], the models used belong to the group of special similarity transformations SSim(2), since it allows for translations, rotations, and scalings while only two trajectories are required for the computation of a single model's parameters. Model selection is performed on trajectories visible throughout the subsequence using their correspondences in the first and last frame. If the number of trajectories covering all the frames of the subsequence is small due to occlusion/disocclusion we decrease the length of the subsequence. Later on, when we want to measure the motion compatibility of a selected pair of trajectories with a trajectory that is not visible in the entire subsequence, we recompute the motion model parameters in their common time span. The formulas for the computation of motion model parameters are borrowed from [20] and we reproduce them for the sake of completeness. In particular, for a pair of trajectories (tl,tm) the motion model sl,mfor frames (f1,f2) comprises a scaling parameter sl,m, a translation vector ul,mand a two dimensional rotation matrix Rl,mparameterized by an angle φl,mwhich are defined as:(2)sl,m=∥tlf2−tmf2∥2∥tlf1−tmf1∥2,ϕl,m=arccostlf2−tmf2Ttlf1−tmf1∥tlf2−tmf2∥2∥tlf1−tmf1∥2,ul,m=12tlf2+tmf2−sl,mRl,mtlf1+tmf1.The simplest approach to compute a model would be to randomly sample a pair of trajectories. However, this could produce erroneous models traversing different objects as is the case for trajectories tl,tmin Fig. 1. Erroneous models could hinder segmentation between objects following similar motions. To alleviate this problem our algorithm samples triplets (tl,tm,tn) of spatially close trajectories visible throughout the subsequence and requires that each of the corresponding pairs (tl,tm),(tm,tn),(tl,tn) is compatible to the third trajectory, i.e. tn,tl,tmrespectively. To measure how well a model Tl,mapproximates a trajectory tn, we use the normalized l2 distance,(3)Elmn=∥tnf2−Tl,mtnf1∥2max0.5,∥tnf2−tnf1∥2where(4)Tl,mtnf1=sl,mRl,mtnf1+ul,mand for model selection f1,f2 correspond to the first and last frame of the subsequence. When a trajectory moves sufficiently relative to the camera, we normalize the model error using the distance covered by the trajectory. This makes sense since a 2-pixel error is negligible for a 40-pixel motion (0.05 normalized error) whereas it is significant for a motion of 5 pixels (0.4 normalized error). In the denominator of Eq. (3), we use the maximum between a small value and the actual distance covered by tnto avoid any numerical instabilities when the apparent motion of tnis negligible. In the example of Fig. 2in order to keep a model Tl,mthe distances E(l,m)n,E(m,n)l,E(l,n)mshould be lower than a threshold. Following this strategy we can easily reject a model arising from a scenario as in Fig. 1 while accept it in the case of Fig. 1. After a triplet is validated, we keep the model with the lowest error. The respective pair of trajectories is not repositioned in the sampling space in order to avoid extracting models exclusively from larger objects that have many trajectories, thus neglecting smaller objects. This sampling procedure continues until either no trajectory is left in the sampling space or a certain number of motion models are reached. For our experiments this number is equal to 5000. An example of this sampling scheme is illustrated in Fig. 3, where the vast majority of the models sampled belongs to a single object.After model sampling has concluded and Q motion models have been extracted for a specific subsequence, we can compute for every trajectory tian error vector Ei. Each of its elements is the normalized l2 error of the respective model as is defined in Eq. (3).(5)Ei=El1m1i…ElQmQiSorting the entries of Eiin ascending order we get the sorted error vector Eiand the ranking vector ri, which contains the indices of the models, starting with the index of the model having the lowest error and ending with the index of the model having the highest error. Note that a ranking vector can be computed even for an incomplete trajectory in the subsequence. Since a motion model corresponds to a pair of trajectories visible throughout the subsequence we can recompute its parameters using their point correspondences in the first and last frame of the incomplete trajectory's time span.Our algorithm uses the computed ranking vectors to compute an affinity between trajectories even if they have different time spans and segments the objects with different motion in a subsequence. Like most available methods we formulate segmentation as a graph partitioning problem where trajectories correspond to graph nodes connected by an affinity metric. To partition this graph we use spectral clustering [24,25]. In particular we compute the normalized Laplacian matrix of the graph [24] and apply k-means clustering [26] on the eigenvectors with the smallest eigenvalues. The number of clusters is equal to the number of eigenvalues below a small threshold, which we set to 0.02 for our experiments. Although recent advances in graph theory have provided us with the ARV algorithm [27] that has better approximation guarantee, we chose spectral clustering because of its simple implementation and its successful utilization by various segmentation algorithms [12,28]. The novelty of our algorithm lies on the affinity metric that we propose which we call Model Ranking Correlation (MRC).The MRC for trajectories tiand tj, denoted as K(ti,tj), is computed by taking into account the ranking vectors ri,rjand the sorted error vectors Ei,Ej. The rationale underlying MRC is that trajectories arising from the same motion will have low error for the same set of models. In such a case the parts of their ranking vectors corresponding to low error models, will contain a similar set of model indices. As it is shown through a series of examples by focusing on the low error part of the ranking vectors, we get high affinities for trajectories following the same motion even if they are spatially distant. At the same time trajectories arising from different motions have a low affinity regardless of their distance.At first, using Ei,Ejwe compute hi,hjwhich are 1×P vectors whose elements are the positions where the ascending values of Ei,Ejrespectively start to exceed predefined thresholds T1,…,TP. For example in some of our experiments we use P=3 and set the thresholds to (0.07,0.14,0.2) for the normalized error. Next we compute vector(6)hij=h1ij…hPijas the element-wise minimum of hi,hj. An illustrative example is shown in Fig. 4. MRC uses the index limits of Eq. (6) and takes into account only the low error part of the ranking vectors. As we will see through examples this choice allows for the computation of high inner-object affinities even for large objects without giving rise to high intra-object affinities. To compute MRC we define:(7)Lkij=0,k=0|r1i…rhkiji∩r1j…rhkijj,0<k≤Pwhich measures the number of common model indices in ri,rjup to each element of h(i,j). Finally the MRC for trajectories tiand tjis:(8)Ktitj=∑k=1PwkLkij−Lk−1ijhkij−Lk−1ijwhere wkis a normalized Gaussian weight defined as:(9)wk=e−μk∑l=1Pe−μlwith μ=0.75 for our experiments. In Eq. (8) we measure the number of new common indices between the ranking vectors as we progressively take larger parts into account. The denominator works as a normalizing factor, bounding the fraction in the [0,1] range. We use the Gaussian weights to attenuate the influence of the ranking vectors' parts corresponding to higher error.Similar to ORK[12,29], we use model ranking vectors, however MRC has certain advantages when the two metrics are compared. ORK uses entire ranking vectors by searching for common elements in a progressively increasing part of the vectors starting from their first element. At each step the length of the subvectors examined is controlled by a step parameter which is constant for all trajectories and does not take into account the error distribution. On the other hand, MRC uses part of the ranking vectors, ignoring models with large error and consequently saving computational time. Moreover, as we will see through a series of examples MRC is inherently insensitive to an imbalanced trajectory set. Namely, in most cases the objects in a scene will have different number of trajectories. Thus larger objects will have more models sampled compared to smaller ones. Consequently, ranking vectors corresponding to objects dominating a scene will contain many indices of models with low error. Differences in the relative ranking of such models should not be penalized, something that is difficult to achieve with a constant step parameter. Additionally, smaller objects will have a small number of models and only the first few entries of the respective ranking vectors will correspond to models with low error. One should use only these entries to measure the affinity between trajectories. Otherwise deceptive affinities could arise.To make the comparison between ORK and MRC more tangible, we consider the simple but instructive example of Fig. 5. In this case we have 2 objects, the background and a person walking. This is a characteristic case of an imbalanced trajectory set since the vast majority of sampled models belong to the background. Here we focus on the affinities of 4 trajectories where the first 2 are on the background while the other 2 are on the person. We can distinguish 3 different types of trajectory pairs. Type I: A pair of background trajectories (t1,t2) will have many models of low error. Their ranking vectors can be quite different. However, this difference should not be penalized since the respective models have low error. Using only the first entries of the ranking vectors will unfortunately result in a low affinity. Type II: Two trajectories of the moving person (t3,t4) will have a few compatible models that will be concentrated on the beginning of the ranking vectors. In general the affinity will be high if one examines a small but sufficient part of the ranking vectors. Type III: One trajectory belongs to the background while the other to the moving object (t1,t3),(t2,t4). If one focuses on the beginning of the ranking vectors there will be acute differences in the rankings and the respective affinity will be low as desired. However, doing so contradicts the requirement of Type I pairs for taking many models into account. If one does use the entire ranking vectors, whereas the beginning of the vectors would be different the rest of the vectors would have enough common entries that would result in an incorrect strong affinity. Fig. 6displays the affinity matrices for MRC and ORK (Ordered Residual Kernel) after we sort the trajectories according to the ground truth. A block diagonal form is ideal. MRC has strong intra-object affinities and low inter-object ones. On the other hand when the step parameter for ORK is set to 1000 spurious Type III affinities arise and the two objects cannot be separated. If the step parameter is lowered to 100 these affinities disappear but also weaken Type I affinities. The segmentation results using MRC and ORK are shown in Fig. 7.Fig. 8displays a comparative example of MRC with the metric proposed in [14] as is implemented in [19] for a subsequence of the cars2 video. Following the implementation of [19] and in order to speed up computation we set the affinity of far apart trajectories to zero. Hence all metrics have a similar pattern of zeros. However, MRC has stronger intra-object affinities even in the case of the background which has many spatially distant trajectories since no spatial smoothing is applied. When applying spectral clustering on the respective affinity matrices, these strong intra-object connections allow the extraction of an additional object which [14] cannot detect even if one resorts to over-segmentation. This favorable behavior of MRC would be impossible if the error distribution for each trajectory was not taken into account and a constant vector hiwas used for all trajectories.A natural question is what happens when no motion models are sampled from an object either because it vanishes before the end of the subsequence or due to its small size. Generally, in such a case all the models will have high error, above the thresholds we have set. Therefore, we default the parameter vector hito small values and focus on the first elements of the rankings with the expectation that the trajectories of the vanishing object will have the lowest error for the same set of models. Although this assumption will fail under certain circumstances, it is an adequate approximation for small objects following a translational motion, which is a common scenario for vanishing objects. An example of such case is shown in Figs. 8 and 10for the cars1 video where the smaller car vanishes before the end of the subsequence. The affinities for the trajectories of the smaller car are high and it is easily extracted. We should also add that even if an object is not detected due to an insufficient number of models, it can still be correctly extracted in the final segmentation if it is successfully detected in a previous subsequence. The underlying procedure is described in Section 6.Furthermore, the affinity matrices for [14] and ORK are also shown in Fig. 9. For all metrics there are sporadic off-diagonal affinities for trajectories of the moving cars and the background. These affinities are marked by a yellow rectangle in Fig. 9 and are also shown in greater detail in Fig. 10. These erroneous affinities are due to corrupted trajectories that although belong to the background are mistracked and follow the motion of the cars due to the drifting effect of optical flow. For MRC these background trajectories are highlighted in Fig. 10, where all of them are on the boundary between the cars and the background. These affinities are weaker for [14], because spatial smoothing is applied. However, as Fig. 10 demonstrates the segmentation computed with MRC is still accurate although the corrupted trajectories are mislabeled.Up to this point our algorithm has segmented each video subsequence with no concern for over-segmentation. The next step is to aggregate the individual segmentation results into a final one. To achieve this, a vital task is to measure the motion dissimilarity between segments. Our algorithm uses the motion distance we originally proposed in [22], which we also present here in greater detail.Assume that we want to compute the motion dissimilarity between segments siand sjwhere for every frame transition fv→fv+τof their common time span they have respectively Pi(fv) and Pj(fv) point correspondences. For every frame transition, we use the Pi(fv) correspondences to sample M affine fundamental matrices and compute the reprojection error [5] of the Pj(fv) correspondences. For our experiments M is equal to 100 and τ to 3. As a result we get an error vector I(i,j)(fv) that has (M×Pj(f)) elements. We apply kernel density estimation on I(i,j)(fv) [30] using a Gaussian kernel and get a discretized error probability density. Using the probability for the zero binpfvije=0, we define the motion dissimilarity for transition fv→fv+τas:(10)dijfv=−logpfvije=0.When two segments perform the same motion, it is natural to assume that the error distribution will approximate a delta function centered on zero. If we look at Eq. (10) from an information theoretical aspect, it is the relative entropy [31] between a probability density concentrated on zero andpfvije.The authors of [14] noted that to decide if two trajectories belong to the same object, one should focus on the time instant where their motions are most dissimilar, since they could follow the same motion for most of the time. We adopt this line of thought but instead of single trajectories we now deal with segments. Therefore, the motion distance for segments si,sjis computed as:(11)dij=maxfvdijfv,where the maximum is computed over the common time span of the trajectories in segments si,sj. Moreover, since d(i,j) is not symmetric, the final motion distance is:(12)Dij=maxdij,dji.In the case where a segment siis very small and does not have a sufficient number of trajectories, M fundamental matrices cannot be reliably sampled. In that case the motion distance is D(i,j)=d(j,i).Fig. 12shows an example with 2 segments, one being the static background while the other is a person that initially stands still and starts moving at frame 26. In this example we choose a subsequence of 50 frames to emphasize the fluctuation of the motion dissimilarity as the video progresses. The dissimilarity starts off as zero up to frame 26 since the person stands still. As he gradually starts moving from frames 26 up to 36 the dissimilarity increases and reaches a higher level when the person's motion is in full swing. Respectively the error probability density starts as an indicator function centered on zero and obtains a gradually longer tail.In the current section, exploiting the subsequence architecture, we formulate segmentation as a graph coloring problem, keeping in mind that occlusions may cause under-segmentation. At first, we demonstrate through example the negative effect of occlusions and then we provide an algorithm that remedies this problem while at the same time recovers from the over-segmentation that merging successive subsequences causes.After processing each subsequence Wi, the algorithm has produced a segmentationSi=s1i…sMii. Adopting the spirit of [21] we merge the segmentations of two successive subsequences Wi,Wi+1 by computing the intersection between the segments in Si,Si+1. As a reminder, assuming that for each subsequence we have a segment consisting of the trajectories that do not appear in it but appear on the other, Siand Si+1 are partitions of the same set of trajectories. By grouping trajectories that belong to the same segments in both Siand Si+1 we get a finer segmentation spanning both subsequences. An example of this procedure is illustrated in Fig. 13for the cars9 of the FBMS-59 dataset. It is worth highlighting the similarity of this technique with the one proposed in [23], as in both cases isolated trajectories are merged in larger groups. Notably, preliminary merging of trajectories is a quite reliable algorithmic choice in order to process longer video sequences and reduce memory consumption.Using the motion dissimilarity metric described in Section 5 we can partition the segments' graph as in [21]. However, this straightforward approach can produce erroneous results, because occlusions lead to incomplete dissimilarity measurements along the temporal axis. In the case of no temporal overlap between segments, where dissimilarity is undefined, one can hope that partitioning can recover through transitivity. Nonetheless, if there is only partial temporal overlap spurious dissimilarity measurements can arise.Consider the fictional example of Fig. 14. Suppose that segments s1 and s2 belong to object 1, while s3 belongs to object 2. Moreover, let us assume that both objects follow a similar motion up to frame f1, when they start to move differently and d(1,3)(f) increases. Consequently, after taking the maximum distance, D(1,3) will be high, capturing the fact that the segments belong to different objects. Nonetheless, s2 disappears at frame f0 before f1 either due to occlusion or tracking limitations. This results in low dissimilarities D(1,2) and D(2,3). Therefore, s2 bridges s1 and s3 not due to noisy measurements but because of partial temporal overlap between the segments. Even if a more elaborate partitioning algorithm such as spectral clustering is used, one will still have to risk over-segmentation or under-segmentation.A real example of this phenomenon is depicted in Figs. 13 and 15. In the first two rows of Fig. 13 we see for video cars9 of the FBMS-59 dataset the segmentations S5 and S6 of subsequences W5 and W6 spanning frames (33,42) and (41,50) respectively. The time span of trajectories in a subsequence may exceed its temporal boundaries. Therefore, in Fig. 13 we can see the segmentation of W5 continuing in W6 and vice versa. In its last row we see the segments that arise after combining both subsequences. For illustrative purposes, the two outer frames of the last row are also repeated in the top row of Fig. 15.A graph representation of the segments is shown in the bottom row of Fig. 15 along with their dissimilarity matrix D, where each node has the same color with the corresponding segment. The empty entries in D correspond to segment pairs with no temporal overlap. To facilitate understanding, we have arranged the nodes, in a way where nodes of the same object are consecutive. To avoid clutter in the graph we have added edges only for segments having a distance below 2. We expect that the distance matrix should have a block diagonal pattern. Indeed, there are blocks with low inter-block distances. To achieve a correct segmentation we also need the off diagonal blocks to have high distances, which is also the case with the exception of the D(4,8) (the entries with red background in the dissimilarity matrix of Fig. 15). This exception occurs not due to noisy measurements but because of occlusion. Namely, the magenta segment of the background gets occluded by the time the van starts moving (red segment) and provides a link between the van and the background, as for its time span the van and the background perform the same motion. This is also depicted in the graph of Fig. 15 where there is an edge between the magenta and the red node. This cross-object leakage is ubiquitous both in the case of trajectories and segments. However, when using segments it is more easily identified and modeled. In Fig. 16we see the segmentation produced by [20], which although in general has shown exceptional results, in this case it fails and merges the van with the background.To resolve such cases, we model segmentation as a graph coloring problem [32]. The goal in graph coloring is to assign colors to the nodes of a graph in a way where nodes connected by an edge have different colors while the minimum number of colors is used. In our problem assume that we have segments S={s1,…,sM} and a distance matrix D. From the initial segments' graph, we extract a complementary graph with the same set of nodes but with edges U where,(13)U=sisj|Dij≥εand ε is a global, predefined threshold. Intuitively, U contains pairs of segments with high dissimilarity that we eventually do not want to assign to the same object. The complementary graph for the example of Fig. 15 is shown in Fig. 17for ε=3.2.When our graph coloring algorithm concludes the chromatic number (the minimum number of required colors) will be equal to the number of objects and segments with the same color will be assigned to the same object. Of course there can be more than one coloring with the same number of colors. In Section 6.3 we will define a criterion to select a single one.In general, to decide whether a graph admits a coloring with a certain number of colors is an NP-complete problem and can be intractable even for a small graph. To avoid such cases, the algorithm we have devised takes into account some of the inherent characteristics of our application domain. First of all, the size of the graphs is small (usually less than 20 segments). Even for long video sequences where a large number of segments may accumulate as subsequences are merged, segments that are far in the past or in the future with respect to the common time span of the subsequences can safely remain isolated, thus being excluded from the coloring procedure. For example, assume that we are processing a video and at some point we have to merge 2 subsequences spanning frames [1,100] and [99,200]. A segment that appears at frame 190 can be excluded from the coloring procedure and be reintegrated when coloring concludes. Moreover, since the possible colors for a segment are constrained by its neighbors, segments with more edges will usually have a limited color range compared to segments with a smaller number of edges. Another useful attribute of our application domain is that the edges of the complementary graph and consequently the possible colorings have to respect the physical correspondence between the segments. Hence, the number of possible colorings is limited since each coloring has to represent a realistic interpretation of the relationship between the segments with respect to their motion.When our algorithm starts, it will try to color a graph with one color. If that is not possible the number of available colors will increase by one until all segments can be colored. For a certain number of colors N our algorithm proceeds as follows. Initially, segment s with the largest number of edges is selected and a random color is assigned to it. The next uncolored segment selected is one that is preferably connected to s and in case of draws or if there is none connected to s, the segment with most edges. We denote this segment as s′. If s′ is connected to s it must be colored with a different color otherwise it can either take the same color or a new one and two partial colorings will arise. Next our algorithm will examine every partial coloring separately. For a given partial coloring the next segment s′′ chosen, is primarily the one with most edges to the already colored s,s′ and secondly the one with most edges in general. All possible colors for s′′ will be examined. At any point partial colorings with more than N colors are discarded. If no partial colorings remain, our algorithm restarts with N+1 colors. The flow of our algorithm is illustrated in Algorithm 18. Instances of the color tree structure mentioned in Algorithm 18 are depicted in Fig. 19.To facilitate understanding let's consider the example of Fig. 17. Initially, our algorithm will try to color the graph with a single color. The first node selected is 2 as no other node has more edges. The single available color will be assigned to 2. The next node selected will be 4 as it is connected to 2 and has more edges compared to the other uncolored nodes. Since 2 and 4 are connected, 4 must be colored with a different color. As no other color is available our algorithm will restart for two colors. Again, there are no possible colorings since after nodes 2 and 4 are assigned different colors, node 7 is chosen which is connected to both 2 and 4 and consequently must be colored differently. Since no other color is available, our algorithm will restart with three colors. The course of our algorithm for our example with three colors is illustrated in Fig. 18. By the same logic node 2 will be selected and colored, then node 4 will be selected and assigned to a different color. The next node selected is 7, because it is connected to both 2 and 4 and also has more edges compared to the rest of the nodes that are also connected to both 2 and 4. Again node 7 will take a different color and at this point no new colors are permitted. The next node will be 1. No new colors are allowed but 1 can take the same color as node 2, since they are not connected by an edge. Our algorithm continues by selecting and coloring nodes 5, 6 and 3 using the available colors. The final node will be 8 which is connected to nodes 2 and 3 that both have the same color as can be seen in Fig. 18. Node 8 can take any of the other two available colors. Our algorithm will return both possible colorings. In the next section we will see how to pick one out of the two.It is interesting to compare Fig. 18a to b where the only difference is that nodes are selected sequentially without taking into consideration their edges. Although the final colorings are identical in both cases, in Fig. 18 many more partial colorings had to be examined and rejected since more than N=3 colors where required. In general, our algorithm will return all possible colorings, since for every node every possible color is considered and in most cases of the FBMS-59 dataset our algorithm computes all possible colorings in a matter of seconds.However, we should make clear that the complexity of our algorithm is still exponential since the underlying decision problem is NP-complete. In general, if N colors are used to color a graph with M nodes there are NMpossible colorings to be examined. To avoid enumerating all the colorings our algorithm processes each node sequentially and stores only feasible partial colorings. Concretely, under the i-th partial coloring of j−1 nodes, the colors to be stored for the j-th node will not be N but vij,(14)vij=uij−cij,uij=Nuij−cij+1,uij<Nwhere uijis the number of colors used so far in the partial coloring and cijis the number of colors used by already colored nodes that are connected to the j-th node. While there is no universal guarantee that vijwill be substantially smaller than N, it is what usually happens in our application domain since in most cases nodes corresponding to different objects will be connected and therefore cannot admit the same color. Nonetheless, there are still cases where the coloring tree grows intractably large (more than 20000 partial colorings). If that happens our coloring algorithm stops and we proceed by greedily merging segments with low dissimilarity in order to reduce the number of nodes. Then we reapply our coloring algorithm on the reduced graph. We should note that when preliminary merges are required, we do not allow segments in set U to be assigned to the same object.As we saw in the previous section when our algorithm concludes, it will return all possible colorings. Suppose that we have a set of segments S={s1,…,sM}. Let's assume that our algorithm computed Γ different colorings and the chromatic number is N*. We denote the set of all possible colorings as,(15)pN*S=pN*S1,…,pN*SΓwhere:(16)pN*Si=C1i,…,CN*iand Cj(i) contains the nodes that are assigned to color j and comprise a single cluster.To evaluate each of the possible colorings we define for cluster Cj(i) the notion of in-cluster resistance:(17)Rji=maxDlm|sl∈Cji∧sm∈Cjiand for a pair of clusters Cj(i),Ck(i) the cross-cluster resistance as:(18)Rjki=maxDlm|sl∈Cji∧sm∈Ckiwhere ∧ denotes logical conjunction.Usually, in graph theory the sum of distances is used [33], however in our case we use the maximum as small distances can be deceptive. The normalized resistance of a coloringpN*Siis defined as:(19)Ri=∑j=1N*Rji∑k≠jRjki.We choose the coloring with minimum normalized resistance. Namely,pN*Si*where:(20)i*=argminiRi.

@&#CONCLUSIONS@&#
In this paper we presented an efficient algorithm for motion segmentation that is both scalable and accurate. It can operate in a streaming environment and allows the seamless use of higher order motion models through MRC, thus not penalizing non translational motion. Moreover, we provided a solution to the cross-object leakage problem that is caused by occlusion. Our algorithm performs very well even as we vary its parameters.We plan to optimize our current implementation since there is a large margin for improvement in terms of computational time. After completing this, we will use our method to facilitate the creation of annotated sets for tasks such as activity analysis and video retrieval. Another application where our method would be useful is pose estimation. Furthermore, the deployment of MRC in areas other than motion segmentation would be of interest.