@&#MAIN-TITLE@&#
Mixed integer second-order cone programming formulations for variable selection in linear regression

@&#HIGHLIGHTS@&#
AIC/BIC minimization, and adjusted R2 maximization problems are considered.These problems are formulated as mixed integer second-order cone programming problems.Experiments shows results of better quality than obtained by stepwise regression.

@&#KEYPHRASES@&#
Integer programming,Variable selection,Multiple linear regression,Information criterion,Second-order cone programming,

@&#ABSTRACT@&#
This study concerns a method of selecting the best subset of explanatory variables in a multiple linear regression model. Goodness-of-fit measures, for example, adjusted R2, AIC, and BIC, are generally used to evaluate a subset regression model. Although variable selection with regard to these measures is usually performed with a stepwise regression method, it does not always provide the best subset of explanatory variables. In this paper, we propose mixed integer second-order cone programming formulations for selecting the best subset of variables with respect to adjusted R2, AIC, and BIC. Computational experiments show that, in terms of these measures, the proposed formulations yield better solutions than those provided by common stepwise regression methods.

@&#INTRODUCTION@&#
Variable selection in statistics, also known as feature selection or attribute selection in machine learning, is the method of choosing a set of significant variables from many candidate variables for model construction. Major benefits of variable selection are as follows (Guyon & Elisseeff, 2003; Whittingham, Stephens, Bradbury, & Freckleton, 2006): (i) improving predictive performance of a statistical model by preventing overfitting, (ii) identifying a model that captures the essence of a system, and (iii) providing a computationally efficient set of explanatory variables. Because of these benefits, studies on variable selection are of supreme importance in multiple regression analysis (Burnham & Anderson, 2002; Hocking, 1976; Miller, 2002).There are several goodness-of-fit (GOF) measures, such as adjusted R2 (Wherry, 1931), Akaike information criterion (AIC; Akaike, 1974), and Bayesian information criterion (BIC; Schwarz, 1978), that can be used to evaluate a subset regression model. A direct way of searching for the best-subset regression model is evaluating all possible subset models. Though procedures for this have been described (Furnival & Wilson Jr., 1974; Gatu & Kontoghiorghes, 2006; Hofmann, Gatu, & Kontoghiorghes, 2007), this task is practically infeasible unless the number of candidate variables is small. Accordingly, the previous studies have focused on search strategies for approximately solving the problem (Blum & Langley, 1997; Guyon & Elisseeff, 2003; Kohavi & John, 1997; Liu & Motoda, 2007). Among them is the well-known stepwise regression method (Efroymson, 1960), which repeats forward selection, i.e., adding one significant variable, and backward elimination, i.e., eliminating one redundant variable, until a stopping condition is satisfied. Ridge regression (Hoerl & Kennard, 1970), Lasso (Tibshirani, 1996), and several metaheuristics (e.g., Meiri & Zahavi, 2006) are also used for variable selection. Moreover, algorithms that maximize a submodular set function have received attention for the purpose of variable selection (Das & Kempe, 2008, 2011).Although these heuristic optimization algorithms can handle large-scale variable selection problems, they do not necessarily select the best set of variables. In addition, other shortcomings of stepwise regression have been pointed out (e.g., Whittingham et al., 2006).In contrast to heuristic optimization algorithms, integer programming has the potential to determine the best subset of explanatory variables in a multiple linear regression model. When the number of variables to be selected is given a priori, the variable selection problems using adjusted R2, AIC, and BIC as GOF measures can be reduced to ones of minimizing the sum of squared deviation, which can be formulated as mixed integer quadratic programming (MIQP) problems (Arthanari & Dodge, 1981; Bertsimas, King, & Mazumder, 2014; Bertsimas & Shioda, 2009; Konno & Yamamoto, 2009). Bertsimas and Shioda (2009) utilized a tailored branch-and-bound procedure to solve such MIQP problems. Bertsimas et al. (2014) developed discrete first-order algorithms to warm start the MIQP computation. It is also known that, by using the mean absolute deviation as a deviation measure, the variable selection problem can be formulated as a mixed integer linear programming (MILP) problem (Arthanari & Dodge, 1981). Konno and Yamamoto (2009) solved similar MILP problems by using a mathematical programming solver, and Konno and Takaya (2010) developed a multi-step method to obtain a nearly optimal solution to large-scale problems.However, if one is to take these MIQP or MILP approaches, the number of selected variables has to be fixed in advance. This is disadvantageous because the optimal number of selected variables in terms of a GOF measure is unknown before solving the corresponding variable selection problem. Another tractable case is that, when the residual variance of the best-subset regression model is given, the variable selection problem of minimizing AIC can be formulated as an MIQP problem (Emet, 2006; Skrifvars, Leyffer, & Westerlund, 1998); yet this assumption clearly does not coincide with reality. Recently, the authors of this paper pointed out that the variable selection problem using Mallows’ Cpas a GOF measure can be posed as an MIQP problem (Miyashiro & Takano, 2015); in this case, we need not to fix the number of selected variables in advance.On the other hand, a straightforward formulation for minimizing commonly used information criteria, such as AIC and BIC, leads to a mixed integer programming problem with a nonconvex objective function, which is computationally intractable even if the integrality of the problem is relaxed.The purpose of this study is to develop an exact method for selecting the best subset of explanatory variables in a multiple linear regression model. To this end, we propose mixed integer second-order cone programming (MISOCP) formulations to build the best-subset regression model in terms of adjusted R2, AIC, and BIC, without pre-specifying the number of variables to be selected. The continuous relaxation of an MISOCP problem is a second-order cone programming (SOCP) problem, which is solvable in polynomial time; thus, the proposed MISOCP formulations can be handled by recent mathematical programming solvers using a branch-and-bound procedure11A concise description of SOCP and MISOCP is provided in Appendix A. See Lobo, Vandenberghe, Boyd, and Lebret (1998) for more details on SOCP, and Benson and Sağram (2013) for an explanation of MISOCP.. Although SOCP has been used in the context of regression/classification analysis for, e.g., efficient computation (Debnath, Muramatsu, & Takahashi, 2005), robust modeling (Shivaswamy, Bhattacharyya, & Smola, 2006; Trafalis & Gilbert, 2006), and dimensionality reduction (Yuan, Ekici, Lu, & Monteiro, 2007), none of the existing studies have applied SOCP/MISOCP techniques to variable selection based on a GOF measure.Using data sets from the UCI Machine Learning Repository (Bache & Lichman, 2013), we conducted computational experiments to assess the effectiveness of the proposed MISOCP formulations. The results show that they can provide the best subset of variables for small-sized instances in minutes. Furthermore, for medium-sized instances, they often generate better subsets of variables than stepwise regression methods do.This section contains a brief review of variable selection and GOF measures and mentions previous research on variable selection using integer programming.Given n data points,(yi;xi1,xi2,…,xip)fori=1,2,…,n,yiis referred to as an explained variable (or dependent variable) andxij(j=1,2,…,p)as explanatory variables (or independent variables). In multiple linear regression analysis, the following linear model is constructed for predicting the value of yi:yi=b+a1xi1+a2xi2+⋯+apxip+ϵi,whereϵiis a prediction residual corresponding to the ith data point. The ordinary least squares method estimates the value of the intercept b and coefficientsa=(a1,a2,…,ap)such that the sum of squared residuals∑i=1nϵi2is minimized.This study considers the variable selection problem, i.e., the problem of selecting the best subset of variables from the set of candidate explanatory variables. We denote byS(⊆{1,2,…,p})an index set of selected variables and by k the number of selected variables, i.e.,k=|S|. The resultant subset regression model is expressed as(1)yi=b+∑j∈Sajxij+ɛi,where εiis a prediction residual corresponding to the ith data point given by the regression model (1).Several GOF measures have been proposed to evaluate the subset regression model (1), and some of them are explained in Sections 2.2 and 2.3. Throughout this paper, it is assumed that the number of data points, n, is much larger than the total number of candidate variables, p.The adjusted R2 (Wherry, 1931), hereafterR¯2,for the subset regression model (1) is defined asR¯2=1−∑i=1nɛi2/(n−k−1)∑i=1n(yi−y¯)2/(n−1),wherey¯=(∑i=1nyi)/n. Note that variable selection with respect toR¯2is to maximizeR¯2,which is equivalent to minimizing(2)∑i=1nɛi2/(n−k−1),because other terms are all constants. Accordingly, if the sum of squared residuals∑i=1nɛi2is equal in two models, the model with the smaller k is better. Conversely, if k is fixed a priori, minimizing∑i=1nɛi2leads to the best model.Assume that the prediction residuals, i.e.,ɛ1,ɛ2,…,ɛn,are independent and all normally distributed with zero mean and the variance σ2. Then, the log likelihood function of the subset regression model (1) can be written as follows:(3)ℓ(aS,b,σ2)=−n2log2π−n2logσ2−12σ2∑i=1nɛi2,whereaS=(aj;j∈S). After partial differentiation, the maximum likelihood estimator of σ2 becomes(4)σ2=1n∑i=1nɛi2.By substituting (4) into (3), the maximal value of the log likelihood function is expressed asmaxaS,b,σ2ℓ(aS,b,σ2)=maxaS,b(−n2log2π−n2log(1n∑i=1nɛi2)−n2).The Akaike information criterion (AIC; Akaike, 1974) of the subset regression model (1) is defined as(5)−2maxaS,b,σ2ℓ(aS,b,σ2)+2(k+2)=minaS,b(nlog2π+nlog(1n∑i=1nɛi2)+n)+2(k+2),wherek+2is the number of parameters (aS, b, and σ2) in the model. By omitting the constant terms from (5), the variable selection problem with respect to AIC reduces to one of minimizing(6)nlog(1n∑i=1nɛi2)+2k.The Bayesian information criterion (BIC; Schwarz, 1978) is another information criterion, as popular as AIC. The BIC of the subset regression model (1) is defined as−2maxaS,b,σ2ℓ(aS,b,σ2)+(k+2)logn.As in the case of AIC, the function (7) is minimized to solve the variable selection problem in terms of BIC:(7)nlog(1n∑i=1nɛi2)+klogn.The following also holds for AIC and BIC: if the sum of squared residuals∑i=1nɛi2is the same in two models, the model with the smaller k is better. Conversely, if k is fixed a priori, minimizing∑i=1nɛi2leads to the best model.From (2), (6), and (7), if the number k of selected variables is predetermined, one only has to minimize the sum of squared residuals∑i=1nɛi2in order to select the best k variables by means ofR¯2,AIC, and BIC, respectively. This minimization problem can be formulated as an MIQP problem (Arthanari & Dodge, 1981; Bertsimas et al., 2014; Bertsimas & Shioda, 2009; Konno & Yamamoto, 2009), as follows.Letzj(j=1,2,…,p)be a 0–1 variable such thatzj=1if the jth candidate variable is selected, andzj=0otherwise. The variable selection problem with a specified k is formulated as the following MIQP problem22When solving problem (8)–(12), substituting constraint (9) into the objective function (8) leads to a simpler MIQP formulation.:(8)minimize∑i=1nɛi2(9)subjecttoɛi=yi−(b+∑j=1pajxij)(i=1,2,…,n),(10)−Mzj≤aj≤Mzj(j=1,2,…,p),(11)∑j=1pzj=k,(12)a∈Rp,b∈R,ɛ∈Rn,z∈{0,1}p,where M is a sufficiently large positive constant. Ifzj=0,the jth candidate variable is eliminated from the regression model, because its coefficient ajhas to be 0 from constraint (10); ifzj=1,constraint (10) is invalidated. Thus, the number of chosen explanatory variables in the model becomes k due to constraint (11). Hence, problem (8)–(12) is a correct formulation for minimizing the sum of squared residuals under the condition that the number of selected explanatory variables is k.Formulation (8)–(12) is a minimization problem with a convex quadratic objective function subject to linear and integrality constraints, i.e., an MIQP problem. A branch-and-bound procedure handles an integer programming problem by relaxing its integrality constraints and then repeatedly solving the relaxation problems with additional constraints; hence, a key factor for a branch-and-bound procedure is whether the continuous relaxation problem is computationally tractable. In this regard, the continuous relaxation of an MIQP problem is a quadratic programming (QP) problem, which is solvable in polynomial time. Therefore a branch-and-bound procedure should work well at solving problem (8)–(12).Other than minimizing the sum of squared residuals, some researchers have considered minimizing the mean absolute deviations, i.e.,1n∑i=1n|ɛi|(Arthanari & Dodge, 1981; Bertsimas et al., 2014; Konno & Takaya, 2010; Konno & Yamamoto, 2009). In this case, the corresponding variable selection problem with fixed k can be formulated as an MILP problem, which is simpler than an MIQP one.Nevertheless, in the MIQP and MILP approaches, the value of k needs to be fixed in advance; as mentioned before, the optimal number of selected variables in terms of a GOF measure is unknown before solving the variable selection problem. Accordingly, to find the best subset of variables using these approaches, it is necessary to solve allp+1problems, i.e., MIQP/MILP problems fork=0,1,…,p.In this section, we propose MISOCP formulations for maximizingR¯2,minimizing AIC, and minimizing BIC, to select the best set of variables under the condition that the number of selected variables is not a given constant but a variable. To emphasize this, we denote the number not by k but by∑j=1pzj,as in the integer programming manner. Note that the continuous relaxation problems of the proposed MISOCP formulations belong to a computationally tractable class, SOCP (see Appendix A for descriptions of SOCP and MISOCP).In view of (2), the variable selection problem of maximizingR¯2can be formulated as follows:(13)minimize∑i=1nɛi2/(n−∑j=1pzj−1)(14)subjecttoɛi=yi−(b+∑j=1pajxij)(i=1,2,…,n),(15)−Mzj≤aj≤Mzj(j=1,2,…,p),(16)a∈Rp,b∈R,ɛ∈Rn,z∈{0,1}p.The continuous relaxation problem of the above is a convex programming problem (see Boyd and Vandenberghe, 2004, example 3.18), which is solvable in polynomial time. In fact, except for the integrality constraint in (16), problem (13)–(16) has the same structure as problem (A.1) and (A.2), which can be transformed into an SOCP problem (A.3)–(A.7); see Appendix A. Hence, importing the integrality constraint, we easily come up with an MISOCP formulation for maximizingR¯2:(17)minimizef(18)subjecttoɛi=yi−(b+∑j=1pajxij)(i=1,2,…,n),(19)∑i=1nɛi2≤f·(n−∑j=1pzj−1),(20)−Mzj≤aj≤Mzj(j=1,2,…,p),(21)a∈Rp,b∈R,ɛ∈Rn,f∈R+,z∈{0,1}p.Note that constraint (19) is a nonlinear but hyperbolic constraint becausef·(n−∑j=1pzj−1)in (19) corresponds to f · g, the right-hand-side of constraint (A.4) in Appendix A; thus it can be represented as a second-order cone constraint.In view of (6), the variable selection problem of minimizing AIC can be formulated as follows:(22)minimizenlog(1n∑i=1nɛi2)+2∑j=1pzj(23)subjecttoɛi=yi−(b+∑j=1pajxij)(i=1,2,…,n),(24)−Mzj≤aj≤Mzj(j=1,2,…,p),(25)a∈Rp,b∈R,ɛ∈Rn,z∈{0,1}p.However, this straightforward formulation forces us to solve a mixed integer programming problem with a nonconvex objective function (22). Hence, even its continuous relaxation problem is computationally intractable. This is an undesirable outcome.In the following, we make the above problem computationally tractable. The objective function (22) can be converted as follows:(26)minimizenlog(1n∑i=1nɛi2)+2∑j=1pzj⟺minimizelog(1n∑i=1nɛi2)+2n∑j=1pzj⟺minimizeexp(log(1n∑i=1nɛi2)+2n∑j=1pzj)⟺minimize(1n∑i=1nɛi2)·exp(2n∑j=1pzj)⟺minimize(∑i=1nɛi2)·exp(2n∑j=1pzj).By introducing a continuous variable f, which represents an upper bound of the objective function (26), we compose an intermediate formulation:(27)minimizefsubjectto∑i=1nɛi2≤f·exp(−2n∑j=1pzj),constraints (23) and (24),a∈Rp,b∈R,ɛ∈Rn,f∈R+,z∈{0,1}p.We can now resolve the nonlinearity in constraint (27). Note that∑j=1pzjis always integer-valued because ofz∈ {0, 1}p. Letwj(j=0,1,…,p)be a 0–1 variable such thatwj=1if and only if∑j=1pzj=j; this can be achieved by applying the following constraints:∑j=0p(j·wj)=∑j=1pzjand∑j=0pwj=1. Constraint (27) is equivalently expressed using 0-1 variables wj, as follows:∑i=1nɛi2≤f·exp(−2n∑j=1pzj)⟺∑i=1nɛi2≤f·g,g=exp(−2n∑j=1pzj)⟺{∑i=1nɛi2≤f·g,g=∑j=0p(wj·exp(−2jn)),∑j=0p(j·wj)=∑j=1pzj,∑j=0pwj=1,wj∈{0,1}(j=0,1,…,p),where g is another continuous variable; here g is redundant but used for ease of understanding. Note that the constraintg=∑j=0p(wj·exp(−2j/n))is a linear function with respect to wj. This linearization technique of a nonlinear function is called “special ordered set type 1” (SOS type 1; Beale, 1963; Beale & Tomlin, 1970), and it is well-known in the area of integer programming.Again, the constraint∑i=1nɛi2≤f·gis a nonlinear but hyperbolic constraint, and thus, it is representable as a second-order cone constraint (see Appendix A). Consequently, substituting g with∑j=0p(wj·exp(−2j/n)),we obtain the following MISOCP formulation for minimizing AIC:(28)minimizef(29)subjecttoɛi=yi−(b+∑j=1pajxij)(i=1,2,…,n),(30)∑i=1nɛi2≤f·∑j=0p(wj·exp(−2jn)),(31)∑j=0p(j·wj)=∑j=1pzj,(32)∑j=0pwj=1,(33)−Mzj≤aj≤Mzj(j=1,2,…,p),(34)a∈Rn,b∈R,ɛ∈Rn,f∈R+,w∈{0,1}p+1,z∈{0,1}p.Next, we propose an MISOCP formulation for minimizing BIC. The difference between minimizing AIC and minimizing BIC lies in the second terms of the objective functions (6) and (7). Hence, replacing 2j in constraint (30) with j log n yields the following constraint:(35)∑i=1nɛi2≤f·∑j=0p(wj·exp(−jlognn))=f·∑j=0p(wj·n−j/n).Accordingly, as an MISOCP formulation for minimizing BIC, we obtain a problem consisting of (28)–(35) except (30).Finally, we mention minimizing other information criteria besides AIC and BIC. In fact, there are several information criteria, for example, corrected AIC (Sugiura, 1978), the Hannan–Quinn information criterion (Hannan & Quinn, 1979), and the residual information criterion (RIC; Leng, 2013; Shi & Tsai, 2002). Using the transformation technique explained in this section, the problem of minimizing such an information criterion can also be formulated as an MISOCP problem.This section compares the computational results of the proposed MISOCP formulation with those of well-known stepwise regression methods.We downloaded eight data sets from the UCI Machine Learning Repository (Bache & Lichman, 2013) for the regression analysis. The “Solar Flare” data set has three variables (i.e., three classes of flares production) to be predicted, and accordingly, ten instances of variable selection problems were prepared. Table 1lists these instances, where n and p are respectively the number of data points and that of candidate variables.For the ForestFires instance, we created interaction terms from the variables of the x-axis and y-axis spatial coordinates. In the Crime instance, variables having missing values for most of the data points were removed. For all data sets, each categorical variable was transformed into as many dummy variables as its distinct values. To avoid numerical instability, each quantitative variable was standardized so that its mean was zero and its standard deviation one. In addition, data points including a missing value and redundant variables having a constant value were all eliminated.We solvedR¯2maximization, AIC minimization, and BIC minimization problems in the proposed MISOCP formulations by using CPLEX 12.5 (ILOG, 2012) as a mathematical programming solver. We implemented constraints (20) and (33), i.e.,−Mzj≤aj≤Mzj(j=1,2,…,p),via indicator, a function implemented in CPLEX. Using this function lets us avoid manually determining the value of M in constraints (20) and (33). For further details of the implementation of constraints (20) and (33) in MISOCP, see Appendix B.All computations involved in solving the MISOCP problems were performed on a Windows PC33CPU: Intel Xeon W5590 3.33 gigahertz × 2; RAM: 24 gigabyte; OS: 64bit Windows 7 Ultimate SP1; chipset: Intel 5520.. For each instance, 16 gigabyte of memory, eight threads, and up to 10,000 seconds were allocated to the branch-and-bound procedure. For comparison, we also solved the same instances by using stepwise regression methods with the LinearModel.stepwise function implemented in the statistics toolbox of MATLAB R2012b (MathWorks, 2012) on a Windows PC44CPU: Intel Core i7-2600S 2.80 gigahertz; RAM: 8 gigabyte; OS: 64bit Windows 7 Professional SP1; chipset: Intel Q67 Express.. The stepwise regression methods iteratively add or eliminate a variable to improveR¯2/AIC/BIC.The results of maximizingR¯2,minimizing AIC, and minimizing BIC are shown in Tables 2–4, respectively. The entries in the “method” column mean as follows:•SWconst: stepwise regression starting with no explanatory variables,SWall: stepwise regression starting with all candidate variables,MISOCP: the proposed MISOCP formulations, i.e., (17)–(21) for maximizingR¯2,(28)–(34) for minimizing AIC, and (28)–(35) except (30) for minimizing BIC.For each instance in the tables, the bestR¯2/AIC/BIC value among SWconst, SWall, and MISOCP is bold-faced. The column “#vars” is the number of selected variables, and the column “time (second)” is computational time in seconds. Regarding MISOCP, each computation was terminated if the computation took more than 10,000 seconds; the column “B&B gap” is the branch-and-bound gap. For each instance, the obtained subset of variables is not necessarily optimal if the branch-and-bound gap is more than zero.First, we found that the differences between the results of SWconst and those of SWall are large. Tables 2–4 show that SWconst and SWall selected quite different sets of variables in many cases. For example, for the Automobile instance, the differences in the obtained AIC values between SWconst and SWall were more than 20, which is hard to ignore. Additionally, in many cases, the number of selected variables (#vars) greatly varied between SWconst and SWall.Next, the results show that MISOCP finished selecting the subsets of variables in minutes for small-sized instances involving less than 30 candidate variables. Note that integer programming proves that these subsets of variables are the best, and this is in clear contrast to heuristic approaches. Although the MISOCP problems for the ForestFires, Automobile, and Crime instances were not solved within 10,000 seconds, the GOF measures of the obtained subsets were comparable to those obtained by SWconst and SWall. In addition, for all instances of maximizingR¯2,MISOCP had the maximalR¯2values among the three methods. This result implies that the stepwise regression methods have difficulty selecting the best subset of variables based onR¯2.Table 5 shows the number of times SWconst/SWall/MISOCP obtained the bestR¯2/AIC/BIC values out of the ten instances; Table 6 shows those for instances that were not solved within 10,000 seconds by MISOCP (ForestFires, Automobile, and Crime forR¯2and BIC; BreastCancer, ForestFires, Automobile, and Crime for AIC). These tables clearly prove the superiority of MISOCP in terms ofR¯2/AIC/BIC, even for the large instances. Stepwise regression methods are greedy heuristics and naturally do not always provide the best subset of explanatory variables; however, one can also see that the results for SWconst and SWall are less robust than expected.Regarding the computational time, MISOCP took much longer than the stepwise regression methods did. This reflects the difference between an exact method that pursues proof of optimality through a branch-and-bound procedure and the heuristic nature of stepwise regression methods.The previous subsectionconfirmed that the MISOCP approach generally provided better solutions than those of the stepwise methods, but took longer doing so. This subsection examines the effect of giving a solution obtained by stepwise methods to the MISOCP approach as an initial solution for the branch-and-bound process.Tables 7–9show that the effect of supplying initial solutions for maximizingR¯2,minimizing AIC, and minimizing BIC, respectively. The rows corresponding to MISOCPSW in the “method” column are the results obtained by the MISOCP formulations that employed the better of the two solutions of SWconst and SWall as the initial solution. For each instance in the tables, the better of the GOF values is bold-faced.In terms of solution quality, for the instances that were solved within 10,000 seconds, GOF values obtained by MISOCPSW were naturally the same as those obtained by MISOCP. For the instances that were not solved within 10,000 seconds, no solutions obtained by MISOCPSW were worse than those obtained by MISOCP. In fact, about half of the solutions obtained by MISOCPSW were better. In particular, forR¯2and AIC of Automobile, the solutions obtained by MISOCPSW were superior to not only those of MISOCP but also those of SWconst and SWall.Regarding computational time, giving an initial solution was not so effective for the instances that were solved within 10,000 seconds. In fact, for those instances, the average computational time increased by 4.54 percent. However, for the instances that were not solved within 10,000 seconds, giving an initial solution decreased the branch-and-bound gap by 0.32 percentage points on average.From the above, we concluded that giving a solution obtained by stepwise methods as an initial solution is somewhat effective for large instances.The MISOCP approach should also be compared to another exact method, the MIQP approach explained in Section 2.4. Computations for solving the MIQP problems were performed on the same PC used for the MISOCP approach, and CPLEX 12.5 (ILOG, 2012) was also employed as a mathematical programming solver. Table 10lists the computational times for solving one MISOCP problem and for solving MIQP problems fork=0,1,…,p.For the instances from Housing to BreastCancer, i.e., the instances that were solved within 10,000 seconds by MISOCP, it was faster to solvep+1MIQP problems than to solve one MISOCP problem. However, neither the MIQP nor MISOCP approaches finished solving the other instances, i.e., ForestFires, Automobile, and Crime, within a day of computation (Table 10). For such large instances, one should limit the computational time for practical purposes, say, to within 10,000 seconds. As shown in Tables 2–4, the MISOCP approach succeeded in finding good subsets of variables for the large instances within 10,000 seconds. On the other hand, subsets of variables obtained by the MIQP approach in the limited computational time depended heavily on the order of solving thep+1MIQP problems. In fact, our supplementary experiments found that, the MIQP approach failed to solve many of the problems within 10,000 seconds (11 out of 64 MIQP problems for ForestFires, 25 out of 66 MIQP ones for Automobile, and 71 out of 101 MIQP ones for Crime). In such cases, the MIQP approach likely wastes 10,000 seconds in finding subsets of variables of poor quality, because few MIQP problems can be solved within 10,000 seconds. Hence, the MISOCP approach, which optimizes the number of selected variables simultaneously, has an advantage over the MIQP one in getting better subsets of variables for large instances when the computational time is limited.The reason why the MIQP approach was faster at proving optimality for small instances can be explained as follows. In each node of the branch-and-bound procedure, the dual-simplex method implemented in the mathematical programming solver handles a series of QP problems that correspond to a continuous relaxation of the original MIQP. At a child node in the branch-and-bound tree, the QP problem to be solved is almost the same as the problem at its parent node. Accordingly, the dual-simplex method needs only a few iterations to solve the QP problem, i.e., “warm-start” works well for an MIQP problem. In contrast, for an MISOCP problem, the branch-and-bound procedure solves a series of SOCP problems, which need an interior point method. Developing warm-start algorithms for interior point methods is still a topic of study in mathematical programming, and such algorithms have not yet been implemented in current commercial solvers that can handle MISOCP. This results in the difference between the computational times of the MIQP and MISOCP approaches.Although at present the MISOCP approach needs a longer computational time than the MIQP one does, we emphasize that the proposed formulation technique allows us to transform the variable selection problem into a single MISOCP problem, not a collection of problems. Numerical techniques for solving SOCP and MISOCP problems are areas of active research, and thus, we expect that the proposed MISOCP formulations will be more valuable in the near future.

@&#CONCLUSIONS@&#
