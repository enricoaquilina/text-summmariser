@&#MAIN-TITLE@&#
Evolution of software in automated production systems: Challenges and research directions

@&#HIGHLIGHTS@&#
Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved.

@&#KEYPHRASES@&#
Evolution,Automation,Automated production systems,Software engineering,

@&#ABSTRACT@&#
Graphical abstractImage, graphical abstractModern trends in manufacturing are defined by mass customization, small lot sizes, high variability of product types, and a changing product portfolio during the lifecycle of an automated production system (aPS) (Lüder et al., 2005; Rzevski, 2003). These trends imply more complex aPS (Mcfarlane and Bussmann, 2000), which support changes in the physical layout of the aPS including extensive technical updates. The complexity of the aPS, including automation hardware and automation software (called software henceforth), is rising. Since the proportion of system functionality that is realized by software is increasing (Thramboulidis, 2010), concepts for supporting automation engineers in handling this complexity are strongly required. Software as well as software engineering in this domain need to fulfill specific requirements, e.g. regarding real-time and reliability (Vogel-Heuser et al., 2014a, 2014b, 2014c). Fundamental methods like variability modeling, tracing etc., which enable software evolution, are until now limited to the software domain. For automated products, e.g., washing machines, and automated production systems, i.e., systems that produce (automated) products, those fundamental methods need to be adapted to the other disciplines and linked to well-known and well-established domain-specific methods like the design structure matrix in mechanical engineering. This article shows the state of the art of those fundamental methods in software engineering and indicates the weaknesses when transferred to evolution of aPS.aPS are comprised of mechanical parts, electrical and electronic parts (automation hardware) and software, all closely interwoven, and thus represent a special class of mechatronic systems (Bonfè and Fantuzzi, 2003; Rzevski, 2003) and consist of mechatronic sub-systems like sensors and actuators. Therefore, development methods for mechatronic systems can be applied (see Section 2). However, aPS impose special requirements on the development process:•Some mechatronic systems are only designed for a short lifetime of several months or few years (e.g. consumer audio/video devices). These devices will typically not be changed/adapted to new requirements during their life and will not be in the focus of this paper.Sensors and actuators as mechatronic sub-systems of an aPS are designed for a longer lifetime of several years. It can be foreseen that requirements will probably change over their lifetime. Therefore, suitable means should be planned during the development which allow for adaptions to the functionality of these mechatronic systems later. As software can be changed more easily (especially remotely) than mechanical or electrical parts, adaptions to further requirements are often taken into account by changes of the firmware (software) of such systems.The longer the presumable lifetime, and the more complex the mechatronic system is, the more important it is to take changing requirements into account during the development, and to provide suitable means (workflows, models, tools) that allow for adapting these mechatronic systems later on.aPS, a particular type of mechatronic system on which this paper focuses, is designed-to-order systems. These are complex systems, and they have a typical life time in operation of several decades. Naturally, requirements imposed on these systems will change over life time (e.g., caused by changing customer requirements or by changes in legislation). Evolution is, therefore, to be seen as an integral part of their lifecycle.This results in two similar but differing assumptions requiring different design approaches for software engineering for those aPS:A.The need for later evolution should be taken actively into account already in the initial development (requirements engineering, design, implementation) of an aPS, which is intended to remain in operation for years (“Design for Future”).During operation of the aPS, evolution should not be seen as an exception, but as a repetitive activity to maintain – or even increase – the value of the aPS, but in a more reactive manner. Consequently, evolution should be planned, carried out and documented systematically in order to keep the system maintainable over time (“Managed Software Evolution”).The basis for software evolution management was laid within the 1980s in the computer science domain. Lehman (1980) defined laws of software evolution and – among others – identified that systems are subject to dynamics causing continuing changes of software resulting into increasing complexity. Evolution might be triggered in each phase of an aPS's life. Hence, due to their evolution and the long-living character of aPs, their life is characterized as a cycle (Birkhofer et al., 2010). As changes are often performed in order to improve the system by, e.g., correcting faults, maintainability is strongly related to evolution. Software maintainability is “the ease with which a software system can be modified to correct faults, improve performance or other attributes or adapt to change environments” (IEC, 1990). The same standard defines maintainability as “the ease with which a hardware system or component can be retained in, or restored to, a state in which it can perform its required functions”. Accordingly, maintenance can either involve repair or modification actions, which in turn can be adjustments to the environment (referred to as adaptive maintenance) or augmentation of a system's function (Avizienis et al., 2004). In the context of maintainability, obsolescence management, i.e., managing, mitigating and resolving the impact of (sub-)component obsolescence, is one important issue regarding long-living systems (ISO/IEC 25010, 2011).Buckley et al. (2005) introduced four aspects of software changes as the basis for software evolution: temporal properties (when), object of change (where does the change occur), system properties (what), and change support (how). The authors neglect the stake-holders (who) and the reason for change (why), which is essential to address the “nature of [the] evolution phenomenon, its drivers and impact” (Lehman and Ramil, 2002). An approach to explicitly modeling changes of aPS' physical structures and for analyzing their effects on the system's functions is proposed in Göring and Fay (2013) alongwith the physical causes of these changes. However, changes in the aPS domain are often performed for specific reasons beyond pure physical causes – e.g., to adjust an aPS for certain circumstances or to configure it for producing a certain product. Hence, in the context of evolution, the terms changeability and reconfigurability are often named.According to Wiendahl et al. (2007), changeability “is defined as characteristics to accomplish early and foresighted adjustments of the factory's structure and processes”. In contrast, reconfigurability is defined as the “ability of a manufacturing or assembly system to switch with minimal effort and delay to a particular family of workpieces or subassemblies through the addition or removal of functional elements” (Wiendahl et al., 2007).Drivers for evolution are manifold (Westkämper, 2003), and basically result in changes of requirements on the aPS (Legat et al., 2013). Managing (evolving) functional requirements is a well-studied topic in the literature (Ramesh and Jarke, 2001). In contrast, managing non-functional requirements – especially dependability requirements – for aPS is rarely addressed (Fay et al., 2015; Ladiges et al., 2013). One reason is that the relationship between evolution and dependability of a system is vaguely understood until now because both are very complex challenges (Felici, 2003; Machado et al., 2006; Vogel-Heuser et al., 2014c). Continuous integration is a hot topic in software engineering practice. Its goal is to increase the speed of development by very often integrating the different modules of a complex software system including automated integration tests – typically every few days, daily or even after every committed change. This avoids that the different modules diverge too much, which can make integrations in longer intervals very complex. Continuous integration in aPS is addressed in a different way, since changes in the development and operation process of a product and the related automated production systems need to be closely coupled to cope with the increased speed of innovation. For example the different development and operation activities are cyclically operated with different frequencies.To achieve consistent data and activities cross-disciplinary changes were until now often only performed at synchronization points to avoid inconsistencies. Different research groups (CRC 61411http://www.sfb614.de/en/home/, retrieved 8/21/2015., CRC 76822http://www.sfb768.tum.de/index.php?id=5&L=1, retrieved 8/21/2015.) are working on continuous improvement in between these synchronization points. The restructuring of a production unit versus a continuous improvement process of these units is another topic in this field (Schuh et. al 2013). Furthermore, continuous integration of the software and the hardware of an aPS is more complicated than in a pure software setting as the changes in hardware are much slower to realize than in software as well as that simulation models of the hardware system are required for automated integration tests.The paper is structured as follows: in Section 2, the development process for aPS according to VDI/VDE 3695 (2010) is presented introducing classified evolution scenarios and a simple pick and place unit (PPU) as application example. The PPU will be used in the different sections to illustrate selected aspects. Sections 3–5 highlight the different life-cycle phases: requirements and system specification, system design, system realization and implementation. Furthermore, we address several topics, which are relevant for all phases. In Section 6, validation and verification with a view on aPS is discussed, in Section 7 variability management, in Section 8 model-driven engineering and finally in Section 9 traceability are discussed. Sections 3–9 start with the challenges for cross-disciplinary aPS followed by the state of the art in computer science and aPS, if appropriate, demonstrated by the PPU application, and close each with elements of a research roadmap. In Section 10, the contribution closes with a short conclusion and an outlook regarding open challenges and future work.aPS are typically designed-to-order, i.e. they are unique systems, which are designed and implemented once a customer has awarded a contract to an aPS supplier (Birkhofer et al., 2010). Referring to systems' complexity as well as reuse of subcomponents there is a difference between machine and plant manufacturing industry. In this paper, we focus on special purpose machinery as aPs that is in between of machine manufacturing and plant manufacturing industry regarding complexity. The specification and implementation is carried out in the form of a project (VDI/VDE 3695 Part 2, 2010). In the lower part of Fig. 1, the project-related activities are depicted from left to right, following the process of the project. Until completion, such projects usually last between several weeks and several months.In order to shorten project durations and to reduce costs, reusable (partial) solutions are usually developed by system suppliers, for aPs in this paper by special purpose machine manufacturers. The development of these (partial) solutions follows a typical product development workflow (cp. upper part of Fig. 1), as described e.g. in VDI/VDE 2206 (2004), and is decoupled from the projects on the timeline. The so-created reusable solutions are stored in a repository and are taken into account to, if suitable, be used in the customer-specific projects, in particular in the phases of system design and detail design (cp. dashed arrows in Fig. 1) (VDI/VDE 3695 Part 2, 2010).aPS are supposed to be in operation for decades before finally they are taken out of operation and are demolished. During operation, they are aging. Reasons for aging are physical effects like wear and tear and corrosion resulting in limited life expectations of mechanical components. These components have to be replaced after a couple of years (cp. the corresponding arrow in Fig. 1), known as re-engineering and modernization. Besides, mechanical components are usually not replaced by identical ones, because identical parts are no longer available, and/or the customers prefer more modern solutions, e.g., with higher energy efficiency. The same holds for electronics/electrical components, including automation hardware including communication components, just with a shorter lifetime horizon (cp. Fig. 1).Other reasons for aging are changing requirements, e.g. market requirements or legal requirements. Many of those changes can be provided by adaptations of the software. Such adaptations are comparably more frequent and may be conducted even during runtime, see the arrow at the bottom of Fig. 1. These arrows point back to the phases of requirement and system specification: in a systematic approach, when change is required, new or changed requirements are gathered, and/or the existing requirements which are affected by this change are checked whether they are still valid. On this basis, the system design is adapted. The adaptation of an aPS due to the need for change is called “evolution” henceforth. Evolution is characterized by the gradual adaption of the aPS to preserve its value: for abandoning, the aPS is too valuable, and a revolutionary modification would require a longer standstill of the aPS, which is not desirable for economic reasons and may be only acceptable during annual closing.At first, different reasons for evolution are introduced as well as the different sequences to cope with the need for evolution resulting in different categories of evolution scenarios. These scenario categories are linked to the scenarios of the application example PPU, which will be explained in detail in the following section.As described in Section 1, evolution can be initiated by different reasons for change, and evolution can affect the software, the mechanical parts, and/or the electrical and electronic parts of the aPS (automation hardware). In the following, a categorization is introduced, which allows for distinguishing between the different causal orders by which evolution affects the different aspects of an aPS. This is important, as the different categories impose different methods to support evolution. The categorization is based on and extends the one introduced in Ladiges et al. (2013), Vogel-Heuser et al. (2014c) and (2014d).An aPS is commonly undergoing various types of changes in order to fulfill both the changed requirements and those of the previous requirements which are still valid. The functional requirements (FR) and the non-functional requirements (NFR) on the production and on the aPS are pre-set by the owner or the management of the facility and its immediate interests. They are usually described in a textual, informal manner. Along the lifetime of an aPS, several reasons prompt the management to change requirements. Such reasons are, e.g., modified or added types of products, additional boundary conditions of production such as laws or environmental regulations, or a new target for production performance. Since requirements describe the desired behavior of the whole aPS, they can affect the physical machine and/or the information processing and control algorithm in the aPS. Consequently, the customers' maintenance staff might, during the evolution, adapt the control software and/or mechanical parts and/or electrical and electronic parts of the system. Ideally, the workflow is as follows:(1)The management changes plant requirements.The semi-formal system requirements specification (SRS) is adapted accordingly.The software of the plant is redesigned by the customers' maintenance staff or by the machine supplier and then implemented accordingly.The changes are validated/verified.The changed aPS is operational again.This evolution scenario category is referred to as “Ia” henceforth, its main characteristics are shown in column “Ia” of Table 1. With respect to the letters in the first column of Table 1, the order of changes is (a)→(b)→(c) in category “Ia”. Similar evolution scenario categories are those where electrical parts (Ib) and/or mechanical parts (Ic) of the plant are changed after specification of requirements, which results in the sequence (a)→(b)→(d) in “Ib”, (a)→(b)→(e) in “Ic” and (a)→(b)→(d and e simultaneously) in “Id”. A simple example of an evolution scenario of category “Ic” is the planned enlargement of a plant with an additional mechanical device.In the second row of Table 1, references are given to evolution scenarios of the PPU case study (cp. Section 2.2) which are examples of this scenario category.The entire evolution scenario category “I” (i.e. “Ia”, “Ib”, “Ic” and “Id”) has in common that modifications are based on requirements specification, and that either software or physical parts of the aPS are changed. Evolution scenarios of category “I” require means (i.e. methods, models and tools) for formalizing informal requirements (both FR and NFR) into a usually semi-formal system requirements specification (SRS). Furthermore, they need methods to trace requirements from the initial specification via the SRS to the software. Section 3 deals with details regarding the formalization and the traceability of the requirements.Not all changes of requirements can be fulfilled by changes of only the software or only the physical parts alone. Quite often in an aPS, changes of the mechanical and/or the electrical/electronic parts are required, which lead to a subsequent adaptation of the software. The workflow of this category “II” is described in Table 1 as (a)→(b)→(d and/or e)→(c). Category “II” further details whether the electrical/electronic parts (“IIa”) or the mechanical parts (“IIb”) or both (“IIc”) are changed before the software is changed. This category of evolution scenarios requires, in addition to category “I”, an SRS which combines mechanical, electrical/electronic and software aspects, and allows for tracing requirements across the borders of these disciplines.However, a well-managed and documented engineering procedure is not always performed in practice when requirements change. When the plant's staff has the impression that changed requirements can be fulfilled by minor software changes (such as simple code adaptations of Programmable Logic Controllers (PLCs)), such changes are usually performed instantaneously to react quickly, or even executed during a plant's operation to avoid standstills of the production facility. Those frequent minor adjustments are often carried out without a proper requirements engineering process, thus missing consistent documentation. Often, the adaptation of the system requirement description (b) is omitted and a gap between the specification, the “as-built” documentation, and the real facility occurs. This evolution scenario category “III” is described by the sequence (a)→(c), see Table 1. As this engineering procedure is frequently found, methods should be developed to support regaining consistency and traceability between requirements, specification, and software code – see Section 9 for details.A new requirement of the facility manager (a) can also result primarily in a change of the plant's mechanical and/or electrical parts. For example, a new requirement might demand for a sensor with higher accuracy, and the software might need to be adapted in consequence. Evolution scenario category “IV” is therefore described by the development sequence (a)→(d and/or e)→(c). The similarity between category “IV” and “III” is that a formal SRS for the change is omitted. Similarly to category “II”, category “IV” is further detailed into subcategories that describe whether the electrical parts (“IVa”) or the mechanical parts (“IVb”) or both (“IVc”) are changed. This results not only in a discrepancy between the implemented software code and the specification, but also in discrepancies between the specification and engineering documents of electrical/electronic and/or mechanical engineering disciplines and their implementations.A further evolution scenario category “V” describes scenarios which do not result from new requirements (a), but frequently occur in industrial practice during operation and maintenance phase: the change is initiated at the shop floor by maintenance personnel and affects either only software as such, e.g., an upgrade of a software functionality without changes in hardware (“Va”), or electrical/electronic and/or mechanical parts without effects on software (categories “Vb”, “Vc”, “Vd”).Evolution scenario category “VI” is characterized by changes which, similarly to category “V”, do not result from new requirements (a), but instead from changes in the plant's physical parts (d and/or e), which affect software (c). For example, a deliberate change of mechanical or electrical parts can require a subsequent software adaptation (c). An example of this category is the exchange of a failed sensor by a newer version (just due to the fact that the original version is not available anymore) which performs slightly different and, therefore, requires a software adaptation accordingly. Also in category “VI”, a distinction has to be made whether the change originates in the electrical system (“VIa”) or the mechanical system (“VIb”) or both (“VIc”).As depicted in the lower rows of Table 1, the scenario categories can be related to the definitions introduced by Buckley et al. (2005): history of change, granularity of change (Buckley et al., 2005; Katzke et al., 2004), history of change and anticipation of change. Anticipated software changes in accordance to Buckley et al. (2005) are software changes, which “can be foreseen during the initial development of the systems and, as such, can be accommodated in the design decision taken”. Anticipated software changes also include changes during operation as long as a model-based approach is followed (cp. categories “I” and “II”). Such changes are called “offline” in aPS, because the changes are carried out in a model (maybe also PLC code), independent of the production process. Buckley calls those changes static. Unanticipated software changes according to Buckley et al. (2005) are not foreseen during the development phase, but they are frequently undertaken at short notice during commissioning and operation in order to, e.g., clear defects in other disciplines, such as an unexpected behavior of the mechanics. Such changes are called “online”, as they are implemented directly in the aPS during runtime, and are called dynamic referring to Buckley. Once such unanticipated changes are completed, documentation needs to be adapted accordingly. This is the case in categories “III”, “IV”, “V” and “VI”.Fig. 2 displays which of the project-related engineering phases (cf. Fig. 1) are executed in evolution scenarios of the different categories. In evolution scenarios of categories “I” and “II”, all engineering phases are executed according to the ideal workflow of Fig. 1. This is depicted in Fig. 2 by an arrow, which starts at “I”/”II” and is aligned with all engineering phases. In the scenarios of categories “III” and “IV”, the phases “system specification” and “system design” are omitted (cp. the arrow in Fig. 2 which starts at “III”/”IV” and bypasses these phases). In evolution scenarios of the categories “V” and “VI”, the requirements remain unchanged, i.e. the workflow starts at the detail design phase (cp. respective arrow in Fig. 2).A simple lab size model, the pick and place unit (PPU) is used as a demonstrator to research methods and technologies on evolving aPS. The PPU performs a manufacturing (discrete) process and handles, stamps and sorts different kinds of workpieces (Fig. 3) (Vogel-Heuser et al., 2014d). The PPU consists of software, electrical/electronic and mechanical parts.Sixteen scenarios were developed that cover different combinations of mechanics, automation hardware and software changes (cp. Tables 2and 3) (Legat et al., 2013; Vogel-Heuser et al., 2014c).The initial scenario is the evolution scenario Sc0 where the stack, the crane and a slide (cp. Fig. 3, left bottom) are existing. The stack pushes a single black plastic workpiece out of the stack into the crane's pick-up position. At the pick-up position, the crane picks up single workpieces by moving the crane down and by using a vacuum gripper to suck the separated workpiece. Upon rotation of 90°, the crane reaches the slide's position, where the workpiece has to be placed. After moving down, the vacuum gripper releases the workpiece, which then glides down the slide.Table 2 introduces possible evolutionary changes of the PPU. In Table 1, these changes are related to the evolution scenarios. For instance, in category “I” changes are made for increasing the capacity of the slide where workpieces of PPU are finally stored after production.Regarding Buckley et al.'s (2005) criteria temporal change, also history of change is introduced with the characteristic sequential and parallel synchronous and parallel asynchronous changes. With sequential software evolution, multiple persons cannot make changes to the same data at the same time, but with parallel evolution different software developers can simultaneously make changes to the same software component. The latter is frequently the case in aPs.The first evolutionary step, from Sc0 to Sc1 (cf. Table 1, category “Ic”), has impact only on mechanical parts of the PPU. Requirements are derived from customers' demand that the slides capacity has to be increased. Hence, a Y-shaped slide has to be used. No changes to software or electrical parts have to be made, because increasing capacity has to be realized only by evolving the mechanical shape of the slide. The second evolutionary step, from Sc1 to Sc2 (cf. Table 1, category “IIc”), is an anticipated change, where all mechatronic parts are affected by evolutionary changes forced by customers' demands on processing an additional metallic workpiece (step 1. in Table 1, category “IIc”). Hence, the requirements specification is changed (step 2.), inductive sensors are mechanically attached (step 3.) to the PPU, electrically wired to added PLC clamps (step 3.) and implemented software has to be changed to recognize and process the new signal (step 4.). Furthermore, in case of unanticipated changes, i.e., if the demand turns out after delivery of the plant and evolutionary changes have to be made immediately, even mechatronic parts are affected while neglecting requirements changes in informal and semi-formal stages (cf. Table 1, category “IVc”). Successively, evolutionary changes are made until Sc13 is completed. Hence, plants' manufacturers have a vast quantity of variants to handle.In the following, parallel evolution is discussed according to Buckley et al.'s (2005) criterion history of change. A set of typical variations in aPS based on the PPU's scenario Sc12 are given in Table 3. Due to the demand for higher throughput of workpieces (WPs), scenario Sc12a with a faster sorting of WPs is developed. A drive with increased dynamics is installed to realize faster WP movement, which entails that faster pushers are required for extruding WPs. In parallel, a customer demands as a non-functional requirement an adjusted variant of PPU's scenario Sc12 (scenario Sc12b) which is able to handle larger and heavier WPs. Depending on the country a machine or plant shall be located in, different supply and control voltage must be supported by field devices. Whereas the existing PPU is engineered to be located in Germany, a customer requests a PPU which can be operated with different supply and control voltage (as used, e.g., in the United States). Accordingly, all field bus components, which are not capable to handle the desired control voltage, have to be changed (scenario Sc12c). In aPS, different customers require specific vendor-platforms, resulting in parallel software variants for the different platforms that support different programming language dialects and use different operating systems, which leads to variants of programs even if the functionality is the same (scenario Sc12d). The implementation and runtime environment may be changed, too: the PLC software may be implemented according to IEC 61131-3 or IEC 61499 (scenario Sc12e). In parallel to these variants, scenario Sc12f is more reliable due to the realization of self-healing functionality. To extend the business cases of aPS suppliers, a variant supporting remote service is provided with scenario Sc12g. To realize remote service functionality, data logging and analysis techniques are required as well as the possibility to remotely access operational data (Vogel-Heuser et al., 2014c).Requirements engineering in general includes the activities of identifying, documenting, verifying and validating, coordinating, and managing requirements (Pohl & Rupp, 2011). Requirements play a crucial role in the engineering of production plants, as they describe the stakeholder's needs and therefore the intention of the plant as well as demanded properties to be competitive and economic. Accordingly, the development of requirements on a production plant is an integrated part of the production design (Blanchard, 2004; Buede, 2000). In the development process model described in (VDI/VDE 2206, 2004) (cp. Section 2) for the design of mechatronic systems, such as production plants, the phases of identification and documentation of requirements are explicitly foreseen at the beginning of the process. Similarly, other common development process models like the spiral model (Sage and Rouse, 2009) (for software-intensive systems) or the waterfall model include requirements elicitation and specification as a (repetitive) action during engineering. Requirements can be categorized as functional requirements, quality requirements, and external conditions (Pohl and Rupp, 2011).The functional requirements can usually be derived from the product that has to be produced (Vyatkin, 2013) and are the minimum set of requirements to be specified completely describing the design problem (Braha and Maimon, 1997). They refer to the main intention and are the “nonnegotiable characteristics of the desired solution” (Braha and Maimon, 1997). Therefore, the production development phase is often performed subsequently to the product development phase and is part of the product realization (Bellgran and Säfsten, 2010). Quality requirements, also called non-functional requirements or extra-functional requirements, are usually more general and include, among others, performance requirements expressed in Key Performance Indicators (KPIs) (see e.g. ISO 22400-2, 2012), flexibility requirements, reliability and availability requirements, safety and security requirements, and maintainability requirements (ISO/IEC 25010, 2011).A variety of norms, guidelines, recommendations and approaches is available for performing a structured and systematic requirements engineering. However, in industrial practice these approaches are not strictly followed. This was, for example, shown in a survey regarding requirements engineering practice for software projects in industry by Neill and Laplante (2003). Morris et al. (1998), identified the following reasons for missing requirements engineering practices in research and development projects in industry:•Missing training due to lack of time and cost.Inherent complexity like number of stakeholders, requirement conflicts, number of requirements, or new arising requirements in later phases.Problems with business integration.Low acceptance in management due to lack of familiarity with requirements engineering.Evolution requires repetitive requirements engineering over the lifetime of the aPS. As the evolutionary changes are in most evolution scenarios small compared to the original development of the aPS, often the qualified staff which was responsible for the requirements engineering of the original aPS is not involved in the evolution, which aggravates the problem of missing requirements engineering.As shown by Frey and Litz (2000) in the example of the specification of PLC code, the formalization of requirements is often omitted in practice. This results in a direct implementation of the informal requirements. Different expert surveys with manufacturing companies showed that structured and systematic ways of working in aPS development are missing (Bellgran and Säfsten, 2010). Additionally, a study by Bellgran and Säfsten (2010) showed that only a few companies formulated a thorough requirements specification before system development started. Choices are often made as a result of project team discussions and not by comparing design possibilities with requirements specifications to find the best solution. Decisions are made based on employer experience as well as reusable existing solutions and done intuitively. Reasons named by Bellgran and Säfsten (2010) are mainly high time-pressure and low priority despite of the assumption that a well performed requirements engineering helps reducing time for fixing design and implementation errors. The evolutionary changes usually have to be carried out under even tighter schedules and higher time pressure, because production standstill must be minimized, thus resulting in even worse boundary conditions for a systematic requirements engineering.aPS need to be adapted, expanded or somehow changed in order to continue to meet their respective actual requirements. Unfortunately, the adaptation of the formal specification is often omitted, especially when changes need to be implemented within a short time window and during operation (Vogel-Heuser et al., 2014a). Adequate support for requirements engineering of aPS, which is on the one hand manageable regarding effort needed and benefit gained and, on the other hand, which copes with variability and version management also of sub-systems, is still missing. Besides, the maintenance of requirements seems to be an issue also from the viewpoint of roundtrip or reverse engineering (from code/mechanical feature/automation hardware feature to model to requirement or directly from code/mechanical feature/automation hardware feature to requirement).Non-functional requirements impose severe challenges during the engineering of aPS: whereas functional requirements (FRs) can usually be broken down to basic functions, and each of them can be fulfilled by a dedicated module of an aPS, the non-functional requirements (NFRs) are usually fulfilled (or violated) by the emerging behavior of the aPS. The assignments of NFRs to components of the aPS during requirements and system specification are therefore a critical task, if possible at all. By recording NFRs early, later expensive conceptual changes can be avoided. During the design process, these NFRs have to be met by the developer. Different NFRs have to be considered in different life-cycle phases. In the following the most important NFRs for distributed control systems, thus also for complex aPS, are dealt with, cf. Fig. 4(Frank et al., 2011). These should be considered in any case during requirements engineering and later in the design process. All other requirements mentioned in ISO/IEC 25010 (2011) do not have intensified influence on aPS but have to be considered as well. The main requirements to be considered are reliability, performance efficiency, compatibility, maintainability and portability (Frank et al., 2011). Table 4gives explanations of the reason and the importance of the NFRs to be considered during the development process. The most important ones are resource utilization, time behavior, testability and analyzability (Frank et al., 2011). The requirement testability has to be specified for every other functional and non-functional requirement. Not verifiable requirements have to be decomposed until a test case can be specified.The NFRs “compatibility”, “modifiability” and “portability” are of utmost relevance for the ability of an aPS to undergo an evolution: if they are taken into account seriously during the initial design of an aPS, this system will be well-prepared for evolution, and this will ease and reduce efforts during the evolution steps to come.At each evolution step, it has to be checked and confirmed that, despite all changes, the NFRs “reliability” and “performance” are still fulfilled. For example, it has to be proven that the system design of an aPS has at least the same level of fault tolerance after the evolution step, and that e.g. the worst-case transmission time of a of a message from a sensor is still within the predefined boundary.To be able to operationalize the requirements given in Fig. 4, they have to be annotated with attributes.In the following, the state of the art regarding requirements engineering in the context of aPS is described. At the beginning, the elicitation and the documentation, the refinement and the formalization of requirements are dealt with in general. Subsequently, we describe previous work concerning requirements engineering in aPS to tackle the challenges described above.General sources of requirements are usually stakeholders, documents, and systems in operation (Pohl and Rupp, 2011). Documents are usually law texts, norms and standards, internal quality guidelines and others. Systems in operations with similar purpose may give information about good or bad practices and can therefore be used as a source of requirements. Stakeholders, especially the customers, are the main source of requirements for an aPS. Their requirements have to be communicated in a way allowing systematically documenting and analyzing the requirements (Pohl and Rupp, 2011). There are a lot of techniques proposed in order to identify requirements. Common techniques are conducting surveys and interviews, performing brainstorming, and using checklists (Blanchard, 2004). Further methods, like e.g. the quality function deployment (Chan and Wu, 2002), are proposed by research. However, which practices are really performed is project and company specific (Bellgran and Säfsten, 2010).Following the “IEEE Guide for Developing System Requirements Specification”, first raw requirements result, among others, in documents describing the concept of operation as well as the system concept in an informal and rough manner which are to be developed with the customer (IEEE-Standard 1233, 1998). These documents should already describe the requirements (1) unambiguously and consistently, (2) in a clear structure, (3) modifiable and extendable, (4) completely, and (5) traceably (Pohl and Rupp, 2011). This kind of description is also called “well defined requirements”, and “ill-defined requirements” may be transformed in a refinement procedure (Braha and Maimon, 1997). Different literature proposes different document types for the requirements documentation, as for exampleBlanchard (2004), Buede (2000), IEEE-Standard 1220 (1999), IEEE-Standard 1233 (1998) and Pohl and Rupp (2011).Usually a requirement refinement has to take place in order to concretize the requirements resulting in an objective hierarchy. Haubeck et al. (2013), for example, derive four stages in the requirement hierarchy of a single system. Namely those are:•Originating requirements: general requirements focusing on the boundary of the system and as design independent as possible.System requirements: derived from the originating requirements translated into engineering terminology (formalization).Component requirements: system requirements broken down to subsystems or subcomponents.Configuration item requirements: refined component requirements.Similar hierarchies can be found in the other literatures, likeBlanchard (2004) and IEEE-Standard 1220 (1999). However, the hierarchy presented by Haubeck also refers to the process of formalizing requirements. Originating requirements are usually documented in an informal manner, like text documents and tables formulated in natural language. Translating the originating requirements into a formal description results in the system specification which is a first description of how the requirements are to be realized (VDI/VDE 3694, 2008). Formalization may be a first refinement of the informal requirements (VDI/VDE 3694, 2008). However, informal requirements sometimes need to be defined on several or each hierarchical level (IEEE-Standard 1233, 1998).Formalization usually results in a set of models, and the variety of used models is huge, and the choice of the right model type depends on the domain, industry branch, as well as the specific plant to be designed (VDI/VDE 3681, 2005). The right model choice also depends on the degree of granularity and the current refinement stage.For many aPS in operation, a formal specification has never been provided (Frey and Litz, 2000). This often includes that no assessment of the quality of changes is done because of lack of time, or cost constraints, or missing measurements (Bellgran and Säfsten, 2010). Consequently, a lack of explicit knowledge about the process behavior and its actual occurring properties unfolds.An approach to narrow the knowledge gap, or even to close it, is by establishing an automatic linkage between evolving plant behavior occurring due to performed changes and formally specified requirements in order to keep specifications up to date and consistent (Haubeck et al., 2013). To establish this linkage, specifications should consist of interpretable models representing different aspects of the plant. These models are directly connected to the actual system behavior because they only contain information available during plant operation. In this way, the lack of explicit and up to date specification can be alleviated because the models are automatically generated and adapted by observing the in- and output data of the automation system. This approach cannot provide a complete specification of each possible action of the production machine, but can satisfy the need of an evolution support with a minimal human effort in order to tackle the practical problems of tight time and cost restrictions during evolution. This helps keeping specifications up to date even after undocumented evolution steps (Haubeck et al., 2013).Additionally, the adapted models are specified in such a way that they can be analyzed in order to derive current plant properties. Here, especially non-functional properties like “performance” and “flexibility” (which is part of “modularity”) are of interest. These properties can be operationalized in a way that they are measurable by the available online measurements (Ladiges et al., 2013). Subsequently, the properties can be checked against specified values for requirement verification. In accordance, the needed assessment of performed changes can then be done automatically while still providing information on the abstraction level of operationalized requirements. In summary, the approach described in Haubeck et al. (2013) enables a requirement verification during operation by (1) building models out of online measurements, (2) derive plant properties out of these models, and (3) check those properties against specified limits.Feldmann et al. (2014b) presented a light weight notational approach (cp. 3.3) to model requirements for testing and evaluated it with experts from industry showing that the modeling approach provides the means to systematically, comprehensively and efficiently specify mechatronic sub-systems as sensors and actuators. They introduced an integrated modeling notation of requirements and customer functions based on function and requirement templates. One important goal of the template specification was the early consideration concerning the testability of requirements; therefore the valid parameter range is explicitly included as well as the classification in functional or non-functional requirements, the priority and a description field. This approach is presented for the PPU scenario Sc12f in the following to demonstrate different NFRs, i.e. fault tolerance (reliability), analyzability (maintainability) and time behavior.Referring to the self-healing scenario Sc12f of the PPU in Section 2.2 and the above mentioned challenge to include NFR into requirements modeling of aPS, the light weighted approach of Feldmann et al. (2014b) is introduced. The realization of self-healing functionality presumes analyzability of failures, i.e. failures have to be observable by the software (i.e., adequate sensors are installed to identify a failure), and automatic compensation mechanisms (analogously to maintenance instructions) have to be implemented (fault tolerance → reliability).As a system may have a sequence of states, it is also possible for requirements to relate to a reaction of an output resulting from another requirement. An example is the requirement “Fault management” (cp. Fig. 5), which requires a reaction on the occurrence of alarm 104. Indicated by three levels in Fig. 5, requirements are used to refine functionality, e.g., the function “Maintenance instructions” (Fig. 5, right level 3) is derived based on the “Fault management” requirement on the level above. The function “Maintenance instructions” comprises the parameter “WP_jam”; the corresponding requirement on the same level defines the expected variation, i.e., possible values of this parameter.To increase the quality of requirements Rösch et al. (2015) and Teufl and Hackenberg (2015) introduced description of behavior based on MSCs included in the MIRA (Broy and Stølen, 2001) approach and UML SD. Both approaches have been evaluated with the crane being a part of the PPU. With MSC verification of the behavior is target at and with the UML SD the generation of test cases.An adequate light weight and efficient way to model requirements for aPS and refine or change them during the design process needs to be developed including both FR and NFR. NFRs should be operationalized in a way that they are quantifiable but still technology-independent. Thus, their continuous fulfillment can be verified before and after an evolution step, despite of changes of the implementation technology.An adequate support for aPS is still missing, which is on the one hand manageable regarding effort needed and benefit gained and which on the other hand copes with variability and version management also of sub-systems. Besides, the maintenance of requirements seems to be an issue also from the viewpoint of roundtrip or reverse engineering (from code/mechanical feature/automation hardware feature to model to requirement or directly from code/mechanical feature/automation hardware feature to requirement), tracking of requirements to design and maintain along life-cycle and vice versa and cross variants and versions is mandatory.In the discussion of the system design phase, we cover both the overall system design as well as the detailed design of different parts of the system. This includes both the individual system parts and also the different involved disciplines. The section will at first discuss the challenges separated into consistency in structural design and between other systems design aspects as well as architectural technical debt followed by the state of the art with the same sub-structure. The first two challenges are explained using the PPU case study. The chapter is concluded with the roadmap.aPS consist of mechanics, automation hardware incl. electrics/electronics, and software. They are built by mechanical engineers, electrical engineers and software engineers. For specific tasks, also more than one electrical engineering discipline, e.g. drives, closed loop control and human machine interfaces, will be included. A result of that is that the different parts of the system design are built iteratively and according to methods and tools of the different disciplines. Consequently, evolution challenges in the system design do not only cover the evolution of each discipline's design, but also the evolution between the disciplines and are conducted in iterations between the disciplines.An important part of the system design is the definition of components and interfaces, which in software engineering is called the architectural configuration (Barais et al., 2008; Medvidovic and Taylor, 2000). During evolution, the engineers working on the different parts of the architecture will evolve their respective system parts. Evolving the design of an industrial system is already difficult if only one discipline is concerned as the discipline's system design can reach high levels of complexity.On the software side, a typical problem is the consistency between interfaces of components both on the syntactical as well as semantic levels. While syntactic consistency of interfaces is largely solved, ensuring the semantic consistency of interfaces is a problem, i.e., even though interfaces match on the syntactic level of messages or methods, the behavior of a component may change in such a way that another component does not work anymore. Problems here could be a change in the order of sending messages in specific situations or a change in how subcomponents are connected and interact resulting in changes in the real-time behavior. In mechanical systems, interfaces can be distinguished as material, information, energy flows (Pahl and Beitz, 1997). Here, interface mismatches could be results of changes in material flow characteristics like type of material or viscosity of fluids.While engineering approaches exist in the different disciplines to address these problems (e.g., architectural description languages in software engineering Medvidovic and Taylor (2000), the problems become more difficult to identify and solve if a change in one discipline's parts results in problems in another discipline. For example, the change of a feedback controller in the control model could result in characteristics of system inputs (e.g., limits on the value or change rates), that the employed mechanical actuactor might not be able to satisfy. A similar problem arises if a sensor is replaced in the mechanical system that has different quality characteristics, which affect the quality of a feedback controller of the software.The completesystem design covers different aspects (Goldschmidt et al., 2012). Examples of these aspects are requirements, structure, and behavior that are relevant for all disciplines or discipline-specific aspects like spatial information about the mechanical system. However, the different aspects typically contain overlapping information whose consistency needs to be ensured. In software systems, the structural aspect can be a specification of the architecture, which contains components and connectors including definition of messages, which are exchanged between the components over the connectors. The behavioral aspect can be a set of state machines, which define the behavior of the components and specify the exchange of the messages. Considering these two aspects, the message definition is an overlapping information as it is specified both in the structural aspect and in the behavioral aspect. Inconsistencies arise if during the evolution now one of the aspects is changed but not the other.Similar issues arise between the different disciplines. An example is the specification of deployment information in a software architecture model that defines which automation hardware certain software components will be executed. The electrical part of the system architecture has to cover also the information about execution hardware. Furthermore, the electrical architecture needs to contain the communication buses, which the software components use for their communication. Similarly, the mechanical system needs as well to consider the execution hardware and its cabling in the 3-dimensional physical specification of the system.In the following, we give a concrete example of overlapping information between the designs of the different disciplines. Fig. 6 shows an excerpt of the state chart of the PPU's scenario Sc12 (manual mode, left) and the self-healing mode in scenario Sc12f (right) which defines three modes of the PLC software with respect to behavior in case of a failure. In both scenarios, the expected time for extracting the pusher is monitored (“TimerPusher” in Fig. 6, Timer “p1” in Fig. 7). As it can be seen in the figure, the state machine explicitly refers to sensors. Thus, a change in the mechanics and/or automation hardware by replacing or removing the sensors will lead to problems in the state machine of the software. The additionally installed sensors in scenario Sc12f compared to scenario Sc12 allow for detecting a WP jam. The analog transducer measuring the position of the pusher (“Sens_Transducer” in Fig. 6, “p3” in Fig. 7) indicates that the pusher blocks at a certain position. The sensor measuring the pneumatic pressure is used to exclude a failure at the pneumatic system leading to a not extracted pusher, too (“Sens_Pressure” in Fig. 6, “p4” in Fig. 7). The sensor at the infeed of the slide (“Sens_Slide” in Fig. 6, “p2” in Fig. 7) detects that the expected workpiece did not reach the slide.Similarly, a change in the characteristics of the execution time of the pusher will lead to different behavior of the state machines since the state machine refers to the execution time of the pusher as guards in the transitions, e.g., in the outgoing transitions of the state “Eject_WP” at the bottom of the state machine.A special case of the previous challenge is the impact of architecture and architectural evolutions on non-functional (quality) properties of the system (cf. Section 3). Particular examples are the relations between the system architecture and probabilistic non-functional properties like safety (cf. Grunske and Han, 2008), availability, and reliability. That means, that a change of the architecture to incorporate new functionality or an architectural refactoring, which does not influence the functionality, can at the same time also change the system's non-functional properties. Quality models like fault trees, queuing networks and Markov chains are used to estimate the non-functional properties for systems. A consistency between the quality models and the system and its architecture is therefore crucial. That means that after an evolution the quality model estimation results should still be consistent with the actual system's behavior by propagating the changes (e.g., new components, changed connectors) of the system to the quality model. Additionally, the quality model needs to be updated at run time with respect to parameters (e.g., failure rates, speed of physical items) of the running system.As an example, Fig. 7 shows a fault tree of one of the PPU's evolution scenarios (Vogel-Heuser et al., 2014c). The fault tree refers to elements of the system architecture, for example the mechanics part of the fault tree refers to a compressor and in the automation hardware part events refer to I/O module, broken wires, and sensor failures. Consequently, the consistency between the fault tree and the system architecture needs to be ensured so the fault tree does not refer to non-existing architectural elements, and all relevant failures of the system architecture elements are covered in the fault tree. Getir et al. (2013) show, also on the PPU case study, that the evolution between system architecture and fault frees cannot be automated and that instead expertise from engineers is required to correctly evolve both the system architecture and the system's fault trees to ensure consistency.Even if the consistency problem is addressed, the actual solving of the quality models is time-consuming. Here, incremental approaches (cp. Section 6) are required, which reuse results of unchanged parts of a quality evaluation model to improve the performance.Recently, the term “technical debt” has been coined (Kruchten et al., 2012) for the effects when sub-optimal solutions are chosen to meet short-term goals similar to financial debt. In the long run, interest has to be paid due to the effects of the chosen sub-optimal solution that makes future development more difficult. In the context of (software) architecture, this is known as architectural technical debt. In a multiple-case embedded system case study in 5 large software companies (Martini et al., 2014), different factors for architectural technical debt have been identified, like pressure to deliver, prioritization of features over products, non-complete refactoring, or technology evolution. An identified example of architectural technical debt was a situation where a company used three different communication networks in a system, decided to refactor the complete system towards using a unified communication networks to make evolution easier. However, during the refactoring, difficulties and time pressure lead to only a partially refactored system with the result that now the system has four different communication networks which makes it even more costly to evolve. The concept of architectural technical debt is unfamiliar for aPS so far. Especially in plant manufacturing industry, the architectural technical debt may be high because of unexpected environmental conditions in the country to be delivered, which leads to the necessity of on-site adaptation during commissioning and start-up even. In plant manufacturing industry, machines size parts may be assembled in the facilities of the supplier, but most of the equipment will be installed on-site for the first time, which leads to decisions under time pressure neglecting architectural concepts.In the following, we review current approaches, which address the mentioned challenges directly or provide technology which can be used to address the mentioned challenges.Ensuring the consistency of the structural system design, particularly between the designs of the different disciplines, can be achieved by using specification languages that specifically address the design related to the multiple different disciplines. CONSENS (Anacker et al., 2011) – CONceptual design Specification technique for ENgineering of complex Systems – is a method and specification language that targets the overall, discipline independent system design. CONSENS supports the specification of different system aspects. Examples are the structural specification in the form of the active structure that covers the overall system structure of all disciplines on a high level, behavioral specification in form of state machines, the functional hierarchy that covers the functional requirements of the system, and CAD-models for the 3D-design. The main idea of that approach is to cover all relevant aspects in a single high-level specification abstracting from the details of the different disciplines. This high-level specification is then detailed by each discipline. Additionally, the high-level specification enables the engineers of the different disciplines to identify changes during evolution that may affect other disciplines as well. Based on the identification of these changes, Rieke presents an approach to synchronize changes between the high-level specification of CONSENS and the discipline-specific specifications to ensure consistency between them using Triple Graph Grammars (Rieke, 2015).The MechatronicUML (Heinzemann et al., 2013) is a modeling language and process for the engineering of mechatronic systems. It is based on a rigorous specification of structure and behavior based on a refinement of the Unified Modeling Language (UML) (Object Management Group, 2011). With respect to consistency in evolution, it specifically supports the software engineering and control engineering disciplines and ensures the consistency of their structural designs, i.e., the software architecture – components, ports and connectors – and block diagrams for the specification of feedback controllers. Furthermore, the MechatronicUML also supports the specification of architectural reconfiguration, e.g., creating, deleting component instances, ports and connectors, blocks and inputs/outputs. Again, consistency is ensured by checking that both the software architecture as well as the block diagrams reconfigure consistently with respect to components, blocks, ports, and inputs/outputs.Another approach for ensuring the consistency in the design of embedded automation systems is the Systems Modeling Language (SysML) (Object Management Group, 2012; Weilkiens, 2008), which is a profile for the UML. The SysML solves the consistency problem by providing a single modeling language which covers not only the software engineering aspects as the UML, but can also be used to specify other system design aspects like block diagrams. SysML4Mechatronics (Kernschmidt and Vogel-Heuser, 2013) is a language for interdisciplinary modeling, which addresses mechanical, electrical/electronic and software aspects in product automation and for aPS explicitly. A formal semantics for automatic verification of structural compatibility has been proposed (Feldmann et al., 2014a), but verifying functional conformance is not considered yet. Shah et al. (2010) present a multi-discipline modeling framework based on SysML. The approach prescribes structural, behavioral and requirements aspects each composed into a common model. Transformations are used to map between different discipline-specific SysML profiles, but verifying model correctness is not addressed.A formal modeling framework for verifying aPS' engineering models has been proposed in (Hackenberg et al., 2014). The formal models contain the necessary aspects for verifying the system's correctness after evolution changes. The approach is based upon three basic viewpoints: the requirements viewpoint enables to concretize from abstract aims of the aPS via design decisions to specific requirements imposed on the aPS. The process viewpoint targets the specification of the technical process to be implemented by the aPS independently from a specific solution. Therein, aspects from multiple disciplines are included describing how a specific product is manufactured. The system viewpoint models the actual implementation of the aPS containing the necessary aspects of the involved engineering disciplines (software engineering, electrical engineering, and mechanical engineering). For modeling these viewpoints, the modeling framework relies on the Focus theory (Broy and Stølen, 2001) which provides strict formal semantics.Addressing consistency basically requires two activities. First, inconsistencies need to be identified. Secondly, the inconsistencies need to be repaired. One major language for the definition of consistency constraints is the Object Constraint Language OCL (Object Management Group, 2014), which allows for the specification of constraints based on first-order predicate logic. The constraints are specified on the meta model of the modeling language and executed on the abstract syntax of the model, i.e., they concern only the static and not the dynamic semantics. A rule based approach to identify structural inconsistencies during evolution of aPS is presented inStrube et al. (2011).Dynamic semantics is concerned with behavior of the model. Here, approaches can be used, for example, to compare whether two finite automata are consistent by identifying whether one simulates the other or both simulate each other (bisimulation) (Clarke et al., 1999). Bisimulation intuitively means that one automaton shows the same behavior as the other one. More complex relations (called refinement notions) between automata exist, which can be used to define and check various degrees of consistency (Baier and Katoen, 2008; Heinzemann and Henkler, 2011; Jensen et al., 2000; Weise and Lenzkes, 1997).With respect to the repair of inconsistencies, approaches have been proposed to use OCL constraints and the part of the constraint, which has not been satisfied, to create an appropriate repair action (Nentwich et al., 2003). In the area of model transformations, Bidirectional Model Transformations like JTL (Cicchetti et al., 2010) and Triple Graph Grammars (TGG) (Hermann et al., 2014; Hildebrandt et al., 2013; Schürr, 1995) have been developed to transform between models. They enable the synchronization of different design aspects by re-creating one model from the other and, thus, ensuring consistency by construction. Transformation languages that support incremental change propagation (Giese and Wagner, 2009) are particularly interesting as they enable to only propagate single changes from the source, i.e., they change only those parts of the target model which are affected by the change in the source. This enables preserving manual changes in unrelated parts of the target. In contrast, non-incremental approaches, which fully recreate the target model, will each time overwrite manual changes also in those parts of the target which are not affected by the changes in the source.Several approaches specifically address the consistency of architectures and quality evaluation models, e.g., Markov chains, fault trees and Queuing Networks. These typically enrich the architecture model with annotations about, for example, failures, failure rates, and abstract behavior. Most approaches addressing safety employ this kind of additional annotations (Giese and Tichy, 2006; Grunske et al., 2005; Papadopoulos et al., 2001). Tribastone and Gilmore (2008) and Becker et al. (2009) propose similar approaches addressing performance.Those approaches are restricted to be used during the design of the system as they only use assumptions of the behavior of the system with respect to its characteristics affecting probabilistic quality attributes, e.g., failure rates, performance of certain parts. Consequently, the analysis results could be significantly different from the actual behavior of the system. The approaches of Filieri et al. (2012), Goševa-Popstojanova and Trivedi (2001), Zheng et al. (2008), and Filieri et al. (2015) address this problem by inferring the characteristics at runtime from the running system in order to increase the accuracy of the quality predictions. For example, Filieri et al. (2015) combine a Kalman Filter with self-adaptive behavior in order to both reduce noise and increase the speed of responding to changes in the behavior of the system.With respect to incremental analysis approaches for non-functional properties, Kwiatkowska et al. (2011) present an incremental technique for quantitative verification of Markov decision processes, which is able to reuse results from previous verification runs and exploiting a decomposition of the model. Similarly, Song et al. (2013) propose a compositional approach for the probability reachability analysis of Discrete Time Markov Chains that decomposes the system into strongly connected components or even parts of them, analyze them using Gauss–Jordan Elimination (Althoen and McLaughlin, 1987), and afterwards use value iteration to compute the result for the complete model based on the individually analyzed parts. The performance improvement by their approach, however, depends highly on the employed decomposition algorithms. Both approaches further enable parallelizing the analysis due to the decomposition.One specific type of architectural technical debt (Martini et al., 2014) is non-compliance between architectural guidelines and the system architecture. Architectural guidelines, patterns and styles have been presented in Buschmann et al. (1996). A specific architectural pattern for embedded systems is the Operator Controller Module (OCM) (Burmester et al., 2008), which defines concrete layers and interfaces between them for different parts of the embedded software – feedback controllers, hard real-time communication and reconfiguration of the feedback controllers, and a soft real-time layer.Herold et al. (2013) present an approach to automatically check the conformance of the architecture against guidelines and architectural styles. Their approach formalizes architectural compliance rules using first-order logic and provides a checking algorithm. A complementary approach has been proposed in Herold and Mair (2014) which uses a meta-heuristic to efficiently search for violations of architectural compliance rules and to propose a sequence of repair actions to remove the identified violations automatically.To cope with cross-discipline evolution the prediction of evolution steps and synchronization milestones for evolution to ensuring consistency should be in focus of research. Particularly, the different modeling approaches for aPS need to consider the use case of modeling a big system with many different developers in distributed locations and companies (producer and suppliers) and with a different education (developers vs. on-site technician). While technical solutions are available to work on one or multiple models concurrently and to ensure consistency, unanswered questions are, how much consistency is necessary and when it is necessary to be consistent during the development of a big aPS.First steps towards addressing technical debt generally in aPS have been recently proposed by Vogel-Heuser et al. (2015). Particularly related to decreasing architectural technical debt, the impact of cross discipline decisions should be predicted and visualized referring to the functional and non-functional requirements influenced. The non-compliance between architectural guidelines and system architecture should be evaluated qualitatively and quantitatively for controlling purposes during and after the individual project. Additionally, a synchronization mechanism for reused components of the system including their versions across-projects is needed.Quality evolution models are used to validate whether the system satisfies quality requirements, like performance, throughput, safety, availability, and reliability. However, if a system is large, the time to analyze the quality evaluation models increases. Here, there is still the need to develop incremental and compositional approaches for the analysis of quality evaluation models. Incremental means that the change in the quality evaluation models due to an evaluation are exploited to only re-analyze the changed parts and other relevant parts, but not parts that do not need to be re-analyzed.Compositional means that the quality evaluation models are decomposed and individually analyzed. The individual results then need to be appropriately integrated. Both techniques allow analyzing large systems during evolution. While some approaches have been developed, the application to aPS is missing. However, aPS provide a good application area as they use reusable components to build large aPS and thus their design is a good match for compositional analysis. An important question however is whether the composition imposed by the architecture of aPS is both suitable for designing the system during development and at the same time also suitable for a good decomposition for a compositional analysis. Or in other words, it could be the case that the composition by the architecture yields a bad (in terms of performance) decomposition for the analysis, particularly, the incremental analysis. In that case, not only a different decomposition for analysis than for the system architecture but also different decompositions for different types of quality requirements might be required.Furthermore, aPS are developed by engineers from different disciplines and changes happen both during design as well as during operation. As quality requirements are typically cross-cutting and are affected by changes from all disciplines, it remains to be studied, how the multi-disciplinary character of aPS effects decompositions, as well as how technicians working on the plant's site can use these analysis tools to re-asses quality requirements after a change on site during maintenance (or during operation, in case of aPS systems which cannot easily be switched off).Along with an aPS's overall complexity, the automation software's complexity is growing as well. Since the proportion of system functionality that is realized by software is increasing (Thramboulidis, 2010), concepts for supporting automation engineers in handling this complexity are required. According to ARC (2011), in most manufacturing systems the use of IEC 61131-3 (IEC, 2009, 2013) compliant runtime environments currently is and will be the state of industrial practice in the next 5–10 years. Therefore, by focusing on IEC 61131-3 compliant code, a wide range of applications can be addressed. Consequently, the focus in this article is on aPS controlled by PLCs that use an IEC 61131-3 compliant runtime environment.PLCs are characterized by their general cyclic data processing behavior, which can be divided into the following steps: 1. read input values (I), 2. execute task(s) (X), 3. write output values (O), 4. wait (W). This whole sequence is called cycle (C) (cp. Fig. 8). PLCs execute cycles continuously. Assuming the number and bandwidths of input and output signals is constant for a running system, the durations needed for reading input (TI) and writing output values (TO) are also constant (but generally different).However, if the algorithms and the tasking situations are equal between two cycles Cjand Ck, the durations TX,jand TX,kare also equal. As real-time systems, PLCs ensure that the duration Tcyclebetween two cycles Cnand Cn+1is always constant and that within each cycle, all signals can be read (I), processed (X) and written (O).The IEC 61131-3 standard contains both textual, i.e. Structured Text (ST) and Instruction List (IL), as well as graphical programming languages, i.e., Function Block Diagram (FBD), Ladder Diagram (LD) and Sequential Function Chart (SFC). Furthermore, the standard defines three types of Program Organization Units (POUs) for structuring and reusing the PLC code. A program (PRG) represents the assembly of logical elements necessary for the machine or process control by a PLC. One PRG is the main program and thus provides the entry point into the plant code. Function blocks (FBs), which calculate output values based on input and persistent internal values, and Functions (FCs), which yield the calculation of a result value based solely upon input values, can be combined within a PRG. Furthermore, PRG and FB types are instantiated during design time and hold their data memories within PRG and FB instances during run time. Thus, data exchange is realized between POU instances, namely PRG and FB instances as well as FCs: a PRG instance can call FB instances and FCs, and a FB instance can call further FB instances and FCs, but FCs can only call other FCs as no data memory is stored within FCs.Despite efforts toward including object-oriented programming aspects within IEC 61131-3 (IEC, 2013), the standard in its current version has not yet been fully established in the industry. Thus, we focus on IEC 61131-3 without object-oriented extension (IEC, 2009), which is mostly used within state of the art industrial applications (Thramboulidis and Frey, 2011).Evolution of software functions in aPS requires the modification of the implementation of the functions. Modularity is still rarely fully applied in aPS software: “Reusable artefacts are mostly fine-grained and have to be modified on different positions, so that errors are probable and important reuse potential is wasted” (Maga et al., 2011). Reuse is mostly achieved through copy, paste and modification actions (Katzke et al., 2004). Feldmann et al. (2012) identified as reasons for this situation: the multitude of disciplines involved (such as software engineering, automation engineering, mechanical engineering, electrical engineering, safety engineering, perhaps also chemical engineering), and the interdependencies of software modules with mechanical and electrical modules (Jazdi et al., 2011). The evolution of aPS especially during operation as discussed in scenario category “V” (Table 1) is performed on code level by suppliers' start-up personnel or customers' operation staff with limited software education, but huge knowledge about the process. This may lead to inconsistencies between implementation and design artifacts as well as to unclear code structure (Katzke et al. 2004; Vogel-Heuser et al. 2014a).But also in traditional software engineering, the main technique to derive adapted or improved implementation versions of software functionality is clone and own (Ray and Kim, 2012). The main idea of clone and own is to copy existing code and modify the copy until the desired functionality is realized. Clone and own has high reuse potential without requiring much additional effort. Its main drawback is, however, that it leads to unclear code structures that may hinder maintenance and further evolution. For example, if a bug is discovered in one part of the software, it may also be contained in cloned parts. But as these are not documented, those cloned bugs are sometimes difficult, if not impossible, to discover. Additionally, changes in the code may further lead to a decay in the code structure impeding understandability and maintainability, as, for instance, modularity boundaries or architectural design decisions are broken by the changes. Furthermore, code changes may lead to inconsistencies between the implementation and other design artifacts. Software engineering for aPS is still struggling with modularity and the best level of software component size (Feldmann et al., 2012, 2015; Maga et al., 2011) and the interfaces between these components. Katzke et al. (2004) and Maga et al. (2011) found different component sizes in aPS software (called granularity) and described the challenge to choose the best size and interface in between components for reuse and evolution. As a disturbing factor, cross component functions such as fault handling and modes of operation (manual, automatic) occur. Because in the plant manufacturing industry software engineering has been mostly project driven for decades, the challenge is to restructure legacy code from different projects with similar or even equal functionality. To make things worse, the different platforms (Section 2.2) require software variants for the same functionality due to different IEC 61131-3 dialects.Code clones have been known for more than two decades as serious flaw that may impede evolution. For instance, Juergens et al. (2009) have shown that code clones are more prone to introduce errors. Moreover, Harder and Tiarks (2012) have shown, that error-correcting tasks tend to be incorrect in the presence of clones. However, in the recent past, code clones have been seen not only as number one smell of source code anomalies. Most importantly, Kapser et al. (2008) provide a methodology of how cloning is used as a reuse techniques such as, by using templating or forking to evolve software systems. Especially with current source code management systems, this may be a very reasonable way to make use of existing, reliable code. However, managing the evolution of cloned code itself may be challenging, especially the fact that developers must be aware of changes that possibly have to be propagated, which is usually a tedious and time-consuming task (Bettenburg et al., 2009; Ray and Kim, 2012). Unfortunately, neither clone detection nor code management systems, beside simple versioning, are available for aPS development platforms and IEC languages, yet. As mentioned above, the challenge in aPS is that “clones” are embedded in different projects and eventually on different platforms, i.e. PLC suppliers, which is certainly a necessary advancement in clone detection mechanism. Additionally, clones are not only software clones but of course also mechanic clones and electric/electronic clones embedded in different engineering tools.Beyond code clones, a broader notion of code smells has been introduced (Fowler, 1999) and extended by the notion of anti-patterns (Brown et al., 1998). Both terms describe the fact that design and implementation of a software system may exhibit certain anomalies due to non-controlled evolution. For instance, a diverging architecture, missing abstraction, and non-modular implementation are typical indicators of an ongoing decay of the software system. Particularly, Abbes et al. (2011) have shown that the presence of two anti-patterns impedes the performance of developers. Similarly, Sjoberg et al. (2013) conducted a large-scale study and concluded that code smells are good indicators for assessing maintainability on file level. While other studies show that perception of code smells may differ between developers (Yamashita et al., 2012), it is generally admitted that code smells hinder evolution because they impede extending and maintaining the underlying system.Besides the necessity to restructure legacy code, the best component size is one of the main challenges while evolving aPS. Another one is to evolve the software in a way such that a clear, maintainable and extendable code structure is kept. This is particularly important as aPS needs to satisfy real-time and safety-critical requirements. Furthermore, consistency and traceability to other design artifacts need to be maintained. Additionally, version control systems are required to keep track of the changes in the code in order to allow tracing changes over a number of software versions and variants.Design patterns such as those proposed by Gamma et al. (1994), are means in classical software engineering to support code modularity and evolution. Most design patterns deal with modularizing design decisions on code level so change can be applied locally in order to keep well-structured code. Amaptzoglou et al. (2011) reveal that using design patterns increases reusability, and thus, supports the evolution of software systems. However, other studies also indicate that design pattern may also impede evolution and maintenance and have to be applied properly (Khomh et al., 2008, 2009).Despite a multitude of work within high level programming languages, design patterns have been scarcely considered within the PLC programming domain. Efforts towards evaluating different methods of implementing logic control algorithms within IEC 61131-3 were conducted (Hajarnavis and Young, 2008), but specific patterns have not been derived yet. However, design patterns within control engineering would address a multitude of issues such as controller design, architectural design as well as implementation aspects (Sanz and Zalewski, 2003). Patterns have been especially investigated within the emerging model-based design of automation software, e.g., using UML (Fantuzzi et al., 2009). The authors introduced design patterns for solving typical problems such as hierarchical control, alarm handling and motion control using state charts as well as guidelines for implementing these patterns in IEC 61131-3 programming languages (Bonfè et al., 2013). In Preschern et al. (2012), patterns for improving system flexibility and maintainability are introduced. In Fay et al. (2015), an approach is presented which integrates the use of function-oriented design patterns into the engineering workflow of aPS to assist the designer regarding the fulfillment of NFRs. It could be shown that the application of these design patterns has a positive impact on the correct design of the software functions and on the appropriate deployment to automation hardware (Fay et al., 2015).Fuchs et al. (2014) conducted a detailed analysis of IEC 61131-3 code from machine manufacturing industry and introduced an analysis and visualization approach. Using this approach, dependencies and encapsulations of software units were analyzed, and first common software design patterns derived. Using industrial applications, the appearance of the machine code design patterns were evaluated and discussed regarding their benefits and programmer's intention with industrial experts. Feldmann et al. (2012) and Fuchs et al. (2012) analyzed and refactored the software structure on IEC code level in an industrial case study in a world market leading plant manufacturing company. Following the approach of large universal components including all relevant variants by switching functionality on and off leads to a code overhead of 80% useless code for many machine variants and the lack of comprehensibility as well as maintainability. The case studies' success resulted from restructured code consisting of smaller components by encapsulation of functions as well as separation of code functions, i.e. diagnosis, human machine interface etc.For IEC 61499-based applications (IEC, 2011), common solutions and guidelines were proposed for hierarchical automation solutions (Zoitl and Prähofer, 2013), failure management (Serna et al., 2010) and portable automation projects (Dubinin and Vyatkin, 2012). Even software design patterns for IEC 61499 programs, e.g., Distributed Application, Proxy and Model-View-Controller, were defined (Christensen, 2000) and evaluated (Strömman et al., 2005). However, although IEC 61499 runtimes on state of the art controllers exist (Vyatkin, 2011), “IEC 61499 has a long way in order to be seriously considered by the industry” (Thramboulidis, 2013).Besides the aforementioned shortcomings, best practices have been proposed that overcome code anomalies and, thus, improve and retain evolvability of software systems. The most popular means is refactoring, which provides systematic techniques for restructuring the internal system (i.e., the source code) while the external visible behavior is preserved (Fowler, 1999). As such, refactoring has been established and numerous studies have demonstrated its usefulness (Fanta and Raijlich, 1999; Kegel and Steinmann, 2008). However, certain challenges such as proper tool support and side-effect freeness of refactorings are still open research questions.The evolution of aPS especially during operation is performed on code level (Fig. 9) by suppliers' start-up personnel or customers' operation staff under high time pressure and mental workload. As discussed in Section 4.1.2, self-healing mode (scenario Sc12f) requires the identification of faults, e.g. “WP jam” (Figs. 6 and 7). The fault handling for “p1” to “p4” (Fig. 7) requires evolution of the initial FBs as discussed in the following: FB1 covers the monitoring of the time constraint (“TimerPusher” in Fig. 6) and the additional digital sensor input (“Sens_Slide” in Fig. 6, “p2” in Fig. 7). Therefore the internal function of FB1 has been changed, which is represented by a new function (“g” in Fig. 9). The added FB2 handles the newly added analog sensor (“Sens_Transducer” in Fig. 6, “p3” in Fig. 7; “Sens_Pressure” in Fig. 6, “p4” in Fig. 7). The also added FB3 offers a sequence of actions to the operator to solve the WP jam based on the faults identified in FB1 and FB2. FB3 output signals are forwarded to the centralized handling of modes of operation and to the technical process.FBD is specified in IEC 61131-3 and widely applied in Europe being a regional market standard. A hierarchical function block often represents a module and used as the basis for reuse in industrial practice (Vogel-Heuser et al. 2014b; Wenger and Zoitl 2012). Such on-site code changes may lead to inconsistencies between the implementation and the library as well as other design artifacts (Vogel-Heuser and Rösch, 2015), discussed using the PPU case study and the replacement of a binary sensor by an analogue one to increase the accuracy of pushing the work pieces into the slides and detecting the potential faults.An analysis of existing PLC code variants and variability management mechanisms in different world market leading plant manufacturing industry companies including organizational and qualification aspects will be beneficial to achieve a deeper understanding of the underlying requirements and mechanisms hindering modularity in plant manufacturing industry. Advanced clone detection mechanisms are required for aPS to detect clones in the different disciplines as well as software clones across projects and even worse across different platforms based on model transformation and PLCopen XML.While refactoring is a generally accepted approach in software engineering to improve code structure, the specific settings of aPS that are, the close relation of mechanic and automation hardware and software and their multi-disciplinary nature, the strong relation to different regional market leading platforms as well as the need for adjustment during operation, and the existing project oriented legacy code poses new challenges for refactoring that have not been addressed so far. A particular research question is to come up with refactoring operations that work across the different disciplines and projects including legacy code in order to improve the overall structure of the system in all its parts, while maintaining the fulfillment of the NFRs.Design patterns have been shown to be beneficial for software developers in order to provide more reusable, maintainable and evolvable code. For aPS, design patterns have not yet been developed extensively. So, an important research question in this respect is to develop design patterns that cross-cut the different disciplines of modern aPS in order to improve their evolvability and to maintain code quality and structure after evolution and fulfill the challenges mentioned above.Clone and own is a widely used reuse approach in practical software and systems engineering. Instead of forcing other reuse mechanisms such as object-orientation to be used in practice, the existing reuse techniques need to be better supported by appropriate tools, such as versioning systems that are capable of feature annotation, feature harvesting or feature propagation. This holds in particular as versions of the aPS may become parallel existing variants of the aPS at later points in time. For aPS, these tools need to be made capable of dealing with the different disciplines, the different modeling formalisms and the relationships between the models of the different disciplines. These tools need to be capable of being integrated into the proprietary development environments for aPS as a prerequisite for applicability in industry.Changes due to evolution are multidisciplinary. The interdisciplinary dependencies as well as the complexity of aPS lead to the risk of unpredictable side effects of evolution in the resulting system (Jaeger et al., 2011). A detection of these side effects becomes necessary and should be carried out automatically to avoid much effort, as explained in Braun et al. (2012). Under these circumstances, an automatic detection of these side effects based on the validation and verification of requirements by exploiting information which is already present in the model and/or in the software of the aPS is the ultimate goal.Management of aPS evolution has to deal with diverging evolution cycles of the involved disciplines (Li et al., 2012). Changing system elements during evolution, e.g., adding, modifying or deleting mechanical constructions or sensors can have discipline-specific as well as interdisciplinary influences on other elements and the system's functionality. Accordingly, managing evolution demands for ensuring correctness of the system's design. Hence, when applying model-based (systems) engineering, evolution of aPS needs to be supported by assuring model correctness in an interdisciplinary, holistic way. Nevertheless, ensuring the correctness of an aPS' engineering artifacts is tightly related to verifying the models' conformance with respect to (a set of) given requirements, i.e. ensuring requirement fulfillment.Because of the high complexity of the automation software and the plant itself, it is usually not obvious how the evolution in one part of the system affects other parts or the whole process (Jaeger et al., 2011). One of the main questions is how each change influences the fulfillment of requirements in different abstraction layers and in different engineering artifacts. For this, we need techniques to efficiently and safely establish the impact of changes in order to isolate them and to identify the unchanged system parts. This step can be supported by modular system representations that encapsulate changes within separate modules. This is a special challenge for aPS that consist of mechanics, automation hardware and software which may not be modularized according to the same structures and may possess cross-cutting properties.The main challenge for validation and verification under system evolution is to provide efficient techniques to establish the desired system properties after evolution without the need to re-verify the complete evolved system from scratch. Rather, previous verification results should be reused or adapted as much as possible. In particular, compositional and incremental verification and validation techniques should be developed to cope with cross-discipline models and reduce the effort for re-establishing properties to only considering the changed parts while neglecting the unchanged parts (as determined by change impact analysis techniques). Compositional reasoning techniques need to be designed to relativize the verification effort to the changed modules while reusing the verification results for the unchanged modules. Particularly, existing approaches need to be adapted for the specific requirements of aPS.Software engineering techniques available in automation engineering today are not sufficient to assure the required level of availability, functional safety and reliability along the systems' life-cycle in the light of shortened innovation cycles. Due to the lack of holistic system models and applicable analysis techniques, the consequences of system modifications (e.g., adapting the control software, the control platform, the IT systems or mechanical components) cannot be verified and validated against the respective requirements beforehand. Recent approaches to address this challenge are digital factories and virtual commissioning techniques. Such approaches are based on simulation and virtual controllers. Thereby, the (physical) state of the controlled system can be simulated, which allows for validating the system behavior in a restricted time frame. On the downside, rare events are difficult to detect. However, these seldom events, which might occur only after long operation times, are potentially very critical. Complementary to simulation approaches, formal analysis techniques can be applied to close this gap. Instead of analyzing the state space of a model within a certain time frame, formal techniques aim at analyzing models exhaustively with respect to all reachable states (Bérard et al., 2001; Clarke et al., 1999). For example, Lahtinen et al. (2012) state that in the nuclear engineering domain, automatic verification is beneficial compared to simulation because it can – besides exploring all reachable states – be applied earlier in the design phase.In aPS, manual testing is still dominant as few tools for supporting automated testing are established up to now. In recent years, some companies have identified the need to support automated testing and introduced first solutions that support the automation of manually programmed test code (CODESYS Testmanager33http://store.codesys.com/codesys-test-manager.html, retrieved 8/21/2015.). These solutions are a first step towards automated testing, but still demand a high effort in test creation. To address these shortcomings, researchers have been introducing several approaches in the field of model-based testing. To support system and test engineers in creating models for testing, semi-formal modeling languages and notations are further developed and formalized to receive a basis for test case generation. The UML is one of the most widely used notations for modeling the structure and behavior of the software up to now, thus, it is no surprise that many approaches focus on deriving test cases from this language.Hametner et al. (2010) and Hussain and Frey (2006) identify useful diagrams for modeling and deriving test cases from UML for the field of automation software development and especially for IEC 61499 implementations. Interaction diagrams are recommended for the extraction of test sequences. In Hussain and Frey (2006), the extraction of test sequences from state charts using round-trip path coverage is shown. Hametner et al. (2011) show a first implementation of the recommended test case generation process using state chart diagrams especially for IEC 61499 applications. Hametner et al. (2010) also mention the timing diagram of the UML as a useful diagram for test case generation but no implementation is shown in their work. Rösch et al. (2014) realize this test case generation, but focus especially on testing machine's reaction to faults by using fault injection. Krause et al. (2008) introduce an approach to automatically generate test cases from UML state charts by first transforming them into a formal model (extended safe place/transition nets). Having the formal model, the test case generation is easily made possible using methods such as unfolding the nets. In Kumar et al. (2011), UML test case generation approaches from state charts are combined with the aim of making them executable by mapping them to the Testing and Control Notation (TTCN-3). The evaluation of the approach is done using a simple communication protocol but the extension of the approach in order to test PLC control software applications is planned as well. Making UML models, and in this work especially sequence diagrams, executable is another focus of using UML diagrams in the testing process. In Kormann et al. (2012), the semantics of sequence diagrams are adapted in order to make direct IEC 61131-3 code generation possible. In this way, the modeled test scenarios can be executed directly. In recent years, SysML is increasingly established for supporting the development process of real-time systems by Detommasi et al. (2013). However, investigations on the possibilities to derive test cases from these models or adapting these models are still missing.In order to integrate the requirements and test specification, a template for test cases has been developed by Feldmann et al. (2014b). The template includes fields for specifying the input and expected output parameters. If the input parameters have been completely specified along with a step size (cp. “step”, Fig. 10), the test cases can be automatically generated by combining the different possible inputs including test cases for invalid input parameters. Including the invalid parameters helps testing the reaction to faults. The expected outputs (attribute “Expected Goal”) may either be specified manually or generated automatically if the requirements have been sufficiently formalized (Feldmann et al., 2014b).An example for a requirements and test specification, namely the self-healing variant on component level (cp. Section 4.1.2), is shown in Fig. 10. For better comprehension, the templates from Fig. 5 have been reduced to the most important attributes illustrating the case study. The test case “Automatic detection WP jam”, shows 1 of 252 possible test cases based on the different possible input parameter combinations according to the step size (9 possible inputs for “Sens_Transducer” {−0.5; 0; 0.5,…,3.5} times 2 possible inputs for “Sens_Slide” {TRUE; FALSE}, etc.). As shown, the test case illustrates an example where if neither the valid input for pressure nor the correct position for ejecting the WP of 3 cm is reached, the output alarm 201 is expected.Besides test case generation and selection of test cases to reduce testing efforts while testing and retesting, approaches have been introduced for the domain of factory automation as well. Ulewicz et al. (2014) present a first approach for selecting and reusing existing test cases based on changes within the control program for efficient testing of changes. Lochau et al. (2014) propose model-based testing for variant-rich aPS. Based on a 150% UML state chart test model incorporating all variant-specific test models with explicit specification of differences by means of feature annotations, test case generation for the complete system family is applicable in order to test the corresponding PLC control software. In contrast to test case generation for each variant in isolation, their approach exploits the specification of commonality and variability between variants to reuse generated test cases also for other variants. Structural coverage criteria are used to derive test cases from the state chart test model by using model checking techniques.Static code analysis is successfully applied for several programming languages and environments, e.g. Lint for C (Johnson, 1988) and FindBugs for Java (Ayewah et al., 2008). However, static code analysis for IEC 61131-3 is not yet supported sufficiently (Angerer et al., 2013). Up to now, only tools supporting parts or specific languages of IEC 61131-3 are provided, e.g. CoDeSys Static Analysis44http://store.codesys.com/codesys-static-analysis.html, retrieved 8/21/2015., logi.Lint55http://www.logicals.com/products/logi.LINT/, retrieved 8/21/2015.by Logicals and PLC Checker66http://www.plcchecker.com/, retrieved 8/21/2015.by Itris. Nevertheless, in Fuchs et al. (2014) and Prähofer et al. (2012) the benefits of static code analysis for IEC 61131-3 software quality improvement are highlighted and an approach for improving compliance to programming conventions and guidelines is proposed, e.g. by identifying incorrect naming conventions, deviating program element complexity or detecting bad code fragments. Thereby, analysis rules are used to express the search criteria.Model Checkers for embedded software for hybrid systems are investigated for nearly two decades now, debuted in 1997 (Henziger et al., 1997). Ortmeier et al. (2004) investigated verification of embedded software focusing on safety aspects, but did not take automation and PLC software into account. The aforementioned MechatronicUML method (Becker et al. 2012; Heinzemann et al., 2013) supports links between engineering disciplines. It supports a compositional approach for the verification of discrete real-time behavior (Giese et al., 2003) to improve scalability. The compositional approach supports multiple refinement notions to guarantee different types of system behavior (Heinzemann and Henkler, 2011). Another aspect is the support of the verification of system's behavior specified in the models of multiple disciplines by combining results of verification activities both on the feedback control as well as on the real-time mode management to prove system wide properties. Finally, the MechatronicUML provides specifically support for systems which self-adapt their behavior at runtime by modeling adaptations by architectural reconfiguration based on graph transformations (Eckardt et al., 2013; Tichy et al., 2008). Verification of the adaptation behavior is based on graph transformation verification approaches (Becker et al., 2006; Ghamaraian et al., 2012). However, the approach does not address the specifics of aPS, e.g., the employed languages.Sünder et al. (2013) propose an approach on verifying PLC programs by means of model checking, taking predefined modifications of the software into account. The proposed formalism is based on the IEC 61499 standard for automation software; its application for cyclically operated PLC software according to the IEC 61131 was not investigated. Verification of PLC programs – written mainly in the programming languages Sequential Function Charts (Bauer et al., 2004) and Instruction List (Huuck, 2005) – was investigated by means of the model checker UPAAL, but lack in analyzing industrial PLC programs due to size and structure. A verification platform called PLC. Arcade supporting model checking to of PLC programs – written for the programming languages ST, IL as well as vendor-specific programming language and their combinations as often applied in IEC 61131 environments – is presented in Biallas et al. (2012). It provides a counterexample-guided abstraction refinement mechanism, which is already applied successfully for verification of embedded systems in Stursberg et al. (2004) and hierarchical predicate abstraction (Biallas et al., 2013) to cope with the challenge of state explosion. Nevertheless, the approach was not applied to industrial PLC software to identify its applicability in practice. Gourcuff et al. (2008) presented an approach for verifying cyclically executed PLC software. The applicability of the approach to larger sized programs is achieved by applying different interpretation and data abstraction techniques. This work focuses on analyzing the textual programming languages ST and IL but is solely limited to Boolean variables. Therefore, its applicability to real industrial PLC software is limited. Recently, Legat et al. (2014) and Hackenberg et al. (2014) applied model checking to also consider the interplay between PLC programs and electrical as well as mechanical aspects in order to verify conformance to requirements resp. a technical process by using extensive behavioral abstractions.Witsch and Vogel-Heuser (2011) proposed an approach to implement PLC software based on UML state charts (so called PLC state charts) and described its behavioral semantics and application for model checking. A transformation of the state chart into a timed automaton, which can be analyzed by the model checker UPPAAL, is presented. The applicability of the approach was evaluated with multiple case studies. An application of the approach to languages according to IEC 61131-3 standards was not investigated until now. Mertke and Frey (2001) proposed an approach on formal verification based on Petri nets by using the SPIN model checker. The scalability of the approach by means of industrial PLC programs was not investigated. Greifeneder and Frey (2007) proposed the modeling language called DesLaNAS which can be applied in combination with a probabilistic model checking but an automatic translation of the description model into the form needed for model checking is not available. Machado et al. (2006) investigated the impact of applying a model of the physical plant additionally to the formal specification of controller behavior. In this work, the model checker NuSMV is applied; cyclic operation of PLCs was considered, but timing constraints were not taken into account. Nevertheless, they identified the necessity of a plant model in order to verify specific dependability requirements successfully.An integrated approach to verify conformance of aPS designs is presented in Vyatkin et al. (2009). It considers the overall behavior of mechatronic components in a plant model, i.e. the behavior resulting from the combination of different disciplines. However, the approach focuses on the IEC 61499 standard for control software. The applicability of the approach for PLC software according to the IEC 61131-3 standard is not available. When applying a plant model, further aspects can be taken into account, e.g. behavior of real-time communication buses, electric/electronic or mechanic component behavior, etc. For example, in Witsch et al. (2006) an approach to verify the timing behavior of standard Ethernet networks was evaluated by means of a simple application example.Runtime monitoring allows for establishing the linkage between evolving plant behavior and formally specified requirements, in particular for legacy aPS for which no formal requirements for the evolved plant exist (Haubeck et al., 2013). From the observable system behavior during system operation, a model of the actual evolved system behavior can be constructed. In this way, the lack of explicit and up to date specification for the evolved system can be alleviated because the models are automatically generated and adapted by observing the in- and output data of the aPS. The constructed models can then be analyzed in order to derive current plant properties. Subsequently, the properties can be checked against specified values for requirement verification.To sum up, a variety of approaches to verify the control software exist. Sophisticated approaches are applied in the domain of embedded software without taking typical characteristics of PLC software into account. In the domain of production automation, also various approaches for verifying a system's behavior by means of model checking exist. Some approaches are based upon software code taking solely the software's and controller characteristics (e.g., cyclic execution) into account. In contrast, also a variety of approaches consider additionally system structure and process characteristics. Here, the complexity of the verification task increases drastically. The applicability of the approaches depends typically on the different, applied abstraction mechanisms in order to address the state explosion problem: the more precise the model, the bigger the state explosion problem. The best granularity to be chosen depends on the requirements of the verification task and characteristics of the considered system. Here generic statements are still an open issue. Therefore, the applicability to industrial PLC software is still in an open issue.In order to efficiently verify system properties after evolution, we require incremental and compositional verification techniques. Incremental techniques rely on the results of a change impact analysis identifying the influences of the changes performed during the evolution in all disciplines and across disciplines. Using the identified change, incremental techniques can (re-)establish system properties by only considering the changed parts while neglecting the unchanged parts. The particular challenge for change impact analysis and incremental verification in aPS is the analysis of properties cross-cutting several disciplines.Compositional verification techniques allow for establishing system properties by analyzing only the system parts (its components) and deriving the overall system properties from the properties of the system parts. While there exist approaches for compositional verification of software, the particular research question for aPS is to develop compositional approaches that can deal with properties cross-cutting different disciplines. A particular question is how to find and deal modularization concepts over different disciplines as there may be different component structures in the mechanical, electrical/electronic and software parts of an aPS.Due to the complexity of aPS, the right level of abstraction for (automated) verification with respect to verification use-cases is crucial. Therefore, future research should investigate the classification of such phenomena, patterns and levels of abstraction suited to the specific needs of aPS, and methods to encapsulate those abstractions for verification, e.g., by using contracts, and thus support a more modular approach suitable for evolving systems as needed in the field of aPS.Hence, different simulation, test and formal analysis techniques need to be integrated to fully verify the behavior of aPS. In particular, the integration of simulation and testing approaches for mechanical and electrical properties in combination with formal analysis approaches for software properties is a challenging question for future research, especially when combined with the requirement for incremental and compositional verification.aPS are highly diverse. They may differ in the processes which they are designed to execute, but also in the mechanical and electrical/electronic parts and software parts used to complete their tasks. The inherent variability in such systems results in a huge number of possible variants which may be described by a product family, e.g., a software product line (SPL) (Clements and Northrop, 2001; Pohl et al., 2005). An aPS SPL can be called multi-disciplinary as variability cross-cuts several disciplines, i.e., mechanical, automation hardware and software aspects. For the development of variant-rich aPS, the knowledge of commonality and variability as well as its integration in engineering artifacts across its different disciplines is an important task. Variability management comprises (1) variability models in the problem space to define the configuration space which may be of different granularity for the overall product system, the mechanical, electrical/electronic and software parts, (2) variability models in the solution space to define variable and reusable engineering artifacts which span mechanical, electrical and software parts, and (3) the configuration knowledge to guarantee that a valid variant of the aPS in all its disciplines is obtained (Czarnecki and Eisenecker, 2000).For the evolution of variant-rich systems, new challenges arise because not only engineering artifacts across all disciplines evolve, but also the variability management has to handle the changes in the different variability models. aPS are often designed for a very long lifetime. Due to limited durability of aPS components or technology changes during the lifetime, changes in hardware may occur. This also affects software counterparts (Li et al., 2012). Besides, there are several other reasons for changes, such as differing production requirements, process improvements or product variations. Additionally, there can be different variants of the same aPS at the same point in time (Braun et al., 2012), e.g. to satisfy varying customer needs. Such changes lead to a high diversity in modern aPS: variability resulting in several simultaneous system variants (such as the variants Sc12a to Sc12g of the PPU) and evolution resulting in different versions of these variants over time (such as the evolution scenarios Sc0 to Sc13 of the PPU). This diversity creates high complexity for system management and maintenance (Brooks, 1995). In particular, if changes occur, new software has to be extended or changed. Additionally, the proper functioning of the system has to be ensured for all variants and versions.In the following, challenges in the evolution of variant-rich aPS are explained. Second, an overview of the state-of-the-art techniques in variability management is given, and approaches for variability-aware evolution are presented. Finally, we present a research roadmap with respect to modeling and managing multidisciplinary variability of evolving aPS.Engineering artifacts span the complete aPS and cross-cut all disciplines. The same holds for the variability models which may cover the variability of one discipline at a suitable level of granularity or cross-cut different disciplines introducing even constraints between the disciplines. A particular challenge is to design variability models and configuration knowledge in such a way that a working configuration of an aPS can be (automatically) derived. Feldmann et al. (2012) analyzed the approaches and challenges for modularity, variant and version management in aPS. For variability management from an electrical engineering viewpoint, tools like EPLAN Engineering Center77http://www.eplan.de/en/solutions/product-overview/eplan-engineering-center/, retrieved 8/21/2015.as well as the Siemens Platform COMOS88http://w3.siemens.com/mcms/plant-engineering-software/en/comos-overview/, retrieved 8/21/2015.exist, and for mechanical engineering, Design Structure Matrices are most popular (Kortler et al., 2011). Feldmann et al. (2012) showed that module structures in different disciplines differ. “The case studies demonstrate that modularity structures in software engineering are different from structures in electrical engineering: software modules are structured according to their functions whereas electrical modules are structured according to the electrical devices. Subsequently, according to the different modularity paradigms, the term function in software engineering refers to an action that can be called, whereas in electrical engineering a device's functionality is addressed.”In order to deal with evolution of variability models within and across disciplines, both in the problem and in the solution space, as well as for the configuration knowledge, we need incremental modeling approaches which allow us to capture change to the involved artifacts in a precise and modular manner without the need to alter or refactor the already existing engineering artifacts.For the evolution of the problem space, means to add, remove or modify configuration options are needed. This yields the need for analyses whether previously existing variants are still configurable and whether the variability model is backwards-compatible. Furthermore, after evolving the variability model, it may contain inconsistencies, such as dead features or false optional features, or it may not be possible any longer to generate valid product variants (void feature model). This leads to the need for efficient consistency checking of the evolved feature model. For the evolution of variability in the solution space, i.e., of the reusable/configurable engineering artifacts in the different disciplines of an aPS, we have the following challenges:For the evolved variant-rich aPS, we need to ensure the syntactic and semantic correctness of the configured aPS variants despite the changes of the artifacts with respect to the requirements. This requires efficient analyses for syntactic and semantic compatibilities and consistencies within and across disciplines. We need to maintain the consistency of the variability in different kinds of engineering artifacts within or across the different disciplines of aPS and across the development process in order to allow for a seamless variability management for current and future variants.For the configuration knowledge, we have to ensure the consistent co-evolution between problem and solution space variability models. We have to ensure that all variants expressible in the problem space are also configurable in the solution space and, in the other direction, that all possible solution space variants are also covered by the problem space variability model. When the solution space evolves without evolving the corresponding explicit variability model, we need means to derive those variability models for the evolved engineering artifacts in order to allow for better further evolution or debugging.Variability modeling in the problem space defines the commonality and variability of a product family in order to specify the valid configuration space i.e., specifying the set of valid software variants (Czarnecki and Eisenecker, 2000). Various approaches for variability modeling in the problem space exist, for instance, feature models (Kang et al., 1990), orthogonal variability models (Pohl et al., 2005) and decision-models. Those variability models usually take a high-level abstract system view and consider user-visible product characteristics as features. We describe those approaches in the following and refer to the literature (Chen et al., 2009; Schmid et al., 2011; Sinnema and Deelstra, 2007) for further information.In the context of feature-oriented domain analysis, Kang et al. (1990) introduce feature models for capturing the commonality and variability of variant-rich systems by means of features and their interrelations. The work by Kang focuses on general software-intensive systems, but does not consider multidisciplinary aPS and multidisciplinary artifacts and features. A feature, in the context of aPS, represents a configuration option of the aPS, which may be a mechanical, electrical or software-specific parameter, or a visible functionality that cross-cuts all disciplines. Features may be referring to the overall aPS or to some of its components and may have different granularity. A feature is realized by one or several engineering artifacts in the solution space within one or across several disciplines depending on its granularity. Each valid feature configuration, i.e., each feature is either selected or deselected for a configuration, represents a unique product variant. For further information regarding feature modeling and analysis, we refer to the literature (Benavides et al., 2010). An example of the use of feature models in the design of aPS is given in Feldmann et al. (2015) and Schröck et al. (2015).In Fig. 11(Lochau et al., 2014), the graphical representation of a feature model for the overall system of the PPU at a high level of granularity and in Fig. 12(left) a more detailed view for the crane are shown. The tree-like structure is defined based on different decomposition relations (Fig. 11). A mandatory feature is part of each variant if its parent feature is also selected. For instance, the mandatory feature “Crane” is a part of every configuration as its parent the root feature “PPU” is always selected for a configuration. For optional features, we are able to decide whether the feature is selected for a configuration or not if its parent feature is selected. An alternative feature group allows for the selection of solely one feature of the group, whereas feature groups allow for the selection of at least one feature up to all features within the group. For instance, the selection of either “Standard Routing” or “Extended Routing” is possible, whereas for feature “Workpiece”, we are able to choose “Plastic”, “Metal” or both features. In addition to the decomposition relations, feature models comprise crosstree constraints in terms of require and exclude edges. A require edge represents the implication of feature selections, e.g., the selection of “Stamp” implicates the selection of “Metal”. Exclude edges denote that only one of the connected features can be selected for a valid configuration. Nevertheless, the feature model neglects the different customer features from scenarios Sc12a and Sc12g (Section 2.3), consisting of NFR like platform supplier and runtime environment and FR like higher throughput, increased weight or size of workpiece.Fig. 12 shows the detailed feature models of the crane from the different disciplines point of view. The customer view (Fig. 12, left) defines the “Crane” features that can be chosen by a customer (e.g., either heavier or lighter workpieces, cp. scenario Sc12b only weight changed). The developer view (Fig. 12, right) focuses on the variable features that can be combined from a discipline perspective (e.g., mechanical, electrics/electronics and software engineering). In some cases, choosing a feature in one discipline imposes adjustments to features in other disciplines (illustrated as adaption in Fig. 12 – e.g., an additional “Rail” to move the “Crane” linearly imposes adaptions to the “Gripper” and the “Motor control unit”). Feldmann et al. (2015) showed the mapping of the customer feature models to the developer feature model with a mapping matrix. Such a mapping matrix however merely provides a rough mapping between customer and developer feature models; hence, these relations between the different views need to be investigated in more detail. The use of feature models in aPS intents to automatically configure the aPS on basis of these models. The benefit from using feature modeling and the effort needed to manually develop and maintain the models is much too small to be applicable for aPS, yet. Besides scalability for real world applications needs to be further evaluated (Feldmann et al., 2015).Pohl et al. (2005) proposed the orthogonal variability model also for capturing the commonality and variability of variant-rich software systems in a graphical way by means of variation points. A variation point represents a customer and/or developer-visible property of the software system and is implemented by one or several engineering artifacts or rather related to other variability models in the solution space. Variation points are decomposed by at least one variant. A variant describes the difference between configurations as for instance the difference of the stamp' s pressure of the PPU. Similar to feature models, variants can be optional, mandatory, or alternative and also be constrained by require and exclude relations. Each valid selection of variants represents a unique product configuration.Another variability modeling approach is decision modeling introduced by the Synthesis project (Synthesis, 1993). In contrast to feature models and orthogonal variability models, where the domain with its features/variation points are specified, decision models focus on development decisions, such as selecting or deselecting specific configuration options or choosing a particular configuration parameter, and their interrelation to describe the variability of a variant-rich systems. Therefore, a unique product variant is defined by a valid selection of decisions. For further information regarding decision modeling, we refer to the literature (Schmid et al., 2011). It has been applied to the modeling of steel manufacturing systems (Dhungana et al., 2014), considering multidisciplinary artifacts.In addition to variability modeling in the problem space, variability management comprises variability modeling also in the solution space to define variable and reusable engineering artifacts by integrating variability. Variability modeling approaches in the solution space of software-based SPLs can be separated into three different classes: annotative, compositional and transformational (Schaefer et al., 2012). Annotative methods consider one model representing all products of the product line. Variant annotations, e.g., using stereotypes in UML models (Gomaa, 2006) or presence conditions (Czarnecki and Antkiewicz, 2005), define which parts of the model have to be removed to derive a concrete product variant. Annotative variability models become easily very complex and unmanageable for large SPLs with many variants.Compositional approaches associate model fragments with features, which are composed for a specific feature configuration. A well-known example is AHEAD (Batory et al., 2004), in which a product is incrementally built using a base module and a sequence of feature modules. In Noda and Kishi (2008), models are constructed by aspect-oriented composition. Other approaches to represent variability on a modeling level, e.g. in Klein et al. (1997) and Prehofer (2004), have focused more on composing and adding features and not on capturing evolution. Compositional approaches can only add functionality to an existing product, and the impact of a feature is limited by the used composition technique. Evolution inevitably needs refactoring, when using compositional methods.Model transformations are, for instance, applied in the common variability language (Haugen et al., 2008) where the variability of a base model is described by rules how modeling elements of the base model have to be substituted in order to obtain a particular product model. Delta modeling is a modular transformational approach to design and implement software product lines (e.g., see Schaefer, 2010). Deltas encapsulate all necessary changes to get from one variant, which is, e.g., the most basic variant, to an arbitrary variant in the product line. Delta modeling has so far been applied to represent variability of software architectures (Haber et al., 2011), Java programs (Schaefer, 2010) and a multi-perspective modeling approach for manufacturing systems (Kowal et al., 2014). Each perspective consists of a different UML diagram on which deltas can be applied making variability on all abstraction levels of the system manageable.There is an extensive amount of work ongoing concerning evolution and variability in aPS, especially due to the participation of different research communities (e.g. mechanical, electrical or automation engineers and computer scientists). Elsner et al. (2010) consider evolving software product lines and focus on variability in time. They propose a common terminology for variability in time and space in evolving software product lines. A second survey article focuses on research on diversity in software and how it affects all development phases from modeling over design and implementation to quality assurance, including software evolution (Schaefer et al., 2012). Ideas to manage the evolution of mechatronic systems can also be divided into the two categories of evolution in the problem space and evolution in solution space.As already mentioned before, feature models are widely spread for variability modeling in the problem space. Thüm et al. (2009) present an algorithm computing the differences between two feature models after changes to a feature model. In general, the inputs are the original and the evolved feature models. The algorithm computes deleted or added product variants and scales up to large feature models with thousands of features. However, it only focuses on a homogeneous feature model and does not cover multidisciplinary aspects and features of different granularity.A second approach creates a special kind of feature model in addition to the existing one. This EvoFM-feature model captures different evolution steps just as a standard feature model captures an SPL. Each configuration of the EvoFM represents exactly one evolution step and can be transformed to the explicit feature model. Tool support is fully available and is already tested with an Eclipse as a large product line example (Pleuss et al., 2012). But, again, multidisciplinary feature modeling is not considered.Similar to the previous approach, Seidl et al. (2014) describe how to extend the standard feature models to capture evolution over time. They propose Hyper Feature Models, in which all features get version numbers to express different versions of the feature in the solution space over time. The different feature versions are bound by a successor relation. This approach does not support explicit changes to the feature model, e.g., a feature changes from mandatory to optional. Nevertheless, automatic derivation of valid product configuration is possible, and a more powerful cross-tree constraint language is given to support dependencies between several feature versions and their (possible) more complex interactions. Their application example is robotic systems, however, only the software aspect of driver software is considered in the solution space.Not all approaches follow the concept of feature models. Schubanz et al. (2013) developed their own specific modeling approach, which is integrated in a prototypical tool chain and focuses on a high level of abstraction, i.e., evolution of development goals and requirements. Their main goal is to provide a tool which is able to manage evolution over several releases (variability in time) as well as the ability to give the developer traces from the requirements to their respective implementation variants (variability in space). This approach, however, covers software systems only.In the solution space, there are only a few approaches covering variability and evolution at the same time. The approach by Dhungana et al. (2010) uses model fragments for capturing the solution space variability following the principle that smaller models are better to understand. The variability of the whole product line is encapsulated in these fragments, so that individual developers can concentrate on their specific parts of the system. Each fragment can evolve independently from other fragments. All fragments are connected by a set of interrelations. The implementation, which was done for Siemens, supports semi-automatic merging of model fragments to a complete product line. In addition, consistency checks after another evolution step can be done automatically. This approach is based on a uniform software modeling approach and does not address multidisciplinary aspects.Another appropriate solution for capturing variability and evolution in the solution space is the usage of delta modeling, which was already introduced in the previous section. Variants and versions are managed in the same manner by delta modeling. Therefore, the multi-perspective modeling approach in Kowal et al. (2014) is also applicable for evolution (cp. Section 7.2). It already contains software architectures, but a more detailed and powerful architecture description language (ADL), and the benefits of delta modeling for architectural evolution can be found in Haber et al. (2012).It is also possible to mine commonalities and differences in models for the creation of a family model, which is a so called 150% model (Schaefer et al., 2012) containing the complete variability of the system. This vastly improves the commonly used clone-and-own practice in mechanical engineering, which introduces lots of redundancies and is error-prone for debugging and maintenance. Wille et al. (2013) apply this idea to block-based diagrams, e.g., as available in Matlab/Simulink. Holthusen et al. (2014) apply it in a prototypical manner to the IEC 61131-3- language FBD. However, an extension to the full language scope of IEC61131 is still missing.The main research question concerning variability management of aPS is the management of multi-disciplinary variability in problem and solution space for FR and NFR with different levels of abstraction and granularity. A particular problem is here to ensure the consistency of the variability models at different levels of granularity within and across disciplines. Support for automatic generation of feature models out of legacy code from different projects is a challenging task to reduce the effort introducing feature modeling approaches in aPS.Also the integration into engineering workflow, organization and development frameworks is a big issue. Scalability for real world applications needs to be further evaluated, too. The application of feature models in aPS intents to automatically configure the aPS across the different disciplines on basis of these models. Therefore an integrated feature model would be beneficial.In order to support developers both in domain engineering where reusable artifacts are built and in application engineering where the actual variants are derived and customized, we further need visualization of changes in aPS between different variants and their versions. In particular, these visualization techniques have to cover the changes within different disciplines and across disciplines.Importance and applicability of model-driven engineering rose during the last decade in aPS. After discussing the different life-cycle phases in Sections 3–5, this chapter focuses aPS from a model-driven engineering point of view highlighting the challenges, discussing the state of the art and giving a roadmap for future development.While model-driven engineering shows an increase in effectiveness and quality and is used in the embedded software industry (Liebel et al., 2014) by exploiting domain knowledge, it also introduces general challenges and challenges specific to aPS.Typically, the models that are developed during system design are in later phases converted to source code by code generation. This process is called Forward Engineering (Sendall and Küster, 2004). A challenge is now to ensure consistency between the different models and the generated code in case of manual changes, which might be required in general, for example, to integrate source code with specific third-party libraries or platforms, other source code, or specifically in aPS to perform on-site adaptations. These changes have to be propagated back to the models and documentation of all discipline to guarantee that they are not overwritten in case the component is used again, i.e. for code: the source code is generated again after other changes to the model. This cycle of generating code from models and propagating changes on the source code back to the models is known as round-trip engineering (Hettel et al., 2008) in contrast to the one-way forward engineering of source code from models and the one-way reverse engineering from models from source code. A particular issue, which complicates this challenge, is that manual changes in source code often do not have a corresponding equivalent in the model, i.e., the modeling language is not able to express the changed behavior.This challenge is related to the specific situation in aPS in which changes on the automation system have to be done on-site by technicians or even skilled workers (Vogel-Heuser, 2009) to perform generally small-scale adaptations, e.g., adapt the behavior of the automation system due to replaced hardware or slightly different characteristics of hardware or workpieces or material, environmental characteristics etc. (cp. categories “V” and “VI” in Table 1). Depending on the project's phase, further personnel is involved, e.g., before system handover the staff will be suppliers' start-up personnel (before acceptance test and handover) or even customer personnel. The additional challenge here compared to the above-discussed challenge of round-trip engineering is that these on-site technicians or skilled workers are typically not trained to use model-driven engineering approaches and the acceptance of modeling languages is low. Furthermore, the additional licenses of the modeling software required for on-site changes might be prohibitive for supplier staff and even more applicable for customer staffFinally, three recent surveys of practice in model-driven approaches for embedded systems and usability of model-driven engineering notations in embedded systems and, specifically, aPS (Hutchinson et al., 2011; Liebel et al., 2014; Vogel-Heuser, 2014e) show that tool integration and tool usability as well as high effort to reap benefits are big challenges in employing model-driven engineering in industry. Education is an issue related to this. As Vogel-Heuser et al. (2012) showed usability of MDE approaches is strongly related to students' basic skills and appropriate fade out training approach. Sim and Vogel-Heuser (2010) proved the benefit of active learning comparing mechatronic engineering students with students of computer science, but still an appropriate education for MDE with a focus on aPS is missing.Several approaches focus on the development of modeling approaches that support the development of various aspects of aPS. An approach for model-driven engineering that sufficiently supports the development and implementation of software in aPS needs to provide an automatic generation of executable software, as, e.g., presented by Bonfè et al. (2013), Estévez et al. (2007), Witsch and Vogel-Heuser (2011) and Yang and Vyatkin (2012). Furthermore, in Vepsalainen et al. (2010), it was identified that the modeling of user-defined control logic is required in addition to the application of predefined control blocks. For acceptance of novel concepts, those have to be easily applicable and reproducible for other researchers (Goldberg, 2012). By the adaptation of a widespread modeling language, the reproducibility of a model-driven engineering approach can be increased, because other researchers may already be familiar with the language, and basic tool support is available. Furthermore, integration with related concepts such as model-based test case generation and execution (Kormann et al., 2012) is simplified.The modeling approaches used in these and other works (Bassi et al., 2011; Hackenberg et al. 2014; Bonfè et al., 2013) enable an integration of software models and physical models into a single consistent syntax. In contrast to these integrated approaches, there are many research works that combine control code (or an executable model thereof) with an object-oriented simulation model of the physical parts of the aPS. Such approaches require to include a separate simulation tool, which is possible for the designer but not for the technician on-site, and are therefore not considered here in more detail.As a basis to describe different aspects of aPS at different hierarchical levels, adaptations of UML and SysML are proposed in Bassi et al. (2011), Bonfè et al. (2005) and Secchi et al. (2007). Concepts for supplying object-oriented models with a formal basis, e.g., in order to apply methods for verifying modeled system requirements (Sünder et al., 2013), have been proposed by Secchi et al. (2007). The modeling approaches used in these and other works (Bassi et al., 2011; Bonfè et al., 2013) enable an integration of software models and physical models into a single consistent syntax. To generate detailed automation software, design patterns and transformation rules are proposed in Bonfè et al. (2013). Although this approach enables the transformation of state chart models into executable PLC code, a direct integration of the software model and executed code is not provided. The integration of MDE, i.e. plcML being a UML dialect, into a PLC programming environment is realized by Witsch and Vogel-Heuser (2011) and available for state chart and class diagrams in CODESYS V399http://www.codesys.com/products/codesys-engineering/development-system.html, retrieved 8/21/2015.. Sequence diagrams (Kormann et al., 2012) and activity diagrams (Bayrak et al., 2011) have been adapted and integrated too, but are not available on the market yet. Code generation from plcML to Siemens S71010http://w3.siemens.com/mcms/simatic-controller-software/en/step7, retrieved 8/21/2015.platform has been introduced in Tikhonov et al. (2014). For hybrid models combining closed loop control and interlocking, Bayrak et al. (2012) and Schneider et al. (2014) developed the model transformation from MATLAB/Simulink to CFC for different PLC programming environments.Aside from works addressing IEC 61131-3 implementations, event-driven implementations conforming to the IEC 61499 standard (Bianchi et al., 2003; Chhabra and Emami, 2011; Hirsch, 2010; Hirsch et al., 2008; Vyatkin et al., 2009) were proposed. A systems engineering framework based on SysML and IEC 61499 is considered in Hirsch (2010) and Hirsch et al. (2008). However, with the approach of Hirsch (2010) and Hirsch et al. (2008), debugging of automation software directly inside the SysML model is not provided. An approach to generate IEC 61499 compliant software from UML-based models is presented in (Dubinin et al., 2005). In Vyatkin et al. (2009) and Yang and Vyatkin (2012), a concept for model transformations between IEC 61499 and Matlab/Simulink models is proposed that supports transforming automation software to MATLAB/Simulink for verification purposes and the transformation of controllers designed using MATLAB/Simulink into executable software. However, a separate modeling environment is used for the model development that is not integrated in the software development or coupled with the runtime environment in which the generated code is executed.The general area of round-trip engineering is well-researched (Hettel et al., 2008). However, the approaches mostly focus on ensuring the consistency of multiple models (cp. Section 4). With respect to ensuring consistency of models with manually changed generated code, the approach by Bork et al. (2008) exploits the templates used for code generation to re-parse the generated code with manual changes. This approach is independent from the code generation templates and, therefore, easily applicable. However, it does not support the case that the manual changes in the source code do not fit any of the code generation templates. The challenge of round-trip engineering is supported by the close integration of UML into the IEC 61131-3 environment (Witsch and Vogel-Heuser, 2011) because the model is the code. But as mentioned above, the benefit and the acceptance is still underachieved in experiments and in industry.Völter et al. (2006) present design patterns for the integration of generated code with manual written code to avoid the problem of manual changes in generated code. However, this requires that the potential manual changes are already known and planned for during system development. This conflicts with the need in aPS to perform arbitrary changes on-site by non-developers.The directions for future work in model-driven engineering are manifold, i.e., to increase appropriateness of the modeling notation for software but also for the entire mechatronic system, the aPS, taking into account the qualification level of the maintenance staff, to support round trip engineering for many platforms and to understand obstacles using even domain specific notations integrated into well-established development frameworks. Regarding evolution support, we still do not understand the real obstacles in reuse and modification from a human centered point of view.Furthermore, the problem with manual changes of code generated from models is not addressed in aPS. This problem needs to be addressed from various perspectives: (1) the programming languages in the aPS domain are not object-oriented, thus other options to extend/integrate/adapt generated code by manual changes need to be developed and (2) the education background is different between the developers from the company producing the aPS and the onsite technician, thus, if the technicians are allowed to make changes to the generated code, approaches need to be developed to ensure the correctness of the changes, to only allow those changes which by design cannot affect the correctness, or to ensure that the behavior of the plant can only change to a certain degree.Traceability is required in many engineering standards (e.g., IEEE-Standard 830-1998). Therefore, managing the traceability links and exploiting them during system evolution and across disciplines for different activities, like ensuring that requirements are properly tested, that the impact of source code changes on tests can be calculated, that on-site changes in code or hardware are traced to the design artifacts and documents is important.The system design plays a central role in ensuring the traceability from requirements to code to validation activities. Hence, changes in the system design can affect the traceability in different ways. One effect is that traces might be simply lost if a component is replaced by another one during evolution as long as trace links are not properly handled. This would lead to the loss of the information that a certain component implements a specific requirement and which parts of a system a test case checks. Even if the trace information is not lost, trace information can deteriorate. For example, if a trace link between several requirements and the implementing component is automatically replaced by several trace links in the case the component is refactored into multiple components during evolution, it is unclear which of the resulting component implements which requirement.Furthermore, while traceability is already difficult to ensure automatically without manual activities, the diversity of tools and disciplines in the different disciplines addressing aPS makes it even more challenging, since the different disciplines are interwoven and proceed iteratively during design depending on new data or new chosen solutions in one discipline (cp. Table 1).The problem of ensuring traceability from requirements to system design and verification and validation artifacts is partly addressed by model transformation approaches. If model transformation approaches are used to create and update artifacts, most of them also create and update trace links between the elements of different artifacts during the transformation. ATL (Jouault and Kurtev, 2006) and QVT (Object Management Group, 2008) are examples of model transformation approaches, which automatically create the traces. Henshin (Arendt et al., 2010) allows transformation developers to manually add trace links, whereas the Triple Graph Grammar approach of Hermann et al. (2014), which is based on Henshin, automatically creates trace links.Trace links can also be created using information-retrieval techniques (Cleland-Huang et al., 2007). The authors give an overview on two techniques for automatic generation of trace link candidates from requirements and other system artifacts based on probabilistic network models (Antoniol et al., 2002) and vector space models (Cleland-Huang et al., 2005). While the reported recall for the probabilistic network model recall in four case studies between textual requirements and system design models (UML) is between 60% and 90%, the reported precision is 4–32% much lower.Berkovich et al. (2011) investigate tracing on interdisciplinary Product-Service Systems (PSS), e.g. mechatronic products including services. Wolfenstetter et al. (2015) analyzed and evaluated different traceability techniques from a requirements engineering perspective regarding ten criteria, e.g., variability and configuration management, version management, simultaneous development of different views, with the result provides sufficient support.While there exist approaches to handle tracing in the different disciplines, support for tracing between development artifacts (or elements in the artifacts) is still missing. This problem needs to be addressed from the following directions: (1) the different disciplines use different tools (and type of tools), which need to support traces cross-cutting the discipline artifacts, and (2) the traces are will be created, managed and exploited by engineers which are typically part of different departments in the organization, which may lead to organizational challenges like who is responsible with respect to the effort and the costs.While there are some approaches to automate some parts of creating traces, most of the work related to tracing is still manual. Here, research needs to be done to identify options to automate parts of the trace management. Particularly, one interesting direction is developing machine-learning approaches based on historical user behavior or repositories containing many versions of already manually traced artifacts.According to the project dependent and project independent activities presented in the adapted (VDI/VDE 3695, 2010) life-cycle model, six evolution scenarios for automated production systems were introduced and linked to the evolution scenarios of a simple pick and place unit as application example highlighting evolution steps in different disciplines and their processing. The resulting product in aPS is an often unique customer-specific machine or plant. The most important constraints are the strong relation and dependencies between software, automation hardware and mechanics. None of the disciplines should be developed without the knowledge and consideration of these dependencies. Evolution in aPS may be necessary during runtime of the aPS and may also include unanticipated changes. Variability is an issue across disciplines and across projects and in all phases and referring to all cross-cutting topics. Classical solutions from computer science are not applicable due to cyclic behavior of platforms, mostly proprietary operating systems and specific programming languages like IEC 61131-3.Along the life-cycle the different phases with their specific constraints were discussed in detail and specific challenges were derived. Requirements engineering still needs further development to be efficiently applied and successfully introduced in industry, compared to software engineering. In systems design, consistency of models from disciplines for specific purposes (fault analysis, safety) is required, but needs further support in the future. Architectural debt in systems design is compared to software engineering enlarged by automation hardware and mechanics. Regarding system realization, implementation as well as operation and maintenance changes may be conducted on code level on-site by customer staff as well as in hardware which results in the challenge to keep code and model (documentation) consistent. Especially testing is an upcoming topic in the context of validation and verification but strongly depending on the detail and quality of the requirements. Code analysis as a basis for test automation is an interesting approach, too. Simulation approaches are, compared to model checking approaches, already applied in machine manufacturing industry. A key issue to cope with complexity of evolution in aPS is variability management (Section 7), because aPS are variant-rich and require cross-discipline variability models coping also with parallel evolution and different versions. The mapping between problem space variability models and solution space artifacts may be different, because variability models in the different disciplines from a developer point of view seem to be beneficial. Model-driven engineering needs to support round-trip engineering. One of the challenges for aPS is to assure consistency between models in the different phases of the life-cycle with the real aPS on site for all involved disciplines. This leads immediately to the required tracking and tracing of changes as discussed in Section 9.The identified aspects for future research discussed in the roadmap in each chapter are summarized in the following. An adequate light weight and efficient way to model requirements for aPS and refine or change them during the design process needs to be developed including both FR and NFR. NFRs should be operationalized in a way that they are quantifiable but still technology-independent. Thus, their continuous fulfillment can be verified before and after an evolution steps, despite of changes of the implementation technology.An adequate support for aPS is still missing, which is on the one hand manageable regarding effort needed and benefit gained and on the other hand copes with variability and version management also of sub-systems.To decrease architectural technical debt, the impact of cross discipline decisions should be predicted and visualized referring to the functional and non-functional requirements influenced. The non-compliance between architectural guidelines and system architecture should be evaluated qualitatively and quantitatively for controlling purposes during and after the individual project.Advanced clone detection mechanisms are required for aPS to detect clones in the different disciplines as well as software clones across projects.While refactoring is a generally accepted approach in software engineering to improve code structure, the specific setting of aPS that is the close relation of mechanic and automation hardware and software and their multi-disciplinary nature, the strong relation to different regional market leading platforms as well as the need for adjustment during operation and the existing project oriented legacy code pose new challenges for refactoring that have not been addressed so far. A particular research question is to come up with refactoring operations that work across the different disciplines and projects including legacy code in order to improve the overall structure of the system in all its parts, while maintaining the fulfillment of the NFRs.For aPS, design patterns have not yet been developed extensively. So, an important research question in this respect is to develop design patterns that cross-cut the different disciplines of modern aPS in order to improve their evolvability and to maintain code quality and structure after evolution, and fulfill the challenges mentioned above.Clone and own is a widely used reuse approach in practical software and systems engineering. Instead of forcing other reuse mechanisms such as object-orientation to be used in practice, the existing reuse techniques need to be better supported by appropriate tools, such as versioning system that are capable of feature annotation, feature harvesting or feature propagation. This holds in particular as versions of the aPS may become parallel existing variants of the aPS at later points in time. These tools need to be capable of being integrated into the proprietary development environments for aPS as a prerequisite for applicability in industry.The particular challenge for change impact analysis and incremental verification in aPS is the analysis of properties cross-cutting several disciplines. While there exist approaches for compositional verification of software, the particular research question for aPS is to develop compositional approaches that can deal with properties cross-cutting different disciplines. A particular question is how to find and deal with modularization concepts over different disciplines, as there may be different component structures in the mechanical, electrical/electronic and software parts of an aPS.Due to the complexity of aPS, the right level of abstraction for (automated) verification with respect to verification use-cases is crucial. Therefore, future research should investigate the classification of such phenomena, patterns and levels of abstraction suited to the specific needs of aPS, and methods to encapsulate those abstractions for verification, e.g., by using contracts, and thus support a more modular approach suitable for evolving systems as needed in aPS.Furthermore, different simulation, test and formal analysis techniques need to be integrated to fully verify the behavior of aPS. In particular, the integration of simulation and testing approaches for mechanical and electrical properties in combination with formal analysis approaches for software properties is a challenging question for future research, especially when combined with the requirement for incremental and compositional verification.The main research question concerning variability management of aPS is the management of multi-disciplinary variability in problem and solution space for FR and NFR with different levels of abstraction and granularity. A particular problem is here to ensure the consistency of the variability models at different levels of granularity within and across disciplines. Also the integration into engineering workflow, organization and development frameworks is a big issue as well as scalability. The application of feature models in aPS intent to automatically configure the aPS across the different disciplines on basis of these models. Therefore an integrated feature model would be beneficial.In order to support developers both in domain engineering where reusable artifacts are built and in application engineering where the actual variants are derived and customized, we further need visualization of changes in aPS between different variants and their versions.The directions for future work in MDE are manifold, i.e., to increase appropriateness of the modeling notation for software but also for the entire mechatronic system, the aPS, taking into account the qualification level of the maintenance staff, to support round trip engineering for many platforms and to understand obstacles using even domain specific notations integrated into well-established development frameworks. Regarding evolution support we still do not understand the real obstacles in reuse and modification from a human centered point of view.Finally, tracing is an important aspect to enable engineering during evolution to assess the impact of changes to various parts of the system. Here, approaches need to be developed which address the different disciplines that are involved in the development of aPS. Furthermore, automation support for trace management would be beneficial as aPS are typically very complex and as such the number of traces are very high.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
