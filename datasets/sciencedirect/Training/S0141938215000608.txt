@&#MAIN-TITLE@&#
A study on the possibility of implementing a real-time stereoscopic 3D rendering TV system

@&#HIGHLIGHTS@&#
We present a real-time stereoscopic 3D rendering pipeline to reduce visual fatigue when watching 3D TV.We implement the rendering pipeline in a system based on Linux and Windows.Each algorithm of the pipeline is programmed into the system.Computational complexity of each algorithm blocks are evaluated with sample 720p and 1080p stereo frames.It is found that the real-time stereoscopic 3D rendering is impossible with software implementation only.

@&#KEYPHRASES@&#
Stereoscopic 3D,Computational complexity,Real-time image processing,Hardware acceleration,

@&#ABSTRACT@&#
3D video has recently seen a massive increase in exposure in our lives. However, differences between the viewing and shooting conditions for a film lead to disparities between the reformed media and the original three-dimensional effect, which cause severe visual fatigue to viewers and result in headaches and dizziness. In this paper, a series of image processing algorithms are introduced to overcome these problems. The image processing pipeline is composed of four steps, eye-pupil detection, stereo correspondence computation, saliency map generation, and 3D warping. Each step is implemented in an S3DS-3D rendering system and its time complexity is measured. From the results, it was found that real-time stereoscopic 3D rendering is impossible using only a software implementation because SIFT and optical flow calculation requires a significant amount of time. Therefore, these two algorithm blocks should be implemented with hardware acceleration. Fortunately, active research is being conducted on these issues and real-time processing is expected to become available soon for applications beyond full-HD TV screens. In addition, it was found that saliency map generation and 3D warping blocks also need to be implemented in hardware for full-HD display although they do not have significant time complexity compared to SIFT and optical flow algorithm blocks.

@&#INTRODUCTION@&#
3-D media services for major events, such as the 2010 FIFA World Cup, are spreading stereoscopic 3D (S-3D) contents to homes in earnest [1]. A large number of high-profile films have been already produced in S-3D as well, and these movies are offered via 3D VoD (Video on Demand) services. In addition, recent advances in hardware led to a scramble to produce home stereoscopic television sets. Likewise, 3D video has recently seen a massive increase in exposure in our lives.A stereoscopic image is media reformed into a spatial image in the brain from two images transmitted to each eye with different phases. However, differences between viewer conditions, such as the viewer’s binocular disparity, watching position, and size of display panel, and the shooting conditions of a film lead to disparities between the reformed media and the original three-dimensional effect. The viewer suffers from severe visual fatigue, headache and dizziness as a result because the human 3D perception system is quite sensitive.In other words, viewers can only experience the 3D effect comfortably if they have the exactly same binocular disparity as the stereoscopic 3D camera and watch the film from the same shooting position. However, it is impossible to account for a multitude of watching or viewing conditions during filming. Films generated for a big screen in a theater cannot provide the appropriate 3-D effect to most VoD viewers with uniform parameters because each viewer’s conditions differ. Especially, the fact that animation contents for children are also made with parameters for adults leaves much to be desired. As a result, a real-time stereoscopic rendering system that can compensate for different viewing conditions is necessary to reduce visual fatigue. Men and women of all ages and races have different binocular disparities, their watching conditions are different, and lots of display devices have different sizes. However, the 3D display devices currently on the market do not provide the necessary rendering functions due to technical limitations.Fig. 1shows a change in disparity while approaching an object. The upper image shows a scene with natural disparity that is comfortable for the viewer. The bottom image depicts a situation that causes dizziness because of distortion from excessive disparity. In this paper, a series of image processing algorithms to overcome these problems are presented and their time complexity is estimated. In doing so, we study the possibility of implementing a real-time stereoscopic 3D rendering TV system to reduce visual fatigue. In Section 2, backgrounds and related works are introduced. In Section 3, we explain an algorithm pipeline for the proposed system, followed by an implementation of the system in Section 4. With analysis of the computational complexity of each algorithm block comprising the pipeline, the possibility of implementing a real-time rendering system is discussed in Section 5. Finally, we conclude this paper in Section 6.Stereoscopic 3D media is so complex that it involves a combination of various research and experience on human visual perception [3], display technologies [4], and industrial best practices [5]. Sometimes progress in one field results in the need for new research in related fields. However, the broadly accepted opinion is that disparity (or depth) is the central parameter in stereoscopy.Studies on perception have found that we have a limited zone of the disparity for comfortable viewing. This is demonstrated in Fig. 2. The painful area in the figure is the zone where viewers suffer from dizziness because objects are visible to only one eye. The dark colored area is an uncomfortable zone even though both eyes can see the objects because they are too close to or too far from the viewer. The admissible disparity range, the so-called comfort zone that permits proper stereo vision and depth perception, exists in the light gray area around the display panels in the figure [3].Disparities of objects in a stereo movie continuously change over time. A depth (or disparity) histogram storyboard tells us that it is very difficult to ensure that objects in a movie are kept within the comfort zone during filming for display panels of all sizes. In addition, viewing conditions also need to be considered. The comfort zone varies according to the viewer’s binocular disparity and eye positions as shown in Fig. 2. At the filming stage, it is impossible to produce images that account for all viewing conditions. Hence, real-time rendering technology should be developed to reflect these factors (i.e., both display conditions and viewing conditions) in real time and to adjust S-3D content immediately.Modifying stereoscopic content and disparity ranges to a comfort zone suitable for a large population has already been investigated. Mendiburu, Sun, and Holliman proposed a linear disparity remapping method [5,6] that changes the disparities of a footage from a given range to the desired range. Applications for linear disparity range mapping are the adaptation of stereoscopic content to display devices of different sizes, the uniform compression of scene depth for a more comfortable viewing experience, or moving scene content forward or backward by adding a constant offset to the disparity range. However, such adaptations have to be performed using expensive and cumbersome manual per-frame editing (i.e., not automatically).Manuel Lang et al. presented a set of disparity mapping operators that provide a basic formalization of perceptually motivated and production-oriented rules and guidelines for automatic nonlinear disparity editing of stereoscopic 3D content [7]. In order to implement these operators, they proposed a technique based on stereoscopic warping, which modifies input video streams in order to meet a desired disparity range. By applying the technique to some scenarios, they demonstrated that automatic image-based warping could be used as a general alternative for rendering even complex depth manipulations. However, these approaches do not consider real-time issues and viewing conditions, only focusing on automatic reproduction of a movie for a different sized display in VoD services. This paper studies the possibility of implementing a ‘real-time’ rendering system that incorporates the viewing condition parameters into the rendering process.This section describes an image processing pipeline required to implement a real-time S-3D rendering system for visual fatigue reduction, and explains each algorithm block in detail. The first step of this pipeline is eye-pupil detection to measure pupillary distance and identify the location of each pupil in three-dimensional space. Through this step, viewing conditions are identified. The second step is the automatic computation of sparse stereo correspondences selectively using the SIFT algorithm [8] and optical flow [9], depending on whether the target area belongs to dominant objects or the background in the image. These correspondences are used to create a disparity-based saliency map in the third step. The correspondences identified by the optical flow calculation are also used to warp (or perspective transform) the stereo videos in the fourth step. The saliency map generated in the third step is a weighted combination of the existing image-based saliency map and the disparity-based saliency map.The fourth step involves 3D warping to remap stereo videos reflecting the viewing condition parameters extracted in the first step and the display condition parameters (i.e., display size and depth information of objects) extracted in the third step.The third and fourth steps were originally proposed by [7] and are accepted widely as a prominent disparity control solution today. Fig. 3depicts each algorithm block and relationship between the algorithms in the processing pipeline.The first step of the proposed S-3D rendering pipeline was to detect the exact position of the viewer’s eyes. Two HD USB cameras (i.e., stereo cameras) were mounted on both ends of the edge of the S-3D flat panel TV, and the pupillary distance and view position were calculated by identifying the precise location of the pupils in three-dimensional space. The pupil detection process involved four sub-steps as shown in Fig. 4.The underlying pupil recognition algorithms were improvements of the circular Hough transform (or CHT) used to find a circular shape in the source video image [10,11]. However, it was difficult to distinguish eye pupils from other circle-like objects when the algorithm was directly applied to images taken with the camera modules. Thus, the circle detection algorithm was applied after a face was first detected using the facial recognition process and after identifying the eye zone within the facial area. This process significantly increased the recognition rate of both eyes. We first considered the widely known Viola-Jones algorithm for detecting the facial area [12], but the luminance of the facial image can be affected by the TV backlight if the viewer is in front of the screen. Therefore, in this paper, we used the modified census transform (MCT) algorithm [13], which is unaffected by changes in luminance.To find the eye area, we could have used training methods such as the Viola-Jones algorithm, but the detection rate of the eye zone is known to be considerably lower than that of the face zone. Thus, we inferred the eye area by applying a mask [14,15] as shown in Fig. 5to the facial images acquired with the face recognition algorithm, which resulted in a high recognition rate of over 90% in experimental environments for TV watching. The dark zones on the upper part of the figure represent eye areas. The result showed that the detection rate was almost the same as the facial recognition rate).The improved CHT algorithm [10] was applied to find a circular shape within the detected eye zone segment (see Fig. 6). In order to apply this algorithm, the contour information must first be obtained by performing edge detection. Among a variety of edge detection algorithms available, we used the canny edge detection algorithm. This algorithm is well known to have more points of excellence than others although its computational complexity is relatively high.A circle was drawn around every white pixel of the binary image that resulted from canny edge detection and the most overlapped point in the Hough space was determined as the center of the circle. After applying the CHT algorithm, seven circles were detected and it was difficult to determine which ones were real.After applying a mask, circle numbers 3, 4, 5, and 6 were the only circles remaining, making detection of the eye-pupil easier (see Fig. 7).The histogram characteristics of the eye area were used to detect a real eye-pupil among the circles. The eye has the characteristics of a histogram in the surrounding area as shown in Fig. 8. Thus, we reverse-binarized the image with a threshold value after identifying the histogram distribution on the right and left of each eye region.Afterward, it was possible to obtain a binarized image in which the dark part of the eye contained white pixels with a high density as shown in Fig. 9. Through the analysis of pixel distribution in the detected circle, the eye-pupils were finally determined. Our tests showed a detection rate of 99%. We also reduced the computational complexity to count the pixel position in the circle area by using a square instead of a circle for the above determination process.While performing calibration for the stereo camera using a check board, we first calculated the focal length (focal lengths) f, principal points, xland xr, and baseline distance T between the center of the two camera lenses. Triangulation is a standard method for measuring distance using two parallel cameras. In Fig. 10, an arbitrary point in space (X, Y, Z) is represented by the coordinates (XL, Y) and (XR, Y) on the left and right images respectively after it is photographed with a stereo camera while the Z-axis reflects the distance from the screen. Then, xl=(f/z)×x, yim=(f/z)×y, and xr=(−f/z)×(T−x), where x, y, and z represent real space coordinates and xl,xr, and yimrepresent coordinates in the image photographed. With the three equations above, the viewing distance z=T×f/(xl−xr) can be established.With the formula above, the distance Z on the z-axis can be calculated. We can get the value of x from xl, the xr, y value with calibration parameters and lastly, the distance from the principal point to the eye-pupils through the Pythagorean theorem. We can obtain the distance between the two eyes (i.e., disparity) and the viewing position by applying the process to each pupil.The pupil distances of five men were measured by a pupilometer. The watching distances were also measured using a laser distance meter as each person changed locations 20 times along the center line from a TV screen ranging from 60cm to 2m. The eye detection process was performed simultaneously using a stereo camera installed on top of the TV and the pupil and watching distances were estimated. The estimated pupil distances showed errors below 2.2% while the estimated watching distances showed errors less than 2.5%.In this step, the SIFT algorithm finds key point pairs (i.e., feature set) of dominating objects in the left and right image of each frame and optical flow vector for each macro block is calculated. In the next step of saliency map generation, the SIFT feature set is used to generate depth map for dominating objects of images and optical flow vectors are used to generate depth map for other areas of images, respectively.The SIFT [8] is a method of transforming image data into scale invariant feature points. Here, the features means edge elements, corners, line segments, curve segments, circles, ellipses, blobs, and polygonal regions.These feature (or key) points are robust to various changes of scale, rotation, illumination, and viewpoint. Feature points of SIFT detect the orientation of the dominant gradient at every location and records that according to its orientation to the histogram bin.Fig. 11shows the computation flow of the SIFT algorithm. The flow consists of two main steps: key-point detection and generation of descriptor. First, the input image is scanned to find the locations of key-points. Then, a feature is created to characterize each key-point. This feature consists of the histograms of gradients around the key-point. The key-point detection is composed of three sub-steps: scale-space image generation, local extrema detection, and key-point detection. The descriptor generation is composed of two sub-steps: orientation assignment and descriptor generation.Gaussian-blurred images are produced by Gaussian filters. The convolution (1) generates a Gaussian-blurred image, Li(x,y), from an input image I(x,y).(1)L(x,y,i)=G(x,y,σi)∗I(x,y),fori=0,1,…S+2G(x,y,σi)=12πσi2e-x2+y22σi2σi=(σ0·2iS)2-σin2,fori=0,1,…,S+2where σiis the scale of the Gaussian filter and i is the scale index. S is the number of scaled images and normally 3 is chosen for S. In the following sub-steps, the scaled images are used to find key-points. The Gaussian filter mask, G(x,y,σi) is made depending on σiwhile σ0 is the scale of the first Gaussian-blurred image. Once σ0 is a given, the scales of the other images are determined, where it is assumed that the original input image is Gaussian-blurred with σin.After the Gaussian Blurred image is generated, the Difference of Gaussians (or DoG) image, Di(x,y) is calculated by subtracting Lifrom Li+1 as described in (2). With (S+3) Li(x,y) images, (S+2) DoG images are produced.(2)Di(x,y)=L(x,y,i+1)-L(x,y,i),fori=0,1,…,S+1The (S+2) DoG images is called the first octave of the DoG images. The L0(x,y) for the second octave is derived from the S-th Gaussian-blurred image, LS(x,y), by down-sampling it. Then, Li(x,y), for i=0,1,…,S+2 is generated using (3). Note that the scale σiin (1) is replaced byσi′, in (3).(3)Li(x,y)=G(x,y,σi′)∗L0(x,y),fori=0,1,…S+2G(x,y,σi′)=12πσi′2e-x2+y22σi′2σi′=σ0·2iS2-1The next octaves can also be produced in the same manner as the second octave.Each DoG pixel at the location (x,y) of the i-th scale, Di(x,y) is compared with the eight DoG pixels around it, and also the DoG pixels in the 3-by-3 windows of the (i+1)-th and (i−1)-th scales. The Di(x,y) is marked as a key-point candidate if it has extreme value among the 27 DoG pixels.In order to select the final key-points from all key-point candidates derived in the previous sub-step, contrast check and eliminating edge responses are performed. For contrast check, Di(x,y) of every key-point candidate is thresholded with a predefined value. In eliminating edge response, inequality (4) is tested. Only the key-point candidate which satisfies it is finally chosen as the key-point. Here, r is a predefined threshold value.(4)Tr(H)2Det(H)<(r+1)2rH=ΔxxΔxyΔxyΔyyΔxx=Di(x+1,y)+Di(x-1,y)-2Di(x,y)Δyy=Di(x,y+1)+Di(x,y-1)-2Di(x,y)Δxy=(Di(x+1,y+1)-Di(x-1,y+1)-Di(x+1,y-1)+Di(x-1,y-1))/4Tr(H)=Δxx+ΔyyDet(H)=ΔxxΔyy-Δxy2For a window of size(2×Round(2×3σ×N+12)+1)by(2×Round(2×3σ×N+12)+1)around a keypoint chosen in the previous step, gradients are computed for all pixels in this area. Note that σ denotes the scale of a keypoint, N is normally chosen as 4, and Round() represents the rounding function. Gradient is computed in the horizontal and vertical directions as follows:(5)Δx=(Li(x+1,y)-Li(x-1,y))/2Δy=(Li(x,y+1)-Li(x,y-1))/2Then, the gradient magnitude, m(x, y) and the gradient orientation, θ(x, y), are obtained from (6).(6)m(x,y)=Δx2+Δy2,θ(x,y)=tan-1ΔyΔxFor the same window of size as above, a sub-region of size(2×Round(4.5σ)+1)by(2×Round(4.5σ)+1)centered at the same pixel as the window is defined and all locations within this sub-region are used to produce a gradient orientation histogram. The gradient orientation at each location within the sub-region is mapped to one of the 36 bins. Each bin covers 10 degree and total 36 bins cover 360 degree of orientations. The gradient magnitude at each location within the sub-region is weighted by a Gaussian window with a scale σ that is 1.5 times that of the scale of the key-point.The value of the histogram bin is calculated by summing all of the weighted magnitudes for the same gradient orientation. After the orientation histogram is built, the bin with the largest value is picked as the dominant orientation of the key-point. Along the derived dominant orientation, all gradients obtained from (6) are rotated. To this end, the gradient orientation, (x, y) obtained as in (6), is subtracted from the dominant orientation. The gradient magnitude is weighted by a Gaussian weighting function with its standard deviation, σwindow, which is equal to a half of the width of the descriptor window. Within the rotated gradient window, another sub-region, the descriptor window, is defined as the square of size (W-by-W) around a key-point where W is defined as follows:(7)W=2×Round2×3σ×N+12+1The descriptor window is partitioned into (N+1) by (N+1) sub-regions. There are N-by-N points at which the edges of four adjacent sub-regions cross. Each point of them has a gradient histogram with 8 orientation bins. The 8-bin histogram are used and a patch around the feature is split into separate 4-by-4 region each has its own orientation histogram, so the descriptor is a 128 dimensional vector (8×4×4).The SIFT key-point descriptors, which gives unique information about the surroundings, is very useful for accurate feature matching between right and left images. However, the time complexity of the SIFT is very demanding and thus this paper uses a smaller vector for SIFT descriptor, which consist of 2-by-2 window around every pixel and a 6 bin histogram to represent the local gradient for each pixel, so the descriptors dimension becomes down to 24 (=2×2×6), which makes the algorithm much faster and gives similar matching results with the original 128 dimensional descriptor. A restriction is applied in the number of the key-points to be extract in every frame by 200 (for 720p) and 400 (for 1080p) to reduce the computational time complexity further.Feature points matching between frames also needs to be done. By pruning, correspondence points are sorted based on lifetime in long video sequence, stable feature point sets are selected by filtering out points with low priority. SIFT feature point’s descriptors matching is done by Brute-Force Matcher. The aim is to make the matching results as good as possible. This method gives better result than the ration test proposed by Lowe [8].Fig. 12shows basic sparse feature correspondence set found by applying the modified SIFT algorithm to example stereo footages. From these correspondences’ disparities, depth map for the dominating objects is directly generated.Optical-flow is a well-known technique used to recover 2D motion from image sequences. It was originally proposed as a method to estimate the disparity map in stereo-pair images [16]. The optical flow method tries to calculate the displacements between two image frames. Let I(x,y,t) be an intensity function of the pixel position (x, y) at time t. Assume that the intensity value of that pixel is unchanged and only the position is changed by δx and δy for very short time δt. Actually, the image at time t means a left image and the other at time t+δt means a right image for stereoscopic images. Then, the following Eq. (8) come into existence.(8)I(x,y,t)=I(x+δx,y+δy,t+δt)Assuming the change to be small, the image constraint at I(x,y,t) with Taylor series can be developed to get (9).(9)I(x,y,t)+∂I∂xδx+∂I∂yδy+∂I∂tδt+H.O.T.=I(x+δx,y+δy,t+δt)From these equations, since H.O.T. is very small, it results in (10).(10)∂I∂xδx+∂I∂yδy+∂I∂tδt=0By dividing both sides of the equations by δt,(11)∂I∂xδxδt+∂I∂yδyδt+∂I∂tδtδt=0∂I∂xδxδt+∂I∂yδyδt+∂I∂t=0In (11),∂I∂x,∂I∂y, and∂I∂tare the derivatives of the image at (x,y,t) in the corresponding directions, andIx,Iy, and Itcan be written for the derivatives.δxδtandδyδtare the x and y components of the optical flow of I(x,y,t) and can be written as Vxand Vy, respectively.Since the number of variables is two, more than two equations are required to solve for the variables. However, only one equation is given for one pixel and thus equations for neighbor pixels are used to solve for the variables as follows (12).(12)Ix(q1)Vx+Iy(q1)Vy=-It(q1)Ix(q2)Vx+Iy(q2)Vy=-It(q2)…Ix(qn)Vx+Iy(qn)Vy=-It(qn)Here, q1,q2,…,qnare positions of the neighbor pixels including the center pixel. They are called pixel group and usually form a rectangular window. They are arranged in the matrix form (13) and then Vx,Vyare calculated by matrix operations as follows (14).(13)A=Ix(q1)Iy(q1)Ix(q2)Iy(q2)⋮⋮Ix(qn)Iy(qn),v=VxVy,b=-It(q1)-It(q2)⋮-It(qn)(14)Av=bATAv=ATbv=(ATA)-1ATbVxVy=∑iIx(qi)2∑iIx(qi)Iy(qi)∑iIx(qi)Iy(qi)∑iIy(qi)2-1-∑iIx(qi)It(qi)-∑iIy(qi)It(qi)A huge amount of arithmetic operations are required if the optical flow vectors are calculated at all the pixel positions. For the stereoscopic 3D rendering process proposed in this paper, the optical flow vectors are calculated only at regularly down-sampled points (normally by a unit of 8 or 16 pixels). Fig. 13shows optical flow vectors calculated at down-sampled points in example stereo images. These vectors are used to generate disparity map of the image region where feature correspondence points are not found through the previous SIFT operation.This step combines existing image-based saliency (such as edges and global texture saliency) with disparity-based saliency. The image-based saliency map is created using edge detection and the global scale method, while the disparity-based saliency map is formed using disparity information from the SIFT correspondence feature set and the optical flow vector found in Section 3.2. The unit of disparity is a pixel.The combined saliency map is completed using the two maps as follows.(15)S(x,y)=α·Si(x,y)+(1-α)·Sd(x,y),0<α<1Here, (x, y) is the coordinate of an arbitrary pixel in the left side images of the stereo footages, Sirepresents the image-based saliency and Sdrepresents the disparity-based saliency. The remapping function used in the next step of 3D warping is obtained by integrating the histogram graph of the saliency image S(x, y).This step solves for optical image warp. In other words, it performs a perspective transformation and enforces stereo deformations. When the remapping function is established based on the viewing condition parameters, display condition parameters, and saliency map, optical flow vectors are entered into this function as input values, with magnitude-controlled vectors as output values. The disparity is controlled by warping all macro-blocks along each direction of optical flow vectors at four vertices. By controlling disparity, image objects are placed in the comfort zone and visual fatigue is decreased. Fig. 14demonstrates the 3D warping process.A system was constructed as shown in Fig. 15to implement the proposed rendering pipeline. Two mini-PC boards were prepared and Linux and Windows operating systems were installed, respectively. A cable was connected to the HDMI port to output the video signal to a 3D TV. The boards were also connected to a FPGA board for hardware acceleration of algorithm blocks that are difficult to handle with software because of a large amount of computational time. With the programming of each algorithm block, the entire pipeline progressed from eye-pupil detection to 3D warping. Also, the program to control the board was written and the processing time for each algorithm block was measured.Fig. 16shows disparity control examples where stereo images were reconstructed by the system above. The original image (a) is too distorted but the adjusted image (b) is comfortable to view.The goal of this paper is to determine the possibility of implementing a real-time system through analysis of the computational time complexity for each algorithm block necessary to implement a 3D rendering system for reducing visual fatigue. In the next section, they are analyzed and discussed.In this section, the computational complexity for each algorithm block was evaluated with sample 720p and 1080p stereo frames. We measured the computational complexity on the Linux CentOS and Windows XP machines described in the previous section using test stereo footages of 720p and 1080p respectively. The specifications of the processor used on the Linux and Windows systems are as follows:<Linux machine>Processor: Intel i5 750 CPU @ 2.67 GHzInstalled memory (RAM): 8.00 GB<Windows machine>Processor: Intel i5- 3450S CPU @ 2.80 GHzInstalled memory (RAM): 4.00 GBTable 1summarizes the average time requirement for each algorithm block of the S-3D rendering pipeline on each machine. As shown in Table 1, while saliency map generation, remapping and 3D warping algorithm blocks have low processing time, SIFT and optical flow calculation blocks require a lot of time. Although camera initialization and eye-pupil detection require long processing times, they are not a problem since camera initialization is performed once at system start and eye-pupil detection is performed intermittently.It was found that the USB camera driver of the Windows machine requires a relatively long time for camera initialization, which is part of the process for obtaining a facial image from the USB camera module. In the SIFT block, the figure below shows how the computational time complexity is saved by reducing the SIFT descriptor vector‘s dimension and restricting the number of key-points to 200 (720p) and 400 (1080p) in every frame. In the study by Zhu, Qidan and Li [17] which employs a similar approach, the SIFT computational complexity is reduced by simplifying the descriptors, but their result still shows that the computational time per frame is not well optimized as they do not restrict the number of key-points extracted. In this paper, the computational complexity is further reduced by restricting the number of key-points as well as reducing the size of the descriptors. The average time per frame with this approach was 4.52 (720p) and 9.26 (1080p) seconds on average on the Linux machine, and 3.40 (720p) and 7.62 (1080p) seconds on average on the Windows machine. The data shows the values were reduced by about half. Table 2summarizes the time saved compared to measurements with the descriptor size of 128 and no restriction on the number of key points.The optical flow calculation also turns out to be a huge burden because of its demanding computation complexity. The implementation of a real-time system to handle more than 30 frames per second with only software proved impossible from these evaluation results. In particular, it was necessary to develop a hardware accelerator for optical flow, SIFT, and 3D warping. Fortunately, a dedicated hardware accelerator for those algorithms such as optical flow and SIFT was studied and developed recently.A few papers [18–21] implemented a real-time Lucas-Kanade optical flow hardware accelerator for VGA displays or smart devices (not full-HD). Also, the SIFT algorithm has been implemented in a hardware accelerator [22]. It can process the SIFT operation in real-time only up to VGA size (roughly 33ms per image frame) when the number of key points in the image frame is less than 890. However, it is likely that optical flow and SIFT hardware accelerators for display sizes equal to or larger than full-HD will be developed sooner or later.Thus, it seems that implementation of a real-time S-3D rendering processor or system will soon be feasible with the development of hardware accelerators for other algorithm blocks as well as for a combination of optical flow and SIFT.

@&#CONCLUSIONS@&#
