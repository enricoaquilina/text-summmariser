@&#MAIN-TITLE@&#
Molten steel temperature prediction model based on bootstrap Feature Subsets Ensemble Regression Trees

@&#HIGHLIGHTS@&#
Large-scale and noise data impose strong restrictions on building temperature models.To solve these two issues, the BFSE-RTs method is proposed in this paper.First, feature subsets are constructed based on multivariate fuzzy Taylor theorem.Second, smaller-scale and lower-dimensional bootstrap replications are used.Third, considering its simplicity, an RT is built on replications of each feature subset.

@&#KEYPHRASES@&#
Ladle furnace,Molten steel temperature prediction,Large-scale data and noise data,Ensemble method,

@&#ABSTRACT@&#
Molten steel temperature prediction is important in Ladle Furnace (LF). Most of the existing temperature models have been built on small-scale data. The accuracy and the generalization of these models cannot satisfy industrial production. Now, the large-scale data with more useful information are accumulated from the production process. However, the data are with noise. Large-scale and noise data impose strong restrictions on building a temperature model. To solve these two issues, the Bootstrap Feature Subsets Ensemble Regression Trees (BFSE-RTs) method is proposed in this paper. Firstly, low-dimensional feature subsets are constructed based on the multivariate fuzzy Taylor theorem, which saves more memory space in computers and indicates ``smaller-scale'' data sets are used. Secondly, to eliminate the noise, the bootstrap sampling approach of the independent identically distributed data is applied to the feature subsets. Bootstrap replications consist of smaller-scale and lower-dimensional samples. Thirdly, considering its simplicity, a Regression Tree (RT) is built on each bootstrap replication. Lastly, the BFSE-RTs method is used to establish a temperature model by analyzing the metallurgic process of LF. Experiments demonstrate that the BFSE-RTs outperforms other estimators, improves the accuracy and the generalization, and meets the requirements of the RMSE and the maximum error on the temperature prediction.

@&#INTRODUCTION@&#
Ladle furnace (LF) steel refining technology plays a substantial role in secondary metallurgic process. When LF is taken over at downstream secondary metallurgy units or at a continuous caster, the main purpose of LF refining processing is to get the qualified molten steel temperature and composition [1–5]. In practice, the molten steel temperature cannot be measured continuously, making it difficult to realize the precise controlling. Therefore, it is important to build a precise molten steel temperature prediction model.Several molten steel temperature models of LF based on thermodynamics and conservation of energy are developed in earlier works. [6] ``However, these models cannot be used efficiently for online accurate prediction because the parameters are hard to obtain. It is attributed to the harsh operating environment of ladle metallurgy, especially the high temperatures and corrosive slag associated with the process. Practically, some of the parameters are estimated according to experience. Consequently, it is hard to ensure the accuracy of mechanism models'' [1].To overcome the limitations of mechanism models, various data-driven modeling methods have been applied to establish the temperature prediction models. For example, Sun et al. [7] build a temperature model based on the Neural Networks (NN). Later, a PLS-SVM based temperature model is proposed by Wang [8], where input variables are firstly dealt with Partial Least Squares (PLS) algorithm to get rid of the linear dependency and then Support Vector Machine (SVM) is utilized to establish the prediction model. However, these temperature models only learn from a single model, and their performance are hard to improve when the object is complicated or the samples are with high noise [9].Compared with a single model, an ensemble model can further improve the accuracy and the generalization [10,11]. In the past decade, ensemble methods have been applied to establish temperature models. Tian & Mao [1] propose to establish an ensemble temperature prediction model based on a modified AdaBoost.RT algorithm. Lv et al. [9] propose a hybrid model in which the pruned Bagging model based on the negative correlation learning is used to predict the unknown parameters and the undefined function of the thermal model simultaneously.Today, the accuracy and the generalization of most existing molten steel temperature models cannot satisfy industrial production. They have been estimated on small-scale data. With the development of the information and the computer techniques, large-scale data are accumulated from the production process in LF. They contain more useful information, and make it possible to improve the accuracy and the generalization of the temperature prediction.However, large-scale data impose more restrictions on modeling and increase the costs of building models [12]. Furthermore, high complexity models increase the computational burden in the phase of actual use [13] and cannot be used efficiently for online accurate prediction. The existing ensemble temperature models are not suitable for large-scale data. In [1], the Extreme Learning Machine (ELM) [14] is selected as the base learner and the modified AdaBoost.RT is employed as the ensemble structure. Although the learning speed of the ELM is extremely fast, the AdaBoost.RT is a serial ensemble in which every new sub-model relies on previously built sub-models. In [9], the pruning process of sub-models is also one by one, and Bagging is changed into a serial ensemble. A serial ensemble is often more complex than a single model [15], especially on large-scale data. Additionally, data sampled from the production process are with noise, which reduces the accuracy and the generalization of the temperature prediction.To deal with the large-scale and the noise issues, the Bootstrap Feature Subsets Ensemble Regression Trees (BFSE-RTs) method is proposed in this paper. Firstly, low-dimensional feature subsets are constructed based on the multivariate fuzzy Taylor theorem [16], which saves more memory space in computers and indicates ``smaller-scale'' data sets are used. Secondly, to eliminate the noise data, the bootstrap sampling approach [17] of independent identically distributed data is introduced into the feature subsets. Bootstrap replications consist of smaller-scale and lower-dimensional samples. Thirdly, considering its simplicity the Regression Tree (RT) [18] is employed as the base learner, and a RT is built on each bootstrap replication. The BFSE-RTs method is expected to successfully utilize the large-scale data accumulated from the production process in LF, to improve the accuracy and the generalization of the temperature prediction, and to meet the requirements that the Root Mean Square Errors (RMSE) of the temperature is less than 3°C and the maximum error of it is less than 10°C.The remainder of this paper is organized as follows. Section 2 introduces the mechanism of the production process of LF. In Section 3, existing ensemble methods are reviewed. In Section 4, the BFSE-RTs method is proposed, and the differences from other ensembles are introduced. In Section 5, the experimental investigations are brought out, and the BFSE-RTs temperature model is compared with the FSE-RTs, the RF, the stacking trees, the modified AdaBoost.RT, and the pruned Bagging temperature models. In Section 6, the conclusion of this paper is summarized.To establish the data-driven based model of the temperature in LF, the energy change during the refining process in Fig. 1 is considered [1,6,14].The refining process starts from the entry of LF and ends at the exit of LF. The data are sampled in the entire refining process that is subdivided into many different sampling periods. In any sampling period, the same steps are executed, and they are the argon bowing and slag adding, the power supply, and the power off.In any sampling period,(i)The initial temperature is measured before the argon bowing and slag adding.The power supply of the refining process in LF is from the electric arc.The power off mainly contains three parts: ① The heat loss from the ladle refractory wall and the top surface. In this part, the heat loss is relatively stable, increases as the time goes on, and may be reflected by refining time. ② The heat change from additions, i.e. the chemical reaction heat and the heat exchanges. ③ The heat loss by the argon purging.The end temperature is measured after the power turned off.In the refining process of LF, there are 10 main factors determining the molten steel temperature. They are the heat effects of additions, the time interval of temperature measure, the volume of argon purging, the refining power consumption, the initial temperature, the refining time, the temperature of the empty ladle, the ladle states, the number of the ladle, and the weight of molten steel. And more factors are considered in this study than those considered in paper [1].There are two main issues of building the temperature model in LF. Firstly, with the development of the information and the computer techniques, large-scale data are accumulated from the production process. They increase the costs of building temperature models. And high complexity models increase the computational burden in the phase of actual use and cannot be used efficiently for online accurate prediction. Secondly, the data are with noise, which reduces the accuracy and the generalization of the temperature prediction. The large-scale and noise data impose more restrictions on building a temperature prediction model.Ensembles of learnt models constitute one of the main directions in machine learning and data mining [19]. An aggregation of many simple but different predictors can reduce the complexity and achieve better performance which cannot be achieved by a single model [11,20]. Thus, we focus on building an ensemble temperature prediction model.A training set L consists of data{(Xk,Yk)}kN,whereX={x1,...,xn}∈Rnis n-dimensional input, Y ∈ Ris the 1-dimensional output, and N is the size of L (i.e. the number of samples in L). LetLi,i=1,..,Pdenote P training subsets obtained from L. Consider a regression ensemble modelfE(X)that consists of P sub-models takes the form:(1)fE(X)=∑i=1Pβifi(X),whereβi,i=1,...,Pis the weight of the ith sub-modelfi(X)that is trained on the ith training subset Li.In general, there are 3 steps to build an ensemble model:Firstly, generate P different training subsets, such as sample subsets in Bagging [21] and the Random Forests (RF) [11], feature subsets in the Random Subspace (RS) [20] and the Genetic Ensemble Feature Selection (GEFS) [22], and re-weight subsets in Boosting [23]. In a sample subset, the number of samples is reduced, but the dimension of input features is the same as that of the training set. In a feature subset, the dimension of input features is reduced, but the number of samples is the same as that of the training set. In a re-weight subset, both the number of samples and the dimension of input features are the same as those of the training set, but the weights assigned to samples are changed. There are also many famous ensemble methods that do not construct training subsets, but build different sub-models directly on the training set, such as Neural Network Ensembles (NNE) [10], Stacking (i.e. Stacked Generalization) [24], and stacked regressions [25].Secondly, build sub-models. In ensembles, learning algorithms of sub-models also called base learners. Based on base learners, ensembles could be divided into two primary kinds: homogeneous and heterogeneous [26]. In a homogeneous ensemble, all sub-models are with the same base learner; on the contrary, in a heterogeneous ensemble, sub-models are with different base learners. Most of existing ensemble methods are homogeneous ensembles. The most popular heterogeneous ensemble is Stacking [24].Lastly, aggregate outputs of sub-models, i.e., obtain weights of sub-models. There are two primary kinds of aggregating approaches: the constant and the non-constant [27]. Non-constant aggregating approaches weigh each sub-model as a function of the sample being predicted, such as the level-1 model in Stacking. In constant aggregating approaches, weights of sub-models do not change with the sample being predicted, such as the basic ensemble method (i.e. the simple average) and the generalized ensemble method, which are defined using the terminology of Perrone & Cooper [28].Among these ensemble methods, the RF is one of the most popular regression ensemble methods suitable for the large-scale and noise data. Unfortunately, the temperature model estimated with the RF cannot meet the strict requirements of the production process. As more input features (factors) considered, the RF cannot well describe the nonlinear characteristics of the temperature in LF. In the RF, a tree is grown on a sample subset using the random selection of features at each node to determine the split. The sample subsets based RF cannot resolve the dimensional problem effectively in temperature prediction.To handle the large-scale data, based on the multivariate fuzzy Taylor theorem, the Feature Subsets Ensemble (FSE) method proposed and presented in our paper [12] is selected. In the FSE method, low-dimensional feature subsets are constructed, which saves more memory space in computers and indicates ``smaller-scale'' data sets are used.Considering its simplicity, the RT [18] is employed as the base learner of the FSE model. In Fig. 2the structure of an FSE-RTs model is shown. The generalized ensemble method [28] is the aggregation approach used and the detailed calculation process is showed in [29], from Page 194-195.To deal with the noise data, the bootstrap sampling approach [17] of independent identically distributed data is introduced into the FSE method. Each bootstrap replication consists of less than N samples, drawn at random, with replacement. Two modes can be considered to combine the bootstrap sampling approach with the FSE method.(i)	Two combination modes of the bootstrap and the FSEIn the combination mode ①, the bootstrap sampling approach is applied to the training set, and an FSE model is built on the bootstrap replication with full-dimensional input features. In the combination mode ②, the bootstrap sampling approach is applied to each feature subset of the FSE, and sub-models are built on bootstrap replications with low-dimensional input features. In Figs. 3and 4, the diagrams of the two combination modes are showed, respectively.Here, the combination mode ② is preferred. The main disadvantage of the combination mode ① is that, if a sample is not selected in the bootstrap replication, both the dirty input features and the clear ones in the sample are deleted. It indicates that the information on the full-dimensional input features is deleted, and more useful information losses. The combination mode ② can well avoid this disadvantage of the combination mode ①.In the combination mode ②, each bootstrap replication of the ith feature subset consists ofNiB,(NiB<N),i=1,...,Psamples, drawn at random, with replacement, andθiS=(NiB/N)×100%denotes the re-sampling rate in the ith feature subset.On any feature subset, more than one bootstrap replication can be considered based on the modeling problem at hand. Ji, i=1,...,P denotes the number of the bootstrap replications generated from the ith feature subset. The numbers of the bootstrap replications on different feature subsets can be the same or different.(ii)	Two ways of sub-models outputs aggregationOn each low-dimensional bootstrap replication, an RT is built. Then,(∑i=1PJi)×(∑h=1h*Cnh)RTs are needed for aggregation. The value of(∑i=1PJi)×(∑h=1h*Cnh)is easily more than hundreds, with more bootstrap replications considered. Fig. 5 shows that all RTs are aggregated at one level (level-1). Sometimes it is impossible in practice.Alternatively, two aggregation levels are used as showed in Fig. 6. The JiRTs from the ith feature subset are built one after another on a computer and aggregated at level-1. Then, the∑h=1h*Cnhoutputs of level-1 are further aggregated at level-2. In this way, the JiRTs from the ith feature subset are viewed as a sub-model of the BFSE-RTs model, totally∑h=1h*Cnhsub-models. The executing time is recorded by sub-model, not by RT. The aggregation way is selected based on the modeling problem at the hands.(i)As a homogeneous ensemble, the BFSE-RTs is different from Stacking [24] which is a heterogeneous ensemble.Stacking is proposed by Wolpert [24] to solve classification problems, its idea is as follows: A set of sub-models are constructed firstly. The sub-models can be of the same base learner but differ in complexity, and also can be derived using different base learners. Then, a set of models are constructed from K-fold cross-validation or leave-one-out of the training set, and their outputs are used as the training set to a “meta”-model. The purpose of the “meta”-model is to aggregate the predictions of the sub-models so as to correctly classify the target [30].As a parallel ensemble, the BFSE-RTs is different from AdaBoost [23] and the GEFS [22] which are serial ensembles.Based on structures, ensembles can be divided into two primary kinds: serial and parallel [15]. A parallel ensemble independently constructs both training subsets and diverse sub-models. Using the same base learner, the execution time to generate a parallel ensemble is much less than that to generate a serial ensemble. Bagging, the RF, the RS, NNE [10], Stacking, stacked regressions [25] and the BFSE-RTs we proposed are exemplars for parallel ensembles.In a serial ensemble, every new sub-model relies on previously built sub-models so that the weighted aggregation forms an accurate model. It is targeted to reduce both bias and variance. A serial ensemble is often more complex than a single model [15]. Boosting and the GEFS are exemplars for serial ensembles.AdaBoost [23] is the most well known and successful in the Boosting family, though there are many variants specialized for particular tasks, such as cost-sensitive and noise-tolerant versions. The probability of each sample is initially set as 1/N, where N is the number of samples. These probabilities change after the first sub-model (classifier) built.ζi,i=1,...,Pdenotes the sum of the misclassified sample probabilities of the currently trained sub-model Zi. The probabilities of the next test are generated from multiplying the probabilities of Zi's incorrectly classified samples by the factor(1−ζi)/ζi, then these probabilities are re-normalized. All the sub-models are aggregated by the weighted voting method.Opitz [22] present a Genetic Algorithm (GA) approach for searching for an appropriate set of feature subsets for ensembles, called the GEFS method. The GEFS firstly creates an initial population of sub-models (classifiers), and then each sub-model is built by randomly selecting a different subset of features. On the feature subsets, new candidate sub-model is developed continually by using the genetic operators of crossover and mutation. In the GEFS, NNs are used as the based learner, and they train the neural networks using standard back-propagation learning.As a feature subsets based ensemble method, the BFSE-RTs is different from Bagging and the RF which are sample subsets based ensembles, and also different from NNE [10], Stacking [24] and stacked regressions [25] which build sub-models directly on the training set.Bagging [21] bases on the bootstrap sampling approach, in which each sub-model is trained on a sample subset. Each new sample subset is drawn, with replacement, from the original training set. The sub-models are aggregated by a uniform average for regression or vote for classification. Bagging works best with unstable learners which produce different generalization patterns with small changes to the training data.The RF [11] is a revised version of Bagging [31], which employs the RT as the base learner. In the RF, a tree is grown on a new bootstrap replication (sample subset). To determine the split at each node, it uses the randomly selected features. On the contrary, in the BFSE-RTs, the construction process of feature subsets is independent from the generation process of sub-models. Thus, instead of the RT, a base learner can be chosen depending on the problem at hand.The NNE is proposed by Hansen & Salamon [10]. In their paper, two modes for aggregating NNs are proposed to improve the performance of a single NN for classification. In mode one, the same training set for all NN sub-models is used, and in mode two, independent training sets are used for different NN sub-models. They demonstrate that, while individual performance remains unaffected, mode two gives markedly better results than mode one.Stacking regressions [25] originate with Stacking [24] which is to improve the generalization of models for classification problems. Then, Breiman [25] expands it for regression problems. The stacking regressions method forms linear aggregations of different sub-models. In the aggregation, it uses cross-validation data and least squares under non-negativity constraints to determine the coefficients. The effectiveness of it is proved in stacking regression trees, stacking subset regression, and stacking ridge regression.The GEFS and the RS are two famous ensemble methods based on features subsets. However, there are many differences between the BFSE-RTs and the two ensembles.Firstly, the idea of the construction of feature subsets in the BFSE-RTs is fundamentally different from that in the GSEM and the RS. In the BFSE-RTs, feature subsets are constructed based on the multivariate fuzzy formula. Instead of GA considered by Opitz [22], the BFSE-RTs directly, exhaustively, independently subdivides the original feature space and generates feature subsets rapidly and simply. In BFSE-RTs, the bootstrap sampling approach is introduced in feature subsets to delete noise. In the RS, the bootstrap is introduced in the training set to generate feature subsets.Secondly, the BFSE-RTs uses the all input features, and each input feature appears at least once, which is contrary to both the GSEM and the RS. Opitz mentions [21] that in the GSEM some features may be picked multiple times while other may not be picked at all. In the RS, the random “re-selecting” process of features cannot insure that all input features in the original feature set are used, that is, each input feature may appear repeatedly or not at all in any feature subset.As a regression ensemble, the BFSE-RTs is different from the ECOC method which is classification ensembles.The idea of Error Correcting Output Codes (ECOC) [32] is quite different from the ensemble methods mentioned above. ``The ECOC is applied to a problem with multiple classes, decomposing it into several binary problems. Each class is first encoded as a binary string of length T, assuming we have T models in the ensemble. Each model then tries to separate a subset of the original classes from all the others. For example, one model might learn to distinguish “class A” from “not class A.” After the predictions, with T models we have a binary string of length T. The class encoding that is closest to this binary string (using Hamming distance) is the final decision of the ensemble'' [33]. The temperature prediction is a regression problem, thus the ECOC method is not suitable for it.Semantic drift is an inherent property when the bootstrap method is used for classification problems. It often occurs when ambiguous or erroneous terms and/or patterns are introduced into and then dominate the iterative process [34–36]. Interestingly, semantic drift does not puzzle the BFSE-RTs method which is applied for a regression problem. Due to the continuous output variable, generally the performance of a regression task is measured by the bias between predictions and true values. The popular performance indexes include the mean absolute error(∑i=1N|Yi−Y˜i|)/N,the mean squared error(∑i=1N(Yi−Y˜)2)/N,and the RMSE(∑i=1N(Yi−Y˜)2)/N, where Yidenotes the true value andY^idenotes the prediction. For a regression model, biases between predictions and true values may be high, but ambiguous or erroneous terms are not introduced into it.The numberof the input features is 10, and the 1714 data of production from 300t LF are used. Ninety-five percent of the data are randomly selected without replacement from the original data set to train the model, and the others are used to test. The data are normalized in the range [–1,1].For all estimators, we repeat this process 20 times and present the statistical results (say average and standard deviation) of obtained performance. Any estimator is constructed using 10-fold cross-validation. All programs are compiled by Matlab and run on the computer with Pentium(R) Dual-Core CPU, 2.00GB EMS memory, Microsoft Windows XP Professional System.To demonstrate the potential and necessity of the BFSE-RTs for the temperature prediction in LF on large-scale and noise data, we compare the accuracy, the generalization, the training and the testing time of the following temperature models:(i)The BFSE-RTs versus the FSE-RTs.The BFSE-RTs is a version of the FSE-RTs, but more suitable for noise data. Thus, we compare the BFSE-RTs with the FSE-RTs for the temperature prediction in LF, and demonstrate this conclusion by experiments.The BFSE-RTs versus the RF and the stacking trees. The RF and the stacking trees are two popular regression ensemble methods on large-scale data, and possess strong robustness on noise. Thus, they are selected as comparers.The BFSE-RTs versus the modified AdaBoost.RT [1] and the pruned Bagging [9]. The revisions of Boosting and Bagging have been used to predict the temperature in LF, and they are the modified AdaBoost.RT based ensemble ELM proposed by Tian et. al. in [1] and the pruned Bagging based on the negative correlation learning proposed by Lv et. al. in [9]. These two ensembles are selected as comparers of the BFSE-RTs for temperature prediction on large-scale and noise data.In all runs the following procedures are used:(i)The RMSE and the maximum error are employed as the index to quantify the accuracy on training set and the generalization on testing set.For any RT, the rule “don't split if the leaf node size is < θ” is enforced. It is useful to setθ=5for all RTs (except for the stacking trees) to predict the temperature. Methods to prune back the trees are not considered.In fact, only one computer is used and the process of building sub-models is serial running in an ensemble. In order to demonstrate the time reduction in comparison to a single model, for parallel ensembles, a total of the training/testing time is measured, and then, the total time is divided by the number of sub-models to get the average training/testing time per sub-model.In this experiment, for simplicity, the re-sampling rate in all of the feature subsets is set as the same value, i.e.θiS=θS,i=1,...,P, and the same number of the bootstrap replications generates from each feature subset, too, i.e. Ji=J. RTs sub-models are aggregated at two levels as showed in Fig. 6.(i) Setting the parameter h*=3For an unknown nonlinear function, although by now there have been no useful methods to determine the value of h*, instead of using heuristics, the feature space is subdivided based on a user-specified dimension of input features. To predict the temperature, it is useful to set h*as 3 for the BFSE-RTs model.Table 1 shows the performance of the BFSE-RTs with different values of h*for the temperature prediction. On each feature subset, the re-sampling rate θSis equal to 80% and the number of the bootstrap replications J is equal to 2. It demonstrates that the performance is sensitive to the value of h*. As the value of h*increases from 1 to 3, the performance of the BFSE-RTs estimator is improved dramatically. The performance of the BFSE-RTs estimator with h*=3 meets the requirements of the RMSE and the maximum error of the temperature, on both the training and the testing sets.In Table 1, the results of the BFSE-RTs with h*≥ 4 are not given. Because, when h*≥ 4 the number of sub-models is more than∑h=1h*=4Cnh=385which is too large. Additionally, the performance of the BFSE-RTs estimator with h*=3 is well enough to meet the requirements of the RMSE and the maximum error of the temperature.(ii) Setting the parametersθS=80%and J=2Fig. 7 shows the performance of the BFSE-RTs for the temperature prediction with different re-sampling rates and numbers of the bootstrap replications with h*=3.On the training set, with the same re-sampling rate, as the value of J increases, the RMSE and the max error reduce; and with the value of J, as the re-sampling rate increases, the RMSE and the max error reduce.On the testing set, the RMSE and the max error do not always reduce as the value of J or the re-sampling rate increases. When the number of the bootstrap replications J=2 and the re-sampling rateθS=80%,the BFSE-RTs obtains the best generalization performance.The performance of the BFSE-RTs estimator with h*=3,θS=80%, J=2 meets the requirements of the RMSE and the maximum error of the temperature on both the training and the testing sets. Thus, in this paper, the BFSE-RTs estimator with h*=3,θS=80%, J=2 is used for the temperature prediction, and called the BFSE-RTs estimator for short in the following.The curve of the temperature estimated with the BFSE-RTs on the testing set is showed in Fig. 8. Here, the real temperature means the measure temperature in the practical production process. Based on the BFSE-RTs estimator, the curve of the real temperature is fitted well by the predictive curve. As mentioned above, we separate the whole data set into disjoint training and testing sets randomly, and repeat this process 20 times. Due to limited space, in Fig. 8, only the temperature curve of the last separation is showed.Table 2 shows the performance of the FSE-RTs with different values of h* for the temperature prediction. The same as the BFSE-RTs, the accuracy and the generalization of the FSE-RTs estimator with h*=3 is the best. Due to two replications used in each feature subset (or sub-model), the training time cost by the BFSE-RTs is longer than that cost by the FSE-RTs, but the testing time cost by the BFSE-RTs is a litter longer (less than 3ms) than that cost by the FSE-RTs. The maximum errors on both the training and the testing sets are reduced by the BFSE-RTs. On the large-scale and noise data, the BFSE-RTs outperforms the FSE-RTs in the accuracy and the generalization of the temperature prediction.It is reasonable to compare the performance of the BFSE-RTs and the RF estimators on the same benchmark of installations. That is, in the RF estimator, we also build 175 sub-models. Breiman [11] demonstrates that the RF is insensitive to the number of features selected to split each node. Usually, selecting 1 or 2 features gives near optimum results. Thus 2 features are selected in this experiment.The comparison of the performance of the BFSE-RTs and the RF estimators with different re-sampling rates for the temperature prediction is presented in Fig. 9. On both the training and the testing sets, the RMSE and the maximum error of the temperature estimated with the BFSE-RTs are less than those of the temperature estimated with the RF. The RF cannot meet the requirements of the RMSE and the maximum error.The detailed results of the RF temperature model with P=175,θS=80%are presented in Table 3. Due to two replications used in each sub-models, the training time cost by the BFSE-RTs is longer than that cost by the RF in each sub-model of which only one replication is used, but the testing time cost by the BFSE-RTs is a litter longer (less than 2ms) than that cost by the RF. On both the training and the testing sets, the RMSE and the maximum error of the BFSE-RTs temperature model are less than those of the RF temperature model.Considered the RT as the base learner, the stacking regressions is applied to stacking trees of different sizes. Similar to the above, 175 sub-models are built in the stacking trees temperature model. Different θ values are used to construct different sizes of the trees. Wolpert uses the leave-one-out cross-validation to generate the level-1 data, which is very computer intensive. As Breiman mentions in [25], the level-1 data generated by much cheaper 10-fold cross-validation is more effective than the leave-one-out level-1 data. Thus, the 10-fold cross-validation is considered in this experiment.Fig. 10shows the performance of the stacking trees with different θ values in the “meta”-model for the temperature prediction. As θ increases, the performance of the stacking trees is not always improved on both the training and the testing sets. Whenθ=130, the stacking trees model obtains the optimal generalization performance, and the detailed results of it are presented in Table 3. In the stacking trees, 2 levels of models are built. The training (or testing) time cost by stacking trees is the sum of the training (or testing) time of level-0 and level-1. Sub-models with parallel structure are built on level-0, and the training (or testing) time of each sub-model is measured as that of level-0. The “meta”-model is built on level-1, and the training (or testing) time of the “meta”-model is measured as that of level-1.On both the training and the testing sets, the time cost by the BFSE-RTs is less than that cost by the stacking trees; the RMSE and the maximum error of the BFSE-RTs are less than those of the stacking trees; the maximum error of the stacking trees is more than 10°C. On the training set, the RMSE of the stacking trees is less than 3°C; but on the testing set it is more than 3°C.On the large-scale and noise data set, the BFSE-RTs temperature model outperforms the stacking trees temperature model in the forecasting speed, the accuracy and the generalization. The stacking trees cannot meet the requirements of the RMSE and the maximum error of the temperature prediction.In the modified AdaBoost.RT temperature model, the structure, the parameters of ELM, and the training error are the same as [1]. The number of nodes in the hidden layer is 20. In the pruned Bagging temperature model, the re-sampling rate is set as 80%. Fig. 11shows the performance of the modified AdaBoost.RT and the pruned Bagging with different numbers of sub-models for the temperature prediction. On both the training and the testing sets, as the number of sub-models increases, the performance of the modified AdaBoost.RT and the pruned Bagging are improved. However, within 500 sub-models, the modified AdaBoost.RT temperature model cannot meet the requirements of the RMSE and the maximum error; when the pruned Bagging with more than 400 sub-models, the RMSE on both the training and the testing set, and the maximum error on the training set can meet the requirements, but the testing maximum error are always more than 10°C.We also compare the performance of the BFSE-RTs, the modified AdaBoost.RT, and the pruned Bagging with the same number of sub-models. The detailed results of the modified AdaBoost.RT and the pruned Bagging temperature models with 175 sub-models are presented in Table 3. Due to the serial ensemble structure, the training time of the modified AdaBoost.RT and the pruned Bagging are the executing time of all sub-models. To predict the output of a new sample, the two temperature models can be with the parallel structure, thus the testing time of a sub-model is viewed as that of each temperature model.On both the training and the testing sets, the time cost by the BFSE-RTs is less than that cost by the modified AdaBoost.RT and the pruned Bagging; the RMSE and the maximum error of the BFSE-RTs are less than those of the modified AdaBoost.RT and the pruned Bagging; the RMSE of the modified AdaBoost.RT and the pruned Bagging are more than 3°C; the maximum errors of the them are more than 10°C.On the large-scale and noise data set, the BFSE-RTs temperature model outperforms the modified AdaBoost.RT and the pruned Bagging temperature models in the forecasting speed, the accuracy, and the generalization.

@&#CONCLUSIONS@&#
The main contribution of this paper is to significantly improve the accuracy and the generalization of the temperature prediction in LF by the proposed BFSE-RTs method on large-scale and noise data accumulated from the production process.On both the training and the testing sets, the RMSE and the maximum error of the BFSE-RTs temperature model are less than those of the FSE-RTs, the RF, the stacking trees, the modified AdaBoost.RT and the pruned Bagging temperature models. The BFSE-RTs temperature model meet the requirements of the temperature prediction: the RMSE of the temperature is less than 2.4°C and the maximum error of the temperature is less than 6.7°C.The BFSE-RTs temperature model is with the low complexity compared to the FSE-RTs and the RF temperature models. The testing time cost by the BFSE-RTs temperature model is quite similar to that of the FSE-RTs and the RF temperature models. The complexity of the BFSE-RTs temperature model is lower than that of the stacking trees, the modified AdaBoost.RT, and the pruned Bagging temperature models. The training time cost by the BFSE-RTs is about 576.7 times less than that by the stacking trees, about 17.5 times less than that by the modified AdaBoost.RT, and about 3617 times less than that by the pruned Bagging. The testing time cost by the BFSE-RTs is about 9 times less than that by the stacking trees, about 2.6 times less than that by the modified AdaBoost.RT, and about 1.7 times less than that by the pruned Bagging.To summarize, on the large-scale and noise data set, the BFSE-RTs estimator is more suitable for the temperature prediction in LF than the other 5 estimators. It improves the accuracy and the generalization, and meets the requirements of the RMSE and the maximum error of the temperature prediction.