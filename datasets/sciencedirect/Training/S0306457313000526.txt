@&#MAIN-TITLE@&#
Dealing with metadata quality: The legacy of digital library efforts

@&#HIGHLIGHTS@&#
A characterization of metadata quality.A discussion on the role of metadata quality in data infrastructures.A survey of metadata quality frameworks to assess metadata quality.An array of solutions to mitigate the effects of metadata quality issues.Directions for future works related to metadata quality in data infrastructures.

@&#KEYPHRASES@&#
Quality,Digital Libraries,Metadata quality frameworks,Data infrastructures,

@&#ABSTRACT@&#
In this work, we elaborate on the meaning of metadata quality by surveying efforts and experiences matured in the digital library domain. In particular, an overview of the frameworks developed to characterize such a multi-faceted concept is presented. Moreover, the most common quality-related problems affecting metadata both during the creation and the aggregation phase are discussed together with the approaches, technologies and tools developed to mitigate them. This survey on digital library developments is expected to contribute to the ongoing discussion on data and metadata quality occurring in the emerging yet more general framework of data infrastructures.

@&#INTRODUCTION@&#
Data and metadata represent a key element in our knowledge-based society. In the light of the critical role they play in domains including business, government and science (Nature, 2008; Hanson, Sugden, & Alberts, 2011; Hey et al., 2009; Borgman, 2010, 2011), dealing with their quality is fundamental. Being conscious of data and metadata quality aspects is a primary need in environments supporting and promoting sharing and reuse of data and metadata like modern data infrastructures do (Thanos, 2012; Ashley et al., 2012; Boulton et al., 2012). In particular, metadata – being data that give information about other data – cover a fundamental function in enabling any form of data management, and their “quality” deeply influences the overall quality of the services offered by relying on the data they characterize.Despite that the relevance and impact of metadata quality is universally recognized in the literature, there is no agreement yet on what metadata quality is. This lack has several implications, including the impossibility of introducing systematic approaches to its automatic measurement and enhancement. Similarly to data quality (Madnick, Wang, Lee, & Zhu, 2009), metadata quality is a complex concept that can intuitively be defined as “fitness for use” (Wang & Strong, 1996; Eppler, 2006). Very often (Strong, Lee, & Wang, 1997; Batini & Scannapieco, 2006), (a) its understanding and assessment change from one community of practice to another, (b) its notion depends on the actual use of the data, and (c) an actual characterization can only be built by taking into account its multiple facets, and, therefore, by defining it in terms of a number of specific quality dimensions.Digital Libraries (Candela, Castelli, & Pagano, 2011, chap. 1) have been conceived since the beginning as tools aiming at supporting and revolutionizing the practices through which citizens have access to human knowledge and produce new artefacts (Ioannidis, 2005). The typology of data they offer is not limited to texts, images and music only. Rather, a Digital Library is nowadays called to make available the rich array of data that is needed by the community of practice it is serving. Very often such data are borrowed from other Digital Libraries or repositories thus data are expected to be (re-)used in domains different from their initial one. All this is achieved by heavily relying on metadata. Digital Libraries have faced a plethora of metadata quality issues and have developed solutions aiming at mitigating the effects of such issues.The paper surveys how metadata quality issues have been addressed until now in the digital library domain. Such a survey investigates two diverse yet complementary elements: (i) the quality frameworks introduced to characterize “metadata quality” as to lay its foundations and promote a systematic approach to methods for the automatic evaluation and improvement of metadata quality; (ii) the approaches presented in the literature to actually deal with metadata quality issues, both to evaluate and to improve metadata quality. Through the analysis of the work done and of the lessons learned in the addressed context, we expect to contribute to solution of similar issues faced in other contexts such as the emerging yet more general framework of data infrastructures. This contribution range from ready to use solutions and approaches to typologies of strategies and methodologies to be eventually adapted and exploited in context different from the Digital Library one.The rest of this paper is organized as follows. Section 2 introduces the concept of “metadata quality”. Section 3 presents a number of quality frameworks that have been proposed to identify an effective way to define and measure metadata quality. Section 4 reviews metadata quality problems analyzed in the recent literature in the field of Digital Libraries and digital repositories, and also describes proposed possible solutions for specific quality problems, namely strategies for quality assurance in the metadata creation phase, quality evaluation, and cleaning. Section 5 concludes by highlighting research directions for data and metadata quality issue in data infrastructures.Metadata is a key element in the digital library domain. Actually, such a kind of data has characterized this domain since the beginning and for a long time it has been – in some cases this is still the case – the sole data digital library and repository systems have been requested to manage since they act as placeholders for real resources. Because of this core role, metadata quality is a characteristic that is directly associated with the digital library value and effectiveness, e.g., if metadata quality is poor so is the discovery of digital library information objects.However, defining “what metadata quality is” is a very challenging task. It can be affirmed that no consensus has been reached on this concept until now, apart from the shared understanding that the difficulties in defining it come from its intrinsic characteristic of being a multidimensional and context specific concept. Bruce and Hillmann (2004, chap. 15) stated that “Like pornography, metadata quality is difficult to define. We know it when we see it, but conveying the full bundle of assumptions and experience that allow us to identify it is a different matter”. In the rest of this section a brief survey of the evolution of the “metadata quality” concept understanding is presented.Early discussion on quality of metadata – actually, bibliographic records since the term “metadata” was not largely diffused – mainly concerned the rising costs of making bibliographic descriptions and the need to provide access to the increasing volume of library materials in the context of the Library of Congress as well as large OPACs. To solve such issues Graham (1990) urged catalogers to distinguish truly important and necessary aspects of cataloging from those elements that were nonessential for the average user. Thus, in Graham’s view, the conception of quality seems to be made independent of conformance to traditional cataloging rules rather be seen as related to the “fitness for use” understanding.The theme of quality of metadata for networked resources remained a relatively unexplored research area until it was discussed within a study to assess metadata records from 42 Federal agencies’ implementation of the Government Information Locator Service (Moen, Stewart, & McClure, 1997). The study concluded that “no consensus has been reached on operational and conceptual definitions of quality; likewise, validated procedures for assessing metadata are lacking”. Actually, great interest in these results rose when they were presented at the IEEE International Forum on Research and Technology Advances in Digital Libraries, ADL ’98 (Moen, Stewart, & McClure, 1998) as there were emerging environments characterized by increasing diversity of resources, data formats and application-specific functions, thus requiring quality criteria to consider contextual requirements – e.g., the specific functionality needed by the application, the nature of the described resources, the particular metadata formats conveying the information. Similar considerations had already been made by Moen et al. (1997) that concluded saying that “… given the force of user perspective on the representation of volatile information, and the lack of proven standards, systems of metadata … may require uniquely tailored approaches to quality assessment”; however, “the results of this analysis of metadata content will contribute to a developing dialog about assessing the quality of metadata”.A stronger debate about metadata quality issues in networked domains emerged around 2003, possibly moved by the pioneering work performed by Dushay and Hillmann (2003) in creating the National Science Digital Library (NSDL) as an aggregator gathering, through the OAI-PMH protocol, large amounts of metadata from repositories of resources in the fields of science, technology and mathematics. In that context, the strict relation between quality and compliance to bibliographic description praxis is still present. As a matter of fact authors state that most quality problems arose in that context because “increasingly complex array of resources were being described by untrained people instead of well trained librarians, or by automated means with ill-documented methods”. This statement assimilating “quality” with conformance to bibliographic principles, could apply to the quite uniform context characterizing how NSDL was being created (i.e., Dublin Core records describing scientific literature, aggregated through the OAI-PMH protocol). However, short later, Bruce and Hillmann (2004) recognized that metadata quality issues deriving from dependence on context are particularly evident in aggregated environments. Some years later, reflecting on the NSDL experience, Lagoze et al. (2006) recognized that the need for expressing context for resources was not taken into account in the NSDL experience. In the meanwhile this need was largely being recognized in the literature (cf. Section 4).Just starting from Bruce and Hillmann (2004), a number of studies and research efforts have been performed aiming at systematizing metadata quality concepts. All these studies have resulted in proposing frameworks to identify and assess quality parameters and metrics in specific contexts, rather than defining what metadata quality is. And such results could not be different, being “fitness for use” the ultimate, vague meaning of quality. These frameworks are presented in the next section.Digital Libraries are devoted to manage different typologies of data and corresponding metadata, possibly coming from different contexts. How evaluating metadata quality in such environments has been explored since early 2000 when networked repositories were started to diffuse.In their analysis based on the Learning Objects and e-Prints communities of practice, Barton, Currier, and Hey (2003) point out that, in an environment where each metadata repository or archive is part of a wider system that aims at interoperability, quality assurance for metadata is a much more difficult issue than in a local context. Similarly, Stvilia, Gasser, Twidale, Shreeves, and Cole (2004) recognize that one of the most ticklish issues in the theory of information quality is how to account for the context-sensitive nature of information quality and value. As observed by most authors, it may happen that in the original context metadata quality is high, but in an aggregated environment the same metadata has low quality. This generally happens because quality parameters that are valid in the original context may be different from the parameters adopted in the aggregated environment, where metadata will likely be used to reach a purpose different from the one for which they were originally created. Furthermore, in the original context there may be information which is not explicitly specified because it is considered as assumed knowledge, e.g., the controlled vocabularies used in metadata fields, while in the aggregated environment this unspecified information leads to a deterioration in quality because it does not allow a correct data interpretation. Therefore, as digital repositories grow in size, number and diversity, and aggregated environments become increasingly widespread, the problem of ensuring a sufficient general level of quality becomes fundamental.The remainder of this section presents frameworks to assess metadata quality. In all of them, the definition of the qualities to be assessed, i.e., parameters or dimensions, and methods to assess them, i.e., metrics, is the most critical activity. The first frameworks discussed were devised for repositories aimed at meeting the requirements of finding, identifying, selecting, obtaining information objects through their metadata. Namely: (i) the Bruce and Hillmann framework (cf. Section 3.1) that elaborates on 7 generic metadata quality characteristics ranging from completeness to accessibility; (ii) the Ochoa and Duval framework (cf. Section 3.2) that complements Bruce and Hillmann framework by proposing 13 quality metrics to evaluate the quality of item-level metadata in a collection; (iii) the Stvilia et al. framework (cf. Section 3.3) that identifies 22 quality dimensions and proposes 41 metrics for their calculation, of which 30 are based on object or collection attributes. Then, two studies are briefly cited coming from specialized communities, namely, the Open Language Archives Community and the Community of educational Digital Libraries (cf. Section 3.4). The remaining ones have grown in the context of modern Digital Libraries, intended as frameworks assessing the quality of the entire service (cf. Section 3.5). A comparative summary is presented in Table 1while concluding remarks on the discussed frameworks are in Section 3.6.Among the first attempts to give a general definition of metadata quality dimensions is Bruce and Hillmann (2004). They identify seven general characteristics of metadata quality: completeness, accuracy, provenance, conformance to expectations, logical consistency and coherence, timeliness, and accessibility. These seven dimensions aim to be domain-independent, i.e., “one might think of these characteristics as places to look for quality in collection-specific schemas and implementations, rather than checklists or quantitative systems suitable for direct application”.The authors accurately introduce and describe each dimension and its characteristics, but give no formal definition nor metrics. In addition to the seven dimensions, they identify several levels of quality for metadata, i.e., the element set or metadata scheme level, the syntactic level, and the data values themselves. In fact, they affirm that any definition of quality should evaluate the attributes of metadata at such different levels.The authors note that it is not possible to state which of the seven dimensions they describe is most important, nor which most urgently needs to be present for a given application, since the importance of each quality criterion is strictly influenced by the nature of the resource to be described, as well as by the environment in which the metadata is to be constructed or derived. Thus great emphasis is put on the fact that perception of quality strictly depends on context.As an application of the seven quality dimensions composing their framework, these authors propose a “what to ask for and where to look” compliance checklist: in practice, they suggest a series of questions that might be asked in order to check whether metadata under assessment is in conformance with the established criteria, as well as several quality compliance indicators that might be used to answer the questions. Compliance indicators may be automated means, or human techniques, or both, and they include the use of graphical analysis software, as well as the presence of controlled vocabularies, provenance information at several levels of detail, and advanced documentation such as an expression of the metadata intentions. Clearly, the checklist questions imply subjective evaluation and do not provide any way for giving a quantitative measurement of the quality criteria.About the above framework, Hillmann and Phipps (2007) point out that “Although the criteria provide opportunities to converse about quality, without ways to measure that quality, they remain frustratingly beyond reach”. Therefore, they suggest to consider a view where Application Profiles (Heery & Patel, 2000) are seen as a “template for expectation”, and where metadata under assessment can be compared to that template for obtaining quantitative measurements of the quality parameters. In particular, Hillmann and Phipps see a potential for assessing the following dimensions: (i) completeness, by relying on the use of “obligation” imposed by the profile it is quite straightforward to verify whether a metadata is complete or not; (ii) conformance to expectations, by relying on descriptions of conditions that should occur when a value is present it is possible to verify whether a metadata satisfies the expectations or not; (iii) accuracy, for instance it is possible to quantify the level of invalid vocabulary terms when a vocabulary encoding scheme is specified. With regard to the other dimensions, these authors admit that it may be too difficult or even impossible to assess them by relying on Application Profiles for expressing expectations.There are studies proposing to supplement the Bruce and Hillmann Framework dimensions with other related to shareability. Grounded in experiences in cultural heritage institutions, Shreeves et al. (2005) observed that metadata may be of high quality within a local database but this quality may be lost when metadata are combined in a federated environment. Thus, “understanding of the criteria for high quality, ‘shareable’ metadata is crucial to the next step in the development of federated Digital Libraries”. Accordingly, Shreeves, Riley, and Milewicz (2006) suggest shareability as an adjunctive metadata quality dimension. In the authors’ view, shareable metadata is metadata which can be understood and used outside of its local environment by aggregators to provide more advanced services. That is, shareable metadata should be useful and usable to services outside of its local context given the resource described. For this, they suggest the following characteristics specifically conceived to assess shareability in addition to characteristics of quality metadata: (i) content is optimized for sharing; (ii) metadata within shared collections reflects consistent practices; (iii) metadata is coherent; (iv) context is provided; (v) the metadata provider communicates with aggregators through direct or indirect means; and (vi) metadata and sharing mechanisms conform to standards. It is to note here that the concept of “shareability” has become a basic one in the cultural heritage community as a result of increased expectations for making descriptive metadata openly available (Riley & Shepherd, 2009). The centrality of such a concept, as well as that of “interoperability” which is strictly connected to it, has revealed in the field of digital library infrastructures as to become one of their main issues (Candela, Castelli, & Thanos, 2010).The framework defined by Ochoa and Duval (2009) is strictly related to the Bruce and Hillmann one (cf. Section 3.1). While Bruce and Hillman devised their framework to guide human reviewers, Ochoa and Duval work aims at proposing a framework that comprises meaningful quality parameters, i.e., quality parameters that might be used by human reviewers, associated with automatic calculable measures of quality. In particular, they complement the Bruce and Hillmann framework by proposing automatic measurement methods for the seven parameters of such a framework.For each quality parameter one or more metrics are proposed with a rationale, the calculation formulas and some guidelines.The authors point out that the proposed metrics are not intended to be a comprehensive or definite set, but should be considered as “a first step for the automatic evaluation of metadata quality”. In fact, the following characteristics and limitations are observed: (i) they are easy to implement in real environments and can be used for a wide range of digital repositories; (ii) they are are standard- and community-of-practice-agnostic, even though the parameters needed to initialize the calculations are context-dependent; (iii) they are mainly designed to work over metadata in the form of text and numbers. For metadata containing non alphanumeric information new approaches are needed; (iv) they are mainly conceived for metadata instances conforming with a relatively stable metadata schema; (v) the normalization of the metrics may not always be possible; and (vi) the mix of the quality parameters is the general quality of the metadata instance, although no proposal is made on how to mix them since there may be several tradeoffs between the characteristics.Ochoa and Duval (2009) conducted three validation studies to evaluate the proposed metrics with respect to: the correlation with human-made quality assessment; the effectiveness in discriminating key properties of diverse metadata sets; and the effectiveness as automatic low-quality filters. The following results have been obtained: (i) some metrics correlated well with human reviewers while others seems completely unrelated. In particular, the Qtinfo, i.e., the metric measuring the information content of the text fields, seems to be a very good approximation of human perceived quality. Thus human reviewers tend to evaluate metadata as content, i.e., longer and specialized text receive an higher score than a shorter one; (ii) the metrics are effective in discriminating manually generated metadata (expected to have an high quality) from automatically generated ones. In particular, the metrics point out that human experts filled more fields than the automatic approach as well as they tend to select a richer set of categorical values; (iii) some of the metrics (Qcomp,Qwcomp,Qtinfo) as well as the average of all the proposed ones are an effective approach for building automatic quality filters; and (iv) some quality parameters are very difficult to be evaluated by humans, e.g., the variability of categorical values, while the metrics were able to capture them.Stvilia, Gasser, Twidale, and Smith (2007) proposal has been driven by the need to define a general framework and an effective measurement model, which is a pre-requisite for information quality (IQ). By revising previously defined IQ assessment frameworks in the data management field (Eppler, 2006; Wang & Strong, 1996) they observe that most of them are “ad hoc, intuitive, and incomplete and may not produce robust and systematic measurement models”. Therefore, in contrast to context-specific quality assessment models depending on variables determined by local needs, these authors focus on studying the causes of quality changes, and present a framework consisting of typologies of IQ problems, related activities, and a systematically organized taxonomy of IQ dimensions. In this framework, an IQ problem is said to be occurring when the IQ of an information entity does not meet the IQ requirements of an activity on one or more IQ dimensions, and an IQ dimension is defined as any aspect characterizing the IQ concept. In any case, the authors clearly recognize that information quality is contextual, and state that their framework could be used as “a knowledge resource and guide for developing IQ measurement models for many different settings”.Four major sources of IQ problems are identified: mapping, changes to the information entity, changes to the underlying entity or condition, and context changes.From the analysis of these sources, the authors develop a taxonomy of 22 IQ dimensions,1The 22 dimensions are taken from a taxonomy previously derived from an analysis of 32 representative quality assessment frameworks from the IQ literature (Stvilia et al., 2004).1systematically organized into three categories: (i) intrinsic, i.e., dimensions that can be assessed by measuring information aspects in relation to reference standard (e.g., spelling mistakes); (ii) relational, i.e., dimensions that measure relationships between the information and some aspects of its usage (e.g., accuracy); and (iii) reputational, i.e., IQ dimensions that measure the position of an information entity in a given structure (e.g., authority).In addition to the taxonomy, a set of 41 general metric functions (30 of them are object- or collection-based, 11 are process-based) are proposed. The authors also provide a Java implementation of these functions.Moreover, from the analysis of the sources of IQ problems the authors propose a clustering of information activities that are affected by such problems: (i) representation dependent, i.e., activities depending on how well an information represents an entity or a condition; (ii) de-contextualizing, i.e., activities depending on the use of information in contexts different from the one the information is produced for; (iii) stability dependent, i.e., activities depending on the level of stability of information; and (iv) provenance dependent i.e., activities depending on the quality of information provenance.Such a framework can be used for analyzing a specific context and developing an appropriate IQ measurement model. System activities are analyzed to identify those prone to quality problems and to select the relative IQ dimensions and metrics. Two concrete use cases (assessing the quality of aggregated metadata records, assessing the quality of English Wikipedia articles) are discussed in Stvilia et al. (2007).Many authors, e.g., Park (2009) and Ochoa and Duval (2009), assert that it is interesting to note the overlap between this framework and the Bruce and Hillmann one (cf. Section 3.1). In particular, the relation between the frameworks is highlighted by Shreeves et al. (2005) that graphically depict the mapping between the parameters of the two.The frameworks presented in this section diverge from the way traced by Bruce and Hillmann. In particular, in the framework presented by Hughes (2004) metadata quality assessment is related to context.Hughes discusses the metadata quality assessment issue in the context of Open Language Archives Community (OLAC). He posits that “any measure of metadata quality benefits from both contextual and referential assessment – metadata on a per record and per collection basis is legitimately assessed against the baseline of broader community of practice, as well as for compliance to any external standard”. Accordingly, he proposes 7 metrics (Archive Diversity, Metadata Quality Score, Core Elements Per Record, Core Element Usage, Code Usage, Code and Element Usage, “Star Rating”) and an algorithm aiming at giving a metadata record a score between 0 and 10 representing the adherence of the metadata to best practices for Dublin Core and domain-specific controlled vocabularies.Bethard, Wetzler, Butcher, Martin, and Sumner (2009) presents a different path toward automatic characterization of resource quality in the realm of educational Digital Libraries to help the identification of resources to use for learning goals. The quality indicators considered as most predictive are: has prestigious sponsor, content is appropriate for age range, has sponsor, identifies learning goals, has instructions, identifies age range, organized for learning goals. Such indicators are “boolean”, i.e., each of them can be present or not when assessing a given resource. Moreover, the authors propose an approach relying on machine learning models for assessing the presence of such indicators.In the Digital Library domain, frameworks aiming at assessing the quality of the entire Digital Library service have been proposed.The DL.org Digital Library Reference Model (Candela, Athanasopoulos et al., 2011) is a comprehensive framework aiming at laying the foundations of the whole Digital Library domain. Among its core concepts there is the quality domain, i.e., the set of concepts characterizing Digital Libraries from the quality point of view. Such a domain is quite extent yet basic, it formalizes the following characteristics: (i) quality aspects can be associated with any “resource” contributing to form a Digital Library; (ii) quality aspects worth to be captured cannot be identified a-priory and are described via a number of “quality parameters”; (iii) quality parameters can be assessed by any “actor” (human or inanimate entity such as a software program); (iv) quality parameters are associated with a “measure” that is assessed according to a “measurement” which can be subjective or objective as well as quantitative or qualitative. In addition to that, the model presents a number of quality parameters (more than 40), clustered according to the domain they are primarily conceived for (e.g., generic, functionality) by clearly stating that this list is open, i.e., a community of practice can extend it with specific parameters. For the same reason, the model does not prescribe nor describe any quality measurement that is needed; rather, it leaves this decision to the community of practice that will instantiate the model in its application domain. Approximately half of the quality parameters are potentially suited for assessing metadata quality either because are “content quality” parameters (11 parameters including authenticity, integrity, freshness) or because are “generic” ones (9 parameters including reputation, compliance with standard, sustainability).The 5SQual (Moreira, Gonçalves, Laender, & Fox, 2009) is a tool supporting the evaluation of core elements in Digital Libraries, i.e., digital objects, metadata, and services. Such a tool actually implements a theoretical quality model for Digital Libraries (Gonçalves, Moreira, Fox, & Watson, 2007) which has been defined by having the 5S framework (Gonçalves, Fox, Watson, & Kipp, 2004) as the underlying model for characterizing Digital Libraries. In such a model a number of quality dimensions have been proposed and associated with Digital Library concepts. In particular, 3 quality parameters are associated with “metadata specification”, i.e., accuracy, completeness, and conformance, and 7 quality parameters are associated with “digital objects”, i.e., accessibility, pertinence, preservability, relevance, similarity, significance, timeliness. For each of them, the authors propose potential approaches for their calculation and the 5SQual offers an implementation of a subset of them.There are two main aspects characterizing every data quality assessment framework: the definition of the qualities to be assessed through the framework, i.e., parameters or dimensions, and the methods to be used to assess the identified qualities, i.e., metrics. Although among the discussed frameworks there is no one that is expected to supplant the others, it can be observed that: (i) diverse frameworks tend to share a number of quality parameters, e.g., accuracy, completeness; (ii) the dimensions captured by a framework tend to grow in number when the goal is to accommodate the needs of diverse communities of practice; (iii) it is not expected to reach a global agreement on which dimensions are to be defined and their exact meaning, simply because this is a community specific requirement, (iv) there is the need to develop frameworks having multiple metrics for assessing a given quality parameter as to be able to accommodate diverse settings; (v) there is the strong need to develop a comprehensive set of mappings supporting the transformation of quality parameters assessed according to a given framework into quality parameters assessed via another framework.Batini, Cappiello, Francalanci, and Maurino (2009) provide a comprehensive and systematic description of methodologies for selecting and applying data quality assessment and improvement techniques. In essence, they draw similar conclusions, e.g., there is a set of basic quality dimensions that occurs across the frameworks like accuracy, completeness, consistency and timeliness, yet no shared meaning. This confirms that in essence, data and metadata quality issues are two very close worlds. Moreover, they observe that the whole data quality research field is evolving, it cannot be considered mature, and it is moving towards considering a wider number of data types and information systems.The set of frameworks discussed so far are mainly conceived to characterize the metadata quality concept and provide methods for measuring to what extent a given resource has to be considered a quality one according to the given characterization. In this section, we discuss concrete approaches aiming at dealing with metadata quality issues.Yasser (2011) analyzed and compared problems reported in the literature and identified five categories of problems. These categories are: (i) incorrect values, i.e., metadata records contain values that do not represent a given resource correctly even though elements are applied correctly; (ii) incorrect elements, i.e., the values assigned are appropriate to describe the resource but have been assigned to the wrong element; (iii) missing information, i.e., the metadata record is not complete; (iv) information loss, i.e., some details characterizing the information are lost (this generally occurs during the process of converting metadata from one scheme to another); (v) inconsistent values, i.e., different values associated with an element may equally represent a characteristic of the resource, but they may be different enough in recorded form to undermine system functionality.To resolve these problems a number of approaches are described in the literature. Depending on the solution proposed, four categories can be identified: approaches aiming at achieving a common understanding of metadata (cf. Section 4.1); approaches aiming at highlighting the problems affecting metadata (cf. Section 4.2); approaches aiming at supporting the generation of metadata (cf. Section 4.3); and, approaches aiming at repairing and homogenising metadata (cf. Section 4.4). Remarks on these categories are given in Section 4.5.Metadata guidelines are agreed policies potentially governing every aspect of metadata including the values and elements to be used to characterize the resources the system manages. Cataloguing guidelines have a long and well recognized tradition, and their role in the creation of quality metadata has necessarily been recalled in the context of Digital Libraries and repositories, where the practice of metadata creation by authors is much diffused. In fact, guidelines are among the most used approaches (Park & Tosaka, 2010b) and they can be a very effective tool to convey rules and principles thus to establish a common knowledge.Metadata standards and applications profiles are two approaches for realizing metadata guidelines, e.g., Guy, Powell, and Day (2004) and Hillmann and Phipps (2007) discuss on how to establish guidelines by using Application Profiles.Some guidelines are associated with metadata standards, e.g., Dublin Core metadata guidelines (Apps, 2005). Others are tailored to promote a certain use of a metadata standard in a given context, e.g., the CDP Dublin Core Metadata Best Practices (CDP Metadata Working Group, 2006) provide guidelines for digitized cultural heritage resources by using the Dublin Core element set. Others are oriented to enhance the quality of metadata offered via web-based services, e.g., the CrossRef guidelines define nine easy steps aiming at enhancing the produced metadata (crossref.org, 2012). Others are oriented to support the realization of services resulting from aggregating metadata from different “providers”, e.g., Vanderfeesten, Summann, and Slabbertje (2008) defined guidelines characterizing a number of aspects including metadata standards, OAI-PMH implementation, best practices and semantics such as the use of “inverted name” syntax for “creator” which is also a mandatory element.However, these approaches are not problem free. Park and Tosaka (2010a, 2010b) evaluate metadata creation practices in digital repositories and collections. Such studies – conducted on cataloging and metadata professionals dealing with a great variety of digital objects – highlighted that although the analyzed sample uses a wide range of metadata standards, only a few are widely used, namely MARC and DublinCore. DublinCore, although widely used, was considered difficult to apply because of semantic overlaps and ambiguities.Metadata evaluation approaches consists in assessing quality dimensions with computer assisted facilities (Bruce & Hillmann, 2004). However, Nichols et al. (2009) observe that because of the community specific nature of the metadata quality dimensions, evaluation approaches aiming to identify general quality problems may not meet the requirements of a specific application environment. Thus such tools can be “only” a valid help for identifying metadata problems, but informed interpretation is necessary to understand the actual problems and take the correct decisions in the specific context.These approaches fall in two main categories: analytic-oriented approaches and crowdsourcing-oriented approaches.Analytic-oriented approaches are aimed at extracting quality dimensions. For instance, Hillmann and Phipps (2007) underline the potential of Application Profiles to support metadata quality automated validation. They observe that when metadata has to conform to a specific Application Profile an automatic validator should be able to validate metadata characteristics such as the presence of appropriate encoding schemes, as well as the correctness of the vocabulary terms themselves. For example, the validator should be capable to determine whether a metadata element is qualified by the correct encoding scheme, or whether a value term is correctly expressed according the related syntax encoding scheme. However, the authors have to recognize that not all characteristics can be validated automatically, e.g., an automatic validator may not be able to determine the correctness of a date expressed in free text format.Dushay and Hillmann (2003) proposed SpotFire, a software instrument that produces visual representations of data thus allowing humans to recognize visual patterns and derive appropriate conclusions. In particular, such a tool offers a number of data visualization and analysis facilities including operations such as checking for conformance to a particular controlled vocabulary or string pattern, and looking for anomalies in data such as typographical errors and bad values. A similar tool is also proposed by Nichols, Chan, Bainbridge, McKay, and Twidale (2008). The main functionalities offered are a summary description of metadata elements, a sorted presentation of metadata element lists and a completeness oriented visualization.Crowdsourcing-oriented approaches are based on user feedbacks. Feedbacks can be obtained from the activities of system’ final users, as well as from the work of digital library and repository administrators. For instance, Manghi, Manola, Horstmann, and Peters (2010a) proposed a facility which allows final users to submit data curation feedbacks in the form of “delete”, “add” and “update” annotations in order to help improve the quality of the aggregated content. Savino and Schulze (2011) propose two tools for collecting feedback: (i) a “content checker” aiming at helping archivists to discover and signal errors in their metadata elements once represented in the aggregated metadata format as the result of existing mapping rules; (ii) a “vocabulary checker” aiming at helping managers to discover elements that do not match the agreed controlled vocabularies.Semi-automatic metadata generation approaches promote the creation of metadata by combining software facilities with human practices (Greenberg, Spurgin, & Crystal, 2006; Park & Lu, 2009). These facilities range from metadata editors to tools aiming at automatically generating metadata.Park (2009) discusses an approach for using metadata guidelines by embedding them within a tool for semi-automatic metadata generation, so to enable catalogers or document authors to create metadata in compliance with the guidelines. Greenberg, Pattuelli, Parsia, and Robertson (2001) studied how a simple Web form with textual guidance and selective use of features such as drop-down menus and pop-up windows could assist document authors in the generation of quality metadata.Greenberg (2004) discusses two methods for automatic metadata generation: metadata extraction, i.e., metadata are produced by relying on the resource content, and metadata harvesting, i.e., metadata are collected from human-created tags embedded in the header source code of the resources. Greenberg et al. (2006) provide a very brief overview of automatic metadata generation approaches and revise both experimental research and application developments with the goal to identify functionalities for tools supporting automatic metadata generation. Park and Lu (2009) analyzed the extent and types of research initiatives and systems, and discuss their practical application. All these studies conclude that (a) automatic processes will never replace human evaluation or production, rather they have to aid humans while creating metadata; (b) two fundamental functionalities are (i) helpers supporting the acquisition of metadata that a human can evaluate and edit; and (ii) helpers supporting the integration of content “standards” (e.g., subject thesauri, name authority files) into the metadata generation applications.This family of approaches complements the others since they focus on “repairing” existing metadata rather than identifying potential issues like evaluation approaches or producing quality-level metadata like guidelines and semi-automatic generation approaches. Because of this characteristic, its coverage is quite large and multifaceted.Among the issues receiving a lot of attention due to their difficulty there are those related with name disambiguation. Lee, On, Kang, and Park (2005) observe that name ambiguity in bibliographic citations can be divided into two specific sub-problems: mixed citation (a.k.a. homonym problem) and split citation (a.k.a. synonym problem). Mixed citation occurs when the same name refers to more than one person, family or organization; this may happen due to abbreviations or because the different entities have exactly the same name spelling. Split citation, on the other hand, occurs when a person has different name variations which are treated as if they belonged to different persons; this may be due to pseudonyms, differences in language or script, transcription errors, abbreviations, as well as change in the order of the name components or change of name for many reasons such as marriage or divorce. The authors present solutions based on one of the state-of-the-art sampling-based approximate join techniques declaring them as scalable yet highly effective. Laender et al. (2008) propose a solution for name disambiguation consisting in a heuristic-based hierarchical clustering (HHC) method, stemming from the following considerations: (i) it is very rare that two authors with very similar names and sharing a common co-author are two different persons in the real world and (ii) authors tend to publish in the same subjects and publication venues for a considerable portion of their careers. The authors claim that HHC performs competitively when compared with existing supervised machine-learning methods, without requiring any training phase. Beall (2010) analyzes strengths and weaknesses of manual and automatic approaches and concludes that a hybrid approach may become the most successful and widely used, especially for resources in the open world of Internet. Beall also discusses some features of name metadata records (such as birth and death dates, family, life events, institutional affiliation) and how these might help. Moreover, Beall highlights the role that services like the Library of Congress Authorities, the Virtual International Authority File (VIAF), or Wikipedia, might have. Kan and Tan (2008) propose to use uninformed string matching (e.g., the cosine distance or the edit distance) and informed record matching (i.e., record similarity is calculated by combining string similarity in a weighted formula thus to consider the different type of elements). Smalheiser and Torvik (2009) surveyed the literature related to name disambiguation and proposed a probabilistic model based on a multi-dimensional vector space for features representation. Recently, Manghi and Mikulicic (2011) presented an open source tool for authority control which aims to (i) offer administrative user interfaces for customizing the structure of authority files, (ii) tune-up probabilistic disambiguation of authority files through a set of similarity functions for detecting record candidates for duplication and overload, (iii) curate such authority files by applying record merging and splitting actions, and (iv) expose authority files to third-party consumers in several ways.Another problem that has been extensively discussed is the homogenisation and enhancement of metadata when they are integrated into an aggregative system aiming at offering a unifying access to the resources and a number of added value services (Manghi, Mikulicic, Candela, Castelli, & Pagano, 2010b). Hillmann, Dushay, and Phipps (2004) propose to apply “safe transforms” to metadata records, i.e., an automated technique which is designed for addressing some of the common quality problems (e.g., missing data, incorrect data, confusing data, insufficient data), and which can be applied to enhance the information of the original metadata with no risk of degradation, e.g., (i) by removing “noise” like metadata with no information, (ii) by detecting and identifying controlled vocabularies, and (iii) by normalizing metadata presentation. Moreover, they propose an approach for metadata recombination and augmentation which is based on the idea that a metadata record can be built by aggregating metadata “statements” included in different “records” coming from diverse providers. A similar idea comes from Phipps, Hillmann, and Paynter (2005), that suggest to create an “orchestra” of automated services for aggregating source statements into enhanced descriptions and exposing them to users. The “orchestra” include services for metadata augmentation, safe transformations, equivalence services, crosswalking, archiving and persistence checking, annotation services, and metadata improvement and rating. Hillmann (2008) analyses the difference between transformative processes (e.g., modifying metadata based on the structure or values already available in statements) and augmentative ones (e.g., adding information based on information gleaned from the resource itself). This distinction is relevant for determining the sequence of processes. Transformations do things like: detection of controlled vocabulary values and attribution of those values to a particular vocabulary; detect and fix common typographical errors; deprecation of “promiscuous defaults”, e.g., values that provide no information value, added to metadata only to fill a slot or provide functionality only. Instead, augmentation includes: machine-based processes that add values, for example, topics or formats; human-based augmentation, such as the addition of topics, relationships to educational standards. In some cases transformation can be orchestrated by humans. For instance, Savino and Schulze (2011) described a “metadata editor” through which an authorized user can interact with the entire set of aggregated metadata and modify existing metadata records as well as create new records. While editing an existing record, a user can correct the metadata values, as well as enrich the metadata elements, and then store the modified record back into the aggregating system. The editor automatically performs a check on incorrect values and on missing mandatory elements by relying on established policies and guidelines. Groat (2009) gives a description of desired services aiming to remediate the metadata in order to achieve certain expectations with respect to the quality of service offered by the aggregator service. Such a report surveys desired and existing tools for the following metadata element typologies: topical subjects, genre, names, geographical information, dates, title information, type of resource, addressable raw object, rights, and identifiers.The enhancement or augmentation of metadata records can be performed on specific elements of the records. For instance, in case where it is requested to have access to the real resource described by the metadata, it is possible to complete deficient records. Laender et al. (2008) propose a strategy consisting in an extensible service called PaperMetaSearch. This service searches for a document full text on the Web, by submitting parameterized queries to existing search engines (e.g., Google, Yahoo), with the employment of metadata information already available to the user as potential query arguments, e.g., the title and the list of the authors of the document.Each of the approaches discussed above has been introduced to solve specific quality problems. None of them can be considered as the ultimate and optimal solution to all quality issues, especially in complex and heterogeneous contexts as those addressed by the new evolutionary types of digital library systems, like the data infrastructure ones. A summary of the pros and cons of the presented approaches is given in Table 2. Methods forcing metadata with shared meaning (cf. Section 4.1) are potentially effective since explicitly declare the agreed aspects, yet it is impossible to identify generic and expressive enough agreements capable of accommodating the needs of every community of practice. Approaches aiming at highlighting the problems affecting metadata (cf. Section 4.2) are very useful for calling attention to potential problems, yet the list and semantics of the potential problems is usually community specific. Solutions focused on supporting the semi-automatic generation of metadata (cf. Section 4.3) are fundamental in contexts characterized by a large amount of resources, yet the need of human assessment of what has been generated it is an important limitation to the scalability of the approach. Approaches aiming at repairing and homogenising metadata (cf. Section 4.4) enable the usage of metadata in contexts different from their initial ones, yet may bring information loss and inconsistency.Crowdsourcing-based approaches, which offer a problem-solving strategy that well apply to the data infrastructure settings, offer new opportunities as well as potentially introduce new challenges such as how to assess users and their contributions (Doan, Ramakrishnan, & Halevy, 2011; Oomen & Aroyo, 2011). For instance, the so diffused social tagging (Huang, Lin, & Chan, 2012) might have a very important impact on quality and effectiveness of resources metadata, especially if combined with semantic technologies. However, tags are often community specific and thus difficult to exploit in multidisciplinary contexts.

@&#CONCLUSIONS@&#
Data and metadata quality is a very important yet challenging issue affecting the effective usage of such a kind of resources playing a key role in our information society. Although no definitional agreement has been achieved yet, it is commonly recognized that metadata quality is a subjective, multi-dimensional and context-dependent concept. All the issues characterizing it in a given community of practice are further amplified when dealing with “big data” scenarios where data and metadata (a) come from multiple and heterogeneous sources, (b) are collected with different approaches, and (c) are expected to be used in contexts different from their initial ones.In this work, we have surveyed efforts done so far in modelling and assuring metadata quality in the Digital Library domain with the aim of providing a comprehensive and update picture of the progresses achieved so far in this area. In particular, we have discussed a number of attempts aiming at proposing frameworks characterizing the metadata quality issue and promoting effective methods for assessing the quality of given metadata. Moreover, a number of metadata quality issues arising in the creation and/or the aggregation phases have been discussed and the approaches aiming at mitigating them – e.g., guidelines, (semi-)automatic generation, validation, cleaning, improvement – have been presented. In spite of the fact that automatic approaches fit better with the given setting, best results in all stages of the metadata life cycle are obtained when automatic means are integrated with human intervention.In spite of these results, there are still many open issues in dealing with data and metadata quality. The most relevant among these issues are: (i) the need to develop a comprehensive and open framework enabling diverse research communities to characterize their data quality concepts and tools by means of a lingua franca; (ii) the demand for the development a generic (i.e., non-domain-specific) and machine-processable way to capture data quality aspects that can be effectively used to acquire genuine indicators of quality-oriented aspects; (iii) the need to develop generic tools that, by relying on given characteristics of the data and known strategies, can augment quality-oriented aspects and certify the degree of the resulting data with respect to such aspects.So far systems addressing more complex contexts, like those enabling data infrastructures, have developed solutions that largely resemble those just discussed for Digital Libraries. For instance, the OpenAIRE initiative (Manghi, Bolikowski, Manola, Schirrwagen, & Smith, 2012), which is called to develop a data infrastructure for open access research products, has developed guidelines that complement and reuse the DRIVER guidelines (Vanderfeesten et al., 2008) discussed in Section 4.1. Similarly, in the context of the biodiversity domain, Hardisty and Roberts (2013) envisage a data infrastructure serving this domain in which data and metadata quality issues and potential solutions are borrowed from the digital library domain. The US DataOne initiative (Allard, 2012), dedicated to provide an e-infrastructure for Earth observational data, has recently published tutorials on “How to write Quality Metadata” (Henkel, Hutchison, Strasser, Rebich Hespanha, & Vanderbilt, 2012) focusing primarily on “accuracy” and “completeness”. Finally, in the framework of EUDAT (Lecarpentier, 2011), a large data infrastructure initiative that plans to act as an European aggregator of datasets metadata, solutions close to the one described above are proposed for the initial phase of infrastructure development, however it is recognized that much more has still to be done to deal with multi-disciplinary and multipurpose contexts (Witt, 2012).Given the increasing relevance of data-driven research and decision-making it is expected that research on data and metadata quality will largely reinvigorate in the future. This expectation has motivated the authors in compiling this survey so to offer a reference point for all those that are not familiar with the large amount of work already done on metadata quality in the Digital Library area.