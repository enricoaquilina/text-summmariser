@&#MAIN-TITLE@&#
Carbon-efficient scheduling of flow shops by multi-objective optimization

@&#HIGHLIGHTS@&#
A scheduling framework is presented to improve carbon efficiency in flow shops.We extend NEH-Insertion Procedure to incorporate energy criterion.Two multi-objective optimization algorithms (MONEH and MMOIG) are proposed.Numerical computations show that the proposed algorithms outperform NSGA-II.

@&#KEYPHRASES@&#
Flow shop,Carbon efficiency,Makespan,Total energy consumption,extended NEH-Insertion,

@&#ABSTRACT@&#
Recently, there has been an increasing concern on the carbon efficiency of the manufacturing industry. Since the carbon emissions in the manufacturing sector are directly related to the energy consumption, an effective way to improve carbon efficiency in an industrial plant is to design scheduling strategies aiming at reducing the energy cost of production processes. In this paper, we consider a permutation flow shop (PFS) scheduling problem with the objectives of minimizing the total carbon emissions and the makespan. To solve this multi-objective optimization problem, we first investigate the structural properties of non-dominated solutions. Inspired by these properties, we develop an extended NEH-Insertion Procedure with an energy-saving capability. The accelerating technique in Taillard’s method, which is commonly used for the ordinary flowshop problem, is incorporated into the procedure to improve the computational efficiency. Based on the extended NEH-Insertion Procedure, a multi-objective NEH algorithm (MONEH) and a modified multi-objective iterated greedy (MMOIG) algorithm are designed for solving the problem. Numerical computations show that the energy-saving module of the extended NEH-Insertion Procedure in MONEH and MMOIG significantly helps to improve the discovered front. In addition, systematic comparisons show that the proposed algorithms perform more effectively than other tested high-performing meta-heurisitics in searching for non-dominated solutions.

@&#INTRODUCTION@&#
In recent decades, global warming and climate change have gained increasingly more public attention. As is widely acknowledged, global warming is caused by the increasing amount of worldwide greenhouse gas emissions, particularly the carbon dioxide (CO2) produced in the fossil fuel combustion process. Since fossil fuels are the main source of energy generation, rationalized energy consumption will significantly reduce carbon dioxide emissions, and thereby slowing down global warming. According to Fang, Uhan, Zhao, and Sutherland (2011), the industrial sector contributes about one-half of the world’s total energy consumption. Thus, manufacturing enterprises have become a major source of global warming and their carbon footprints are likely to be restricted by high taxes and related regulations in the future. Faced with this situation, manufacturers will have to seek for practical approaches to reduce energy consumption and carbon footprints in the production process.One natural approach for reducing carbon footprints in the manufacturing process is to develop power-efficient machines (Li, Zein, Kara, & Herrmann, 2011; Mori, Fujishima, Inamasu, & Oda, 2011) and to design embodied product energy framework (Kara, Manmek, & Herrmann, 2010; Rahimifard, Seow, & Childs, 2010). However, the above energy improvement methods require considerable financial and personnel costs, which are not applicable for most small- and medium-sized manufacturing enterprises. In the meantime, some researchers observed that the reduction of carbon emissions can also be achieved by some operational strategies and advanced scheduling schemes (Drake et al., 2006; Gutowski, Dahmus, & Thiriez, 2006). This approach is much more practical for improving carbon efficiency in the production systems and has attracted growing research interests and attentions.Among the increasing research attempts to reduce environmental impact through production scheduling, the work by Mouzon, Yildirim, and Twomey (2007) is one of the most well known. They collected the operational statistics of four CNC machines in a machine shop and observed that a significant amount of energy is consumed when the non-bottleneck machines are left idle. Therefore, they proposed a machine turn-on and turn-off scheduling framework to reduce energy cost while optimizing other scheduling objectives. This framework is further explored in a follow-up work of Mouzon and Yildirim (2008) for a single machine problem to minimize the total tardiness and the total energy consumption. More recently, Dai, Tang, Giret, Salido, and Li (2013) extended this “ON–OFF” strategy to the flexible flow shop scheduling problem (FFSP) to make a trade-off between the makespan and the total energy consumption.Although turning on and off the machines can achieve the reduction of energy consumption, this strategy may not be practical in some workshops, where the machines and appliances cannot be switched off completely during the manufacturing process (Luo, Du, Huang, Chen, & Li, 2013). Recently, a new speed scaling framework is proposed for the scheduling problem (Fang, Uhan, Zhao, & Sutherland, 2013). In this framework, machines are allowed to run at varying speed levels when processing different jobs. When a machine is processing at a higher speed, the processing time decreases while the electricity consumption increases. These settings would lead to an obvious contradiction between the total processing time and the total carbon emissions. Under this framework, several scheduling problems have been investigated. Fang, Uhan, Zhao, and Sutherland (2012) considered a flow shop scheduling problem with a restriction on peak power consumption. They proposed two mixed integer programming formulations for this problem and investigate their computational performance. Liu and Huang (2014) studied a batch-processing machine scheduling problem and a hybrid flow shop problem, both of which involve the economic- and environmental-related criteria.The permutation flow shop (PFS) problem is one of the most widely discussed scheduling model over the decades (Gupta & Stafford, 2006). However, little attention has been paid to the energy perspective in this model (Fang et al., 2011; 2012). To fill in the gap, we consider the problem of minimizing the completion time (makespan) and the total carbon emissions in a m-machine PFS problem. The speed scaling strategy is also considered in the presented model to achieve a better trade-off between the two objectives. However, obtaining the Pareto optimal sets for this problem is quite difficult. The traditional m-machine PFS problem with makespan criterion is already known to be NP-hard when m ≥ 3 (Garey, Johnson, & Sethi, 1976) and solving the considered problem with speed scaling and multiple objectives is expected to require much more computational efforts. Thus, it is impractical to solve the problem optimally with mixed integer programming for the medium- and large-sized instances.Meta-heuristics, on the other hand, can provide an optimal or near-optimal solution with acceptable time consumption. They are widely adopted for combinatorial optimization. One of the most famous meta-heuristics for multi-objective optimization is the non-dominated sorting genetic algorithm II (NSGA-II) (Deb, Pratap, Agarwal, & Meyarivan, 2002). It has been proved to be the most effective algorithm for several benchmark problems (Zhihuan, Yinhong, & Xianzhong, 2010). However, the “No Free Lunch” principle (Wolpert & Macready, 1997) suggests that, without utilizing the specific information of a problem, all algorithms will perform no better than random blind search when averaged across all possible problems. In addition, most of the meta-heuristics are very time-consuming for large-scale problems, which leads to slow convergence and low efficiency. Therefore, we may develop a more efficient and effective algorithm than the NSGA-II and other high-performing meta-heuristics if some structural properties of the problem and some accelerating techniques are considered in the searching progress.Research on using problem properties and acceleration techniques to improve the performance of meta-heuristics is rather scarce for the carbon-efficient scheduling problems. Most research works concerning this topic design a sophisticated evolutionary computation algorithm for the multi-objective optimization problem and validate its performance through computational analysis. The nature of the problem is usually not fully utilized in the search progress. On the other hand, there is a considerable amount of literature on how the problem structural properties and speedup methods can help to improve the performance of meta-heuristics for the traditional PFS problem with a single objective (Grabowski & Pempera, 2001; Nowicki & Smutnicki, 1996; Taillard, 1990). The Taillard’s acceleration method for makespan evaluation (Taillard, 1990) is one of the most well-known speedups for the PFS problem. It can evaluate a total of n + 1 candidate solutions generated by the NEH-Insertion procedure (Nawaz, Enscore, & Ham, 1983) within O(mn) computational time. Inspired by the process of Taillard’s method, we notice that the NEH-Insertion procedure is also well-suited for the considered multi-objective PFS problem if we make a proper extension to it. This extension is based on the problem property that there could be some energy reduction if we slow down some operations of the inserted job while keeping the makespan unaffected. Use of this property in the energy saving process can result in the exclusion of a large number of dominated candidate solutions in O(1) time without evaluating these solutions exactly. Therefore, the extended NEH-Insertion procedure can lead the search process to more promising regions in the solution space efficiently. Based on the above results, we develop and implement a multi-objective NEH (MONEH) algorithm and a modified multi-objective iterated greedy (MMOIG) algorithm for the considered problem to minimize the makespan and the total carbon emissions.The contribution of this work is at least threefold:•A multi-objective optimization framework is presented for the flowshop scheduling problem when considering machine energy consumption cost.We make a good extension to the well-known NEH-Insertion Procedure. Incorporated with a novel energy saving process, the extended procedure can help to improve the discovered fronts significantly for the considered problem.An efficient constructive heuristic algorithm (MONEH) and a high-performing meta-heuristic algorithm (MMOIG) are proposed for the problem. The high-quality non-dominated solutions generated by these algorithms can help decision makers to balance between the productivity-related criterion and the sustainability-related criterion in production.The rest of the paper is organized as follows. In Section 2, the problem to be studied is stated, and some useful problem properties are investigated. In Section 3, we develop the extended NEH-Insertion procedure for the problem. Section 4 presents the MONEH algorithm and the MMOIG algorithm. In Section 5, computational results are provided to show the effectiveness of the proposed procedure and algorithms. In Section 6, we give some concluding remarks and highlight some future research directions.The multi-objective PFS problem is described as follows.•Each of the n jobs from the setJ={1,2,…,n}is to be processed sequentially through m machines denoted byM={1,2,…,m}in the same order with respect to the indices of machines. One jobj∈Jmay start its processing at machinei∈Monly after its operation on the previous machine(i−1)∈Mhas been completed. In addition, each machine is allowed to process at most one job at a time.There is a finite and discrete set of s different processing speedsS={v1,v2,…,vs}for each machine, and the processing speed of a machine cannot be changed during its execution of a job. Every jobj∈Jhas a given processing requirement pijon every machinei∈M. When a jobj∈Jis processed on machinei∈Mat speedv∈S,the corresponding processing time is pijv= pij/v and the corresponding power consumption per unit time is PPiv.The machines will not be turned off completely until all jobs are finished. Instead, each machinei∈Mwill be running at the stand-by mode with power consumption SPiper unit time when no job is processed on it.The first objective of the problem is to minimize the makespan Cmax, which is defined as the completion time of the last job on the last machine.The second objective of the problem is to minimize the total carbon emissions (TCE) throughout the manufacturing period. Let yiv(t) be a binary variable that is equal to 1 if machine i is in processing mode at speed v at time t, and 0 otherwise. Let zi(t) be a binary variable that is equal to 1 if machine i is in stand-by mode at time t, and 0 otherwise. Thus, TCE can be calculated as follows:(1)TEC=∫0Cmax(∑v=1s∑i=1mPPiv·yiv(t)+∑i=1mSPi·zi(t))dt,(2)TCE=ϵ×TEC,where TEC is the total energy consumption and ε refers to the carbon emissions per unit of consumed energy (kilogram CO2 equivalent/kiloWatthour).To illustrate the multi-objective PFS problem introduced above, the Gantt chart of a permutation flow shop with 3 jobs and 4 machines is presented in Fig. 1. Since the makespan Cmax is the completion time of the last job leaving the system, it can be easily obtained as shown in the figure.A schematic real-time power consumption curve for the previous PFS problem is then provided in Fig. 2. We take the situation at time t0 as an example to show how this curve is depicted. It is observed in Fig. 1 that machine 1 and machine 3 are in stand-by mode at time t0 and their real-time power consumption are SP1 and SP3, respectively. Suppose that machine 2 is processing at speedu∈Sand machine 4 is processing at speedv∈Sat time t0, and thus the real-time power for them are PP2, uand PP4, v, respectively. The real-time power consumption for the system at time t0 is calculated by summing up the power of the four machines, that is P(t0) = SP1 + PP2, u+ SP3 + PP4, v. The system power at different time points is calculated similarly, and in this way the system power curve is depicted. With this curve, the total energy consumption TEC is obtained by calculating the area under it and above the time axis. Finally, the total carbon emissions TCE is computed according to Eq. (2).Once the permutation schedule is determined and the processing speed for all operations are assigned, a feasible solution is obtained. Therefore, a solution for the considered problem can be expressed by a permutation vector Π and a speed matrix V as follows:Π=(π(1),π(2),…,π(n)),V=[v(1,1)v(1,2)⋯v(1,n)v(2,1)v(2,2)⋯v(2,n)⋮⋮⋮⋮v(m,1)v(m,2)⋯v(m,n)],whereπ(j)∈Jrepresents the j-th job to be processed on each machine, andv(i,j)∈Srepresents the operation speed of job j on machine i. For notational convenience, a feasible solution is denoted by (Π, V) and the corresponding makespan and total energy consumption are denoted as Cmax(Π, V) and TEC(Π, V), respectively. Note that the only difference between total carbon emissions and total energy consumption is a constant coefficient according to Eq. (2), and thus optimizing TEC or TCE will produce equivalent results. For the sake of expression simplicity, we will consider the total energy consumption (TEC) as the environmentally-related criterion in the rest of the paper.The size of the search space for the considered problem is quite large. We can easily conclude from the solution representation that the size is as large as smn· n!, which is much larger than the n! size for the traditional PFS problem. This significant expansion in solution space will inevitably introduce new computational challenges in the progress of seeking for (near) optimal solutions. Therefore, problem properties and accelerating techniques are to be explored to improve the search efficiency and enhance the solution qualities.The considered problem in this work is a multi-objective optimization problem. No absolutely optimal solution exists for this problem due to the obvious contradiction between the makespan and the total energy consumption. However, a set of Pareto optimal (non-dominated) solutions can be obtained by making a trade-off between the two objectives (i.e., by sacrificing one objective in the interest of the other objective). In this section, we will explore some specific properties of the problem, which will help us to exclude plenty of dominated solutions in the search progress.Before the discussion of problem properties, we give the following assumption for the considered problem, similarly to the work of Fang et al. (2012).Assumption 1It is assumed that when a jobj∈Jis processed at a higher speed on machinei∈M,the power consumption increases while its processing time decreases. This implies that∀u>v(u,v∈S),(3)piju<pijv,(4)PPiu·piju>PPiv·pijv.Based on the assumption, we investigate the following properties for the considered problem.Property 1If we fix the processing speed of all operations, one solution (Π1, V1) will be dominated by another solution (Π2, V2) if the latter solution has a smaller makespan. That is, if Cmax (Π1, V1) > Cmax (Π2, V2) and V1 = V2, we have(5)(Π1,V1)≺(Π2,V2),where (Π1, V1)≺(Π2, V2) means that solution (Π1, V1) is dominated by solution (Π2, V2).The total energy consumption can be written as(6)TEC(Πk,Vk)=TECs(Πk,Vk)+TECp(Πk,Vk),k=1,2,where TECs( · ) and TECp( · ) represents the total power consumed in the stand-by mode and in the processing mode, respectively.Since V1 = V2, it follows that the processing time and power consumption for each operation are the same for the two solutions. Therefore, the total power consumption in the processing mode are the same for the two solutions, i.e.,(7)TECp(Π1,V1)=TECp(Π2,V2)Lettis(Πk,Vk)andtip(Πk,Vk)denote the total stand-by time and the total in-processing time of machine i for solution (Πk, Vk) (k = 1, 2), respectively. Since one machine is either in stand-by mode or in processing mode during the manufacturing period, it is obvious that(8)tis(Πk,Vk)+tip(Πk,Vk)=Cmax(Πk,Vk),∀i=1,…,m,k=1,2.Becausetip(Π1,V1)=tip(Π2,V2),∀i=1,…,mand Cmax(Π1, V1) > Cmax(Π2, V2), it further implies thattis(Π1,V1)>tis(Π2,V2),∀i=1,…,m.Therefore, we haveTECs(Π1,V1)=∑i=1mtis(Π1,V1)·SPi>∑i=1mtis(Π2,V2)·SPi=TECs(Π2,V2).The above equation together with Eqs. (6–7) establishes the following result:TEC(Π1,V1)>TEC(Π2,V2).As Cmax(Π1, V1) > Cmax(Π2, V2), it follows that(Π1,V1)≺(Π2,V2).□If we fix the makespan of solutions, one solution (Π1, V1) will be dominated by another solution (Π2, V2), if the previous solution has a faster processing speed. That is, we have(9)(Π1,V1)≺(Π2,V2),if Cmax (Π1, V1) = Cmax (Π2, V2) and ∀i ∈ {1,…,m}, j ∈ {1,…,n}, v1(i, j) ≥ v2(i, j) and V1 ≠ V2.Similarly to the proof of Property 1, Eq. (8) holds for our problem. As (Π1, V1) possesses a faster processing speed and that the makespan of the two solutions are the same, it follows thattip(Π1,V1)≤tip(Π2,V2),∀i=1,…,m,tis(Π1,V1)≥tis(Π2,V2),∀i=1,…,m.In addition, the above inequalities will hold strictly for somei∈Msince V1 ≠ V2. Therefore,TECs(Π1,V1)=∑i=1mtis(Π1,V1)·SPi>∑i=1mtis(Π2,V2)·SPi=TECs(Π2,V2).By taking Assumption 1 into account, we haveTECp(Π1,V1)>TECp(Π2,V2).Thus, we obtain the following result:TEC(Π1,V1)>TEC(Π2,V2).As Cmax(Π1, V1) = Cmax(Π2, V2), we conclude that(Π1,V1)≺(Π2,V2).□The carbon-efficient PFS problem is receiving research attention only recently, while the traditional PFS problem has been a focus of extensive research for about six decades (Gupta & Stafford, 2006). Various heuristics and meta-heuristics have been proposed for the traditional PFS problem. The famous NEH algorithm, which was first proposed by Nawaz et al. (1983), has been proved to be the most efficient heuristic for the PFS problem to minimize makespan (Kalczynski & Kamburowski, 2007). The NEH-Insertion procedure, which is the core process of the NEH algorithm, was adopted by many researchers to improve the performance of other meta-heuristics (Ruiz & Stützle, 2007; Taillard, 1990). It is expected that the NEH-Insertion will be effective as well for the multi-objective PFS problem considered in this work if we make a proper extension to it.The original NEH-Insertion procedure for the traditional PFS problem is stated as follows and graphically illustrated in Fig. 3. For a partial sequence Π with l jobs, try inserting a new jobj∈Jto each of the l + 1 possible positions in it. Select the sequence with the minimum resulting makespan among the l + 1 candidates as the new partial sequence.Note that the traditional PFS problem does not consider speed scaling, and thus the processing time of each job on each machine is a predetermined value. For the sake of notational consistency, we hold the speed matrix fixed by setting V = V0 for all solutions when we introduce the original NEH-Insertion. Thus the processing time of job π(j) on machine i is expressed aspi,π(j),v0(i,π(j)).As shown in Fig. 3, there are a total of l + 1 possible solutions. Evaluating the makespan of each solution requires O(ml) computational effort, and thus the overall computational complexity of the NEH-Insertion procedure is O(ml2) (Nawaz et al., 1983). As the number of l can be as large as 500 for some large-scale instances, calling this procedure many times will be very time-consuming when dealing with such instances. To improve its efficiency, Taillard (1990) developed a speed-up method and reduced the computational burden of this procedure from O(ml2) to O(ml).The speed-up method first computes the earliest completion time of π(j) on machine i (Ei, j) and the tail (Qi, j) which is defined as the duration between the latest starting time of π(j) on machine i and the end of all the operations. The earliest completion time matrix E and the tail matrix Q are calculated as follows:(10)Ei,0=0,i=1,…,m,(11)E0,j=0,j=1,…,l,(12)Ei,j=max{Ei−1,j,Ei,j−1}+pi,π(j),v0(i,π(j)),i=1,…,m,j=1,…,l,(13)Qi,l+1=0,i=1,…,m,(14)Qm+1,j=0,j=1,…,l,(15)Qi,j=max{Qi+1,j,Qi,j+1}+pi,π(j),v0(i,π(j)),i=1,…,m,j=1,…,l,Then, the method calculates the earliest completion time of the inserted job q on machine i (Fi, k) if the new job is inserted at the k-th position. The F matrix is computed as:(16)F0,k=0,k=1,…,l+1,(17)Fi,k=max{Fi−1,k,Ei,k−1}+pi,q,v0(i,q),i=1,…,m,k=1,…,l+1,Based on matrices F and Q, the makespan Cmax(Πk, V0) is given by:(18)Cmax(Πk,V0)=maxi=1,…,m(Fi,k+Qi,k),k=1,…,l+1,where Πkis the sequence generated by inserting job q at the k-th position. For more details about this speed-up, please refer to the work of Taillard (1990).The original NEH-Insertion procedure focuses on the makespan minimization in the traditional PFS problem. In this work, however, energy consumption is also taken into consideration based on the speed scaling framework. Therefore, the following problems are to be considered if we expect to extend the original NEH-Insertion to our multi-objective problem.1.In the original NEH-Insertion Procedure, the insertion position for a new job q is selected for minimizing the makespan criterion only. In the extended procedure, however, we also wish to reduce the energy consumption. Then how could we make a good trade-off between the two objectives?For the original NEH-Insertion procedure, Taillard’s speed-up method can be applied to improve its time efficiency. Then, is it also possible for us to devise a speed-up method for the multi-objective problem?To solve the first problem, we resort to the problem properties discussed in the previous section. Letvq=(v(1,q),v(2,q),…,v(m,q))Tdenote the processing speed of the inserted job q and is initialized asvq0. According to Property 1, the solution with the minimum makespan will be the only non-dominated solution among the l + 1 candidates if we fixvq=vq0. This is exactly the same as in the traditional NEH-Insertion. However, some new and better non-dominated solutions can be obtained in the speed scaling framework for the considered multi-objective problem. According to Property 2, if we slow down some operations of job q while keeping the resulting makespan unaffected, energy consumption will be reduced and better solutions are generated. Since the operations take discrete speed levels, it is of great importance to identify the minimum processing speed that an operation of job q can take in order to keep the resulting makespan unchanged.The most straightforward approach of identifying the minimum speed of an operation is to directly calculate the resulting makespan under different speed settings. This approach takes O(ml) time for the calculation under a certain processing speed. A better approach is to calculate the resulting makespan using Taillard’s method. This approach takes O(m) computational time on average, provided that the E and Q matrices are computed in advance. In the following, we describe a more efficient energy saving method as Procedure 1. This approach can verify whether the makespan will increase under a certain processing speed in O(1) time on average.The energy saving method considers slowing down some operations of job q, when q is inserted at the k-th position. The operation speed for job q is initialized asvq0and is denoted asvqkafter the speed adjustment by this procedure. The makespan Cmax(Πk, V0) should not change when we slow down the operations. Matrices E, Q and F are calculated in advance using Taillard’s method according to Eqs. (10–17). The meaning of other variables in Procedure 1 is graphically illustrated in the Appendix.Line 1 of the procedure calculates the maximum processing time increment Δmfor job q on the last machine m while keeping the makespan unaffected. This can be easily verified according to Eqs. (17–18). Lines 2–3 of the procedure minimize the operation speed for job q on machine m, with respect to Δm. Since the operations take discrete speeds, the processing time increment may not be as large as Δmon machine m, and therefore the starting time of job q on machine m can be delayed by at most Gapmunits of time as calculated in Line 5 by setting i = m − 1. Next, we calculate the maximum processing time increment for job q on machine m − 1. This value is determined by two constraints. The first constraint is that the increment on machine m − 1 should not cause any further delay of job q on machine m, as stated in Line 6. The second constraint is that the increment should not cause any delay of the subsequent operation on the same machine, as stated in Line 7. Then the maximum processing time increment Δm − 1 is determined and the minimum operation speed on machine m − 1 is obtained, as described in Lines 8–10. The speeds on the other machines are calculated similarly to the case of machine m − 1 using the steps in Lines 4–11. The energy saving procedure is terminated after the adjusted speeds on all machines are determined. It is noticeable that none of the operations of job q can get further slowdown under this permutation without affecting the makespan or the processing speed of another operation. To make a better understanding of this procedure, we demonstrate it using a 4-machine example in the Appendix.The presented energy saving method is actually a greedy heuristic. Starting from the last machine, this method considers decreasing each machine’s operation speed for job q to the largest extent. Although this approach does not always generate the best slowdown result due to its greedy nature, it can provide a good balance between the energy performance and the execution time of the procedure. The main computational burden in this procedure lies in the selection of the possible minimum vk(i, q) in Line 9. This step requires O(log s) time using a binary search. Since there are a total of m − 1 iterations, the computational burden of the energy saving procedure for one insertion position is O(mlog s). Taking all the l + 1 possible positions into account, the overall computational complexity for energy saving in the extended NEH-Insertion is O(mllog s). Note that the total number of evaluated operation speeds is also O(mllog s), so the average computational effort to verify whether the makespan will increase under a certain processing speed is O(1). This complexity result is quite encouraging compared with other potential heuristics and exact methods. Faced with the situation that the solution space is incredibly large and that the computational resource is limited, lower complexity means more regions in the solution space are to be explored in the limited running time of an algorithm.The original NEH-Insertion procedure achieves a good performance on makespan minimization and the energy saving method can reduce the energy cost without worsening the makespan. A large number of dominated solutions are identified and excluded by this procedure according to Property 2, while none of them have been evaluated accurately. Thus, the search trajectory is quickly directed to more promising regions in the solution space, and the solution quality gets improved efficiently. In addition, new non-dominated solutions can be generated since the energy saving method gives different results for different insertion positions. This indicates that the solution diversity is also enlarged. Therefore, the energy saving method can make a good trade-off between the two objectives. The effectiveness of this method will be tested and verified by computational analysis in Section 5.To solve the second problem raised at the beginning of Section 3.2, we expect to accelerate the evaluation of both objectives for the solutions generated in the extend NEH-Insertion procedure and select the non-dominated solutions among them.The makespan of each candidate solution does not change in the energy saving procedure, and thus equals to the result from the original NEH-Insertion. Let (Πk, Vk) denote the solution generated by inserting job q at the k-th (k = 1,…,l + 1) position in the partial sequence after energy saving. It follows that:(19)Cmax(Πk,Vk)=Cmax(Πk,V0)=maxi=1,…,m(Fi,k+Qi,k),k=1,…,l+1,where the F and Q matrices are calculated according to Eqs. (10–17). It is easily verified that the computational complexity for the makespan evaluation is O(ml).Then we consider the evaluation of the total energy consumption TEC. There are a total of l + 1 candidate solutions and calculating the TEC for each solution requires O(ml) time. Therefore, the computational complexity will be O(ml2). However, we notice that all the l + 1 candidate solutions (Πk, Vk) (k = 1,…,l + 1) are generated from the same initial solution (Π, V0). So it may help to reduce the computational burden if we consider the difference in energy consumption between the candidates and the initial solution. Let ΔTECkdenote the increment of energy consumption caused by inserting job q at the k-th position. It follows that:(20)ΔTECk=TEC(Πk,Vk)−TEC(Π,V0)=(TECp(Πk,Vk)−TECp(Π,V0))+(TECs(Πk,Vk)−TECs(Π,V0))=∑i=1mpi,q,vk(i,q)·PPi,vk(i,q)+∑i=1m((Cmax(Πk,V0)−Cmax(Π,V0))−pi,q,vk(i,q))·SPi,k=1,…,l+1.where TECs( · ) and TECp( · ) represents the total power consumed in stand-by mode and in processing mode, respectively. Thus, we can reduce the computational complexity for evaluating the total energy consumption to O(ml) according to the above equation.After the makespan Cmax(Πk, Vk) and the energy increment ΔTECkfor all the l + 1 candidate solutions are obtained, the non-dominated solutions will be selected as the output of the extended NEH-Insertion procedure. We present an O(llog l) procedure as Procedure 2for the selection of non-dominated solutions, similarly to the work of Kung, Luccio, and Preparata (1975).Finally, we summarize the extended NEH-Insertion procedure as Procedure 3. The computational complexity of the extended NEH-Insertion procedure is summarized as follows: Line 1 and Line 3 both take O(ml) time, Line 2 takes O(mllog s) and Line 4 takes O(llog l). Therefore, the overall complexity of the extended NEH-Insertion procedure is O(mllog s + llog l). Compared with the O(ml) complexity of the original accelerated NEH-Insertion, the extended one requires a bit more computation, but it is able to obtain a satisfactory trade-off between the two objectives in the speed scaling framework. Hence, this extension is suitable for the considered problem, and new heuristics and meta-heuristics are to be developed based on this extension.The problem studied in this work is expected to require more computational effort than the original PFS problem, which is already NP-hard. Therefore, we develop heuristic and meta-heuristic algorithms for this multi-objective PFS problem. For the heuristic, a multi-objective NEH (MONEH) algorithm is designed. For the meta-heuristic, a modified multi-objective iterated greedy (MMOIG) algorithm is proposed. Both approaches incorporate the extended NEH-Insertion procedure in the algorithm framework, and thus the problem properties will be fully utilized in the search progress.The original NEH algorithm is the most effective constructive heuristic for minimizing makespan in the PFS problem (Kalczynski & Kamburowski, 2007). Starting from an initial sort of jobs in non-ascending order of their total processing times, it constructs a complete solution by inserting these jobs into the existing partial schedule one after another, using the original NEH-Insertion method.However, the NEH algorithm has not so far been applied to multi-objective PFS problems. To do so, we present a multi-objective NEH algorithm based on the extended NEH-Insertion procedure for the considered problem. Unlike the original NEH algorithm, the MONEH algorithm generates a set of non-dominated partial solutions when a new job is inserted into the sequence. The main idea of MONEH is as follows. Let NSjbe the non-dominated set of partial solutions with j jobs (j = 1,…,n). NS1 is initialized with only one partial solution: a sequence of one job and a given speed Matrix. Then the algorithm performs n − 1 iterations to construct a set of non-dominated complete solutions. In the k-th iteration, a new job will be inserted into the sequences in the partial solution set. The extended NEH-Insertion procedure is applied to determine the insertion positions as well as the new speed matrices for every partial solution in NSk. By selecting the new non-dominated partial solutions containing k + 1 jobs generated by the extended NEH-Insertion, the set of non-dominated partial solutions of k + 1 jobs (denoted as NSk + 1) is obtained. To prevent the explosion in solution numbers, some of the crowded non-dominated partial solutions are deleted according to the crowding distance. The crowding distance is calculated according to Procedure 4proposed by Deb et al. (2002).The detailed procedure of the MONEH algorithm is described in Algorithm 1. The inputsC=(C1,…,Ct)andE=(E1,…,Et)are the makespan and energy consumption value of the t non-dominated partial solutions to be selected. The notation NSk + 1∪udSiin Line 7 represents the set of non-dominated solutions picked from the union of the two sets NSk + 1 and Si. The expression |NSk + 1| in Line 9 refers to the number of elements in the set NSk + 1. The complete solution set NSnconstitutes the non-dominated front discovered by this algorithm.Note that the MONEH algorithm described here starts with an initial speed matrix V. A high level of initial speed will lead to a good performance in minimizing makespan but a relatively poor performance in saving energy consumption, while a low level of initial speed will produce the opposite effect. Therefore, we initialize the speed matrix at different speed levels and run the MONEH algorithm several times. By combining the Pareto fronts at different initial speed levels and selecting the non-dominated and less crowded solutions, we will obtain a better and broader discovered front for the considered problem.The iterated greedy (IG) algorithm is a simple yet effective stochastic local search meta-heuristic for various types of flowshop problems (Ribas, Companys, & Tort-Martorell, 2011; Ruiz & Stützle, 2007). This algorithm consists of two core phases, i.e., the destruction phase and the construction phase. In the destruction phase, some jobs are randomly removed from a complete sequence, and thus a partial sequence is generated. In the construction phase, the removed jobs are reinserted into the partial sequence at the best position one after another. The two phases iterate until a predetermined termination condition is met.The main idea of this algorithm is also applied by researchers to solve the multi-objective flowshop problems (Framinan & Leisten, 2008; Minella, Ruiz, & Ciavotta, 2011). However, none of the previous work about multi-objective IG algorithm have considered the speed scaling framework and the objective of reducing energy consumption. To adapt IG to our problem, we firstly modify the destruction and construction phases which will help to make the algorithm more suitable for the considered problem. In the modified destruction phase, d jobs are removed from a complete sequence and the processing speed of each operation from the removed jobs get changed randomly with probability ρ. In the modified construction phase, the removed jobs are reinserted into the partial sequences one after another using the extended NEH-Insertion procedure, and thereby a set of non-dominated solutions are obtained. Then, a two-step local search method is developed to further improve the solution quality, which will lead to better Pareto fronts. The local search method will be discussed shortly in Procedure 5. Based on these settings, a modified multi-objective IG (MMOIG) algorithm is proposed to solve the problem. The pseudo-code of this algorithm is given in Algorithm 2.As shown in the pseudo-code of the MMOIG algorithm, Lines 1–2 initialize the solution set randomly under different speed levels. Thus, a set of non-dominated solutions NCSoriis obtained and then the iterations start. For each solution in NCSori, the destruction phase and construction phase are applied. Lines 6–7 describe the destruction phase and Lines 8–14 describe the construction phase. The notation NPSjdenotes the non-dominated partial solution set with j (j ∈ {0, 1,…,d}) jobs re-inserted into the destructed solutions. Once the two phases for one solution are completed, the current non-dominated solution set NCScuris updated as shown in Line 15. When all the solutions from NCSoriare reconstructed, a two-step local search method is applied to further improve the solution quality of the current set NCScur. Finally, Lines 20–23 select the non-dominated and less crowded solutions from the two sets: NCSoriand NCScur. The selected solutions are the initial solutions for the next iteration. The iterations terminate when a certain stopping condition is met.The design of the two-step local search method largely benefits from Properties 1 and 2 introduced in Section 2.2. Property 1 indicates that one solution (Π, V) will get improved if we hold the speed matrix V fixed and make adjustment to the permutation Π to achieve a smaller makespan. On the other hand, Property 2 shows that one solution will also get improved if we hold the permutation Π fixed and slow down some operations in the speed matrix V while keeping the makespan unchanged. Therefore, in the first step of the proposed local search procedure, we fix V and search for the permutation Π in its insertion neighborhood (Ruiz & Stützle, 2007) that results in the best makespan. To improve the time efficiency of implementing this step, the original NEH-Insertion procedure with Taillard’s acceleration is adopted in this step. In the second step, the energy saving procedure is applied to slow down some operations while stabilizing the makespan. The pseudocode of this procedure is provided as Procedure 5.This section describes the computational results to evaluate the proposed MONEH algorithm and MMOIG algorithm for minimizing makespan and total energy consumption in the multi-objective PFS problem. Their performances are measured by comparisons with the results of the well-known NSGA-II (Deb et al., 2002) algorithm, as well as the results of two recently published algorithms for energy-efficient production scheduling: AMGA (Liu & Huang, 2014) and IGSA (Dai et al., 2013). In addition, since the extended NEH-Insertion procedure is the core process of both algorithms, its effectiveness and efficiency is also validated through comparisons. All the algorithms are coded in Matlab® and are executed on a PC with 3.20 Gigahertz frequency and 4GB RAM.To evaluate the proposed methods under different problem sizes, we need to consider flow shop scheduling problems with different machine numbers and job numbers. Thus, we generate a set of different-sized instances that are representative of the industrial data. The factors that influence the performance of the manufacturing system are identified and listed in Table 1. For the simplicity of presentation, an instance with n jobs and m machines is denoted as an n × m instance.It should be noted that the performance of the compared algorithms are affected by algorithm’s parameter settings. Pilot experiments were conducted under a set of potential parameter values to find the best combinations. The maximum population size maxPop is a common parameter for all the five tested algorithms and is selected as maxPop = 25 to make a fair comparison. For the MMOIG algorithm, the number of destructed jobs (d) and mutation probability (ρ) in the destruction phase are the major parameters. For the NSGA-II, AMGA, and IGSA algorithms, mutation probability pmand crossover probability pcare crucial common parameters. In the following experiments, we set d = 3, ρ = 0.4 for MMOIG and set pm= 0.10, pc= 0.80 for NSGA-II, pm= 0.05, pc= 0.90 for AMGA, and pm= 0.01, pc= 0.60 for IGSA algorithms. Parameter θ and δ in AMGA are selected as 0 and 50, respectively, and parameter α in IGSA is selected to be 0.01. In addition, the performance of NSGA-II, AMGA, IGSA and MMOIG are affected by the termination condition, i.e., the number of iterations (Nite) or the maximum execution time (tmax ). These settings take different values in different experiments and thus will be specified later.Note also that the initial speed matrices can influence the performance of the tested algorithms. To ensure solution diversity, we initialize the speed matrices under 10 different speed levels. The k-th level speed matrixVinitkis obtained as follows (k ∈ {1,…,10}). For each elementvinitk(i,j)inVinitk(i∈M,j∈J), randomly generate 10 integers in the set {1,…,s}. Select vI(k) as the value of this element, where I(k) denotes the k-th largest one of the generated integers.The extended NEH-Insertion procedure is the core process of the proposed MONEH and MMOIG algorithms. This procedure is developed based on two techniques. The first technique is the energy saving method, which aims at making a good trade-off between both objectives. The second technique is the speedup evaluation method, which aims at improving the time efficiency. Therefore, we demonstrate the effectiveness of the extended procedure from these two aspects through computational analysis.First, the energy saving method is evaluated. To show the contribution of this method in the extended NEH-Insertion, computational experiments are designed as follows. For one instance, an algorithm A is run twice, A ∈ {MONEH, MMOIG}. In the first run, A is executed according to the algorithm description in previous sections without any modification. In the second run, however, A is executed without implementing the energy saving method in its running process. By comparing the discovered front obtained in the two runs, the contribution of the energy saving method is identified. For notational convenience, the algorithm executed in the first and second run is denoted as A1 and A2, respectively.We depict in Fig. 4 the Pareto fronts for the 40 × 8 instance as an example to show the contribution of the energy saving method to the MONEH and MMOIG algorithms. It is observed from the figure that the energy saving method helps to improve the discovered fronts significantly for both algorithms. This result clearly demonstrates the importance of implementing the energy saving method. Note that for the MONEH algorithm, MONEH2 also provides some non-dominated solutions where the makespan is relatively small while the energy consumption is high. This situation can be explained by the fact that the energy saving method in MONEH1 slows down some operations in its running process, and thus the average makespan obtained by MONEH1 tends to be larger than that obtained by MONEH2 since it is a one-pass constructive algorithm. For the MMOIG algorithm, such a situation does not arise since the processing speeds of operations have chance to be changed in the destruction phase in each iteration.Next, we show the contribution of the speedup evaluation method. To do this, computational experiments are designed as follows. For one instance, an algorithm A is run twice, A ∈ {MONEH, MMOIG}. In the first run, A is executed with the speedup evaluation method. In the second run, however, the speedup technique is not applied. Note that the MMOIG algorithm is implemented without the local search procedure here and is allowed to run for 500 iterations. Most computational burden of both algorithms lies in the execution of the extended NEH-Insertion procedure. Therefore, the contribution of the speedup method can be clearly identified by comparing the CPU time consumed in the two runs.Table 2shows the time consumption of the executed algorithms with and without the speedup method. It is revealed from the table that the average CPU time is reduced from 114 seconds to 8.94 seconds for the MONEH algorithm, and from 2159.73 seconds to 175.44 seconds for the MMOIG algorithm when the speedup technique is applied. Both algorithms get accelerated for at least 12 times due to the speedup method. Therefore, we conclude that the time efficiency of both MONEH and MMOIG algorithms gets improved significantly when the accelerated evaluation method is applied. It is also noticeable that the acceleration ratio is higher for the large-scale instances, which indicates that the proposed method can save more computational effort for the large-scale problems.The performance of the proposed algorithms are evaluated by comparisons with the well-known multi-objective optimization algorithm NSGA-II, as well as two recently published algorithms for energy-efficient production scheduling: AMGA (Liu & Huang, 2014) and IGSA (Dai et al., 2013). NSGA-II algorithm is a general optimization framework, which is also suitable for the multi-objective scheduling problem in this study. To apply the AMGA and IGSA algorithms, however, some modifications are required. AMGA and IGSA were initially designed for the carbon efficient hybrid flowshop, and flexible flowshop scheduling problem, respectively. To adapt them to the problem at hand, we also consider the “(Π, V)” encoding scheme for them to represent a feasible solution as in MMOIG. Note that the original IGSA optimizes a weighted additive utility function once at a time. In order to obtain the Pareto frontier, we run IGSA multiple times with different weighing factors:w1=0,w1=1maxPop−1,w1=2maxPop−1,…,w1=1,and each run is allowed for an execution time oftmaxmaxPop.Unlike the evaluation of single-objective optimization algorithms, there is no simple criterion to measure the performance of different multi-objective optimization algorithms. On the one hand, excellent searching ability is desired, which means to find solutions that are close to the optimal Pareto front. On the other hand, widely spread solutions are required, which helps the decision maker to choose a satisfactory solution according to his/her preference. In this paper, we adopt the following indicators to reflect the quality of solutions.(1)Coverage metric (CM) (Zitzler, 1999): this metric is used to compare two solution sets A and B (obtained by two different algorithms). It maps the ordered pair (A, B) to a value in the interval [0, 1] as follows:(21)C(A,B)=|{b∈B|∃a∈A:a≻bora=b}||B|.The value of C(A, B) reflects the dominance relationship between the solutions in the two sets. If all the solutions in B are dominated by some solutions in A, then C(A, B) = 1. On the contrary, if none of the solutions in B are dominated by any solution in A, then C(A, B) = 0. Note that C(A, B) is not necessarily equal to 1 − C(B, A) since some solutions in A and B may not be dominated by each other.Distance metrics (Dav, Dmax ) (Czyzżak & Jaszkiewicz, 1998; Knowles & Corne, 2002): these metrics are used to measure the quality of a solution set A relative to a reference set R. The reference set consists of either the Pareto optimal solutions, or the high-quality non-dominated solutions. Since the Pareto optimal solutions are not known for the considered problem, we obtain the set R by selecting all the non-dominated solutions found by the three algorithms. The distance metrics are defined as follows:(22)Dav=1|R|∑xR∈Rminx∈Ad(x,xR),(23)Dmax=maxxR∈R{minx∈Ad(x,xR)},where d(x, xR) = maxj = 1, …, N{(fj(x) − fj(xR))/Δj} and Δjis the range of the j-th objective function fj. Obviously, a smaller value of Dav and Dmax  suggests better algorithm performance.Distribution spacing (DS) (Tan, Goh, Yang, & Lee, 2006): this metric describes how evenly the obtained solutions are distributed along the discovered front. It is defined as follows:(24)DS=1|A|∑i=1|A|(Di−D¯)2/D¯,whereD¯=∑i=1|A|Di/|A|and Diis the Euclidian distance between solution i and its nearest solution (in the objective space). A smaller DS suggests a more even distribution of the solutions in A.Running time (tmax ): This metric describes the execution time of an algorithm. It is a key metric to reflect the efficiency of an algorithm.To make a fair comparison, the MMOIG, NSGA-II, AMGA, and IGSA algorithms are allowed to run for the same execution time.11The original AMGA iterates until a given number of iterations are reached. The weighing factor w1 increases gradually from 0 to 1 during the iterating process. To make AMGA terminate within a given execution time tmax, we record the present running time during the algorithm’s iterating process and increase the value of w1 by estimation of future number of iterations. Assume that in the k-th iteration, the present execution time is tk, then we change the value of w1 for the next iteration in the following manner:w1←w1+1−w1(tmax−tk)/(tk/k),where the expression (tmax − tk)/(tk/k) can be viewed as the estimated number of future iterations.Since different computational efforts are required for different-scale instances, the maximum execution time shall be determined according to the problem size. In the following computational experiments, we set the running time of the MMOIG, NSGA-II, AMGA, and IGSA algorithms to be 5 · m · n2 milliseconds. As for the MONEH algorithm, the running time cannot be predetermined due to the nature of this algorithm. For each instance, all the tested algorithms are run 10 times and the performance metrics averaged across the 10 runs are collected.The relative solution quality achieved by the considered algorithms are reported in terms of the coverage metric in Tables 3and 4. Table 3 presents coverage metric comparisons between the MONEH algorithm and NSGA-II, AMGA, and IGSA algorithms. It is revealed from the table that 74 percent of solutions generated by NSGA-II algorithm are dominated by those generated by the MONEH algorithm, while the percentage is only 2 percent if we reverse the positions of these two algorithms. The result is similar when comparing MONEH with AMGA or IGSA. Hence, the proposed MONEH algorithm outperforms NSGA-II, AMGA, and IGSA algorithms in relative solution quality, which means that MONEH has a better searching ability than the other tested meta-heuristics in literature for the considered problem.Table 4 reports coverage metric comparisons between the MMOIG algorithm and NSGA-II, AMGA, IGSA, MONEH algorithms. It is observed from the table that MMOIG performs much better than NSGA-II, AMGA, and IGSA since only 0–1 percent of the solutions generated by MMOIG are dominated by those generated by NSGA-II, AMGA, or IGSA. Note that the coverage metrics between the two proposed algorithms (i.e., MONEH and MMOIG) are also presented in the table. The MMOIG algorithm shows obvious advantage over the MONEH algorithm. Such a situation can be explained by the fact that MMOIG runs for many iterations while MONEH is a one-pass algorithm. More solutions are explored and evaluated in the search progress of MMOIG, thus leading to better performance.Table 5shows the statistical performance metrics obtained by each of the five algorithms. The best results for each metric from the algorithms are highlighted in bold font. It is observed in the table that the MMOIG algorithm performs the best among the tested algorithms in terms of the Dav and Dmax  metrics, which means that MMOIG achieves the most proximity to the reference front. For the comparison between MONEH algorithm and NSGA-II, AMGA, IGSA algorithms on these two metrics, the results are quite interesting. The MONEH algorithm outperforms NSGA-II, AMGA, and IGSA in Dav metric, while the NSGA-II, AMGA, and IGSA algorithms in turn outperform MONEH in Dmax  metric. This result indicates that the MONEH is closer to the reference front in the average sense, while the NSGA-II, AMGA, and IGSA algorithms is closer to the reference front in the max–min sense. This situation arises because some non-dominated solutions with high energy consumption and small makespan cannot be found by MONEH due to the algorithm nature. For the comparison of distribution spacing metric, it is clear from the table that the MMOIG algorithm gives the smallest value in average in DS metric among the compared algorithms, which implies that the solutions generated by MMOIG algorithm are spread most uniformly in its own discovered front. As for the tmax  metric, the MONEH algorithm clearly outperforms the other four algorithms in time efficiency. This is due to the fact that the MONEH algorithm is a single-pass constructive algorithm, while the others are more complicated meta-heuristics with many iterations.To make the evaluation results statistically convincing, we conducted paired-sample t-tests to compare the performance of NSGA-II, AMGA, IGSA algorithms and that of the proposed MONEH, MMOIG algorithms. The p-value results for these hypothesis tests are reported in Table 6. The term ‘‘t-test (A,B)” in the first column indicates that a paired t-test is conducted to judge whether algorithm A gives a smaller sample mean than algorithm B for a certain indicating metric. In these comparisons, we assumed a significance level of 0.01 , which means that algorithm B performs better than algorithm A in the statistical sense if the corresponding p-value is smaller than 0.01. The testing results from the table clearly show that MMOIG outperforms NSGA-II, AMGA and IGSA in all the three metrics, and that MONEH outperforms NSGA-II, AMGA, and IGSA in the Dav metric. These statistical results are consistent with previous analysis on the performance comparisons of the tested algorithms, and thus further justify the effectiveness of the proposed algorithms.In order to visualize the solution quality and diversity, we depict the discovered front obtained by each algorithm under different instance scales in Fig. 5. Since there are as many as 15 different scales, we only provide the graphical representation for the 20 × 4, 60 × 8 and 100 × 16 cases. They are selected as typical examples of the small-, medium- and large-scale instances, respectively. These figures give an intuitive illustration for some results derived from the performance metrics analysis. As shown in the figure, the MMOIG algorithm performs the best among the three algorithms in solution quality and diversity. As for the comparison between the MONEH algorithm and the NSGA-II, AMGA, and IGSA algorithms, the MONEH algorithm provides solutions that are relatively closer to the Pareto fronts, whereas the NSGA-II, AMGA, IGSA algorithms provide solutions that are more diversified. This result well explains the above-mentioned performance anomaly between MONEH algorithm and NSGA-II, AMGA, IGSA algorithms in Dav and Dmax  metrics.It can be summarized from the above experiments that the proposed MMOIG algorithm outperforms the other tested algorithms for the considered multi-objective problem. Taking the computational effort into account, it is shown that the MONEH algorithm can provide solutions that are comparable to the solutions generated by NSGA-II, AMGA, and IGSA algorithms within much shorter execution time. Therefore, the proposed algorithms have a better searching ability than the the other high-performing meta-heuristics for the problem studied in this work.

@&#CONCLUSIONS@&#
