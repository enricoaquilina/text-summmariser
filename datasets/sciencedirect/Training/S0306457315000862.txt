@&#MAIN-TITLE@&#
A cross-benchmark comparison of 87 learning to rank methods

@&#HIGHLIGHTS@&#
We propose a novel way to compare learning to rank methods.We perform a meta-analysis on a large set of papers that report ranking accuracy on a benchmark dataset.LRUF, FSMRank, FenchelRank, SmoothRank and ListNet are the most accurate, with increasing certainty.

@&#KEYPHRASES@&#
Learning to rank,Information retrieval,Evaluation metric,

@&#ABSTRACT@&#
Learning to rank is an increasingly important scientific field that comprises the use of machine learning for the ranking task. New learning to rank methods are generally evaluated on benchmark test collections. However, comparison of learning to rank methods based on evaluation results is hindered by the absence of a standard set of evaluation benchmark collections. In this paper we propose a way to compare learning to rank methods based on a sparse set of evaluation results on a set of benchmark datasets. Our comparison methodology consists of two components: (1) Normalized Winning Number, which gives insight in the ranking accuracy of the learning to rank method, and (2) Ideal Winning Number, which gives insight in the degree of certainty concerning its ranking accuracy. Evaluation results of 87 learning to rank methods on 20 well-known benchmark datasets are collected through a structured literature search. ListNet, SmoothRank, FenchelRank, FSMRank, LRUF and LARF are Pareto optimal learning to rank methods in the Normalized Winning Number and Ideal Winning Number dimensions, listed in increasing order of Normalized Winning Number and decreasing order of Ideal Winning Number.

@&#INTRODUCTION@&#
Ranking is a core problem in the field of information retrieval. The ranking task in information retrieval entails the ranking of candidate documents according to their relevance to a given query. Ranking has become a vital part of web search, where commercial search engines help users find their need in the extremely large collection of the World Wide Web. Among useful applications of machine learning based ranking outside web search are automatic text summarization, machine translation, drug discovery and determining the ideal order of maintenance operations (Rudin, 2009). In addition, McNee, Riedl, and Konstan (2006) found the ranking task to be a better fit for recommender systems than the regression task (continuous scale predictions), which is currently still frequently used within such systems.Research in the field of ranking models has long been based on manually designed ranking functions, such as the well-known BM25 model (Robertson & Walker, 1994). Increased amounts of potential training data have recently made it possible to leverage machine learning methods to obtain more effective ranking models. Learning to rank is the relatively new research area that covers the use of machine learning models for the ranking task.In recent years, several learning to rank benchmark datasets have been proposed with the aim of enabling comparison of learning to rank methods in terms of ranking accuracy. Well-known benchmark datasets in the learning to rank field include the Yahoo! Learning to Rank Challenge datasets (Chapelle & Chang, 2011), the Yandex Internet Mathematics 2009 contest,2http://imat2009.yandex.ru/en.2the LETOR datasets (Qin, Liu, Xu, & Li, 2010), and the MSLR (Microsoft Learning to Rank) datasets.3http://research.microsoft.com/en-us/projects/mslr/.3There exists no agreement among authors in the learning to rank field on the benchmark collection(s) to use to evaluate a new model. Comparing ranking accuracy of learning to rank methods is largely hindered by this lack of a standard way of benchmarking.Gomes, Oliveira, Almeida, and Gonçalves (2013) analyzed the ranking accuracy of a set of models on both LETOR 3.0 and 4.0. Busa-Fekete, Kégl, Éltető, and Szarvas (2013) compared the accuracy of a small set of models over the LETOR 4.0 datasets, both MSLR datasets, both the Yahoo! Learning to Rank Challenge datasets and one of the datasets from LETOR 3.0. Both studies did not aim to be complete in benchmark datasets and learning to rank methods included in their comparisons. To our knowledge, no structured meta-analysis on ranking accuracy has been conducted where evaluation results on several benchmark collections are taken into account. In this paper we will perform a meta-analysis with the aim of comparing the ranking accuracy of learning to rank methods. The paper will describe two stages in the meta-analysis process: (1) collection of evaluation results, and (2) comparison of learning to rank methods.We collect evaluation results on the datasets of benchmark collections through a structured literature search. Table 1 presents an overview of the benchmark collections included in the meta-analysis. Note that all these datasets offer feature set representations of the to-be-ranked documents instead of the documents themselves. Therefore, any difference in ranking performance is due to the ranking algorithm and not the features used.For the LETOR collections, the evaluation results of the baseline models will be used from LETOR 2.0,4http://research.microsoft.com/en-us/um/beijing/projects/letor/letor2.0/baseline.aspx.43.05http://research.microsoft.com/en-us/um/beijing/projects/letor/letor3baseline.aspx.5and 4.06http://research.microsoft.com/en-us/um/beijing/projects/letor/letor4baseline.aspx.6as listed on the LETOR website.LETOR 1.0 and 3.0, Yahoo! Learning to Rank Challenge, WCL2R and AOL have accompanying papers that were released with the collection. Authors publishing evaluation results on these benchmark collections are requested to cite these papers. We collect evaluation measurements of learning to rank methods on these benchmark collections through forward literature search. Table 2 presents an overview of the results of this forward literature search performed using Google Scholar.The LETOR 4.0, MSLR-web10/30k and Yandex Internet Mathematics Competition 2009 benchmark collections are not accompanied by a paper. To collect evaluation results for learning to rank methods on these benchmarks, a Google Scholar search is performed on the name of the benchmark. Table 3 shows the results of this literature search.Table A.5 in the appendix gives an overview of the learning to rank methods for which evaluation results were found through the described procedure. Occurrences of L2, L3 and L4 in Table A.5 imply that these algorithms are evaluated as official LETOR 2.0, 3.0 and 4.0 baselines respectively.Some studies with evaluation results found through the literature search procedure were not usable for the meta-analysis. The following enumeration enumerates those properties that made one or more studies unusable for the meta-analysis. The references between brackets are the studies to which these properties apply.1.A different evaluation methodology was used in the study compared to what was used in other studies using the same benchmark (Geng, Qin, Liu, Cheng, & Li, 2011; Lin, Yeh, & Liu, 2012).The study focuses on a different learning to rank task (e.g. rank aggregation or transfer ranking) (Ah-Pine, 2008; Argentini, 2012; Chen et al., 2010; Dammak, Kammoun, & Ben Hamadou, 2011; De, 2013; De & Diaz, 2011, 2012; De, Diaz, & Raghavan, 2010, 2012; Derhami, Khodadadian, Ghasemzadeh, & Zareh Bidoki, 2013; Desarkar, Joshi, & Sarkar, 2011; Duh & Kirchhoff, 2011; Hoi & Jin, 2008; Lin, Yu, & Chen, 2011; Miao & Tang, 2013; Pan, Lai, Liu, Tang, & Yan, 2013; Qin, Geng, & Liu, 2010; Volkovs & Zemel, 2012, 2013; Wang, Tang et al., 2009).The study used an altered version of a benchmark that contained additional features (Bidoki & Thom, 2009; Ding, Qin, & Zhang, 2010).The study provides no exact data of the evaluation results (e.g. results are only in graphical form) (Adams & Zemel, 2011; Agarwal & Collins, 2010; Alejo, Fernández-Luna, Huete, & Pérez-Vázquez, 2010; Benbouzid, Busa-Fekete, & Kégl, 2012; Chang & Zheng, 2009; Chen, Weinberger, Chapelle, Kedem, & Xu, 2012; Ciaramita, Murdock, & Plachouras, 2008; Geng, Yang, Xu, & Hua, 2012; He, Ma, & Niub, 2010; Huang & Frey, 2008; Karimzadehgan, Li, Zhang, & Mao, 2011; Kuo, Cheng, & Wang, 2009; Li, Wang, Ni, Huang, & Xie, 2008; Ni, Huang, & Xie, 2008; Pan, Luo, Tang, & Huang, 2011; Petterson, Yu, Mcauley, & Caetano, 2009; Qin, Liu, Zhang, Wang, Xiong et al., 2008; Sculley, 2009; Shivaswamy & Joachims, 2011; Stewart & Diaz, 2012; Sun, Huang, & Feng, 2011; Swersky, Tarlow, Adams, Zemel, & Frey, 2012; Wang & Xu, 2010; Wang, Kuai, Huang, Li, & Ni, 2008; Wu et al., 2011; Xia, Liu, Wang, Zhang, & Li, 2008; Xu, Chapelle, & Weinberger, 2012; Xu, Kersting, & Joachims, 2010; Zhu, Chen, et al., 2009; Zhou, Ding, You, & Xiao, 2011).The study reported evaluation results in a different metric than the metrics chosen for this meta-analysis (Kersting & Xu, 2009; Mohan, Chen, & Weinberger, 2011; Pahikkala, Tsivtsivadze, Airola, Järvinen, & Boberg, 2009; Thuy, Vien, Viet, & Chung, 2009; Yu & Joachims, 2009).The study reported a higher performance on baseline methods than official benchmark runs (Acharyya, Koyejo, & Ghosh, 2012; Asadi, 2013; Banerjee, Dubey, Machchhar, & Chakrabarti, 2009; Bian, 2010; Bian, Li, Li, Zheng, & Zha, 2010; Carvalho, Elsas, Cohen, & Carbonell, 2008; Dubey, Machchhar, Bhattacharyya, & Chakrabarti, 2009; Peng, Macdonald, & Ounis, 2010; Song, Ng, Leung, & Fang, 2014; Tran & Pham, 2012).The study did not report any baseline performance that allowed us to check validity of the results (Buffoni, Gallinari, Usunier, & Calauzènes, 2011; Chakrabarti, Khanna, Sawant, & Bhattacharyya, 2008; Wang, Bennett, & Collins-Thompson, 2012).Qin, Liu, et al. (2010) statethat it may differ between datasets what the most accurate ranking methods are. They propose a measure they call Winning Number to evaluate the overall performance of learning to rank methods over the datasets included in the LETOR 3.0 collection. Winning Number is defined as the number of other algorithms that an algorithm can beat over the set of datasets, or more formallyWNi(M)=∑j=1n0.35em0ex∑k=1mI{Mi(j)>Mk(j)}where j is the index of a dataset, n the number of datasets in the comparison, i and k are indices of an algorithm, Mi(j) is the performance of the i-th algorithm on the j-th dataset, M is a ranking measure (such as NDCG or MAP), andI{Mi(j)>Mk(j)}is an indicator function such thatI{Mi(j)>Mk(j)}={1if0.35em0exMi(j)>Mk(j),0otherwiseThe LETOR 3.0 was a comparison on a dense set of evaluation results, in the sense that there were evaluation results available for all learning to rank algorithms on all datasets included in their comparison. The Winning Number evaluation metric relies on the denseness of the evaluation results set. In contrast to the LETOR 3.0 comparison, our evaluation results will be a sparse set. We propose a normalized version of the Winning Number metric to enable comparison of a sparse set of evaluation results. This Normalized Winning Number takes only those datasets into account that an algorithm is evaluated on and divides this by the theoretically highest Winning Number that an algorithm would have had in case it would have been the most accurate algorithm on all datasets on which it has been evaluated. We will redefine the indicator function I in order to only take into account those datasets that an algorithm is evaluated on, asIMi(j)>Mk(j)′={1if0.35em0exMi(j)0.35em0exand0.35em0exMk(j)0.35em0exarebothde-finedand0.35em0exMi(j)>Mk(j),0otherwiseFrom now on this adjusted version of Winning Number will be referenced to as Normalized Winning Number (NWN). The formal definition of Normalized Winning Number isNWNi(M)=WNi(M)IWNi(M)where IWN is the Ideal Winning Number, defined asIWNi(M)=∑j=1n0.35em0ex∑k=1mD{Mi(j),Mk(j)}where j is the index of a dataset, n the number of datasets in the comparison, i and k are indices of an algorithm, Mi(j) is the performance of the i-th algorithm on the j-th dataset, M is a ranking measure (such as NDCG or MAP), andD{Mi(j),Mk(j)}is an evaluation function such thatD{Mi(j),Mk(j)}={1if0.35em0exMi(j)0.35em0exand0.35em0exMk(j)arebothdefined,0otherwiseNDCG@{3, 5, 10} and MAP are the most frequently used evaluation metrics in the used benchmark collections combined, therefore we will limit our meta-analysis to evaluation results reported in one of these four metrics.The following subsections provide the performance of learning to rank methods in terms of NWN for NDCG@{3, 5, 10} and MAP. Performance of the learning to rank methods is plotted with NWN on the vertical axis and the number of datasets on which the method has been evaluated on the horizontal axis. Moving to the right, certainty on the performance of the method increases. The Pareto optimal learning to rank methods, that is, the learning to rank methods for which it holds that there is no other method that has (1) a higher NWN and (2) a higher number datasets evaluated, are identified as the best performing methods and are labeled. Table B.6 in the appendix provides raw NWN data for the learning to rank methods at NDCG@{3, 5, 10} and MAP and their cross-metric weighted average.Fig. 1shows the NWN of learning to rank methods based on NDCG@3 results. LambdaNeuralRank and CoList both acquired a NWN score of 1.0 by beating all other algorithms on one dataset, with LambdaNeuralRank winning on the AOL dataset and CoList winning on Yahoo Set 2. LARF and LRUF scored very high scores of near 1.0 on three of the LETOR 3.0 datasets, which results in more certainty on these methods’ performance because they are validated on three datasets that additionally are more relevant than AOL and Yahoo Set 2 (number of evaluation results for LETOR 3.0 are higher than those for AOL and Yahoo set 2). FenchelRank, OWPC, SmoothRank, DCMP and ListNet are ordered decreasingly by NWN and at the same time increasingly in number of datasets that they are evaluated on, resulting in a higher degree of certainty on the accuracy of the algorithms.LambdaNeuralRank, CoList, LARF, LRUF, OWPC and DCMP evaluation results are all based on one study, therefore are subjected to the risk of one overly optimistic study producing those results. FenchelRank evaluation result are the combined result from two studies, although those studies have overlap in authors. SmoothRank and ListNet have the most reliable evaluation result source, as they were official LETOR baseline runs.Fig. 2 shows the NWN of learning to rank methods based on NDCG@5 results. LambdaNeuralRank again beat all other methods solely with results on the AOL dataset scoring a NWN of 1.0. LARF, LRUF, FenchelRank, SmoothRank, DCMP and ListNet are from left to right evaluated on an increasing number of datasets, but score decreasingly well in terms of NWN. These results are highly in agreement with the NDCG@3 comparison. The only modification compared to the NDCG@3 comparison being that OWPC did show to be a method for which there were no methods performing better on both axes in the NDCG@5 comparison, but not in the @3 comparison. Like in the NDCG@3 comparison, SmoothRank and ListNet can be regarded as most reliable results because the evaluation measurements for these methods are based on LETOR official baselines.Fig. 3 shows the NWN of learning to rank methods based on NDCG@10 results. LambdaMART and LambdaNeuralRank score a NWN of 1.0 on the NDCG@10 comparison. For LambdaNeuralRank these results are again based on AOL dataset measurements. LambdaMART showed the highest NDCG@10 performance for the MSLR-WEB10k dataset. The set of Pareto optimal learning to rank algorithms is partly in agreement with the set of Pareto optimal methods for the NDCG@3 and @5 comparisons, both include LARF, LRUF, FSMRank, SmoothRank, ListNet, RankSVM. In contrast to the NDCG@3 and @5 comparisons, DCMP is not a Pareto optimal ranking method in the NDCG@10 comparison.Fig. 4 shows the NWN of learning to rank methods based on MAP results. Comparisons on the NDCG metrics where highly in agreement on the Pareto optimal algorithms, MAP-based NWN results show different results. RankDE scores a NWN of 1.0 on one dataset, which is achieved by obtaining highest MAP-score on the LETOR 2.0 TD2003 which has many evaluation results are evaluated.LARF and LRUF score very high NWN scores, but based on only few datasets, just as in the NDCG-based comparisons. Notable is the low performance of SmoothRank and ListNet, given that those methods were top performing methods in the NDCG-based comparisons. Table B.6 in the appendix shows that LAC-MR-OR is evaluated on more datasets on MAP than on NDCG, thereby LAC-MR-OR obtained equal certainty to ListNet with a higher NWN. SmoothRank performed a NWN of around 0.53 on 7 datasets, which is good in both certainty and accuracy, but not a Pareto optimum. RE-QR is one of the best performers in the MAP comparison with a reasonable amount of benchmark evaluations. No reported NDCG performance was found in the literature search for RE-QR. There is a lot of certainty on the accuracy of RankBoost and RankSVM as both models are evaluated on the majority of datasets included in the comparison for the MAP metric, but given their NWN it can said that both methods are not within the top performing learning to rank methods.Fig. 5shows NWN as function of IWN for the methods listed in Table A.5. The cross-metric comparison is based on the NDCG@{3, 5, 10} and MAP comparisons combined. Fig. 5 labels the Pareto optimal algorithms, but also the Rank-2 Pareto optima, which are the labels the algorithms with exactly one algorithm having a higher value on both axes. Pareto optimal are labeled in large font while Rank-2 Pareto optima are labeled using a smaller font size. In addition, Linear Regression and the ranking method of simply sorting on the best single feature are labeled as baselines.LRUF, FSMRank, FenchelRank, SmoothRank and ListNet showed to be the methods that have no other method superior to them in both IWN and NWN. LRUF is the only method that achieved Pareto optimality in all NDCG comparisons, the MAP comparison as well as the cross-metric comparison. With FenchelRank, FSMRank, SmoothRank and ListNet being Pareto optimal in all NDCG comparisons as well as in the cross-metric comparison, it can be concluded that the cross-metric results are highly defined by the NDCG performance as opposed to the MAP performance. This was to be expected, because the cross-metric comparison input data of three NDCG entries (@3, @5, and @10) enables it to have up to three times as many weight as the MAP comparison.LARF, IPRank and DCMP and several variants of RankSVM are the Rank-2 Pareto optima of the cross-metric comparison. LARF was also a Pareto optima on the NDCG and MAP comparisons and DCMP was a Pareto optimal ranker in a few of the NDCG comparisons. C-CRF, DirectRank, FP-Rank, RankCSA, LambdaNeuralRank and VFLR all have a near-perfect NWN value, but have a low IWN value. Further evaluation runs of these methods on benchmark datasets that they are not yet evaluated on are desirable. The DirectRank paper (Tan, Xia, Guo, & Wang (2013)) shows that the method is evaluated on more datasets than the number of datasets that we included evaluation results for in this meta-analysis. Some of the DirectRank measurements could not be used because measurements on some datasets were only available in graphical form and not in raw data.LAC-MR-OR and RE-QR showed very good ranking accuracy in the MAP comparison on multiple datasets. Because LAC-MR-OR is only evaluated on two datasets for NDCG@10 and RE-QR is not evaluated for NDCG at all, LAC-MR-OR and RE-QR are not within the Pareto front of rankers in the cross-metric comparison.In this section we evaluate the stability of the obtained results when one of the evaluation measures (5.1) or one of the datasets (5.2) are left out of the comparison. We scope this sensitivity analysis to those ranking methods that showed to be Pareto optimal in the trade-off between IWN and NWN: ListNet, SmoothRank, FenchelRank, FSMRank and LRUF.To analyze the sensitivity of the comparison method in the evaluation measure dimension we repeated the NWN and IWN calculation while leaving out one evaluation measure. Table 4 shows the NWN and IWN results when all evaluation measures are included in the computation and when MAP, NDCG@3, NDCG@5 or NDCG@10 are left out respectively. From this table we can infer that FSMRank is not a Pareto optimal ranking method when MAP is left out of the comparison (LRUF scores higher on both NWN and IWN) and FenchelRank is not a Pareto optimal ranking method when either NDCG@3 or NDCG@5 are left out (FSMRank scores higher on both NWN and IWN). All other orderings of ranking methods on NWN and IWN stay intact when one of the evaluation measures is left out of the comparison.Notable is that all Pareto optimal ranking methods have the largest increase in IWN as well as the largest decrease in NWN when the MAP measure is left out of the comparison. The NWN score of FSMRank increased almost 0.1 when the MAP evaluation measure was left out, which is the highest deviation in NWN score seen in this sensitivity analysis. Note that MAP uses a binary notion of relevance, where NDCG uses graded relevance. The fact that all Pareto optimal rankers obtain an even higher NWN score when the MAP measure is left out shows that apparently the Pareto optimal rankers perform even better on ranking on graded relevance, compared to non-Pareto-optimal rankers.In Table 1 in Section 2 shows the 20 datasets used in our comparison, originating from eight data collections. We analyzed the variance in NWN and IWN scores of the Pareto optimal rankers for the situations where one of the 20 datasets is not included in the NWN and IWN computation. The results are visualized in Fig. 6in a series of bagplots, which is a bivariate generalization of the boxplot proposed by Rousseeuw, Ruts, and Tukey (1999). Bagplot extends the univariate concept of rank as used in a boxplot to a halfspace location depth. The depth median, shown in orange, is the deepest location. Surrounding it is a bag, the dark blue area in Fig. 6, containingn2observations with the largest depth. The light blue area represents the fence, which magnifies the bag by a factor 3.The bagplots give insight into the degree to which the found IWN and NWN scores of the ranking methods are dependent on evaluation results on a small subset of the datasets that these ranking methods were evaluated on. A large bag and fence indicate that the IWN and NWN performance was not consistent over all the datasets on which the ranking method in question was evaluated, while a small bag and fence indicate consistent IWN and NWN performance over all the datasets on which that ranker was evaluated.Note that the number of unique observations on which the bagplots are created is equal to the number of dataset on which a ranking method is evaluated (in any of the evaluation measures), as removing a dataset on which a ranking algorithm is not evaluated does not have any effect on the NWN and IWN scores. The difference between the leftmost and the rightmost points of the bags seems to be more or less equal for all ranking methods while the NWN variance seems to be more or less consistent for all ranking methods except LRUF and LARF. As the NWN mean decreases from top-to-bottom and left-to-right, the variance-to-mean ratio increases. It is important to stress that the low NWN variance of LRUF and LARF does not imply high certainty about the level of ranking performance of these ranking methods, it solely shows the low variance in the evaluation results that were available for these ranking methods. As the number of evaluation results for LRUF and LARF is lower than for the other Pareto optimal rankers, the certainty of their ranking performance is considered to be lower.

@&#CONCLUSIONS@&#
