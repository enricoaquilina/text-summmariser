@&#MAIN-TITLE@&#
Signal analysis and classification methods for the calcium transient data of stem cell-derived cardiomyocytes

@&#HIGHLIGHTS@&#
Calcium cycling is important in cardiomyocytes and in cardiac functionality.Data analysis techniques for stem cell derived cardiomyocytes were developed.The techniques were based on peak detection and classification.Signals were classified into normal or abnormal classes with machine learning.It is crucial to detect abnormal signals corresponding to abnormal cells of culture.

@&#KEYPHRASES@&#
Human pluripotent stem cells,Cardiomyocytes,Calcium cycling abnormalities,Biomedical data analysis,

@&#ABSTRACT@&#
Calcium cycling is crucial in the excitation–contraction coupling of cardiomyocytes, and therefore has a key role in cardiac functionality. Cardiac disorders and different drugs alter the calcium transients of cardiomyocytes and can cause serious dysfunction of the heart. New insights into this biochemical phenomena can be achieved by studying and analyzing calcium transients. Calcium transients of spontaneously beating human induced pluripotent stem cell-derived cardiomyocytes were recorded for a data set of 280 signals. Our objective was to develop and program procedures: (1) to automatically detect cycling peaks from signals and to classify the peaks of signals as either normal or abnormal, and (2) on the basis of the preceding peak detection results, to classify the entire signals into either a normal class or an abnormal class. We obtained a classification accuracy of approximately 80% compared to class decisions made separately by an experienced researcher, which is promising for the further development of an automatic classification approach. Automated classification software would be beneficial in the future for analyzing cardiomyocyte functionality on a large scale when screening for the adverse cardiac effects of new potential compounds, and also in future clinical applications.

@&#INTRODUCTION@&#
Calcium (Ca2+) cycling plays a critical role in the excitation–contraction coupling of cardiomyocytes, and it is the link between the electrical signaling in the cardiomyocyte and contraction. Changes and variability in Ca2+ transients can be seen because of cardiac diseases or different drugs, which can have profound consequences for the function and phenotype of cardiomyocytes. For example, when cardiac failure progresses, changes in Ca2+ regulation and flux are observed, and failing hearts are also characterized by more arrhythmic Ca2+ signals [1]. Characterization of Ca2+ cycling is crucial in cardiac research in order to facilitate investigations of cardiac disorders and dysfunction, and to study disease management with different compounds. Ca2+ imaging of cardiomyocytes is a widely used technique for monitoring their Ca2+ cycling activity in vitro. Intracellular Ca2+ cycling can be recorded with the help of fluorescent Ca2+ indicator dyes.Cardiac functionality can be studied with the help of cardiomyocytes differentiated from human pluripotent stem cells [2–4]. Induced pluripotent stem cell (iPSC) technology – where pluripotent stem cells are generated by reprogramming differentiated cells into a pluripotent state – provides an especially useful tool for studying the pathophysiology of various disorders and drug responses in human cells. Human iPSCs can be differentiated into the desired cell type, retaining the original genotype. New insights into calcium handling in different cardiac diseases have been achieved after the invention of iPSCs [5–10].To empower cardiologic investigations, we have developed a signal analysis procedure for the detection and classification of Ca2+ cycling or transients (peaks) in cardiomyocyte signal data, plus another procedure for the classification of entire signals into either a normal or abnormal class on the basis of results of the preceding procedure. These computational tasks are essential for the development of automatic tools for the selection of valid cell lines, the observation of abnormal Ca2+ transients, and the analysis of different drug responses.Spontaneously beating cardiomyocytes were differentiated from iPSCs derived from patients with catecholaminergic polymorphic ventricular tachycardia (CPVT), a genetic cardiac disease. CPVT is an exercise-induced malignant arrhythmogenic disorder, which can cause increased calcium (Ca2+) sensitivity and lead to spontaneous Ca2+ release from the sarcoplasmic reticulum, the generation of after-depolarizations, and triggered activity [11]. We were interested in the recognition of the Ca2+ transient abnormalities of these cardiomyocytes, as revealed by the frequency and especially the shape deformations being manifested within the peaks of the cycling signals. For this purpose, we developed signal analysis and classification procedures to enable the automatic processing of cardiomyocyte data [12]. In the present study, we have developed and extended our method to identify and classify Ca2+ transients efficiently. To the best of our knowledge, this kind of classification has so far been done only subjectively and visually. We have also developed a separate method for the interactive use of human experts to select and analyze Ca2+ transients that supports our efforts for the current method [13]. This analysis area will be important for the cardiology and pharmaceutical industry and, therefore, computational methods will be needed for future medical research and its applications.The study was approved by the Ethics Committee of Pirkanmaa Hospital District (R08070). Patient-specific iPSC lines were established with retroviruses encoding for OCT4, SOX2, KLF4, and MYC, as described earlier [2]. All the cell lines were characterized for their karyotypes, mutations, pluripotency by RT-PCR, immunocytochemistry, embryoid body (EB), and teratoma formation. The IPSCs were then co-cultured with murine visceral endoderm-like (END-2) cells (Humbrecht Institute, Utrecht, The Netherlands) to differentiate them into spontaneously beating cardiomyocytes. The beating areas of the cell colonies were dissociated mechanically and enzymatically with collagenase A (Roche Diagnostics) [14].Ca2+ imaging was conducted in spontaneously beating, 4µM Fura-2 AM (Invitrogen, Molecular Probes)—loaded dissociated cardiomyocytes as described earlier [7]. Cardiomyocytes were continuously perfused with 37°C HEPES-based perfusate during measurements. The perfusate consisted of (in mM) 137 NaCl, 5 KCl, 0.44 KH2PO4, 20 HEPES, 4.2 NaHCO3, 5 d-glucose, 2 CaCl2, 1.2 MgCl2, and 1 Na-pyruvate (the pH was adjusted to 7.4 with NaOH). Ca2+ measurements were conducted on an inverted IX70 microscope (Olympus Corporation, Hamburg, Germany), and cells were visualized with a UApo/340×20 air objective (Olympus). Images were taken with an ANDOR iXon 885 CCD camera (Andor Technology, Belfast, Northern Ireland) and synchronized with a Polychrome V light source by a real time DSP control unit and TILLvisION or Live Acquisition software (TILL Photonics, Munich, Germany). Fura 2-AM in the cardiomyocytes was excited at a light wavelength of 340nm and 380nm, and the emission was recorded at 505nm. For Ca2+ analysis, regions of interest were selected for spontaneously beating cells and background noise was subtracted before further processing. Signals were acquired as the ratio of the emissions at 340/380nm wavelengths.The data were generated with two different software programs. Thus, various sampling frequencies were used to record signal data: frequencies of approximately 8, 10, and 11Hz were used with one program and a frequency of 23Hz was used with the other program. Cycling peaks and other properties varied remarkably in signals. We recorded 280 signals, the lengths of which varied from approximately 11 to 24s. The recorded signals were fairly short since the Ca2+ imaging method can damage the cells by photo-toxicity; therefore, this limits their exposure time. On the other hand, short signals contained scant information, such as peaks, making the computational decision-making (classification) tasks difficult.At first, a linear descending trend was removed from each signal according to the best straight-line fit, because such a trend was present in all signals. These modified signals were only used up to the step of the peak detection, after which, the feature values of peaks detected were computed from the original signals. Removal of a trend was used in order to facilitate peak detection. Signals of the highest sampling frequency were also filtered with a median filter [15], including a filtering window of 3 samples. However, this was not done for the signals of the lower sampling frequencies of 8, 10, and 11Hz, since the smallest pertinent peaks of these signals included only a few samples, i.e., they would have been too sensitive even to this light filtering. By using median filtering for the highest sampling frequency signals (23Hz), the signals were smoothed out to more closely resemble the lower sampling frequencies’ types of signals. Furthermore, the same constant thresholds for peak detection and some of the percentage bounds for peak classification employed for the low frequencies’ signals also suited the higher frequency signals better. The signal’s minimum was subtracted from all samples to set a zero minimum for simplicity, e.g., for visual exploration in later figures.All samples were computed as amplitude values to explore the distribution of these values. In order to compare the average amplitude values at the beginning and the end, distribution estimate A was computed for an average amplitude of the large peaks (seeFig. 1). The purpose of A is simply to aid the subsequent peak detection as a rough estimate. The amplitudes of normal peaks are virtually always this kind of large peak. The amplitudes of abnormal peaks can be either smaller than the normal ones or also equally high. Of course, their shapes could also be affected (Figs. 1–3).Ultimately, the first derivative signal was approximated from a preprocessed signal using linear regression by sliding it through the signal as a window of 3 or 7 (the latter for the highest sampling frequency only) successive samples, depending on the sampling frequency. Slopes of linear regression were employed to approximate the first derivative values of the signal. These values were used in the peak detection. All programming was made with Matlab.The next phase of computation involves the detection of peaks: every peak-like occurrence is searched for, but only those occurrences satisfying the peak definitions are selected as actual calcium transients.In order to find a possible peak, the approximated values of the first derivative of a signal were successively searched for. As an example, see the right plot inFig. 4. When values greater than a threshold tupof 30 were encountered – i.e., the start of an increasing signal segment on the left-hand side of a peak – such a segment indicated the beginning of a peak. Correspondingly, when values after the beginning and peak maximum again increased above tdown=−0.6∙tup, the end segment of the peak was found. The magnitude of the latter threshold value was smaller, since the left-hand side of a peak was almost always steeper than the right-hand one, as can be seen in the right plot in Fig. 4. For the sake of the variability of the amplitude distribution of acceptable peaks, we also used smaller thresholds for those rare signals where sample distributions were from integer intervals [0,100] only. Instead, the great majority of signals included wider sample distributions, such as [0,500] in Figs. 1–3. All these threshold values were derived experimentally from our data.Next, the exact beginning and end of each possible peak were searched for from the beginning and the end segments of the peaks as determined above. Those rough locations were computed based on the approximated first derivative values, but the exact locations of a peak beginning and end were then computed from the signal itself. Inside such a segment of a few samples, a minimum was searched for and that location determined either the exact beginning or end of a peak. In addition, the maximum value (top) of a peak was searched for between the beginning and end. These three values defined the location of a peak within the signal considered. These values are used in further processing: first for the selection of actual, valid cycling peaks, to compute variable values in order to determine whether a peak is normal or abnormal, and, finally, to classify an entire signal as either normal or abnormal.Finally, for the peak detection, acceptable peaks representing actual cycling peaks were selected by dropping out those that were assessed to be noise or other irrelevant occurrences. The rules used were as follows: If at the beginning or end of a signal, there appeared a “partial” peak without a peak beginning or end as defined above, such an occurrence was rejected. For example, the signal in Fig. 1 incorporates an incomplete peak at its beginning, so the occurrence was rejected. Furthermore, small spikes of less than 10% of amplitude estimate A (modified from [7]) were rejected. In Fig. 1, such spikes between peaks are marked with vertical bars at their beginnings, maximal amplitudes and ends. Here, the highest point of an occurrence was used to determine its amplitude.Ultimately, variable values were derived from the selected acceptable peaks. All variable computation was accomplished after returning to the original signals, i.e., the trend removal was only utilized for peak detection and localization. After having found the beginning, maximum and end of every peak in a signal and after the selection of acceptable peaks, variable computation along with the peaks was performed on the original signals. Thus, trend removal did not affect peak locations on the horizontal time-related axis, but possibly affected slightly peaked shapes related to the vertical axis. Fig. 4 shows six variables: left and right amplitude, left and right duration, and the maximum and minimum of the differentiated signal approximating the first derivative. The latter two were also included because they measure the steepness of the peak’s sides. As the seventh variable (not seen in Fig. 4), we applied the time interval between peaks, which was calculated from the preceding peak maximum to that of the current peak. Nevertheless, for the first accepted peak, the interval was calculated from the beginning of the signal, or, if there was a preceding partial peak rejected at the beginning of the signal, from the maximum of this rejected one to the corresponding part of the accepted peak. The purpose of the interval variable was to describe how regularly or irregularly peaks appeared in the recording. We saw that irregularity denoted abnormal cycling signals. We also considered other possibilities. All of the former seven variables can be understood to be local variables, i.e., they represent some locally restricted part in a signal. Thus, it is possible that rather than the separate peaks, other, more global alternatives could characterize the normality or abnormality of a signal as a whole. We attempted, among others things, autocorrelation, but eventually we excluded these efforts because they did not improve the classification results of signals presented in the following section.Because the classification of cycling signals basically depends on the classification of individual peaks, we began by determining whether each occurrence accepted above as an actual peak was either normal or abnormal. For this purpose, we constructed the following procedure: If the amplitude of the greater side of a peak was more than 65% (modified from [7]) of that of the preceding peak, the current peak was determined to be normal under the condition that either the preceding peak was marked as normal or, if this was not true or there was no preceding peak, the current peak was more than 50% of the overall amplitude estimate A. Otherwise, the current peak was deemed to be exceptionally small according to its amplitude and was considered abnormal. In Fig. 1, the small peaks marked with the vertical arrows were considered to be abnormal. Furthermore, some of the peaks in Fig. 3 were classified as abnormal in this way.Naturally, those classified as abnormal according to the preceding condition were no longer tested in the following phase. This next phase investigated the asymmetry of peaks, i.e., whether one side of a peak was clearly smaller than the other. A threshold of 86% symmetry was chosen experimentally. If the amplitude of one side was smaller than 86% of the other, a peak was assessed to be asymmetric and was recognized as abnormal. If this was not the case, it was recognized as normal. Thereby, those peaks not classified as abnormal in Fig. 3 on the basis of the former rule were considered abnormal if they did not meet the subsequent rule.To study the distribution of the peaks of a signal, we computed a principal component analysis for peaks classified as normal or abnormal with the two aforementioned rules. The results are shown inFig. 5, where the first and second principal components are used for visualization. As usual, by computing eigenvectors of the covariance matrix of the data matrix of 4120 peaks ×7 variables, data variances projected onto the principal components were calculated to explain the effects of the principal components. In this case, the first principal component explained variance as highly as 95.4% and the second principal component explained variance at 3.8%. This implies that the first two components (from seven) mapped the data almost entirely. However, since there is an extensive overlapping area for the classes – from approximately −1000 to 0 according to the first principal component in Fig. 5 – it is understandable that our signal classification task, even for the 2-class type only, was demanding.For a comparison of peaks and signals, the data set was also assessed by a human expert. The classification was performed blinded, since the analyzer did not take part in the construction of the peak procedure given above and labeled all the signals before the programming of the peak detection and classification procedure.Comparing the classification results of the human expert and the classification procedure introduced is not straightforward, because even a single peak assessed as be abnormal is enough to judge the entire signal abnormal, i.e., to be a normal signal, all of its peaks have to be normal. On the other hand, in an abnormal signal there can both abnormal and normal peaks. To look at the data used, we took a random sample of 30 signals and perused these, peak by peak, after running the peak classification procedure. We followed our principle that if even a single peak was classified as abnormal, the classification of the entire signal should also be abnormal. Finally, we compared the results gained with the decisions of the human expert. There are four sections as follows (here the decisions of the expert against which computational results were compared are always interpreted to be correct): (1) The first 12 signals were normal signals, the prerequisite of which is that all peaks (4 to 39) were recognized as normal by both the expert and the peak classification procedure. (2) Next, one signal alone determined by the expert as normal was misclassified, because 4 peaks out of 15 were classified as abnormal. (3) Eleven signals defined as abnormal by the expert were correctly classified to be so, although a few peaks (1 to 7) were incorrectly classified as abnormal or normal (1 to 4 peaks) or were not detected at all (1 peak or 4 peaks). The last situation merely consisted of peaks of small amplitude that were difficult to separate from noise. After all, the signal classifications were correct, since typically most abnormal (one enough in principle) peaks were classified as such. (4) Ultimately, the last six signals were classified incorrectly as normal even when the expert’s decision judged them to be abnormal. This occurred since in two situations, two abnormal peaks by the expert were misclassified, and in four situations one small peak in each signal was not detected. The type of distribution in the four sections described seemed to be present in the whole signal set. The summary of these results is given inTable 1. Normal signals were classified well in particular. Abnormal signals were clearly more difficult, and the most difficult of these were some peaks of very small amplitudes—less than approximately 10% of amplitude estimate A.After the detection of peaks for every signal and the determination of those peaks as normal or abnormal, we ran our classification method and tested our data according to the principle of leave-one-out. Thus, every single signal formed a test set as such, and all other n−1 signals (n=280) were then the corresponding training set. This testing technique is effective for relatively small data sets because it maximizes the size of every training set. Thus, n different test cases were run for the n slightly different models built. True positive and negative rates were computed as well as classification accuracy for all classification methods implemented, and a signal was judged to be abnormal even if at least one of its peaks was classified abnormal.There were two main test alternatives. First, all signals were run as described above and the accuracy of our classification method – i.e., classes determined by our peak procedure and classification – was compared to the computed classes in the data. In other words, after running the peak detection procedure for a signal, the classification procedure determined the signal to be abnormal if one or more of its peaks was found to be abnormal. If no abnormal peak was found, the signal was classified as normal. This procedure was run for all 280 signals. Then, using the class labels generated by the procedure for all the signals, we classified every signal with the classification methods named inTable 2. In this classification, the above-mentioned seven variables were applied to characterize all the individual peaks of each signal. In Table 2 as well as later tables, sensitivity equals the number of normal signals classified correctly and specificity that of abnormal signals classified correctly.Second, the classification results were calculated by comparing how well they matched those labeled by the human expert. We assume that human classification is superior, but we acknowledge that human visual assessment is somewhat subjective and may vary slightly from time to time, even when the expert is very experienced. Consequently with the preceding premises, the results for the former main test set-up were better than expected in comparison to those of the latter. Thus, since the latter is seen a more realistic comparison – even if it is not completely perfect – we present the results for the former test only for three basic classification methods [16], such as nearest-neighbor searching. For the latter test, in comparison, we extensively applied different classification methods [16–18] such as support vector machines.All classification runs were performed by applying the seven variables computed for all signals. In 280 signals there were 4120 peaks in total, varying from 1 to 45 peaks per recording, with 14.7 peaks on average. At the beginning, we show classification results where the classification was made and compared with classes determined by the signal classification procedure itself. These results, showing k-nearest neighbor searching, and linear and quadratic discriminant analysis, are given in Table 2. The best classification was obtained with 3-nearest-neighbor searching. The class of the normal was easier to classify with its nearest neighbors, but for discriminant analysis, the reverse was true. Note that in Table 2 as well as in the subsequent tables, the results in the first row contain true positive rates (normal seen positive) and the second row includes true negative rates (abnormal), since there were only two classes.Table 3 shows the results obtained with the same classifiers as those in Table 2, but the real class of each signal (normal or abnormal) was defined by the human expert. Quadratic discriminant analysis yielded the best accuracy, and 3-nearest-neighbor searching was the next best. The accuracies of nearest-neighbor searching were approximately 10% worse than in Table 2, but those of discriminant analysis were only around 2% worse. The cause for this was that the class labels for Table 2 to which the classification results were compared were made uniformly based on the peak detection and classification procedures. For Table 3, on the other hand, the real class labels to which the classification results were compared were decided by a human expert who did not make decisions precisely similarly to the peak and classification procedures, and whose decisions might have varied slightly from signal to signal.Table 4 shows the classification results produced by discriminant analysis with Mahalanobis (generalized Euclidean) distance, two different Bayes classifiers, tree classifier, and self-organizing maps of three sizes used in the way of supervised learning.Table 5 shows the results given by support vector machines with different kernels. The support vector with the third-degree kernel was superior to the others in Tables 4 and 5, and approximately 3% inferior to the best of all, the quadratic discriminant analysis shown in Table 3.Sometimes classification with principal component (PCA) values may induce better accuracy results than the use of the original variable values. Nonetheless, the current data running with all seven PCA components resulted in classification accuracies 0–10% worse than the results of the original data. Therefore, the results given by PCA are not presented.

@&#CONCLUSIONS@&#
