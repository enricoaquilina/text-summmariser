@&#MAIN-TITLE@&#
Multi-fidelity non-intrusive polynomial chaos based on regression

@&#HIGHLIGHTS@&#
We present a multi-fidelity (MF) extension of non-intrusive polynomial chaos based on regression for uncertainty quantification purposes.The proposed method uses the principle of a global correction function, nested sampling plan, and uses regression to estimate the coefficients.The results show that high correlation and moderately low errors between the low- and high-fidelity functions are important to improve the MF approximation’s accuracy.On aerodynamic problem, a computational cost saving of about 60% can be obtained by using the present method.

@&#KEYPHRASES@&#
Uncertainty quantification,Multi-fidelity,Polynomial chaos,Point collocation,Flexible sampling,Hyperbolic truncation,

@&#ABSTRACT@&#
In this paper we present a multi-fidelity (MF) extension of non-intrusive polynomial chaos based on regression (point collocation) for uncertainty quantification purposes. The proposed method uses the principle of a global correction function from a previous similar method that uses spectral projection to estimate the coefficients. Due to its usage of regression to estimate the coefficients, the present method offers high flexibility in the sampling and generation of the polynomial basis. The method takes advantage of a nested sampling plan to create the samples for the low-fidelity (LF) and correction expansions where the high-fidelity (HF) samples are a subset of the LF ones. To build the polynomial basis, a total order or hyperbolic truncation strategy is used with a highly flexible combination of the LF and correction polynomial expansions. The method is demonstrated on some artificial test problems and aerodynamic problems of the Euler flow around an airfoil and common three-dimensional research models. In order to derive the strategies for successful MF approximation, the effect of the correlation and the errors between the LF and HF functions is also studied. The results show that high correlation and moderately low errors are important to improve the MF approximation’s accuracy. On a common research model problem, the MF approach with partially-converged simulations as the LF samples can successfully reduce the computational cost to about 40% for similar accuracy compared to an approach using a single HF expansion.

@&#INTRODUCTION@&#
The presence of uncertainties is inevitable in the real-world implementation of engineering or physical systems. An example in the aerospace engineering field is the uncertainties in the flight condition such as Mach number and angle of attack that affect the performance of the aerodynamic bodies. Computationally expensive partial differential equation (PDE) solvers are frequently employed to predict the physics of the system being studied, but such tools only predict performance at a single design condition without considering the uncertainties. Therefore, in order to understand and predict the effect of the uncertainties on the model being studied, a methodology called uncertainty quantification (UQ) was developed.Much of the research in the UQ field revolves around the development of methods that can compute accurate statistical properties at lower computational cost. Some of these methods are Bayesian Monte Carlo  [1], Taylor-based methods of moments  [2], or the univariate reduced quadrature method  [3]. The main goal of all these methods is similar: to reduce the number of deterministic evaluations while retaining accuracy. One of the UQ methods that is still in active and rapid development is the method based on the polynomial chaos expansion (PCE)  [4–6]. PCE works by approximating the stochastic response surface between the random input and output of interest using orthogonal polynomials  [7]. The power of PCE lies on its mathematically rigorous concept  [7], strong theoretical basis, and ability to converge to machine precision. In its early development, PCE was a highly intrusive method  [8,9] where it is necessary to modify the existing codes in order to be able to perform the UQ procedure. A popular, less cumbersome alternative is using non-intrusive polynomial chaos (NIPC)  [10] by performing multiple simulations on the collocation points and then using special techniques to estimate the coefficients of the polynomial expansion.To build a PCE, a polynomial basis has to be determined and then the polynomial coefficients are calculated. There are two main classes of techniques to calculate coefficients in NIPC: (1) spectral projection, where the responses are projected onto the basis functions using inner products and the orthogonal polynomials employed to obtain the coefficients, or (2) point collocation (PCNIPC), where the coefficients are obtained by performing regression to minimize the error of the approximation  [11–13]. Tensor products and sparse grids  [14–18] are routinely employed in the spectral projection approach to expand the polynomial expansion, while the point collocation method commonly uses a total order expansion  [7].The tensor product is a traditional method used to expand the polynomial expansion and the collocation points if the spectral projection approach is used, but it suffers greatly from the curse of dimensionality. To mitigate the curse-of-dimensionality problem, the sparse tensor product spaces method was introduced by Smolyak to reduce the number of collocation points. This method has been investigated for applications of the stochastic collocation method and uncertainty quantification. Moreover, adaptivity can be employed to further reduce the number of collocation points  [15–17]. A formal method for calculating the polynomial chaos coefficients using a sparse grid, known as the sparse pseudospectral approximation method (SPAM), has been developed  [18]. An adaptive version of SPAM was developed to reduce the computational cost by focusing the growth of the sparse grids on important dimensions  [19]. If the point collocation approach is used, the least angle regression (LAR) method  [20] can be employed to select the best polynomial basis to improve accuracy. Another approach to efficiently choose the best polynomial basis is to use a non-adapted sparse approximation based on compressive sensing  [21], whichℓ1-minimization estimates can be enhanced using basis selection  [22].Multiple levels of simulation fidelity are often available in PDE-based systems. Generally, a high-fidelity (HF) simulation takes more computational time but offers higher accuracy, whereas a low-fidelity (LF) simulation is faster at the cost of accuracy. The exploitation of the availability of multiple levels fidelity is popular in assisting the process of optimization, notably in the aerodynamic optimization field. Several techniques can be applied to correct a LF simulation with a HF one, such as model fusion  [23], space mapping  [24], or shape-preserving response prediction  [25]. Kriging method also take advantage of multiple levels of fidelity through the development of the co-Kriging method  [26]. Examples of such work are the multi-fidelity (MF) optimization of low-boom supersonic business jets  [27], transonic airfoils  [25,28], rotor blades  [29], and flapping flight  [30].Recently, an interest in exploiting simulations with multiple levels of fidelity for UQ has fostered the development of MF UQ methods. A multi-level Monte Carlo (MLMC) method was proposed to speed up standard MCS in simulations of stochastic partial differential equation (SPDEs)  [31,32]. Following these works, an MLMC that can solve general engineering model, treated as a black-box problem, was developed by implementing a control variate method and information reuse estimator  [33]. This method successfully reduces the computational cost for aircraft conceptual design optimization under uncertainty  [34]. Another recent MF UQ method is MF NIPC developed by Ng and Eldred  [35], which employs MF simulation to perform UQ incorporating different levels of sparse grid. The low-level sparse grid serves as a corrector for a higher-level sparse grid because a low-level sparse grid is a subset of a higher-level sparse grid. The resulting expansions (LF and correction) are then combined into a single expansion (model fusion) so the moments can be calculated using this single expansion. The collocation points used in this method are structured because it is based on the roots of the orthogonal polynomials. The method has been shown to successfully reduce the computational cost in several test problems as long as the LF stochastic response output trend could predict the HF one sufficiently well  [35]. UQ for a vertical axis wind turbine under extreme gusts  [36] is an example of the method’s application.A drawback of MF NIPC with spectral projection is that the number and location of collocation points cannot be arbitrary. These restrictions mean the user has less flexibility in performing UQ with a limited computational budget. Furthermore, the method still suffers from the curse of dimensionality even though the use of sparse grids reduces the effect. This motivates us to develop a MF extension of PCNIPC (MF-PCNIPC) that has the practical advantage of offering flexibility in choosing the number and location of collocation points. A somewhat similar method is the MF point-collocation method, which uses output space mapping (OSM) to create surrogate models for robust optimization purposes  [37,38]. However, our method uses the correction expansion to estimate the coefficients and is specifically designed for UQ purposes. We also perform a thorough study of the effects of the correlation and errors between the LF and HF functions and derive strategies for effective UQ procedures using MF-PCNIPC.The paper starts with a brief explanation of the polynomial chaos method for uncertainty quantification in Section  2. A MF extension of NIPC based on regression is proposed in Section  3 where the algorithm, sampling strategies, and polynomial basis truncation process are explained in detail. Numerical studies on artificial test problems are presented in Section  4, while implementation details and tests of the method on real-world aerodynamic problems are presented in Section  5. Some implementation issues for real-world problems are discussed in Section  6. Finally, the conclusions of this papers are given in Section  7.Polynomial chaos expansion (PCE) is a method for the systematic quantification of uncertainties in a system that uses a set of orthogonal polynomials to approximate the stochastic response surface. Orthogonal polynomials and a spectral representation of the random process are at the heart of the polynomial chaos method.The concept behind UQ using a stochastic expansion method is to approximate the functional form between the stochastic response output and each of its random inputs (ξ={ξ1,…,ξn}) with the following chaos expansion  [7]:(1)S(ξ)=∑j=0∞αjΨj(ξ)whereΨjandαjare the product of the one-dimensional orthogonal polynomial basis and their corresponding coefficients, respectively.For practical purposes the expansion has to be truncated at a finite expansion orderPso the expression becomes:(2)S(ξ)=∑j=0PαjΨj(ξ).Orthogonal polynomials are an important building block of the PCE method and an understanding of orthogonal polynomials is fundamental in polynomial chaos. An orthogonal polynomial sequence is a family of polynomials such that the polynomials in the sequence are mutually orthogonal by the following inner product relation:(3)〈Ψi(ξ)Ψj(ξ)〉=∫abΨi(ξ)Ψj(ξ)ρ(ξ)d(ξ)=δijwhere the Kronecker deltaδij=1ifi=jand 0 ifi≠j.aandbare the interval for integration whileρ(ξ)is the weight function. For a given probability distribution, a specific polynomial sequence serves as an optimal basis for approximation where the polynomial itself comes from the Askey scheme  [39] of hypergeometric orthogonal polynomials. As an example, Legendre polynomials provide an optimal basis for a uniform distribution that has a weight function of 1 and support range of[−1,1]. Table 1shows the Askey scheme of some continuous hypergeometric polynomials and the corresponding probability density functions (PDFs).To extend the one-dimensional (1D) orthogonal polynomial basis for multi-variable approximations, a tensor product expansion that includes all combinations of the 1D polynomials can be employed. The collocation points can then also be expanded by using the tensor product operator.The coefficients can then be estimated by performing spectral projection:(4)αj=〈S(ξ),Ψj(ξ)〉〈Ψj2(ξ)〉where the denominator in Eq. (4) is the norm squared of the multivariate orthogonal polynomial.The total number of polynomial termsNtfor a tensor-product expansion is:(5)Nt=1+P=∏i=1n(pi+1)wherepandnare the polynomial order and random input dimension, respectively.The main drawback of a tensor-product expansion is that the number of collocation points grows exponentially with the dimension of probability space. In high dimensions (n>4), tensor products become inefficient due to the curse of dimensionality and sometimes impractical for real applications. Smolyak sparse grids  [14,18] offer a remedy to the curse of dimensionality by requiring fewer collocation points. A Smolyak sparse grid is a combination of a smaller tensor product with a small number of collocation points. Similar to a tensor product, the sparse grids method is usually used with a spectral projection method to ensure an optimal approximation by using the collocation points from the roots of orthogonal polynomials. However, the sparse grids method is still not flexible in determining the number and location of collocation points: certain rules have to be followed. Sparse grids also suffer, to a lesser extent, from the curse of dimensionality in moderate-to-high dimension problems. It becomes much more difficult to control the growth of collocation points for relatively high numbers of dimension. Difficulties are also encountered if some collocation points fail to return any value. Even though this problem can be handled by approximating the value of failed collocation points  [40], the sparse grids method still offers the user little flexibility. An alternative is to use the point collocation method, which offers flexible sampling, and a polynomial basis that uses regression to estimate the coefficients.The point collocation method uses linear regression to calculate the coefficients as follows  [12,13]:(6)Ψα=S(ξ).This can be expanded further as:(Ψ0(ξ1)Ψ1(ξ1)⋯ΨP(ξ1)Ψ0(ξ2)Ψ1(ξ2)⋯ΨP(ξ2)⋮⋮⋱⋮Ψ0(ξNs)Ψ1(ξNs)⋯ΨP(ξNs))(α0α1⋮αP)=(S(ξ1)S(ξ2)⋮S(ξNs)).The coefficients are obtained by solving the above linear system of equations which needsNsdeterministic function evaluations. This approach allows flexibility in choosing the number of sampling points and the polynomial basis. Although interpolation properties are not preserved for this approach, reasonable accuracy can be obtained with a method such as least angle regression (LAR)  [20] and compressive sensing  [21]. An overdetermined system where the number of samples is higher than the number of coefficients of the polynomial basis can be solved by using a least squares approach. Hosder, Walters, and Balch  [41] recommend using an over-sampling ratio (OSR), which is defined as a ratio between the number of samples and the polynomial basis, of 2. This OSR value reduces the condition number and the possibility of overfitting. By introducing this OSR, the number of function evaluations needed will beNs=2Nt. Yi Zhang proposed an adaptive method to reduce the OSR by carefully adding samples based on a convergence check on the difference of total response surface error until the error convergence criterion is reached  [42].A total order expansion that preserves basis polynomials up to a fixed total order specification  [7] is normally used with PCNIPC. Consider an index defined by  [20]:(7)|φ|=‖φ‖1=φ1+⋯+φn.The total order expansion retains the indices with the following scheme:(8)An,p≡{φ∈Nn:‖φ‖1≤p}.The total number of polynomial termsNtfor the total order expansion is:(9)Nt=(n+p)!n!p!.To further reduce the number of terms, the hyperbolic scheme  [20] could be used:(10)Aqn,p≡{φ∈Nn:‖φ‖q≤p}where(11)‖φ‖q≡(∑i=1nφiq)1qwhereq≤1is a real number that defines the truncation level.Depending on the value ofq, the hyperbolic scheme places greater or lesser emphasis on retaining the main effects and the interactions. The lower the value ofq, the smaller the size of polynomial basis retained. In fact, applyingq=1results in an ordinary total order expansion. The hyperbolic scheme can be used with the normal strategy of applying an OSR of 2 to ensure numerical stability. Reducing the value ofqresults in a scheme that retains low-order interactions and favors the main effects, as illustrated in Fig. 1.Because of the orthogonality property of the PCE polynomials, the statistical moments of the response can be readily evaluated using coefficient information. The mean is simply the first PCE coefficient, which is given by:(12)μS=E[S]=∫ΩS(ξ)ρ(ξ)d(ξ)=α0.The variance can be obtained from the following equation:(13)σS2=Var[S]=∫Ω(S(ξ)−μS)2ρ(ξ)d(ξ)=∑j=1Pαj2〈Ψj2(ξ)〉whereΩis the domain of integration. In addition, sensitivity analysis can be readily performed using these PCE coefficients  [43].In engineering simulations it is common to encounter various simulation methods with different levels of fidelity. An obvious example of this is any simulation where the mesh density used to discretize the domain can be varied. A simulation with a larger number of elements provides greater accuracy but consumes more computational resource compared to a simulation with fewer elements. If simulations with multiple levels of fidelity are available, these can be exploited to aid the UQ process using a polynomial chaos expansion. The extension of NIPC to MF UQ was first proposed by Ng and Eldred  [35]. When using simulations with different levels of fidelity, UQ accuracy can be improved as long as the LF expansion can capture the trend well. The method has been shown to work well on several test problems but also to give no significant improvement in a case where the trends of the HF and LF simulations differ. A disadvantage of this method is that the number of collocation points cannot be defined arbitrarily because these points have to follow set rules, such as using the roots of the orthogonal polynomials, and extend into higher dimensions with tensor products or sparse grids. It is advantageous for the number of collocation points (or samples) to be flexible in order to be able to easily control the number of deterministic simulations needed for UQ purposes, especially when the random variable dimensionality is moderate to high. We have therefore developed a MF UQ scheme using PCNIPC (MF-PCNIPC) as the base algorithm.Ng and Eldred’s MF expansion is a combination of LF and correction expansions that can take additive or multiplicative form, or a combination of the two  [35]. In this work, we only use the additive form; other details can be found in Ng and Eldred’s original paper. The additive version of the combined expansion is:(14)C(ξ)=Shigh(ξ)−Slow(ξ)so(15)Shigh(ξ)=Slow(ξ)+C(ξ)whereC(ξ)is the correction term, defined as the error between the LF (Slow) and HF expansion (Shigh). In the paper by Ng and Eldred  [35], the expansion is built using a sparse grid rule but here it can be generalized to any polynomial term built by any rule (tensor product, sparse grid, or total order expansion). The most important rule is that the polynomial term in the correction expansion has to be a subset of the LF expansion. The polynomial expansion for the response surface approximation can then be expressed as:(16)Shigh(ξ)=∑j∈Γw−r,n(αlowj+αCj)Ψj(ξ)+∑j∈Γw,n∖Γw−r,n(αlowj)Ψj(ξ)whereΓw,nare the indices for the bases of the combined fidelity expansion with a number of polynomial baseswand dimensionn, whereasΓw−r,n⊂Γw,nare the common bases of the LF and correction expansions andr<wis the indices offset between the LF and correction expansions. In our application where a total order or hyperbolic truncation is used,wandrcan be defined as the LF PCE degree and the degree offset between the LF and correction expansion, respectively. For example, if the polynomial degree of the correction and LF PCE are 2 and 5, respectively, then the value ofris 3.Using this single expansion, the moments can then be calculated using the following formulation:(17)μS≈αlow0+αC0(18)σS2≈∑j∈Γw−r,n∖0(αlowj+αCj)〈Ψj2(ξ)〉+∑j∈Γw,n∖Γw−r,n(αlowj)〈Ψj2(ξ)〉.The key difference between MF-PCNIPC and MF-NIPC based on spectral projection is that, instead of using spectral projection, the coefficients (Eq. (4)) are now calculated using regression (Eq. (6)). Such an approach offers flexibility in the choice of the number and location of samples, and the polynomial basis.Because multiple fidelity simulations are now involved, a strategy has to be derived and developed to identify the best practice for using MF-PCNIPC. We will explain the methodology by explaining in detail the components of MF-PCNIPC. The important components of MF-PCNIPC are the sampling method, the algorithm to generate the polynomial basis, and the way to combine the LF and correction expansions into a single expansion. These components are explained in the following subsections. Moreover, we also explain the differences between the OSM based MF-NIPC and MF-PCNIPC developed in this paper.PCNIPC allows great flexibility in choosing the polynomial basis. If structured grids are used the polynomial basis has to be built using the same rules that were used to build the grids. We do not use structured grids since they are the preserve of the spectral-projection approach. The polynomial basis for unstructured grids should be built using a different method, and a common method used to truncate the polynomial basis in PCNIPC is the total order expansion truncation method. The curse of dimensionality (especially for high-dimension random variables) can be further reduced by using a hyperbolic truncation  [20], with the benefits explained in Section  2.3.In practice, the polynomial basis can adapt to the number of collocation points because the number of samples can be arbitrary. For example, in a case wheren=2withNs=17and an OSR of 1, then the selected polynomial basis is generated usingp=4, giving a polynomial basis size of 15. If UQ is performed by determining the polynomial basis first, the number of collocation points can be determined by multiplying the polynomial basis by the OSR value. Too high an OSR value might result in an overkill approximation, whereas an OSR of 1 gives a high probability of over-fitting due to the low condition number. The optimal OSR value according to Hosder et al. is 2  [41], and we use that value in all our examples to ensure the numerical stability of the approximation. Nonetheless, if the MF scheme is employed, the polynomial order of the correction expansion should be lower than that of the LF expansion, irrespective of what methods are used to generate the basis and collocation points. The concept is to correct the lower-order elements of the LF expansion by the correction function, and then to capture the higher-order elements (not captured by correction expansion) to predict the higher-order behavior of the true function.To perform MF-PCNIPC, sampling is performed on the probability space according to the defined probability distribution. To be able to incorporate MF UQ, some of the HF simulation samples have to be a subset of the LF ones to allow some of the LF samples to be corrected by the HF simulation. The sampling method used in MF-PCNIPC can be structured (orthogonal polynomial roots, Newton–Cotes, with tensor product or sparse grid rules) or unstructured (random sampling, Latin hypercube sampling, low-discrepancy sequence). Unstructured sampling is typically used for uniform distributions but transformations to non-uniform probability distributions can readily be performed using the information of the given distribution. If a structured grid is employed, the polynomial basis should be constructed in the same fashion with the type of the grid employed (tensor product or sparse grids) to allow optimal approximation of the stochastic response surface given the structured grid. This is not possible for an unstructured grid since the sampling points are not correlated with the polynomial basis, so the least squares method is the most convenient way to obtain the coefficients of the polynomial basis. However, structured sampling is a special domain of spectral-projection-based NIPC, so we do not use such sampling methods in our approach. The use of unstructured grids allows more flexibility in the number and location of samples and is much more robust to problem of failed simulations compared to structured grids. Here we suggest the use of a low-discrepancy sequence such as a Halton sequence  [44] to build the sampling plan in an unstructured way. The advantage of using a low-discrepancy sequence as the sampling method is that every high fidelity sampleXhigh={x(1),…,x(Nsh)}is a subset of low-fidelity samples in a higher number of samplesXlow={x(1),…,x(Nsl)}, soXhigh⊂XlowwhereNsh<Nsl.Another similar methodology is nested Latin hypercube sampling (LHS)  [20], which also allows the nesting of sampling points. We suggest the use of a low-discrepancy sequence due to its high space-filling and nested properties. An advantage of performing MF UQ with a low-discrepancy sequence is the ease with which the location of the LF and HF samples can be adjusted. Another advantage of using a random sampling plan, LHS, or a low-discrepancy sequence is that the number of samples can be chosen arbitrarily. In application, an arbitrary number of sampling plans can be employed and the polynomial basis will adjust to the number of sampling plans. This option is convenient if the available computational budget is limited and the location of samples cannot be arranged in a structured way. In Ng and Eldred’s approach, the LF and HF samples have to be built using sparse grids, so there is a restriction on the flexibility in determining the number of samples.If the sampling plan is fixed and allows no additional samples, a good option is to use an optimized LHS plan  [45]. An optimized LHS plan is built by maximizing the space-filling property given a definite number of LF samples. The HF samples are then chosen from the optimized LF samples using specific algorithms such as an exchange algorithm  [45]. In the experiments in this paper we use a Halton sequence because consistency is needed to study the performance of MF-NIPC on various test problems.Fig. 2shows examples of LF and HF samples generated using various methods.The main algorithm is then as follows:1.Low-fidelity expansion(a)Build the LF polynomial expansion with any rule. If total order/hyperbolic truncation is used, determine the value of LF expansion hyperbolic exponentql.Determine the number of LF collocation pointsNsl.Build the LF sampling plansXlow.Calculate the deterministic output value for each LF collocation point.Solve the linear equations to obtain the LF polynomial coefficientsαlow.Multi-fidelity expansion(a)Build the correction polynomial expansion with any rule. If total order/hyperbolic truncation is used, determine the value of correction expansion hyperbolic exponentqc(qhfor single HF-PCE).Determine the number of HF collocation pointsNsh, whereNsh<Nsl.Build the HF sampling plansXhigh.Calculate the deterministic output value for each HF collocation point.Calculate the difference between the HF and LF deterministic output values atXhigh(C(ξ)).Solve the linear equations to obtain the correction polynomial coefficientsαC.Calculate the moments(a)Build a single expansion as a combination of the LF and correction polynomial expansions.Use the combined expansion to calculate analytic moments using Eqs. (17) and (18).Strategies with different truncation orders for the LF and correction expansions are also possible. For example, the LF expansion can be used to capture high-order interactions while the correction expansion captures the main effects. Of course, it is assumed here that LF samples are very quick to evaluate compared to HF ones, so an abundant number of LF samples are readily available. This strategy is particularly useful if the aim is to reduce the number of HF samples while keeping an OSR of 2, the value recommended in many references  [41]. To further reduce the OSR or improve the efficiency, an adaptive strategy  [42] or adaptive sampling  [46] could be employed. These options are not explored in this paper.The choice of polynomial order and hyperbolic truncation order for the correction and LF expansions will depend on user preference, which is constrained by the available computational budget and the ratio of the computational cost of HF and LF evaluations. Note that a higher degree andqvalue typically give more accurate approximations since most of the polynomial basis is captured. However, care needs to be exercised, because if the order of the polynomial basis is too high, this could result in overfitting.The difference between the model fusion (combining two or more model into one) method used in MF-PCNIPC in this paper and the correction method used in MF-NIPC based on OSM needs to be highlighted. MF-NIPC based on OSM  [37,38] works by correcting the output first and then building a single PCE. In contrast, the MF-PCNIPC model fusion method in this work uses two PCEs, the LF and correction expansions, in the form of combined model. Moreover, the corrected model in OSM based MF-NIPC is a simple, usually linear, transformation  [47] while MF-PCNIPC in this work is able to model complex correction surface since polynomial with arbitrary degree is used. This kind of linear correction used in OSM is suitable for cases where the high- and low-fidelity responses are similar in shape with slight distortion  [48]. In reality, the discrepancies could be complex and difficult to be precisely captured by simple mapping; this is where the correction PCE plays its role to model complex surface. The advantage of using model fusion compared to OSM is that the correction can be applied to the whole domain using the correction PCE, for which the polynomial degree can be tuned to increase accuracy.Figs. 4 and 5 give further illustration of the difference between the method in this paper and Ng and Eldred  [35] with the one based on space mapping. The HF function is a simple quadratic functionfh=x2while the LF one is expressed asfl=(x2)/3depicted in Fig. 3. Note that this quadratic function (which is easy to be approximated by PCE) is used only for illustration purpose. Shown in Fig. 3 is also the HF and LF samples for illustration purpose. The number of LF and HF samples are 41 and 5, respectively, which are uniformly distributed and intersect atx=[−1,−0.5,0,0.5,1]. In this paper, MF-PCNIPC starts by calculating the correction values in the intersection of the HF and LF samples, which are depicted as green squares in Fig. 4(a). The next step of MF-PCNIPC is to create a low-fidelity PCE and correction PCE using the information from the LF and correction values, respectively. Finally, the LF-PCE and correction PCE are combined into a MF PCE, which is depicted as the red line in Fig. 4(b). On the other side, OSM based MF-NIPC directly corrects the LF-samples with linear transformation (see  [37,38] for details) as it could be seen in Fig. 5(a). A PCE is then used to approximate these OSM corrected low-fidelity samples (Fig. 5(b)).In the sections that follow, we will present the results of the application of our MF-PCNIPC methodology to a number of artificial and real-world computational test cases.We applied MF-PCNIPC to several artificial and real-world problems to examine the capabilities of the method. The test cases consist of three artificial problems and two engineering (aerodynamic) problems. On some artificial functions, the LF expression of the original function is constructed in such a way as to simulate various cases of correlation between the LF and HF functions. For the airfoil problem, we tested several types of LF simulation to investigate how this affects the accuracy and computational cost of MF-PCNIPC on the particular problems. Monte Carlo simulation with 107 samples is used as a benchmark for all artificial test problems to monitor the convergence of the mean and standard deviation. For calculation of mean absolute error and mean absolute relative error, an independent validation set consisting of 104 samples is used.The statistical similarities between the high- and low-fidelity function are measured byR2correlation and the mean absolute error (MAElh) defined as:(19)R2=(∑i=1m(yhi−ȳh)(yli−ȳl)∑i=1m(yhi−ȳh)2∑i=1m(yli−ȳl)2)2and(20)MAElh=1m∑i=1m|yli−yhi|respectively whereyhandylare a set ofmobservations of the high- and low-fidelity data for identical inputs with the bar denoting the mean of these sets. Because brute values ofMAElhare not so easy to interpret, we also used the mean absolute relative error (MARElh) which is theMAElhnormalized by the mean value of the high-fidelity function (ȳh):(21)MARElh=MAElhȳh.R2correlation measures how well the trend of the high-fidelity function is replicated by the low-fidelity function. In the context of multi-fidelity Kriging, high correlation between low- and high-fidelity functions is necessary in order to be able to create successful multi-fidelity model  [49]. Therefore, the value ofR2correlation should also affect the accuracy of MF-PCNIPC approximation. Moreover, Ng and Eldred  [35] suggest that ‘while a smooth and/or less complex correction function provides a faster convergence rate, it is also important to consider the magnitude of the correction’. This statement implies that the errors between the low- and high-fidelity functions also affect the convergence rate of the multi-fidelity scheme. Because of this, we used bothR2andMAElh/MARElhas the measure of statistical similarities between the low- and high-fidelity functions to analyze the performance of MF-PCNIPC on all test problems.Fig. 6gives an illustration of howR2andMAElh/MARElhmeasure different qualities of the low-fidelity function. A simple one dimensional quadratic functionfh=x2+0.2within the range of[−1,1]that has a mean value of 0.335 was used for this illustration. An offset constant of 0.2 was applied to avoid division with zero value in order to be able to useMARElhmeasure. Three representative low-fidelity functions were constructed:fl1=|x3|+0.2(22)fl2=|x3|+0.4fl3=x2+0.4.TheR2andMAElhof the LF functions to the HF one are shown in Table 2.All of these functions are easily fitted using PCE due to their polynomial nature. However, our purpose here is only to show thatR2andMAElh/MARElhare providing different information of the discrepancies between the LF and HF function. As it can be seen from Table 2, the first low-fidelity functionfl1has the lowestMAElh/MARElhand relatively highR2. On the other side,fl2, which was constructed by adding an offset tofl1, has exactly the same correlation valueR2asfl1but with larger error value. As it can be seen again from the figure, the third LF functionfl3has perfect correlationR2=1with the HF function but it has largest error value among all of the LF functions. This example shows that whileR2correlation measures how much of the variance in the HF function is explained by the LF function,MAElh/MARElhmeasures the average deviation of the LF function from the HF function. Thus, in this paper we usedR2andMAElh/MARElhbecause these two measures provide different information of the discrepancies between the LF and HF functions.To measure the accuracy of the MF- and HF-PCNIPC when approximating the HF function, we used MAE and MARE of the PCE prediction relative to the HF function defined as:(23)MAE=1m∑i=1m|ypi−yhi|(24)MARE=1m∑i=1m|ypi−yhi|ȳhwhereypiis the prediction from the PCE approximation. Note that MAE and MARE without subscriptlhdenote the error between HF function and prediction, not the LF function.The borehole function is an eight-dimensional function that models water flow through a borehole  [50]. Mathematically it is expressed as:(25)fh(x)=2πTu(Hu−Hl)ln(r/rw)(1+2LTuln(r/rw)rw2Kw+TuTl)where the various parameters are as defined in Table 3.The LF function  [50] is expressed as:(26)fl(x)=5Tu(Hu−Hl)ln(r/rw)(1.5+2LTuln(r/rw)rw2Kw+TuTl).For UQ purposes, the random variables are distributed as shown in Table 3.As can be anticipated given the similarities between Eqs. (25) and (26), theR2correlation analysis of the LF borehole function with respect to the HF one confirm that the functions are highly correlated with anR2value of 0.9999. The LF function, however, is inaccurate as indicated by theMAElhvalue of 15.85, orMARElhof 0.204 to the high-fidelity function. As for references, the mean and standard deviation of true borehole function calculated by MCS are 77.6498 and 45.5953, respectively.For this problem, a 12th-order polynomial is used to approximate the LF function and aid the MF expansion. Applying total order expansion results in a rapidly increasing polynomial basis size (number of terms in the PCE) due to the high-dimensionality of the borehole function, and, because of this, we compare the MF and single-fidelity schemes using a low value of the hyperbolic truncation exponent. We applied two sets of hyperbolic exponents:ql,qc=0.45andql,qc=0.5. The growth in the polynomial basis size can be seen in Fig. 7. The number of the collocation points equals this number multiplied by the OSR value, which is 2 in our study. The convergence of the mean and standard deviation values and errors are shown in Fig. 8and Fig. 9, respectively, while the MARE histories are in Fig. 10Thanks to the very high correlation between the LF and HF borehole functions, MF-PCNIPC can provide accurate approximations of the mean and standard deviation values at lower computational cost than HF-PCNIPC. It can be seen that the higher the value ofq, the greater the proportion of the polynomial basis captured by the LF expansion, thus giving a more accurate prediction of the stochastic response surface. However, one has to be careful in determining the value ofqand order of the LF expansion by considering the available computational budget, especially if the dimensionality of the problem is high. An error level in the mean and standard deviation (compared to the result obtained by MCS) of about 0.2% can be achieved on this problem with MF-PCNIPC with a 4th-order polynomial andq=0.5, while HF-PCNIPC cannot achieve the same level of accuracy even with an 8th-order polynomial. Convergence of MARE also shows the same trend where the accuracy of MF-PCNIPC is higher than its single high-fidelity counterpart. A MARE value below 0.02 can be achieved with MF-PCNIPC with 4th-order polynomial for both value ofq, while the lowest MARE obtained by HF-PCNIPC is 0.023.The MF Branin function was first introduced in the context of testing the performance of co-kriging with different correlation levels of the LF function  [49]. We use the same function now in the context of MF approximation with polynomial chaos and analyze the effect of various LF Branin functions on the approximation quality of MF-PCNIPC. Such an investigation is important because not all LF representations will necessarily increase the accuracy of the approximation with respect to the HF one. The LF Branin function is equipped with a tunable parameterAthat alters the correlation andMAElh/MARElhbetween the LF function and the HF one. Again, note that the LF and HF functions here are not computationally ‘cheap’ and ‘expensive’ in any real sense.The HF Branin function is expressed as:(27)fh=(x2−5.14π2x12+5πx1−6)2+10(1−18π)cos(x1)+10and the LF function as:(28)fl=fh−(A+0.5)(x2−5.14π2x12+5πx1−6)2wherex1,x2∈[0,1]andAis tunable in the range[0,1].To simplify the analysis,Atakes only three values in this investigation: [0, 0.514, 1] as shown in Table 4. As for references, the mean and standard deviation of true Branin function calculated from MCS are 54.3076 and 51.2518, respectively. These parameter values correspond to three extreme levels of correlation andMAElh/MARElh. The case withA=0generates a LF Branin function that has the highest level of correlation and lowestMAElh/MARElhand theoretically is the best LF representation of the real function. In contrast, the case withA=0.514gives a LF Branin function that has the lowest correlation level and a moderate error. The case withA=1correlates quite well with the true Branin function but has the highest error. For MF-PCNIPC, we use a 10th-order polynomial for the LF expansion andqc,ql=1. The convergence of mean and standard deviation is shown in Fig. 11, while that of the errors is shown in Fig. 12. The convergence of MARE is shown in Fig. 13.The results show that with a very low degree (1 and 2) correction PCE, the LF functions withA=0andA=0.514could reduce the error of the approximation compared to the single-fidelity one. In contrast, MF-PCNIPC with the LF Branin functions given byA=1combined with a very low degree correction PCE only results in a worse approximation or gives no significant advantage for which the MARE values between the MF approximation and the true Branin function become poor or only slightly improved. This is evident with the lowest correction PCE degree of 1, but the same can also be seen for a PCE degree of 2. The higher accuracy forA=0.514compared toA=1is surprising since the former appears to exhibit no correlation at all with the HF function. The case withA=1suggests thatMAElh/MARElhalso affects the accuracy of the approximation even though the LF function has a high correlation with the real one; indeed, this confirms the previous observations of Ng and Eldred in the context of MF-NIPC  [35]. At correction PCE degree of 3, all multi-fidelity scheme could successfully reduce the approximation error compared to the single-fidelity one. To be noted is that the MF scheme with all types of LF function offers greater accuracy than the single-(high)-fidelity approach from correction PCEs of degree 3 upwards. Furthermore, there is no further decrement of MARE beyond correction PCEs of degree 3.To further investigate this result, we perform further analysis by checking the behavior of the PC coefficients of the LF and HF Branin functions. Analysis of Fig. 14 shows that the discrepancies between the LF and HF functions are mainly associated with the lower-order polynomials. The errors in the higher-order polynomials are tolerably small and need only small corrections. The LF Branin functions have different degrees of correlation with the true Branin function. Even in theA=0.514case, where the correlation is zero, the LF Branin function captures the behavior of the higher-order main effects of the true Branin function. This is why with a sufficiently high degree correction polynomial all the LF functions give a very accurate approximation: the lower-order degree approximation is corrected by the MF scheme. The HF expansion without the aid of a LF-PCE does not obtain a very accurate approximation until the PCE degree is at least 8.Results for the Branin function show that, as long as the LF function can capture the important features of the HF function, the MF scheme can yield a more accurate approximation. A questionable approximation will be obtained if there are aspects of the LF basis that are not corrected by the correction expansion and these aspects are not sufficiently representative of the HF function. In theA=0case, the higher-degree basis can mimic the features of the HF basis function very well so a more accurate approximation can be achieved.However, it is important to note that even though the LF low-correlation Branin function can be used to improve accuracy with respect to the HF-PCE on moderate degree polynomials, using a function with a very low correlation in the MF-PCNIPC scheme on real applications is not suggested or recommended. This is because the user cannot be sure whether such similarities with the HF function exist or not.The third test is the four-dimensional Park function  [51] defined as:(29)fh(x)=x12[1+(x2+x32)x4x12−1]+(x1+3x4)exp[1+sin(x3)]with its low-fidelity representation expressed as  [50]:(30)fl(x)=[1+sin(x1)B]f(x)−2x1+x22+x32+0.5wherexi∈[0,1]andBis a tunable parameter that alters the correlation andMARElhbetween the LF function and the HF one. In the original paper for the low-fidelity Park function  [50],Bis fixed to 10 while in this paper we varied theBvalue to alter the correlation andMAElh/MARElhto the high-fidelity Park function. For the experiment in this paper, we variedBto three values that represent various levels of correlation and error as listed in Table 5. For this problem, a 10th-order polynomial is used to approximate the LF function and aid the MF expansion. The value of bothqcandqlwas set to 0.8.The convergence histories of the statistical moments and those of the errors are shown in Figs. 15 and 16, respectively, with MARE histories depicted in Fig. 17. It is easier to analyze the convergence of MARE due to the difficulties in observing the convergence of statistical moments on Park function. On this particular function, the case withB=1fails to improve the MF scheme. It made the approximation became worse as it is indicated by higher MARE compared to the HF-PCNIPC. Fig. 16 shows the same trend where MF-NIPC with LF Park function ofA=1only made the absolute errors of statistical moments become larger. The failure of MF-PCNIPC using LF Park function withB=1was caused by a relatively highMAElh/MARElhand moderateR2correlation. On the other side, the MF scheme withB=4andB=10successfully reduces the approximation error when PCE with correction degree of 2 and 3 was employed. With correction PCE degree of 4, only the scheme withB=10has lower error than HF-PCNIPC. Beyond PCE degree of 4, only the MF-PCNIPC withB=10has comparable performance to the HF-PCNIPC but without evidence of improvement. This means that the correction function with LF Park function ofB=10is more complex than the HF Park function itself and need high-order polynomial terms to resolve. Nonetheless, sufficient accuracy of mean and standard deviation was still obtained although the MARE of MF-PCNIPC withB=10is comparable to HF-PCNIPC at high-order PCE. Comparing the scheme withB=4andB=10, it is clear that the scheme withB=10produces lower error. Although LF Park function withB=4has higher correlation thanB=10, its higher MARE made the approximation quality worse than the one withB=10. Results on Park function suggest thatMAElh/MARElhmight have greater influence to the accuracy of MF-PCNIPC than theR2correlation. This is because the LF Park function withB=4that has higherR2and higherMAElh/MARElhthan the one withB=10, produces multi-fidelity models with lower accuracy.We tested MF-PCNIPC on a couple of selected aerodynamic test problems. The cases tested were a 2D NACA 0012 airfoil and three-dimensional (3D) common research model (CRM) with an Euler solver, both in the transonic flow. The open source SU2 code  [52] was employed to find the flow field and obtain the aerodynamic coefficients. The work ratioW[35], which measures the ratio of the computational cost of the HF to the LF simulations, is defined by:(31)W=CChigh/CClowwhereCChighandCCloware the computational times for the HF and LF simulations, respectively.The total computational cost needed for UQ can be calculated as:(32)CCt=Nsh+Nsl/WwhereCCtis the total computational cost for UQ andNshandNslare the numbers of samples for the HF and LF simulations, respectively. If only partially-converged simulations are used for the LF simulations (as explained later), the total computational cost is calculated as:(33)CCt=Nsh+(Nsl−Nsh)/Wbecause some of the LF samples are calculated jointly with the corresponding HF samples.To calculate MARE for each aerodynamic case, a validation set consisting of 800 samples was generated using LHS. These validation samples were not the subset of the samples used to build the PCE model.The first real-world demonstration is a NACA 0012 airfoil case governed by the Euler equation. For this problem we use and compare two types of the LF simulations:•Type A: partially-converged simulationsType B: simulations with fewer mesh elements and partially-converged simulationsThe first type of LF simulation that we tested is partially-converged simulations that stop at a certain iteration before full convergence. Application of partially-converged simulations is well-established in the aerodynamic optimization field to accelerate the optimization process  [53,54] but not for UQ purposes. Here we use the partially-converged simulations to increase the accuracy of UQ at reduced computational cost in our MF framework. Using partially-converged simulations as the LF simulations has the advantage of reducing computational cost because LF samples that coincide with HF samples are evaluated together. Moreover, the computational cost and the accuracy of the LF samples can be varied by flexibly choosing the cut-off iteration of the partially-converged simulations. The criterion for fully-converged simulations in the NACA 0012 case was the drag residual reaching a value of 10−6. Eq. (33) was used to calculate the total computational cost for the MF scheme employing Type A low-fidelity samples.The second type of LF simulation has fewer mesh elements compared to the HF one: the number of elements for LF and HF are 3601 and 11,825, respectively (see Fig. 18). To further reduce the computational time for the simulation with fewer mesh elements, partially converged simulation was also used. The computational time needed for a LF simulation with fewer mesh and partially converged simulation elements is roughly 7.7% of that for a HF simulation. The LF simulation time is not negligible, so it also has to be used wisely in conjunction with HF simulations. Eq. (32) was used to calculate the total computational cost for the MF scheme employing Type B low-fidelity samples.The case is evaluated at a nominal Mach number of 0.8 and angle of attack of 2°. For this problem, the random variables are the Mach number and angle of attack with the probability distributions listed in Table 6. The quantity of interest is lift-to-drag ratio (L/D).To determine the cut-off iteration, which is the number of iteration where the simulation was stopped for partially converged simulation, we first investigated the convergence of the aerodynamic coefficient. An ensemble of measurements ofR2andMAElh/MARElhfrom 800 simulations was gathered and analyzed to appropriately define the cut-off iteration. In average, high-fidelity simulation needs about 501 iterations to reach full convergence. Our investigation shows that a very highR2value (in excess of 0.99) with lowMARElhis observed at about 100 iterations. Thus, we used a cut-off iteration value of 100 as the partially converged simulation for both meshes. This value ensures that the flow is already developed with a very high correlation and lowMARElhwhich are the criteria for accurate MF UQ. Moreover, this value represents a good trade-off between accuracy and computational cost resulting in high correlation and lowMARElh(R2=0.998,MAElh=0.0044,MARElh=0.0003) as it is shown in Table 7. The work ratio for Type A simulations is roughly about 5.01. The LF samples with a coarser mesh and partially converged simulation have a high correlation but larger errors compared to the HF mesh (R2=0.996,MAElh=0.3282,MARElh=0.0237) with a work ratio of 13.The convergence histories of the mean and standard deviation for this particular problem (NACA 0012 in Euler flow) are shown in Fig. 19and those of the errors are shown in Fig. 20, MARE convergence histories are depicted in Fig. 21. We also vary the degree offset valuer, which is defined as the offset between polynomial degree of the low-fidelity and correction expansion, by 1 and 2. The value of bothqlandqcwas set to 1. We compare these results with the statistical moments generated by a HF expansion with 7th-degree polynomials as the benchmark. Because monotonous errors convergence of mean and standard deviation is hard to observe, the error convergence plot is shown in linear scale for they-axis in both NACA 0012 and CRM cases.The results generally show that improved estimates of the statistical moments can be obtained by employing the MF scheme rather than the single HF expansion. Comparison of the Type A and Type B LF samples reveals that Type A LF simulations give a more accurate approximation of the stochastic response surface with low additional computational cost.In approximating the statistical moments ofL/D, an error level below 0.5% for the mean and standard deviation was obtained with the equivalent of 8.8 HF function calls andr=2using Type A LF samples, while the single-fidelity approximation with 6 HF functions (1st order polynomials) gives an error of 68.45% for the standard deviation. A significant gain can be obtained using Type A LF samples because the correlation with HF samples is very high. Applying Type B LF samples also yields higher accuracy in both mean and standard deviation compared to a HF expansion with a similar order of correction polynomial expansion; high correlation values contribute to this improved accuracy. However, the use of Type B LF samples with eitherr=1orr=2is not as effective as Type A samples, mainly due to the higherMARElh. Moreover, there is no additional advantage gained when thervalue is increased from 1 to 2 if Type B LF simulations were employed. The HF-PCNIPC process gives error levels in mean and standard deviation below 0.5% with 20 HF function calls, whereas when Type A LF samples are used only 8.8 HF function calls are needed to achieve the same accuracy (a saving of about 56% in computational cost). Fig. 21 shows both types of LF simulations able to reduce the MARE values compared to HF-PCNIPC up to correction PCE degree of 4 (30 HF function evaluations for HF-PCNIPC). Beyond that, the approximation error became worse for MF-PCNIPC with both types of LF simulations when the number of high-fidelity function evaluation count is higher than 42. This breakdown of accuracy means that the correction functions require a high-order PCE expansion which add additional complexity relative to the high-fidelity function; the presence of numerical noise seems to be one of the causes for this. However, MF-PCNIPC was able to efficiently reduced the approximation error on NACA 0012 case as long as the order of the correction PCE was kept to reasonably moderate value (≤4). To be noted is that at high-degree PCE (>4), MF-PCNIPC still obtained accurate values for mean and standard deviation, although its MARE is higher than HF-PCNIPC.To summarize, both types of LF samples were able to reduce the MARE compared to its HF-PCNIPC counterpart. Type A LF samples (partially-converged simulations) offer advantages over Type B LF samples (coarser mesh simulations and partially-converged simulations) in terms of accuracy. Due to the high correlation between the LF and HF samples, using a degree offsetr=2gives more benefit than usingr=1for Type A LF samples. Moreover, the use of Type A LF samples for MF-PCNIPC results in higher accuracy compared to HF-PCNIPC at relatively low additional computational cost.Simulation in aerodynamics typically requires a tight convergence criterion to ensure that the flow is already full-developed. Therefore, partially-converged simulations, although they have lowMAElh/MARElhand seems to be accurately represent the HF simulations, should not be trusted as a high-fidelity representation of the real problem. As it could be seen in Fig. 21, the use of Type A LF samples with correction PCE of high-degree results in increasing error. This means that there is additional complexities, such as numerical inaccuracy that causes noise in the function, that result in a difficulty to create high quality MF PCE approximation which relies only on the partially converged simulation.After the 2D (airfoil) numerical experiments, we tested MF-PCNIPC for the Euler simulation of a 3D common research model (CRM). This case is available among the SU2 test cases distributed freely online  [55]. For this simulation the number of mesh elements is 38,402 arranged in a structured way, as depicted in Fig. 22. HF simulations in this case are fully-converged simulations while LF simulations are partially-converged simulations after a certain number of iterations. The criterion for the full convergence is that the residual value of the drag reaches 10−6. To determine the cut-off iteration for the LF simulations, an analysis of the convergence trends inR2andMAElh/MARElhwas performed, just as for the previous aerodynamic example.This second aerodynamic test case is more complex because it involves a greater number of random variables, namely Mach number (M), angle of attack (α), sideslip angle (β), freestream pressure (P∞) and temperature (T∞), as listed in Table 8.The quantity of interest in this case is againL/D. For this case we useql,qc=0.6to reduce the number of collocation points, but still retain the interaction term between the variables. Before the multi-fidelity UQ was performed, we investigated the convergence trend of the simulation to give an information of at what typical iteration is that the partially converged simulation was predictive enough of the fully-converged one. The result shows that the iteration is quite fast to reach steady level value where the residual reach the value of 10−6 in about 600 iterations. Analysis of theR2and MARE convergence histories obtained by averaging 800 realizations shows that high correlation (R2=0.997) can be achieved at 100 iterations at which point theMAElh/MARElhis acceptably low (MAElh=0.2826,MARElh=0.0158). In light of this investigation we used simulations lasting 100 iterations as the LF samples. Averaging over 800 samples, the average iteration needed for convergence is 602 which means that the work ratio is roughly about 6.02.The convergence histories of the mean and standard deviation ofL/Dapproximated by MF-PCNIPC and HF-PCNIPC are shown in Fig. 23and those of the errors are shown in Fig. 24. Also shown, as a benchmark for comparison, is the result obtained using an 8th-order HF expansion. The trends show the approximate means and standard deviations converging towards the values given by the higher-degree PCE. The figure shows that MF-PCNIPC with degree offsetr=2is able to yield sufficient accuracy (defined as error levels below 0.5% for mean and standard deviation) at a computational cost of 45.3 HF simulations, while MF-PCNIPC withr=1and HF-PCNIPC need about 70 and 112 HF simulations, respectively. This means that the computational cost of the MF scheme withr=2is about 40% of that for HF-PCNIPC to achieve sufficient level of accuracy. From the MARE histories shown in Fig. 25, it is clear that MF-PCNIPC withr=2consistently decreases the MARE value relative to its HF-PCNIPC counterpart. Using a degree offset of 1 is not sufficient because the LF expansion captures little additional information to aid the HF (correction) expansion. By using a degree offset of 2, additional useful information is captured, thus greatly reducing the approximation error at low extra computational cost. Therefore, it is a good practice to set the offset parameter to a reasonably high value, but not too high due to the possibility of overfitting.In this example the application of MF-PCNIPC using partially-converged Euler simulations as the LF samples enables the mean and standard deviation inL/Dto approximate with sufficiently accurate at much reduced computational cost. It is important to check the values ofR2,MAElh/MARElh, and the computational fluid dynamics convergence trend to determine an appropriate cut-off iteration for the LF samples in order to maximize the benefit gained.For very expensive real-world problems it is necessary to be very strict with the computational budget. The number of samples that can be afforded might need to be determined before deciding the polynomial degree and generating the collocation points. An excellent method for single-(high)-fidelity NIPC to deal with this issue is by applying the least-angle-regression (LAR) method on a fixed number of samples. The LAR method retains the polynomial basis that makes the largest contribution and ensures the numerical stability of the approximation. However, this could cause a problem for MF-PCNIPC because there is the possibility of mismatch between the LF and correction expansions where the correction basis is not a subset of the LF-PCE. This could result in a situation where aspects of the LF basis needing correction are not corrected by the correction expansion. Therefore, we still use ordinary PCNIPC as the basis algorithm by using an OSR value of 2 for our application and leave the implementation of more advanced methods such as LAR and compressive sensing to future work.As for the sampling method, an adaptive sampling scheme could be employed but we have not explored this possibility. If the number of samples is already fixed, a good choice is to use an optimized LHS for the LF samples and select the optimum subset for the HF samples. The adaptive step could be performed afterwards to enrich the sampling plan. An advantage of the low-discrepancy sequence that we used in this paper is that its nested properties allow easy enrichment of the sampling plan. We employ low-discrepancy sequences for all the test problems described in this paper because our present focus is to develop the framework of MF-PCNIPC rather than adaptive features. Moreover, it might be difficult to achieve machine precision accuracy for real problems. This might be caused by the presence of noises, whose sources could come from round-off or discretization error in PDE-based solver.

@&#CONCLUSIONS@&#
In this paper we present a MF extension of NIPC based on regression for UQ purposes; the method itself could be employed for other applications such as building a surrogate model for optimization. In contrast to the similar framework that uses spectral projection to estimate the coefficients, the MF-PCNIPC method allows flexible sampling and choice of polynomial basis and is especially useful on high-dimensionality problems where the sparse grids/tensor product scheme suffers from the curse of dimensionality or in cases where arbitrary sample locations and numbers are needed. This is particularly advantageous if the LF function is very cheap to evaluate compared to the HF one. We tested the method on a number of artificial and aerodynamic test problems to investigate the performance and practical aspects of MF-PCNIPC on various characteristics of the problems.The results show that more accurate approximations can be obtained if the LF function can predict the behavior of the HF function sufficiently well. In cases with high correlation between the LF and HF functions (R2>0.9), the MF scheme can be safely assumed to be able to increase the approximation accuracy relative to a single HF expansion. However, it must be noted that, even if the LF and HF functions are well-correlated, the result could be disastrous if theMAElh/MARElhis high. In some cases, like the Branin function, where the correlation is low but the LF function still can capture some fundamental behavior of the HF function, the MF scheme is able to increase approximation accuracy. However, it is decidedly risky to use a LF function with low correlation to assist the HF approximation in practical applications. A successful MF approximation can be achieved with a LF function that correlates well with the HF function and has the lowest possibleMAElh/MARElhvalue. In some cases where the LF function is found to be a poor representation of the HF function (low correlation with highMAElh/MARElh), it is better to use the single-(high)-fidelity expansion alone and discard the LF samples. To determine whether the LF function is sufficiently representative of the HF function or not, theMAElh/MARElhandR2should be checked. This procedure is recommended even if there are only a few samples available. Another factor to be considered here is that it is very difficult to achieve convergence to machine precision in real problems because of the presence of numerical errors in the quantity of interest calculated by the PDE solver. Therefore, it is good practice to retain relatively low-to-moderate degree polynomials for real applications. There is also a possibility that the MARE becomes larger when high-order correction PCE is employed. However, if the LF function is sufficiently predictive, the approximated mean and standard deviation are still highly accurate when high-order correction PCE is employed.The tests on the aerodynamic problems showed that tolerable accuracy compared to ordinary PCNIPC can be obtained by employing MF-PCNIPC with lower computational cost. Partially-converged simulations turn out to provide good LF samples in these aerodynamic cases because the same physical problems are solved but without achieving fully-converged results. Nonetheless, more studies are needed in more complex cases, such as full-configuration aircraft design using a Reynolds-averaged Navier–Stokes solver. LF simulations with a coarser mesh and partially-converged simulations can also provide a moderately good prediction of the response surface to aid MF UQ. However, coarser-mesh LF simulation is not as flexible as partially-converged LF simulation because additional mesh generation is needed. Coarser meshes mean faster evaluation but with greater likelihood of higherMAElh/MARElhto the HF simulations. Tuning the fidelity level of partially-converged simulations is easy because the necessary information is already available when the fully-converged simulation is finished. The only parameter to be tuned to control the fidelity level is the iteration number for which theR2andMAElh/MARElhvalues can easily be calculated and plotted.To further improve the efficiency of MF-PCNIPC, several directions and avenues could be explored. Adaptive sampling is a promising way of focusing the samples in the locations where the errors are highest. A strategy needs to be developed to perform adaptive sampling for both LF and HF samples, especially if the computational cost of the LF samples is not negligible. Least angle regression, which has been successfully applied to regression-based NIPC, is a potential method that could be employed in the MF-PCNIPC methodology. However, practical issues such as how to correct the LF expansion with LAR (or a similar approach) also need to be considered.