@&#MAIN-TITLE@&#
Finding multivariate outliers in fMRI time-series data

@&#HIGHLIGHTS@&#
Multivariate outlier detection methods are applicable to fMRI time-series data.Removing outliers increases spatial specificity without hurting classification.Simulation shows PCOut is more sensitivity to small outliers than HD BACON.

@&#KEYPHRASES@&#
Outlier detection,fMRI,High dimensional data,

@&#ABSTRACT@&#
A fundamental challenge for researchers studying the brain is to explain how distributed patterns of brain activity relate to a specific representation or computation. Multivariate techniques are therefore becoming increasingly popular for pattern localization of functional magnetic resonance imaging (fMRI) data. The increased power of these techniques can be offset by their susceptibility to multivariate outliers, a problem not directly encountered when fMRI data are analyzed in more common univariate analysis techniques. We test how two algorithms,High Dimensional Blocked Adaptive Computationally Efficient Outlier Nominators (HD BACON) andPrincipal Component based Outlier detection (PCOut), can detect multivariate outliers in high-dimensional fMRI data, in which the number of variables is larger than the number of observations. We show how these methods can be applied to individual, voxel time-series to identify outlying voxels within a region of interest. Finally, we compare these methods with simulated data to identify which aspects of the data each method is most sensitive to. Voxels identified by both algorithms were primarily on the edges of univariate activation clusters and near the boundaries between different tissue types. Simulation results showed the PCOut outperformed HD BACON, maintaining both high sensitivity and specificity across a wide range of outlier contamination percentages. Our results suggest that multivariate analysis of fMRI can benefit from including multivariate outlier detection as a routine data quality check prior to model fitting.

@&#INTRODUCTION@&#
Within cognitive neuroscience, there is increasing interest in using multivariate pattern analysis (MVPA) to classify brain states based on the spatial (and sometimes temporal) pattern of brain activity [9,14,16,20,17]. An entire issue of NeuroImage (May, 2011) was devoted to research in decoding brain states, and many of these articles relied on MVPA. The fMRI datasets often contain multivariate observations which behave differently from the remaining of the data, the so called outliers, due to scanner stabilities, acquisition or issues in the underlying bio-medical experimental protocol. Many standard statistical methods used to characterize the multivariate signal (e.g., support vector machines) in fMRI data analysis require the assumption of homogeneity of data (i.e., free of outliers) in order to obtain correct conclusions based on the research questions of interest. For instance, the presence of outliers in an fMRI dataset can lead to wrong conclusions about which brain regions are being recruited to perform a particular task. Therefore, the task of identifying outliers is especially critical for multivariate analyses of functional magnetic resonance imaging (fMRI) data because of their increased sensitivity to small changes in the underlying neural signal ([12]).The most common way to analyze fMRI data is the massively-univariate approach [10], and therefore current outlier approaches are designed around univariate analyses (e.g., the 3dToutcount program in AFNI). Tutorials focusing on the use of MVPA in neuroimaging have also avoided the issue of multivariate outliers [15]. For small datasets, visualization of joint scatter plots between variables is often helpful for identifying multivariate outliers. For fMRI data, this approach would require the formidable task of visualizing tens of thousands of joint distributions. Instead computational techniques designed for large, high dimensional data are needed to ensure researchers employing multivariate techniques achieve accurate results without adding a large computational burden. Recently, Fritsch et al. [5] suggested using robust and regularized covariance estimate for detecting multivariate outliers in high dimensional setting where the dimension (i.e., p) is low, around 100 and, also they treat entire subjects as the observations (n=1500). It is well known that the Minimum Covariance Estimator (MCD) [18] method, which was used in the proposed algorithms by Fritsch et al. [5], is suitable for medium-sized datasets, but it is too slow for data-mining type of applications due to convergence issue [7].Section 2 of this report motivates the need for multivariate outlier detection independent of univariate outlier detection and describes two techniques designed to identify outliers in high-dimensional data. Section 3 shows how these techniques can be applied to fMRI data from a visual object recognition task [8]. Section 4 compares the methods using simulated data to show how each method trades off detection rate and false alarm rate. Section 5 provides some conclusions and future directions for the use of outlier detection methods in fMRI research.A natural way to characterize an outlier is as an observation that does not fit the general pattern of the data. For a single variable, we can assess how unusual an observation is by first mean-centering the data and then scaling it by the standard deviation. If the variable is approximately normally distributed, the magnitude of the vast majority of these standard scores should be less than 3 – observations with standard scores above 3 warrant closer inspection. With more than one variable, we have to consider how to calculate the distance along both dimensions.Fig. 1 shows a bivariate scatterplot relating two variables, X1 and X2. The scatterplot clearly identifies the outlying data point, but if we calculate the distance by simply adding the scaled distance for each variable, the outlier does not appear to have an abnormal distance. Critically, when the data have more than one dependent variable, we must take into account the correlation between each variable to identify unusual observations. The Mahalanobis distance [13] captures this intuition, taking into account both the scale of each variable and the covariance between variables:(1)δi=(xi−μ^)′Σ^−1(xi−μ^)where xiis the ith row in the data matrix of X,μ^is the vector of column means, andΣ^is the sample variance–covariance matrix. If the variables are independent, the off-diagonal components ofΣ^will be identically 0 and reduce to the sum of squared standardized distances along each dimension. Assuming the underlying Xiare jointly multivariate normal, the distances can be sum of squared standardized distances along each dimension. Assuming the underlying Xiare jointly multivariate normal, the distances can be compared to quantiles from the χ2 distribution (with p the number of variables, or dimension of the dataset) and a suitable cutoff determined (e.g., the 95th percentile). When we calculate the Mahalanobis distance for each observation of X1 and X2 the outlier is immediately obvious (Fig. 1).The Mahalanobis distance has two specific problems that limit its effectiveness for many data types, including fMRI time-series data. First, the distance measure relies on adequate estimates of the mean vector and covariance matrix of the dataset. The estimates of these parameters, however, are themselves biased by the presence of outliers. The scenario in which we would like to use the Mahalanobis distance is thus the situation the measure becomes invalid. Second, fMRI data often have more variables than observations (p>n), rendering the covariance matrix is non-invertible and therefore the Mahalanobis distance cannot be calculated. An additional concern with straight-forward application of the Mahalanobis distance, even if it could be calculated, is that its usefulness for outlier detection rests on the assumption that the shape of the data is well-approximated by a high-dimensional ellipse, a difficult condition to verify for raw fMRI data. Next, we present two modern outlier detection algorithms that attempt to overcome these limitations while still maintaining computational efficiency.Outlier nomination with the BACON algorithm. Billor, Hadi, and Velleman present an algorithm, called Blocked Adaptive Computationally efficient Outlier Nominators (BACON), that addresses the first problem of the Mahalanobis distance directly [2,6]. The algorithm first selects a subset of the data that is close to the median vector, and therefore assumed to be free of outliers. BACON then iteratively adds new observations only if the Mahalanobis distance of the excluded observations is less than a scaled χ2. Importantly, the mean vector and covariance matrix used by the Mahalanobis distance are calculated using only the “good” subset of the data (which grows each iteration), avoiding the biasing effect of the potential outliers. The algorithm iterates until the size of the clean subset does not change. Any observations not included in the final clean dataset are flagged as potential outliers. In practice, the algorithm converges on the initial subset in just a few iterations even for datasets with tens of thousands of observations, and shows excellent detection rates even when the percentage of outliers in the dataset nears at 40% [1,2], beyond which the estimation of the initial clean subset becomes more difficult and the algorithm begins to break down.As originally formulated, the BACON algorithm relies on the Mahalanobis distance for each iteration, requiring an inversion of the covariance matrix and rendering it unusable with high dimensional data. Kondylis et al. [11] showed how that the iteration phase of the BACON algorithm could be applied to a robust projection of the dataset (akin to dimension reduction via principal components), rather than on the original dataset. Kondylis also suggested a modification of the cutoff value because of the high dimension of the data. This modified cutoff value takes into account the original size of the dataset and makes the algorithm less likely to nominate a given point as an outlier. We will see later that specifying this cutoff value is critical to the algorithm׳s overall success. We refer to this modified BACON algorithm as HD BACON to emphasize its applicability to high dimensional data, although in principle the dimension reduction step could be applied to any multivariate dataset. We believe that it is helpful to summarize the algorithm in point format as below.Algorithm 1HD BACON [11]Input: A multivariate dataset, Xn×p with p>n. A choice for initial subset size, m in {k, k +1,…, n}Step 1: Obtain robust scores matrix,X˜n×k=(X−μ^)V˜whereμ^is the estimate of the true mean population vector andV˜p×k=X′Uwhere Up×p is the matrix of the eigenvectors of the covariance matrix of the data matrix X.// Obtain initial subset, Xm×kStep 2a: Calculate δi for each observation inX˜usingδi2≈∑j=1kx˜ij2λjwhereX˜ijdenoting the value in the ith row and jth column of the robust scores matrix,X˜n×kand k indicating the reduced dimension of the dataset.Step 2b: X⁎← the m observations fromX˜with smallest δiStep 3: BACON Iterationr0←0r1←m// r0 and r1 hold the size of the previous and current subset, respectivelywhile (r1≤n and r0≠r1) dor0←r1Obtain C the sample covariance andμ^using X⁎Calculate δi for each observation inX˜as in Step 2aX⁎← observations fromX˜with δi<cnkrχk,α/p withcnkr=c1+c2c1=1+k+1n-k+2n−1−3k,c2=max{0,n+k+1−2rn+k+1+2r}r1←number of observations in X⁎end whileO(n−r1)×k←observations fromX˜not in X⁎Output: A (possibly empty) set of observations,O, nominated as outliers. Robust location and scale estimates,μ^and S, may be calculated from the clean set of observations X⁎r1×k.Outlier nomination with PCOut. PCOut is another modern outlier nomination algorithm developed specifically to handle large, high-dimensional data sets [4]. The algorithm looks for points that have either a different mean vector or covariance matrix than the true data. For fMRI data, the points in space are a given voxel׳s BOLD signal value over time. The true data are taken to be the idealized time series evoked by a given stimulus. PCOut begins by robustly rescaling the data and reducing dimension via principle components, keeping only the components comprising a desired percentage of the total variance. Next, the remaining components are robustly rescaled. To determine location outliers, a robust Mahalanobis distance is calculated, weighting each dimension by the absolute value of a robust kurtosis measure, with an additional transform to allow the distances to more closely follow a χ2 distribution. These distances are then used to assign a weight to each observation. Observations with distances at least 2.5×MAD from the median distance are assigned a weight of 0, whereas the lower one-third of the distances have a weight of 1. Intermediate distances are assigned intermediate weights (smoothly). In the scatter outlier step, PCOut assigns weights in a similar manner. The weights from each step are combined (and scaled) to provide an overall weight for each observation. After comparison with a suitable cutoff, points with sufficiently low combined weight are flagged as potential outliers. It is helpful to summarize the two stages of the algorithm in point format as in the following:Algorithm 2PCOut [4]Input: A multivariate dataset, Xn×p with p>n.Phase 1: detection of location outliersStep 1:Robustly sphere the data matrix X using the coordinatewise median and the MAD (mean absolute deviation) asxij=xij−med(x1j,…,xnj)MAD(x1j,…,xnj);j=1,…,p.Calculate the sample covariance matrix of the transformed data X⁎.Step 2:Compute a principal component decomposition of the semi-robust covariance matrix from Step 1 and retain only those p⁎ eigenvectors whose eigenvalues contribute to at least 99% of the total variance. Robustly sphere the transformed data aszij⁎=zij−med(z1j,…,znj)MAD(z1j,…,znj);j=1,…,p⁎.Step 3: Compute the robust kurtosis weights for each component aswj=|1n∑i=1n(zij⁎−med(z1j⁎,…,znj⁎))4MAD(z1j⁎,…,znj⁎)4−3|,j=1,…,p⁎.hence weighted norms for the sphered data from Step 2. Since the data have been scaled by the MAD, these Euclidean norms in principal component space are equivalent to robust Mahalanobis distances. Transform these distances asdi=RDiχp⁎,0.502med(RD1,…,RDn);i=1,…,n.Step 4: Determine weights w1i for each robust distance according to the translated biweight asw1i={0,di≥c(1−(di−Mc−M)2)2,M<di<c1,di≤Mwith M equal to the3313rd quantile of the distances (d1,…,dn) and c=med(d1,…,dn)+2.5×MAD(d1,…,dn).Phase 2: detection of scatter outliersStep 5:Use the same semi-robust principal component decomposition calculated in Step 2 and compute the (unweighted) Euclidean norms of the data in principal component space. Transform them as in Step 3 to yield a set of distances for use in Step 6.Step 6:Determine weights w2i for each robust distance according to the translated biweight as in Step 4 with c2 equal to theχp⁎299thquantile and M2 equal to theχp⁎225thquantile.Combining Phase 1 and Phase 2: Use the weights from Steps 4 and 6 to determine final weights for all observations according to equationwi=(w1i+s)(w2i+s)(1+s)2where typically the scaling constant s=0.25.PCOut performs reasonably well with low-dimensional data in comparison to other methods, but shows increased sensitivity and specificity as the number of variables increased (for a fixed n). At n=p=2000, PCOut detected over 99% of the outliers, and had a false positive percentage of around 3%. PCOut also performed well on real datasets that had large number of variables and only a few observations [4]. Important for the current work, the computational time of PCOut did not grow as sharply with increases in the number of variables as other algorithms Filzmoser and colleagues investigated.To assess the PCOut and HD BACON algorithms on a real dataset, we selected a publicly available visual object recognition dataset that included six participants [8]. This dataset is distributed as part of the PyMVPA software (www.pymvpa.org), and has been used to benchmark classification performance using various regimes. A primary result from the study was that the class of a visual object (e.g., a house vs. a face) that a participant is viewing may be reliably inferred from the multivariate pattern in the fMRI BOLD signal in a region of interest (ROI) in the ventral temporal region of the brain. The primary goal of the current analysis was to determine if this ROI contained multivariate outliers that may have biased classification performance. The outlier algorithms were completely blind to the design matrix (i.e., what the participant was experiencing at a given time), operating only on the obtained data matrix to ensure that we do not bias which voxels are selected for further analyses.Description of fMRI dataset. Each of the six participants in the Haxby et al. [8] dataset were measured in 12 separate functional runs, but the data are here analyzed as a single run with n=1452 time points. To decrease the likelihood that the algorithms were contaminated by nuisance variance (e.g., scanner drift) we removed polynomial trends in each voxel׳s time series up to order 10 (using the AFNI program 3dDetrend). We restricted our analysis to voxels in the ventral temporal region of the brain (using the functional masks that accompanied the dataset), selected by a univariate analysis to be important for the visual object recognition task [8]. The ROI approach is desirable because it eliminates some of the signal heterogeneity inherent to voxels in different locations in the brain and comprised of different substances (e.g., gray/white matter, cerebrospinal fluid). After masking, the algorithms were given a matrix with 456 voxels (on average, the ROI approach often leads to differing numbers of voxels across subjects) measured at 1452 time points. The voxels are the observations and the time points are the dimension of the data. Note that the dimension of the data (1452) is around 3 times larger than the average number of voxels (mean=456) in each ROI. The goal of each algorithm is to find voxels that have abnormal patterns of activity over time in order to have an unbiased estimate of both the average time series and the voxels that are important for visual object recognition.Voxel Outlier Results. We first consider a single subject before presenting the average results across subjects.Fig. 2 shows the distance measure for each of the 662 voxels for this subject from each algorithm. For HD BACON, the distance measure is the scaled Mahalanobis distance of each data point (after dimension reduction) for each data point, using the center vector and covariance matrix from the non-outlying observations. For PCOut, the final weight for each observation ranges from 0 to 1 with lower values corresponding to a further distance from the center of the dataset. To facilitate comparison with HD BACON, we used 1 – this final weight in all comparisons. In this subject, PCOut flagged a higher proportion of the voxels as potential outliers (19% vs. 12%). The overall agreement in the distance ranking of the voxels, is strong, however, as indicated by a Spearman correlation of 0.85. We use the Spearman correlation to compare the distances because it relates the rank of the distances. If two algorithms produce identical rankings of voxels, but their distance measures have a non-linear relationship, the more common Pearson correlation can show a low correlation. We also considered the distribution of voxel distances as a way to assess the adequacy of the outlier cutoff. If the distribution of distances shows strong separation between the non-outliers and outliers, we can have more confidence in the threshold. Histograms (Fig. 2) show that PCOut produced a sharper distinction between outliers and non-outliers than HD BACON.Across subjects, PCOut nominated a higher percentage of voxels as outliers (mean=0.16, SD=0.04) than HD BACON (mean=0.10, SD=0.04). The average Spearman correlation was strong, however (mean=0.79; SD=0.06), indicating that both algorithms were sensitive to the same properties of the dataset. This strong relationship between the output from the two algorithms suggests they primarily differ in terms of the threshold (which may be modified) used to flag a given voxel as an outlier. Future work should consider a data-driven way to set these thresholds rather than relying on a priori assumptions about the distribution of the data.To compare the spatial clustering of the potentially outlying voxels,Fig. 3 overlays the distance measures from PCOut and HD BACON onto the high-resolution anatomical scan (from the same subject considered previously). The coronal and sagittal slices shown at top include slice guides to indicate the location of the axial slices containing the overlays. Both algorithms nominate voxels in the more inferior locations, as well as those located near ventricles or the gray/white matter boundary. Additionally, some voxels in the cerebellum are flagged as potential outliers. Because these voxels may be sampling from tissue with very different magnetic properties than gray matter, it is unsurprising that these voxels have different time-courses. The major benefit of the current algorithms, then, is their ability to flag voxels in a way that allows for easy follow-up inspection by the researcher, but still allowing the researcher to exclude voxels from further analysis.Finally, we looked in detail at the time course of the voxels that were flagged as outliers by both algorithms to determine if any particular features distinguished outlying and non-outlying voxels.Fig. 4 shows a portion (363 time points, corresponding to the last 3 functional runs) of the BOLD signal over time for 5 non-outlying and 5 outlying voxels (we chose these voxels randomly from the set that were given the same label by HD BACON and PCOut). The primary distinguishing feature of the outlying voxels is the larger variability in their time-series (mean SD=53.2) compared to non-outlying voxels (mean SD=23.1). The distribution of these values (Fig. 4, right) shows that the range of the outlying voxels was much higher as well.Across subjects, the standard deviation of voxels that both algorithms agreed were outliers was more than twice the standard deviation of non-outlying voxels (mean ratio=2.2, SD=0.2). Because this feature is so prominent, and because both algorithms use the difference in variability to identify voxels, we assessed the similarity of the distance measure for each algorithm and with the simple standard deviation measure. Across subjects, each voxel׳s standard deviation was strongly correlated with the distance measure from both HD BACON (mean Spearman correlation=0.91, SD=0.02) and PCOut (mean Spearman correlation=0.79, SD=0.07). This high correlation between the univariate voxel standard deviation and the distance measure suggests that the algorithms (especially HD BACON) were sensitive to differences univariate data, rather than making heavy use of the correlations across voxels. This effect may be exaggerated in the current dataset, in which many different kinds of stimuli were used, but a given voxel may be primarily sensitive to only a single type of visual stimulus. The lack of strong correlations across voxels sensitive to different stimuli Haxby et al. [8] would lead to a larger number of retained dimensions (akin to the number of unique voxel time series) by both PCOut and HD BACON than an ROI defined by only a single stimulus type (e.g., voxels that respond to pictures of faces significantly more than to any other visual stimulus type).Comparison with univariate methods. To test if the particular dataset had a multivariate structure that would require multivariate outlier methods, we compared (using the Spearman rank correlation) the distance measures from each method for each subject to a univariate measure that calculated the sum of the squared z-scores for each voxel at each time point (cf. Fig. 1B). Across subjects, HD BACON showed a high rank-correlation with the univariate distance measure (mean correlation=0.93). PCOut also showed a reasonably large correlation with the univariate measure (mean correlation=0.82). A Wilcoxon rank sum test confirmed that PCOut had a significantly lower correlation with the univariate measure than HD BACON (W=36, p=0.002). The magnitudes of these correlations indicate that although the rankings of the voxels are similar across methods, they are still distinguishable between methods. We return to the difference between the univariate and multivariate algorithms when we consider simulated data where the ground truth is known.Impact of outliers on brain classification. Researchers interested in decoding multivariate patterns have a wide range of available analysis approaches, but the presence of multivariate outliers in the dataset may lead to incorrect conclusions about which brain areas are active, or those that are most informative for a given classification task. The aim of the outlier identification step would be to increase out-of-sample generalization (e.g., cross-validation performance) by providing a more representative training dataset to the classifier.We tested the impact of outlier removal on classification performance using the current object recognition dataset. We trained a support vector machine (SVM) to decode the object class each subject was viewing. Because each subject was shown 8 different object types and the data were recorded in 12 runs, we created 96 samples that could be classified. Each object type was shown for 18s, and the samples represented the mean BOLD signal for a given voxel for 6–18s (we did not use the first 6s to allow time for the rise of the hemodynamic response). We first assessed baseline SVM performance across 100 leave-one-out classification runs. Each run left out one sample from each class, so that the training set for each run contained 88 samples (11 from each class) and the testing set contained 8 samples (1 from each class). We repeated this analysis after removing either the voxels nominated by HD BACON or the voxels nominated by PCOut as outliers.The baseline SVM performance ranged across subjects from 36% to 67% (mean=52%), reasonable results for brain decoding using SVM on data with 8 possible classes (nominal chance rate of 12.5%). Performance was largely unchanged after removing outliers nominated by HD BACON (average change=0.40%) and PCOut (average change=0.35%). The indifference to outlier removal suggests that PCOut and HD BACON are not removing voxels contributing greatly to classification performance. Alternatively, for the given dataset, many voxels may code similar information, and so removing arbitrary voxels might produce the same small boost to performance. However, for each subject, randomly removing voxels (the number removed was the mean of the number of outliers nominated by HD BACON and PCOut) led to an increase in error as compared to outlier removal (average change=−2.6%). Although small in magnitude, the direction and relative size of the difference as compared with outlier removal suggests that the outlier algorithms are not removing arbitrary voxels.The previous section showed how HD BACON and PCOut could be used to find outliers in fMRI time-series data. Without ground truth about a voxel׳s state, however, we cannot make strong claims about either algorithms performance, beyond noting that PCOut is more likely to flag voxels as outliers and that the standard deviation of the time-series predicts the algorithms׳ behavior. In this section, we directly compare HD BACON, PCOut and the univariate, scaled distance measure (as in Fig. 1B), using a dataset with known percentages of outlier contamination.Description of the dataset. Each simulation started with data generated according to the general experimental design from the Haxby data just considered. The ideal time series were calculated using 3dDeconvolve in AFNI [3] and the event-timing file distributed with the Haxby dataset. We simulated the BOLD response by generating noisy (Gaussian noise) approximations to the ideal time series for each stimulus type specified in the event timing file. The amount of noise increased proportional to the amplitude of the ideal time series. The dataset contained 800 voxels measured at 1452 time points. The 800 voxels were split into 8 types of voxels, which were designed to respond primarily to one class of stimuli and weakly to other classes by creating weighted averages of the ideal time series for each type. To induce autocorrelation within each voxel, the value at time T was a weighted average of that voxel׳s value at time T−1 and the noisy sample from the ideal time series.We randomly selected a percentage of the 800 voxels to be outliers. We created two kinds of outliers, and tested them in separate simulations. The first kind, labeled mean-zero outliers, were generated as noisy approximations to a time-series with expected value of 0 at each time point. We induced an autocorrelation in the outlier voxels using the same procedure as for the true voxels. We created 100 datasets with these mean-zero outliers using each of 6 different outlier percentages (5%, 10%, 20%, 30%, 40%, and 50%) to obtain representative values for the sensitivity and specificity of each algorithm at each percentage.The second type of outlier was created to mimic weakly active voxels. We generated these voxels identically to the true voxels, except we scaled the ideal time series for the appropriate class using a random draw from a Uniform (0.25, 0.75). As with the mean zero outliers, we tested 6 outlier percentages and using 100 sample datasets for each. We included this test because an aggressive outlier algorithm may detect the mean difference between these weak voxels and the stronger voxels and flag them as outliers, even though they have similar temporal structure. Depending on the scientific question, this may or may not be a positive feature. Regardless, it is important to know how the algorithms handle data with a large portion of strongly active voxels and only a portion of weakly active.Simulation results. We first compared the hit rate (correct outlier identification) and false alarm rate (incorrectly identifying a good voxel as an outlier) for each algorithm for data that contained the mean zero outliers (Fig. 5A). For outlier percentages 5% through 40%, both HD BACON and PCOut showed hit rates near ceiling. The univariate algorithm shows a hit rate of only around 0.5 for the lowest outlier percentage and performance steadily declines, reaching 0.0 for percentages around 20%. All of the algorithms break down once the percentage reaches 50% there is no way to know which part of the data are most representative. Considering the false alarm data, both PCOut and HD BACON show very low false alarm rates from 5% to 40% outliers. Despite its lower hit rate, the univariate method also has a higher false alarm rate showing that it is not just a cutoff placement problem.We next compared the algorithms using the datasets containing the weakly active outliers (Fig. 5B). These outliers will have a higher correlation with the true voxels than the mean zero outliers, but will differ by a scale. Most strikingly, above 5% outliers, HD BACON showed no sensitivity at all. Even at the lowest outlier percentage, the hit rate for HD BACON was 0.28, lower than both the univariate method (hit rate of 0.37) and PCOut (hit rate of 0.94). From 5% to 20%, PCOut had hit rates over 0.90, but declined to 0.58 by 40%. As with the mean zero outliers, the 50% outlier percentages show hit rates near 0 for all three algorithms.The flat performance of HD BACON for the “weakly active” outliers is caused by how it estimates the initial “clean” subset of the data. Recall, the algorithm uses the distance from the coordinate-wise (here, time points) median to select the initial subset. For fMRI data, because a given voxel is only active for a relatively small percentage of the time, the median (centered and scaled) BOLD Signal across voxels at a given time point will be close to 0, even if there are no outliers in the dataset. In our test dataset, the weakly active time series voxels peaks differ by only a scale and so once noise is added to the signal they can look very similar. In contrast, PCOut and the univariate method are much more sensitive to these small changes in the mean signal value and flag a large portion of these values as outliers.To illustrate in detail how the algorithms are affected by increasing outlier percentages, we compared the outlier classification results for the 5% and 40% outlier percentages.Fig. 6A compares the voxel distances assigned by each algorithm at 5% outlier contamination for the mean zero outliers. HD BACON shows the most separation between the outlier and non-outlier voxels, leading to its better false alarm rate. The univariate method shows a lack of a clear distinction between the outlier and non-outlier voxels. Once the outlier percentage is changed to 40% (Fig. 6B), the difference between the univariate and multivariate algorithms is even more dramatic. HD BACON and PCOut are robust to this increase in outlier percentage, whereas the univariate measure incorrectly estimates the true dataset as being all the outliers (because of the low mean value) and the good voxels that happen (because of random noise) to have small deviations from 0.Fig. 7 shows the voxel distances assigned by each algorithm at the 5% and 40% outlier percentages using the weakly active voxels. At 5% the outlier voxels have high distances for both HD BACON and PCOut, but the univariate shows poor separate between the true data and the outliers. When the outlier percentage is 40%, the three algorithms respond quite differently. HD BACON includes the weakly active voxels in its estimate of the true data and nominates no voxels as outliers. PCOut also shows high specificity (0.0 false alarm rate), but nominates some of the weakly active voxels as outlying and others as having the same distance as the rest of the voxels. The univariate measure shows the opposite pattern, with a decreased hit rate and a marginal increase in false alarms. Interestingly, HD BACON and the univariate measure show a distinctly different ranking of the voxels than PCOut. Simply changing the threshold would not produce similar results for HD BACON and PCOut, emphasizing that they are tapping into different parts of the data.We tested two general-purpose multivariate outlier nomination algorithms, HD BACON and PCOut, using actual and simulated fMRI data. In actual data, PCOut nominated more voxels as potential outliers than HD BACON, although the ranking (by distance) of the voxels was similar across algorithms. Comparing the results with a univariate outlier measure revealed that the HD BACON results were largely similar (though distinguishable) to a method that did not consider the correlation structure of the data. PCOut showed a smaller correlation with the univariate measure than HD BACON. The large correlation with the univariate measure may indicate more about the structure of these particular data, rather than a lack of sensitivity to the multivariate structure of the data.Comparing the impact of outlier removal on brain state classification revealed negligible changes in accuracy. This result is positive because it shows that classification on the unaltered dataset was not due merely to the presence of outliers in the data. Reducing the number of voxels in the final “clean” set has the additional benefit of restricting which voxels are considered important for a given task. The ability to confine the spatial extent of a given pattern may provide insights about the kinds of information processed within a given brain region. Importantly, the outliers in the current dataset were found after a preliminary feature selection stage, indicating that feature selection itself does not provide a guard against multivariate outliers.Using simulated data, we showed that PCOut was more sensitive than the HD BACON algorithm to even small differences in voxel time-series. Both multivariate algorithms showed consistent performance from 5% to 40% outlier percentages, but showed no sensitivity at 50% contamination. For mean-zero voxels, both multivariate algorithms performed well, but PCOut showed much higher sensitivity to detect the “weakly active” voxels. This sensitivity is a double-edged sword. On one hand, these voxels were generated from a different distribution, and are anomalous in that sense. On the other hand, a researcher might prefer a more inclusive outlier algorithm if these scale differences are unimportant for the question at hand. For example, if a researcher is only interested in finding a small subset of voxels that best decode a pattern of activity, filtering out weak voxels may be beneficial as they will have lower signal-to-noise ratios. Alternatively, if a researcher is interested in understanding how the spatial extent of a pattern׳s distribution across the brain, such a strict algorithm would not be preferred. Because HD BACON and PCOut produce a different ranking of the voxels, achieving these different goals would require switching algorithms, not just lower (raising) an outlier threshold.When compared to a simple univariate outlier method, the multivariate methods showed both higher hit rates and lower false alarm rates across a wide range of simulation conditions. Because the univariate measure is not taking into account the correlation between the voxels it is, in effect, is standardizing by a value that sometimes over estimates (low hit rate) and other times underestimates (high false alarm rate) the multidimensional shape of the true data. This error is made worse as the percentage of outliers in the data increases and the estimates of the mean and standard deviation (used for the distance calculation) are themselves influenced by the presence of the very outliers the method is trying to detect.A limitation of these multivariate outlier techniques is that they were designed for the case of stationary, multivariate distributions rather than consider a stationary distribution for each voxel, outlier approaches based on functional data analysis techniques (e.g., [21]) may provide a more principled way to detect anomalous voxels and show greater deviations from univariate techniques. These approaches are able to estimate a smooth function for the voxel times-series while still achieving dimension reduction (in a way similar to PCOut and HD BACON). Recent work on outlier detection methods for such functional data ([19]) may provide an improvement over the current methods and a more principled approach to time-series outlier detection in general.As the resolution of fMRI scanners increases, the detection power of multivariate analysis techniques will increase, but so will their sensitivity to multivariate outliers. Continued use of multivariate techniques should be met by an increase in the development and deployment of diagnostic procedures that can handle the special features of fMRI time-series data.None declared.

@&#CONCLUSIONS@&#
