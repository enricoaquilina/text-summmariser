@&#MAIN-TITLE@&#
A hybrid metaheuristic approach for the capacitated arc routing problem

@&#HIGHLIGHTS@&#
The CARP is a difficult combinatorial optimization problem.We present a new hybrid metaheuristic approach (called HMA) for the CARP.HMA integrates an effective tabu thresholding procedure within the memetic framework.HMA quickly discovers the (improved) best known results for almost all benchmarks.

@&#KEYPHRASES@&#
Capacitated arc routing problem,Memetic search,Tabu thresholding,

@&#ABSTRACT@&#
The capacitated arc routing problem (CARP) is a difficult combinatorial optimization problem that has been intensively studied in the last decades. We present a hybrid metaheuristic approach (HMA) to solve this problem which incorporates an effective local refinement procedure, coupling a randomized tabu thresholding procedure with an infeasible descent procedure, into the memetic framework. Other distinguishing features of HMA include a specially designed route-based crossover operator for solution recombination and a distance-and-quality based replacement criterion for pool updating. Extensive experimental studies show that HMA is highly scalable and is able to quickly identify either the best known results or improved best known results for almost all currently available CARP benchmark instances. In particular, it discovers an improved best known result for 15 benchmark instances (6 classical instances and 9 large-sized instances whose optima are unknown). Furthermore, we analyze some key elements and properties of the HMA-CARP algorithm to better understand its behavior.

@&#INTRODUCTION@&#
The capacitated arc routing problem (CARP) has been the subject of a large number of studies during the last decades due to its wide applicability in logistics, such as household waste collection, product distribution, winter gritting and postal deliveries, among others (Dror, 2000). The CARP model can be informally described as follows. We are given a graph with a set of vertices and edges, where each edge has a predefined traversal cost and where a subset of edges, which are required to be serviced by some vehicles, are additionally associated with a service cost and a demand. A fleet of identical vehicles with a limited capacity is based at the depot vertex. The objective of CARP is to find a set of vehicle routes with a minimum cost such that: (1) each required edge is serviced on one of the routes; (2) each route must start and end at the depot vertex; and (3) the total demand serviced on the route of a vehicle must not exceed the vehicle capacity.From a theoretical point of view, CARP is known to be NP-hard (Golden & Wong, 1981), and hence is not expected to be solved by any exact algorithm in a polynomial time in the general case. The computational difficulty of solving CARP is also confirmed in practice. Indeed, the best existing exact algorithms are limited to moderate instances with only 140 vertices and 190 edges (Baldacci & Maniezzo, 2006; Bartolini, Cordeau, & Laporte, 2013; Bode & Irnich, 2012). For these reasons, intensive research has been devoted to developing heuristic and metaheuristic methods. Representative heuristic methods include Augment-Merge (Golden & Wong, 1981), Path-Scanning (Golden, DeArmon, & Baker, 1983), the route first-cluster second heuristic (Stern & Dror, 1979) and Ulusoys Heuristic (Ulusoy, 1985). Among the metaheuristic methods, neighborhood search approaches are popular, e.g., tabu search (Brandão & Eglese, 2008; Hertz, Laporte, & Mittaz, 2000; Mei, Tang, & Yao, 2009), variable neighborhood search (Hertz & Mittaz, 2001; Polacek, Doerner, Hartl, & Maniezzo, 2008), guided local search (Beullens, Muyldermans, Cattrysse, & Van Oudheusden, 2003), GRASP with evolutionary path relinking (Usberti, Paulo, & André, 2013). As another class of popular metaheuristics for tackling CARP, population-based algorithms generally achieve better performances, such as the memetic algorithm (Lacomme, Prins, & Ramdane-Cherif, 2004; Tang, Mei, & Yao, 2009), the ant colony algorithm (Santos, Coutinho-Rodrigues, & Current, 2010) and the cooperative co-evolution algorithm (Mei, Li, & Yao, 2014). Among these methods, two population-based algorithms (MEANS Tang et al., 2009 and Ant-CARP Santos et al., 2010) and one local search algorithm (GLS Beullens et al., 2003) represent the state-of-the-art solution methods for the classical test instance set, while RDG-MEANS (Mei et al., 2014) is the current best performing algorithm for the large-scale CARP (LSCARP) instances. Finally, for a thorough and up-to-date discussion of arc routing problems, the reader is referred to the recent book (Corberán & Laporte, 2015) edited by Corberán and Laporte and in particular, chapter 7 by Prins dedicated to heuristic approaches.In this work, we investigate a new population-based algorithm under the memetic search framework (Moscato, 1989). Memetic algorithms (MAs) have been proved to be very effective for solving a large number of difficult combinatorial optimization problems (Moscato & Cotta, 2003; Neri, Cotta, & Moscato, 2011), including CARP (Lacomme et al., 2004; Tang et al., 2009). The success of a MA highly depends on a careful design of two key search components: the crossover operator and the local refinement procedure (Hao, 2012). Based on our previous experiences on MAs applied to various combinatorial problems, we go one step further by providing innovations for these two key components (crossover and local refinement) with the goal of creating a more effective memetic algorithm able to surpass the current state-of-the-art CARP methods.The main contributions of our work can be summarized as follows.•From the algorithmic perspective, the proposed population-based hybrid metaheuristic approach (HMA) combines a powerful local refinement procedure to ensure an effective search intensification with a dedicated crossover operator specially designed for CARP to guarantee a valid search diversification. The local refinement procedure couples a randomized tabu thresholding procedure to locate high-quality feasible solutions, with an infeasible descent procedure to enable tunneling between feasible and infeasible regions. The dedicated crossover operator relies on route information that can be embodied in exchanges of parent solutions to create new promising solutions. Additionally, to maintain a healthy population diversity and to avoid premature convergence, HMA employs a quality-and-distance strategy to manage the pool of solutions using a dedicated distance measure.In terms of computational results, extensive experiments carried out on 8 sets of widely used benchmarks show the competitiveness of the proposed method compared to the state-of-the-art CARP algorithms in solution quality and computational efficiency. For the 7 sets of 181 small-sized and medium-sized instances, HMA consistently matches or improves on all the best known results. In particular, HMA discovers a new best known result (new upper bound) for 6 well-studied instances. For the last set of 10 large-sized CARP benchmarks, HMA exhibits an even better performance. It easily dominates the state-of-the-art algorithms, including those specially designed for these CARP instances, by finding 9 new best known solutions, while yielding significantly smaller average gap values, thus demonstrating the outstanding scalability of the proposed method.The rest of the paper is organized as follows. Section 2 introduces preliminary notation and the solution representation. Sections 3 and 4 are dedicated to the description of the main HMA algorithm. Section 5 presents the computational results. Section 6 investigates some key elements of HMA, followed by the conclusions in Section 7.We are given a graph G(V, E) with a set of vertices (V), a set of edges (E), a set of required edges (ER⊂ E) and a fleet of identical vehicles with a capacity of Q that is based at the depot vertex vd(vd∈ V). Each edgee=(i,j)∈Eis represented by a pair of arcs < i, j > and < j, i > . A required edge is said to be served if and only if one of its two arcs is included in one vehicle route of the routing plan. For the sake of simplicity, we use the term task to represent a required edge hereafter. Let n be the number of tasks, i.e.,n=|ER|. Each arc of a task, say u, is characterized by four elements: the head vertex (head(u)), the tail vertex (tail(u)), the traversal cost (tc(u)) and the demand (q(u)).To represent a CARP solution, we assign to each task (i.e., a required edge) two IDs(i,i+n)where i is an integer number in [1, n], i.e., one ID for each arc of the task. We also define a dummy task with 0 as its task ID and both its head and tail vertices being the depot vertex vd. This dummy task is to be inserted somewhere in the solution as a trip delimiter. Suppose a solution S involves m vehicle routes, S can then be encoded as an order list of (n+m+1) task IDs among which (m+1) are dummy tasks:S={S(1),S(2),…,S(n+m+1)},where S(i) denotes a task ID (an arc of the task or a dummy task) in the ith position of S. S can be also written as a set of m routes (one route per vehicle):S={0,R1,0,R2,0,…,0,Rm,0},where Ridenotes the ith route composed of |Ri| task IDs (arcs), i.e.,Ri={Ri(1),Ri(2),…,Ri(|Ri|)},with Ri(j) being the task ID at the jth position of Ri. Let dist(u, v) denote the shortest path distance between the head vertex of arc u (head(u)) and the tail vertex of arc v (tail(v)), the total cost of a solution S can be calculated as:(1)f(S)=∑i=1n+m(tc(S(i))+dist(S(i),S(i+1)))The total load load(Ri) of a route Rican be calculated as:(2)load(Ri)=∑j=1|Ri|q(Ri(j))In this section, we describe the proposed hybrid metaheuristic algorithm (HMA) for CARP including the main procedure, the procedure for generating initial solutions, the specific route-based crossover as well as the quality-and-distance based pool maintenance procedure. The local refinement procedure of HMA is presented in Section 4.Our HMA algorithm can be considered as a hybrid steady-state evolutionary algorithm which updates only one population solution at each generation of the evolution process (Glover & Kochenberger, 2003). Algorithm 1shows the main scheme of the HMA algorithm. HMA starts with an initial population of solutions (Line 1 of Algorithm 1) which are first generated by a random path scanning heuristic (Section 3.2) and further improved with the local refinement procedure (Section 4). Before entering the main loop, HMA initializes a counter array Cnt (Lines 3 and 4) which is used to record the accumulated number of successful pool updates with the related threshold ratio value in a given set Sr (an external input).At each generation, HMA randomly selects two parent solutions S1 and S2 from the population (Line 6), and performs a route-based crossover (RBX) operation (Line 7, see Section 3.3) to generate an offspring solution S0. RBX basically replaces one route of one parent solution with one route from the other parent solution, and repairs, if needed, the resulting solution to ensure the feasibility of S0. HMA then applies the local refinement procedure (Line 10, Section 4) to further improve S0. The local refinement procedure involves two sub-procedures, namely a randomized tabu thresholding procedure (RTTP) and an infeasible descent procedure (IDP), which can be carried out in two possible orders: RTTP followed by IDP (RTTP → IDP) and IDP followed by RTTP (IDP → RTTP). The applied order is determined randomly before running the local refinement procedure (Line 8, see Section 4.4). RTTP requires a threshold ratio which is probabilistically chosen among the values of a given set Sr according to the probability formula:Pr(i)=Cnt(i)/∑i=1|Sr|Cnt(i),where Pr(i) denotes the probability of selecting the ith value of Sr (Line 9).If the improved solution reaches the lower bound LB, HMA terminates immediately and returns this solution (Lines 11 and 12). Otherwise, HMA ends a generation by updating the recorded best solution and the population with the offspring solution Sc(Line 13, see Section 3.4). If S0 is successfully inserted into the population, the counter in relation to the threshold ratio used in the current generation is incremented by one (Lines 14 and 15). HMA terminates when a stopping condition is reached, which is typically a lower bound cutoff or a maximum number of generations.To generate one initial solution of the population, HMA uses a randomized path-scanning heuristic (RPSH) to construct a solution which is then further improved by the local refinement procedure described in Section 4. RPSH is adapted from the well-known path-scanning heuristic (Golden et al., 1983) by randomizing its five arc selecting rules. Specifically, RPSH builds one route at a time in a step-by-step way, each route starting at the depot vertex. At each step, RPSH identifies a set A of arcs (belonging to a set of unserved tasks) that are closest to the end of the current route and satisfy the vehicle capacity constraint. If A is empty, RPSH completes the current route by following a shortest deadheading path to the depot vertex and starts a new route. Otherwise RPSH randomly selects one arc from A and extends the current route with the selected arc. The selected arc as well as its inverse arc are marked served. This process continues until all tasks are served.The solution constructed by RPSH is further improved by the local refinement procedure of Section 4. The improved solution is inserted to the population if it is unique in terms of solution cost relative to the existing solutions, or discarded otherwise. The population initialization procedure stops when the population is filled with Psize (population size) different individuals or when a maximum of 3 * Psize trials is reached. The latter case helps to fill the population with Psize distinct individuals. If ever only k < Psize distinct solutions are obtained after 3 * Psize trials, we set the population size to k.At each of its generations, HMA applies a crossover operator to create an offspring solution by recombining two parent solutions randomly selected from the population. It has been commonly recognized that the success of memetic algorithms relies greatly on the recombination operator which should be adapted to the problem at hand and be able to transfer meaningful properties (building blocks) from parents to offspring (Hao, 2012). This idea is closely related to the idea of using structured combinations and vocabulary building (Glover & Laguna, 1997).By considering that the solution of CARP is composed of a set of routes, it is a natural idea to manipulate routes of tasks rather than individual tasks. In this regard, the route-based crossover (RBX) operator used for the vehicle routing problem (VRP) (Potvin & Bengio, 1996) seems attractive for CARP. However, given that CARP is quite different from the VRP, RBX must be properly adapted in our context within HMA. Given two parent solutionsS1={R11,R21,…,Rm11}with m1 routes andS2={R12,R22,…,Rm22}with m2 routes, our RBX crossover basically copies S1 to an offspring solution S0 and replaces a route of S0 with a route from S2, and then repairs S0 to establish feasibility if needed. The RBX crossover procedure consists of three main steps:•Step 1: Copy S1 to an offspring solutionS0={R10,R20,…,Rm10}and replace a route of S0 with a route from S2. Generate two random integer values a (a ∈ [1, m1]) and b (b ∈ [1, m2]); Replace the routeRa0of solution S0 with the routeRb2of solution S2, and collect the tasks that are not served in S0 to a set UT;Step 2: Remove duplicated tasks by the following rule. Let S0(pi) be the task in position pi, and letpi−1be the position before piandpi+1be the position after pi. Also let dist(u, v) denote the shortest path distance between vertex head(u) and vertex tail(v). Given a task t0 which appears twice respectively in position p1 and p2, RBX removes the appearance with the largest value of s(pi) (i ∈ {1, 2}), wheres(pi)=dist(S0(pi−1),S0(pi))+dist(S0(pi),S0(pi+1))−dist(S0(pi−1),S0(pi+1)).Step 3: Insert the unserved tasks of UT in S0. Before task insertion, RBX sorts the tasks in set UT in random order. Then for each task t in UT, RBX scans all possible positions of S0 to insert t. If a position is able to accommodate t while respecting the vehicle capacity, RBX further calculates the saving (change of the total cost) with t inserted. The two arcs of t are both considered for insertion, and the minimum saving is recorded. RBX finally inserts the task to a position which causes the overall least augmentation of the total cost while maintaining the solution feasibility. Ties are broken randomly. This process is repeated until UT becomes empty.Our proposed RBX operator not only introduces a new route (taken from S2) into S0 , but also modifies other existing routes due to deletion of duplicated tasks and insertion of unserved tasks. Clearly, RBX could lead to an offspring solution which is structurally different from its parent solutions. This is a desirable feature which promotes the overall diversity of HMA. Moreover, the quality of the offspring is not much deteriorated due to the use of greedy heuristics in Steps 2 and 3. As such, when the offspring is used as a seeding solution of local refinement, it helps the search to move into a new promising region. RBX can be realized in O(n2), where n is the number of tasks.In population-based algorithms, one important goal aims to avoid premature convergence of the population. This can be achieved by adopting a carefully designed strategy for population management. In HMA, we use a quality-and-distance strategy (QNDS) for this purpose. QNDS takes into account not only the solution quality, but also the diversity that the solution contributes to the whole population by resorting to a solution distance measure.We propose in this work to adapt for the first time the Hamming distance in the context of CARP and use it as our distance measure. Any pair of consecutive tasks(S(i),S(i+1))of a solution S is linked by a shortest path (a path with minimum deadheading cost) between head(S(i)) andtail(S(i+1)),called deadheading-link hereafter. Thus, solution S hasn+mdeadheading-links where n is the number of tasks and m is the number of routes. Let VR⊂ V be a set of vertices that belong to the required edges, letVR′=VR∪{vd}be a set containing both the vertices of VRand the depot vertex vd, letΠ={(u,v)|u,v∈VR′}be the set of all possible deadheading-links. Given two solutions Siwith miroutes and S jwith mjroutes, their Hamming distance Di, jis defined as the number of different deadheading-links between Siand S j:(3)Di,j=(n+m)−∑(u,v)∈Πxuvwherem=min{mi,mj},(4)xuv={1,if(u,v)isadeadheading−linkofbothSiandSj0,otherwiseGiven a populationPOP={S1,S2,…,SPsize}of size P size and the distance Di, jbetween any two individuals Siand Sj(i ≠ j ∈ [1, Psize]), the average distance between Siand any other individual in POP is given by:(5)ADi,POP=(∑Sj∈POP,j≠iDi,j)/(Psize−1)QNDS evaluates each solution in the population using the following quality-and-distance fitness (QDF for short) function:(6)QDF(Si)=α*OR(f(Si))+(1−α)*DR(ADi,POP)where OR(f(Si)) and DR(ADi, POP) represent respectively the rank of solution Siwith respect to its objective value and the average distance to the population (objective value is ranked in ascending order while average distance is ranked in descending order), and α is a parameter. We require the value of α to be higher than 0.5 to ensure that the best individual in terms of objective value will never be removed from the population, which formalizes the elitism property of QNDS.Given an offspring S0 (which has undergone both crossover and local refinement), QNDS first inserts S0 into POP, evaluates the QDF value of each individual and finally removes from POP the solution Swwith the largest QDF value.The local refinement procedure is another key component of our HMA algorithm and plays en essential role in enforcing intensification which ensures the high performance of HMA. Our local refinement procedure involves two sub-procedures, i.e., a randomized tabu thresholding procedure which explores only the feasible region, and an infeasible descent procedure which visits both feasible and infeasible regions. Both sub-procedures are based on a set of move operators which are explained below. The implementation of the two sub-procedures are also described.Our local refinement procedure employs six move operators, including five traditional small-step-size operators: inversion, single insertion, double insertion, swap, two-opt; as well as a large-step-size operator called merge-split recently proposed in Tang et al. (2009). These operators are briefly described as follows.Let u and v be a pair of tasks in the current solution S, tasks x and y be respectively the successor of u and v, rt(u) be the route including task u.•Inversion (IV): replace the current arc of task u with its reverse arc in S;Single insertion (SI): displace task u after task v (also before task v if v is the first task of rt(v)); both arcs of u are considered when inserting u in the target position, and the one yielding the best solution is selected;Double insertion (DI): displace a sequence (u, x) after task v (also before task v if v is the first task of rt(v)); similar to SI, both directions are considered for each task and the resulting best move is chosen;Swap (SW): exchange task u and task v; similar to SI, both directions are considered for each task to be swapped and the resulting best move is chosen;Two-opt (TO): two cases exist for this move operator: 1) ifrt(u)=rt(v),reverse the direction of the sequence (x, v); 2) if rt(u) ≠ rt(v), cut the link between (u, x) and (v, y) , and establish a link between (u, y) and (v, x);Merge-split (MS): this operator obtains an unordered list of tasks by merging multiple routes of the current solution, and sorts the unordered list with the path scanning heuristic (Golden et al., 1983). It then optimally splits the ordered list into new routes using the Ulusoy’s splitting procedure (Ulusoy, 1985). Each application of this operator results in five new solutions and the best one is chosen. Interested readers are referred to Tang et al. (2009) for more details.In the following two subsections, we explain how these operators are used in our two local refinement sub-procedures.The proposed randomized tabu thresholding procedure (RTTP) follows the general principle of the Tabu Thresholding (TT) method whose basis was first proposed in Glover (1995). A main ingredient of TT is the candidate list strategy (CLS) which is dedicated to reduce the number of moves to be considered in order to accelerate the neighborhood examination. CLS subdivides the possible moves of the current solution into subsets and executes one move for each subset rather than for the whole neighborhood. CLS, along with the elements of probabilistic tabu search, simulates the tabu mechanism with memory structure. RTTP is a randomized procedure in the sense that it explores multiple neighborhoods in a random order.The randomized tabu thresholding procedure basically alternates between a Mixed phase and an Improving phase where for both phases, five traditional move operators are employed: inversion, single insertion, double insertion, swap and two-opt. Algorithm 2sketches the outline of the RTTP procedure for CARP. RTTP starts by initializing a set of global variables with an initial solution S0 taken from an external input. RTTP then enters the main loop where Mixed phase and Improving phase alternate.In the Mixed phase, for any move operator o and for a given task i, RTTP examines the candidate list MOVE_CL(i, S, o) in random order and accepts the first improving feasible move if any, or the best admissible feasible move otherwise. The admissible feasible move satisfies a quality threshold TV, i.e.,f(S′)≤TVwhereS′is the neighboring solution generated by the accepted move. TV is calculated as:TV=(1+r)*fp,where fpis the current best local optimum objective value, and r is a threshold ratio. With this quality threshold, deteriorating solutions are allowed in order to diversify the search. Solution cycling is prevented through the complete reshuffling of the order in which candidate lists are examined before each neighborhood examination. An iteration of the Mixed phase is based on the examination of the complete neighborhoods of all move operators. The Mixed phase is repeated for T iterations. T is called a tabu timing parameter, which is analogous to the tabu tenure when an explicit tabu list is used. T is randomly selected among the values of a given set St.In the Improving phase, RTTP always seeks an improving move among the feasible moves within each candidate list MOVE_CL(i, S, o). If no improving move is found in a given candidate list, RTTP skips to the next candidate list. This phase is iterated until no improving move can be found in any candidate list.If the local optimum reached in the Improving phase has a better objective value than the recorded best objective value fp, the algorithm updates fpand resumes a new round ofMixed−Improvingphases. RTTP terminates when fphas not been updated for a consecutive WMixed−Improvingphases.When using the five traditional move operators, neighborhoods of the current solution can always be obtained by operating on two distinct tasks. For instance, insertion is to insert one task after or before another task; swap is to swap one task with another task; two-opt is to exchange the subsequent part of a task with that of another one. As such, given a move operator o, a natural choice for the subsets to be used in the candidate list strategy is to define one subset MOVE_SUBSET(i, S, o) for each task i. In order to speed up neighborhood examination, we further use an estimation criterion to discard moves from MOVE_SUBSET(i, S, o) that are unlikely to lead to a promising solution. This estimation criterion is based on a distance measure between two tasks t1, t2 which is defined as:(7)Dtask(t1,t2)=(∑a=12∑b=12D(va(t1),vb(t2)))/4where D(va(t1), vb(t2)) is the traversing distance between t1’s ath end node va(t1) and t2’s bth end node vb(t2). (This distance measure was first used in Mei et al. (2014) to define the distance between two routes.) The candidate move list associated to task i (MOVE_CL(i, S, o)) is restricted to contain Csize most promising moves such that for each move which is associated with two tasks (i, t), t is a member of i’s Csize closest neighboring tasks according to the distance measure of formula (7).For a constrained optimization problem like CARP, it is known that allowing a controlled exploration of infeasible solutions may facilitate transitions between structurally different solutions and help discover high-quality solutions that are difficult to locate if the search is limited to the feasible region. This observation is highlighted by discoveries made with the strategic oscillation approach (see, e.g., Glover & Hao, 2011) which alternates between phases of infeasible descent and phases of improving feasible search. To further intensify the search, we employ in our local refinement procedure, as a complement to RTTP, an infeasible descent procedure (IDP) which allows visiting infeasible solutions. IDP is a best-improvement descent procedure based on three traditional move operators, i.e., single insertion, double insertion, swap, as well as a large-step-size merge-split operator that was recently proposed and proved to be effective for CARP (Tang et al., 2009). We use the merge-split operator in the way as suggested in Tang et al. (2009). IDP basically involves two different stages. In the first stage, IDP examines the complete neighborhoods induced by the SI, DI and SW operators and chooses the best move to perform if it improves the current solution. When no improvement can be found in the first stage, IDP switches to the second stage where it examines the neighboring solutions generated by the MS operator. Since MS is computationally expensive, IDP restricts the examination to a maximum of 100 neighboring solutions which are randomly sampled from theCm2possibilities where m is the number of routes. IfCm2≤100,all neighboring solutions will be examined. Still, the best improving move is performed until no improvement is reported in this stage. If any improvement is found in the second stage, IDP switches back to the first stage to explore the new local region and terminates the algorithm when this stage is finished; otherwise, IDP terminates at the end of the second stage. As in many previous CARP algorithms which allow intermediate infeasible solutions (Beullens et al., 2003; Brandão & Eglese, 2008; Hertz et al., 2000; Tang et al., 2009), we evaluate the solution quality generated in the search process of IDP by adding a penalty item to the original cost:(8)ϕ(S)=f(S)+β*EX(S)where EX(S) is the total excess demand of S and β is a self-adjusting penalty parameter. β is halved (doubled) if feasible (infeasible) solutions have been achieved for five consecutive iterations, and its initiating value is set to:(9)β=f(S)/(2*Q)where Q is the vehicle capacity. One notices that we don’t consider the violation of S in Eq. 9. This is because we always ensure that IDP starts the search from a feasible solution.After presenting the implementation of RTTP and IDP, the order of combining them in the local refinement procedure remains an issue to be addressed. RTTP is the most important component of our HMA algorithm which compared to IDP, makes more contribution to the high performance of HMA, but also consumes more computing time (see the analysis in Section 6.1). IDP is a very simple descent procedure which, when used alone, is not expected to identify very high quality solutions (see the analysis in Section 6.1). However, the search ability of RTTP and IDP can be mutually strengthened when they are combined. Indeed, it is beneficial to put IDP either before or after RTTP. When IDP is placed before RTTP, the best feasible solution found by IDP can be considered as a good starting point for RTTP. This is because the performance of neighborhood search algorithms may highly depend on the initial solution and a high-quality initial solution could help discover still better solutions. When IDP is put after RTTP, the property of tunneling through infeasible regions and the large-step-size MS operator of IDP may help to further improve the high quality solution provided by RTTP. For the above reasons, both orders (i.e., RTTP → IDP and IDP → RTTP) are allowed in our HMA algorithm. The order is randomly determined before the local refinement procedure is carried out.To evaluate the efficacy of the proposed HMA algorithm, we carry out extensive experiments on a large number of well-known CARP benchmark instances, and compare the results11Our best solution certificates are available at: http://www.info.univ-angers.fr/pub/hao/CARPResults.zip.with those of the state-of-the-art algorithms as well as the best known solutions ever reported in the literature.HMA was coded in C++ and compiled by GNU g++ 4.1.2 with the ’-O3’ option. The experiments were conducted on a computer with an AMD Opteron 4184 processor (2.8 gigahertz and 2 gigabytes RAM) running Ubuntu 12.04. When solving the DIMACS machine benchmarks22dfmax: ftp://dimacs.rutgers.edu/pub/dsj/clique/.without compilation optimization flag, the run time on our machine is 0.40, 2.50 and 9.55 seconds respectively for instances r300.5, r400.5 and r500.5.Our HMA algorithm was evaluated on a total of 191 benchmark graphs with 7–255 vertices and 11–375 edges. These instances are very popular and widely used in the CARP literature. They cover both random instances and real-life applications, and are typically classified into eight sets:•gdb: 23 instances randomly generated by DeArmon (DeArmon, 1981), with 7–27 nodes and 11–55 required edges.val: 34 instances derived from 10 randomly generated graphs proposed by Benavent, Campos, Corberan, and Mota (1992), with 25–50 nodes and 34–97 required edges.egl: 24 instances proposed by Eglese (1994), which originate from the data of a winter gritting application in Lancashire (UK), with 77–140 nodes and 98–190 edges that include 51–190 required edges.C: 25 instances generated by Beullens et al. (2003) based on the intercity road network in Flanders, with 32–97 nodes and 42–140 edges that include 32–107 required edges.D: 25 instances modified from the instances of set C by doubling the vehicle capacity for each instance.E: 25 instances, also generated by Beullens et al. (2003) based on the intercity road network in Flanders, with 26–97 nodes and 35–142 edges that include 28–107 required edges.F: 25 instances modified from the instances of set E by doubling the vehicle capacity for each instance.EGL-G: 10 large-sized CARP instances, which like the set egl, were also generated based on the road network of Lancashire (UK) (Brandão & Eglese, 2008), each having 255 nodes and 375 edges with 374 to 375 required edges.Following the commonpractice in the literature, we compare the results produced by our HMA algorithm on these benchmarks to those of the following eight state-of-the-art algorithms:1.A guided local search (GLS) algorithm proposed by Beullens et al. (2003), who reported results on the instance set gdb, val and C-F.A deterministic tabu search algorithm (TSA) proposed by Brandão and Eglese (2008), who reported results on all eight instance sets. Two sets of results (“TSA1” and “TSA2”) were reported, and the one (“TSA2”) yielding better performance will be considered for comparative study for all instance sets except for EGL-G where only results of “TSA1” were reported.A variable neighborhood search (VNS) algorithm proposed by Polacek et al. (2008), who reported results on set val and egl. Two sets of results (“993 megahertz” and “3.6 gigahertz”) were reported, and the one (“3.6 gigahertz”) yielding better performance will be considered for comparative study.A memetic algorithm with extended neighborhood search (MAENS) proposed by Tang et al. (2009), who reported results on set gdb, val, egl and C-F.An improved ant colony optimization based algorithm (Ant-CARP) proposed by Santos et al. (2010), who reported results on set gdb, val, egl and C-F. Two sets of results (“Ant-CARP_6” and “Ant-CARP_12”) were reported, and the one (“Ant-CARP_12”) yielding overall better performance will be considered for comparative study. Hereafter, we use “Ant_12” to represent “Ant-CARP_12”, and use its median results for comparison when we study the average performance of the reference algorithms since their average results are not available.A GRASP with evolutionary path relinking (GRASP) proposed by Usberti et al. (2013), who reported results on the set gdb, val and egl.An iterated variable neighborhood descent algorithm (ILS-RVND) proposed by Martinelli, Poggi, and Subramanian (2013). We reference their reported results of set EGL-G.A cooperative co-evolution algorithm with route distance grouping (RDG-MAENS) proposed by Mei et al. (2014). RDG-MAENS was specifically designed for large-sized CARP instances, and thus we reference their results on set EGL-G.These reference algorithms were tested on different computers with a CPU frequency ranging from 500 megahertz to 3.6 gigahertz. To make a relatively fair comparison of the runtime, all CPU times reported in the reference papers are scaled here into the equivalent AMD Opteron 4184 2.8 gigahertz run times. Like in previous CARP literature (Martinelli et al., 2013; Mei et al., 2014; Santos et al., 2010; Tang et al., 2009), our time conversion is based on the assumption that the CPU speed is approximately linearly proportional to the CPU frequency. We provide in Table 1 the CPU type and its frequency of each reference algorithm, as well as its resulting scaling factors. This time conversion is only made for indicative purposes, since the computing time of each algorithm is not only influenced by the processor, but also by some inaccessible factors such as the operating systems, compilers and coding skills of the programmer. Nevertheless, we show in the following experiments, the outcomes provide interesting information about the performance of the proposed algorithm relative to the best performing algorithms.The HMA algorithm relies on a set of correlated parameters. To achieve a reasonable tuning of the parameters, we adopt the Iterated F-race (IFR) method (Birattari, Yuan, Balaprakash, & Stützle, 2010), which allows an automatic parameter configuration, using the IFR algorithm that is implemented and integrated in the irace package (López-Ibáñez, Dubois-Lacoste, Stützle, & Birattari, 2011). Table 2 summarizes the parameters of our HMA algorithm, along with the range of values that were determined by preliminary experiments. Among these parameters, four of them (Psize, α, W, Csize) need to be tuned and the other two parameters (threshold ratio r and tabu timing parameter T) are adaptively or randomly chosen among the values in the given sets (Sr and St) during the search process. We set the tuning budget to 1000 runs of HMA and each run is given 100 generations. We restrict the training set to contain 8 challenging instances taken from val, egl, C, E and EGL-G sets: val-10D, egl-e3-B, egl-s3-C, C11, E12, E15, EGL-G1-B, EGL-G2-B. The final choices of the parameter values are presented in Table 2 and they are used in all experiments in the following sections unless otherwise mentioned.We first assess HMA on the 7 most commonly used instance sets (181 instances): gdb, val, egl, C, D, E, F. It is compared to 5 current state-of-the-art algorithms: GLS (Beullens et al., 2003), TSA2 (Brandão & Eglese, 2008), VNS (Polacek et al., 2008), Ant-CARP (Santos et al., 2010), GRASP (Usberti et al., 2013), and MAENS (Tang et al., 2009). To give a general picture of the performance of each compared algorithm, we summarize in Table 3, for each instance set and for each algorithm, the number of best results that match or improve on the best known results (#Best), the number of average results that match or improve on the best known results (#BestAvg), the average gap between the average results and the best known results in percentage (AvgGap, the gap is calculated as (favg-fbk) × 100/fbkwhere favgis the average solution value obtained by the algorithm and fbkis the best known solution value reported in the literature), and the average of the instance computing time in seconds (AvgTime). When we count#Best, we refer to the current best known results (BKRs) which are compiled from the “best results” reported in all previous CARP literature. These “best results” could be those obtained by a single algorithm with various parameter settings (e.g., Lacomme et al., 2004; Polacek et al., 2008; Santos et al., 2010) or even with a specific setting tuned for each instance (e.g., TSAbestin Brandão & Eglese, 2008). Finally, to complement these summarized results, Appendix A (Tables A.1–A.7) reports, for each of the 181 CARP instance, the detailed results of our HMA algorithm as well as the average results of the reference algorithms. These tables permit a thorough assessment of all compared algorithms.Note that some results were obtained from a single run of the algorithms (GLS and TSA) whereas other results came from multiple runs (VNS, GRASP, Ant-CARP, MAENS, HMA1 and HMA2). Clearly,#Best favors multiple-run results. To make a fair comparison, we refer to average statistics (#BestAvg, AvgGap, AvgTime) when we compare single-run results with multiple-run results.For each instance, our HMA algorithm was run 30 times under two different stop criteria: 500 generations and 2000 generations. To ease presentation, we denote HMA with 500 generations as HMA1, and HMA with 2000 generations as HMA2. Studying the outcomes of these two termination criteria affords insights into how HMA behaves when more computing time is available.From Table 3, we can see that HMA1 shows a remarkable performance on all 7 tested instance sets compared to the multiple-run reference algorithms. Indeed, it attains the largest number of best known results for all 7 data sets and the lowest average gap to the best known results for 6 out of 7 sets. Compared to Ant_12and MAENS which, like HMA1, are both population-based algorithms, HMA1 clearly shows its dominance in terms of both best results and average results. For set D, HMA1 is the only algorithm which is able to find all BKRs. Additionally, HMA1 obtains improved best known results on three well-studied instances from set egl. By increasing the HMA1 termination criterion of 500 to 2000 generations, HMA2 achieves a still better performance, always obtaining equal or better results in terms of both#Best and AvgGap. In particular, for set egl, HMA2 discovers 6 new BKRs and matches 6 more BKRs, leading to#Best = 23 which is significantly larger than those obtained by the reference algorithms. HMA2 is able to achieve overall 180 current or new BKRs out of 181 instances with one standard parameter setting, while the previous BKRs are compiled from many previous articles, among which some were obtained with parameters specifically tuned for individual instance.Now we turn to compare our HMA algorithm to the single-run reference algorithms. As mentioned before, we should look at average statistics when comparing multiple-run algorithms to single-run algorithms. According to two average indicators, namely#BestAvg and AvgGap, GLS is clearly the best performing single-run algorithm among all 6 reference algorithms (including single-run algorithms and multiple-run ones). Still, compared to GLS, our HMA algorithm remains competitive on 6 instance sets (i.e., gdb, val and C-F). Indeed, when the short time limit (500 generations) is applied, HMA1 performs better in items of AvgGap (by achieving an equal or lower AvgGap for more instance sets: 5 vs. 3), but worse in terms of#BestAvg (by attaining an equal or higher#BestAvg for less instance sets: 2 vs. 6). On set D, both HMA1 and HMA2 are dominated by GLS in terms of both indicators. Finally, one observes that when given more computing time, HMA2 is able to further improve its results.To validate the above observations, we apply a Wilcoxon test with a significance factor of 0.05 for a pairwise comparison of the average performance between HMA1 and TSA2, Ant_12 as well as MAENS, which are three approaches that have been tested on all 181 instances. The resulting p-values of 2.15E-10, 9.31E-10 and 2.20E-16 confirm that the results of HMA1 are significantly better than those of these current best performing algorithms. This conclusion remains valid for HMA2 since it always performs better than HMA1.When it comes to computational time (‘AvgTime’ in Table 3), our HMA algorithm also remains competitive. Recall that the indicated time for the reference algorithms are scaled according to our computer and the average time of a multiple-run algorithm can be compared to the time of a single-run algorithm. Table 3 shows that HMA1 is in overall not slower than any of the reference algorithms. Compared to the fast GLS, TSA2 and Ant_12algorithms, HMA1 generally requires comparable computing time. Compared to the remaining reference algorithms (i.e., VNS, GRASP and MAENS), HMA1 is clearly more efficient. By extending the stop condition to 2000 generations, HMA2, which finds improved solutions, consumes more computing time than HMA1 as expected.Finally, the “best results” reported by previous CARP studies were often achieved by executing tests involving multiple parameter settings to show the extreme performance of the associated algorithms. Following this practice, we report in Appendix A some new best known results discovered by our HMA algorithm with parameter settings other than the standard one given in Table 2. The form of HMA using these additional parameter settings, which we call HMA*, further attains two new BKRs (for S4-A, S4-B) and matches the BKR for S3-C, which finally makes HMA* consistently match or improve on all 181 BKRs.To test the scalability of HMA, we carried out experiments on the EGL-G set containing 10 large scale CARP (LSCARP) instances. As stated in Mei et al. (2014), solving LSCARP is much more challenging than solving small-sized or medium-sized instances since the solution space increases exponentially as the problem size increases. Compared to the classical instance sets which involve instances having at most 190 required edges, all instances in EGL-G have more than 347 required edges. Such a size, as was shown in previous studies (Brandão & Eglese, 2008; Martinelli et al., 2013), is large enough to pose a scalability challenge to the existing CARP algorithms. For this reason, a dedicated algorithm called RDG-MAENS (Mei et al., 2014) has been proposed specifically for solving LSCARP instances. In this section, we evaluate the capacity of our HMA algorithm to solve these 10 LSCARP instances by comparing its performance to those of the current best performing CARP algorithms including RDG-MAENS.As before, HMA was executed 30 runs to solve each instance under two termination criteria: 500 generations (HMA1) and 2000 generations (HMA2). We also report the results obtained by HMA with various other parameter settings (HMA*). Table 4summarizes our results on the EGL-G set, along with those of the current best performing algorithms: TSA1, ILS, MAENS, RDG-MAENS. In Mei et al. (2014), the authors report the results of RDG-MAENS for 6 parameter combinations of (g,α) whereg=2and 3,α=1,5 and 10. We include the results of the best version (g=2,α=10) for our comparative study. Table 4 lists the average results of each algorithm, the solution time of HMA, the best lower bounds (LB), the best known results (BKR), and the best results of HMA. The last two rows show, for each algorithm, the average of the average gaps to the BKR (AvgGap), and the “scaled” average of the average solution time (AvgTime).Table 4 discloses that although HMA2 is the best new algorithm, outperforming the early terminating algorithm HMA1, it is also true that HMA1 in fact dominates all reference algorithms. In terms of average results, HMA1 is much better than any of the reference algorithms for all 10 instances. The very small AvgGap value of−0.02% of HMA1 indicates that HMA1 is on average better than the previous BKRs, and compares favorably to the AvgGap value of more than 0.89% for the other approaches. In terms of best results, HMA1 discovers an improved solution relative to the BKRs for 9 out of 10 instances (90%). As the current best and highly specialized algorithm, the best version of the reference algorithm RDG-MAENS is outperformed by HMA1. Moreover, the average computational time of HMA1 is comparable to that of the reference algorithms. HMA1 requires much less time to find substantially better results than MAENS. Compared to the best version of RDG-MAENS, HMA1 is also on average faster (1522.86 vs. 1633.86 seconds). The fact that HMA2 further significantly improves on HMA1 demonstrates that HMA can reach better performance when more computational time is allowed. By testing several other parameter settings, HMA* further discovers 6 improved best known results.A Wilcoxon test is finally applied to a pairwise comparison of the average performance between HMA1 and each of the four reference methods, which always results in a p-value of 0.001953 (<0.05) for all tested pairs, indicating the superiority of our method relative to the compared approaches.

@&#CONCLUSIONS@&#
Thecapacitated arc routing problem (CARP) is of great practical interest and represents a significant computational challenge due to its NP-hardness. We developed a new hybrid metaheuristic approach (HMA) for effectively solving CARP, which employs a randomized tabu thresholding procedure (RTTP) coupled with an infeasible descent procedure to explore both feasible and infeasible regions. HMA relies on a specialized route-based crossover operator to generate diversified and promising new solutions. Thanks to its quality-and-distance based pool updating strategy, HMA prevents the search process from premature convergence.The proposed approach demonstrates an excellent performance over the eight sets of 191 popular CARP benchmarks. Specifically, on the 7 sets of 181 classical instances, HMA with a standard parameter setting outperforms the current best performing algorithms, in terms of both solution quality and computational efficiency. HMA further improves its own performance when more computing time is available (to run 2000 generations), attaining the best known results for all 181 cases including 6 improved new best results. HMA also proves to be scalable to handle the last set of 10 large-sized instances, by obtaining 9 new best results, dominating the current state-of-the-art algorithms including the approaches which were specially designed for the large-sized CARP instances. We additionally conducted experiments to analyze the contribution of the two sub-procedures for local refinement, the relevance of the route-based crossover operator (and thus the population-based framework), the strategy for combining crossover with the local optimization procedure, as well as the quality-and-distance pool updating strategy.Finally, we observe that the proposed method can be adapted to handle other CARP variants with slight modifications of the route-based crossover operator and of the local refinement procedure to accommodate additional constraints.