@&#MAIN-TITLE@&#
Domain decomposition based coupling between the lattice Boltzmann method and traditional CFD methods—Part I: Formulation and application to the 2-D Burgers’ equation

@&#HIGHLIGHTS@&#
We couple the cache-optimized lattice Boltzmann and ADI methods.Each solver is deployed in the region where it is most efficient.The coupled solver is 4.5× faster than the ADI for the 2D Burgers’ equation.

@&#KEYPHRASES@&#
Lattice Boltzmann method,Alternating direction implicit method,Burgers’ equation,Cache optimization,Domain decomposition,

@&#ABSTRACT@&#
The lattice Boltzmann method is being increasingly employed in the field of computational fluid dynamics due to its computational efficiency. Floating-point operations in the lattice Boltzmann method involve local data and therefore allow easy cache optimization and parallelization. Due to this, the cache-optimized lattice Boltzmann method has superior computational performance over traditional finite difference methods for solving unsteady flow problems. When solving steady flow problems, the explicit nature of the lattice Boltzmann discretization limits the time step size and therefore the efficiency of the lattice Boltzmann method for steady flows. To quantify the computational performance of the lattice Boltzmann method for steady flows, a comparison study between the lattice Boltzmann method (LBM) and the alternating direction implicit (ADI) method was performed using the 2-D steady Burgers’ equation. The comparison study showed that the LBM performs comparatively poor on high-resolution meshes due to smaller time step sizes, while on coarser meshes where the time step size is similar for both methods, the cache-optimized LBM performance is superior. Because flow domains can be discretized with multiblock grids consisting of coarse and fine grid blocks, the cache-optimized LBM can be applied on the coarse grid block while the traditional implicit methods are applied on the fine grid blocks. This paper finds the coupled cache-optimized lattice Boltzmann-ADI method to be faster by a factor of 4.5 over the traditional methods while maintaining similar accuracy.

@&#INTRODUCTION@&#
The lattice Boltzmann method (LBM) is a finite difference method that is applied to solve the discrete velocity Boltzmann equation from statistical mechanics [1]. It is an explicit time marching scheme and is well suited for unsteady flow problems, where the numerical time step size limitation is normally dictated by accuracy and is smaller than the time step size required for numerical stability. When solving unsteady flow problems, both explicit and implicit time discretization methods have a similar time step size limitation for maintaining accuracy. This contributes to the greater computational speed of the LBM over traditional methods for simulating unsteady flow fields because the LBM is an explicit method and does not have to solve systems of simultaneous algebraic equations at each time step, unlike the implicit methods. An earlier study performed by the authors found the cache-optimized LBM to be approximately eight times faster than traditional methods [2]. In the cache-optimized LBM the computational domain is partitioned into subsections that can fit in the cache, and computations are performed separately on each subsection. This is possible due to the highly localized floating point operations in the LBM. The localized computational property also allows efficient implementation of the LBM on GPUs [3].The cache-optimized LBM has not been previously compared against traditional implicit methods for simulating steady flow fields. This study compares the cache-optimized LBM to the alternating direction implicit (ADI) method for solving the steady 2-D nonlinear Burgers’ equation. The ADI method was chosen for its efficient implementation on parallel computers [4]. The results show that the ADI method outperforms the cache-optimized LBM for small grid spacing because the ADI method can take larger time steps and thus converge to steady state with fewer time iterations. In contrast the cache-optimized LBM outperforms the ADI method when solving on coarse Cartesian grids where the LBM shares the same time step size as the ADI scheme. Based on these observations, a coupled LBM–ADI method using a multiblock grid consisting of coarse and fine grids for solving steady fluid flows would combine the computational efficiency of the cache-optimized LBM with the faster convergence properties of the ADI method.In this study, the coupled cache-optimized LBM–ADI method is developed and compared with traditional methods. The 2-D nonlinear Burgers’ equation is chosen as a test case because it has been extensively used as a model for the Navier–Stokes equations and for testing the efficiency and accuracy of various numerical schemes [5–7]. Both the cache-optimized LBM and ADI methods compute the solution by marching in time. Hence, the time dependent Burgers’ equation is solved to provide the numerical time stepping for approaching steady state,(1)∂u∂t+u∂u∂x=μ∂2u∂x2+∂2u∂y2The present comparison study and a previous study on the unsteady Burgers’ equation [2] by the authors suggest that combining the cache-optimized LBM with the ADI method on multiblock grids would generate an efficient solver for steady flows.The 2-D Burgers’ equation is solved on a square computational domain(2)(0,0)⩽(x,y)⩽(1,1)with the following initial and boundary conditions:(3)u(x,y,t)=1-23xatt=0(4)u(x,y,t)=2-expx-1μ2+expx-1μaty=0,y=1The boundary conditions are applied to the 2-D domain on which the 2-D Burgers’ equation is solved. The boundary conditions at the x=0 and x=1 boundaries are constant and are determined from Eq. (4). The boundary conditions at y=0 and y=1 follow the profile in Eq. (4). With these boundary conditions the solution to the 2-D steady Burgers’ equation approaches(5)u(x,y,t)=2-expx-1μ2+expx-1μIt should be noted that these boundary conditions and the solution are 1-D in nature. These boundary conditions were chosen to provide a steady state problem with a readily identifiable region with a steep gradient and a known analytical solution. When the diffusion coefficient, μ, is small (<0.002), a steep boundary layer in the solution that is similar to a shockwave is formed near the right boundary. As discussed later in the paper, this steep boundary layer region is discretized with a fine grid to resolve the detail of the solution, and the remainder of the computational domain is discretized with a much coarser grid.The discrete velocity Boltzmann equation describes the evolution of the particle distribution function,fi(x→,v→,t), where the index i represents a specific velocity. The discrete velocity Boltzmann equation with the Bhatnagar–Gross–Krook (BGK) approximation [1] is(6)∂fi∂t+c→i·∇fi=-ωfi-fieqwhere firepresent the particle distribution functions,fieqare the equilibrium distribution functions, ω is the collision frequency, andc→irepresent the velocities associated with the distribution functions.The lattice Boltzmann equation is obtained when the discrete velocity Boltzmann equation is discretized on a uniform grid or lattice using forward Euler discretization for the time term, upwind differencing for the advection term, and downwind collision terms (right hand side of Eq. (6)) [8]. When the velocity equals the spatial differential over the time step, the lattice Boltzmann equation written in an explicit form is(7)fi(x→+c→iΔt,t+Δt)=fi(x→,t)-ωfi(x→,t)-fieq(x→,t)where i represents the velocity index. A four-speed lattice Boltzmann model is shown in Fig. 1with the speeds defined in Eqs. (8) and (9).(8)c→1=+c,0c→2=0,+cc→3=-c,0c→4=0,-c(9)c=Δx/ΔtwhereΔtis the numerical time step andΔxis the spatial discretization. The macroscopic continuum equations can be recovered from the lattice Boltzmann equation using a multi-scale expansion [1].Using a multiscale expansion, the 2-D Burgers’ equation (Eq. (1)) is derived from the lattice Boltzmann equation (Eq. (7)). The dependent variable u in the Burgers’ equation is defined as the sum over the distribution functionsfi(x→,t).(10)u(x→,t)=∑ifi(x→,t)=∑ifieq(x→,t)i=1,2,3,4The equilibrium distributions are derived using a multi-scale expansion and an ansatz method based on Wolf-Gladrow’s [1] procedure for the diffusion equation.(11)fieq=u4+u24ci=1fieq=u4-u24ci=3fieq=u4i=2,4To perform a multi-scale expansion, the distribution functions fiare expanded around the equilibrium distributionsfiequsing a small expansion parameter ɛ(12)fi=fieq+εfi(1)+O(ε2)and substituted in the following conservation statement after applying a Taylor series expansion to the left hand side(13)∑ifi(x→+c→iΔt,t+Δt)=∑ifi(x→,t)Simplifying this yields(14)∂u∂t+u∂u∂x=12c2Δt1ω-12∂2u∂x2+∂2u∂y2When the diffusion coefficient is(15)μ=12c2Δt1ω-12Eq. (14) becomes the 2-D Burgers’ equation (Eq. (1)). Based on this, the lattice Boltzmann method can be used to numerically simulate the 2-D Burgers’ equation by specifying the appropriate values forΔx,Δt, and ω.In traditional CFD methods, the initial and boundary conditions are specified in terms of the macroscopic variables. To obtain the initial and boundary conditions in terms of the distribution functions, an inverse mapping between the distribution function and the macroscopic variable (a direct mapping being Eq. (10)) is required. The distribution functions can be made equal to the equilibrium distributions or to the sum of the equilibrium distributions and a non-equilibrium term such as(16)fi=fieq+fineqFor the 4-speed lattice Boltzmann model for the 2-D Burgers’ equation(17)f1neq=-Δtω·c4·∂u∂xf3neq=-Δtω·-c4·∂u∂xf2neq=f4neq=0The non-equilibrium distributions were obtained with a multiscale expansion similar to the manner in which Skordos [9] obtained them for the incompressible Navier–Stokes equations.To implement an explicit time marching lattice Boltzmann solver for the 2-D Burgers’ equation, the following substeps are required at each time step:1.Using the initial u, calculate the equilibrium distributions (Eq. (11)) and setfi=fieq+fineqfor the first time step.Compute the right hand side of the lattice Boltzmann equation (Eq. (7)), the collision substep, and propagate the result to the nearest neighbor nodes to obtainfi(x→+c→iΔt,t+Δt). The propagation substep is performed only at the interior nodes. The unknown distribution functions at the boundary are computed using Eqs. (16) and (17).Updateu(x→,t)from the new distributions according to the definitionu(x→,t)=∑ifi(x→,t)(Eq. (10)).Start the next time step with the calculation of the new equilibrium distributions using the newu(x→,t)(Eq. (11)) and proceed with Step 2 of the algorithm. Follow this procedure until the final time.The four-speed model of the discrete velocity Boltzmann equation (Eq. (6)) used to simulate the Burgers’ equation is(18)∂f1∂t+c∂f1∂x=-ωf1-f1eq∂f2∂t+c∂f2∂y=-ωf2-f2eq∂f3∂t-c∂f3∂x=-ωf3-f3eq∂f4∂t-c∂f4∂y=-ωf4-f4eqAs seen above, the discrete velocity Boltzmann equation consists of first-order hyperbolic PDEs. For a first-order hyperbolic PDE such as the 1-D linear advection equation,(19)ut+cux=0the stability of an upwind and forward Euler numerical discretization (same as the numerical discretization in the LBM) is defined by the Courant–Friedrichs–Lewy (CFL) condition [7]:(20)CFL=cΔtΔx⩽1The above condition holds for a 1-D linear advection equation. In the LBM there are four 1-D linear advection equations, two in each dimension, with source terms that couple the four equations. In the LBM it is generally assumed that the advection speed is equal to the spatial discretization over the time step and that this should satisfy the CFL condition for the advection part of Eq. (18). However, computational experiments by the authors have shown that the time step is limited to slightly more than one-half the spatial discretization. This means that the CFL condition is more limited than that given in Eq. (20). The reason for this is the non-linearity in the equilibrium distributions (Eq. (11)). This non-linearity allows local gradients in the solution field to influence the maximum time step size. In contrast to the 2-D Burgers’ equation, when simulating the 2-D diffusion equation [10], there is no stability limitation on the time step size other than the CFL condition because the equilibrium distributions for the 2-D diffusion equation do not contain non-linear terms.A linear stability analysis of the lattice Boltzmann scheme shows that the collision frequency is limited to 0<ω<2 [11]. Sterling and Chen [11] have shown the lattice Boltzmann discretization (Eq. (7)) results in second-order accuracy both in space and time.Acquiring data from the main memory every computational cycle is a costly operation because the CPU remains idle during this process. If the required data were acquired from the cache memory, the data transfer would be much faster, saving the CPU from some of the idle time. However, the size of the cache memory is limited, and to use the cache to store data requires the data (computational domain) to be divided into blocks (subdomains) that can fit into cache memory. The computational algorithm can then call the data from the cache memory, thus minimizing the wait time. Not all algorithms are amenable to this kind of cache optimization because data dependencies may disallow updating subdomains separately. The lattice Boltzmann algorithm requires only the nearest neighbor data dependencies and hence is much more amenable to cache optimization than the ADI algorithm, which requires the matrix inversion of tridiagonal matrices that encompass the whole domain. For example, a comparison of the original lattice Boltzmann algorithm and the cache optimized lattice Boltzmann algorithm showed the cache optimized lattice Boltzmann algorithm to have a speedup (in computation time) by a factor of 1.9–2.5 for the serial implementation. The parallel implementation had a speedup by a factor of 3.6–3.8 [12].The LBM for the 2-D Burgers’ equation uses four distribution functions, each requiring a 2-D array. Considering an 8MB cache on a single processor, the biggest grid size that can be accommodated within the cache is 512×512 because(21)4arrays×5122elements×8bytes/elements=8MBOn a single processor grid, sizes that are larger than 512×512 can be accommodated in the cache by dividing the grid into subsections that can fit in the cache. Cache optimization is achieved when the subsection data is called from the cache repeatedly for performing computations [12]. This is possible because LBM computations depend on local data, such as the computation of the right hand side of Eq. (7) (collision substep), including the computation of the equilibrium distributions. They do not involve any dependencies between the subsections:(22)fi(x→,t)=(1-ω)·fi(x→,t)+ω·fieq(x→,t)The subsections mentioned above are horizontal strips of dimension Nx×m1, where Nxis the number of grid points in the horizontal direction, and m1=Ny/p1, where p1 is the number of strips and is chosen such that the subsection size will not exceed cache memory size (Fig. 2).The propagation substep is an assignment operation of an almost local nature due to nearest-neighbor dependencies:(23)fi(x→+c→iΔt,t+Δt)=fi(x→,t)If LBM computations solely consisted of Eq. (22), then each subsection can be updated separately until the final time or convergence to steady state. However, LBM computations also include the propagation substep that causes the number of updates (tdiv) on a subsection to be limited [12].A comparison of the original lattice Boltzmann algorithm and the cache optimized lattice Boltzmann algorithm showed the latter to have a speedup (in computation time) by a factor of 1.9–2.5 for the serial implementation. The parallel implementation had a speedup by a factor of 3.6–3.8 [12].Parallelization of the lattice Boltzmann method is carried out using domain decomposition. The domain can be decomposed into subdomains, which are vertical strips in this study. Each subdomain is assigned to a processor. The computation of the right hand side of the lattice Boltzmann equation (Eq. (7)) is performed completely in-processor with no need of data communication. The only communication occurring between the processors is that required for transferring fidata at subdomain boundaries before the propagation of the distribution functions is executed simultaneously on all processors.A subdomain may also exceed the size of the cache; hence, cache optimization is performed in the parallel implementation [12]. Here the subdomain (on a processor) is divided into subsections to keep each subsection within cache memory limits.The ADI method for the 2-D Burgers’ equation [7] is an implicit time marching procedure that consists of two half-time steps in a single time step. On a N×N square grid, both half time steps consist of Gaussian elimination computations for solving N independent tridiagonal equation systems, each system containing N unknowns. The ADI scheme is unconditionally stable and has second order accuracy in both space and time [7]. Time step size needs to be chosen such that the truncation error will not be dominated by the temporal component and so that accuracy will not decrease with every time step. The ADI method is implemented for distributed memory systems using domain decomposition following the strategy advocated by Zhu [13].The ADI method is not well suited for cache optimization due to the global nature of the Gaussian elimination computations. During each time step tridiagonal matrices are inverted using elimination along the length of the whole domain. Due to this, sections of the domain cannot be updated separately, which means data cannot be called from the cache repeatedly. The largest square grid size that can be accommodated within an 8MB size cache is approximately 700×700. For the parallel ADI scheme, for instance, cache optimization can occur on a 1200×1200 square grid because data involved in the computation fits into the cache when the number of processors is greater than 12.To evaluate the performance of the LBM and the ADI method relative to each other, their accuracy, and compute times are compared for a Reynolds number of 5000. The results were computed on a parallel computing architecture with twenty-four 400MHz processors, 512MB main memory, and 8MB cache per processor. MPI was used as the message-passing library, and Fortran 90 was the programming language.The numerical solution is compared with the known analytical solution; therefore, accuracy has been measured using the L2-norm error. The steady state solution was obtained through marching the equations of both methods in time until convergence. The convergence criteria utilized with both methods would allow them to converge when the absolute difference between the current time iteration’s L2-norm error and the previous time iteration’s L2-norm error is less than 10e−6. For the LBM on a 4800×4800 grid, absolute difference was increased to 10e−7 to obtain better accuracy that was consistent with the accuracy of the ADI. From Table 1, it can be observed that the LBM has slightly better accuracy than the ADI method. Increasing the grid resolution reduces the error in both cases. The increasing grid resolution causes the LBM time step size to go down to keep the scheme numerically stable. For a Reynolds number of 5000, the ADI time step size is 0.004 for all grid sizes. Taking bigger time steps than 0.004 will not allow the ADI method to converge or would require many more time iterations to converge. In addition to the L2-norm error, the absolute error at specific locations in the domain is discussed here. From x=0 to x=0.9975, the solution is close to 1. The steep shockwave-like drop in the solution starts near x=0.9975 and ends at x=1. The solution at grid points closest to the right boundary at x=1 in the 2-D domain demonstrates the occurrence of a steep gradient and therefore is selected for comparison between the numerical and analytical solution. For Reynolds number 5000 and a 6000×6000 grid, Table 2shows the absolute values for the numerical and analytical solutions and the absolute percentage error for the last six grid points in the x-direction at y=0.5. The maximum percentage error for the LBM is less than 0.4%, whereas for the ADI method it is less than 2.5%.To assess the parallel performance of both methods, the parallel speedup was measured. In this paper, parallel speedup is defined as(24)Parallel speedup=CPU time for4-processor caseCPU time for>4-processor caseTable 3shows the parallel speedup for LBM and the ADI method for grid sizes 2400×2400 to 6000×6000, respectively. The parallel speedups of both methods are linear to superlinear (for bigger grids), thus demonstrating the efficiency of the parallel implementations. The superlinear speedup is achieved because the 4-processor case performs poorly when data size increases to the level of the processor memory size.Relative speed is defined as the ratio of the computation times of the two methods. Table 3 compares the computation times of the LBM with those of the ADI method, and Fig. 3represents the relative speed graphically. The ADI computation time is less than the LBM computation time by a factor of 1.7–4.8 for grids ranging from 2400×2400 to 6000×6000. This is because the ADI scheme has a time step of 0.004 for all grid sizes whereas the LBM’s time step decreases with increasing grid size (Table 3). Therefore, the LBM takes more time iterations to converge. For grid sizes 2400×2400–6000×6000, the number of time iterations taken by the LBM are 6–30 times the number of iterations taken by the ADI scheme. This is reflected in the results for relative speed.As noted earlier, the ADI method is superior to the LBM when solving steady flow fields with high-resolution meshes. As the grid resolution increases, the performance of the LBM progressively decreases because the time step size decreases with grid spacing. However, the LBM outperforms the ADI method and other traditional methods when solving unsteady flows using Cartesian grids because in unsteady cases the time step size is decided by time-accuracy requirements and is usually lower than the time step size required for numerical stability. Study [2] showed the LBM to be faster than the ADI method by a factor of 8–8.5 when solving the 2-D unsteady Burgers’ equation, where both methods possessed the same numerical time step size. Many fluid flow problems have regions within the flow domain in which the solution changes rapidly in a relatively small region (e.g., boundary layers). These regions can be localized in space by dividing the computational domain into subdomains. To adapt the grid to the behavior of the exact solution, greater grid resolution is required in the subdomains containing the singular layers while the rest of the subdomains can be discretized with coarser grids. This brings about an opportunity to use explicit methods on the coarse grid region and an implicit method on the fine grid region. The coarse grid allows the explicit method to take on bigger time steps that can be equal to those of the implicit method. Based on these observations, the coupled LBM-finite difference methods are proposed to harness the computational efficiency of the cache-optimized LBM and the faster convergence properties of implicit finite difference methods for solving steady flow problems. Asproulis et al. [14] used a similar rationale for coupling molecular dynamics and traditional CFD for modeling flow problems in microfluidic devices.The steady 2-D Burgers’ equation with the initial and boundary conditions mentioned in Section 1.1 will be used as the test problem. In this problem, a steep gradient or boundary layer exists along the right boundary. A multi-block Cartesian grid will be used to discretize the domain. A high-resolution grid block will cover the region along the right boundary, and a coarse grid block will cover the rest of the domain, as shown in Fig. 4. The LBM will be applied on the coarse grid block, and the ADI method will be applied on the fine grid block. To enable transfer of information between the two methods, the two grid blocks overlap at the interface.The coupling procedure should enable the matching of the solution between adjacent sub-domains. The Schwarz alternating method [15] is used as the coupling procedure on overlapping subdomains. This leads to the following method for communicating information between adjacent subdomains:1.The variables on the coarse grid block are updated using the LBM. The distribution functions on the interface boundary of the coarse grid are computed from the macroscopic variable (u) at the corresponding position in the neighboring fine grid block.The ADI solver computes the macroscopic variable in the fine grid region covering the boundary layer. The interface boundary conditions are specified using the macroscopic variables computed in Step 1 at the corresponding coarse grid locations. Because the grid spacing for both blocks is different, linear interpolation is performed to obtain the interface boundary conditions at the fine grid points that do not have a corresponding coarse grid point.As seen above, the coupling procedure essentially consists of solving two Dirichlet problems on overlapping subdomains. With the traditional Schwarz alternating method, Steps 1 and 2 are executed alternately until the difference between the current solution and the solution from the previous iteration on the interface boundary of each grid block is less than a given tolerance. This procedure occurs at every time step. But to make full use of the cache-efficient nature of the LBM, both Steps 1 and 2 need to be executed separately for a certain number of time steps keeping the same interface boundary conditions. There are two strategies that can be adopted for making use of the LBM’s cache efficient nature. One strategy is to separately execute Steps 1 and 2 for tdiv (Section 3.1) number of time steps and then update the interface boundary conditions before proceeding to the next installment of tdiv time steps until the convergence criteria for the Schwarz alternating method is achieved. This procedure will be carried on until steady state conditions are reached. A high level description of this strategy (termed Strategy 1) is given below.Let Ω1 and Ω2 represent the two overlapping grid blocks (or subdomains), and ∂Ω1 and ∂Ω2 represent their boundaries. The part of ∂Ω1 lying in Ω2 is represented with Γ1 (an artificial boundary or internal boundary or interface boundary of Ω1). Likewise Γ2 represents the interface of Ω2.1.Initialization of the distribution functions on Ω1 and the macroscopic variable on Ω2.Loop for all time steps (until the final time or until steady state conditions are reached in the whole domain).Loop for the Schwarz iterations (to update the interface boundary conditions until they do not change significantly).Distribution functions | Γ1 are computed using u | Ω2 near Γ1.Loop for tdiv number of time steps (for the LBM cache optimization).Update the distribution functions on Ω1 using the LBM (boundary conditions remain unchanged).End loop for tdiv time steps.Compute u | Γ2 from the distribution functions | Ω1 near Γ2.Loop for tdiv number of time steps (this way Ω2 will have the same final time as Ω1).Perform ADI computations to update u on Ω2 (boundary conditions remain unchanged).End loop for tdiv time steps.The loop ends for the Schwarz iterations when the solution on both interface boundaries does not change significantly with the next iteration.End loop for all time steps when a steady state solution is reached.The second strategy is to execute Steps 1 and 2 independently without updating the interface boundary conditions or communicating any information between the subdomains until steady state conditions are reached in the respective subdomains. After this the Schwarz alternating method is applied so that Steps 1 and 2 are executed alternately and the interface boundary conditions are updated accordingly until the solution at both interface boundaries does not change significantly with new Schwarz iterations or updates. A high level description for this strategy (termed Strategy 2) is given below.1.Initialization of the distribution functions on Ω1 and the macroscopic variable on Ω2.Loop for time stepping until steady state conditions are reached in Ω1 (keeping the same boundary conditions).Loop for time stepping until steady state conditions are reached in Ω2 (keeping the same boundary conditions).Loop for Schwarz iterations (to update the interface boundary conditions until they don’t change significantly).Distribution functions | Γ1 are computed using u | Ω2 near Γ1.Update distribution functions on Ω1 using the LBM (boundary conditions remain unchanged).Compute u | Γ2 from the distribution functions | Ω1 near Γ2.Perform ADI computations to update u on Ω2 (boundary conditions remain unchanged).The loop ends for the Schwarz iterations when the solution on both interface boundaries does not change significantly with the next iteration.The coupled LBM–ADI method is implemented on a parallel computer with the parallel LBM and the parallel ADI method being executed alternately in the Schwarz method. Fig. 5shows the parallel domain decomposition for the respective solvers. At the beginning of each Schwarz step, information needs to be transferred from one grid block to the other. This requires global communication using mpi_scatter when transferring information from the lattice Boltzmann grid block to the ADI grid block or mpi_gather when transferring information from the ADI grid block to the lattice Boltzmann grid block.The LBM–ADI method is a coupled explicit–implicit method. In order to quantify its computational performance and accuracy, it should be compared with traditional finite difference methods that are applied over the whole domain. The LBM–ADI method is compared with the ADI method implemented on the multiblock grid (ADI–ADI method). The ADI–ADI procedure implements the ADI method separately on all grid blocks in the domain. The coupling procedure described in Section 7.2 is therefore applied here too. In the parallel implementation, communication between the two subdomains is non-global unlike the coupled LBM–ADI method.

@&#CONCLUSIONS@&#
From the results for both strategies, it is expected that an increase in the size of the region represented by the LBM will result in increased reduction in the CPU times relative to traditional methods. The results presented here are for steady state problems. The methods outlined above can also be used for numerical solutions of unsteady flow fields. For example, cases where very high grid resolution is required in resolving boundary layers, the time step size for the lattice Boltzmann method may be smaller than the time step size required for time accuracy. Therefore an implicit method can be used in the boundary layer subdomain, and the lattice Boltzmann method can be used in the outer subdomain for unsteady flows. When using the coupled LBM–ADI method for unsteady flows, Strategy 1 is the viable choice to provide a time accurate solution.To further explore the full potential of the coupled method, it should be applied to solve incompressible fluid flow problems and compared against traditional numerical solvers for incompressible fluid flows.