@&#MAIN-TITLE@&#
Identification of human drug targets using machine-learning algorithms

@&#HIGHLIGHTS@&#
Identification of candidate drug targets.Development of sequence based prediction method.SMOTE as a pre-processing step for balancing the training data.Complementary tool for novel drug target prediction.

@&#KEYPHRASES@&#
Drug targets,Ensemble learning,SMOTE,Dipeptide composition,Property group composition,ReliefF,

@&#ABSTRACT@&#
Identification of potential drug targets is a crucial task in the drug-discovery pipeline. Successful identification of candidate drug targets in entire genomes is very useful, and computational prediction methods can speed up this process. In the current work we have developed a sequence-based prediction method for the successful identification and discrimination of human drug target proteins, from human non-drug target proteins. The training features include sequence-based features, such as amino acid composition, amino acid property group composition, and dipeptide composition for generating predictive models. The classification of human drug target proteins presents a classic example of class imbalance. We have addressed this issue by using SMOTE (Synthetic Minority Over-sampling Technique) as a preprocessing step, for balancing the training data with a ratio of 1:1 between drug targets (minority samples) and non-drug targets (majority samples). Using ensemble classification learning method-Rotation Forest and ReliefF feature-selection technique for selecting the optimal subset of salient features, the best model with selected features can achieve 87.1% sensitivity, 83.6% specificity, and 85.3% accuracy, with 0.71 Matthews correlation coefficient (mcc) on a tenfold stratified cross-validation test. The subset of identified optimal features may help in assessing the compositional patterns in human drug targets. For further validation, using a rigorous leave-one-out cross-validation test, the model achieved 88.1% sensitivity, 83.0% specificity, 85.5% accuracy, and 0.712 mcc. The proposed method was tested on a second dataset, for which the current pipeline gave promising results. We suggest that the present approach can be applied successfully as a complementary tool to existing methods for novel drug target prediction.

@&#INTRODUCTION@&#
The identification of drug targets is one of the foremost requirements in the drug-discovery process. During this drug-discovery process, we come across certain proteins which are “druggable” (a target is said to be druggable when it can interact with drug molecules), according to their structure, but their binding does not lead to any therapeutic effect. At that time, we were limited in the ability about the identification of such human drug target proteins, to ascertain their specific character, nature and efficacy. Researchers have been working on drug research and development for many years, but only about 324 drug targets have so far been identified as suitable for clinical use [1]. Therefore, it is imperative to determine more potential targets for drug design and discovery [2]. Ideal drug targets should have some desirable properties, such as druggability, and must show active involvement in a significant biological pathway. Knowledge about its structure-function relationship is equally desirable [3].According to some recent studies, most of the drug targets fall into the family of enzymes, transporters, GPCRs (G protein-coupled receptors), ion channels, nuclear receptors etc. GPCRs and enzymes represent the most important target classes of proteins for drug discovery [4]. More than50% of drug targets are located on only 4 key protein families, namely GPCRs, nuclear receptors, ligand-gated ion channels and voltage-gated ion channels [2].Sequence homology and domain search methods of existing drug target families have been developed to identify new drug targets [5]. Other studies have revealed certain binding sites which might bind to drug-like compounds on the protein surface, based on 3D structures [6].But these methods have limited scope because of the lesser number of proteins they contain, with known 3D structure.Previously, Li et al. [7]used SVM (Support vector machines) for the exclusive classification of human drug target proteins from human non-drug target proteins, with an overall accuracy of 84%. SVM was also used by Han et al. [8] for the discrimination of drug targets and non-drug targets, with 83.6% overall accuracy, using sequence-based features such as amino acid composition and other physicochemical properties. The work of Han et al. [8] focused on successfully commercialized and research targets in the therapeutic target database [9]. Using physicochemical features Bakheet et al. [3] analyzed the differences between drug targets and non-drug targets, and subsequently, generated druggability rules.A dataset is said to be imbalanced when there is a large difference between the numbers of examples belonging to different classes. Human drug target protein discrimination from human non-drug target proteins presents a class imbalance problem, in which the class of interest—i.e. drug targets—are the minority samples, and non-drug targets are the majority samples. This presents a challenge for the machine-learning algorithms in learning the concepts of the minority class samples, and causes a sharp decline in accurately predicting (the accuracy for the positive class, i.e. sensitivity) the minority class samples, as the learning is biased towards the majority class samples. In the present work, we have addressed the issue of imbalanced datasets on predictive accuracy, and have used SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset, which is used later to generate a predictive model for the successful discrimination of human drug targets and non-drug targets.We used two datasets, the first is the dataset of Bakheet et al. [3], which consisted of 148 human drug target proteins (positive samples), and 3573 non-drug target proteins (negative samples), having less than 20% pairwise sequence identity. The second dataset was created using 186 human drug target proteins from Li et al. [7] as positive samples, and with the negative samples from the first dataset.We calculated three sets of features to represent the drug target proteins and non-drug target protein sequences.Amino acid composition (AAC): The percentage composition of the 20 amino acid residues was calculated using the following formula:(1)%Aminoacidcompositionofnthproteinsequence=totalnumberofaminoacidsoftypeitotalnumberofaminoacidsinthenthproteinssequence×100This constitutes the first 20 components of the feature vector.Amino acid property group composition (PGC): Physicochemical properties of amino acids play an important role in determining the function and structure of a protein sequence. Amino acid property groups divide the amino acid residues into overlapping sets of similar physicochemical properties. It is included in the feature vector for encapsulating the compositional differences of similar groupings of amino acids.(2)%Aminoacidpropertygroupcompositionofnthproteinsequence=totalnumberofaminoacidsoftypeithpropertygrouptotalnumberofaminoacidsinthenthproteinsequence×100This constitutes the next 11 components of the feature vector. The amino acid property groups [10], which we have taken for the current study, are presented inTable 1.Dipeptide composition (DPC): There can be 400 dipeptides in a protein sequence; these are included in the feature vector to encapsulate the local sequence order of amino acids.(3)%Dipeptidecompositionofnthproteinsequence=totalnumberofdipeptidesofithtype400×100This constitutes the last 400 components of the feature vector. Every drug target protein and non-drug target protein is represented by a maximum of 431 length feature vector.When the dataset is imbalanced in terms of the number of positive and negative class instances, the common evaluation metrics, such as accuracy with which most of the learning algorithms are optimized to perform, tend to be biased towards majority class instances, which is undesirable. SMOTE (Synthetic Minority Over-sampling Technique) [11] is a sampling technique which oversamples the minority class, by using randomly selected k nearest neighbors of the minority class, and generating a synthetic minority class instance, by interpolating nearest minority class instances. As the present dataset is highly imbalanced we used SMOTE to balance the dataset. We have taken all the 148 drug target proteins from the first dataset, and 500 random non-drug target proteins. We increased the number of drug target proteins to be equal to the non-drug proteins for the training set. Subsequently, this balanced dataset, having an equal number of positive and negative class instances, was used for training and model generation. The same procedure was followed for balancing the second dataset.It has been seen in many empirical studies that an ensemble of classifiers outperforms a single classifier [12]. A classifier ensemble combines the prediction of multiple classifiers. It is like combining the classification outcome of many base classifiers trained on different regions of the input space. For the successful learning of an ensemble classifier there should be a best-possible tradeoff between diversity and accuracy among the different base classifiers. Rotation Forest (ROF) is an ensemble classifier developed by Rodriguez and Kuncheva [13]. The concepts of bagging, feature randomization, and principal component analysis are used to increase diversity in the ensemble. Experimentally, it has been shown to perform better than Bagging [14], boosting [15] and Random Forest [16]. The base classifiers in the rotation forest ensemble are the decision trees. For each tree a bootstrapped sample is drawn. Then the full feature set is split randomly into k subsets. These subsets are then transformed linearly and placed in a rotation matrix, to be used for training for each decision tree classifier. The average rule is used to combine the outcome of all the base classifiers. Rotation Forest has found its applications in various domains of bioinformatics, as in [17,18].When large numbers of attributes are used to represent the protein sequences, then the predictive model may suffer from the curse of dimensionality. The presence of redundant and uninformative features increases the training time, and the generalization ability of the generated model. Given a set of features F and target class C, the aim of the feature selection is to find a minimum set of features F, in order to achieve maximum classification performance. The ReleifF algorithm [19] assesses the quality of the attributes by assigning higher and lower weights to the different attributes, according to their discriminating ability. The algorithm begins by randomly selecting an instance I from the training sample. Then it finds the k nearest neighbors of the same class, called nearest hits H, and also k nearest neighbors of the different classes, called nearest miss M. It then assigns weights to features, based on their ability to discriminate similar samples. Higher values of weights are assigned if the instances I and M have different values on the given attribute—i.e. if the difference is large between them—and lower values are assigned if instances I and H have different values on the given attribute. ReliefF has been used successfully for feature reduction, for example, in [20,21]. We have incorporated the feature-selection process along with the classifier itself. The feature-selection process was performed firstly on the training folds (in a tenfold stratified cross-validation), and then the selected features were used to evaluate the testing fold. In this way the process avoids exceedingly optimistic evaluation parameters, as the feature-selection algorithm has not seen the test data during the feature-selection process.All the experiments were performed in WEKA [22] which is an open-source machine-learning platform.When the amount of training data is limited then cross-validation is typically used. We tested the generated predictive models by using a tenfold stratified cross-validation. During this process the dataset is first divided equally into 10 subsets. Then each of the subsets is, in turn, used as the testing set, and the remaining 9 subsets are used as the training sets. In stratified cross-validation, each fold or subset contains equal proportions of instances belonging to different class labels.For further validation of the performance of the generated models we used leave-one-out cross-validation (LOOCV), which is deemed to be a more rigorous and very objective statistical test [23]. It is now being widely used and accepted by researchers, to estimate the accuracy of various predictors [24–31]. In this method, each sequence is, in turn, singled out as a test instance, and the model is trained using the remaining training instances. This process is repeated n times (n is the total number of sequences in the dataset). The results for all n sequences are for each sequence of the dataset. These are averaged and that average represents the final error estimate.We used the following evaluation metrics to assess and compare the different machine-learning models. These parameters are calculated from the values of true positives (TP), false negatives (FN), true negatives (TN) and false positives (FP).Sensitivity: This parameter allows the computation of the percentage of correctly predicted drug target proteins.(4)Sensitivity=TP/(TP+FN)×100Specificity: This parameter allows the computation of the percentage of correctly predicted non-drug target proteins.(5)Specificity=TN/(TN+FP)×100Accuracy: Percentage of correctly predicted drug target and non-drug target proteins.(6)Accuracy=(TP+TN)/(TP+FP+TN+FN)×100Area under ROC (AUC): It is the area under the curve (AUC) of a receiver-operating characteristic (ROC) curve. The area under the ROC is an important evaluation metric for assessing the performance of the model. The value of the AUC ranges from 0 to 1. In the best case it is 1; in the worst case it is zero; in random ranking it is 0.5.Matthews Correlation Coefficient (MCC): Its value ranges from −1 to +1, where a value of +1 means accurate prediction, and a value of zero means random prediction.(7)MCC=(TP×TN)−(FP×FN)(TP+FN)(TP+FP)(TN+FP)(TN+FN)F-measure: This is a combined measure of precision and recall, and is calculated as:(8)F−measure=2×Precision×RecallPrecision+RecallThe best value is 1, and in the worst case it is 0.We tested five different machine-learning algorithms with three different sets of feature vectors: (i) amino acid composition; (ii) amino acid composition and amino acid property group composition; and (iii) amino acid composition, amino acid property group composition and dipeptide composition. A distinct improvement in all the learning algorithms was visible after the inclusion of all three types of sequence-based features. We achieved a 100% accuracy in a self-consistency test on the first dataset (as compared to 96% in the previous study [3]), and 99.7% on the second dataset. In the self-consistency test, the class label of drug targets and non-drug targets is predicted by using the rules of the same set on which they were trained. It gives an optimistic error estimate, and is not sufficient to evaluate the true discriminating power of the learning algorithm. The performance of different machine-learning algorithms, using different feature sets on a tenfold stratified cross-validation, are presented inTables 2–4. It can be seen from these tables that the ensemble-based methods, such as random forest and rotation forest, performed consistently better than other non ensemble methods. Overall performance of the rotation forest algorithm was better than all other algorithms.The feature set consisting of AAC, PGC and DPC resulted in maximum discrimination between protein drug targets and non-targets, with sensitivity of 83.9%, specificity of 82.2%, and accuracy of 83.1% (seeFig. 1 for ROC plot).The length of the dipeptide feature set is 431, including AAC, PGC and DPC. In such long feature vectors there is a possibility of the presence of redundant and uninformative features. Redundant and irrelevant features do not provide any important information for classification/prediction. Instead, they can result in increased training time and overfitting, which are undesirable for the successful development of prediction/classification models. Using the ReliefF feature-selection technique, we gradually increased the number of selected features, and observed their effect on the model evaluation metrics (Table 5). We systematically increased the number of features in the optimal feature subset. The feature set with 400 features gave the best performance, with 85.3% accuracy, 0.71 mcc and F-measure value of 0.853.The tradeoff between the evaluation parameters was best for this subset of features.We tabulated the subset of optimal features (AAC and PGC only) selected by the ReliefF feature-selection technique, in accordance with their importance in discriminating drug and non-drug target proteins (Table 6). The dipeptides which were selected and discarded from the optimal subset of features are shown inFig. 2. The white boxes with annotated dipeptides in Fig. 2 represent the discarded dipeptides. The dipeptides of Q, P and W are mostly absent from the optimal subset of features.For the second dataset we used the same pipeline as that applied to the first dataset. Based on overall accuracy, AUC and F-measure rotation forest outperformed all other machine-learning algorithms, and the inclusion of all three features viz AAC, PGC and DPC significantly improved the prediction accuracy (Table 7) (see Fig. 3 for ROC plot).The results of leave-one-out cross-validation are tabulated inTable 8, for both first and second datasets. The rotation forest algorithm achieved an accuracy of 85.5% and 87.2%, on the first and second dataset, respectively, which is better than other tested machine-learning algorithms.

@&#CONCLUSIONS@&#
