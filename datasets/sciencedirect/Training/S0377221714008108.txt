@&#MAIN-TITLE@&#
The continuous p-centre problem: An investigation into variable neighbourhood search with memory

@&#HIGHLIGHTS@&#
A powerful VNS algorithm is proposed for the p-centre problem on the plane.The local search is enhanced by speed-up rules used in Elzinga–Hearn's algorithm.Well structured neighbourhoods and intelligent allocation schemes are designed.An adaptive learning process is developed and embedded within the search.Extensive computational experiments are presented with new best results.

@&#KEYPHRASES@&#
p-Centre problem,Continuous space,Variable neighbourhood search with memory,Adaptive search,Elzinga–Hearn algorithm,

@&#ABSTRACT@&#
A VNS-based heuristic using both a facility as well as a customer type neighbourhood structure is proposed to solve the p-centre problem in the continuous space. Simple but effective enhancements to the original Elzinga–Hearn algorithm as well as a powerful ‘locate–allocate’ local search used within VNS are proposed. In addition, efficient implementations in both neighbourhood structures are presented. A learning scheme is also embedded into the search to produce a new variant of VNS that uses memory. The effect of incorporating strong intensification within the local search via a VND type structure is also explored with interesting results. Empirical results, based on several existing data set (TSP-Lib) with various values of p, show that the proposed VNS implementations outperform both a multi-start heuristic and the discrete-based optimal approach that use the same local search.

@&#INTRODUCTION@&#
In the p-centre problem, the objective is to locate a given number (p) of facilities in order to minimise the maximum distance from a set of fixed points to their closest facilities. In this study, we investigate the case where the facilities can be located anywhere in the plane. This is contrary to the commonly used case where the facility locations are restricted to a candidate set of potential sites. The continuous solution though has some weaknesses in terms of practicality can be of help to identifying potential sites that are nearer to the best locations as gathering the data can, in some situations, be expensive. Also, the information obtained can be used as a green field solution for assessing the company's chosen facilities. The most cited application of the p-centre problem involves the location of emergency facilities where response times are critical. Thus, to obtain an ‘equitable’ solution, the objective is framed as the minimisation of the worst response time instead of the average response time. Related economical implications include among others the recent work by Murray and Wei (2013) and the one by Lu (2013). The former used set covering and GIS to obtain the least number of facilities to cover the entire area of study. Two real life applications are used where the first one aims at locating emergency sirens in Dublin (Ohio) whereas the second is about the siting of fire station in Elk Grove (California). The paper by Lu (2013) explores the use of p-centre as part of emergency management while taking into account uncertain demand as this is very common in emergency logistics systems aiming at responding to natural disasters. A case study using the earthquake in Taiwan in 1999 is adopted.The existing research on the p-centre problem deals mainly with the network (or discrete) formulation of the problem; this version is usually referred to as the vertex p-centre problem. For a fixed value of p, the vertex p-centre problem can be solved in polynomial time. This can be done by evaluating each of the O (np) possible combinations of p facility sites in polynomial time. For more details refer to Chen and Chen (2009) and Salhi and Al-Khedhairi (2010) and references therein.For the continuous case, efficient solution approaches have been proposed for the one-centre problem (p =1) including Elzinga and Hearn (1972) who devised an exact geometrical approach for solving optimally the problem. Enhancements to speed up the search were also introduced by several authors, see Xu, Freund, and Sun (2003) and references therein. For p = 2, Drezner (1984a) designed an interesting exact algorithm where the idea is to enumerate efficiently all the possible disjoint pairs of subsets (i.e., n(n −1)/2 possibilities) by using the optimal algorithm for p = 1 for each subset. For large values of p (p≥3), the problem is known to be NP hard (see Megiddo &#38; Supowit, 1984).This problem can also be considered as a MinMaxMin type problem with the following objective functionZ=MinXMaxi=1,…,n[wiMinj=1,…,pd(Pi,Xj)]wheren: the number of demand points (fixed points or customers)p: the number of facilities to openPi= (ai, bi): the location of fixed point i (i = 1, …, n)wi> 0: the weight of fixed point i (i = 1, …, n)X = (X1, …, Xp): the decision variables vector related to these p facility locations withXj= (xjyj) representing the location of the new facility j   with Xj∈ ℜ2; j = 1, …, pd(Pi, Xj): the Euclidean distance between Piand Xj(i = 1, …, n; j = 1, …, p)The above multiple facility location problem has been examined by a small number of authors, see Plastria (2002) and the references therein. For larger values of p and n, heuristic methods were developed by Drezner (1984b) and Eiselt and Charlesworth (1986) where the iterative procedures are based on the idea of ‘locate–allocate’ with the use of the add/drop/swap moves. A Voronoi diagram-based heuristic, that has the flavour of Cooper's well-known locate–allocate strategy, was also proposed in Suzuki and Okabe (1995). This method was generalised by Wei, Murray, and Xiao (2006) to account for irregular shapes and constraints on the possible locations of the new facilities. Relaxation methods, based on solving optimally small subsets of the original problem, which are then gradually increased in size by a given number of demand points, were developed by Chen and Chen (2009) for both the discrete and the continuous p-centre problem with excellent results. This method is interesting as it could generate optimal solutions though it is sensitive to the number of added points. Observing the literature two issues arise. Firstly the optimal methods can solve instances with limited sizes and secondly there are no meta-heuristics only greedy and simple improvement type methods.The contributions of the present paper include(i)Enhancements on the Elzinga–Hearn's method which is part of the local search used for the p-centre problem.The design of a powerful meta-heuristic namely a VNS by introducing efficient neighbourhood structures and effective enhancements in its local search to solve large instances.The incorporation of memory in VNS to provide flexibility and guidance to the search.Extensive computational experiments for large instances leading to new results that are useful for benchmarking purposes including the optimal solutions for the discrete case.The paper is organised as follows: Enhancements for the Elzinga–Hearn algorithm (i.e. p = 1) are described in Section 2 alongside an initial application and adaption to the p-centre problem. In Section 3, a VNS implementation is produced followed by improvement schemes in the generation of an effective neighbourhood structure in Section 4 and enhancements on the local search in Section 5. A learning mechanism that systematically responds to the characteristics of a given instance making VNS not memoryless is provided in Section 6. In Section 7 computational experiments are presented followed by our conclusion and suggestions in the last section.Though the algorithm is polynomial of the order O(n2) and hence very fast, any enhancement would seem not to be worthwhile if the aim was to solve the 1-centre problem only. However, our aim is to solve the p-centre problem instead where we need to resolve to solving the 1-centre problem a large number of times and therefore the cumulative computational saving would be, in our view, worth considering. For completeness, a brief recall of the Elzinga–Hearn algorithm is first given followed by our proposed enhancements.The optimal solution for the 1-centre problem (X) can be obtained with a geometrical-based approach using the following two results.Result 1 (case of two critical points say Psand Pt)The optimal solution X lies at the intersection of the set L(Ps, Pt) = {X: wsd(Ps, X) = wtd(Pt, X) ∀s ≠ t = 1, …, n} and the line between the points Psand Pt.Let r = ws/wt. If r = 1 the set L reduces to the perpendicular bisector, otherwise L is a circle with radius rd(Ps, Pt)/|1 − r2| and centre (Ps− r2Pt)/(1 − r2).Result 2 (case of three critical points, say Ps, Ptand Pu)The optimal solution is determined by one of the pairs of points Psand Pt, or Psand Pu, or Ptand Puleading to the points a, b or c respectively, or by all three points in which the solution lies at the intersection of L(Ps, Pt), L(Ps, Pu) and L(Pt, Pu) leading to the points e1 and e2.From these five points {a, b, c, e1, e2}, the choice will reduce to choosing one point either from {e1, e2} or {a, b, c}.In brief, the optimal solution can be determined by one, two or three fixed points only which are referred to, in the literature, as the critical points. Using these interesting results, Elzinga and Hearn (1972) developed the following algorithm (see Fig. 1) to find the optimal location for the 1-centre problem in the continuous space.Regarding the addition of the 4thpoint (Step 6 of Fig. 1), six combinations need to be evaluated only (three using two points and the other three requiring three points). Moreover, if the problem is unweighted, there is no need to check all the six cases.Elzinga and Hearn (1972) noted the following weaknesses of their algorithm: (i) selection of the starting points (Step 1 of Fig. 1) and (ii) the selection of the uncovered points in Step 2 and Step 5 of Fig. 1. Attempts to address these shortcomings were made by Hearn and Vijay (1982), but with less convincing results. Here we propose two simple but effective enhancements for both the weighted and the unweighted cases. The steps of these two enhancements are similar to the original algorithm, except that Steps 1, 2 and 5 of Fig. 1 are replaced as follows:Enhancement 1 (Enh 1)Only Step 1 is changed as follows.Step 1:-Determine the four cornersof the rectangle with horizontal and vertical sides that covers all demand points, namely let B = {i1, i2, i3, i4} withi1=ArgMini=1,…,n(wixi),i2=ArgMaxi=1,…,n(wixi),j1=ArgMinj=1,…,n(wiyi)andj2=ArgMaxj=1,…,n(wiyi).Solve the weighted minimax location problem using Result 1 with(Ps,Pt)=ArgMaxi,j∈Bwiwjwi+wjd(Pi,Pj)to obtain X and its cost Z = wsd(Ps, X).Enhancement 2 (Enh 2)This is an extension of Enh 1 where the uncovered point is chosen as the one with the greatest weighted distance in Steps 2 and 5.Step 1: Same Step 1 as in Enh 1.Step 2 (choice of Pu) and Step 5 (choice of Pv):If wid(Pi, X) ≤ Z ∀Pi; i = 1, …, n stop, else select a pointPu(orPv)suchthatPu(orPv)=ArgMaxi=1,…,n(wid(Pi,X)>Z)(i.e., the uncovered point that has the greatest weighted distance from the previous solution).The two enhancements were tested on random instances varying in size from n = 10 to 100 in increment of 10. For each value of n, 100 random instances were tested and average results are reported. The fixed points are randomly generated in a square (0, 100)2.Our two proposed enhancements are found to yield extremely better results than the original implementation. Table 1 provides summary results with the percentage deviation defined as deviation (percent) = 100 × (CPUE − CPUOrig)/CPUOrig with CPUE and CPUOrig refer to the CPU times for Enh 1 (or Enh 2) and the original algorithm respectively.According to Table 1, both enhancements require fewer iterations than the original algorithm in all cases. The average total number of iterations is 4.46 (1.3 + 3.16) and 2.7 (1.03 + 1.67) for Enh 1 and Enh 2 respectively compared to 9.61 (3.84 + 5.77) for the original algorithm. In general, Enh 1 and Enh 2 yield similar time reduction of the original algorithm though Enh 2 is slightly faster on average (58 percentversus 54 percent) but slightly slower when n ≥ 70, see Fig. 2.In summary, either Enh 1 or Enh 2 can be used instead of the original algorithm when solving the 1-centre problem as part of the p-centre problem but in this study we propose the following rule:(1)Ifnk≤70useEnh2,ElseuseEnh1where nk(k = 1, …, p) represents the number of customers in the kthcluster(∑k=1pnk=n).To validatethis claim further, an extensive testing was carried out based on a large sample with n = 100–1000 with a step size of 50. It was observed that the same trend remains valid. For the weighted case, Enh 2 is always found to outperform Enh 1.This saving in computational effort can have a massive effect within heuristics that perform the ‘locate–allocate’ principle a large number of times as will be shown in the next subsection where a simple multi-start procedure is used for the p-centre problem.We have also experimented with the following modifications but all combinations of these have proved to be slower than Enh 1 and Enh 2.(i)Using the farthest two points in terms weighted distance as initial starting points in Step 1.Using the farthest three points as initial starting points in Step 1.Selecting the uncovered point in Steps 2 and 5 that has the greatest weighted distance from the previous solution.In this section we present computational results of the multi-start using the original algorithm (10 random runs) versus those of Enh 1 and Enh 2 for solving the p-centre problem. For illustration purposes, we chose one of the TSP-Lib instances (n=1002) with p varying from 5 to 25 with an increment of 5. We performed 100 iterations for the original multi-start and used the required average time as a stopping criterion for the enhanced versions for which we record the number of iterations as well as the corresponding CPU time.A summary of the comparison between the original algorithm, Enh 1 and Enh 2 is given in Table 2. The results show that there is a significant difference in terms of the total number of iterations between the original algorithm and those of Enh 1 and Enh 2. The average deviations from the original algorithm (100 iterations) for Enh 1 and Enh 2 are 37.20 percent and 31.60 percent respectively. In terms of CPU time, the deviations are −28.55 percent and −25.46 percent. Note that this saving could be made even larger if the hybrid rule (1) was used instead, as this could have been possible especially when p= 15 and 20. This is not performed here purposely as it may disguise the effect of the two enhancements.The basic idea of VNS is to change neighbourhoods systematically while using a local search within each neighbourhood to get to a corresponding local minimum. A brief outline of the basic VNS approach is given in Mladenovic and Hansen (1997) but new versions as well as advanced implementations and applications can be found in Hansen, Mladenovic, Brimberg, and Moreno-Perez (2010, chap. 3). The different neighbourhood structures which we constructed are based on those used for the multi-source Weber problem (continuous p-median problem) with some additional changes to cater for the properties of the minimax objective function. These include customer-based moves (e.g., the removal/addition of one or more customers from a region), and facility-based ones (e.g., opening/closing one or more facilities).The steps of the VNS that uses a customer-based neighbourhood, and which we call VNS(CN) for short, are given in Fig. 3.Explanation of some of the stepsStep 0 (the construction of the neighbourhood structures)Here we remove k customers randomly and allocate them to other open facilities. We refer to this type of neighbourhood as CNk; k = 1, …, Kmax with Kmax denoting the number of neighbourhood structures(Kmax=⌈p⌉).Step 1 (the initial solution)This is generated randomly by choosing p fixed points, though other schemes could also be used.Step 2b (the solution of the 1-centre problem)Our enhancements on the Elzinga–Hearn algorithm using rule (1) are applied to solve the 1-centre problem for both the source and the destination clusters (i.e., the affected clusters). Note that this step could also be inserted at the beginning of the local search in Step 2c.Step 2c (the local search)A locate–allocate procedure, which is similar to that of Cooper (1964), is used here.(i)Given the p locations with their centre Xj(j = 1, …, p), allocate each customer to its nearest centre and define for each centre j, the subset Wj, asWj={(Pi)i=1,…,n:d(Pi,Xj)=mink=1,…,pd(Pi,Xk)},j=1,…,pIn each subset Wj(j = 1, …, p), determine the optimal location Xjusing (1).While there is a change in at least one of the subset Wjor the location Xj; j = 1, …, p, return to (i), else record the incumbent solution X and stop.This local search will be revisited in Section 5.Customer-based VNS(CN) enhancementGiven that any circle of minimum radius can be determined by two or three critical points on its circumference or simply by a singleton point, we build our enhancement by taking this information into account. A preliminary study showed that allocating a critical point, instead of a non-critical point, to another facility is likely to be more efficient. In this enhancement, which we call VNS(CN), the critical points of the largest circle are allocated to the other facilities. The only step of Fig. 3 which is changed is Step 0. This is replaced byStep 0: Define CNk; k = 1, …, Kmax as the sequence of neighbourhood structures representing the k critical points of the largest circle with Kmax being two or three depending on the number of critical points that define the largest circle at a given iteration.In this section, the facility-based neighbourhood algorithm, VNS(FN) for short, is presented. Its steps are similar to those of the VNS(CN) given in Fig. 3 except that in the shaking part, k open facility locations are selected randomly and inserted into other places. These facilities can be located either in the discrete space (fixed points) or in the continuous space. Therefore, this type of neighbourhood which we denote by FNk(X); k = 1, …, Kmax can be classified under two categories namely VNS1(FN) and VNS2(FN), which are defined as follows:Algorithm VNS1 (FN)Here we define the kthneighbourhood structure FNk(X); k = 1, …, Kmax as(2)FNk(X)=X∖⋃r=1kXr∨⋃r=1kX′rwhereXr∈XandXr′∈{P,…,Pn}−XThe main steps of VNS1(FN) are similar to VNS(CN) of Fig. 3 except that Step 0, Step 2a and Step 2b are replaced as follows:Step 0: Define FNk(X); k = 1, …, Kmax using (2) withKmax=⌈p⌉Step 2a : Generate X' ∈ FNk(X) using (2)Step 2b: This step is now void as there is no destination cluster or source cluster.Algorithm VNS2(FN)Here the kthneighbourhood structure FNk(X); k = 1, …, Kmax is defined as followsFNk(X)=X∖⋃r=1kXr∨⋃r=1kX′rwhereXr∈XandXr′∈S⊂ℜ2with(3)S={(x,y)∈ℜ2:Min(ai=1,…,ni)≤x≤Max(ai=1,…,ni)andMin(bi=1,…,ni)≤y≤Max(bi=1,…,ni)}VNS2(FN) is similar to VNS1(FN) except that Step 0 and Step 2a are replaced byStep 0: Define FNk(X); k = 1, …, Kmax using (3) withKmax=⌈p⌉Step 2a: Generate X' ∈ FNk(X) using (3)Step 2b: This step is also void here as there is no destination cluster or source cluster.Based on a preliminary experiment, the performance of VNS2(FN) is found to be relatively better than VNS1(FN). We therefore concentrate on proposing simple but effective enhancements on VNS2(FN). We first develop an effective neighbourhood structure which is then followed by enhancements on the local search.There are some steps in VNS2(FN), especially in the shaking phases of Step 2a which are worth examining. We aim to shake with a strong perturbation, also known as ‘Intensified shaking’ in the literature, see Mladenović, Todosijević, and Urosević (2013).The first idea which comes to one's mind is to reallocate the facilities with small circles and insert them randomly in the larger ones. However, when the solution of the p-centre location problem is not optimal, it is observed that the facility in the largest circle and at least one of its neighbouring facilities cannot be in the right location. This observation led us to explore the idea of reallocating instead the facility locations of the larger circles (1stlargest, the 2ndlargest, ,…, the (Kmax)th largest circle) including the facilities that are around them. This idea can be further refined by focussing on the largest circle and the facilities that are around it only and then locate any facility removed randomly in the largest circle and in its surrounding areas defined by its neighbouring circles which we will explore next. This is achieved using the following two neighbourhood definitions namely the neighbourhood attraction and the neighbourhood removal.For the sake of simplicity let's index the largest circle as C1 defined by (X1, R1) with X1 as its centre and R1 as its radius. The remaining p − 1 circles are indexed in ascending order based on their distances from the largest circle using the distance measure d(Xj, X1); j = 2, …, p. The following additional notation is used.NotationCj: the jth nearest circle to the largest circle C1; j = 2, …, pC⌢j: the area encompassed by circle Cj; j = 1, …, pCPj= {Pi∈ Wj: d(Pi, Xj) = Rj; i = 1, …, n}; j = 1, …, p: the set of critical points of Cj(|CPj| ≤ 3); j = 1, …, p)RCjl: the area encompassed by the circle centred at l ∈ CPj; j = 1, …, pCRj=C⌢j∨⋃l∈CPjRCjl: the jth critical region made up ofC⌢jand its |CPj| surrounding RCjl(l ∈ CPj); j = 1, …, pUCRk=⋃j=1kCRj;k=1,…,Kmax: the union of the k critical regionsCCk': the facilities encompassed by the artificial circle centred at X1 with a radiusR⌢k′=d(Xk′,X1)ifk′>1andR⌢1=R1otherwise(i.e.,k′=1);k′=1,…,p.We refer to CCk' as the k'th covering circle. This can also be defined as a sequence {CCk'} = {X1, …, Xk'} representing the facility of the largest circle and the k' − 1 nearest facilities to it.For example, consider Fig. 4which shows three regions (i.e.,RC1lwith l representing the critical points a1, a2 and a3). It can be shown that these three regions could not contain any facility worth considering. This is because if one of these regions contained a facility, the point of that region would have been already allocated to this facility. For instance, if the region of point a1 contained a facility, a1 would be closer to this facility than its serving facility p1, and therefore a1 would have already been allocated to that facility instead. This is an interesting and powerful property which is also given and proved in Mladenović, Labbé, and Hansen (2003).We take this observation into account to define our neighbourhood for attracting facilities. This is achieved by exploring those regions defined by RCjlas the regions where a facility could be located; j = 1, …, p and l ∈ CPj.In the kth neighbourhood, instead of removing k facilities randomly from X, we remove these facilities from CCk' where k' is the level at that iteration, k' = 1, …, p, see Fig. 5for an illustration. The way k' is updated is defined next.The new kth neighbourhood structure that combines the facility attraction and the facility removal is defined as follows:(4)FNk(X)=X∖⋃r=1kXr∨⋃r=1kX′r;k=1,…,Kmaxwhere(X1, …, Xk) ∈ CCk'; k ≤ k'; k' = 1, …, p and (X'1, …, X'k) ∈ UCRkand the jth facility is located in the continuous space delimited by CRj; j = 1, …, k.As the removal process of the k facilities and their insertion is linked to VNS and to the corresponding covering circle CCk' at a given iteration, we briefly describe how the value of k' is updated which will also be given in the algorithm that follows in Fig. 6. We first remove a facility from CC1 namely the facility encompassed by the largest circle, this facility is then located randomly in UCR1. The local search is then applied on this perturbed solution. If the solution is not improved, we remove two facilities from CC2 and insert them randomly in UCR2. This process is repeated until we reachCCKmax. At this iteration if there is no improvement we revert back to k = 1 as in the standard VNS but we continue increasing k' by setting k' = Kmax + 1 instead. We continue increasing the radius of the covering circle until we either reach CCp(note thatk can be any value between 1 and Kmax but k' = p) or an improved solution is found where we revert back to k = k' = 1. If the latter case happens, we decrease the radius of CCk' by setting k' = k' − 1 where we remove k = k + 1 facilities from CCk' = CCp − 1 and so on until we reach CC1. However as k ≤ k', to control the increase and the decrease of k' we introduced an indicator which we call Flag. If Flag = 1 the covering circle is increasing (k' = k' + 1), otherwise it is decreasing (k' = k' − 1). However if at any iterationk ≥ k', we reset k = k' and Flag = 1. As we start with CC1 we initialise Flag to 1.Based on the neighbourhood structure described earlier and the way CCk' is updated, the new VNS(FN) algorithm is summarised in Fig. 6.As an example in Fig. 5, from CC1 (the largest circle) we select its facility p4 to locate randomly in UCR1 (in one of the regions RC1lwith l being one of the critical points). If the local search improves the solution, we will record the new solution and start again from the new CC1; otherwise we explore CC2 where we have two facilities p4 and p6. These will be located randomly in the continuous space of UCR2.The second part of the Cooper's locate–allocate procedure (i.e., the allocation phase) is also modified here. We propose two enhancements to be used when there is no improvement after the exchange between the location and the allocation phases. These include the allocation of the critical points and the closure of the non-promising facilities.Here we focuson a simple but effective reallocation of the critical points of the largest circle to their neighbouring facilities.Additional notationsCl′=setoffacilitiesencompassedbythecircleC(l,2RMax),l∈CP1Cl′′=setoffacilitiesencompassedbythecircleC(l,RMax),l∈CP1Vl={Xj∈Cl′∖Cl′′;j=1,…,p},l∈CP1:thesetoffacilitiesthatareencompassedbyCl′butnotbyCl′′The reasoning behind this enhancement is to remove a critical point (l ∈ CP1) and reallocate it in the neighbouring facilities that surround point l based on the subset Vl. This is performed for all l ∈ CP1. The main steps of this procedure, which we refer to ALLOC, are given in Fig. 7.Note thatin case there is more than one largest circle (case of tie) the procedure is repeated. This allocation process continues until a better allocation cannot be found.Fig. 8(a) showsCa1′andCa1′′based on the critical point a1, initially served from facility p1. There are three facilities p2, p3 and p4 in the region ofVa1.Allocating a1 to one of these three facilities can improve the solution as long as the radius of the destination cluster is less thanRmax. Fig. 8(b) shows the case where the critical point a1 is allocated to facilityp2 yielding a new radius R'max < Rmax.To illustrate the impact of this reallocation, computational results of the multi-start algorithm using 1000 runs with and without this scheme are given in Table 3. The existing data set with known optimal solutions (n = 439 TSP-Lib) with p = 10 − 100 is used here.The integration of this reallocation procedure has improved the solution by up to 13 percent (when p = 100), with an average of over 4.5 percent while requiring a negligible extra computing time.The idea is to identify those facilities that serve the critical fixed points only and to allocate them to other facilities which will lead to such facilities having no customers and hence a reduction in the number of facilities. These saved facilities could then be located in the continuous space encompassed by the larger circles.Letδj={1if|Wj|=|CPj|;j=1,…,p0otherwiseLet q be the number of facilities saved. These q facilities are then located one at time near the critical points of the largest circle based on the reallocation scheme described in Fig. 8. The main steps of this removal procedure are summarised in Fig. 9.For instance, Fig. 10(a) shows a feasible solution of a 5-centre problem. Here the critical points of the circle centred at p3, namely c1, c2 and c3 are allocated to the facilities located at p5, p4 and p2 respectively. Note that there are no non-critical points encompassed by this circle. A feasible solution of a 4-centre for the same problem is then presented in Fig. 10(b), where the newR1 = Rmax = R'max. The facility initially located at p3 can now be relocated in the largest circle centred at p1 leading to having two facilities, each with a radius ≤ Rmax.Table 4 shows the computational results of this enhancement, when it is applied on the solutions of the multi-start algorithm with 1000 runs using the existing TSP data (n = 439 TSP-Lib).Our VNS(FN) algorithm is the VNS2(FN) algorithm described in Fig. 6 with the local search in step 2c incorporating the ALLOC procedure of Fig. 7 and the removal mechanism given in Fig. 9.In the traditional VNS-based implementations the search is memoryless. In this section we intend to incorporate learning within the search so to identify any useful values of the parameters that are worth controlling in VNS.The learning consists of two stages. In the first stage, we record some information about the progress of the VNS. This is performed during a certain time period defined asβ (percent)of the maximum CPU. The information that we are interested in includes the use of the kth neighbourhood, the level of the coverage k' and the value of Kmax. The second phase uses the information obtained to guide the search in subsequent iterations of the VNS. A skeleton of the VNS with memory is given in Fig. 11.In Step 2 we can gatherthe information by recording the score of each neighbourhood either as a fraction of its use, or just the number of success. A density function say(5)Prob(k)=Score(k)∑k=1K′maxScore(k)can then be computed with K'max defining the initial value of Kmax.The new value of Kmax could be set as(6)Kmax=|{k∈{1,…,Kmax′}:Prob(k)>0}.The facility-based neighbourhood can incorporate the process of learning by identifying the number of the preselected facility candidates (k) and the levels of the covering circles. Note that the customer-based neighbourhood method does not have such a flexibility as the value of k is fixed to 1, 2 or 3, representing the number of critical points and also the source region is fixed being defined by the largest circle. Since VNS(FN) is found to be the best performer, the learning process is carried out using this variant only.Here, we use β = 0.25 for simplicity. We observe VNS(FN) behaviour by recording the information mentioned above.As the chosen facility is found by dynamically changing the radius of the covering circle CCk'; k' = 1, …, p, the level k' is identified whenever a better solution is found. In other words, if there is an improvement at a given CCk', the frequency of using such a level will be increased by one.We record the number of times the solution is improved using the kth neighbourhood structure; k = 1, …, Kmax (k facilities are removed and inserted somewhere else according to our previous strategies). Furthermore, as part of the process we also derive Kmax accordingly.The information that is recorded in the first phase (the value of k'; k' = 1, …, p and the value of k; k = 1, …, Kmax) is then used to guide the search in VNS(FN). Two schemes are explored:As the size of the covering circle is dynamic, we would like to determine the maximum level that has achieved improvement. The same idea is also applied to fix the range for the value of k, i.e. [a, b]. Note that in the classical VNS, a =1 and b = Kmax whereas here though a = 1,  b is not necessarily Kmax. However, in some cases, it was observed that the values of k' and k can be further away from their respective means than what is deemed reasonable (outliers), those that lie beyond the mean+2 standard deviations. Therefore, such outliers are excluded from our analysis.A preliminarystudy shows that this method has two weaknesses: (i) there is a possibility that some levels within the range did not improve the solution leading to a waste of time in exploring these levels, and (ii) the probabilities of using each level is considered to be the same, meaning that all levels have the same level of importance. It was however observed that some levels improve the solution several times, while others only a few times or none. These two weaknesses also occur in determining the k values. The next scheme attempts to overcome these two weak points.The idea is to choose α ∈ (0, 1) uniformly and computeL⌢=F−1(α)withF(L)=∑t=1LProb(t)where Prob(t) refers to the probability of choosing the tth level (t = 1, …, p) or the tth neighbourhood (t = 1, …, Kmax) and is computed using (5). In other words, the higher the probability of a given level or neighbourhood is, the higher the chance that such level or neighbourhood will be chosen. Fig. 12illustrates how such a scheme can be used.This technique is also referred to, in the literature, as the inverse method. This method is more adaptive as both the values of k and k' are pseudo-randomly selected.A preliminary experiment using both schemes on a TSP data set with n =439 and p varying from 10 to 100 in steps of 10 is given in Table 5. The results based on 10 runs show that applying this scheme is more efficient than the range-based. For instance, the overall average deviations for the best results are 0.80 percent and 1.15 percent, with the average results being 1.96 percent and 2.65 percent. The ST Deviation values of 6.46 and 3.17 of schemes 1 and 2 respectively, also confirm that the frequency-based scheme is more reliable especially for large values of p (e.g.; p ≥ 30).The proposed heuristics are coded in C++ and run on a PC computer with an Intel Core 2 Duo processor, 2.0 gigahertz CPU and 4G memory. For the optimal solution of the discrete case, an integrated C++ code, with CPLEX incorporated within it, is used and run in the same computer. Our enhancements are used to test the following existing data sets (n = 439, 575, 783, 1002 and 1323 TSP-Lib) with various values of p (p = 10–100 with an increment of 10). For n = 439, we compare the computational results of our VNS based approaches to the optimal solutions provided by Chen and Chen (2009). For the other larger data sets no optimal solutions are available. To assess the performance of our approach, we used an efficient implementation of the set covering-based approach that optimally solves the vertex p-centre problem, as will be explained later. The optimal discrete solutions are then refined in the continuous space by applying the same local search as described in this paper. We run the multi-start procedure 10,000 iterations and select the best solution. For consistency, we use the corresponding CPU time for the multi start as a stopping criterion in our VNS methods.For simplicity and ease of repeatability, the initial solution in our VNS-based heuristics is taken as the solution of the multi-start algorithm with 100 runs. In Table 6, the results for VNS(CN) and VNS(FN) with and without memory are reported. Our experiments show that both VNS heuristics (CN and FN) produce better results than the multi-start heuristic as well as the optimal solution based on the discrete case. In brief, the performance of VNS(CN) was slightly inferior to the VNS(FN) memoryless as the overall average deviation values from the optimal solutions are 0.429 percent and 0.362 percent respectively. It can be seen that VNS(FN) with memory is more effective, as the overall deviation has been reduced to 0.233 percent.The optimal solutions are found by Chen and Chen (2009) who used an interesting relaxation method based on solving a succession of small sub-problems. The authors add a number of demand points each time the obtained optimal solution of the sub problem happens to be not feasible for the entire problem. For simplicity, in our subsequent tables, we refer to these values by k as reported in their paper.Four larger datasets (n =575, 783, 1002 and 1323 TSP-Lib) are used to assess the performance of our enhancements, see Table 7. As no optimal solution is available for these cases, we compute the deviation from the best solution as Deviation (percent)=100(ZH − Zbest)/Zbest with ZH denotes the Z value found by heuristic ‘H’ and Zbest refers to the best value of Z found by the heuristics. To provide additional comparisons, two strategies for generating the initial solution are proposed:(a)The solution of the multi-start procedure with 100 runs – Here, we use the solution of the multi-start algorithm with 100 runs, as previously shown in Table 6.The optimal solution of the vertex-centre problem – The idea here is to determine the optimal solution of the vertex p-centre problem as a starting point using the set covering-based approach mentioned earlier.In general, Table 7 shows that the performance of VNS(FN) with memory namely VNS-M outperforms all the others, yielding 34 best solutions in total, (i.e., 20 obtained using the solution of the multi-start and 14 with the optimal solution of the discrete problem). The CN-based approach achieved the best solution 13 times using the multi-start and 4 times with the discrete case (17 in total).The memoryless VNS(FN) obtained 7 and 3 times the best out of 40 for strategies (a) and (b) respectively. It can also be observed that the optimal solution based on the discrete case fails to find even one best solution, while the multi-start algorithm (10,000 runs) achieved the best solution only once. In addition, the average deviation values also confirm that the performance of VNS(FN) with memory always yield relatively better results than those of the other enhancements, with an overall average deviation of 0.538 percentand 2.716 percent when using the solution of the multi-start and the optimal discrete solution respectively. These compare favourably with (0.731 percent, 7.657 percent) and (1.314 percent, 3.378 percent)) for VNS(CN) and VNS(FN) without memory respectively. Note that when p =40, 50 and 60 with n =783 there was no remaining time to run the VNS when the optimal discrete solutions was used as the initial solutions on its own consumed more time than required by the multi start.In brief, we can confirm that the performance of the VNS(CN) is better than the VNS(FN) without memory, but the incorporation of learning into the search has made VNS(FN) with memory to be the best performer.A comparison between the average total CPU time of the multi-start algorithm (10,000 iterations) and the average CPU time when the best continuous solution is found for both cases (using the solution of the multi-start procedure with 100 runs and the optimal discrete solutions as the initial solutions) as well as Chen and Chen's results (when it is available) is presented in Table 8. It is worth noting that the recording of when the best solution is obtained could be useful in designing a more advanced stopping rule. To achieve this, we record the CPU time when the best solution is found by a given heuristic as TH and compute the deviation from the CPU time required for 10,000 iterations of the multi-start algorithm which we refer to as TMS. Deviation is computed as follows:Deviation(percent)=100×(TH−TMS)TMSTo provide a fair comparison in terms of CPU, we use the following transformation as given by Dongarra (2013) with T2 = T1(n1/n2) where T1 represents the reported time in Machine 1 and T2 the estimated time in Machine 2. n1 and n2 refer to the number of Mflops in Machines 1 and 2, respectively. For more information, see http://www.roylongbottom.org.uk. As the computer by Chen and Chen (2009) cannot be easily identified for the number of Mflops, we provide an approximate time using a slightly slower but similar computernamely a PC Intel Pentium 4 (3.06 gigahertz), 2 gigabytes of main memory.Table 8 shows that the overall deviations of CPU time when the best solution is found to increase with n for all the algorithms. For instance, in VNS(CN), when using the solution of the multi-start (100 runs) as the initial solution, the overall deviations vary from nearly −82 percent for n = 439 to nearly −40 percent for n = 783.In general, it can be seen that applying VNS(FN) and VNS(CN) require around 50 percent of the time required by the multi start algorithm.In this section we intensify the local search within VNS which is based on the allocation procedure ‘ALLOC’ of Fig. 7. We achieve this by exploring not only one critical point of the largest circle at a time but also all the 3 pairs of critical points as well as all the three critical points simultaneously.ALLOC is based on the following neighbourhood structure N1(X) which is the removal of one critical point l ∈ CP1 and inserting it in the best region in Vl.Here we extend this to cater for N2(X) and N3(X) to define the neighbourhoods that simultaneously remove all the possible two critical points (three pairs) and the full triplet (three critical points) from the largest circle. This is performed by solving the corresponding 1-centre problem (Step 2 of ALLOC) on the reduced largest circle and allocating these critical points in their respectiveVl,l∈CP1(Step 3 where the construction of Vlis carried out and Step 4 where the choice is made). Steps 1 and 5 of ALLOC remain unchanged. In other words instead to choose the best move from at most three cases we now intensify the search by evaluating at most seven cases.We incorporate the above VND with the three neighbourhood structures N1, N2 and N3 as our local search in Step 2c in both VNS(CN) and in our best variant of VNS(FN) namely VNS-M.The same data sets and the same stopping criterion that were used in the previous computational results are also applied here. The overall results of the best variants investigated in this study are given in Table 9. Note that the results obtained by those methods that are found to be inferior and dominated by at least another method, such as the multi-start for instance, are not reported in this summary table. In particular, the results obtained by VND that starts from the optimal solution of the discrete problem are not reported as the results are almost always inferior. The new results are very competitive as eight new best results were discovered (six based on VNS(CN) and two with VNS-M) excluding nine other already found best solutions. Also, the new variants are found to be better suited for the very large instance where an average of slightly less than 0.7 percent is achieved. In addition, very encouraging overall average deviations of 0.629 percent and 0.839 percent are recorded by VNS(CN) and VNS-M respectively which compare favourably against the best results found by VNS-M (i.e., 0.613 percent). This slight deterioration can be due to the time spent in VND which obviously slightly limit the exploration of VNS given that the same computational time of 10,000 multi-starts is used.For completeness we also provide the optimal solutions for the vertex p-centre problem for these large instances using the set covering-based approach, see Table 10. This is based on Daskin (1995) algorithm but incorporates an efficient data structure for sorting the useful elements of the distance matrix that are used during the search (see Al-Khedhairi &#38; Salhi, 2005) besides starting with tighter initial upper and lower bounds as suggested by Salhi and Al-Khedhairi (2010). This basic enhancement speeds up the convergence considerably as it considers the existing distance elements of the distance matrix only as well as it identifies empty gaps between successive distances including the final empty gap between the last upper and lower bounds.This is also relatively more efficient than the Chen–Chen algorithm which is sensitive to the number of demand points added (the k value in their paper). For instance, when n =1323 the average CPU time are 227.27 and 1188.83 seconds respectively. The current approach also reduces the number of Cplex calls by almost half, which is significant when compared to the original implementation of Daskin (1995).A VNS-based approach is designed to solve the p-centre problem on the plane which seems to have not attracted as many researchers as one may wish especially for larger instances. A local search which is similar to Cooper's algorithm is used. Enhancements on the well known Elzinga–Hearn algorithm for the 1-centre problem are presented that produced nearly 60 percent reduction in CPU time. This is then embedded as part of the local search in VNS for solving the p-centre problem. Two modifications are proposed in our local search (allocation phase) as well as new neighbourhood structures designed for this particular location problem. The idea of incorporating learning within the search on the best enhancement namely VNS(FN) proved to be very effective. This VNS with memory can be considered as a new adaptive VNS variant which can be easily implemented in many other combinatorial and global optimisation problems. To intensify the search within the VNS, a VND type mechanism is embedded into the local search which generated interesting new best results. The multi-start procedure is adopted for comparison purposes and its computing time used as a basis. The optimal solution of the vertex p-centre problem (the discrete case) is also found using an efficient implementation of the set covering based approach which is then refined for the continuous space by the same local search. Four TSP data sets with n =439, 575, 783, 1002 and p varying from p= 10–100 with a step of 10 are used as a platform to test our methodology. To our knowledge, this is the first time such larger instances were attempted. In summary, the VNS that uses facility-based and memory proved to be the best performer and the most robust when compared to the other methods.For future research, it may be interesting to incorporate into our approach the optimal method given by Drezner (1984a) for the case of p = 2, whenever two clusters are considered worth solving optimally. In this study, we explored the removal of the non-promising facilities by identifying those facilities that serve the critical fixed points only, this can be relaxed to also consider those facilities that also have, in addition to the critical point, a small number of non-critical fixed points. Other related location problems with different objective function such as the Min Max Sum or other types of covering (maximum or partial covering on the plane) can also be investigated using our methodology. The methodology can also be extended to cater for area coverage which may or may not be convex as attempted by Wei et al. (2006). The use of memory within VNS, or in any other meta-heuristic that relies on certain parameters or on the sequence in which certain moves are implemented, could, in our view, be a challenging but a promising research avenue that is worthwhile exploring.

@&#CONCLUSIONS@&#
