@&#MAIN-TITLE@&#
Automatic thematic classification of election manifestos

@&#HIGHLIGHTS@&#
We digitized political texts from the 1980s and 1990s.We used these data to learn a classifier that can label more recent political texts.Change of themes over the years affects recall of the learned classifier.But precision is comparable to the precision obtained by a human expert labeller.For political themes, a high level of detail seems to be preferred by domain experts.

@&#KEYPHRASES@&#
Text classification,Political data,Expert evaluation,

@&#ABSTRACT@&#
We digitized three years of Dutch election manifestos annotated by the Dutch political scientist Isaac Lipschits. We used these data to train a classifier that can automatically label new, unseen election manifestos with themes. Having the manifestos in a uniform XML format with all paragraphs annotated with their themes has advantages for both electronic publishing of the data and diachronic comparative data analysis. The data that we created will be disclosed to the public through a search interface. This means that it will be possible to query the data and filter them on themes and parties. We optimized the Lipschits classifier on the task of classifying election manifestos using models trained on earlier years. We built a classifier that is suited for classifying election manifestos from 2002 onwards using the data from the 1980s and 1990s. We evaluated the results by having a domain expert manually assess a sample of the classified data. We found that our automatic classifier obtains the same precision as a human classifier on unseen data. Its recall could be improved by extending the set of themes with newly emerged themes. Thus when using old political texts to classify new texts, work is needed to link and expand the set of themes to newer topics.

@&#INTRODUCTION@&#
Isaac Lipschits (1930–2008) was a Dutch historian and political scientist. One of his works is an annotated collection of election manifestos (party programmes) for the Dutch elections between 1977–1998 (Lipschits, 1977). For each election year he compiled a book with the manifestos published by all parties that participated in that year’s elections. This book was then made available for purchase nationally before the election. To facilitate voters’ decision-making process when comparing parties on election issues, Lipschits manually labelled the manifestos with themes: he segmented the manifestos into coherent text fragments, gave them a unique identifier consisting of the party’s acronym and a number, and added an index of themes in the back of the book referring to these identifiers.In the Political Mashup project (Marx, 2009), Dutch political data from 1814 onwards is being digitized and indexed. The aim of the project is to bring together political information produced by political parties and information on the reception of political promises and actions, in the news as well as in user–generated content. The data are not only digitized and integrated but also disclosed to the public. This means that it will be possible to query the data and filter them on themes, persons and events. In traditional historical and political research, scientists who work with textual data have to open each potentially relevant document or book separately, and search and browse through them using a static register. This requires a vast amount of manual analysis and time. The goal of Political Mashup is to automate part of this manual effort.Election manifestos are traditionally considered to be a key source of information on the ideological stance of a political party on election issues (Budge, Klingemann, Volkens, Bara, & Tanenbaum, 2001). The annotated election manifestos by Isaac Lipschits are part of the Political Mashup data. The aims of the work presented in the current paper are: (1) to digitize the 1977–1998 Lipschits collections and (2) to build an automatic classifier for more recent, unclassified election manifestos. The starting points for our work are the Lipschits books, scanned as PDF files.The challenges we face in digitizing the data and building the classifier are:•the PDFs contain complicating OCR errors;the text fragments have been assigned multiple themes by Lipschits (more than 6 on average), which resulted in a large number of themes (more than 200) in total, for a relatively small corpus;there are inconsistencies in the labelling schemes over time (e.g. ‘fishing industry’ vs. ‘fishing industry policy’)1For the convenience of the reader, we translated all example themes from Dutch to English throughout the paper, except for places where the literal orthography is important.1;new themes have emerged over time (e.g. ‘information technology’) and the same themes may have changing content over time (concept drift);automatic text segmentation leads to shorter and much more segments than in the manually segmented training data.We took the following approach: We first converted the scanned PDFs to XML data in which each text fragment has been annotated with the Lipschits themes (Section 3). We then used these data to train an automatic classifier, using the original segmentation and labelling by Lipschits (Section 4). We optimized the classifier on the task of classifying election manifestos using models trained on earlier years (Section 5). Based on these experiments, we built a classifier that is suited for classifying election manifestos from 2002 onwards using the data from the 1980s and 1990s. We then evaluated the results by having a domain expert manually assess a sample of the classified data (Section 6).We found that our automatic classifier obtains the same precision as a human classifier on unseen data. Its recall could be improved by extending the set of themes with newly emerged themes. In addition, we found that although a smaller theme set could improve classification scores even more, a more fine-grained classification is preferred by domain experts.Automatic text classification (or document categorization) in predefined categories on the basis of pre-categorized examples is a supervised machine learning task (Sebastiani, 2002). If a text in the collection has been assigned to more than one category, the task is called multi-label text classification. In multi-label text classification it is sensible for the classifier to rank the classes according to their estimated relevance to the text. Like any supervised machine learning task, text categorization requires a collection of (manually) labelled examples as training material. When training a classification model, each text in the collection is represented as a feature vector. The features are the terms in the collection, where terms can be usually read as words, although many attempts have been made to extend words with n-grams or phrases (Sebastiani, 2002). Considering each word in the corpus as an independent feature makes text categorization a classification problem in a high-dimensional feature space. The most widely used algorithms for text classification are Support Vector Machines (SVMs) and Naïve Bayes (NB), although NB is mostly considered as a baseline for more sophisticated techniques. Van Mun (1999) argues that in domains with large amounts of features it is better to use Balanced Winnow (Dagan, Karov, & Roth, 1997; Littlestone, 1988) because of its ability to discard irrelevant features. In Section 4, we describe the Winnow classifier that we used for training the Lipschits classifier in more detail.Automatic classification of political texts using supervised learning techniques has been applied to legislative texts, parliamentary documents, manifestos, and even speeches of the Dutch Queen (Breeman et al., 2009; Hillard, Purpura, & Wilkerson, 2008; Louwerse, 2011; Purpura & Hillard, 2006). Developers of classifiers for topical classification of political texts are faced with two challenges: (1) the definition of political topics is often fine-grained, resulting in a large set of topics and (2) the set of topics and the content of these topics is not stable over time (Mourão et al., 2008). The research council of the European Union has trained a multi-label classifier on EU legislation labelled with the official EU EUROVOC thesaurus consisting of more than 7000 classes (Pouliquen, Steinberger, & Ignat, 2003). The classifier proposes a ranked list of classes for each input document. Their JEX system reaches an R-precision of 0.56 for English documents. On average, documents are labelled with six classes, so this means that roughly three from the top 6 proposed classes are correct. Both the classifier and the training data have recently been made available (Steinberger, Ebrahim, & Turchi, 2012). Hillard, Purpura, and Wilkerson (2007) evaluate the efficacy and accuracy of automatic text classification using a corpus of all federal public bills introduced in the U.S. since 1947. The bills have been labelled according to a hierarchical classification scheme comprising 20 major policy topics and 226 subtopics. The authors are able to build a classifier that achieves levels of accuracy comparable to humans (measured as inter-rater agreement: Cohen’sκ=0.8at the subtopic level), while reducing human labor effort by about 80%. An important difference with our work is that the topics in their classification scheme are mutually exclusive; thus, they train a classifier that assigns one topic per document, while the Lipschits data require a multi-label classification approach.A related task to political theme classification is political opinion classification. In this task the goal is to correctly sort political texts depending on whether they support or oppose a given political issue under discussion (Awadallah, Ramanath, & Weikum, 2010; Yu, Kaufmann, & Diermeier, 2008). Political opinion classification is related to sentiment classification but political texts tend to contain few sentiment-bearing words. Hirst, Riabinin, and Graham (2010) aim to classify Canadian legislative speech by party. They initially succeed in this task but an analysis of the discriminative terms show that their classifier not distinguishes between the ideologies of the parties but the roles of the party (government or opposition). More recently, Diermeier, Godbout, Yu, and Kaufmann (2012) succeeded in predicting a senator’s ideological position with 92% accurracy based on terms associated with conservative and liberal ideologies. They conclude their paper with the wish to be able to investigate the content shift of ideologies over time. Yu et al. (2008) use U.S. senate speeches as documents and the speakers’ party affiliation as classification labels. They formulate a number of recommendations for text classification in the political domain: First, due to the subjectivity of the class definitions, human classifiers may not agree on the correct labelling. Second, manual labelling is sensitive to errors. Third, the data may not be stable over time: in political agendas, the relevant issues change over time and new themes can emerge in new data. Fourth, the data is not independent: in the case of political speeches, speakers respond to each other, adopting themes and vocabulary from each other. The fifth problem is the sample size bias in the training data, especially in the case of small data sets. In the development of the Lipschits classifier, we face the first three challenges.Closely related to thematic text classification is topic identification in political texts. Quinn, Monroe, Colaresi, Crespin, and Radev (2010) develop a statistical learning model that uses word choices to infer topical categories covered in a set of speeches and to identify the topic of specific speeches. Their method is unsupervised: they do not use a list of predefined topics, and they do not need a set of human-labelled examples. They use the topic model to examine the agenda in the U.S. Senate from 1997 to 2004. From the topics that are identified by their algorithm, they manually infer category labels and they create a hierarchical topic scheme.Some work has focused on Dutch political data: Jijkoun, Marx, De Rijke, and van Waveren (2007) describe an electoral search engine aimed at helping the general public to make an informed decision whom to vote for in the 2006 elections. They indexed news articles, blogs and election manifestos, and built a search engine in which the users can search by theme or free text. In this work, 179 themes were predefined by the Instituut voor Publiek en Politiek (IPP).2Current name: Prodemos http://www.prodemos.nl/.2Of these, 175 were used as thematic queries by users of the website. In total, 28 thousand thematic searches were performed compared to 117 thousand free text searches. Although the search results are not evaluated in the paper, these statistics show the potential value of thematic labelling of political texts for application in a search system.Gielissen and Marx (2009) describe the development of PoliDocs.nl, a Web Information System for the disclosure of Dutch parliamentary publications. They transformed long PDF documents containing the meeting notes of one day into a uniform XML format and make them available in a new Web Information System. The authors present a list of 15 requirements for the Information System that were formulated by end-users through a collective Dutch weblog. Most requirements are technological in nature. Only one is related to content: “Users should be able to display the results of a keyword search on a timeline where parliamentary papers that belong together are grouped”. This requirement indicates that for end users of a search system for political data, time aspects are important, and grouping of documents (by party or theme) is necessary. Gielissen and Marx (2009) developed a faceted search engine for PoliDocs. In faceted search, the user can filter for categories in addition to search with keywords to find documents. In PoliDocs, it is possible to filter documents on the basis of personal names, year and political party.Kaptein and Marx (2010) use transcripts of meetings of the Dutch parliament for experiments with focused retrieval and result aggregation. They evaluate the above-described PoliDocs.nl interface. They find that users performed thematic search tasks up to eight times faster using focused retrieval and faceted search compared to standard full document retrieval. They stress the importance of time- and party-specific information when searching for themes in political data: for users, “it is interesting to see how that theme developed over time and which political parties claimed that theme” (p. 425).As we have mentioned above, an important aspect of political data is temporal change. Forman (2006) discusses the problem of topic drift in classifying news data. The authors learn a new classifier for each day, and they augment the bag-of-words feature vector with additional binary features that are generated by the predictions previous daily classifiers would have made for today’s cases. Mourão et al. (2008) show evidence that time should be taken into consideration in classification techniques and algorithms. According to Rocha, Mourão, Pereira, Gonçalves, and Meira (2008), the performance of classifiers is affected by three different temporal effects: class distribution, term distribution and class similarity. They try to find the selection of training data that optimizes the trade-off between the sampling effect (more training data gives better classification results) and the time effects (training data from mismatching time moments give worse classification results). They evaluate their approach on scientific articles. The trade-off between training set size and temporal match is also relevant for our data, but the difference with scientific literature is that the election manifestos originate from a few moments in history (election years: 1977, 1981, 1986, etc.), while scientific literature and news can be dated and timed at finer granularity levels. In addition, there is much more scientific literature available to train a classifier than there are annotated political texts.Each Lipschits book contains all election manifestos of the policitical parties that officially participated in that year’s election. The body of data are the manifesto texts, which have been manually segmented by Lipschits. Each segment has been labelled with a number. The numbering starts at 1 for each party. We call these number labels ‘topic numbers’. At the back of the book is a register of alphabetically sorted themes. For each theme, Lipschits has listed the associated parts of the election manifestos per party (e.g. CD, CDA, D66, etc.) by topic number. See Fig. 1a for an example of the register.The PDFs were converted to XML using pdftohtml with the -xml option. Both the body of the election manifestos and the register needed a substantial amount of clean-up before the data could be published digitally, or used for classification. We used a Perl script for textual clean-up; the process is described in the next two subsections.In the election manifesto texts, the start of each new topic is labelled with a topic number. In the source PDF, topic numbers are preceded by a bullet, so the task of passage segmentation can be defined as: finding a bullet followed by a number, and then saving all text between this number and the following bullet as content belonging to the topic number. However, we had to take into account a number of problems. First, during the scanning process the edge of the right page sometimes became part of the scan of the left page, so for a number of pages additional noisy text fragments (sometimes containing a bullet) were part of the XML data. Another scanning problem was that a few pages were mistakenly not scanned, as a result of which some topic numbers were missing.In addition, we came across numerous OCR problems. We included a list of frequently recurring OCR corrections (such as ‘WO’ for the party name ‘VVD’) in our clean-up script. Some bullets were not recognized, or a bullet was recognized where there was none. Moreover, numbers behind bullets were not always recognized (or recognized as letters e.g. ‘l’ for ‘1’). This was further complicated by inconsistencies in text formatting between party programmes: sometimes bullets were used for lists within a topic (no new topic starts at the bullet); in other cases numbers were used for lists within a topic (no new topic starts at the number). We solved these problems by writing a set of regular expression patterns that try to find and reconstruct all topic numbers for each party, using the presence of bullets and expectations on the following number based on the preceding number.In order to assess the quality of the passage segmentation, we counted (1) the number of automatically identified passages and (2) the number of automatically identified text passages that start with a number (indicating correct segmentation). The results are in Table 1.When processing the theme register in the back of the book, we dealt with two challenges. First, we had to decide which subsequent lines should be concatenated and which not. For example, in Fig. 1, the second line should be concatenated to the first line and the line starting with “118, 121” should be concatenated to the line before it. An additional problem was that the OCR had sometimes incorrectly recognized column splits between party names and number sequences. For example, in Fig. 1b, the three last lines should be concatenated to the three lines preceding them. This even gets more complicating when trailing number lines (such as the line starting with “118, 121”) are intertwined with incorrect column splits.Manual cleanup was not an option because the total number of index lines per year was around 5000. We solved the decision problem of concatenating subsequent lines using a set of heuristics that takes into account punctuation, a set of likely topic names (extracted from the register itself), the set of party names, and a regular expression that matched sequences of numbers with possible OCR errors.3[ISBlOa0-9,. ∼-]∗[0-9][ISBlOa0-9,. ∼-]∗.3We solved incorrect column splits using two parallel stacks of party names and number sequences, pairing them up while keeping track of incomplete pairs. We checked the soundness of the converted theme registers by assuring that there are no remaining party names that are not followed by a number sequence and no number sequences that are not preceded by a party name.Using the procedure described above, we were able to convert all texts from the election manifestos of 1998, 1994 and 1986 to annotated XML files of acceptable quality. The result are three XML files in which each text fragment has been annotated with the themes from the register. For the 1981 manifestos, the quality of the scans was too poor. The 1977 and 1989 manifestos use another encoding of topic numbers to text fragments, leading to new and more severe OCR problems. Therefore, we did not convert these years to annotated XML files yet.Having the manifestos in a uniform XML format with all paragraphs annotated with their themes has advantages for both electronic publishing of the data and diachronic comparative data analysis. We briefly discuss these two aspects here. Note that both training data and automatically classified manifestos are stored in the same XML format. With a simple XQuery we can create the input data (in e.g. SPSS or CSV format) for performing diachronic comparative saliency analysis of the parties Laver (2001): i.e., how much attention gives each party to each theme (election topic) and how does that change over the years?Using XSLT and CSS stylesheets we can publish the manifestos as hyper documents with a number of additional features not present in printed or scanned format. First, next to creating an inverted (back of the book) theme-index with hyperlinks to the paragraphs, we can create such an inverted theme-index for each manisfesto, as an alternative to a table of contents for that manifesto. This index also provides a good impression on the saliency of the themes of that party in that election year. An attractive presentation of this hyperlinked index can be given as a dispersion matrix (Bird, Klein, & Loper, 2009), with the themes ordered by their dispersion value. An example is presented in Fig. 2for the manifesto of the ‘pensioners party’ in 1998. We see that dispersion is a good indicator of saliency of a theme: half of the themes in the top 20 directly relate to themes relevant for elderly people. Second, we can ‘tag’ each paragraph with its themes, making it quickly clear what the paragraph is about. The tags are hyperlinked to the inverted index of all manifestos of all parties, or just to the inverted index of the specific party. Third, we may allow users to create ‘theme-views’ of a manifesto: only show the paragraphs labelled by that theme. This makes it easy to compare the standpoints of different parties on a specific theme, or diachronically compare one party on one theme.We have made the following data available for further research: the transformed manifestos in XML format for the years 1986, 1994 and 1998; the automatically labelled manifestos in XML for the years 2006, 2010 and 2012; the RelaxNG schema which validates these XML files; the XSLT stylesheet which transforms the XML into hyperlinked HTML documents as described above; the HTML output for all XML files; the XQuery creating a saliency matrix in CSV, and the output for the year 1998. The data is available at http://data.politicalmashup.nl/lipschits.We aim at developing a Lipschits classifier that can assign themes to unseen Dutch election manifestos that were written after Lipschits’ work, i.e. from 2002 onwards. Since we do not have manually labelled data from these recent years, we have to rely on the older data from the eighties and nineties for training and optimization of the classifier. Therefore, our initial experiments are directed at obtaining the best classification results for the 1998 manifestos, using the data from older years in the training phase. In Section 4.1, we summarize the statistics of our classification data. In Section 4.2, we specify the two classifiers that we use, Support Vector Machines and Balanced Winnow. In Section 4.3, we optimize their parameters, and in Section 4.5, we compare their performance on the classification task.As described in the previous section, we transformed the digitized Lipschits books to a collection of short texts with associated themes. See Table 2for the statistics per year. The table shows that although the texts are only a few hundred words long (317 on average, with a standard deviation of 252), Lipschits assigned more than six themes per text on average.The large number of themes assigned per text segment seems to suggest that Lipschits’ segmentation led to segments that are not coherent and may be too long. We looked into the possibility of splitting the texts in smaller segments, and classifying them separately, but we found that even within one sentence often more than one Lipschits theme occurs (see for example the text in Fig. 3). Furthermore, Lipschits intended his labeling to apply to entire paragraphs. The large number of themes assigned by Lipschits is not so much a result of long text segments (they are only 317 words on average) but of his very detailed classification system, and the political domain, where themes tend to be interrelated.The classification task we consider is a multi-label classification task, with each Lipschits theme being a class. We used the Linguistic Classification System LCS (Koster, Seutter, & Beney, 2003) for our experiments. The LCS is a generic system for the classification and routing of full-text documents.4See http://www.phasar.cs.ru.nl/LCS/.4The LCS trains a model (class profile) for each class in a one-versus-all setting (Van Mun, 1999; Koster & Seutter, 2003; Koster & Beney, 2007). In the test phase, the classifier runs a test document by each of the class profiles and assigns a score to each class, yielding a ranked list of classes for the document. The LCS performs text pre-processing in the form of tokenization and feature selection. For feature selection, we set the minimum document frequency to 2 and the minimum corpus frequency to 3. The LCS has one classifier built in: Balanced Winnow (Littlestone, 1988; Dagan et al., 1997), and it has an interface to the linearSVMlightimplementation by Joachims and Svmlight (1999).The Balanced Winnow algorithm learns two weights for each feature t and classc,Wt,c-andWt,c+. The difference betweenW+andW-is the effective weight of the feature. In the training phase, Balanced Winnow goes through the set of training examples in multiple iterations. The score of a document d for a class c is computed as(1)SCORE(c,d)=∑t∈dWt,c+-Wt,c-·s(t,d)wheres(t,d)is the strength of the term t in d (e.g. its tf-idf weight).In the original Winnow implementation (Littlestone, 1988), a document d is considered to be part of the class c ifSCORE(c,d)>θ, withθ=1. In Dagan et al. (1997)’s version of Balanced Winnow, the threshold is defined by two parameters: an upper boundθ+and a lower boundθ-. During training, the algorithm predicts 0 whenSCORE(c,d)<θ-and it predicts 1 whenSCORE(c,d)>θ+. All positive examples with a score belowθ-, all negative examples with score aboveθ+and all examples with scores in the range[θ-,θ+]are considered as misclassified. When a training document is misclassified, the feature weights are updated using two parameters: the promotion parameterα(α>1) and the demotion parameterβ(0<β<1). When a training document belonging to class c scores belowθfor c, the positive weightWt,c+of the active feature is multiplied byα(it is promoted) and the negative weightWt,c-is multiplied byβ(it is demoted). When a training document does not belong to class c but scores aboveθ, the positive weight of the active feature is demoted and the negative weight is promoted.In SVM, a hyperplane is constructed that separates the positive from the negative training examples in the feature space. The best hyperplane is the one that has the largest distance to the nearest training examples in both classes. The larger the margin of the hyperplane (the wider the gap between the two classes), the lower the generalization error of the classifier. In realistic data, there is no hyperplane that can completely separate the positive from the negative training examples. Cortes and Vapnik (1995) introduced the soft margin approach, in which a hyperplane is constructed that separates the training set with a minimal number of errors. This approach requires a training parameter c that controls the trade-off between the width of the margin and the number of classification errors made during training.In addition to the Winnow and SVM parameters, the LCS has three parameters that govern the selection of classes in the case of multi-label classification. For each text in the test set, the system returns a ranked list of all classes with the classification scores assigned to them. In order to decide which of the (top-ranked) classes should be assigned to the text, the set of selected classes is determined per text using three parameters: the maximum number of returned classes (maxranks), the minimum number of returned classes (minranks), and a threshold on the classification score.SVM’s linear kernal has one parameter c, the tradeoff between training error and margin. For Winnow, we optimized the values for the threshold parametersθ+andθ-and its weight update parametersαandβ. We also tuned the class selection parameters minranks, maxranks and threshold in the LCS.5We did not optimize the number of training iterations but kept it at 10 in all experiments.5For tuning the parameters, we randomly partitioned the 1998 data into 10 equally sized parts and took one of the parts as test set and the rest as training set. We chose F-score withβ=1(F1 for short, the harmonic mean of precision and recall) as optimization criterion. For the SVM parameter c, we followed the suggestion by Hsu, Chang, and Lin (2003) to test exponentially growing values of c, from2-13to213. We obtained the optimal F1-score forc=2.For the Winnow parametersα,β,θ+andθ-, we used a range of parameter values comparable to Koster and Beney (2007)6α∈{1.01,1.02,1.04,1.08,1.16,1.32,1.64};β∈{0.99,0.98,0.96,0.92,0.84,0.68,0.32};θ+∈{1,1.2,1.5,2,2.5,3,5};θ-∈{-1,-0.75,-0.5,-0.25,0,0.25,0.5,0.75,1}.6and we performed a full grid search. The resulting F1 landscape exhibits a lot of variation, with optimal values in divergent areas. The best scores appear to lie in the area of highαandθ+values. We choseα=1.08;β=0.84;θ+=5;θ-=-0.25as optimal parameter combination.With the optimal choice for the SVM and Winnow parameters we tuned the class selection parameters. We set minranks to 1 so that each text gets assigned at least one theme. In the case of Winnow, we used the natural threshold of 1 (like in Koster & Beney (2007)). For SVM, we searched the optimal threshold by varying its value from −2 to 2 in steps of 0.2. For maxranks, we searched over the values from 6 to 20 in steps of 2. For SVM, we observe large differences in precision scores when varying threshold and maxranks. Taking precision into account besides F1, the optimal parameter combination is a maxranks value of 14 and either a threshold of 0.6 (with a good F1 score but low precision) or a threshold of 1 (with lower F1 but much higher precision). For Winnow, the differences in F1 for the best scoring values of maxranks are small; maxranks values of 10, 12 and 14 all give reasonable results for precision and F1.As evaluation measures we use precision, recall and F1.(2)Precision=#ofthemesassignedtotextbyclassifierandexpert#ofthemesassignedtotextbyclassifier(3)Recal=#ofthemesassignedtotextbyclassifierandexpert#ofthemesassignedtotextbyexpert(4)F1=2·precision·recallprecision+recallWe will also report Mean Average Precision (MAP), which gives the quality of the generated ranking of the themes per text:(5)AveragePrecision=∑k=1n(P(k)×rel(k))nc,whereP(k)is the precision at rankk,nis the total number of assigned themes,ncis the number of correctly assigned themes7Note that in the original definition of Average Precision (Zhu, 2004),ncis the total number of relevant themes, but this assumes that the output is a full ranking of all classes. In that case, all relevant classes are present in the ranking. Since we do not evaluate the full ranking but a selection of classes (maximum 10 per document), we cannot evaluate the full ranking and thus definencas the number of correctly assigned themes.7andrel(k)is a function that equals 1 if the theme at rank k is a correctly assigned theme, and zero if it is a false hit. MAP is the mean of this score over all texts in the test set.We trained both SVM and Balanced Winnow with parameter settings as described in Section 4.3 on the data from 1986 and 1994 and evaluated the trained models on the data from 1998. The results are in Table 3. For SVM, we show the results for two threshold values because the threshold that gave an optimal F1 score led to a very low precision. For Winnow, we show the results for two values of maxranks that lead to a slightly different balance between precision and F1. A paired t-test on F-scores for individual texts(n=826)obtained by Winnow 1 and SVM 2 showed that Winnow gives significantly better results than SVM(P<0.001). The other SVM setting (SVM 1) gives an average F1-score that is very close to the F1-score obtained by Winnow (46.9% compared to 47.0%), but against at the expense of a much lower precision (49.3% compared to 70.5%). Considering the results in Table 3, we decided to continue our experiments with Balanced Winnow as classifier. Because the differences between maxranks=10 and maxranks=14 are small, we opt for the somewhat higher precision and MAP and set maxranks to 10.8Note that this means that recall can never be 100% since there are text segments with more than 10 themes. However, a higher value for maxranks will penalize the precision too much.8In all classification experiments, we used the texts from the 1998 manifestos as test data, and the 1986 and 1994 manifestos as training data. In all experiments, we used the optimized Winnow parameters that were found by tuning on a (held-out) subset of the 1998 data (Section 4.3).We first evaluated the use of the years 1986 and 1994 as training data, both separately and in combination. As a comparison, we performed 10-fold cross validation experiments on the 1998 data.9Cross validation is not possible for the data from 2002 onwards because we do not have labelled data for those years.9To give an indication of the complexity of the task, we also calculated the results for a weighted random guessing baseline (D’hondt, Verberne, Koster, & Boves, 2013). This was generated using a script that takes into account the theme distributions and theme frequency distributions in the training data. First, it processes the training set and fills two arrays:•X=(c1,c2,…cN), where N is the number of training examples andciis the number of themes for training example i.Y=(t1,…t1,t2…t2,…tM…tM), where M is the total number of theme occurrences in the training set and in which each themetjis listed as often as it occurs in the training set.For each exampleekin the test set, the script randomly takes a number x from X, and it randomly takes x times a themetjfrom Y, while ensuring that there is notjoccurs more than once forek. Then it creates the random labelling of the test set by assigning the labelstjto the test example.The results are in Table 4. Because not all themes from the 1998 texts are present in the 1986 or 1994 data, a classifier trained on the earlier years and evaluated on 1998 data can never obtain 100% recall. We calculated the maximum recall for each training set as follows. LetTtestbe the set of themes in the test data,Ttrain∩testthe set of themes that occur in both the train and the test data, andcount(t)the number of occurrences of theme t in either of the sets. Then the maximal obtainable recall is:(6)Recallmax=∑t∈(Ttrain∩Ttest)count(t)∑t∈Ttestcount(t)Table 4 shows that training on the data from 1994 gives much better results than training on the data from 1986. Adding the 1986 data to the 1994 data results in a slightly lower recall but an improvement in MAP. The lower recall is probably due to an increase of the number of themes in the training data, making the classification problem more complex. Because of changes in the theme set, adding a new year to the training data tends to add more confusion. For example, having both the themes ‘fishing industry’ from 1986 and ‘fishing industry policy’ from 1994, which cover the same topics but in different years, yields a more complex classification problem than having only one of them.Table 4 also shows that the experiments with the (1986 and) 1994 training data do not reach the level of the cross validation experiment. This means that the data from earlier years are only partly representative for the 1998 data. We investigated the effect of non-overlapping themes between the years: 42 themes from the 1998 data do not occur in the training data (e.g. ‘bed capacity’, ‘city transport’, ‘France’) and 143 themes in the training data (either from 1986 or 1994 or both) do not occur in the 1998 data (e.g. ‘Aids’, ‘property tax’).Of course, better results could be obtained if we would use the theme set from 1998 to train the classifier on. However, this is not realistic since in the future application of the classifier it should be able to classify texts from years for which the theme set is unknown. Since the theme set of the test year will be unknown, we not only aim at selecting the best training data but also the best theme set from the training data. We did an experiment in which we used the data from 1986 and 1994 to train on, but changed the theme set: all themes from 1986+1994 or only the 1994 themes. The results of this experiment are in Table 5. We also included the results for using the 1998 theme set (third row) as reference.The first row of Table 5 is the same as the fourth row in Table 4. The second row of Table 5 shows that by keeping only the themes from the most recent year in the training data we obtain a higher precision. The higher recall in the third row of Table 5 shows that non-overlapping themes are indeed a problem for classifying across years. We continue our experiments with the data from both 1986 and 1994 as training data, but using only the theme set from 1994.There is evidence that text classification can be improved by adding word bigrams (Braga, Monard, & Matsubara, 2009; Bekkerman & Allan, 2004), also for Dutch (Gaustad & Bouma, 2002, p. 11), although the success depends on the domain and corpus size (Sebastiani, 2002). We extracted all within-sentence bigrams from the texts in our corpus and added them to the bag of unigrams for each text. We also lemmatized the texts using the output of the Dutch morpho-syntactic analyzer Frog (Van den Bosch, Busser, Canisius, & Daelemans, 2007) and experimented with lemmatized unigrams and bigrams. Finally, we experimented with removing stopwords from the feature set. The results of classification experiments with the variants of the data are in Table 6.A paired t-test on F-scores for individual texts(n=826)showed that the only settings that gives significantly different results from the unigram baseline is unigrams+bigrams(P<0.0001)– yielding a lower average F-score than the baseline. Since none of the alternative text representations gives results above the unigram baseline, we decide to continue our experiments with unigrams only, wordforms rather than lemmas and without stopword removal.In the experiments presented thus far we have trained on a relatively large number of themes in a relatively small number of texts: 210 themes10This would have been 320 if we would have kept the 1986 classes.10occurring in 1718 texts (1986+1994 data). We investigated whether theme merging can be profitable for classification performance, because we saw that Lipschits chose to formulate distinct themes for very similar content (e.g. ‘Aruba’ and ‘Netherlands Antilles’). For the purpose of reducing the number of themes, we clustered similar themes together. For each theme in the training data, we created a subcorpus by concatenating all texts with this theme. Then we created a term vector with relative term frequencies for all (lemmatized) terms occurring in the theme subcorpus. For each pair of themes, we calculated the cosine similarity. For example, the similarity between ‘teachers’ and ‘school’ is 0.98 while the similarity between ‘teachers’ and ‘economic policies’ is 0.74, and the similarity between ‘teachers’ and ‘weapons’ is 0.26. We then clustered the themes using an agglomerative hierarchical clustering approach (Day & Edelsbrunner, 1984):1.The clustering process is initiated by making each theme its own cluster.Iteratively, the two clusters are selected that are the most similar following a complete-linkage strategy: The similarity between cluster A and cluster Bsim(A,B)is equal to the lowest similaritysim(a,b), where a is a theme in cluster A and b is a theme in cluster B.Stop when there are no cluster pairs with a similarity above a given threshold. As thresholds we tested a cosine similarity range of(1.0…0.2)in steps of 0.05.For each of the threshold values, we replaced the original themes by the merged themes in the example data (both the training and the test set). Then we performed a number of analyses on the resulting data in order to decide on the optimal similarity threshold for merging: (1) we evaluated the classification performance (in terms of F1) with the merged data when classifying the 1998 data; (2) we calculated the entropy of the theme distribution:(7)H(T′)=-∑t′∈T′p(t′)∗log(p(t′))in whichT′is the set of theme combinations that occur in the data (t′is one unique combination of themes, occurring for at least one text); and (3) we counted the proportion of themes on average attached to a text:(8)A(T)=∑dtd/tDin which D is the number of texts,tdis the number of themes on a text and t is the total number of themes in the set.The rationale behind (2) and (3) is to find the optimal level of granularity: On the one hand we would want a classification problem with low entropy, making the classification problem easier, and on the other hand we do not want to move too far from the original fine grained Lipschits classification: if in the extreme case some texts in the set are labelled with all themes, then this labelling makes no sense. Therefore we used the relative number of themes on a textA(T)as an additional criterion. Table 7shows the calculated statistics.Table 7 shows that the lowest entropy is obtained for a very low number of themes (4). Although this makes the classification problem easier (highestF1=58.7%), it is no longer a sensible labelling in the view of the original Lipschits classification. If we multiply entropy with the relative number of themes on a text, then we find the lowest (optimal)H∗Aat the similarity threshold of 0.85. At that point, the number of themes has been reduced from 210 to 120. Among these 120, there are 37 clusters of merged themes and 83 themes that were not merged. Examples of merged themes are:•development aid, international economic cooperation, European cooperation, human rights, United Nations, international monetary collaboration, Middle and Eastern Europe,banks, insurance companies, pension funds,urban renewal, housing, spatial planning, house ownership, municipal administration and policy, provincial administration and policy, recreation, land policy,Aruba, Netherlands Antilles.We see that conceptually similar themes are clustered for a similarity threshold of 0.85 on the 1994 theme set. After optimization of the 1994 theme set, we applied the theme merging to the 1998 theme set. We asked a domain expert, a political journalist, to give her opinion on three versions of the 1998 theme set:1.the set of 101 themes after merging similar themes with a threshold of 0.85 (optimum for the 1994 data);the set of 140 themes after merging similar themes with a threshold of 0.90 (a more conservative merging strategy);the set of 218 original Lipschits themes, without merging.According to our expert, it would be very difficult to judge the merged themes as relevant or irrelevant because some parts may be relevant and others may not. In addition, in the theme set resulting from the 0.85 threshold, some of the merged themes were noisy, such as ‘sports policy, crime prevention, law enforcement, police policy, addiction’. The theme set resulting from the 0.90 threshold was less noisy but still the expert found the combinations of themes too broad to judge. For example, she judged the combined ‘transport’ theme (‘car traffic, public transport, freight transport, airports and aviation, canal shipping, sea ports and sea shipping’) as irrelevant when the text was about airports. Therefore, we decided not to merge the themes from the 1998 set but keep the original, fine-grained Lipschits theme set.As a reference for the quality of the automatic classifier, we asked a domain expert to manually evaluate the labels assigned by Lipschits himself (for more details see Section 6). When presenting the domain expert with 50 randomly selected texts from the 1998 data, she thought they were automatically labelled. The quality of the manual classification can be seen as a reasonable upper bound for our classifier. The results are in Table 8.When we compare the two rows in Table 8, we see that the Lipschits assigned more themes per text than our automatic classifier, leading to a higher recall and F1 score, but that precision is almost equal.We built an automatic Lipschits classifier that can assign themes to unseen election manifestos. We trained the classifier on all available data from 1986, 1994 and 1998, but only kept the themes from the most recent year: 1998. We used a standard bag-of-words representation without lemmatization and removal of stopwords. We configured the classification system to assign a minimum of 1 and a maximum of 10 labels to each text fragment, with a Winnow threshold of 1.We obtained the unlabelled election manifestos from 2006, 2010 and 2012.11The files for 2006, 2010 and 2012 came from verkiezingskijker.nl (Jijkoun et al., 2007), a manifesto search tool made by Google for the 2010 elections, and www.watzegtmijnpartij.nl, respectively.11These manifestos were digitally born, published as PDF files, and automatically turned into HTML while trying to preserve the paragraph structure. The HTML files were manually checked on paragraph breaks and bad breaks were removed. The paragraphs in the unlabelled data are considerably shorter than the text fragments in the training data: around 40 words on average (in the older Lipschits data, multiple paragraphs were often brought together under one topic number). Statistics on the data from 2006 onwards are in Table 9.12We do not know why the 2010 corpus is much bigger than the 2006 and 2012 corpora.12We automatically labelled the texts using our Lipschits classifier. Then we asked a domain expert, a political journalist, to manually evaluate a sample of the automatic annotations. The assessment interface is shown in Fig. 3.We measured precision and recall by counting the number of assigned themes judged as relevant, the number of assigned themes judged as not relevant13The option ‘don’t know’ was selected by the expert only in two instances. These were counted as irrelevant for the presented text.13and the number of themes that the expert typed in the field for missing themes:(9)P=#assignedandrelevant(#assignedandrelevant+#assignedandwrong)and(10)R=#assignedandrelevant(#assignedandrelevant+#missing)

@&#CONCLUSIONS@&#
We digitized three years of Dutch election manifestos annotated by the Dutch political scientist Isaac Lipschits: 2574 short (∼300-words) texts that Lipschits labelled. There are more than six political themes per text on average. We used these data to train a classifier that can automatically label new, unseen election manifestos with themes.The general conclusions that we draw from this paper are (1) in the thematic classification of political texts, a high level of detail seems to be preferred by domain experts; (2) change of themes over the years affects recall of the learned classifier, but (3) precision is comparable to the precision obtained by a human expert labeller. Thus when using old political texts to classify new texts, work is needed to link and expand the set of themes to newer topics.In future work, we will create a faceted search interface (using the themes as facets) for the digitized election manifestos, so that our work can be used by interested researchers, students and journalists. It may be interesting to investigate the trade-off between false hits (precision) and false misses (recall) in the context of faceted search. A second direction of future work is to investigate approaches to expand a political theme set with recently emerged themes. This could be done using a topic identification method (Quinn et al., 2010) on contemporary political texts.