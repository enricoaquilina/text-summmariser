@&#MAIN-TITLE@&#
Systematic mapping study on granular computing

@&#HIGHLIGHTS@&#
This paper discusses on the research on granular computing from 2012 to 2014.We defined four perspectives of classification schemes to map the selected studies.A total of 112 relevant published articles in established journals have been studied.39% of the relevant articles belong to a rough set framework.Fuzzy-GrC algorithm has outperformed other clustering algorithms in experiments.

@&#KEYPHRASES@&#
Granular computing,Granular classifier,Clustering algorithm,Systematic mapping,DBSCAN,k-means,c-means,

@&#ABSTRACT@&#
Granular computing has attracted many researchers as a new and rapidly growing paradigm of information processing. In this paper, we apply systematic mapping study to classify the granular computing researches to discover relative derivations to specify its research strength and quality. Our search scope is limited to the Science Direct and IEEE Transactions papers published between January 2012 and August 2014. We defined four perspectives of classification schemes to map the selected studies that are focus area, contribution type, research type and framework. Results of mapping the selected studies show that almost half of the research focused area belongs to category of data analysis. In addition, most of the selected papers belong to proposing the solutions in research type scheme. Distribution of papers between tool, method and enhancement categories of contribution type are almost equal. Moreover, 39% of the relevant papers belong to the rough set framework. The results show that there is little attention paid to cluster analysis in existing frameworks to discover granules for classification. We applied five clustering algorithms on three datasets from UCI repository to compare the form of information granules, and then classify the patterns and define them to a specific class based on their geometry and belongings. The clustering algorithms are DBSCAN, c-means, k-means, GAk-means and Fuzzy-GrC and the comparison of information granules are based on the coverage, misclassification and accuracy. The survey of experimental results mostly shows Fuzzy-GrC and GAk-means algorithm superior to other clustering algorithms; while, c-means clustering algorithm shows inferior to other clustering algorithms.

@&#INTRODUCTION@&#
Granular computing (GrC) proposed by Zadeh [1,2] and Lin [3] has been emerged as a unified and coherent platform of constructing, describing, and processing information granules. It is growing rapidly as a new paradigm of information processing, and attracting many practitioners and researchers. It can cover any methodologies, tools, theories and techniques to solve complex problems using information granules [4]. Although the term GrC is relatively new [5], the idea and concept related to GrC are not new [4]. GrC identifies the essential commonalities between the surprisingly diversified problems and technologies used there, which could be cast into a unified framework known as a granular world. The outcomes of GrC is achieved through the interaction of information granules and the external world that could be another granular or numeric world by collecting necessary granular information [6]. Granule is a clump of objects drawn together by indistinguishability, similarity or functionality [7]. Granules and their relationships are employed to find solutions to any desired problems. Different granularity levels, which are size of the information granules, are formed by decomposing and composing of granules. GrC is a simple classical view of concepts related to the granules notion. Different concept or rule levels are unrevealed based on defined granularity levels [8]. GrC has become an effective framework in design and implementation of efficient and intelligent information processing systems for various real life decision-making applications [7].Several reasoning formalisms has been applied on GrC such as interval mathematics [9,10], fuzzy sets [6,11,12], rough sets [13,14], cluster analysis [15,16] and hybrids models [17,18]. It has been revealed in various research areas such as image segmentation [19,20], data analysis [13,21], and granular data interaction [22]. It has been also conjunct with learning algorithms [15,17] to form the information granules. The three following criteria align the GrC [4]: (i) Information granules are used as the important constituents of knowledge processing and representation [23,24]. (ii) The size of the information granules (the level of granularity of information granules) is an essential factor to define the overall strategy of solving the problem [25]. The main shortcoming of the GrC is to find the level of granularity of information, which is assigned based on the patterns density in the universe [4]. (iii) Hierarchy of information granules supports an important aspect of perception of phenomena and delivers a tangible way of dealing with complexity by focusing on the most essential facets of the problem [20].A systematic mapping study produces a structure of the research reports in a specific topic area at a high level of granularity to perceive what evidence is available on the topic [26]. It is mostly designed to indicate the quantity of the studies in a specific research area. Moreover, it is critically discussed on the findings of the topic. The overall aims of systematic mapping study are (1) to discover strength and lack of relevant studies (also known as gap of research) and (2) to identify opportunities of future collaboration to further the existing knowledge by categorizing and mapping of the results in a visual summary [26]. In fact, the future research could be directed by the identified gaps in current studies that presented in Table 6 and lead to consideration of several ideas based on the results of the mapping study in a suitable and appropriate area [27]. In the first step of this study, we apply systematic mapping study [27,28] to identify and classify the GrC research in order to find the research gaps in the future research from the year 2012 to 2014. In the next step, we implement the framework among the defined frameworks, which has considered a little based on the mapping of the relevant studies to design the granular structure. The rest of the paper is organized as follows. First, we explain our research method to identify the research gaps in the GrC along with current researches in Section 2. Section 3 discusses the classification schemes of GrC from various aspects. Section 4 elaborates the results of the mapping and discusses on research questions. Section 5 highlights the identified research gaps in GrC with the recommendations for further research. Section 6 presents the design of the granular structure using five clustering algorithms, i.e., DBSCAN, k-means, c-means, Fuzzy-GrC and GAk-means. In Section 7, we include some datasets from UCI machine learning repository [29] to evaluate the performance of clustering algorithms. Finally, Section 8 points out the concluding remarks.This section divides our research method (Fig. 1) into four subsections as research question, research strategy, defining a classification scheme, and mapping relevant studies.Fig. 1 represents the four research questions in this study are answered by defining the relevant classification schemes.This study summarizes the empirical proofs aims at identifying the research gaps in GrC and then fill this gap. We defined the following research questions (RQs) to achieve this aim:RQ1: What are the existing frameworks for GrC?RQ2: Which areas are often considered by researchers by using GrC?RQ3: Which types of contributions are interested by researchers so far?RQ4: Selected journals to which research types are related?We developed a straightforward research strategy to determine the impact of relevant studies in GrC. The following three factors have influenced our research strategy: (i) search strings, (ii) search scope and (iii) search method. Table 1represents the search strings used in the concept of GrC. We extracted the search strings from the titles and keywords of papers. The search scope of this study is based on automatic searches limited to the Science Direct (SD) and IEEE Transactions (IEEET) studies, which are published between January 2012 and August 2014. We performed our search on the recent state-of-art. Therefore, the scope of research is extended into other publications according to the article’s related works and references in automatic searches, and limited to the publications period between January 2012 and August 2014. In the following section, we describe the search method based on search strings and search scope.Fig.2. shows the current research strategy divided into automatic and manual methods. Automatic search confines the papers within January 2012 and August 2014 by given search strings. We perform the automatic search to find the relevant primary studies in GrC. We screen a paper with a set of keywords (Table 1) though the title, keywords and abstract of the journals. Then, the relevant papers are extended by the related works and references of selected papers in automatic searches. We confine the results of manual search similar to automatic search. In the next step, we refine the relevant papers by reading the abstract, conclusion and introduction. Then, the refined papers are evaluated based on inclusion or exclusion criteria define as follows:Inclusion:•Papers that apply GrC technique to solve a problem in a new construction or an extension of existing techniques.Studies that arrange the existing studies in GrC topic in order to analyze and discuss.Papers that implement the existing GrC techniques or frameworks in order to present the results and personal experience.Exclusion:•Papers that only mentioned the GrC search strings (refer to Table 1) in abstraction or conclusion.Papers that are not available in full-text.Papers presented as an introductory for workshop.Papers contain the mentioned strings in Table 1, but not related to GrC technique in Computer Science related area.Finally, relevant studies are used for mapping through the defined classification scheme (refer to Section 3) to identify the existing gaps in GrC. Moreover, based on mapping results (refer to Section 4), we implemented (refer to Section 5) a less considered framework in designing the granular structure. Fig. 2 shows 96 identified papers based on the defined keywords in Table 1 by automatic searches. Then, the number of papers was extended by using a manual search to 122 papers. We have identified between 10 and 112 papers as exclusion and inclusion (refer to Table 56 in Appendix F) papers based on Inclusion/Exclusion criteria, respectively. Table 56 (presented in Appendix F) illustrates all of the Inclusion papers. Each column represents terms which were used as keywords in the papers. It was found that the Granular term was used more than the other terms as a keyword in these papers. Table 2shows the extension of each term that was used in these papers.Relevant studies in GrC are categorized based on the Petersen et al. [26] classification schemes. The contribution type category of Petersen et al. [26] is modified and defined the existing research focus areas in GrC. Furthermore, we added the framework category to Petersen et al. [26] classification schemes. We selected the articles through four perspectives that are focus area, contribution type, research type, and framework. These categories are adopted to map the studies in order to find the existing gaps in GrC. Each of these four categories is presented in Section 3.Relevant studies were mapped to the specific category as defined in Section 3. The results of mapping are given in Section 4.Petersen et al. [26] classification schemes are followed with some modifications to classify the selected relevant studies from the following four perspectives of focus area, contribution type, research type, and framework.Selected studies in GrC were categorized according to the four research focus areas from a broader perspective (RQ2). The four categories of research focus areas are defined based on unique research topics they addressed. Areas of researches in GrC are briefly described in the following subsections.GrC is a rich framework for the data analysis. A Human-centric Way [13,30] and Interval-based Evolving Modeling [31,32] are used to represent the collection of information granules for spatiotemporal data and heterogeneous data in time-varying systems, respectively.A Human-centric Way: A Human-centric Way of data analysis is often dealing with the data established by the user and distributed in space and time [13,30]. This is considered the representation of data in an interpretable way. The data and relationships in the granular way of data analysis are defined in the spatial and temporal domain through a collection of information granules. There are some compelling cases in various applications that considered the spatiotemporal data distributions in time and space [4]. Shifting from machine-centered approaches to human-centered approaches is considered as one of the trends in GrC research [4,33].Interval-based Evolving Modeling: Interval-based Evolving Modeling (IBeM) is an adaptive and flexible procedure to deal with the heterogeneous data in time-varying systems with non_stationary granular data streams [4,34]. IBeM algorithms are used to envelop uncertainty by accumulating the values which are associated with granules and rules. A granular way of data analysis is a rich framework to discover the essence of the structure of the data. System and environment changes are tracked by evolving and updating of rules in terms of IBeM learning algorithms [35,36].From a GrC point of view, we consider the Concept of Formation and Learning as one of the research focus area categories. There are some relations between granulations and classifications and between the granules and concepts. Concepts have been considered as essential units of human thought in various disciplines: philosophy, cognitive science, inductive learning and machine learning [37,38]. GrC is a simple classical view of concepts, which is related to the granules notion. Several issues must be considered in order to make a concept-learning algorithm effective and its results meaningful. One issue is the selection of a set of meaningful basic concepts, from which target concepts can be expressed and interpreted. Another issue is to design strategies for learning. Different strategies may lead to different descriptions of the target concepts [10,39,40].Interaction of objects is a fundamental requirement to perform computations for modeling of complex systems [41]. The notion of the highly interactive granular system is clarified as the system in which intrastep interactions [42] with the external as well as with the internal environments take place. This interaction can occur between defined objects of soft computing approaches, machine learning or data mining techniques [43]. One of the well-known approaches in interactive granular system is an Interactive Rough GrC (IRGC) that used to model interactive computations [44,45].Use of GrC for segmentation is one of the growing areas of interest among researchers [46,47]. GrC has been applied to segment data such as images, words, knowledge, and signals [20]. Color image and video segmentation techniques in GrC can be classified as histogram thresholding based [48], neighborhood based [47,49], clustering based [20] and neuro-fuzzy based [46].Histogram Thresholding Based: Histogram Thresholding is one of the simple and popular techniques for image segmentation that tries to find the valleys and peaks in histogram [46]. The underlying assumption in Histogram Thresholding is to identify the dominating peaks in the image histogram [46,48]. Hence, the segmentation task is reduced to find thresholds dissecting the image histogram [50]. However, a major drawback of the Histogram Thresholding techniques is the lack of use of spatial relationship amongst the pixels [51].Clustering Based: Clustering Based approaches are used to form the collection of information granules. The semantic meaningful constructions of individual pixels drawn together based on their proximity (location) to construct information granules [4]. DBSCAN [52], Fuzzy c-means (FCM) and Fuzzy k-means (FKM) [53] clustering algorithm are the popular clustering algorithms which are used in GrC.Neuro-fuzzy Based: Neuro-fuzzy system is the integration of processing capabilities and readability of neural networks and fuzzy rule base systems, respectively [46]. Neural network and fuzzy systems have the dynamic and parallel processing capabilities to estimate the input–output functions. Fuzzy information granulation theory and fuzzy logic tool [1,54] are the underlying factors to formalize the information granules in the neuro-fuzzy system.Neighborhood Based: The uniformity is a general criterion in the neighborhood-based approach to segment regions in the image [47]. Infinitesimal granules are formalized mathematically by the neighborhood-based model, which led to the invention of calculus, topology and non-standard analysis. Each object in the neighborhood system is assigned as a finite or an infinite family of subsets which referred as a neighborhood [41,46]. Seed points initialization is a problem of neighborhood system methods in order to examine the areas and pixels [46].In order to classify the research approaches in GrC (RQ4), various research types are considered in this study. A brief definition of proposed research types by Wieringa et al. [55] are presented in the following subsections.Definition 1Solution proposalSolution proposal is considered the problem by proposing a novel solution or an essential extension of an existing technique. The benefits of proposed solution are highlighted through an example or thorough argumentation and reasoning.The novel techniques are implemented and investigated by validation research. Validation research examines the solution proposal that has not yet been applied in practice. Validation research is conducted to present any of these: experiments, prototypes, simulations, mathematical analysis, etc.Evaluation research is examining a solution that has already been applied in practice, however, validation research has not yet been practically applied. The proposed methods in evaluation research are implemented to solve a problem in empirical study and usually case studies or field studies are used to present the results.Conceptual proposal is an arrangement to represent things that already exist. It is presented in a novel way; however, it is not exactly considered the particular problem solving. Conceptual proposals may include taxonomies, theoretical frameworks, etc.An experience paper consists of the personal experience report from one or more real life projects. The authors report usually elaborates the process and achievement of the project.An opinion paper is considered the suitability or unsuitability of a specific technique or tool based on the personal opinion of the author. These personal opinions sometimes present the way of tool or technique’s development.In this study, the contribution type in the GrC and granular classification are divided into five categories as below (RQ3):Tool:This type of contribution concentrates on providing a tool in the GrC or granular classification. A tool or prototype form can be integrated with the other frameworks.Model:This type of contribution investigates the relationships, the comparisons of the proposed techniques, the existing challenges, or makes a classification among the papers.Metric:It refers to contribution that focuses on proposed metrics in order to calculate the effectiveness of GrC approaches.Enhancement:This type of contribution is focused on hybridizing the existing framework in GrC with the optimization methods in order to overcome the demerits or limitations of the GrC framework.Technique:This type of contribution focuses on proposing the new technique to discover the information granules. It represents how a specific technique will be used in GrC technology.Information granules can be defined in order to represent granular objects and handle tools in several different reasoning formalisms such as sets (interval analysis) [10], fuzzy sets [6], rough sets [13], shadow sets [56], cluster analysis [15], decision trees [57], neighborhood systems [58] and hybridizations frameworks [17] (RQ1).Interval analysis:Two different areas are concentrated on interval-valued data, namely Symbolic Data Analysis and Interval Analysis [59]. Interval analysis is applied to capture the key part of the GrC structure by set (interval) calculus. Sets construct the areas of the feature space based on the high homogeneity of the patterns [60,61]. Interval Analysis introduces intervals as a fundamental means of representing real data, thus providing methods for numeric processing of these intervals. Interval-number algebra and interval-set algebra are classified as concrete models of GrC [62].Fuzzy sets:Fuzzy sets are defined as sets whose patterns have degrees of membership. Fuzzy sets can be used to generate information granules in GrC [63,64]. In addition, it can be used to cope with the points located outside the information granules created by the other frameworks. In fact, the membership function is used as a solution to solve the problem of allocation of degrees of belongingness [54,60,61].Rough sets:Rough set theory is often considered as one of the fundamental techniques of GrC in order to solve the vagueness of information problem in the data mining field [65]. In this view, granules are formed by means of rough inclusions as the classes of objects close to a specified center of granule to a given degree [66–68].Shadow sets:Shadow sets intents to capture and quantify the factor of uncertainty coming with the construction of any information granule [56,69]. Shadowed sets are algorithmically induced by fuzzy sets assuming three values, which could be interpreted as full membership, full exclusion, and uncertain. Shadowed sets are conceptually close to rough sets in spite of differences in their mathematical foundations [69].Cluster analysis:Cluster analysis is often used as a major data analysis technique widely applied for many practical applications in emerging areas of data mining. The quality of a clustering result depends on both the similarity measure either used by a method and its implementation, or used by its ability to discover some or all of the hidden patterns [53,60]. Clustering algorithms build the seeds of information granules, which are known and designed as the centers of the clusters to grow the granular structures [16,20,61,70]. The design of the granular structure is explained by various clustering algorithm details in Section 5.Decision trees:A top-down or a bottom-up method can be applied for granulation. Decision tree is a kind of intuitive knowledge representation method. It uses tree structures to create decision sets as an efficient classifier. Decision tree algorithms which usually take the top-down greedy algorithm choose the best attribute as the current attribute, and then recursively expand the branches of decision tree until it satisfies a certain terminal condition [57]. There are two main problems [41] in constructing decision trees, where different solutions lead to various classification methods. The first problem is the attribute selection to make new branches in the tree, and the second problem is pruning to omit and decrease the tree. During the construction of a decision tree, a critical step in the context of GrC is to select the node attributes of a tree that has the least number of branches [57].Hybrids:Optimizing the collection of information granules is an important issue in the context of any hybrid system [61]. Every individual technique of a hybrid system should symbiotically overcome the demerits or limitations of other techniques rather than escalating the issues. This hybridization is performed through a fusion of learning and optimization techniques such as: Genetic Algorithm (GA) [18,48], Particle Swarm Optimization (PSO) [36,61,71] and Neural Network [17].In this section, the volume of published papers in various forums are considered (refer to Table 3). Also, the percentage of publications in the defined classification schemes are presented and analyzed (refer to Fig. 3). Then, the existing gap and the emphasis on existing research in GrC are mapped and discussed (refer to Figs. 4 and 5). Finally, the high citation papers in the included papers are presented in Fig. 6.Table 3 represents the number of selected papers in different publications based on the Table 1 search strings. Table 3 shows that the most papers published in Science Direct journals compare to the IEEE Transactions journals. Information Science journal has the most relevant publications compare to the other journals. On the other hand, most of the GrC papers published in 2013.Fig. 4(a)–(d) respectively shows the overviews of four classification schemes in GrC researches, which are research focus area, research type, contribution type, and framework of granular models. Fig. 4(a) shows that almost half of the research focus area belongs to the category of data analysis. Aforementioned, we applied the manual and automated searches to select the GrC papers on IEEET and SD journals. Fig. 4(b) shows most of the selected papers belong to the categories of solution proposal and validation research in research type scheme. Fig. 4(c) shows the distribution of papers between tool, method and enhancement categories of contribution type that are almost equal. Fig. 4(d) represents the 39% of the relevant papers belong to the rough set framework. Almost 42% of selected papers have used the fuzzy sets and hybrid frameworks in GrC. Less than the 20% of papers have applied the other frameworks to solve the existing problems.Figs. 5 and 6 present mappings the category of research focus area and existing frameworks in GrC with respect to research and contribution types, respectively. An outline of aforementioned mapping presents the existing research gaps as well as the emphasis on existing research in GrC. Fig. 5 shows that the majority of research papers are dedicated to propose models in data analysis category of research focus area. Moreover, the composition of enhancement contribution and the concept of formation and learning category, and also the composition of method contribution and data analysis are underdeveloped.As mentioned in Fig. 4(b), most of the papers have been published in solution proposal category of research type. Among these papers, most of them were considered the data analysis and concept of formation and learning categories. Fig. 6 shows that there is a big gap in using the decision trees and neighborhood system frameworks with the existing contribution types and research types. In addition, there is a gap in the composition of cluster analysis framework and existing research and contribution types. On the other hand, most of the researchers proposed the models and methods by using the fuzzy sets and rough set frameworks.Table 4shows the high citation papers among the selected papers in GrC research based on the Google Scholar, Web of Science and Scopus websites. Previous study by Skowron et al. [45] in Information Science journal has the most citation among the other papers with 68, 39 and 57 citation in Google Scholar, Web of Science and Scopus websites, respectively. Moreover, written by Qian et al. [68] in Approximate Reasoning journal is cited 38, 20 and 14 times in Google Scholar, Web of Science and Scopus websites, respectively, during 13months. Table 5addresses the aforementioned papers presented in Table 4 based on the classification scheme’s category. This table presents most of the high cited papers considered the data analysis focus area. In addition, rough set framework is more interested by the researchers in the GrC. On the other hand, little attention is paid to the interaction focus area and fuzzy sets framework in the GrC.From the presented results of defined classification schemes distribution (refer to Fig. 4), research focus areas mapping (refer to Fig. 5) and frameworks mapping (refer to Fig. 6), a number of research gaps in granular computing are identified which are presented in Table 6. The aforementioned research gaps derived from the figures that mentioned in the first column. This table highlights the identified gaps to outline new ideas for further research. The researchers can focus on the following suggestions in a near future to bridge the research gaps in the granular computing: (i) pay on metric contribution. For example, there is not a standard relation between ‘MinPoints’ and ‘EPS’ to create information granules [61]; (ii) extend the development of decision trees, shadow sets, neighborhood systems and interval mathematics frameworks to address this research gap; (iii) consider the GrC to segment the data such as the images and videos with the existing frameworks; and (iv) use the significant capability of clustering algorithms to create information granules which each of them plays a role of classifiers for each class under interest [61].It has gain less attraction on cluster analysis to discover certain category of granular classifiers, referred here as hyperboxes among much works on granular models based on Fig. 4(d). In this section, we defined the geometry of the information granules for designing effective classifiers with five of the popular clustering algorithms as DBSCAN [52], k-means [53], c-means [53], Fuzzy-GrC [61] and GAk-means [77]. The structure of the hyperboxes is created by clustering algorithm to build the granular structures. The hyperboxes are applied to discover clusters and noise points. The underlying concept is that the typical density of patterns in the created cluster is more than outside of that cluster through some predefined probability density requirements. Two global parameters are required to build hyperboxes, namely, radius of the EPS-neighborhood (EPS) and the minimum number of points (MinPoints) in an EPS-neighborhood of that point. It is difficult to create an explicit relationship between ‘MinPoints’ and ‘EPS’ [52]. Current study proposes the relationship between two significant parameters (‘EPS’ and ‘MinPoints’) to create a relationship between two significant parameters. To do so, we follow the design heuristic presented by Ester et al. [52].We describe and introduce some relevant definitions, which are used to construct the granular structure.Definition 1NeighborhoodIt is composed of homogeneous points. These points located based on the predefined distance. The neighborhood figure is made by the distance functions such as Tchebyshev and Euclidean for two points ‘p’ and ‘q’.A point is called a seed_point whenever it has more points than ‘MinPoints’ through a defined ‘EPS’ value as presented in Fig. 7. The neighborhoods of seed_points must represent the same class.The EPS-neighborhood of a point ‘p’ wrt. ‘EPS’, ‘MinPoints’ is defined as follows:•∀p,q∈D:NEPS(p)={q∈D|d(p,q)⩽EPS}where ‘D’ is any dataset.In fact, a point ‘q’ in a cluster ‘C’ is placed inside of the EPS-neighborhood of point ‘p’. EPS-neighborhood figure of ‘p’ is presented in Fig. 8.A point ‘p’ is directly density-reachable from point ‘q’ wrt. EPS, MinPoints if:(1)p∈NEPS(q)∣NEPS(q)∣⩾MinPoints (core point condition).Fig. 8(a) shows that the pairs of core points are symmetrical, however, the core point and border point in a hyperbox are asymmetrical patterns.A density-reachable point in any dataset space is defined as follows:•∀p,q: If a chain of points exist,p1,…,pnexist, wherep1=qandpn=pand there is a point ‘pi+1’ in a chain of points, which is directly density-reachable from ’pi’, then point ’p’ is density reachable from the point ’q’ in regard to ‘EPS’ and ‘MinPoints’.Density-reachability is the same as direct density-reachability. This relation is transitive, but it is not symmetric as depicted in Fig. 8(b).A density-connected point in ‘D’ is defined as follows:•∀p,q,o∈D: If ‘p’ and ‘q’ are density-reachable from ‘o’, then a point ‘p’ is density-connected to point ‘q’ with regard to ‘EPS’ and ‘MinPoints’.Density-connected points are symmetric for pairs as shown in Fig. 8(c).A density-based cluster in any dataset space is defined by the following conditions:(1)∀p,q: ifp∈Cand ‘q’ is density-reachable from ‘p’ with respect to ‘EPS’ and ‘MinPoints’, thenq∈C. (Maximality)∀p,q∈C: ‘p’ is density-connected to ‘q’ in regard to EPS and MinPoints. (Connectivity)A hyperbox is an object that comprises a seed_point, ‘EPS’ and ‘MinPoints’. A seed_point is placed at the center of a hyperbox. In addition, the border of the hyperbox from the seed_point is estimated with the defined ‘EPS’ value. Moreover, the object of hyperbox is included at least ‘MinPoints’ objects within a radius EPS-neighborhood. In fact, each cluster is constructed by a grouping of hyperboxes in which seed_points of hyperboxes are density-connected.Noise point does not belong to any created clusters as presented in Fig. 7.A point is called a border point whenever it is density-reachable from the other seed_point located inside of the hyperbox; however, it is not a seed_point by itself as presented in Fig. 7.We use clustering algorithms to build the seeds of the hyperboxes, which are designed as the centers of the clusters. In fact, interval analysis is applied to capture the key part of the classifier’s structure. Sets construct the areas of the feature space based on the high homogeneity of the patterns. The classifiers used to establish the geometry of the feature space and the decision boundaries are interpreted as classification process and analyzed according to some criteria as presented in Section 6.1.In this section, the procedure that is used in DBSCAN and k-means algorithms will be described. The aim of this phase is to build hyperboxes for discovering the information granules to create clusters and noisy points in a dataset according to noise points and density-based cluster definitions. First, the training datasets are split based on class label. Finding hyperboxes procedure looks at the EPS-neighborhood of each pattern in the training patterns and returns all density-reachable patterns from that pattern with regard to ‘EPS’ and ‘MinPoints’. Thus, this process yields a new hyperbox ‘Hi’ which contains the points in EPS-neighborhood. If there is no a cluster for the specified class label, the new cluster ‘C’ is created for the specified class label. Otherwise, the new hyperbox is added to the existing cluster. Each hyperbox represents only a single class. In the next step, all the unvisited points (border points) in ‘Hi’ are checked by the EPS-neighborhood definition. If EPS-neighborhood of each point in ‘Hi’ contains more than ‘MinPoints’ among datasets, then the new hyperbox ‘Hi+1’ is created and added to cluster ‘C’ in order to build it. This recursive call is performed iteratively to gather all directly density-reachable points until there is no more new point exists for adding to the current cluster ‘C’. DBSCAN algorithm extends its finding hyperboxes procedure for all the points that have not been visited yet to create new hyperboxes for the different classes and create a new cluster. In this research, formula (1) and (2) are used to as a distance formula in DBSCAN [52] and k-means [53] clustering algorithms, respectively.(1)d(x,y)=∑i=1n(xi-yi)2(2)d(x,y)=∑i=1n|xi-yi|2where ‘x’ and ‘y’ denote the vectors of two datasets and ‘xi’ and ‘yi’ denote the attribute of them. ‘n’ presents the number of attributes.The underlying step of using clustering algorithm, to discover clusters in GrC is to build the seeds of the hyperboxes designed as the centers of the clusters. Formula (5) is used as a distance formula to create hyperboxes in c-means [53]. This formula separated into two parts. The first part is the k-means formula and the second part is a membership function. The membership degree of point ‘x’ is indicated here by ‘ui’ (refers to formula (4)). To obtain the ‘ui’, we use formula (2) to find the distance of point ‘x’ and seed point of each cluster. Therefore, in order to solve this problem, we applied the k-means algorithm to discover the seeds. Next, finding hyperboxes procedure looks at the EPS-neighborhood of each pattern in the training patterns and returns all density-reachable patterns from that pattern with regard to ‘EPS’ and ‘MinPoints’. Thus, this process yields a new hyperbox ‘Hi’ and cluster ‘C’ contains the points in EPS-neighborhood.(3)avg(x,Ci)=d(x,S1)+d(x,S2)+…+d(x,Sn)n(4)ui=1∑j=1c(avg(x,Ci)÷avg(x,Cj))2(5)d(x,y)=∑i=1n|xi-yi|2×uiwhere ‘x’ and ‘y’ denote the vectors of two datasets and ‘xi’ and ‘yi’ denote the attribute of them. ‘n’ presents the number of attributes.Ujjwal et al. [77] proposed the GAk-means algorithm. In this study, the k-means algorithm was applied in the first step to discover the seed points. The discovered seed points play the role of chromosomes for genetic algorithm. Then, the mutation and crossover applied on the selected chromosomes. We assumed 60% and 20% for total rate and token rate, respectively. We calculate the fitness value of selected chromosomes in each iteration based on Maulik and Bandyopadhyay [77].Saber et al. [61] proposed the fuzzy granular classifier to discover hyperboxes for classification. This proposed model consists three phases. In the first phase, the set calculus is used to build the core structure of the classifier by using DBSCAN clustering algorithm. In the second phase, the PSO algorithm is applied to enhance the performance of seed_points for classification. Finally, the capability of membership function of fuzzy set is used to improve the geometry of classifier. In fact, the third phase covers all of the pattern which are not included in any hyperboxes in the second phase. Therefore, the misclassification problem is solved by this model.This section aims at obtaining and comparing the best form of information granule in the aforementioned clustering algorithms (Section 6.2) to classify the patterns and define their geometry and belongings to a specific class. In the following subsections, we consider the experimental datasets and the performance evaluation criteria that have used in this study. Finally, we analyze the performances of the clustering algorithms based on evaluation criteria. We have analyzed the effect of the proposed model on testing datasets. The best results display in bold in tables of Appendices A–E, two third or more of the datasets included as seed_points in the training datasets for classification. For example, Table 7shows 398 training datasets for Wisconsin dataset. On the other hand, Table 11 in Appendix A represents 254–387 numbers of seed_points in training phase of k-means clustering algorithm in various ‘MinPts’ and ‘EPS’ values.In this study, we have used three machine learning datasets from UCI repository related to classification problems [29]. Table 7 gives the details of each dataset by their area, number of classes (#class), number of attributes (#Atts.), number of examples (#Ex.), number of training datasets (#Train) and number of testing datasets (#Test). We divided the datasets into training and testing parts by randomly assigning 70% and 30% of total datasets to the training phase and testing phase, respectively. Tables 8 and 9illustrate the various categories of Glass and Ecoli datasets for classification, respectively.Performances of k-means, c-means, DBSCAN, Fuzzy-GrC and GAk-means structure were evaluated by the classification error and coverage percentage. In fact, the classification error shows the division of patterns which do not belong to any hyperboxes and are known as misclassified patterns by the whole number of patterns (refer to formula (6)). In addition, formula (7) depicts and quantifies the percentage of patterns which are covered by each structure of clustering methods. We calculate the coverage percentage by dividing the number of patterns that are covered by the clustering structure by the whole number of patterns.(6)α(%)=number of misclassified patterns in the primary structurenumber of patterns in the primary structure×100(7)Cov(%)=number of patterns covered by the primary/secondarynumber of patterns×100In this research, accuracy formula is used to calculate the performance of the proposed technique in classification. Classification accuracy is calculated in dealing with False Negative (FN) and False Positive (FP). FP represents the percentage of legitimate data and FN represents the percentage of non-legitimate data [78].(9)Accuracy=TP+TNNumber of Patterns×100

@&#CONCLUSIONS@&#
