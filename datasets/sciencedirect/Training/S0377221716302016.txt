@&#MAIN-TITLE@&#
Offsetting inventory replenishment cycles

@&#HIGHLIGHTS@&#
The inventory-staggering problem is considered for multi-item inventory systems.Symmetry breaking is used with mixed-integer programs for optimizing small problems.Efficient heuristics are proposed that achieved near-optimal solutions.A continuous-time formulation greatly reduces the maximum inventory requirement.The effect of stochastic demand on the maximum inventory distribution is analyzed.

@&#KEYPHRASES@&#
Inventory,Replenishment staggering,Symmetry reduction,Heuristics,

@&#ABSTRACT@&#
The inventory-staggering problem is a multi-item inventory problem in which replenishment cycles are scheduled or offset in order to minimize the maximum inventory level over a given planning horizon. We incorporate symmetry-breaking constraints in a mixed-integer programming model to determine optimal and near-optimal solutions. Local-search heuristics and evolutionary polishing heuristics are also presented to achieve effective and efficient solutions. We examine extensions of the problem that include a continuous-time framework as well as the effect of stochastic demand.

@&#INTRODUCTION@&#
Much of the inventory-research literature assumes the replenishment and stocking of an item is independent of that for other items. This may be a valid assumption if there are no restrictions on storage space, capital requirements, transportation capacities, or any other required resources. In the presence of such constraints, however, determining the inventory policies of each item independently may call for replenishment quantities that ultimately violate these restrictions. Thus, the coordinated replenishment of multiple items becomes a practical necessity.A traditional textbook approach to the deterministic, multi-item problem involves the use of Lagrange multipliers within the economic order quantity (EOQ) calculation (see, for example, Hadley & Whitin, 1963). This effectively increases the holding costs to reduce the resulting order quantities, thus satisfying the resource restriction (Nahmias & Olsen, 2015). However, a shortcoming of the Lagrangian approach is that it effectively assumes all items will be received simultaneously at some point in the planning horizon; consequentially, the maximum resource requirement will occur at that time.Another prevalent approach to the constrained, multi-item problem is to stagger, or offset, the orders in order to avoid the simultaneous receipt of orders as much as possible (see De Schrijver, Aghezzaf, & Vanmaele, 2013 for a recent review of constrained multi-item inventory systems). Much of the early research in this area assumes that all items have a common order cycle length (e.g., Homer, 1966, Page & Paul, 1976, Rosenblatt & Rothblum, 1990, Zoller, 1977). Other research has focused on developing bounds for the problem (e.g., Gallego, Queyranne, & Simchi-Levi, 1996, Hum, Sharafali, & Teo,, 2005). Gallego, Shaw, and Simchi-Levi (1992) have shown the offsetting problem to be strongly NP-complete.Research on the offsetting problem without restricting the order cycle lengths has been considerably less prevalent. Murthy, Benton, and Rubin (2003) considered the basic offsetting problem of minimizing the maximum resource requirements for situations in which the order cycle lengths of the items are known and are allowed to be independent of one another. They provided an optimal offsetting procedure for two items and a heuristic procedure for offsetting more than two items. Subsequent studies have focused on the development of heuristics to solve this problem, including genetic algorithms (Moon, Cha, & Kim, 2008, Yao & Chu, 2008), a smoothing procedure utilizing a Boltzmann function (Yao, Chu, & Lin, 2008), and local-search procedures (Croot & Huang, 2013). Boctor (2010) presented a mathematical formulation to solve small instances of the problem—up to 20 items—with commercially-available software and developed a hybrid heuristic and a simulated-annealing algorithm to solve larger instances of the problem.The purpose of this research is twofold. First, we generalize the existing literature by utilizing symmetry reduction to reduce the problem size in order to optimally solve larger problems than previously achieved and by proposing heuristics that we show to provide better solutions than previous methods. Then, we extend the current research by analyzing continuous-based replenishment systems as well as stochastic demand. In the next section, we present the problem definition and the mixed-integer programming formulation of Boctor (2010) as well as a description of the data used to analyze our results. We then discuss the symmetry-reduction approach in Section 3 and provide experimental results quantifying the effect on computational requirements. Two types of heuristics are utilized, and their performance is analyzed in Section 4. Further investigations into the offsetting problem are presented in Section 5, including a continuous-time model and the effect of stochastic demand. Finally, we present the conclusions of our research.Consider the situation in which there are a number of items (i=1,……,N) to be stocked, each with a deterministic demand rate, di, and a deterministic order cycle, ki(and, hence, a known order quantity,qi=diki). The demand rates of the items are assumed to be independent of one another, as are the order cycle lengths. Lead time is also assumed to be deterministic—as noted by Murthy et al. (2003), we can set it to zero without loss of generality—with instantaneous replenishment. The planning horizon spans T time periods (t=0,……,T−1). Even with an infinite time horizon (as T → ∞), the length of the planning horizon would need to be no more than the least common multiple of the order cycle lengths (the cycles then repeat); from a practical perspective, though, the planning horizon for which the parameters are known and constant (demand, order cycles, product mix, etc.) may be somewhat less. Finally, the objective is to identify when each item should be replenished in order to minimize the maximum resource requirement (for ease of exposition, we will present the problem in terms of the maximum inventory level for the remainder of the analysis), S, at any time during the planning horizon.Boctor (2010) presented a mixed-integer program for this problem, in which the decision variables, Xij, are defined to be equal to 1 if item i is first replenished at time j (j=0,……,ki−1). The mixed-integer programming formulation is then:(1)MinimizeS(2)Subjectto:∑j=0ki−1Xij=1fori=1,…,N(3)∑i=1N∑j=0ki−1sijtXij≤Sfort=0,…,T−1(4)S≥0,Xij∈{0,1}fori=1,…,Nj=0,…,ki−1wheresijt=qi−diτijtis the inventory level of item i at time t, andτijt=(ki+t−j)mod(ki)is the time elapsed since the most recent replenishment. This formulation requires∑ikibinary variables, one continuous variable, andN+Tconstraints. For example, the MIP formulation for the 9-item problem of Murthy et al. (2003) would require 73 binary variables.To evaluate the effect of symmetry reduction and the quality of the heuristics developed, data sets ranging from 9 to 500 items are used in the following sections. The 9-item example is taken directly from Murthy et al. (2003). Demand rates for 20-, 50-, and 200-item instances were generated using the approach of Boctor (2010) in which demand is uniformly distributed between 5 and 30 units per period. For the larger data sets, Boctor (2010) generated demand rates for each item from one of three ranges: uniformly distributed from 5 to 30 units, from 50 to 100 units, and from 150 to 200 units; we do this for 100-, 200-, and 500-item instances. We generated order cycle lengths randomly selected from the divisors of 360 between 2 and 20—to maintain a planning horizon of 360—for all sizes of problems; to evaluate the heuristics, we also follow the approach of Boctor (2010) for 200-item instances in which the order cycle length is uniformly distributed between 2 and 12 (as this provides a planning horizon of 27,720, it is not used in our optimization experimentation). Table 1provides a summary of the data used in the subsequent analyses.As is typical with many binary programs, the solution time required to identify an optimal solution increases rapidly with respect to the problem size. The mixed-integer programming formulation presented above requiresK=∑ikibinary variables; thus, there will be 2Ksolutions, of which∏ikiare feasible. Lower bounds can be developed for the staggering problem (i.e., the greater of∑iqi/2ormaxi{qi}), but the branch-and-bound procedure quickly identifies superior bounds.Thus, we turn our attention to reducing the problem size through symmetry reduction, as branch-and-bound procedures can become quite inefficient when the problem contains many symmetries (Bosch & Trick, 2005). The offsetting problem under consideration is highly symmetric. For example, an optimal solution to the Murthy et al. (2003) 9-item problem is such that the time of the first replenishment for each item is at timej=[0,0,1,3,2,2,0,9,1]. An alternate optimal solution would simply advance all of these times by one period; that is,j=[1,1,2,4,3,3,1,10,0](see Proposition 2 of Yao et al. (2008)). These symmetries are problematic in a branch-and-bound process, since they increase the size of the search space and, perhaps even worse, result in wasted time searching the branch-and-bound tree that are symmetric to already visited—and failed—states (Walsh, 2012). Symmetry-breaking constraints can be included in the mixed-integer program to assist in eliminating this type of symmetry. In our implementation, we will “fix” some variables by including constraints that ensure the first replenishment of an item is at time 0 (i.e., for item i,Xi0=1andXij=0forj=1,…,ki−1).As indicated by Yao et al. (2008), we can fix the variables for one item without loss of generality. To identify which item(s) should be selected for assignment, we consider two guidelines: (1) fix the item with the largest ki, since the number of binary variables and the number of feasible solutions are dependent on these values, and (2) using the concept from the first-fit decreasing algorithm of bin packing, first assign the largest items (i.e., the largest qi) placing the smaller items into the residual space (Constraint 3). To incorporate both guidelines, we propose fixing the item with the largest ki× qivalue.While previous research has suggested fixing only one item, we propose fixing additional items without compromising the optimal solution for situations in which the length of the planning horizon is greater than or equal to the least common multiple of the order cycle lengths of all items. The Chinese remainder theorem (e.g., van Tilborg, 2011) guarantees that, for two items with coprime order cycle lengths kaand kb, there is a time at which they will both be replenished at the same time period, t0:(5a)t0=Yamod(ka)(5b)t0=Ybmod(kb)where Yi(i={a,b}) is the time at which item i is first replenished (Yi=jwhenXij=1). Furthermore, there exists a repeating cycle of length ka× kbsuch that at some point in the cycle, item a will be replenished one period after item b, two periods after item b, etc. Regardless of where we start within the cycle (Yi=0,…,ki−1), we are assured of cycling though each instance. Finally, the Chinese remainder theorem can be extended to multiple items that are pairwise coprime, so we can fix several items—as long as the order-cycle lengths are pairwise coprime—and maintain optimality. We propose fixing the items with the largest ki× qivalues with coprime ki. Again, fixing multiple items in this manner is only valid when the planning horizon exceeds the least common multiple of all order cycle lengths; if the planning horizon is shorter, this may result in a suboptimal solution.To test the effect of symmetry reduction on the computational requirements of optimally solving the offsetting problem, we solve the mixed-integer program presented in the previous section using the branch-and-bound procedure contained in commercially-available software. The full model is solved for problem sizes of 9, 20, and 50 items, then as many as three items with pairwise coprime order cycle lengths are fixed for each. Table 2provides the CPU requirements (in seconds) obtained on an Intel i7 980 processor running at 4.0Gigahertz with 12gigabyte of memory, using Cplex Optimization Studio, Version 12.6.2. It should be noted that Cplex can perform symmetry-breaking reductions without explicitly specifying constraints to do so. For our implementation, we tested the Cplex default level (i.e., automatic) against three levels of symmetry breaking that are offered (moderate, very aggressive, and extremely aggressive) as well as turning off Cplex’s symmetry-breaking procedure.As indicated in the table, considerable savings in computational effort can be obtained with symmetry reduction. For the 9- and 20-item instances, the CPU time required to optimally solve the problem can be reduced by an order of magnitude; furthermore, each additional item that is fixed (i.e., explicitly including a constraint in the MIP) for these instances consistently results in a faster solution time. For the 50-item problem with no items fixed, an optimal solution is obtained only for the Cplex automatic setting; fixing items with pairwise coprime order cycle lengths provides solutions for all settings. An interesting observation is that fixing three items is inferior to that of fixing only two items (except when the Cplex’s symmetry-breaking procedure is turned off); perhaps this is due to eliminating branches of the branch-and-bound process that may identify better solutions more quickly.To further investigate the effect of symmetry reduction, problem sizes of 100, 200, and 500 items are considered. Although these problems cannot be solved to optimality within a reasonable time frame, good solutions can be identified fairly quickly. Table 3presents the quality of the solution—as measured by the percent deviation from the lower bound—after 5minutes of CPU time for each of Cplex’s symmetry-breaking settings described above, and after 16hours (960minutes) for the Cplex automatic setting.As illustrated in the table, solutions within one percent of the lower bound can generally be identified within five minutes, even without symmetry reduction. Croot and Huang (2013) note that the Law of Large Numbers would suggest that, for large problem sizes, there are a large number of “good” solutions that may be identified quickly. Still, applying symmetry reduction can result in improved solutions. After five minutes, the average percent gap from the lower bound tends to improve as additional items are fixed. When using extended solution times (16hours), the effect of increasing levels of symmetry reduction is not as apparent, again perhaps due to the possibility that additional reductions eliminate branches of the branch-and-bound process that can identify superior solutions in the allowed time frame.In this section, we propose heuristic methods that are capable of providing effective and efficient solutions to large-scale problems. The first type of heuristic involves modifications of Boctor's two-step hybrid heuristic (TSH) which consists of a construction procedure followed by an improvement procedure (Boctor, 2010). The second type of heuristic involves using an evolutionary “solution polishing” procedure as described by Rothberg (2007). Finally, these heuristics are compared to the genetic algorithm of Moon et al. (2008).The modifications to the TSH local-search heuristic are to increase the search strategies to include multiple passes. These three additional passes are created in part by ordering items not only in the original ascending order but also in descending order. Additionally, a first-improvement strategy is implemented as well as the original best-improvement strategy. The resulting heuristic has four passes and is labeled TSH4. To describe the TSH4 heuristic, we use Boctor's notation and description in which:Ej=the replenishment schedule after the jth iteration of the construction procedure.Sijf= the maximum inventory requirement if item i is added to Ej, and its first replenishment is scheduled at period f.Construction Phase:Order the set of items in ascending (descending) order of their order quantity qi.Schedule the first item, denoted u, in the list to be replenished at periodsmku+1, wherem=0,1,……,T/ku−1.Iteration j:Consider the next item in the list denoted i.Schedule its replenishments atfi+mki, wherefi=argmin(Sijf)andm=0,1,……,T/ki−1.Improvement:The improvement methodology employs two strategies. The original TSH strategy considers items one at a time and implements the replenishment period that leads to the maximum inventory reduction. This is a best-improvement strategy with respect to each item. The alternative strategy employed is a first-improvement strategy that implements any replenishment period move that leads to an inventory reduction. Both the best-improvement and first-improvement strategies stop when no further improvement can be found.The evolutionary solution-polishing heuristic is designed to improve MIP solutions within a large-neighborhood search framework. In our implementation, the mixed-integer program presented in Section 2 is used to find a solution, then the polishing heuristic is called to improve on this solution. The MIP solver employed two symmetry-breaking constraints, and the initial solution time was limited to 10 minutes in determining a starting solution. The polishing heuristic was then executed for 10minutes after the initial MIP solution was found.The TSH and TSH4 heuristics were implemented in Matlab, and the polishing heuristic was implemented on Cplex Optimization Studio, Version 12.6.2. All heuristics were executed on the same computing platform as described in the previous section. Table 4compares the results achieved by the TSH, TSH4, MIP polishing heuristic, and the genetic algorithm (GA) of Moon et al. (2008). The genetic algorithm was implemented with Evolver Version 7. The benchmarks include the percentage gap from the lower bound and the CPU time (in seconds).The multiple-pass TSH4 heuristic offers a moderate improvement with respect to the TSH heuristic. It improved seven of the nine test problems with only a marginal increase in computation time. The geometric mean gap with respect to the MIP lower bound was reduced from 2.702 to 1.376%. The computation times are typically a fraction of a second except for the 200c and 200d test problems which had a much longer planning horizon of 27,720 time periods instead of 360. The TSH4 heuristic compared favorably to Boctor's simulated-annealing approach that required 100 times more computing time and only achieved a very marginal improvement compared to the TSH heuristic. The polishing heuristic was able to achieve a significant reduction in the gap to 0.566% but at a significant increase in computation time. However on the 200c and 200d problems, the TSH4 construction heuristic was able to achieve superior solutions in 8.64 and 12.19seconds, respectively. Thus, the TSH4 construction heuristic appears to provides an efficient means to achieve effective solutions on large-scale inventory staggering problems with a large least common multiple.The genetic algorithm was initially implemented with Moon et al.’s stopping criterion and achieved results slightly inferior to the TSH4 heuristic. However, letting the GA run for a 10-minute time limit achieved solution values of 2536, 4040, and 62,264, respectively, for the 20-, 50-, and 100-item problems. These values are slightly superior to the TSH4 heuristic on these smaller problems. However, on the size 200 problems, the GA achieved inferior solutions and appears to suffer from the curse of dimensionality. The GA was able to solve the 200b problem with a 360 least common multiple of the order cycle lengths of all items. The GA could not improve a randomly-generated, initial starting solution on problem 200a. Also, the 200c and 200d problems each have a least common multiple of 27,720, and the GA could not improve a randomly-generated, initial starting solution. The GA was then implemented with a 30-minute time limit and an initial solution provided by the TSH4 heuristic. The results in Table 4 show a further improvement on problems of size 20, 50, and 100 compared to the TSH4 heuristic. However, the warm-started GA was able to improve only one of the size 200 problems and the other eight solutions obtained are inferior to the MIP polishing heuristic. Given the inferior performance and computational requirements, the GA was not applied to solve the 500-item problem.We now consider two generalizations to the basic offsetting problem. First, we remove the restriction that replenishments can only be made at discrete points in time and analyze the continuous-time formulation of the problem. Then, we evaluate the effect of stochastic demand on the maximum inventory levels.Lean logistics practices require frequent deliveries within specific time intervals. Freeland (1991) provided an operating definition of Just-In-Time purchasing that includes:“1.Small delivery lot sizes based on the immediate needs for production usage. Deliveries are very frequent, e.g., several times per day.”“2.Deliveries are synchronized with the buyer's production schedule.”Liker and Wu (2000) noted that with Toyota's JIT implementation, “suppliers would receive a signal just a few hours before parts were needed on the assembly line and could ship exactly what Toyota needed many times throughout the day.” In this section, we examine a formulation of the inventory-staggering problem with continuous-based replenishment and explore the potential benefits compared to optimal integer-based scheduling.To formulate the continuous-replenishment model, let Yibe the time period in which item i is first replenished. All subsequent replenishments for item i will then take place every kitime periods throughout the planning horizon. So the mth replenishment of item i will be at timerim=Yi+(m−1)ki. LetRiconsist of the resulting set of replenishment times for item i, given the values of Yi, form=1,……,T/ki, and letR=∪iRiSince the replenishment times are determined by the decision variables, Yi, any solution methodology must account for this.The maximum inventory level will occur at a replenishment time for one of the items. The inventory level of item i at time t issijt=[qi−di(ki+t−Yi)mod(ki)], so the objective is to minimize the maximum inventory level over each of the potential replenishment times:(6)Minimizemaxt∈R∑i[qi−di(ki+t−Yi)mod(ki)]The only constraints on this optimization involve the domain of the decision variables; that is, 0 ≤ Yi< ki.This formulation involves the non-smooth, nonlinear modulus function in terms of the decision variables, Yi. Consequently, classical nonlinear optimization techniques do not apply in solving the continuous-replenishment model. However, evolutionary solvers can be applied to intelligently search for an effective solution. In order to ascertain the potential benefits of continuous-time replenishments, the three test problems for which an optimal integer solution exists are solved with the evolutionary solver in the Analytic Solver Platform add-in to Microsoft Excel and the genetic algorithm of Moon et al. (2008). The starting point for both the TSH4 heuristic and the GA was with all initial replenishments at time zero. CPU time limits were set at 10minutes for the 9-and 20-item problems and 20minutes for the 50-item problem.Table 5compares the maximum inventory levels achieved with the integer and continuous replenishment times. Improvements range from 4.587 to 20.099% by allowing the replenishment times to be continuous. It also appears that the percentage gains appear to decrease with increasing problem size. The GA found a slightly better solution to the 9-item problem, and the evolutionary solver found better solutions to the 20- and 50-item problems. The results suggest that the potential reduction in inventory levels is much greater than the difference between heuristic and optimal solutions to the integer-restricted replenishment problem.Our analysis to this point has assumed that the demand rate of each item is deterministic. This obviously simplifies the problem, as the maximum inventory level for each item will simply be its order quantity. Silver (1976) noted, “when this assumption is inappropriate, the decision rules can still be used as guidelines for sizes or time durations of replenishments, in much the same way that the EOQ is still useful, in combination with a safety stock, when demand is probabilistic.” Thus, we will evaluate whether the deterministic solution, with an added safety stock for each item, provides an accurate representation of the maximum inventory level in a stochastic environment.To do this, we consider the situation in which the demand rate of each item follows a truncated normal distribution, with a meanμi=di(the demand rate used in Section 3) and a coefficient of variation of 0.1, 0.2, or 0.4. The demand of each item is assumed not to be serially correlated, nor correlated with other items. An (R, S) model will be used, with the reorder interval for each itemRi=ki(the order cycle length used in Section 3), and a lead time of one time period (Li=1). The order-up-to levelSi=μi(Ri+Li)+⌈ΦασiRi+Li⌉, where Φαis the cumulative distribution function of the standard normal distribution for a Type I service level (i.e., the probability of a stockout is1−α), and ⌈x⌉ is the smallest integer greater than or equal to x. All shortages are backordered. The offset to be used (i.e., the time to make the first replenishment of each item) is taken from the optimal deterministic solution. A simulation model using @Risk Version 6.3 (Palisade 2013) is used to identify the distribution of the maximum inventory level.To illustrate, consider the 9-item example from Murthy et al. (2003). The optimal deterministic solution (maximum inventory level) is 786, which is realized at time t = 272 of the planning horizon. With a coefficient of variation of 0.20 and a Type I service level of 0.90, the aggregate safety stock,∑i⌈Φ0.90σiRi+Li⌉, of the nine items would be 116. Thus, the expected maximum inventory level—deterministic solution plus safety stock—would be 902. And, in fact, the simulated mean of the maximum inventory level at t = 272 is 902. The simulation also indicates a near-normal distribution (skewness = 0.01, kurtosis = 3.05, median = 902) with a standard deviation of 16.9. Of course, while the expected maximum can be well predicted, there is approximately a fifty percent chance that it will exceed 902.However, given the fact that the maximum inventory levels at other times in the planning horizon are near 786 (33 time periods have an inventory level within ten percent of this in the deterministic solution), we need to consider the maximum inventory level over the entire planning horizon. Thus, we are considering the maximum of T = 360 normally-distributed variables. Extreme value theory suggests that the maxima of iid normal variates is not normal, but will have an asymptotic limit distribution of the Gumbel distribution; however, the distributions of inventory levels in each time period are neither identical nor independent (the inventory level of one period is determined in part on that of the previous period).The simulated distribution of the maximum inventory level throughout the entire planning horizon is illustrated in Fig. 1. As expected, the distribution appears to be nonnormal (skewness = 0.39, kurtosis = 3.16). The median is 916 (14 units higher than the deterministic solution plus safety stock), the mean is 916.8, and the standard deviation is 11.7. In fact, 90% of the simulated results exceed 902, almost assuredly resulting in underestimating resource requirements. Safety stock may be effective in meeting demand with a given probability, but not in determining the resource requirements; perhaps some “safety space” would be necessary to ensure meeting these requirements. The simulated 95th percentile is 938, meaning 36 units of additional space would be needed.Table 6presents the 80th, 90th, and 95th percentile for the 9-, 20-, and 50-item problems. First, the amount of safety space necessary becomes greater as the problem size increases. This is likely due to the fact that for large problems, there are a large number of time periods with inventory levels close to the maximum. For the optimal solution to the deterministic 50-item problem, there are 54 time periods that have an inventory level within one percent of the maximum. Second, as the coefficient of variation of demand increases, additional safety space would be required. This is not surprising, since higher demand variability will result in higher inventory level variability which, in turn, will result in higher maximum inventory levels. Finally, although the demand service level will affect the expected (mean) maximum inventory levels, it seems to have little impact on its variability and, therefore, on the amount of safety space necessary.To better quantify these relationships, regression analyses were conducted to estimate the safety space needed (i.e., the difference between the simulated 80th, 90th, and 95th percentiles of the maximum inventory-level distribution and the deterministic solution plus safety stock). The predictor variables are the number of items, the coefficient of variation of demand, and the Type I service level; however, the service level was, as expected, insignificant and not included in the final model. The nonlinear-regression procedure, NLIN, in SAS (2015) was used. A measure of the quality of fit of the models is given by Pseudo r2 = 1 – SS(Residual) ÷ SS(Total Corrected). The estimated equations are as follows.80thpercentile:−35.9+23.4N0.524CV0.734Pseudor2=0.95190thpercentile:−38.7+86.4N0.506CV0.732Pseudor2=0.95095thpercentile:−42.9+100.3N0.487CV0.724Pseudor2=0.947The coefficients for the number of items and the demand variability are fairly consistent across the three percentiles, with (roughly) a square-root function for the number of items and a slightly greater effect for the coefficient of variation of demand.

@&#CONCLUSIONS@&#
