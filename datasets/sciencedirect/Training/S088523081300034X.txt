@&#MAIN-TITLE@&#
Exploring high-level features for detecting cyberpedophilia

@&#HIGHLIGHTS@&#
High-level features for cyberpedophilia detection are proposed.The fixated discourse model is suggested.Experiments on distinguishing between pedophiles’ and non-pedophiles’ chats are performed.Feature analysis is presented.

@&#KEYPHRASES@&#
Cyberpedophilia,Sentiment analysis,Emotion detection,

@&#ABSTRACT@&#
In this paper, we suggest a list of high-level features and study their applicability in detection of cyberpedophiles. We used a corpus of chats downloaded from http://www.perverted-justice.com and two negative datasets of different nature: cybersex logs available online, and the NPS chat corpus. The classification results show that the NPS data and the pedophiles’ conversations can be accurately discriminated from each other with character n-grams, while in the more complicated case of cybersex logs there is need for high-level features to reach good accuracy levels. In this latter setting our results show that features that model behaviour and emotion significantly outperform the low-level ones, and achieve a 97% accuracy.

@&#INTRODUCTION@&#
Child sexual abuse and pedophilia are both problems of great social concern. On the one hand, law enforcement is working on prosecuting and preventing child sexual abuse. On the other hand, psychologists and mental specialists are investigating the phenomenon of pedophilia. Even though pedophilia has been studied from different research points, it remains to be a very important problem that requires further research, especially from the automatic detection point of view.Previous studies report that in the majority of cases of sexual assaults the victims are underaged (Snyder, 2000). On the Internet, attempts to solicit children have become common as well. Wolak et al. (2003) found out that 19% of children have been sexually approached online. However, manual monitoring of each conversation is impossible, due to the massive amount of data and privacy issues. A good and practical alternative is the development of reliable tools for detecting pedophilia in online social media.In this paper, we address the problem of distinguishing pedophiles in chat logs with natural language processing (NLP) techniques. This problem becomes even more challenging because of the chat data specificity. Chat conversations are very different not only from the written text but also from other types of social media interactions, such as blogs and forums, since chatting on the Internet usually involves very fast typing. The data usually contains a large amount of mistakes, misspellings, specific slang, and character flooding. Therefore, accurate processing of this data with automated analyzers is quite challenging and can result in very noisy output.Previous research on pedophilia reports that the expression of certain emotions in text could be helpful to detect pedophiles in social media (Egan et al., 2011). Following these insights we suggest a list of features, including sentiments as well as other content-based features that could unveil semantic dimensions important in detecting cyberpedophilia. We propose a model of fixated discourse, one of the characteristics of cyberpedophile conversations described in previous research. The model we propose is based on lexical chains. We include this feature in further experiments as well as other high-level features. We investigate the impact of the proposed features on the problem of distinguishing pedophile chats from non-pedophile chats. Our experimental results show that binary classification based on such features discriminates pedophiles from non-pedophiles with high accuracy.The remainder of the paper is structured as follows: Section 2 overviews related work on the topic. Section 3 outlines the profile of a pedophile based on previous research. Our approach to the problem is presented in Section 5. Experimental data is described in Section 4. We show the results of the conducted experiments in Section 6. In Section 7 we discuss in more detail the findings from our research. We finally draw some conclusions and share plans for future research in Section 8.The problem of automatic detection of pedophiles in social media has been rarely addressed so far. In part, this is due to the difficulties involved in having access to useful data. There is an American foundation called Perverted Justice (PJ), that investigates cases of online child sexual abuse: adult volunteers enter chat rooms as juveniles (usually 12–15yearold) and if they are sexually solicited by adults, they work with the police to prosecute the offenders. Some chat conversations with cyberpedophiles are available at http://www.perverted-justice.com and they have been the subject of analysis of recent research on this topic.Pendar (2007) experimented with PJ data. He separated the lines written by pedophiles from those written by pseudo-victims and used a kNN classifier based on word n-grams to distinguish between them.Another related research has been carried out by McGhee et al. (2011). The chat lines from PJ were manually classified into the following categories:1.Exchange of personal information.Grooming.Approach.None of the classes listed above.Their experiments have shown that kNN classification achieves up to 83% accuracy and outperforms a rule-based approach.It is well known that pedophiles often create false profiles and pretend to be younger or of the opposite sex. Moreover, they try to copy children's behaviour. Automatically detecting age and gender in chat conversations could then be the first step in detecting cyberpedophilia. Peersman et al. (2011) have analyzed chats from the Belgium Netlog social network. Discrimination between those who are older than 16 from those who are younger based on a Support Vector Machine classification yields 71.3% accuracy. The accuracy is even higher when the age gap is increased (e.g. the accuracy of classifying those who are less than 16 from those who are older than 25 is 88.2%). They have also investigated the issues of the minimum amount of training data needed. Their experiments have shown that with 50% of the original dataset the accuracy remains almost the same, and with only 10% it is still much better than the random baseline performance.NLP techniques were as well applied to capture child sexual abuse data in P2P networks (Panchenko et al., 2012). The proposed text classification system is able to predict with high accuracy if a file contains child pornography by analyzing its name and textual description.A shared task on a similar problem was organized at PAN 2012 (http://pan.webis.de/). Given many short conversations, the task was to identify which user was convincing others“to provide some sexual favour”. Conversations were not longer than 150 messages and the percentage of predators was lower than 4%. The system that achieved the highest performance (Villatoro-Tello et al., 2012) was based on lexical features, prefiltering and a two-step classification. First, conversations were prefiltered, e.g. by removing those containing only one user. Then, suspicious conversations were identified and lastly, “predators” were detected among the suspicious conversations. In contrast, this research is not about identifying users convincing others to provide some sexual favour. It neither aims at classification of chat lines into categories, as it was done by McGhee et al. (2011), nor at discriminating between victim and pedophile as it was done by Pendar (2007). Our goal is to reveal behaviour and emotion dimensions, i.e. high-level features based on clues provided by psychology and sentiment analysis, that can help to distinguish chats that belong to a pedophile from those of non-pedophiles.Pedophilia is a “disorder of adult personality and behaviour” which is characterized by sexual interest in prepubescent children (World Health Organization, 1988). Even though solicitation of children is not a medical diagnosis, Abel and Harlow (2001) reported that 88% of child sexual abuse cases are committed by pedophiles. Therefore, we believe that understanding behaviour of pedophiles could help to detect and prevent children sexual abuse in social media. While an online sexual offender is not always a pedophile, in this paper we use these terms as synonyms.Previous research reports that about 94% of sexual offenders are males. With respect to female sexual molesters, it is reported, that they tend to be young and, in these cases, men are often involved as well (Vandiver and Kercher, 2004). Sexual assault offenders are more often adults (77%), though in 23% of cases children are solicited by other juveniles.Analysis of pedophiles’ personality characterizes them with feelings of inferiority, isolation, loneliness, low self-esteem and emotional immaturity. Moreover, 60–80% of them suffer from other psychiatric illnesses (Hall and Hall, 2007). In general, pedophiles are less emotionally stable than mentally healthy people.Hall and Hall (2007) noticed that five main types of computer-based sexual offenders can be distinguished: (1) the stalkers, who approach children in chat rooms in order to get physical access to them; (2) the cruisers, who are interested in online sexual molestation and not willing to meet children offline; (3) the masturbators, who watch child pornography; (4) the networkers or swappers, who trade information, pornography, and children; and (5) a combination of the four types. In this study we are interested in detecting stalkers (type (1)) and cruisers (type (2)).The language sexual offenders use was analyzed by Egan et al. (2011). The authors considered the chats available from PJ. The analysis of the chats revealed several characteristics of pedophiles’ language:•Implicit/explicit content. Typically, pedophiles shift gradually to the sexual conversation, starting with ordinary compliments:Offender: hey you are really cuteOffender: u are prettyOffender: hi sexyOffender: can we have sex?Offender: you ok with sex with me and drinking?Fixated discourse. Pedophiles are not willing to step aside from the sexual conversation. For example, in this conversation the pedophile almost ignores the question of pseudo-victim and comes back to the sex-related conversation:Offender: licking dont hurtOffender: its like u lick ice creamPseudo-victim: do u care that im 13 in march and not yet? i lied a little bit b4Offender: its all coolOffender: i can lick hardOffenders often understand that what they are doing is not moral:Offender: i would help but its not moralThey transfer responsibility to the victim:Pseudo-victim: what ya wanta do when u come overOffender: whatever – movies, games, drink, play around – it's up to you – what would you like to do?Pseudo-victim: that all sounds goodPseudo-victim: lolOffender: maybe get some sexy pics of you:-POffender: would you let me take pictures of you? of you naked? of me and you playing?:-DOffenders often behave as children, copying their linguistic style. Colloquialisms appear often in their messages:Offender: howwwww dy…Offender: i know PITY MEEEEThey try to minimize the risk of being prosecuted: they ask to delete chat logs and warn victims not to tell anyone about the talk:Offender: don’t tell anyone we have been talkingPseudo-victim: kPseudo-victim: lol who would i tell? no one's here.Offender: well I want it to be our secretThough they finally stop being cautious and insist on meeting offline:Offender: well let me come see youPseudo-victim: why u want 2 come over so bad?Offender: i wanna see youIn general, Egan et al. (2011) have found online solicitation to be more direct, while in real life children seduction is more deceitful.Pendar (2007) has summarized the possible types of chat interactions with sexually explicit content:1.Offender/other(a)Offender/victim (victim is underage).Offender/volunteer posing as a child.Offender/law enforcement officer posing as a child.Adult/adult (consensual relationship)For our current study, the most interesting data is that of the type 1(a). However, obtaining data from actual cases of offender/other is not easy. In contrast, data of the type 1(b) is freely available at the web site http://www.perverted-justice.com. Therefore, have extracted chat logs from the perverted-justice (PJ) website, where pedophiles have been identified and prosecuted by law enforcement agencies. Since the victim is not real, and our goal is to learn the patterns of cyberpedophiles, we considered only the chat prompts written by the pedophiles.Table 1.For our task of distinguishing sex-related chat conversations where one of the parties involved is a pedophile, the ideal negative dataset would be chat conversations of type 2 (consensual relations among adults). However the PJ data will not meet this condition for the negative instances. We need additional chat logs to build the negative dataset. We used two negative datasets in our experiments: cybersex chat logs and the NPS chat corpus.11http://faculty.nps.edu/cmartell/NPSChat.htm.From each dataset we randomly selected 20 files for testing.The cybersex chat logs were downloaded from http://oocities.org/urgrl21f/. This dataset belongs to type 2. We assume that the users on these chats are adults, although no explicit attempt was done to verify this. The archive contains 34 one-on-one cybersex logs. We have separated lines of different authors, thereby obtaining 68 files.We decided to use a subset of the NPS chat corpus (Forsythand and Martell, 2007), even though it is not of type 2, to explore how the high-level features work on the data when distinguishing cyberpedophiles from ordinary conversations. We have extracted chat lines only for those adult authors who had more than 30 lines written. Finally the dataset consisted of 65 authors.The datasets differ in length. The PJ conversations are much longer, with an average number of words and lines equal to 3618 and 526 respectively. For the cybersex data the averages are 1428 (words) and 97 (lines). The NPS data has even shorter conversations with, an average of 225 words and 52 lines. Balancing the data by trimming the conversations to the same size was not an option because our high level features attempt to model behaviour, thus some of the features we use span over the whole conversation. Moreover, as it is reported by previous research (Egan et al., 2011), cyberpedophile's behaviour changes during the conversation. So, instead of trimming the conversations, we normalize all the features we use by the document length.As already mentioned, while previous studies were focused on classifying chat lines into different categories (McGhee et al., 2011) or distinguishing between offender and victim (Pendar, 2007), in this work we address the problem of revealing which high-level features are discriminative when distinguishing pedophile chats from non-pedophile ones.We formulate the problem of detecting pedophiles in social media as the task of binary text categorization: given a text (a set of chat lines), the aim is to predict whether it is a case of cyberpedophilia or not. We describe our proposed features in the following sections.On the basis of previous analysis of pedophiles’ personality (described in the previous section), we consider as features those emotional markers that could unveil a certain degree of emotional instability, such as feelings of inferiority, isolation, loneliness, low self-esteem and emotional immaturity.It has been observed that pedophiles try to be nice with a victim and make compliments, at least in the beginning of a conversation. Therefore, the use of positively charged words is expected. However, pedophiles tend to be emotionally unstable and prone to loose temper easily. Hence words expressing anger and negative lexicon are an expected pattern in their chat logs. Other emotions can be as well a clue to detect pedophiles. For example, offenders often demonstrate fear, especially with respect to being prosecuted, and they express anger and emotions reflecting frustration:Pseudo-victim: u sad didnt car if im 13. now u car.Offender: well, I am just scared about being in trouble or going to jailPseudo-victim: u sad run away now u say no. i gues i dont no what u doinOffender:I got scaredOffender: we would get caught sometimeOffender: helloOffender: r u thereOffender:Offender: thnx a lotOffender: thanx a lotOffender:Offender:u just wast my timeOffender: drive down thereOffender: can u not im any moreOffender: u didnt callOffender:i m angry with uFinally, we suggest the following sentiment and emotional markers as the features:•percentage of positive words;percentage of negative words;percentage of JOY markers;percentage of SADNESS markers;percentage of ANGER markers;percentage of SURPRISE markers;percentage of DISGUST markers;percentage of FEAR markers;We have also borrowed several features from (McGhee et al., 2011):•Percentage of approach words. Approach words include verbs such as come and meet and nouns such as car and hotel.Percentage of relationship words. These words refer to dating (e.g. boyfriend, date).Percentage of family words. These words are the names of family members (e.g. mum, dad, brother).Percentage of communicative desensitization words. These are explicit sexual terms offenders use in order to desensitize the victim (e.g. penis, sex).Percentage of words expressing sharing information. This implies sharing basic information, such as age, gender and location, and sending photos. The words include asl, pic.Since pedophiles are known to be emotionally unstable and suffer from psychological problems, we consider features reported to be helpful to detect neuroticism levels by Argamon et al. (2009). In particular, the features include percentages of personal and reflexive pronouns and modal obligation verbs (have to, has to, had to, must, should, mustn’t, and shouldn’t).We consider the use of imperative sentences and emoticons to capture the pedophiles’ tendencies to be dominant and copy childrens’ behaviour respectively. The full list of features is presented in Table 2.As it was mentioned above, the study of Egan et al. (2011) has revealed several recurrent themes that appear in PJ chats. Among them, fixated discourse: the unwillingness of the cyberpedophile to change the topic. We believe that lexical chains are appropriate to model the fixated discourse of the pedophiles chats. We follow the definition of lexical chains discussed below and include these as higher-level features in our approach.Lexical chains have applications in many tasks including Word Sense Disambiguation (WSD) (Galley and McKeown, 2003) and Text Summarization (Barzilay and Elhadad, 1997). A lexical chain is a sequence of semantically related terms (Morris and Hirst, 1991). In order to find semantically related terms, we used metrics of semantic similarity. In particular, the similarity of Leacock and Chodorow (Leacock and Chodorow, 1998), and the Resnik similarity (Resnik, 1995). Leacock and Chodorow's semantic similarity measure is defined as:SimL&Ch(c1,c2)=−loglength(c1,c2)2*depthwhere length(c1, c2) is the length of the shortest path between the concepts c1 and c2 and depth is depth of the taxonomy.The semantic similarity measure that was proposed by Resnik (1995) relies on the Information Content concept:IC(c)=−logP(c)where P(c) is the probability of encountering the concept c in a large corpus. Thus, Resnik's similarity measure is defined as follows:SimResnik(c1,c2)=IC(lcs(c1,c2))where lcs(c1, c2) is the least common subsumer of c1 and c2.

@&#CONCLUSIONS@&#
This paper presents the results of a research project on the detection of cyberpedophilia. Following the clues given by research in psychology, we have suggested a list of high-level features that aim to model the level of emotional instability of pedophiles, as well as their feelings of inferiority, isolation, loneliness, and low self-esteem. We have considered as well such low-level features as character bigrams and trigrams and word unigrams, bigrams and trigrams. The SVM classification based on combinations of high-level features achieves 97% accuracy in distinguishing conversations with cyberpedophiles from cybersex chat logs, whereas low-level features achieve only 50–64% on the same data. In case of the common chat conversations (the NPS data), the low-level features, character trigrams in particular, are the most discriminative.Here we have presented experiments on two toy datasets, but the obtained results give some clues for solving the real-world problem. The most reasonable way could be first, to find suspicious conversations (sex related topics) with low-level features, and then apply high-level features to identify cyberpedophiles among them.For future work we want to gather a much larger data set and if possible one with actual victims involved. That will give us the opportunity to try to model the mental state of the victim with a similar approach to what we have used here. Being able to predict the vulnerability of children and young adults in social media interactions could also have large impact on society as it can be used to trigger intervention strategies to prevent cyberpedophilia. In addition, having the ability to model both, the potential victim and the pedophile can increase the detection rate of pedophiles.