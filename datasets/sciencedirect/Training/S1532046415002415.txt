@&#MAIN-TITLE@&#
Crowdsourcing Twitter annotations to identify first-hand experiences of prescription drug use

@&#HIGHLIGHTS@&#
Study on the automatic identification of first-hand drug intake reports in Twitter.Crowd-sourced judgements compared against expert judgements show moderate agreement.The experimental set up used 6 machine learning models with 7 different feature sets.Bayesian Generalized Linear Model performs the best (F1=0.64 and Informedness=0.43).

@&#KEYPHRASES@&#
Crowdsourcing,Pharmacovigilance,Twitter,Natural language processing,

@&#ABSTRACT@&#
Self-reported patient data has been shown to be a valuable knowledge source for post-market pharmacovigilance. In this paper we propose using the popular micro-blogging service Twitter to gather evidence about adverse drug reactions (ADRs) after firstly having identified micro-blog messages (also know as “tweets”) that report first-hand experience. In order to achieve this goal we explore machine learning with data crowdsourced from laymen annotators. With the help of lay annotators recruited from CrowdFlower we manually annotated 1548 tweets containing keywords related to two kinds of drugs: SSRIs (eg. Paroxetine), and cognitive enhancers (eg. Ritalin). Our results show that inter-annotator agreement (Fleiss’ kappa) for crowdsourcing ranks in moderate agreement with a pair of experienced annotators (Spearman’s Rho=0.471). We utilized the gold standard annotations from CrowdFlower for automatically training a range of supervised machine learning models to recognize first-hand experience. F-Score values are reported for 6 of these techniques with the Bayesian Generalized Linear Model being the best (F-Score=0.64 and Informedness=0.43) when combined with a selected set of features obtained by using information gain criteria.

@&#INTRODUCTION@&#
The scale of serious and fatal adverse drug reactions (ADRs) has been a key focus of concern for public health systems, especially in the United States, since at least the turn of the century [1] with an estimated 100,000 deaths attributed to adverse drug reactions (ADRs) every year in US hospitals [2]. An ADR is defined as any noxious and unintended response to a medicinal product. We also understand an adverse drug event (ADE or AE) as any unfavourable and unintended sign, symptom, or disease temporally associated with the use of a medicinal product [3].Given the limitations, and relatively small-scale of clinical trials for new drugs, post-market pharmacovigilance is vital. Traditional surveillance methods have focused on active clinician (or patient) reporting. The United States Food & Drug Administration’s (FDA) Safety Information and Event Reporting Program (i.e. MedWatch) [4] collects reports from the pharmaceutical industry, but these typically undergo significant reporting delays and systematic under-reporting [5].Social media has been shown to be a promising data source for pharmacovigilance data due to its real-time nature and utility in providing insights into off-label consumer habits [6,7]. Interest in social media as a signal source seems to be growing as can be seen by recent official announcements: On June 2014, the FDA presented its guidelines on how to use social media [8], and the Medicines and Healthcare products Regulatory Agency (MHRA) announced an application intended to report suspected ADRs, called WEB-RADR [9], on September 2014. EMA (European Medicines Agency) also published guidelines on good pharmacovigilance practices during 2013 [10] indicating that “marketing authorisation holders should regularly screen internet or digital media”, and stating that web sites, web pages, blogs, vlogs, social networks, internet forums, chat rooms, and health portals should be considered [11]. It seems clear that there is an increasing awareness of the potential for social media as a source of evidence. Our work here is focused on automatically identifying those Twitter messages that contain useful evidence for ADRs independently of whether these self reports comply with the guidelines or use the tools provided by the agencies mentioned above.Twitter offers several potential benefits as a source for pharmacovigilance surveillance data. First, a significant fraction of the content is freely available via a public application programming interface (API). Second, the volume of data available is huge, and unmediated by gatekeepers, with approximately 500million tweets sent per day in 2013 [12]. Third, Twitter content is “real-time”, allowing health researchers to potentially investigate and identify new ADE types faster than traditional methods such as physician reports. As such, we regard Twitter as an excellent testbed for our goal of identifying reports of ADRs among potential off-label drug users that may go under-reported by general practitioner visits [13] or undetected in clinical trials [14].At least one potential unknown is the influence of population bias. Since Twitter users tend to have a particular demographic [15] this may influence the ability of the media to provide useful evidence for some classes of drugs, e.g. those drugs used primarily by paediatric and geriatric patients. In this study, we focus on two classes of drugs: Selective Serotonin Reuptake Inhibitors antidepressants (SSRIs) (e.g. fluoxetine, citalopram) and cognitive enhancers (e.g. modafinil, methylphenidate). SSRIs were selected due to public concerns regarding the risk of suicidal ideation in children and adolescents [16]. The cognitive enhancer drug category was chosen due to the wide spread off-label use of prescription drugs such as Ritalin and Adderall as study aids by university students [17].A key difficulty in working with Twitter data, and social media data more generally, is distinguishing between first-hand experiences (“I feel real groggy after taking <DRUG>”), second-hand experiences (“I’ve heard <DRUG> makes you real tired”), and other kinds of information related to the drug, like news (“Court found <DRUG> company liable”) or advertising (“Buy <DRUG> now!”). In this paper we present a set of crowd-sourced Twitter annotations for SSRIs and cognitive enhancers, focusing on automatically identifying first-hand experiences. We show that annotations derived using the crowdsourcing service CrowdFlower are as reliable, in terms of inter-annotator agreement, as annotations derived from experienced annotators. Furthermore, we present a series of machine learning experiments based on these crowd-sourced annotations to show how first-person reports of ADRs can automatically be identified.As a first stage in gathering data on ADRs, it is vital to identify first-hand drug usage experience. This is a challenging area for natural language processing (NLP) as social media messages contain a high proportion of ungrammatical constructions, out of vocabulary words, abbreviations and metaphoric usage. First-hand experience is defined as being where the person making the report has actually taken the drug. For example, “<DRUG> is no joke have you up forever took it at 8 haven’t been sleepy since #<HASHTAG> #<HASHTAG> #<HASHTAG>”. On the other hand, a tweet like “Think I’ll just take some <DRUG> and get stuff done instead of sitting here like a worthless piece of shit.”, or “New Years resolution. Be less boring by staying up past 8pm. #<HASHTAG> or <DRUG>” would not be classified as first person as there is doubt as to whether the authors have taken the drug.Previous studies [18] used a reduced set of drugs to compare the adverse events reported on social networks with the adverse events registered in official databases such as FAERS [19], but to the best of our knowledge no studies have explored the genre, i.e. the type of tweet, in which the users refer to the drugs.The drugs selected for our study were either cognitive enhancers, i.e. drugs that enhance some mental function like attention and memory (see Table 1), or SSRIs (see Table 2). For cognitive enhancers we took into account some of the drugs that are anecdotally reported as being popular among the student population [20]. In the case of the SSRIs we analysed widely prescribed drugs identified by previous studies [21]. In both cases we read the existing articles available at Wikipedia on each of the target drugs and obtained a list of synonyms for these drug names as shown in Tables 1 and 2.We used the Twitter streaming API [22] to obtain a random sample from all public tweets for a 12month period (8th May 2012–20th April 2013). This gave us 420,983,674 messages. These data allowed us to understand how Twitter users mention the drugs of interest against a standard background.Once the full random sample was gathered we used our synonym list to identify tweets mentioning any of the drugs of interest (see Tables 1 and 2). We then applied a further filter where we would only keep a maximum of 300 matching tweets (selected at random among the matched tweets) for each one of the 11 drugs, aiming at a maximum of 3300 tweets. This was done after we noticed that some drugs such as Adderall and Prozac had a far higher number of mentions than the other drugs. In order to obtain a balanced sample we set that upper bound of 300 samples for each drug. Moreover, in the case of “Adrafinil” we did not get a single mention on any of the synonyms we used. This can be considered an important finding on the sensitivity of the data source. The final data set used for our study consisted of 1548 tweets (see Tables 1 and 2). Since the distribution of drug mentions is not evenly balanced we will investigate a targeted approach in the future in order to increase the volume of rare drug name mentions. With the data in hand we constructed our gold standard annotation set by selecting 496 tweets to be annotated by 2 PhD students with training in computational linguistics (including the first author).In order to check for influences on reporting bias we looked for popular stories that appeared during the time frame when we collected the tweets to check possible environmental influences from the media. The stories we found were “FDA warns of counterfeit Adderall”[23], “John Moffitt on Adderall: ‘It was a total mistake’ ”[24], and “Aurobindo Pharma gets USFDA nod for Modafinil tablets”[25]. But on the whole there was no major evidence showing that these would have an impact on the data set we collected during the sample period.The annotation categories we used were:•Tweet written in English language? This question reported which tweets were written in English language.Tweet about the drugs of interest? Some drug names appeared as strings within the tweet, providing texts that were not of interest to us.First-hand experience: Used to identify personal use of the drug.Other’s Experience: Used to identify someone else’s use of the drug.Activism: Used to identify an alarm or call for change in the drug policy.Cultural reference: Used to identify when the annotator found the tweet referring to a song lyric, movie title, etc.Humor: Used to indicate that a tweet contained a formulaic joke, bumper sticker, etc.News: Used to identify news items.Info/resource: Used to identify factoids or informational resources.Marketing: Used to identify sales of the drug product/accessory.Opinion: Used when the writer was reporting a personal opinion related to the drug.Sentiment: Used to describe whether the author was positive, negative or neutral in terms of sentiment about the drug.Pleasure: Used to indicate that the writer reports the drug usage as a pleasurable activity.Craving: Used to indicate that the writer reports stress relief related to the usage of the drug.Disgust: Used to indicate that the writer sees the studied drug usage or the drug users as repulsive.For the initial annotation effort, we obtained the Cohen’s Kappa [26] and Fleiss’ Kappa [27] values comparing the inter-annotator agreement between experienced annotators as shown in Table 4 (columns 2 and 3) by using R’s irr package [28]. We studied the Kappa values and identified possible causes of disagreement. These were loosely classified as follows:•Lack of context: Some tweets were written using only proper and common nouns making it hard for the annotator to understand the tweet and whether the tweet was written in English. For example, “@<PERSON> Ronaldo”, “@<PERSON> @<PERSON> @<PERSON> <DRUG> #rx” or “@<PERSON> <DRUG> FTW.” Major causes of disagreement were identified specifically in short tweets, the use of acronyms, emoticons, popular names and multilingual keywords.Meaningless mention: As the tweets were extracted based on keywords that matched the drug of interest’s name it was very important to read the tweet carefully to confirm that the drug itself was mentioned, especially given that some user names in Twitter can resemble the drug name, e.g. “@Adderall_RB I’m on it”, “RT @Adderall_XR: SO excited for the #entouragemovie”. Here we can see how drug names do not appear in the tweets once we remove the user names (“@<PERSON> I’m on it!” and “RT @<PERSON>: SO excited for the #entouragemovie”, respectively).Identifying first-hand reports: We found that in some cases it was not straightforward to distinguish a first-hand experience from rhetorical thought: “I wish I could prescribe <DRUG> myself for all these depressing ass tweets cheer tf up”, and also how to annotate the tweet in the case of forwarding a tweet from someone else (doing a Retweet): “RT @<PERSON>: @<PERSON> @<PERSON> – Fear not! I’ve got a couple of bottles of #<DRUG> right here. Pass me a doughnut, plea …”. In other cases it was not easy to tell for sure whether the writer was actually taking the drug: “Popular antidepressants <DRUG>, <DRUG> and <DRUG> can lower libido and prevent orgasms #fact”. In the same way it is not straightforward to realize whether the user took the drug and stopped taking it or whether she still takes it as in the following example: “@<PERSON> yep. i honestly think the <DRUG> has messed up my memory and concentration or something because they suck now”, “Hello, <DRUG>. Miss me?”.Ambiguous genre: Another area of disagreement was when annotating “Opinions” and “Other’s experience”, as in some examples it could be understood in either way as in: “@<PERSON> go to sleep already Joe and put down the <DRUG> really shit!”, “@<PERSON> @<PERSON> I just found it funny that people used <DRUG> against him.”, “Jesse needs to lay off the <DRUG> lmao”.Our annotation guidelines for laymen and experienced annotators (included in “Supplements” file) elaborate on the basic questions shown in Table 4.Although the two PhD annotators could have annotated all the tweets within the data set, given that experienced annotators are a scarce resource we decided to study other possibilities and rely on a crowdsourcing engine, also taking into account that the annotations obtained from the experienced annotators could be used as the gold standard when collecting laymen annotations.We opted for CrowdFlower as the service allowed us to use a subset of the tweets previously tagged by our experienced annotators, enabling us to provide a set of data items with correct responses, which in turn were used to discard tainted contributions. We also configured the settings to target contributors from several English speaking countries (Australia, Canada, New Zealand, the United Kingdom, and the United States) on the assumption that annotators from these countries were more likely to be native English speakers.We decided that the gold standard to be used in the crowdsourcing platform would be composed of 100 tweets where both expert annotators agreed on all fields. After that selection, the annotations provided by the expert annotators were then analysed by N.A. and N.C. to understand the cause of disagreements observing the points presented in the previous section. These 100 gold questions became the testing questions for laymen in CrowdFlower, acting as a filter to discard all the annotations coming from any annotator scoring lower than 70% on those test questions.The experienced annotators used the extended version of the guidelines prior to annotation (Supplement 1: Expert annotator guidelines). These guidelines were based on those created for a study into usage of electronic tobacco products reported on social media [29]. All the categories in our study except three were also used in the electronic tobacco product study. We added two categories in order to refine the results by annotating whether the tweet was written in English, and also to focus on the drug reporting tweets. The third category we added was used to understand if the tweet was reporting a first hand experience.Laymen annotators were presented with a simplified set of the annotation guidelines (Supplement 2: Laymen annotator guidelines) in the form of a questionnaire.Once we obtained the aggregated results from CrowdFlower1A modified version of this file complying with Twitter’s TOS can be found on github https://github.com/nestoralvaro/JBI_Pharmacovigilance/tree/master/1548_CrowdFlower.1we extracted the tweets that were written in English language and mentioned drugs of interest. This yielded 899 tweets that became our gold standard.2A modified version of this file complying with Twitter’s TOS can be found on github https://github.com/nestoralvaro/JBI_Pharmacovigilance/tree/master/899_CrowdFlower.2

@&#CONCLUSIONS@&#
In this paper we explored the classification of Twitter messages into first-hand drug user experience. For the task of selecting ADR data on the crowdsourced annotations Bayesian Generalized Linear Model (BGLM) was observed to be the model providing the overall highest F-Score among those tested, only surpassed by C50 when using the top 50% and the 100% of the features, although in terms of Informedness BGLM obtained the best scores all the time.We also used the subset of the same data for which both the laymen and one expert agreed on the annotation for the fields “First-class experience”, “Tweet written in English language”, and “Tweet about the drug”. In this case BGLM obtained the best F-Score values, and also the highest Informedness measure, showing the predictive power of this model for this dataset.For our last experiment we used the dataset where the annotations from two expert annotators were in agreement for the fields “First-class experience”, “Tweet written in English language”, and “Tweet about the drug”. In this experiment we observed that BGLM had the highest F-Score values, only matched by GLM when using the top 1% features. This is particularly interesting because the annotators were not laymen, and the data were collected during a different period and also using a different method, but the best performing model was the same as in the previous experiments.We also observed that most models had a stable performance independent of the set of features. We also realized that “SVM” predictions were lower than the baseline in all the experiments, and “Multi-Layer Perceptron”, and “Naive Bayes” only scored above the baseline when using the dataset annotated by the two experts.We believe this line of research can be meaningful given the volume of tweets that are constantly generated. Having a first filter to detect user reports on Twitter on the drug use can help in pruning valuable data since the beginning of other studies. Our aim is to continue exploring this path to automatically identify tweets reporting first-hand experiences on a set of drugs, and our plan is to further study how a feature re-engineering process should be performed, in particular when combined with LDA topics and ensemble models. We will also consider expanding the list of synonyms to include slang names for the selected drugs.Importantly, we also showed that the inter-annotator agreement from CrowdFlower is of comparable quality to the inter-annotator agreement obtained from experienced annotators, confirming that we can rely on crowdsourced annotations to identify personal drug reports, although there are still difficulties such as some notable disagreements (e.g. cultural references, disgust) that need to be recognised. To overcome this we have to analyse how human agreement might be improved as there are some open areas of work such as better guideline development and better interface selection.The authors declare that there are no conflicts of interest.