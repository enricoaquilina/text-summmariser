@&#MAIN-TITLE@&#
Multi-focus image fusion using sharpness criteria for visual sensor networks in wavelet domain

@&#HIGHLIGHTS@&#
A new method of fusion using sharpness criteria in wavelet domain is used in this paper.The suggested method uses the two criteria of spatial frequency and variance for fusion.A lot of experiments indicate the efficiency of the recommended method both in quality and complexity.

@&#KEYPHRASES@&#
Multi-focus,Image fusion,Sharpness criteria,Wavelet transform,Visual sensor network,

@&#ABSTRACT@&#
Image, graphical abstract

@&#INTRODUCTION@&#
Multi-sensor methods are used in a wide range of military and civil applications such as remote sensing, machine vision, concealed weapon detection, and medical imaging in order to enhance robustness and performance. However, using more sensor data sources brings about the problem of overload information. To solve this problem, some researches have been carried out on the techniques to counteract the data overload caused by sensors without losing useful data. The aim of fusion in each application is to combine images of several sensors, which leads to the decreased amount of input image data, producing an image with more accurate data [1]. Based on information theory perspective, the fused image has better performance in data and can be used effectively for computer processing and human visual perception.Cameras applied in visual sensor network and machine vision have limited depth of field and can focus only on the objects at the scene located within particular depth or certain distance. The main properties of such objects are high contrast and sharpness whereas objects at other distances become blurred. So it is necessary to use several cameras to extend depth of focus. Thus, multi focus image fusion techniques are appropriate for fusion of a series of images. The main challenges in image fusion process include: evaluate the blurriness of each image in an effective way, identify appropriate data of images with high focus and contrast, as well as areas with high sharpness, and combine these useful data to create an image with more information.The simplest algorithms of image fusion perform no decomposition or transformation which results in low contrast. To tackle the problem, it is recommended to use fusion technique which is based on decomposition. Multi resolution decomposition has recently used widely in image fusion [2–6]. The key step in these approaches is to apply multi scale transform on each source image and then to combine their coefficients, which is done in an appropriate way to acquire the best quality in fused images. Some examples of these approaches include morphological, gradient pyramids and laplacian pyramid (LAP) and the most popular ones are discrete wavelet transform (DWT) [7] and shift invariant discrete wavelet transform (SIDWT) [8]. Most of the current pyramid transforms do not have directionality and cannot receive details and edges information; hence no high quality fusion is achieved. With the development of wavelet theory and its many advantages such as localization, direction and superb display of image information, combining images based on wavelet analysis came into attention. Using this technique provide an improved combination of fused images for human perception. These approaches achieve the best quality from the source images by monitoring a quantity called "activity level" and then fuse them. Activity level measure indicates source image's quality. Next, fusion is done according to the rules called fusion rules, two common methods of which are described as follows. In first rule which is called "choose max", coefficients with maximum "activity level" are selected and other coefficients are discarded. In the other rule, named weighted average, each multi scale discrete coefficient of the fused image is acquired from combination of weighted coefficients of the source image. It is worth mentioning that wavelet-based algorithms, using different fusion rules, can deal with various frequency bands.An image fusion technique in wavelet area is presented in [9]. Fusion is performed by applying wavelet analysis on each source image. In this study, DWT is replaced with SIDWT to decompose the image. By observing marginal distribution of wavelet coefficients and their differences at different focus levels, a new statistical sharpness measure is presented to measure the blurredness of the image exploiting the spreading of the wavelet coefficients. In [9], wavelet coefficients distribution is evaluated using locally adaptive laplacian mixture model (LMM). With the estimation that is provided by this model, detail sub-band coefficients are fused. To combine approximation sub-band coefficients, weighted average has been used as the fusion rule. To measure information in this sub-band, the region entropy introduced in [10] has been used. Finally, the fused image is acquired by applying inverse wavelet transform. This method attempts to produce an image with high sharpness but it suffers from serious artifacts around the edges.Another approach suggested in [11] to maintain the visual quality and the efficiency of the fused image and to reduce the complexity, is based on discrete cosine harmonic wavelet (DCHWT) which considers the fusion rule suggested in [12]. Fusion is done by calculating weights for the coefficients of every level and sub-band. However, it has some disadvantages like producing ringing artifacts.A method of multi-focus fusion has been presented in the wavelet field in [13]. In the proposed algorithm, firstly the input images are divided into 8×8 blocks. On each block wavelet transformation is applied. Secondly, the variance of the wavelet coefficient is calculated for every sub-band. In this proposed method, variance is taken as activity level measure. Thirdly, the sub-band variances compared to each other and the coefficients with the largest variance are selected. In the final step, the inverse of wavelet transform is applied to the fused coefficients. This is done for every sub-image of 8×8. The main advantage of this method is the superb quality of the acquired image on the non-edge. This method, however, has few weaknesses in terms of fusing the areas close to the edges and boundaries and, increasing the run time due to the division of the image into small blocks.Recently, two fusing methods have been proposed by Yu Li [14–15]. The proposed method in [14] fuses the image using the two theories of multi scale transform (MST) and sparse representation (SR), which are widely used in image display. First MST is applied on each source image in order to obtain high pass and low pass coefficients. Then, low pass bands are fused via an approach based on SR and high pass coefficients are fused using the absolute value of coefficients. Finally, the fused image is obtained by performing inverse MST. This method works well in non-border areas but creates some artifacts in border areas. In the other method presented in [15], activity level of image patches are measured using dense scale invariant feature transform (SIFT) descriptor and the primary decision map is obtained. In the next step, the ultimate decision map is achieved by matching SIFT property.In some earlier works, several methods suggested in DCT and gradient domain are explained to have an evaluation of the fusion. A fusion approach has been suggested in [16] in DCT domain. As the quantity of the variance is usually considered as contrast in image processing application, in [16] the variance of DCT coefficients of the 8×8 blocks of the source images are calculated, in which the variance is taken as activity level measure. Then, it uses consistency verification (CV) to discard error in block selection procedure caused by noise or undesired effects. Complexity reduction is one of this method's advantages. However, the proposed algorithm has some weaknesses in the boundaries between the fused source images, so it errors in identifying the blocks with high quality.Tang [17] has presented two other fusion techniques in DCT domain named DCT+Average and DCT+Contrast. He has defined his proposed methods on 8×8 blocks. DCT+Average acquires the fused image by calculating the average of DCT coefficients of the input image. This method had some undesirable side effects like blurring.In another method, DCT+Contrast, activity level is considered on the basis of contrast. Therefore, among corresponding DCT coefficients in the source image, each of them which has the most contrast is taken as the equivalent coefficient in the output image. Calculating contrast criterion in this method is very complex. In addition, this method has high blocking effects due to DCT coefficients shifts in different selection of different images.The other method, recently introduced in [18] by Zhou, obtains fused image in gradient domain by exploiting a new multi scale method which detects definite focus regions and then identifies gradient weight near focused boundaries. This method is so complex with long run time and the contrast is decreased in the boundaries between the source images, leading to the production of some artifacts.In this paper, an efficient fusing technique in wavelet domain has been presented to reduce the complexity, increase the speed and maintain the output image quality, seeking to produce artifacts as little as possible. As mentioned above, the method presented in [13] has some weaknesses like long run time and the production of some artifacts. Our motivation for the proposing of this method is both to elevate the quality and to increase the speed of the algorithm by changing the activity level measures and improving the algorithm precision in the edges and boundaries. All are achieved without losing quality of the output image.This paper is organized as follows: Section 2 presents image fusion based on the wavelet transform. In Section 3, proposed multi-focus image fusion method is introduced and discussed. The results of applying of the proposed fusion method, from subjective tests and objective metrics viewpoints, are presented in Section 4. Finally, Section 5 concludes the paper.Wavelet transform is widely used for appropriate localization in frequency domain in image processing and analysis. Mallat, inspiring from Burt and Adelson pyramid's decomposition of image and reconstruction algorithm, proposed a method and a fast algorithm to make orthogonal wavelet methods, calling it Mallat algorithm [19,20].The reason of using wavelet transform is its energy compaction and multi-resolution features which results in successful fusion of important image features such as edges and texture without producing artifacts.Assume that we want to fuse two images F and G and we name the acquired image Z. The first steps of image fusion based on wavelet transform are explained in the following:Step 1: Applying wavelet transform on each one of F and G source image.Step 2: Applying different fusion rules for different frequency bands in each analysis level.Step 3: Carrying out the inverse of transform for fused coefficients of wavelet and reconstructing the fused image Z.The motivation of choosing wavelet transform originates from the fact that scale-space-orientation analysis done by wavelet transform is a mirror model of location analysis which takes place in V1 area of the human eye's cortex [21].To increase the output image quality, an image fusion technique in wavelet domain, which is done in two steps, has been suggested in this paper. In this technique, variance and spatial frequency of wavelet coefficients of 16×16 blocks of the source image are considered as a criterion for contrast, as well as for identifying the blocks with high clarity. They are also used for activity level criterion. Then, consistency verification (CV) enhances the output image quality. Ultimately, using the produced decision map which is created simultaneously with combination procedure, the fusion process is again performed for the areas close to the edges in 8×8 blocks.Fig. 1shows the general framework of image fusion. As it is shown in Fig 1, after dividing the source images into 16×16 pixels and calculating the wavelet coefficients for each block, the fusion algorithm is performed in two steps. In Section 3, we will explain the stages of the proposed algorithm.To decrease the complexity of algorithm, and the consequently reduction in run time. Firstly, the source images are divided into blocks of 16×16 pixels. The main aim of this step in fusion is combining source image textures after dividing them into 16×16 blocks and calculating the wavelet coefficients of each block on two scales. Multi scale analysis produces 4 bands at each level containing detail information of horizontal, vertical and diagonal directions (respectively shown by H, V and D), as well as approximation information (shown by A). Fusion algorithm is done using the calculation of the spatial frequency of approximation sub-band coefficients and detail coefficient variance. Here, the amount of spatial frequency and variance of the blocks of the source image are taken as activity level criterion.To simplify the process, only two source images are considered, in case of more than two images, they are combined two by two using recursive procedure. Assume that we have two source images F and G and the fused image Z. Multi scale decompositions (MSD)display the blocks of the source image I is shown by XI and its activity level by ACI. Thus, XZ, XG, XF are respectively considered as MSD display of fused and source images and ACF and ACG are considered as activity level.P=(m,n,k,l)shows the index of one of the MSD coefficient, in which m and n show the spatial position in the given frequency band, k indicates the activity level, and l shows the frequency band of MSD.This part evaluates the proposed method information containing approximation sub-band of each input image and combines them to generate the fused approximation sub-band. Spatial frequency, inspiring from human visual system, is considered as one of the activity levels of the images. Complete understanding of human visual system by using the present physiological tools is very complex, however, using spatial frequency leads to introducing effective objective quality metric for image fusion which is our motivation to fuse approximation coefficients. Spatial frequency of approximation sub-band coefficients for one block of the image is defined as: assume approximation sub-band sub-image with the size of M×N, in which M is the number of rows and N number of columns. Row frequencies and column frequencies are shown in Eqs. (1)-(2).(1)RF=1MN∑m=0M−1∑n=1N−1[XI(m,n,k,l)−XI(m,n−1,k,l)]2(2)CF=1MN∑n=0N−1∑m=1M−1[XI(m,n,k,l)−XI(m−1,n,k,l)]2In the above relations, l refers to approximation sub-band and k is 2. The total of spatial frequency of an approximation sub-band sub-image is:(3)ACI(P)=SF=(RF)2+(CF)2Now in this step of the algorithm, the coefficients with bigger activity level measure are chosen and the related approximation sub-band coefficients become one block of the fused image. Noteworthy is that when the evaluated sub-band has bigger activity level at the time of combining coefficients, all of its coefficients are selected, so m and n can be ignored in the relations. Approximation sub-image coefficients are constructed according to Eq. (4).(4)XZ(P)={XF(P)ifSFF>SFGXG(P)otherwise,As explained above, the focused area has more useful information in multi focus images. Usually the information relates to the areas with bigger variance. Variance is considered as a measure of contrast in image processing application and the areas with bigger variance have more contrast, more clarity and sharper edges, which is true about wavelet coefficients. Fig. 2(a) shows a 64×64 block extracted from the image of “Lena”. Fig. 2(e)–(b) are their blurred versions with radius of 3, 5, 7 and 9, respectively.Wavelet coefficients of each of the images in Fig. 2 are now calculated in 2 scales according to the proposed algorithm assumptions. The variances of detail sub-bands (horizontal, vertical and diagonal which are respectively represented by H, V and D, with indices 1 and 2 relating to the wavelet analysis level) at each level of wavelet transform and the variance of approximation sub-band coefficients at level 2 are shown in Table 1.For the 2 levels of decomposition, H1 and H2 represent horizontal sub-bands, V1 and V2 represent vertical sub-bands, D1 and D2 represent diagonal sub-bands and A2 represents approximation sub-band. As shown in Table 1, the more blurred, the less coefficient variance. Similar results for the “Cameraman” image (Fig. 3) are obtained and shown in Table 2.As shown, variance can be used as a criterion for clarity or sharpness of the image. In this phase of the fusion, the detail sub-bands of the input image blocks are combined and variance is used to measure the amount of detail information coefficients.Variance is calculated for each detail band in every level for every 16×16 block of the image. Therefore, the variance of the coefficients of the fused image, which has been considered as the activity level criterion, is achieved through Eq. (5).(5)ACI(P)=[1NM∑(XI(P)−X¯I(P))2]The block sub-band which has the bigger AC, is considered as the clearer one. There for, by applying an appropriate fusion rule, the coefficients with bigger activity level are selected and others will be discarded. If Z is the fused image, it can be explained that Xz(P)=Xi(P) where i=F or G, considering which source image, sets up Eq. (6).(6)Ai(P)=max(AF(P),AG(P))Simultaneous with the procedure mentioned in wavelet coefficients combination, a binary decision map is created to register the selection results based on maximum selection rule in the Eq. (4) and Eq. (6).Assume that there is an area in the scene consisting of several blocks completely in the depth of the focus of image F, so all the blocks of this area must be selected from image F. However, there might be an error in selection procedure due to noise or undesirable effects, and some blocks are selected from image G. This drawback can be solved by consistency verification [16].Li [7] using a majority filters applied consistency verification. If the central block has come from source image G, whereas most of the neighboring blocks are from source image F, then the central sample is replaced with a block from source image F. The primary fused image is ultimately acquired through decision map.Using large blocks with 16×16 size, result in high speed process. However, in some areas like what is shown in Fig. 4, there might be an error in selecting a sub-image of the source image of the suggested method. To increase the precision and enhance the operation in edge areas and the boundaries between the areas which are in the focus or out of focus, this step is added to the fusion operation explained in part 3.1.Considering the decision map obtained in previous step, we deal with the points in the borders between the areas of the different source which have high resolution. Having identified these areas, fusion operation is again carried out by blocks with the size of 8×8. Then the combined pixels of this step substitute the pixels of the previous step. Consequently, the final image quality is enhanced.It should be mentioned that the speed of image fusion using this method is approximately two times faster than when the image combination was done with the blocks of 8×8. In addition, considering the objective criteria of measuring the fused image quality, dealt with in part 4, the quality of the output image has no significant quality loss. In Table 3, the time of fusing two images of Pepsi and Clock by the suggested method and/or the method using 8×8 block fusion is explained.The block diagram of the proposed fusion method is depicted in Fig. 5. In this figure the each stage of proposed algorithm is shown in detail.In this part, the experimental results of the suggested method are shown and the method is evaluated in comparison with the present outstanding techniques. The presented methods which were discussed briefly in part 1, are as follow: DCT+Average which is presented in [17]. DCHWT suggested in [11]. LAP, DWT, SIDWT and LMM (laplacian mixture model based algorithm) which is introduced in [6, 7, 8] and [9] respectively. DSIFT, MWGF (multi-scale wieghted gradient-based fusion) algorithm are mentioned in [15] and [18] respectively. All these methods are compared with the proposed method in this paper. This method is called DWT+SV-CV (DWT+ spatial frequency and variance-consistency verification). For all fusion methods, we have used the authors’ original implementations. Image fusion toolbox, provided by Rockinger [22] has been used for LAP, SIDWT and DWT simulation and [23] has been used for LMM method presented in [9] and the presented algorithm in [14,18] is available in [24,25].The sizes of Figs. 5 and 6are 512×512, Fig. 7is 1280×960 and Fig. 8is 640×480, with 256 gray levels.It should be mentioned that Coiflet has been used in the suggested method for wavelet analysis with two decomposition levels.Average pixel intensity, standard deviation, average gradient and entropy are used for evaluation to compare the effectiveness of the proposed fusion algorithm. Structural similarity measure (SSIM) [26] has been used to evaluate the images obtained from the ideal image artificially. The results of which are shown in Table 4.Average pixel intensity measures an index of contrast and is given by:(7)API=∑i=1m∑j=1nf(i,j)mnWhere f(i,j) is pixel intensity at (i,j) and m×n is the size of the image.Standard deviation is the square root of variance, which reflects the spread in data and is given by:(8)SD=∑i=1m∑j=1n(f(i,j)−F¯)2mnAverage gradient measure a degree of clarity and sharpness and is given by:(9)AG=∑i∑j((f(i,j)−f(i+1,j))2+(f(i,j)−f(i,j+1))2)12mnThe Entropy of an image is the measure of information present in an image and is define as follows:(10)H=−∑i=0L−1p(i)logp(i)Where p(i) is the probability of intensity value L in an 8-bit image.However, in a practical situation, an ideal image may not be available. Hence, mutual information, Petrovic and feature mutual information criterions are used as the measures for evaluating the performance.Mutual information (MI) [27] as a concept of information theory indicates the amount of information the fused image F has from source images A and B, which can be calculated as follows:(11)IFA(f,a)=∑f,aPFA(f,a)log2PFA(f,a)PF(f)PA(a)(12)IFB(f,b)=∑f,bPFB(f,b)log2PFB(f,b)PF(f)PB(b)Eqs. (7)-(8) respectively indicate the mutual information between the source image A and the fused image F and the mutual information between the source image B and the fused image F.Finally, the efficiency evaluation measure of fusion method is defined as follows:(13)MIFAB=IFA(f,a)+IFB(f,b)Suppose A and B are input images, while F is a fused image, the contribution of the image A (or B) to the fused image F can be found using FS to evaluate the fusion performance:(14)FS=abs(MAFMI−0.5)Another measure introduced in [28] is called Petrovic measure which measures the amount of information related to the edges transformed from source images (A and B) to the fused image (F).Considering expressing the concept of information in the image by its features, known as feature image, measuring the amount of information transferred from the source images to the fused image can be considered as an appropriate measure in the evaluation of the fused approaches. Feature mutual information (FMI), introduced in [29], provides this.The results of these evaluations are brought in Tables 6–10.Before starting the comparison between the proposed algorithm and the related algorithms proposed recently, we evaluate the algorithm performance with diverse wavelets based on convolution (bior6.8، coif1، coif5، db9 و sym8) with different decomposition levels. The quantitative amounts for the proposed method performance have been given in Tables 4 and 5for the 2 source images Disk (with the size of 640×480) and Clock. As it can be seen, the performance of different wavelets is similar to each other. Among the wavelet methods based on convolution, Coiflet and Daubechies have good performance in QAB/F, AG and MI measures (more information has been transferred from the source images). These two methods present better performance in other measures than the other wavelet methods. It should be mentioned that, due to the closeness of the amounts, the desirable answers can be obtained in two analysis levels.First, the algorithm is done for 20 pairs of artificially blurred images, which these images can be seen in Fig. 6. The images have been obtained with blurredness in right and left halves. The SSIM averages of different algorithm, including the suggested one, are given in Table 6. In the last column titled “DWT+SV-CV” is related to our proposed algorithm. Viewing the amounts of SSIM average in Table 4, it is clear that the suggested method has a remarkable improvement in image quality.To have a more meticulous statistical analysis and evaluation, the standard deviation of the amounts of SSIM of the different algorithms has been given in Table 6. Considering the standard deviation amounts and Table 6 it is understood that the amount of SSIM of the proposed method is high for all the images. Unlike other methods, these amounts of proposed method are close to average SSIM.Second experiment is evaluating of computational complexity of different image fusion approaches. These image fusion approaches are implemented using MATLAB programming language and run on an Intel core i5-2450M, 2.50 GHz and a 4GB RAM. Ten experiments are performed for each of above methods, then the average of their run-time is presented in Table 7. Proposed method is comparable with recent prominent methods.Now, as an example, we evaluate carefully a set of images shown in Fig. 7. The subjective evaluation of the fused results of the different algorithms verifies the amounts obtained from objective amounts.Observing the fused results of Fig. 7, we see a decrease in contrast or blurring of the fused image in DCT+Average method (Fig. 7(c)). Also some artifacts are discerned in DWT, SIDWT, LAP, LMM, DCHWT, MWGF and DSIFT methods, respectively brought in Fig. 7(d)-(j).As it can be seen in the magnified images referred to DWT, SIDWT, LAP, LMM, DCHWT, MWGF and DSIFT, respectively, in Fig. 7(n)–(t), the wavelet based algorithms suffer from a kind of ringing artifact. This ringing artifact is a common problem in wavelet based algorithms and some wavelet based methods (e.g. SIDWT) has somewhat blurring problem. Even sometimes, other artifacts like blocking are seen in these methods (such as LMM method in Fig. 7(q)). Close observation of the images reveals that LMM method produces a sharp and clear image but with artifacts around the edges and severe blocked effect. For example in Fig. 7(g), noise edges can be seen. Similar artifacts are also observable in the flat surfaces.As it can be seen in the images related to DCHWT method in Fig. 7(h) and its enlargement in Fig. 7(r), the method suffers from ringing artifacts. Losing contrast is observable around the edges.By careful observing of the result of fused MWGF method in Fig. 7(s), artificial rings’ creation can be seen. The areas near the edges have blurring problem in this method, decreasing contrast in the borders between the areas.Fig. 7(j) shows the results of the proposed method in [15]. Of the advantages of this method we can point to the good performance in focused and smooth points. Paying close attention to the magnified images related to DIFST method (Fig. 7(t)), artifacts like ringing and blurredness around the borders are discernibleFig. 7(k) is the result of the algorithm in [13]. As it is observable in the enlarged photo (Fig. 7(u)), this method produces artifacts in the areas close to the edges and borders. The quantitative amounts obtained from the method are close to the proposed method in some images but they do not affect the image quality and, with more than three times increase in the speed of the proposed algorithm, the little decrease in the quality can be ignored. It should be mentioned that this decrease in the quality does not take place in all the images. According to Table 4, which contains SSIM average amounts and their standard deviation, the superiority of the proposed method is evident.The results of the suggested method stimulation are given in Fig. 7(l). Viewing the image quality results we can see improvement in the suggested method compared with common methods. Some other experiments have been carried out on recognized images such as Clock, Lab and Book.Similar subjective results are obtained in these experiments as shown in Fig. 8–10. Finally, the objective results related to MI, Petrovic and FMI metrics are given in Tables 8–11. Conventional performance measure, SD is tabulated in Tables 8–11. Theoretically, higher the value of these parameters, better the quality of the fused image. Evaluating of these quantities confirm the subjective results and prove the superiority of proposed method.

@&#CONCLUSIONS@&#
