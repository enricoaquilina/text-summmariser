@&#MAIN-TITLE@&#
Automated retinal layers segmentation in SD-OCT images using dual-gradient and spatial correlation smoothness constraint

@&#HIGHLIGHTS@&#
We develop an automatic method for segmenting retinal layers based on dual-gradient and spatial smoothness constraint.Experimental results demonstrate the effectiveness of our method.Qualitative and quantitative features extracted from images may be clinically useful for analyzing retinal diseases.

@&#KEYPHRASES@&#
Spectral domain optical coherence tomography,Spatial correlation smoothness constraint,Edge flow,Gradient compensation,Automatic segmentation,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Optical coherence tomography (OCT) is an useful noninvasive imaging tool in the field of ophthalmology, especially the spectral domain OCT (SD-OCT) [1,2]. Due to its greater imaging speed and resolution, retinal layer structure can be identified in the SD-OCT retinal images, and high resolution cross-section images can be provided for clinicians.Accurate quantification of retinal structure as observed in SD-OCT imaging can help expand retinal disease research and improve diagnosis. Thus, extracting structural information of retinal layers is becoming increasingly important; such as for quantifying the thickness measurement of retinal layers and for detecting retinal lesions. Hence, segmentation of the retinal layers (Fig. 1) plays a vital role in the quantitative analysis.Due to the difficult and time-consuming nature of performing manual segmentation by experts who yield subjective results, a series of automatic segmentation methods have been proposed to segment retina layers. The initial segmentation methods of retinal layers are based on intensity information [3–17]. The segmentation results of these methods are limited by intensity discontinuity and inconsistencies in the retinal layers. Consequently, several improved segmentation approaches [18–24] have been proposed recently. For instance, Fabritius et al. [21] incorporated 3D information further to improve this method, but it only extracted two boundaries, ILM and RPE. Yang et al. [23,24] utilized local and global gradient information to segment nine boundaries. However, a weakness of this method is that it did not employ the spatial correlation information to smooth the boundaries between adjacent frames.Active contour approaches [25–29] have been used for OCT image segmentation, minimizing the energy function composed by shape prior knowledge, edge information and regional information. However, active contour methods are time-consuming and have limited accuracy, making clinical application difficult. Machine learning and pattern recognition [30–35] have also been used for OCT image segmentation. In practice, these methods can work well if training set contains various examples of retinal images.Graph search based segmentation algorithms [36–45] have been pursued. A 2-D graph segmentation method was introduced by Chiu et al. [37,38] to segment retinal layers from normal eyes and patients with age-related macular degeneration (AMD), showing accurate results. Their method did not use information from neighboring B-Scans when segmenting a volume. Several improved optimal 3-D graph search approaches, including varying feasibility constraints [43], soft constraints [44,45], were designed for multiple surfaces detection, which greatly improved the segmentation accuracy. However, these methods calculated constraint parameters from the prior model that is derived from the labeled training dataset, which makes them time-consuming and susceptible to noise. In addition, Gotzinger et al. demonstrated the use of unique polarization scrambling characteristics to locate the RPE [46], but polarization sensitive OCT was needed to acquire the polarization data.The retinal tissue layers each have distinct layer structure, and our research demonstrates the ability of edge detection to segment retinal layer. To the best of our knowledge, a segmentation algorithm using edge flow has not been reported. The retinal tissue layers may be better segmented using edge flow methods rather than the conventional edge detection methods as shown inFig. 2. Traditional edge detection methods focus on finding the local maxima of the gradient; on the other hand, edge flow methods compute edge energy and direction of flow indirectly to detect and locate the edge of objects in images. Other important information for a successful segmentation algorithm is the local gradient information in the axial direction which can provide useful information that is complementary to the gradient map, and spatial information which can guarantee that the interfaces are smooth.In this paper, we propose a novel method based on edge flow and spatial correlation smoothness constraint for automated retinal layer segmentation. By combining local gradient information in the axial direction, the missing information in the boundaries segmented with edge flow method is compensated by the one without losing boundary details. We utilize spatial information to eliminate outliers and smooth the interfaces. As a result, six retinal layer boundaries, e.g. ILM, RPE-Choroid, OS–RPE, ONL–IS, OPL–ONL, INL–OPL, can be estimated near the central fovea area.The remainder of the paper is organized as follows: the segmentation method is presented in Section 2. Experiments, including quantitative measurements, are discussed in Section 3. Finally, the conclusions are drawn in Section 4.The retina is a biological structure comprising multiple layers, each having different optical reflective properties (different brightness characteristic). For example, the intensity of backscatter light is the largest at RPE complex. As shown in Fig. 1, the change of luminance among adjacent layers is either dark-to-light or light-to-dark.Fig. 3 shows the flowchart of our segmentation method, which comprises three steps: (1) pre-processing is performed to improve the image quality and obtain the image edge; (2) in the main processing stage, six retina layer boundaries are segmented and (3) after segmenting each boundary, anomalous points of the layer boundaries are removed using the spatial correlation smoothness constraint in the post-processing step.The pre-processing stage of our algorithm involves image denoising and edge detection. Sparse 3-D transform-domain collaborative filtering [47] is applied to reduce retinal image noises, as shown inFig. 4(b). This method not only reduces noises inherent to SD-OCT imaging of the retina, but also preserves boundary information and highlights the correlations between layers.Ma et al. proposed a novel boundary detection scheme based on edge flow in 2000 [48]. This method utilizes a predictive model to identify and integrate the direction of change in image attributes such as color, texture, and multi-scale, at each image location. The edge energy and associated probabilities are computed in any image feature space of interest. We apply edge flow method to detect layer boundaries from the denoised retinal image, and use 4-neighborhood to calculate the regional area for removing outliers. Fig. 4(c) shows the boundary detection result of the retinal image. It is difficult to locate inner retinal layer boundaries accurately in the central fovea pit because their curvature translates into low SD-OCT imaging contrast between boundaries and most of the inner layers also have very low thickness in such region, which causes edge discontinuity. Consequently, local gradient information in the axial direction is used to complement the gradient map obtained by edge flow in order to enhance the accuracy of the segmentation.The contrast between the RNFL and the vitreous is high in SD-OCT images because there are almost no high-scattering tissues until light strikes the ILM. Consequently, ILM can be identified by finding the first high-reflectivity pixel from the top to the down in each A-Scan of the edge map. Because the layer boundaries detected by the edge flow algorithm are discontinuous, as shown in Fig. 4(c) corresponding to the foveal region, ILM boundary is estimated inaccurately in the central fovea pit. It is known that the ILM boundary shows a darker layer above a lighter layer [43]. We utilize a gradient map to complement the missing information in the edge image, which can also correct ILM points that far away from the ILM boundary. Let I be the denosied retinal image, the dark-to-light gradient map in the axial direction is computed byd2l=[−11]⁎I, where “⁎” represents the convolution operator. An example of the results of such operation is shown in Fig. 4(d).Estimation of the ILM position is corrected using such dark-to-light gradient map image. LetILMposition(x),x∈[1,M]be the ILM boundary position vector, where M represents the width of retinal image I. For each column, the position distance is defined as(1)Diff=|ILMposition(x)−ILMposition(x−1)|After that, according to the position distance, ILM position is revised as(2)ILMposition(x)={{z|maxz∈Sd2l(z,x)}ifDiff>TILMposition(x)Otherswhere S is the set of pixels within the window size l at the center ofILMposition(x)in x-column,d2l(z,x)is dark-to-light gradient map, T represents threshold of position difference.ILM position matrix is described asILMmatrix(y,x)=ILMpositiony(x),where x∈[1, M] and y∈[1, N] are the column and depth coordinates of the data volume, respectively. N is the position of each B-Scan. We apply 1-D median filter to smooth ILM position matrix in B-Scan direction in order to eliminate some of the outliers. The size of filter template is 5×1 pixels. The final ILM boundary is shown inFig. 5(b).The RPE layer often contains higher gray values with respect to the rest of the SD-OCT image due to its high reflectivity, as shown in Fig. 1. Thus, pixels with highest intensity value in each axial scan are selected to estimate the RPE layer.LetRPEmatrix(x,y)be the RPE position matrix, where x∈[1, M] and y∈[1, N] are the column and depth coordinates of the data volume, respectively. The intensity of the retinal image in each B-Scan is described asIy(z,x). For each column, the largest intensity ofIy(z,x)is defined as{z|maxIy(z,x),x∈M}. The initial RPE position matrix is computed as(3)RPEmatrix0(x,y)={z|maxIy(z,x),x∈[1,M],y∈[1,N]}Because of the speckle noises and signal distortion caused by blood vessels, the first RPE position based on position identification of the maximum intensity pixel is not perfect. To reduce the erroneous pixels, the RPE position matrix is filtered using top-hat filtering [49], which computes the morphological opening of the retinal and then subtracts the result from the original image. The size of the structural elements is 7×7 pixels. An Otsu method [50] can be used to segment the filtered RPE position matrix, which helps to determine the position of erroneous pixels in the RPE position matrix. All positions that are expected to be erroneous in the initial RPE matrix position are filled by the nearest position in 8-neighborhood. The processed RPE position matrix is then smoothed by median filtering to further remove outliers. After the above processing, some erroneous RPE positions also can be located at the IS-OS layer as shown inFig. 6(a). And the RPE positions are fitted using a 2nd polynomial fitting function to the matrix{RPEmatrixfit}. 40 pixels (~80μm) in depth direction around the estimated RPE position are extracted {Iy(z, x)| z∈[RPE matrix fit−10, RPE matrix fit+30], x∈[1, M]} and reprocessed by the above method in order to obtain more accurate RPE position. This process guarantees the probability that the RPE position is identified as the IS-OS layer can be minimized, because the distance between the RPE layer and the top boundary of IS-OS layer is more than 10 pixels (~20μm). The final RPE positions{RPEmatrix}are located at the brightest pixels of RPE layer as shown in Fig. 6(b). Five pixels (~10μm) in depth direction around the final RPE positions are extracted {Iy(z, x)|z∈[RPE matrix−7, RPE matrix−2], x∈[1, M]} to estimate the OS–RPE boundary by finding the first maximum pixel from the edge map. If the maximum pixel (pixel value is 255) cannot be found from the edge map, we utilize RPE matrix−5 to estimate the OS–RPE boundary, and then the OS–RPE boundary is fitted with a 2nd polynomial fitting function to generate the final OS–RPE boundary. The RPE-Choroid boundary identifying extends 10 pixels (~20μm) around the RPE {Iy(z, x)| z∈[RPE matrix−2, RPE matrix+8], x∈[1, M]}, and then we adapt a 2nd polynomial fitting function to smooth the RPE-Choroid boundary. Fig. 6(c) shows OS–RPE boundary and RPE-Choroid boundary.ILM and RPE boundaries are the two most prominent layer boundaries in the SD-OCT images because their intensity contrast is normally the highest. We define a bottom-to-up search strategy for the rest of the retinal boundaries to be identified, in which starting from the RPE, the ONL–IS, OPL–ONL, and INL–OPL boundaries would be encountered in such order. We introduce a method for defining a valid search region that determines the boundary of interest to accurately segment these remaining layers. The main principle of our method is based on prior knowledge of the layer thickness. Twenty pixels (~40μm) in depth direction around the estimated OS–RPE boundary are extracted {Iy(z, x)| z∈[OS/RPE−30, OS/RPE−10], x∈[1, M]} to estimate the ONL–IS boundary by finding the first maximum pixel in the edge map. Due to speckle noises present in SD-OCT images, erroneous pixels would be identified as the ONL–IS position. We corrected this erroneous detection by applying a 1-D median filter to smooth the ONL–IS position in each B-Scan. The size of filter template is 5×1.After detecting the ONL–IS boundary, 10 pixels (~20μm) around the estimated ONL–IS boundary are extracted {Iy(z, x)|z∈[ONL/IS−45, ONL/IS−35], x∈[1, M]} in order to detect the first OPL–ONL position by searching the largest intensity from edge image. Due to the discontinuity of the boundaries obtained by the edge flow method, it is difficult to estimate the ONL–IS boundary accurately. To solve this problem, we introduce a method for searching the largest intensity at a local region that determines the boundary position of interest in the A-Scan of the denoised image. The revised OPL–ONL position is defined as(4)OPL−ONL(x)={z|maxz∈S(Iy(x,z))}where S is the set of pixels within the window size l at the center ofOPL−ONL(x−1)in the x-column. We utilize 1-D median filter to smooth the revised OPL–ONL position and remove erroneous pixels in each B-Scan. The size of filter template is 5×1.Fig. 7(a) shows the final OPL–ONL boundary.We also utilize the above method to estimate INL–OPL boundary. Ten pixels (~20μm) around the OPN–ONL boundary are extracted {Iy(z, x)|z∈[OPL/ONL−15, OPL/ONL−5], x∈[1, M]} to detect the INL–OPL boundary by finding the maximum intensity (pixel value is 255) from the edge map. If the maximum intensity (pixel value is 255) cannot be found from the edge image, we utilize OPL/ONL−10 to estimate the INL–OPL boundary, and then adopt a 2nd polynomial fitting function to smooth the INL–OPL boundary. Finally, 1-D median filter is employed to smooth the INL–OPL position and remove erroneous pixels in each B-Scan. The size of filter template is 5×1. Fig. 7(b) shows the final INL–OPL boundary.From Fig. 7(b), we can see that the INL–OPL boundary is estimated near the macular fovea inaccurately. To solve this problem, we first estimate the INL–OPL, OPL–ONL layer boundaries using the edge detection and spatial constraint information proposed in Sections 2.3 and 2.4. Next, we detect the presence and location of the fovea by calculating the distance between ILM and INL–OPL. The columns with layer thickness of less than 2 pixels (~4μm) can be considered as a fovea region. After locating the fovea, we utilize the ONL–OPL boundary position in the fovea region to subtract 5 pixels (~10μm) to re-estimate the INL–OPL boundary, and then the INL–OPL boundary is smoothed by a moving average filter. The resulting segmentations are more accurate, as depicted inFig. 8(b).We tested our method by compiling a dataset that included 10 SD-OCT cubes from 10 healthy eyes and 15 SD-OCT cubes from 15 eyes in 6 patients examined at different times with non-exudative age-related macular degeneration (AMD), resulting in a total of 1280 and 1920 B-Scans, respectively. All of the cubes were obtained using a Cirrus SD-OCT device (Carl Zeiss Meditec, Inc., Dublin, CA). Each SD-OCT cube contained 128 contiguous 512×1024 pixel B-Scan images (each B-Scan comprising 512 A-Scans containing 1024 pixels). The proposed method was utilized to segment all of the retinal images in each cube. We randomly chose five cubes (each cube containing 128 frames) from healthy patients to constitute a dataset of 640 frames where our algorithm was quantitatively evaluated. These selected cubes were representative of the quality and noise levels that occur throughout our collection of images.The algorithm was implemented in Matlab and run on a 2.53GHz Intel© Xeon© PC with 16GB memory. Although our method achieved an accurate segmentation result, the average computation time was about 60s to estimate six layer boundaries per retina image (1024×512). This computation time was slightly longer than it in literature [37] (the average computation time was about 40s per retina image) in the same experimental environment. However, the purpose of our algorithm was to achieve a high segmentation precision for clinical application and not to compete on execution time.In order to quantitatively evaluate the performance of our method, we compared our segmentation results with graph-based segmentation results [37] and manual segmentations labeled by expert readers on 640 images. The manual segmentation results were provided by two experienced and well-trained expert readers with the use of a computer-aided manual segmentation procedure. In order to ensure the accuracy of the graph-based segmentation method, we download the source code from the page of project Caserel “http://pangyuteng.github.io/caserel/”. The evaluation criteria were based on the following:(1)Evaluation of the precision of boundary location by calculating mean absolute boundary positioning differences between automatic segmentations, graph-based segmentations and manual segmentations.Assessment of the segmentation error of retinal layers between different segmentation methods by calculating the overlap ratio, over-segmentation ratio, under-segmentation ratio.Analysis of thickness measurement of retinal layer to establish the thickness map for the physician, which provides the basis for diagnosing retinal diseases caused by the abnormal retinal thickness.We evaluated inter-expert variability and differences between automatic segmentation results, graph-based segmentation results and manual segmentation results.We employed three metrics to assess boundary differences between pairs of segmentation methods: correlation coefficient (cc), p-value, mean absolute boundary difference. LetXijandYijbe the coordinates of the ith column in the axial direction of B-Scan j, produced by the segmentation methods X and Y, respectively. The mean and standard deviation values were computed across the different B-Scans:(5)mean(X,Y)=1L∑j=1L1n∑i=1n|Xji−Yji|(6)std(X,Y)=1L∑j=1L(1n∑i=1n|Xji−Yji|−mean(X,Y))where L is the width of each A-Scan.Mean absolute boundary positioning differences for each boundary are computed and are presented inTable 1. We compared the segmentations produced by our method with averaged segmentations generated by averaging the two segmentations drawn by the two experts across Eqs. (5) and (6) to gain a result expressed as auto vs. avg.. Mean absolute boundary differences of our method were less than those of graph-based segmentation algorithm. For example, overall mean absolute boundary difference of our method was 4.43±3.32μm, while mean absolute difference in literature [37] was 6.29±4.71μm. Since the published method for graph-based algorithm [37] does not segment the OS–RPE boundary, the mean absolute difference was expressed as “NULL”. The results in Table 1 shows that the automatic algorithm can accurately estimate six interfaces (4.43±3.32μm) more closely to the manual segmentations (3.74±2.78μm), and better than the graph-based method.Fig. 9 shows the performance comparison of our method, graph-based segmentation method [37], and the inter-expert variability.Table 2 summarizes the variability evaluation in the retinal layer boundary segmentation computed by two different experts (inter-expert variability) and different segmentation methods. The high p-values in the U-test indicate that there were no statistical differences in the distribution of areas. The manual segmentation procedure by independent readers showed a low inter-experts variability because the correlation coefficients (cc) between boundaries computed by two experts were all more than 0.9995. However, the cc between boundaries computed by our method and those produced by the experts were high, and approximated those generated by experts for the same dataset.The layer segmentation differences were studied to further assess the performance of our method. We utilized three metrics to evaluate the layer segmentation error between pairs of segmentation methods: overlap ratio (Overlap), over segmentation ratio (OverSeg), and under segmentation (UnderSeg).(7)Overlap(reg(i,j1),reg(i,j2))=(reg(i,j1)∩reg(i,j2))(reg(i,j1)∪reg(i,j2))(8)OverSeg(reg(i,j1),reg(i,j2))=((reg(i,j1)∪reg(i,j2))−reg(i,j2))(reg(i,j1)∪reg(i,j2))(9)UnderSeg(reg(i,j1),reg(i,j2))=((reg(i,j1)∪reg(i,j2))−reg(i,j1))(reg(i,j1)∪reg(i,j2))wherereg(i,jk)represents the number of pixels in a layer region i segmented by the methodjk, the operators ‘∪’ and ‘∩’ indicate union and intersection, respectively. The Overlap represents the ratio of area between the regions outlined by any of the two algorithms. The OverSeg and UnderSeg provide complementary information for the analysis of segmentation differences. For example, if we have a high over segmentation ratio with a low under segmentation ratio, it means that the layer thickness measured by our method is larger than it measured by manual segmentation method.Figs. 10–12 show the errors of retinal layer segmentation. It can be observed that the overlap ratio of each layer was more than 70% except the OPL layer, where the overlap ratio was also close to 70%. The overlap ratio of RNFL–INL reached 93%, and those in ONL and IS-OS layers were more than 80%. Moreover, the under segmentation ratios were all below 10%, which indicates our algorithm can accurately detect each retinal layer. We obtained slightly worse segmentation for the OPL layer, indicating a high over segmentation ratio, which can be explained by the differences of the OPL layer estimated by our algorithm and manual segmentation method using the thickness of the OPL layer. Indeed, the results of the automatic segmentation were very close to the manual ones.The change in the thickness of each layer objectively reflects an important indicator for a variety of ocular fundus pathology. For example, macular edema will increase the retinal nerve cortical thickness; On the contrary, glaucoma will cause the retinal nerve fiber layer to become thin. The thickness of each retinal layer is computed using the adjacent layer boundary positions, which can be expressed as follows:(10)thickness=1L∑j=1L1n∑i=1n|downij−upij|where thickness denotes the thickness of the layer, and L is the width of each A-Scan.upijanddownijrepresent the top and the bottom boundaries of the layer, respectively. The absolute differences of layer thickness (AD) are computed as defined:(11)AD(x,y)=|thickness(x)−thickness(y)|where thickness(x) and thickness(y) indicate the layer thickness produced by the methods x and y.The retinal layer thickness differences are shown inTable 3. The analysis results indicated that the difference between total retinal thickness measured by our method and that assessed by two experts was lower than the difference measured between two experts. For example, the overall mean absolute thickness difference of our method was 0.22±0.24μm, whereas the overall mean absolute thickness difference assessed by two experts was 2.13±0.35μm. The mean absolute thickness difference of graph-based segmentation method was quite large (12.86±0.95μm), because the RPE-Choroid boundary was segmented inaccurately as shown in Fig. 15(c). The quantitative results indicated that our method provided highly accurate results for clinical application. Repeating the segmentation in the same images several times and obtaining the same results does not mean repeatability. An analysis of repeatability would include several images from the same patient taken minutes within each other, and showing that the results are very similar. Our datasets do not contain these images, making us unable to analyze repeatability of our algorithm.To evaluate the quality of the segmentation process, 3 cross-section images (20, 65, 125 frames) from the same cube with six boundaries superimposed on them are shown inFig. 13. It can be observed that the layer boundaries were identified by our method near the fovea region.Fig. 14 shows two examples of segmented results using our method and graph-based method. The illustrative results of the proposed algorithm in comparison with manual segmentation and graph-based method are shown inFig. 15. The segmentation results with graph-based method might not be perfectly precise as shown in Fig. 15(c), which will cause the total mean absolute thickness to become thin (149.24±12.31μm). Furthermore, the layer boundaries lacked smoothness. For example, the ILM boundary was sharp near the macular fovea, and the OPL–ONL, INL–OPL boundaries appeared jagged. On the other hand, our method can smoothly and accurately estimate six boundaries more closely to manual segmentations. This evaluation of our method vs. graph-based method is strictly limited to the images presented in our dataset. And that the graph-based method may be better for the segmentation of other images.Because layer segmentation was performed over the whole image area, we calculated 2D position maps of each retinal layer. The position of each layer boundary was given by the depth from the top of the cross-section image. The obtained ILM, ONL–IS and OS–RPE position maps and total retinal thickness map as shown inFig. 16. We utilized the position of the RPE boundary to subtract the position of the ILM boundary to obtain the total retinal thickness maps, while the scale bars indicated the distance between two boundaries.The robustness of the algorithm to blood vessels has been discussed in some papers [19,23,36,37], and some researchers tried to present a post-processing method to reduce the effect of blood vessels [19,37]. However, the edge map and spatial constraint information make our method insensitive to blood vessels, and our method can obtain the desired layer boundaries, as shown inFig. 17.We employed the proposed method to segment the SD-OCT images with retinopathy or vascular disease to evaluate the potential clinical utility of our method. Because the retinopathy changes the structure of retinal layers, our method is unable to estimate some layer boundaries, such as OPL–ONL, INL–OPL. The segmentation results for a variety of SD-OCT image types are shown inFig. 18, including retinal images with Drusen (Fig. 18(1)), retinal images with geographic atrophy (GA) (Fig. 18(2)), retinal images with Drusen and GA (Fig. 18(3)). From the results, we found that the ILM segmentation was successful; only a few segmentation errors can be seen. On the other hand, due to the distortion of the RPE layer caused by the Drusen and GA, evaluating the quality of RPE segmentation was not possible. Nonetheless, the layer boundaries were estimated by our method in the lesion region, and seemed to be very reliable.

@&#CONCLUSIONS@&#
In this paper, a novel algorithm for segmenting retinal layers in SD-OCT images has been proposed. Our algorithm is mainly based on dual-gradient and spatial information, ensuring a continuous boundary for retinal layers in all images, and has several advantages as follows: (1) The retinal tissue layers each assume a lamellar structure. Thus our method employs a customized edge detection method to segment retinal layers. Compared to the traditional edge detection methods that find the local maxima of the gradient in the image feature space, a customized edge flow utilizes a predictive coding scheme to detect the direction of flow and construct an edge flow energy function for detecting and locating image boundaries. Therefore, the accuracy of localization can be improved, as well as the probability of finding the erroneous boundaries decreases, as shown in Fig. 2. (2) The local gradient in the axial direction could provide compensation information for the missing boundaries segmented by the edge flow method, which is able to detect proper retinal layer boundaries. This may be even more useful in cases for which layers are not visible due to blood vessels, as illustrated in Fig. 16. (3) The spatial information between adjacent frames is utilized to construct a spatial correlation smoothness constraint, which could correct the boundary that far away from the true boundary and make the layer boundaries smooth.Quantitative and qualitative experimental results demonstrate that the proposed method can achieve high segmentation precision for normal cases when compared with segmentations from two experts, with the segmentations produced by our algorithm being very close to the manual segmentations. Thus the proposed algorithm may be a useful clinical tool for SD-OCT image studies. While the proposed algorithm can be easily applied to a number of image segmentation, it also has limitations. The proposed method employs the edge map and local gradient information to achieve the segmentation. The low contrast of intensity near the fovea region will lead to a weaker edge map, which may cause an inaccurate segmentation. Furthermore, obtaining a gold standard for comparison is very difficult since there are inter-reader differences.We have not conducted a thorough evaluation of the method in diseased retina; for the pathological images, the proposed algorithm was only applied to AMD images. Further research will be needed to improve our algorithm to estimate the retinal layers in a variety of diseases. Additional future work will also be to analyze the variation of each retinal layer in different retinal diseases using our segmentation results, as well as improving the computational efficiency of our algorithm.The authors declare that they have no conflict of interest.