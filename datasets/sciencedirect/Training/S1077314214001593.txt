@&#MAIN-TITLE@&#
Discovering salient regions on 3D photo-textured maps: Crowdsourcing interaction data from multitouch smartphones and tablets

@&#HIGHLIGHTS@&#
We model human interest on 3D models using multi-touch interactions from tablets.We create a crowdsourcing system to gather interaction data from thousands of users.We propose using the view frustum and a Hidden Markov model for calculating saliency.We present results comparing the proposed interest model to traditional visual saliency.We report results demonstrating the proposed techniques on over 500,000 interactions.

@&#KEYPHRASES@&#
Crowdsourcing,Visual saliency,3D maps,Multitouch interaction,HMM,Gaze-tracking,Smartphones,Mobile devices,

@&#ABSTRACT@&#
This paper presents a system for crowdsourcing saliency interest points for 3D photo-textured maps rendered on smartphones and tablets. An app was created that is capable of interactively rendering 3D reconstructions gathered with an Autonomous Underwater Vehicle. Through hundreds of thousands of logged user interactions with the models we attempt to data-mine salient interest points. To this end we propose two models for calculating saliency from human interaction with the data. The first uses the view frustum of the camera to track the amount of time points are on screen. The second uses the velocity of the camera as an indicator of saliency and uses a Hidden Markov model to learn the classification of salient and non-salient points. To provide a comparison to existing techniques several traditional visual saliency approaches are applied to orthographic views of the models’ photo-texturing. The results of all approaches are validated with human attention ground truth gathered using a remote gaze-tracking system that recorded the locations of the person’s attention while exploring the models.

@&#INTRODUCTION@&#
We have developed a smartphone/tablet app for the viewing and manipulation of 3D models gathered with an Autonomous Underwater Vehicle (AUV). This app is freely available and has been downloaded and used by a large number of users. The question this paper is attempting to answer is “Can we employ crowdsourcing to perform salient interest point detection from users not specifically tasked to find these points?” A diagram depicting the high-level system presented in this work is shown in Fig. 1.Saliency, particularly visual saliency is a popular construct from the field of biological vision and broadly describes an organisms ability to focus attention on a subset of its sensory input for further processing. In this work data subsetting is the most relevant part of the visual saliency process. While scientists and non-experts will have differing opinions on the high level top-down definitions of saliency, rapid bottom-up visual saliency is much less task and operator dependent [49]. This paper is focused on such processing in the context of a long-term environment-monitoring program using AUVs. At the Australian Centre for Field Robotics there is an ongoing program to perform benthic monitoring with an AUV [73]. This program deploys an AUV in unstructured natural environments where it gathers data for human review. One of the major bottlenecks in this process is the vast amount of data gathered by the AUV. The AUV is capable of gathering orders of magnitude more data than previous techniques. Traditionally divers used hand-held cameras to gather visual data in underwater environments and issues of decompression, airtime, and safety severely limited the quantity of data that scientists could gather. With the AUV in its current configuration, monitoring images can be gathered at up to 4Hz. A typical field campaign lasting two weeks can result in hundreds of thousands of images requiring review.The challenge of how to deal with this massive image archive is being explored on several fronts. A large effort has gone into unsupervised clustering [64], human hand labeling [46], and supervised classification [4]. This work presents an alternative for gathering large amounts of human review data quickly and inexpensively. The assertion we present in this paper is that human visual saliency can be modeled by proxy through the exploratory motions of a large number of users in a 3-D environment.Capturing human curiosity and exploration for robotic platforms is non-trivial. The well established approach is to use visual saliency measures but it is not necessarily clear that they can predict what people find interesting in a 3D scene and how they will choose to interact with it. This paper presents two alternative measures of human interest both based on the motion of the viewpoint used by the operator and compares them to traditional saliency measures. Through the crowdsourcing of many remote smart phone/tablet users we gather data to perform the identification of visual saliency on 3-D photo mosaic maps. Human experiments with ground truth from eye tracking are used to validate our results.Crowdsourcing has emerged as a successful model for solving tasks by leveraging the human intelligence of large groups of remote users in a distributed fashion. The term crowdsourcing was coined in 2006 and first appeared in scientific literature in 2008 as “an online, distributed problem-solving and production model” [6]. The crowdsourcing model has since been adapted to outsource difficult steps in many computational tasks [32]. Recently the computer vision community has begun using crowdsourcing to solve challenging vision problems.In parallel, researchers have started harnessing the power of data-mining over massive user bases to answer many new questions. Search engines and social networking use the interaction from millions of users to refine and improve advertising and site usability [17]. Researchers have used this data to learn about the demographics of users, social trends, and behavioral patterns [41,65]. With the rise of smart phones and ‘app stores’ mobile platforms have quickly become a practical means of gathering massive amounts of user data. App analytics is attempting to turn the millions of smart phones in use into a distributed network of data sources.Traditional crowdsourcing of vision tasks relies upon motivating users through community good will [59], financial incentives (Mechanical Turk) or competitive/entertainment incentives [1,2] by turning a task into a game. The intended motivation for users of our app was education and entertainment. The app was advertised in the education section of the Apple iTunes app store and in its description and screenshots offered the promise of exploration of images from the deep sea. We attempted to capitalize on public interest in science, especially exploratory science, to motivate downloads. A novel aspect of our approach is that the motivation of users was somewhat more decoupled from the task than in a traditional crowdsourcing model. To work with such user data we propose the use of a novel paradigm from big data analytics where the answers to questions can be inferred from the data of many users. The power of our data-mining approach to crowdsourcing is that data is collected from a much larger pool of users. A full discussion of the motivations and demographics of users on various crowdsourcing platforms is beyond the scope of this paper, however Kaufmann et al. [30] present a review of the studies on Mechanical Turk. While these studies reflect a diverse user pool they also show that the Mechanical Turk user base is a fraction the size of the potential smart phone app user pool [31,56]. Using the smart phone platform gives us access to a much more general audience. To further the general appeal of the app we do not ask users to explicitly identify things they find interesting. Rather, we attempt to infer interest from patterns of interaction and in doing so free the user from an artificially constrained task. Without asking users to answer a specific question, their motivations for participating can be much more varied. This potentially gives access to a much larger ‘crowd’.We will be presenting two novel metrics to calculate saliency from human user interaction data. One employs the use of the camera’s frustum to histogram observed points, while the other leverages a Hidden Markov model (HMM) to classify interaction data spatially into a saliency map. These techniques are compared to several state-of-the-art visual saliency techniques and validated using human gaze tracking data. The paper is laid out as follows. Section 2 discusses prior work. Section 3 presents the developed app as a platform for crowdsourcing. In Section 4 the two interaction-based formulations for saliency are laid out. The human trials for validation are discussed in Section 5. Results are presented in Section 6 and finally Section 7 concludes and presents future work.Tools such as LabelMe, ImageNet, BUBL and other systems which leverage Amazon’s Mechanical Turk have provided solutions to the problem of image-labeling using human computation [18,14,33]. Mechanical Turk has become a particularly popular platform for crowdsourcing for vision. It offers flexibility and there has been research into assessing, processing, and rectifying image labelings from large groups of human sources [63,71,55]. All the aforementioned systems deal with image annotation with various types of semantic information ranging from object identification, object classification, and object segmentation.In the field of gaze tracking Rudoy et al. propose a relevant model of crowdsourcing gaze tracking. They project a pattern over video or image data. Then a ‘crowd’ of remote users enter the subsection of the pattern viewed providing a proxy for direct gaze tracking [58].Research on the human perception system has shown that it is selective in its attention [69]. Human perception focuses on salient regions of an image and there has been a great deal of literature published proposing models for that process [13,25,67]. Such work produces a saliency map for an image whereby the relative saliency of each pixel is expressed. The research is coarsely divided into techniques that exploit bottom-up cues and techniques that attempt to learn top-down high-level saliency measures [37,16,10]. These measures have then been applied across a range of applications including retargeting, tracking, compression, and object recognition [36,38,7].Interaction-based saliency is the process of extracting saliency metrics from the way a user interacts with an image, video, or 3D model. Existing research on interaction-based saliency is primarily focused on 2D images and videos, not 3D models, however many relevant analogues exist. For 2D visual data Zoomable User Interfaces (ZUIs) or interfaces that allow users to zoom and pan around a large image or video have gained popularity in recent years. Utilizing these new interfaces several techniques have been developed to gather metadata for a piece of media being viewed in a ZUI. Carlier et al. propose a system where users watch a video and can retarget (zoom and center) it using the keyboard. Users input is then aggregated to produce a global retargeting for the video [8,9]. Similarly, Cricri et al. propose the use of implicit user region-of-interest data by detecting overlapping regions and concurrent events in multiple synced videos and attitude heading reference data streams [12]. In addition to these videos stream retargeting projects, a 3D distributed camera based saliency model was proposed by Park et al. where camera positions and viewpoints are correlated to determine regions of interest in 3D using intersecting camera rays [52]. In another video based technique Lee et al. learn a regressor over several high-level saliency features to predict salient people and objects from egocentric videos streams [34].For static images, one of the most relevant works to this study is that of Xu et al. who propose the use of ‘Touch Saliency’ by capturing the center and size of zooms to produce saliency maps which are compared to traditional image saliency measures using eye-tracking data [74]. In a non-touch based interface Baccot et al. proposed using a smart phone’s picture browsing functionality to capture and store zoom and pan data. This data was then relayed to a central server to produce user interest maps [3].In the 3D realm [48] propose a system for online shopping interfaces where users’ interactions with a 3D model are recorded and aggregated to propose a recommended view of the model to future users.Mobile platforms like smart phones and tablets have advanced significantly in the last 10years. The incorporation of faster processors and mobile GPUs have greatly increased the power and range of applications that can be run on such platforms. The ubiquity of maps and mapping applications for smart phones has familiarized a broad base of users with the navigation and interpretation of 2-D maps. Recently 3-D maps have been introduced on the major smartphone platforms. This phenomenon is increasing users’ familiarity with navigating in 3-D textured environments.For this work we have created an app that allows users to explore and navigate a 3-D photo-textured model of the seafloor. A screenshot of it running can be seen in Fig. 2. Written in Objective-C and using OpenGL ES (OpenGL for Embedded Systems) it runs on both phones and tablets. The app is downloadable for free.1http://www-personal.acfr.usyd.edu.au/mattjr/app.html.1Named SeafloorExplore the app was released into the Apple iTunes app store in 2012. The app itself uses virtual texturing [44,47] and a static Level-of-Detail (LOD) hierarchy [11] to be able to display models of up to 1000km2 with textures up to 128k2 pixels. It is only more recent phone models that have the GPU capability to be able to display 3-D models of such size and scale. It is these advances that are driving the slow but gradual adaptation of 3-D maps on modern phones, replacing their 2-D analogs.The app gathers and logs interaction data from users. This data is periodically relayed back to a central server. The data is aggregated and compressed to minimize network traffic.The models used in the app are created from data gathered in-situ beyond diver depths with an AUV. Once the AUV has completed an image gathering mission the vehicle is retrieved and the data downloaded. The AUV is equipped with a suite of navigation sensors: a 3-axis roll/tilt sensor, an acoustic Doppler Velocity Log (DVL), and a magnetic compass. These sensors along with the visual imagery are used to estimate poses for the vehicle.The pose estimation process is performed in two steps. Firstly the data from cameras, DVL (with the AUV) and tilt sensor are fed to a Simultaneous Localization And Mapping (SLAM) filter. A sparse information-form pose-based SLAM algorithm [40] estimates the pose of the cameras for each pair of stereo images. Once this SLAM filter has completed we perform a second refinement step where the poses are optimized to further reduce the reprojection error. This second step takes the form of a sparse bundle-adjustment globally optimizing the poses given stereo matched scale-invariant feature transform (SIFT) features [66,39].Once an optimized set of poses exists we employ the 3D reconstruction technique proposed by Johnson-Roberson et al. [28] and later extended to full 3D by Johnson-Roberson et al. [29] producing a high-resolution textured structural model. The approach breaks down the problem into manageable steps that allow for out-of-core processing. It first estimates local structure and then combines these estimates to recover a composite georeferenced scene using the SLAM-based vehicle pose estimates. Lighting correction and band-limited blending are applied to produce a texture-mapped surface at multiple scales which can be visualized efficiently. Through the use of a state-of-the-art model parameterization the distortion of mapping the 3D model to a 2D texture is minimized while attempting to maintain the resolution of the original source imagery [29,35,61]. A sample 3D model can be seen rendered on the screen of the iPad in Fig. 2.For multitouch interfaces gestures for 3D interaction are more variable across platforms and applications than for 2D interaction. Several options appear in the literature. Hancock et al. propose shallow depth manipulation with one to three fingers [19]. Martinet et al. propose selecting an object with one finger and indirectly manipulating it with another [42]. We have selected a terrain centric interaction model which emphasizes the separation of degrees of freedom [43]. This model affords the user three gestures for three types of camera movement: pan, tilt/rotate, and zoom. Each type of motion is represented as a discrete value of the variablemt. All operations occur around a point on the model which we will refer to asP0. The camera is pointing atP0and is drawn back along a ray a distance d. The ray’s angle with respect to the model both vertically and horizontally are expressed as λ and ϕ respectively. This model is depicted in Fig. 3.Pan is performed using a single finger. When the user places their finger on the screen a ray is projected into the scene and intersected with the model. If the user’s first touch point does not hit the model the ray is intersected with a plane spanning the x and y axes at the average z-depth of the model. This first intersection point forms the starting camera location for a model translation. As the user drags their finger we perform a camera transformation such that the original point remains under the user’s finger, but the camera’s centerP0in Fig. 3 is changed. This type of motion has an intuitive feel for an object like a terrain model.Zoom is performed similarly to the two-dimensional case. A pinch gesture is used to change the distance d of the camera along the ray between the center of projection and the model in Fig. 3.Tilt and rotate are both performed using a two-finger drag. We have chosen to separate each axis in x and y on the screen into tilt and rotate respectively. That is moving vertically on the screen modifies λ and moving horizontally modifies ϕ as shown in Fig. 3. The limits of tilt are stopped at 90 degrees to prevent going through the model.One advantage of this camera model is that it is a compact representation of the camera position. The parameters to reconstruct the camera’s movement:{mt,P0,d,λ,ϕ,t}are stored continuously along with the time t. This data consists of seven 32-bit floats and one charmt(noteP0is a vector inR3). It is important to keep a compact representation as this data is transmitted (often over low-bandwidth mobile internet) back to a central server. Once aggregated on the central server it can be downloaded and used as the source data for the proposed saliency detection algorithm.The app SeafloorExplore has been available in the Apple iTunes app store since June 2012. Without any additional advertising beyond what occurs automatically in the app store it has gathered over 5000 unique users from all over the world. Each month between 500 and 1000 sessions occur each gathering interaction data. The vast majority of users are casual and only play with the app once or twice. This can be seen in the pie chart in Fig. 4which breaks down the number of users that have n sessions logged. Despite the casual nature of most of the use 15,000 total sessions have been logged. Furthermore there have been 491,232 total camera movement events recorded and transmitted to the central server. On average 32.27 events are logged per session. This has, without direct request for labeling, generated a massive data set for the saliency analysis.Building upon the work in interaction-based saliency discussed in Section 2.3 this section will propose two novel frameworks that attempt to capture the notion of saliency using 3D camera motions (as described in Section 3.2) crowdsourced from the developed app. While traditional visual image saliency experiments rely on static images, this paper presents two key differences to that work. First, 3-D photo-textured models are used which means that not only do intensity and color play a role in saliency, but depth and relief contribute to what is or is not interesting. Second, the maps employed in this work can be thought of as multiscale. The resolution and extent of the maps is larger than could be captured by the eye or the screen in any single view. With such maps there is a relationship between a low-resolution overview and a high-resolution zoom that the user explores by navigating the model. Very simply we make the assumption that the user will attempt to see a higher resolution version of something they are interested in. To this end the multitouch interface (described in Section 3.2) along with the virtual texturing renderer (see Section 3) allow for the exploration of the model at arbitrary resolutions.One key assumption of this work is that users are looking at what they find interesting. In a static single image set up, when the users gaze is tracked, this assumption holds very well. We assert that while this assumption may not hold for any single user selected at random from a pool of remote users it does hold across the group. We assume on average most people are looking at what they find interesting. Usage patterns that violate this assumption do exist. A user may be:•attempting to learn the interface and simply performing actions to understand the mapping between touch and motion, moving randomly not understanding or interested in what they are viewing,have put the device away while the app is still active (which is handled through a rejection of data temporally close to a device lock event),have a gaze pattern that is very uncorrelated with motion.In Section 4.1 we will attempt to show these outlier usage patterns are not the dominant trend in the data and that with a sufficiently large amount of users such patterns do not affect the outcome of the analysis.The first proposed technique which we will refer to as the frustum method uses the camera’s view of the scene to model human interest. This idea is based on the simple principle that users will move the camera to cover areas of the scene they are interested in looking at. The more time a point appears in view the more ‘salient’ it is said to be. To begin the algorithm we initialize every 3D vertex in the scene to have a time counter of zero. Each interaction the user completes triggers the recording of the current camera parameters. These parameters are used to calculate the screen visibility of the vertices of the model through an inverse camera projection. The time between camera moves is recorded. This allows for the increment of a vertex’s counter by the number of seconds it appeared onscreen (i.e. the longer the vertex is in the camera’s view the greater the increment of the counter). After the camera motions from all users have been processed we have the screen time for each vertex in the 3D model. To create a 2D map for comparison to the baseline techniques we spatially histogram the vertices of the model under an orthographic projection. In the map the bins of the spatial histogram contain the sum of all the vertex time counters in that bin. We experimented with several weighting schemes that increased a vertex’s weight in proportion to its distance from the center of the camera. These other methods had a negligible impact on performance and ultimately a uniform time-based weighting proved most effective. This trivial algorithm produced quite compelling results, however a more complex model of the camera’s motion is presented in the following section.Here we present a second technique to capture the notion of saliency from interaction. We propose the use of a Hidden Markov model to capture the motion of the camera through time. Hidden Markov models (HMMs) are probabilistic state machines that have been used extensively for the classification of time series data. Most notably, HMMs have been used for speech recognition and bioinformatics to great success [54,15]. We apply them here to give us a formulation that can learn the relationship between motions which indicate interest: such as spending a long time rotating around a point, zooming in and out of a point, or tilting about a point. To capture these more complex chaining of motions the camera’s path can be thought of as a time series. Through this time series the user is either interested in a single point or moving around looking for something new to explore. The amount of time spent on a point and the motions that occur near it provide a strong indicator of the user’s intentions with respect to that point.In an HMM the Markov property is assumed for the modeled system. The model contains a sequence of observable data and a set of unobserved states. In this formulation the unobserved or hidden states take on binary values: salient or non-salient. The input, observable data, is in this case the camera motions gathered from a user. A HMM is a probabilistic model and three sets of probabilities determine the classification of the input data. The first are the prior probabilities of the states. Here we assume a uniform prior over the states. The second are the observational probabilities. These probabilities describe the likelihood that an observation corresponds to a certain state. Third are the transitional probabilities. These probabilities describe the likelihood of a transition between two states.For the classification of saliency a two state HMM is proposed. A diagram of the proposed two state HMM is shown in Fig. 5. A two-state HMM consists of two random variables O and Y which are both sequences. The variable Y consists of states{y0,y1,…,yn}. Y changes sequentially with the first order Markov property. This means the probability of a state change in Y only depends on its current state.Formally:P(Y(t+1)=yi|Y(0)…Y(t))=P(Y(t+1)=yi|Y(t))These states can take two values s (salient) and n (non-salient).Formally the hidden state Y is:Y=ssalientnnon-salientThe observable data is O a set of continuous value variables. Here it is the camera motion feature data as described in Section 4.4.Three important questions can be asked of an HMM. What is the probability of an observed sequence O? What is the most likely series of states Y that would generate the observed data? And finally, can we learn the parameters for the HMM given a set of data?Here we are interested in finding the single best state sequence Y, for the given observation sequence O. Formally this can be written asargmaxYP(Y|O)Naively we could try every possible combination of states Y that maximize the joint probability of the model. Practically this is computationally expensive. However, the structure of the model can be exploited to formulate a more efficient solution. The probabilityP(Y|O)can be expanded out using Bayes’ rule:P(Y|O)=P(O|Y)P(Y)P(O)In the maximization ofP(Y|O)the constantP(O)can be eliminated and the optimal Y (denotedY^) can be computed using:Y^=argmaxYP(O|Y)︸observation·P(Y)︸transitionThe transition probabilitiesP(Y)are parameterized by a2×2matrix, A:A=assasnansannas the following notation generalizes to an n state HMM we will refer to the s-state as state 1 and the n-state as state 2 thereforeaijin A refers to the probability of transitioning between state i and j where these indices correspond to the rows and columns of A.The observation probabilitiesP(O|Y)are defined by a vector B composed of continuous probability density functions (pdf), whereB={bj(ot)}Every state j has an associated probability distributionbj(ot)which is the probability of generating observationotat time t.The representation for the pdf that is used is a mixture of 3D Gaussian distributions:bj(ot)=∑l=1Lcjl×N(ot,μ→jl,Σjl)where L is the number of mixtures,cjlis the mixture coefficient for the l-th mixture in statej,μ→jlis the mean vector for the l-th mixture component in statej,Σjlis the covariance matrix for the l-th mixture component in state j, andN(ot,μ→jl,Σjl)is a multi-dimensional Gaussian (Normal) distribution.The classification ofY^is performed in standard fashion using the Viterbi algorithm [68].To parametrize the camera data a model that represents the camera’s velocity is used. The intuition is that the user will slow the camera down with respect to a point on the surface of the model that they are interested in. The camera model (described in Section 3.2) has a parameterP0which is the point which the center projective ray of the camera passes through. Differencing this parameter over time the velocity vector v of the camera at time t inm/scan be calculated and this is the observationotused in the HMM.In the HMM formulation described above the parameters require training. The parameters to be trained are λ:λ=(A,B)One option for training is to use labeled data to find a set of parameters λ that maximize the correct classification. This option was explored as there is correlated eye tracking and motion data from the human validation experiments described in Section 5. This data can be treated as a labeling by using fixation points as positive examples of saliency. However, techniques that do not require labeled data are regularly used to train HMMs. This option was selected as performance differences were negligible. Additionally, such techniques remove the need to conduct eye tracking experiments to implement the algorithm. It is important to note that the trained HMM still requires a post-processing step to assign appropriate output labels. At the conclusion of training the mean velocities of the two output classes are compared and the high-velocity data is automatically assigned to the non-salient class label while the low-velocity data is assigned to the salient class.To train without labeled data the Baum-Welch algorithm maximizesP(Y|λ)with respect to λ[70]. The algorithm performs a local search and is therefore sensitive to initial parameters. The result is a set of parameters optimized to explain the input data. A full discussion of this process can be found in Rabiner [54].

@&#CONCLUSIONS@&#
In this paper we have presented a novel technique for extracting saliency from crowdsourced interaction data. We have developed two algorithms to extract interest metrics from camera motions. To our knowledge this system is the first of its kind in using a distributed smart phone app to gather saliency data for 3D maps. The proposed system provides an alternative to traditional visual saliency by harnessing the modality of touch to provide a proxy for interest. Our results show that comparable performance to visual saliency is achievable solely through interaction given a large enough user base. Through a gaze tracking experiment we have validated the techniques showing the proposed metrics are consistent with human attention.The level of agreement between the human experiment and the automated techniques is good, however looking at the human subject data suggests the variability of ‘interest’ as a definition for any saliency metric. The process that governs what we as humans find interesting in a large 3D map is more complex than the traditional model of visual attention for a static image. Additionally because of the amount of time spent exploring the model this process is likely to evolve. People will become more or less interested in certain types of organisms or terrain structures over time. This means the path of a human through the model, especially a large model is rarely going to be similar for small trial sizes. This points to the challenge of applying any saliency technique on a large, visually and structurally rich model. Casual users will not explore a model with the rigor or depth of a scientist reviewing images for scoring. As such the proposed technique cannot replace the ongoing efforts to automate the segmentation and classification of such science data. However the technique offers a promising approach for discovery and data-mining in a vast archive of images (currently we hold an archive of over a million AUV images). Ultimately the ability to understand what users find interesting in 3D models is appealing for applications beyond the one presented here. Mapping companies, advertisers, and geo-statisticians are all interested in what people are looking at on a map. As 3D maps overtake their 2D counterpoints on smart phones, interest metadata could help businesses know what streets are explored commonly, or what terrain features people are drawn to in a region.As we move forward a greater leveraging of the existing work in how to intelligently rectify the labels of multiple operators when crowdsourcing would strengthen the approach presented [50]. The use of more complex inter-operator relationships in terms of cross checking and aggregation of interest could further refine the saliency maps produced. Unsupervised clustering is another important avenue of exploration. Variational Dirichlet Processes (VDPs) have been applied to the clustering of 2D image data [64] and an adaptation of those techniques to saliency in 3D is a future direction of inquiry. An intelligent combination of the two presented techniques could help strengthen the overall results. A more complex HMM model could capture more states such as exploratory motion vs. directed search in addition to the salient/non-salient distinction. Also exploring the use of the depth information as a saliency channel could improve the performance of the visual-only techniques.