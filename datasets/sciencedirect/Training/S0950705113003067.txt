@&#MAIN-TITLE@&#
Feature selection via maximizing global information gain for text classification

@&#HIGHLIGHTS@&#
A novel feature selection metric called global information gain (GIG) is proposed.An efficient algorithm called maximizing global information gain (MGIG) is developed.MGIG performs better than other algorithms (IG, mRMR, JMI, DISR) in most cases.MGIG runs significantly faster than mRMR, JMI and DISR, and comparable with IG.

@&#KEYPHRASES@&#
Feature selection,Text classification,High dimensionality,Distributional clustering,Information bottleneck,

@&#ABSTRACT@&#
Feature selection is a vital preprocessing step for text classification task used to solve the curse of dimensionality problem. Most existing metrics (such as information gain) only evaluate features individually but completely ignore the redundancy between them. This can decrease the overall discriminative power because one feature’s predictive power is weakened by others. On the other hand, though all higher order algorithms (such as mRMR) take redundancy into account, the high computational complexity renders them improper in the text domain. This paper proposes a novel metric called global information gain (GIG) which can avoid redundancy naturally. An efficient feature selection method called maximizing global information gain (MGIG) is also given. We compare MGIG with four other algorithms on six datasets, the experimental results show that MGIG has better results than others methods in most cases. Moreover, MGIG runs significantly faster than the traditional higher order algorithms, which makes it a proper choice for feature selection in text domain.

@&#INTRODUCTION@&#
With the rapid growth of online information, the amount of digital documents has drastically increased. The problem of how to organize these resources effectively has gained increasing attention from researchers. Text classification, also known as text categorization, is the key technology to solve this problem. The goal of text classification is to assign a new document automatically to a predefined category. In order to achieve this, classification algorithms, also called as classifiers, firstly use the labeled documents to train a model, and then use the learned model to classify the unlabeled documents to a predefined category. Many classification methods, such as Naïve Bayes (NB) classifier [1,2]or support vector machines (SVM) [3,4], have been extensively explored and widely apply in the text classification field.Before using classification algorithms, documents have to be transformed into compact representations which can easily feed to classifier. A common text representation model is vector space model (VSM). In VSM model, the content of a document is represented as a vector in the term space, i.e. for jth document, dj=(wj1,wj2,…,wjV), where V is vocabulary count of terms, and wjkis the weight of term k in document j. There are many term weighting methods which can assign different value to wjk. The simplest method is to assign a binary value to wjk, wjkis 1 if the kth word is present in the jth document, and 0, if it is absent. Term frequency is the second most commonly used method. Term frequency-inverse document frequency (tf-idf), as a slightly more complicated weighting method, is also often used. Although weighting methods are important and have an impact on classification accuracy, they are beyond the scope of our research. In this paper, we only use the binary method to represent documents. Though VSM can effectively represent a document as vector, it also has a drawback. Since taking each distinct term in text collection as a feature, even moderate numbers of documents in a text corpus can easily result in thousands or even tens of thousands of features, causing many learning algorithms intractable to use in practice. To solve this problem, two techniques are often used to reduce the dimension of the raw space: feature selection and feature extraction.Feature selection for text classification is the task of reducing the dimensionality of raw feature space by identifying discriminative features and decreasing the computational complexity. Feature selection has been extensively studied [5], a more effective and popular method being information gain (IG) [6]. IG measures the number of bits of information obtained for category prediction by knowing the presence or absence of a term in a document. Though IG has been successfully applied in reducing the raw feature space, and shows that it can improve effectiveness comparing with no feature selection [4,7],it also has a drawback. That is, IG only evaluates features individually, gives a score to each feature without considering the redundancy between them, and selects the predefined number of features with the highest correlation scores. Since in reality data collections are often uneven, major categories have more features to select than minor categories. It is even worse for rare categories because they do not have enough examples for training. As a consequence, features for major categories are more likely to be picked out. Some selected features are highly redundant with each other because the documents they can distinguish are highly overlapping.To tackle the above shortcoming, feature selection algorithms should take the redundancy factor into account. Indeed, numerous of feature selection algorithms which incorporate relevancy and redundancy factors together have been proposed over the past two decades. Many of them are information theory based, such as mRMR [8], JMI [9], DISR [10,11]. In the paper, we call them higher order algorithms. As compared to IG, higher order algorithms can effectively restrain redundancy between selected features. All these algorithms select features one by one, supposing that k features have already been selected. When selecting (k+1)th feature, a redundant score between the (k+1)th feature and k selected features must be calculated. This will cost O(k) complexity. If the total number of terms is V, the cost of selecting a feature from V−k unselected candidate features is about O(Vk), and the total cost of selecting K features is O(VK2). Such a computational complexity is too high to apply if both V and K are large. This is the one reason for which higher order algorithms are seldom used in the text classification field. Though selecting a small number features can reduce computational complexity, too few features may not hold sufficient information required for the high classification accuracy.The feature extraction refers to the process of generating a small set of new features by combining or transforming the original ones. In text classification domain, a way to reduce dimensionality is distributional clustering of words which was first proposed by Pereira et al. [12]. Tishby et al. generalized [13] distributional clustering technique and proposed the information bottleneck (IB) principle. IB tries to compress the features/terms while preserving the information about target labels. Methods applying information bottleneck principle for dimensionality reduction can be divided into three categories: hierarchical agglomerative methods, hierarchical divisive methods and partitioning methods. Information bottleneck method (AIB) is a hierarchical agglomerative method proposed by Slonim [14]. Algorithms like AIB treat each feature as singleton initially. Then they iteratively merge two clusters which cause the mutual information loss between the data and the class labels to be as little as possible. Such greedy agglomeration steps stopped until the desired number of clusters, which we denote by K, is achieved. Lastly, each cluster is treated as a feature for classification. Such algorithms can reduce the dimensionality by two orders of magnitude, almost without sacrificing classification accuracy. However, because V is much greater than K, this aggregation process from down to top is particularly long (V−K iterations) and leads to a high computational cost (O(V3) for AIB). In addition, the greedy nature of agglomerating algorithms above will yield sub-optimal term clusters as compared with splitting algorithms, because splitting algorithms explore collective information to generate term clusters, while agglomerating algorithms merely exploit two clusters information at each agglomeration step [15]. In [16], Bekkerman proposed a method which can be classified into hierarchical divisive categories. [16] resorts the simulated annealing strategy and splits a cluster apart when “temperature” decreases. As for partitioning methods, Slonim and Dhillon et al. separately introduce their divisive clustering algorithm resemblance to the k-means [17,18]. Hierarchical divisive and partitioning methods alleviate two drawbacks of hierarchical agglomerative methods, but they are still feature extraction algorithms and applying them in a feature selection scenario is inappropriate.An important characteristic of AIB is that it treats all terms in vocabulary as a single random variable. Each term is considered as an elementary event. Different from AIB, in all the above higher order algorithms, the term is treated as a random variable. In AIB, we can merge two terms as a virtual term just as two elementary events can be regarded as a compound event. Moreover, the degree of redundancy between two terms can be learned by observing how much information is lost during the merging step. This paper exploits this technique heavily.The contributions of this paper are as follows: firstly, we propose a novel higher order feature selection metric called global information gain (GIG), and give a theoretical explanation as to why this metric can be used for selecting features for text classification. Secondly, an efficient algorithm called maximizing global information gain (MGIG) which reduces the computational complexity from O(VK2) to O(VK) is developed. Thirdly, we conduct a thorough experiment by comparing our proposed approach to four other algorithms on six text collections. The results of the experiment show that MGIG performs better than others in most cases.The organization of the paper is as follows: Section 2 introduces the basic information theory. Section 3 reviews briefly previous related work on feature selection algorithms. Our new algorithm is proposed in Section 4. In Section 5, we introduce the experimental setup. Experiment results are evaluated and analyzed in Section 6. Finally, conclusions and suggestions for future research are provided.In this section, we explain some basic concepts from information theory which will be frequently used in this paper. To learn more, see Cover and Thomas’s [19] works.Let X be a discrete random variable that takes on values from setX, and its probability distribution is p(X). Its uncertainty can be measured by entropy H(X), which is defined as:(1)H(X)=-∑x∈Xp(x)logp(x),for keeping a consistent representation with the following formulas, we also call (1) as the entropy of p and define it as:(2)H(p)=-∑x∈Xp(x)logp(x).Let Y be a discrete random variable that takes on values from setY,p(x,y)is the joint probability distribution of X and Y, then the mutual information of two discrete random variables X and Y is defined as:(3)I(X;Y)=∑x∈X∑y∈Yp(x,y)logp(x,y)p(x)p(y).The mutual information is a quantity that measures the mutual dependence of the two random variables, and can be interpreted as the amount of information shared by the two variables. I(X;Y) will be high if X and Y are closely related; while I(X;Y)=0 means X and Y are independent with each other.Similarly to I(X;Y), the conditional mutual information I(X;Y∣Z) measures the quantity of information shared by X and Y given Z. I(X;Y∣Z) may be either greater or less than I(X;Y) [20,21]. When I(X;Y∣Z)⩾I(X;Y), X and Y are called complementary w.r.t Z[11].For a specificx∈X, we can define point mutual information as:(4)Ip(x;Y)=∑y∈Yp(x,y)logp(x,y)p(x)p(y)=p(x)∑y∈Yp(y|x)logp(y|x)p(y),Ip(x:Y) is the information shared between x and Y.The relative entropy or Kullback–Leibler (KL) divergence [22] between two probability distributions p1 and p2 is defined as:(5)KL(p1,p2)=∑x∈Xp1(x)logp1(x)p2(x).KL-divergence is a measure of the dissimilarity or “distance” between two probability distributions. It is always non-negative, but it is not a true metric, since it is not symmetric and does not satisfy the triangle inequality, and it can be unbounded [19].The Jensen–Shannon (JS) divergence [23] is a popular method of measuring the dissimilarity between two probability distributions p1 and p2 with prior probabilitiesπ=(π1,π2) which π1+π2=1, and πi⩾ 0. Then the Jensen–Shannon divergence is defined as:(6)JSπ(p1,p2)=∑i=12πiKL(pi,p̃)=H(p̃)-∑i=12πiH(pi),wherep̃=π1p1+π2p2is the weighted average of p1 and p2. Comparing with Kullback–Leibler divergence, the Jensen–Shannon divergence is always a finite value and symmetric in {p1,π1} and {p2,π2}, and its square root is a metric.The Jensen–Shannon divergence can be generalized to measure the dissimilarity between n probability distributions {pi: 1⩽i⩽n} with weightsπ={πi:1⩽i⩽n} [23].(7)JSπ({pi:1⩽i⩽n})=∑i=1nπiKL(pi,p̃)=H(p̃)-∑i=1nπiH(pi),where∑inπi=1,πi⩾0andp̃=∑i=1nπipi. The convexity of the entropy and Jensen inequality guarantees the non-negativity of the JS divergence.Information gain of a term is the amount of increasing information when splitting the documents into two different sets according to the term’s presence or absence in documents. LetC={ci}i=1mdenote the set of categories in the target space, and let C be a discrete random variable that takes on values fromC, then the information gain of term t is defined to be:(8)IG(t)=H(p(C))-∑t′∈{t,t¯}p(t′)H(p(C|t′)).Above, m is the number of categories, p(t) andp(t¯)are the probabilities of presence and absence of term t, p(C∣t) andp(C|t¯)are the conditional probability distributions given presence and absence of term t, respectively. Becausep(C)=∑t′∈(t,t¯)p(t′)p(C|t′),letπ={πt=p(t),πt¯=p(t¯)}, we can get(9)IG(t)=JSπ(p(C|t),p(C|t¯)).From (9), we can also see that information gain measures how “far” between two distributions p(C∣t) andp(C|t¯), or the gain of information when splitting the distribution p(C) into two distributions p(C∣t) andp(C|t¯).Higher order algorithms try to identify a meaningful feature subset that maximizes correlation while minimizing redundancy. This paper compares MGIG with mRMR, DISR and JMI, which are the three most effective algorithms assessed by Brown et al. [21]. Brown proposed a unifying framework for information theoretic feature selection. All the above algorithms are the specific cases under this framework. The criterions of feature selection are listed below.JmRMR(Tk)=I(Tk;C)-1|S|∑Tj∈SI(Tk;Tj),JJMI(Tk)=∑Tj∈SI(TkTj;C)=I(Tk;C)-1|S|∑Tj∈S[I(Tk;Tj)-I(Tk;Tj|C)],JDISR(Tk)=∑Tj∈SI(TkTj;C)H(TkTjC),whereSis the set of features already selected. All above algorithms select kth feature Tkwhich make the LHS of formulas maximum. All entries of the RHS of formulas above can be divided into three categories according to their function: I(Tk;C) measures the correlation between term Tkand target variable C, I(Tk;Tj) measures the redundancy between terms Tkand Tj, and I(Tk;Tj∣C) in JMI and DISR measures the complementary between terms Tkand Tj. From the above formulas, it can be seen that mRMR tries to select features that are mutually far away from each other while still having high correlation to the target variables. DISR exploits the intuition that a combination of features can release more information on target variable than the sum of information released by each individual feature. For more details about mRMR, DISR and JMI, please see [8,10,11,9,21].This section consists of four subsections. Section 1 gives a toy example to explain the idea behind GIG. Section 2 introduces the definition of GIG. Section 3 gives explanations on why maximizing GIG can be used to select the features from intuitive and theoretical views. In Section 4, an efficient algorithm called maximizing global information gain is proposed.GIG is motivated by AIB and IG algorithms and exploits the principle behind them further: merging will result in the loss of information; splitting, as the dual operation of merging, will result in information gain.In Table 1, the digital “10” in the first cell means there are a total of 10 documents containing the term “bible” and labeled with class 1. For terms with similarity distributions, the information loss caused by merging these terms is small. Information loss of merging term “bible” and “god” as a virtual term is 8.5e−3. This is also the information gain achieved by splitting the virtual term into these two individual terms. Information loss for merging “health” and “drug” is 6.0e−3, and for merging “hockey” and “player” is 7.8e−3. As Compared to terms with similarity distributions, splitting dissimilarity terms can result in higher information gain. For example, the information gain achieved by splitting “bible” and “player” apart is 0.179, and for splitting “drug” and “player”, is 0.268, for splitting “bible” and “drug” is 0.202, and for splitting “bible” “drug” and “player”, it is 0.512. Intuitively, terms with dissimilarity distributions are better than similarity ones for classification. This paper try to selection terms that release large information when they are split apart. In subSection 4.2, a formal definition of this information gain is provided.The definition of GIG: Suppose i1,i2,…,ikare k (k⩾2) integers randomly sampling from set {1,2,…,V} without replacement, and let tj(1⩽j⩽k) represents the ijth term in the vocabulary, then GIG of t1,t2,…,tkis defined as follows:(10)GIG({tj:1⩽j⩽k})=∑j=1kIp(tj;C)-Ip(t̃Sk;C),whereSk={t1,t2,…,tk}, andt̃Skis just a virtual term if we view all k terms inSkas a whole. Hence(11)p(t̃Sk)=∑j=1kp(tj),and(12)p(ci|t̃Sk)=∑j=1kp(tj)p(t̃Sk)p(ci|tj),whereci∈C.Eqs. (11) and (12) are consistent with AIB algorithm. In (10),Ip(t̃Sk;C)is the information before splitting, and∑j=1kIp(tj;C)is the information after splitting, in this sense, RHS of (10) simulates a divisive operation as k-means does, the difference regarding k-means is: in (10)k features must be selected firstly and treated as a singleton, and then be divided into individuals.For selecting one feature (k=1), definition of Eq. (10) is nonsense, we select the feature tjwhich makes the Ip(tj;C) maximum.The following theorem reveals that GIG is indeed a weighted generalized Jensen–Shannon divergence.Theorem 1Let t1,t2,…,tkbe the terms randomly sampling from vocabulary without replacement, then(13)GIG({tj:1⩽j⩽k})=π(Sk)JSπ({p(C|tj):1⩽j⩽k})whereSk={tj:1⩽j⩽k}andπ(Sk)=∑j=1kp(tj);π=(π1,π2,…,πk),πj=p(tj)/π(Sk)for 1⩽j⩽k; andp(C∣tj) is the distribution of term tjover categories {ci: 1⩽i⩽m}.By the definition of point mutual information (see (4)), and notingp(ci,tj)/π(Sk)=πjp(ci|tj), then∑j=1kIp(tj;C)π(Sk)=∑j=1k∑i=1mπjp(ci|tj)logp(ci|tj)p(ci)=-∑j=1kπjH(p(C|tj))-∑i=1mlog(p(ci))∑j=1kπjp(ci|tj),because of (12),∑j=1kIp(tj;C)π(Sk)=-∑j=1kπjH(p(C|tj))-∑i=1mp(ci|t̃Sk)log(p(ci)).becauseπ(Sk)=p(t̃Sk), thenIp(t̃Sk;C)π(Sk)=∑i=1mp(ci|t̃Sk)logp(ci|t̃Sk)p(ci)=-H(p(C|t̃Sk))-∑i=1mp(ci|t̃Sk)log(p(ci)),soGIG({tj:1⩽j⩽k})=π(Sk)H(p(C|t̃Sk))-∑j=1kπjH(p(C|tj))=π(Sk)JSπ({p(C|tj):1⩽j⩽k}).□IG and GIG are identical in form. However, GIG has a discount weightπ(Sk)(see Eqs. (9) and (13)), and they are different in the evaluated objects. In (9), IG measures each term individually; in (13), interaction between terms is taken into account. In this paper, we use “global” to emphasize the fact that (10) gives a global score for t1,t2,…,tk, and call it global information gain.Intuitively, proper features for text classification should try to hold three virtues at least: informative, representative, and distinctive. Informative means that each feature selected should share more information with class labels. The representative factor tries to guarantee the selection of informative features and the exclusion of outliers. Finally, distinctive means that the selected features should maintain diversity as much as possible.A given number of features, say k, can be selected by maximizing GIG. The distinctive factor is intrinsic in GIG because splitting redundant features apart can only release little information, hence redundancy is largely avoided (see (13)). Maximizing Eq. (10) can also achieve informative and representative goals. For maximizing Eq. (10), p(C∣tj) should situate as “far” as possible away from p(C) and simultaneously try to keepp(C|t̃Sk)as “near” as to p(C). In other words, GIG tries to select more informative features and use their distributions to characterize the most non-informative or plain distribution p(C), which equals∑j=1Vp(tj)p(C|tj). The representative factor is ensured because p(C) cannot be characterized well by poor representative features.The following gives a theoretical explanation as to why maximizing global information gain can be used for selecting features. It is known that the probability of Bayes error is one of the best criterion for class separability. Given target variable C and let T be a random variable taking value fromT={t1,t2,…,tV}, the upper bound of the Bayes error probability [23] is:pBayes(e)⩽12(H(C)-JSπ({p(T|c1),p(T|c2),…,p(T|cm)})),whereπ=(π1,π2,…,πm), πi=p(ci) is the probability of class ci.It is important to note thatJSπ({p(T|c1),p(T|c2),⋯,p(T|cm)})=H∑i=1mp(ci)p(T|ci)-∑i=1mp(ci)H(p(T|ci))=H(C)-H(T|C)=I(T;C).HencepBayes(e)⩽12(H(C)-I(T;C)).NotingI(T;C)=∑j=1VIp(tj;C), we can conclude that I(T;C) ⩾GIG({tj:1⩽j⩽k}) (see definition of GIG), and the upper bound of the Bayes error probability can be written as:(14)pBayes(e)⩽12(H(C)-GIG({tj:1⩽j⩽k}))When selecting a given number of features, the higher of GIG({tj:1 ⩽j⩽k}), the more the RHS of inequality (14) approaches the Bayes error, and the better representing original data set with respect to target variable C, this is the underlying principle used by GIG for feature selection.GIG only gives a metric but does not tell us how to select features. Using an exhausting search method directly is prohibited because of the high dimensionality nature of text classification problem. This subsection proposes an efficient algorithm which selects features one by one sequentially.SupposeSk={t1,t2,…,tk}is the set of terms already selected, we use the following criteria to select (k+1)th feature:(15)argmaxtk+1∈T⧹Skf(tk+1)=GIG({tj:1⩽j⩽k+1})-GIG({tj:1⩽j⩽k}).LetSk+1=Sk∪{tk+1}and using the definition of GIG, f(tk+1) can be expressed as:f(tk+1)=∑j=1k+1Ip(tj;C)-∑j=1kIp(tj;C)-Ip(t̃Sk+1;C)+Ip(t̃Sk;C)=p(tk+1)∑i=1mp(ci|tk+1)logp(ci|tk+1)p(ci)-p(t̃Sk+1)∑i=1mp(ci|t̃Sk+1)logp(ci|t̃Sk+1)p(ci)+p(t̃Sk)∑i=1mp(ci|t̃Sk)logp(ci|t̃Sk)p(ci).Noting(16)p(t̃Sk+1)p(ci|t̃Sk+1)=p(t̃Sk)p(ci|t̃Sk)+p(tk+1)p(ci|tk+1),(17)f(tk+1)=p(t̃Sk+1)H(p(C|t̃Sk+1))-(p(tk+1)H(p(C|tk+1))+p(t̃Sk)H(p(C|t̃Sk)))becausep(t̃Sk)H(p(C|t̃Sk))is a constant givenSk, hence(18)argmaxtk+1∈T⧹Skf(tk+1)≡argmaxtk+1∈T⧹Sk(p(t̃Sk+1)H(p(C|t̃Sk+1))-p(tk+1)H(p(C|tk+1))).Noting two of the feature selecting criteria (8) and (17) are still similar in form, and we call the algorithm as maximizing global information gain (MGIG) when selecting features according (18).As AIB greedily merges two clusters, MGIG greedily selects each feature in each step. Thus, they all get sub-optimal results. However, owing to its divisive nature, MGIG mitigates the drawback of greediness by selecting features from a bird’s eye view. Another advantage of MGIG is its low computational complexity. Because of Eq. (16), we can getp(ci|t̃Sk+1)based onp(ci|t̃Sk), not needing to consider interactions between the term tk+1 and all k terms already selected. For selecting tk+1 from V−k unselected candidate features, the cost is about O(V), and the total cost of selecting K features is O(VK). As Compared to other higher order algorithms, MGIG reduces the computational complexity from O(VK2) to O(VK). This low computational complexity makes MGIG acceptable for applying in text classification domain.We can terminate MGIG when the number of features selected meets a preassigned value K. In the following, we also give a heuristic method for terminating MGIG. Fig. 1makes a comparison between the trend of Micro-F1 and −f(tk+1) when the number of k increasing. The two trends curves are approximately coincident, and the coincidence is not accidental. When the k is small, the features selected are “far” from each other. Splitting them can release more information, while the f(tk+1) value, and the increase of Micro-F1 are both large. With the increase of k, the values ofp(t̃Sk+1)H(p(C|t̃Sk+1))andp(t̃Sk)H(p(C|t̃Sk))approach each other. Meanwhile, the values of f(tk+1) and the increase of Micro-F1 are becoming smaller. Based on the coincidence, we can monitor the variation of f(tk+1), and stop MGIG when|f(tk)-f(tk+1)|f(tk)<∊. The algorithm of MGIG is listed below.Algorithm 1Algorithm of maximizing global information gainInput: give K, the number of features to select, or an alternative variable is ∊.T={tj:1⩽j⩽V} is the set of terms in dataset.PW={p(tj):1⩽j⩽V} is the set of all term weights/priors.PS={p(C∣tj):1⩽j⩽V} is the set of distributions.PC={pj:1⩽j⩽m} is the class distribution.Output: ST is the set of terms selected by MGIG.for each term t in TdoComputing the entropy of term t according (2).end forfor each term t in TdoComputing the point mutual information of term t according (4).end forST={}ST=ST∪{term which point mutual information is maximal}.while ∣ST∣<K or|f(tk)-f(tk+1)|f(tk)<∊doIn unselected terms, find a term t which makes (18) maximal.ST=ST∪{t}end whileSTIn this section, all datasets, classifiers and measures for performance evaluation are presented in the following subsections.In our experiments, six datasets have been selected trying to cover various characteristics of text corpus. Four of them are well-known datasets: Reuters-21578,1Reuters-21578 can be found at http://www.daviddlewis.com/resources/testcollections/reuters21578/.120 Newsgroups,2The 20 Newsgroups can be found at http://qwone.com/jason/20Newsgroups/.2WebKB3WebKB can be found at http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/.3and Industry Sector4Industry sector can be found at http://people.cs.umass.edu/mccallum/data.html.4; in addition to this, we generate two imbalanced datasets from 20 Newsgroups and Industry Sector respectively. We use Bow toolkit [24] and NLTK [25] for data preprocessing, all terms were converted to lower case, stop terms were removed, and no stemming was used.The Reuters-21578 dataset contains documents collected from the Reuters newswire in 1987. It is a standard text classification benchmark and contains 135 categories in the original version. This paper only uses the documents which have a unique topic, i.e., the documents that have multiple topics were ignored, the final category number is 66, and we call it Reuters66. The category distribution of Reuters66 is uneven, the most popular category has 3887 documents, while for other 23 minor categories, the number of documents is less than 10.The 20 Newsgroups dataset is a collection of about 20,000 newsgroup documents nearly evenly divided among 20 discussion groups. Some newsgroups such as, for example, the category of comp.sys.ibm.pc.hardware and category of comp.sys.mac.hardware are very similar to each other. Therefore, as compared with the skewed category distribution in the Reuters66 corpus, the 20 categories in the 20 Newsgroups are approximately uniformly distributed.The WebKB dataset contains web pages collected from the computer science departments of several universities. The pages are divided into seven categories: course, department, faculty, project, student, stuff and other. In this paper, the four most populous categories are used, namely course, faculty, project and student. This associated subset is typically called WebKB4.The Industry Sector dataset consists of company homepages classified in a hierarchy of industry sectors. In our experiments, we did not take the hierarchy into account and we used a flattened version of the dataset. The hierarchical organizational structure of the Industry Sector dataset is completely different from above three datasets, and has an important impact on the performance of many algorithms.To verify whether MGIG performs better than other algorithms on imbalanced dataset, two highly skewed datasets are derived from 20 Newsgroups and Industry Sector respectively. We take the 20 Newsgroups as an example and explain the generating process as follows: we generate a arithmetic sequence which contains 21 entries from 0 to 1, and randomly assign each entry (except 0 entry) to a category. Then each category uses its entry as sampling probability to randomly select documents. Documents selected by each category are collected together and all documents unselected are discarded. We called the new generating corpus as imbalanced 20 Newsgroups dataset. The generating process of the imbalanced Industry Sector dataset is the same as above.Table 2gives detailed information on the datasets used.All feature selection algorithms are examined on two widely used classifiers: a Naïve Bayes classifier (NB) and a support vector machine (SVM).The Naïve Bayes classifier [1] is a simple but very effective classifier. It utilizes the Bayes formula to distinguish which label an instance belongs to. The assumption behind it is that features are statistically independent from each other for the given target labels. Though the assumption is violated for the terms in the document, many experiments have demonstrated that NB has a good performance [26,27] compared with others on various real datasets.SVM [28] is a popular classifier that enjoys a considerable theoretical and empirical support. The method is defined over the vector space where the classification problem is to find the decision surface that “best” separates the data points of one class from the other. Informally, for linearly separable two-class data, the linear SVM computes a hyperplane that maximizes the “margin” between the two classes. For non-linearly separable data, SVM computes a “soft” maximum margin separating hyperplane that allows for training errors, or uses a kernel to transform the data into a high dimensional space where the data is likely to be separable. SVM has been introduced in text classification by Joachims [3] and subsequently confirmed as an outstanding method [4].In our experiments, we resort to Rapidminer [29] and use its implementations of NB and SVM for classification.The F1-measure is a metric which measures the classification accuracy for a category. It considers both the precision and the recall of the category and uses their harmonic average to score effectiveness. When effectiveness is evaluated for several categories, the results for individual categories must be averaged. Micro-F1 and Macro-F1 are all the weighted average of the F1-measures for each category. The difference in that, for Micro-F1, each category’s weight is proportional to the number of documents it holds. In the case that category distribution are biased, large categories will dominate small ones and therefore Micro-F1 will tend to over-emphasize the performance on the largest categories. On the contrary, Macro-F1 gives equal weight to all categories. For highly skewed category distribution, since small categories tend to be more difficult to distinguish, the Macro-F1 tends to be lower. In our experiments, each document is classified to one category, in this situation, Micro-F1 is identical with accuracy. For comparison with Macro-F1, we always use Micro-F1 in the following.For clarity, we further divide the F1-measure into “original” F1-measure and “averaged” F1-measure. In our experiments, in order to increase the statistical significance of the results, for each combination of dataset, classifier, feature selection algorithm and the number of selecting features (e.g., WebKB4, SVM, MGIG, and 10 selecting features make a combination), we conduct 5-fold cross-validations and generate 5 confusion matrices. From each matrix, an “original” Micro-F1 or an “original” Macro-F1 measure can be deduced. We call the average of 5 “original” F1-measures as an “averaged” F1-measure. Each point in Figs. 2 and 3, or each cell in Table 3is an “averaged” F1-measure. For simplicity, we call the “averaged” F1-measure as F1-measure in the following. We also define one trial as the process of 5-fold cross-validations. Therefore, a trial corresponds to 5 “original” F1-measures results or a F1-measure.We conduct a series of experiments to evaluate the performance of the MGIG algorithm. Figs. 2 and 3 display the Micro-F1 and Macro-F1 by using NB for classification. Here we use a logarithm scaled x-axis to emphasize the result when the feature count is small, and select points which evenly distribute on x-axis. Table 3 shows the experimental results achieved by using SVM as a classifier. We do not show the results about imbalanced 20 Newsgroups and Industry Sector because they are similar with results of 20 Newsgroups and imbalanced Industry Sector respectively.Characteristics of the datasets have an important effect on performances, as Bekkerman pointed out [16], Reuters66 and WebKB4 are more similar to each other, but they are different from the 20 Newsgroups. In the case of Reuters66 and WebKB4, using a small set of terms can result in near “optimal” performance. When even more features are selected, rare words are included and these terms do add noise which results in performance degradation. On the other hand, for 20 Newsgroups, the F1-measure is low when selecting a small number of features, but when the feature count increases, it can be persistently improved. This indicates that even rare terms in 20 Newsgroups are useful for classification. In this sense, Bekkerman indicated that the “complexity” of 20 Newsgroups is higher than that of Reuters66 and WebKB4, and although more sophisticated feature selecting algorithms perform better than simpler ones on 20 Newsgroups, this does not necessarily occurs on Reuters66 and WebKB4. Our experiments reproduce Bekkerman’s conclusion. As to Industry Sector and imbalanced Industry Sector datasets, they are the most difficult to classify because of their hierarchical construction. In the Industry Sector, A general sector can contain several subsectors. For example, the financial.sector includes five subsectors, while the services.sector has 19 subsectors. These subsectors are very similar. Taking financial.sector as an example, its five subsectors are: banking.sector, consumer.financial.services.industry, insurance.sector, investment.services.industry and misc.financial.services.industry. In addition, banking.sector and insurance.sector also have their own subsections, making the classification task harder.We first perform a Friedman test to see whether there is a statistical difference among the 5 algorithms. The Friedman test is carried out on a Macro-F1 result set (classifier is SVM) which has 5 treatments (MGIG,IG,mRMR,DISR,JMI) and 60 blocks (6 datasets and 10 options of number of features for each dataset). The p-value (4.332e−12) of Friedman test shows that statistical differences among 5 the algorithms exist. To further identify which algorithms are different, we carry out post hoc analyses using the paired Wilcoxon test; the results are shown in Fig. 4. It can be seen that MGIG is significantly different from IG, mRMR, DISR and JMI, the corresponding p-value being 2.32e−10, 2.56e−04, 4.82e−05 and 3.98e−12. We also conduct the Friedman test and post hoc analyses on three other result sets (Macro-F1+NB, Micro-F1+SVM, Micro-F1+NB), and the MGIG is always significantly different from the rest of the algorithms.Secondly, we want to know in how many trials MGIG performs better than the other algorithms. Because of the diversity of the datasets and because the performance of feature selection algorithms is sensitive to the dataset being categorized, it can be seen that no silver-bullet algorithm can surpass others over all datasets. But for most trials, MGIG performs better than the other algorithms. For four “complexity” datasets: 20 Newsgroups, imbalanced 20 Newsgroups, Industry Sector and imbalanced Industry Sector, MGIG always gives higher F1-measures than IG. When comparing with three higher order algorithms, in 4805480=4*3*2*2*10, and each multiplier respectively represents 4 “complexity” datasets, 3 higher order algorithms, 2 F1-measures, 2 classifier and 10 options of number of features.5pairwise comparisons of F1-measures, the proportion of MGIG superior to higher order algorithms is 98.12% (471/480). For two lower “complexity” datasets: WebKB4 and Reuters66, MGIG still performs better on WebKB4, but poorly on Reuters66.For each pairwise comparison between MGIG and another algorithm above, to verify whether its result has statistical significance, we do a paired Wilcoxon test between MGIG’s 5 “original” F1-measures and other algorithms. We divide test results into four categories: MGIG’s result is significantly different from the comparing result, and better than it (BS); MGIG’s result is not significantly different from the comparing result, but it is nevertheless better than it (BNS); MGIG’s result is not significantly different from the comparing result, and worse than it (NBNS); MGIG’s result is significantly different from the comparing result, and worse than it (NBS). We summarize data in Figs. 2 and 3 and data in Table 3 (also includes data of imbalanced 20 Newsgroups and Industry Sector) according the paired Wilcoxon test results, and we obtain Table 4.Data in Table 4 is summarized over six benchmark datasets. From Table 4, we can see that at least in 82.5% of the trials, MGIG obtains higher F1-measures, regardless of whether it is evaluated by Micro-F1 or Macro-F1 measure. Furthermore, only 4.2% of the trials shows that MGIG is significantly worse than others methods. It is clear that the superiority of MGIG is higher when using NB as classifier rather than SVM. When feeding features to SVM, MGIG can obtain significantly better results only in 22.9% of the trials, while for NB, at least 35% of the trials do. The reason may be that MGIG uses information theory and KL-divergence to select features, while NB has an inherent kinship with information theory and KL-divergence [30].Comparing Figs. 2 and 3, except for the 20 Newsgroups dataset, all Macro-F1 are lower than the corresponding results of Micro-F1, the gap is more evident especially when the number of categories is large. This is because the minor categories are difficult to classify. For 20 Newsgroups dataset, each category has almost the same number of documents. Thus, the gap between Micro-F1 and Macro-F1 is not very obvious. MGIG can perform better than other algorithms for skewed datasets. When evaluating with Macro-F1, MGIG achieves an 82.5% winning rate (BS+BNS) at least. Since Macro-F1 treats each category equally, this high winning rate indicates the ability of MGIG to select features for minor categories. We contribute this advantage of MGIG to its inherent capability of selecting non-redundant features.To select features for high dimensional data, the efficiency of the algorithms must be considered. All experiments are conducted on a workstation which has a Intel Xeon 2.13GHz CPU and 16G RAM. We write MGIG and IG with R language, and a crucial part of MGIG is rewritten with C language. For JMI, mRMR, and DISR, we use Brown’s codes6Source codes can be found at http://www.cs.man.ac.uk/gbrown/fstoolbox/.6which are all written with C language and complied with gcc-o3 options. We take Industry Sector as an example dataset and show the running time of algorithms in Fig. 5. The running time over other datasets is not shown since they are similar with Fig. 5.In the left subfigure of Fig. 5, the curve of MGIG is overlapped by the curve of IG because the running time of MGIG and IG are very close. The right subfigure gives a clearer running time of MGIG and IG. Theoretically, for the running time of IG, selecting 100 features should be similar to selecting 500 features. The fluctuation of running time may contribute to R language’s vectorization technology.When taking running time into account, MGIG shows a strong competitive advantage over other higher order algorithms. For selecting 500 features over Industry Sector, if treating the running time of MGIG as one unit, the running times of IG, MGIG, mRMR, JMI and DISR are 0.32, 1, 124, 226, 335 respectively. The running time of MGIG is comparable with IG, and about two orders of magnitude lower than mRMR, JMI and DISR.In summary, for most of datasets, MGIG performs better than IG, and this advantage is more obvious for more “complex” datasets, such as 20 Newsgroups and Industry Sector. For less “complex” datasets such as WebKB4 and Reuters66, IG may be a better choice if the efficiency is a vital factor. As Compared to higher order algorithms, MGIG is superior both in effectiveness and efficiency in selecting features in text domain. We attribute MGIG’s superiority to the fact it is specialized on text domain. When selecting features according (18), common features are preferable. This is the most important for selecting well terms properly [5,7]. In contrast to MGIG, higher order algorithms such as mRMR, JMI and DISR are more general. These methods do not consider the weight of individual features but use information theory to estimate probability density of features, which renders them usable in various domains [8,9,11]. We believe the specialization of MGIG for text domain explains the fact that it performs better than other algorithms.

@&#CONCLUSIONS@&#
In this work, a higher order feature selecting metric called global information gain (GIG) is presented. We prove that the GIG metric can be used for selecting features for text classification. The features selected according to GIG metric are informative, representative, and distinctive. Based on GIG, an efficient algorithm called maximizing global information gain (MGIG) is also presented.MGIG has been tested on six datasets. The Friedman and post hoc tests show that MGIG is significantly different from all the other algorithms. The subsequent paired comparisons show strong evidence that MGIG is superior to other algorithms in most trials. Though as a higher order algorithm, the running time of MGIG and IG are comparable, MGIG runs impressively faster than other higher order algorithms. Hence, to select features for text classification, MGIG is a proper choice especially for “complexity” datasets. Lastly, MGIG may also be applied in other domains provided distributional clustering or information bottleneck technology can be used.