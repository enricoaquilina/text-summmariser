@&#MAIN-TITLE@&#
Markov Random Field modeling, inference & learning in computer vision & image understanding: A survey

@&#HIGHLIGHTS@&#
We present a comprehensive survey of Markov Random Fields (MRFs) in computer vision.This is a compact and informative summary of literature in the development of MRFs.Techniques in MRF modeling, inference and learning are included.It helps readers rapidly gain a global view and better understanding for using MRFs.It begins with basic notions and then goes onto cover important recent development.

@&#KEYPHRASES@&#
Markov Random Fields,Graphical models,MRFs,MAP inference,Discrete optimization,MRF learning,

@&#ABSTRACT@&#
In this paper, we present a comprehensive survey of Markov Random Fields (MRFs) in computer vision and image understanding, with respect to the modeling, the inference and the learning. While MRFs were introduced into the computer vision field about two decades ago, they started to become a ubiquitous tool for solving visual perception problems around the turn of the millennium following the emergence of efficient inference methods. During the past decade, a variety of MRF models as well as inference and learning methods have been developed for addressing numerous low, mid and high-level vision problems. While most of the literature concerns pairwise MRFs, in recent years we have also witnessed significant progress in higher-order MRFs, which substantially enhances the expressiveness of graph-based models and expands the domain of solvable problems. This survey provides a compact and informative summary of the major literature in this research topic.

@&#INTRODUCTION@&#
The goal of computer vision is to enable the machine to understand the world – often called visual perception – through the processing of digital signals. Such an understanding for the machine is done by extracting useful information from the digital signals and performing complex reasoning. Mathematically, let D denote the observed data and x a latent parameter vector that corresponds to a mathematical answer to the visual perception problem. Visual perception can then be formulated as finding a mapping from D to x, which is essentially an inverse problem[1]. Mathematical methods usually model such a mapping through an optimization problem as follows:(1)xopt=argminxE(x,D;w),where the energy (or cost, objective) function E(x, D; w) can be regarded as a quality measure of a parameter configuration x in the solution space given the observed data D, and w denotes the model parameters.1For the purpose of conciseness, D and/or w may not be explicitly written in the energy function in the following presentation unless it is necessary to do so.1Hence, visual perception involves three main tasks: modeling, inference and learning. The modeling has to accomplish: (i) the choice of an appropriate representation of the solution using a tuple of variables x; and (ii) the design of the class of energy functions E(x, D; w) which can correctly measure the connection between x and D. The inference has to search for the configuration of x leading to the optimum of the energy function, which corresponds to the solution of the original problem. The learning aims to select the optimal model parameters w based on the training data.The main difficulty in the modeling lies in the fact that most of the vision problems are inverse, ill-posed and require a large number of latent and/or observed variables to express the expected variations of the perception answer. Furthermore, the observed signals are usually noisy, incomplete and often only provide a partial view of the desired space. Hence, a successful model usually requires a reasonable regularization, a robust data measure, and a compact structure between the variables of interest to adequately characterize their relationship (which is usually unknown). In the Bayesian paradigm, the model prior, the data likelihood and the dependence properties correspond respectively to these terms, and the maximization of the posterior probability of the latent variables corresponds to the minimization of the energy function in Eq. (1). In addition to these, another issue that should be taken into account during the modeling is the tractability of the inference task, in terms of computational complexity and optimality quality, which introduces additional constraints on the modeling step.Probabilistic graphical models (usually referred to as graphical models) combine probability theory and graph theory towards a natural and powerful formalism for modeling and solving inference and estimation problems in various scientific and engineering fields. In particular, one important type of graphical models – Markov Random Fields (MRFs) – has become a ubiquitous methodology for solving visual perception problems, in terms of both the expressive potential of the modeling process and the optimality properties of the corresponding inference algorithm, due to their ability to model soft contextual constraints between variables and the significant development of inference methods for such models. Generally speaking, MRFs have the following major useful properties that one can benefit from during the algorithm design. First, MRFs provide a modular, flexible and principled way to combine regularization (or prior), data likelihood terms and other useful cues within a single graph-formulation, where continuous and discrete variables can be simultaneously considered. Second, the graph theoretic side of MRFs provides a simple way to visualize the structure of a model and facilitates the choice and the design of the model. Third, the factorization of the joint probability over a graph could lead to inference problems that can be solved in a computationally efficient manner. In particular, development of inference methods based on discrete optimization enhances the potential of discrete MRFs and significantly enlarges the set of visual perception problems to which MRFs can be applied. Last but not least, the probabilistic side of MRFs gives rise to potential advantages in terms of parameter learning (e.g., [2–5]) and uncertainty analysis (e.g., [6,7]) over classic variational methods [8,9], due to the introduction of probabilistic explanation to the solution [1]. The aforementioned strengths have resulted in the heavy adoption of MRFs towards solving many computer vision, computer graphics and medical imaging problems. During the past decade, different MRF models as well as efficient inference and learning methods have been developed for addressing numerous low, mid and high-level vision problems. While most of the literature is on pairwise MRFs, we have also witnessed significant progress of higher-order MRFs during the recent years, which substantially enhances the expressiveness of graph-based models and expands the domain of solvable problems. We believe that a compact and informative summary of the major literature in this research topic will be valuable for the reader to rapidly obtain a global view and hence better understanding of such an important tool.To this end, we present in this paper a comprehensive survey of MRFs in computer vision and image understanding, with respect to the modeling, the inference and the learning. The remainder of this paper is organized as follows. Section 2 introduces preliminary knowledge on graphical models. In Section 3, different important subclasses of MRFs as well as their important applications in visual perception are discussed. Representative techniques for MAP inference in discrete MRFs are presented in Section 4. MRF learning techniques are discussed in Section 5. Finally, we conclude the survey in Section 6.A graphical model consists of a graph where each node is associated with a random variable and an edge between a pair of nodes encodes probabilistic interaction between the corresponding variables. Each of such models provides a compact representation for a family of joint probability distributions which satisfy the conditional independence properties determined by the topology/structure of the graph: the associated family of joint probability distributions can be factorized into a product of local functions each involving a (usually small) subset of variables. Such a factorization is the key idea of graphical models.There are two common types of graphical models: Bayesian Networks (also known as Directed Graphical Models or Belief Networks) and Markov Random Fields (also known as Undirected Graphical Models or Markov Networks), corresponding to directed and undirected graphs, respectively. They are used to model different families of distributions with different kinds of conditional independences. It is usually convenient to covert both of them into a unified representation which is called Factor Graph, in particular for better visualizing potential functions and performing inference in higher-order models. As preliminaries for the survey, we will proceed with a brief presentation on Markov Random Fields and factor graphs in the remainder of this section. We suggest the reader being interested in a larger and more in depth overview the following publications [10–13].Let us introduce the necessary notations that will be used throughout this survey. For a graphical model, letG=(V,E)denote the corresponding graph consisting of a setVof nodes and a setEof edges. Then, for each nodei(i∈V), let Xidenote the associated random variable, xithe realization of Xi, andXithe state space of xi(i.e.,xi∈Xi). Also, letX=(Xi)i∈Vdenote the joint random variable andx=(xi)i∈Vthe realization (configuration) of the graphical model taking values in its spaceXwhich is defined as the Cartesian product of the spaces of all individual variables, i.e.,X=∏i∈VXi.For simplification and concreteness, “probability distribution” is used to refer to “probability mass function” (with respect to the counting measure) in discrete cases and “probability density function” (with respect to the Lebesgue measure) in continuous cases. Furthermore, we use p(x) to denote the probability distribution on a random variable X, and use xc(c⊆V) as the shorthand for a tuple c of variables, i.e., xc=(xi)i∈c. Due to the one-to-one mapping between a node and the associated random variable, we often use “node” to refer to the corresponding random variable in case there is no ambiguity.A Markov Random Field (MRF) has the structure of an undirected graphG, where all edges ofEare undirected (e.g., Fig. 1(a)), and holds the following local independence assumptions (referred to as local Markov property) which impose that a node is independent of any other node given all its neighbors:(2)∀i∈V,Xi⊥XV-{i}|XNi,whereNi={j|{i,j}∈E}denotes the set of neighbors of node i in the graphG, andXi⊥Xj|Xkdenotes the statement that Xiand Xjare independent given Xk. An important notion in MRFs is clique, which is defined as a fully connected subset of nodes in the graph. A clique is maximal if it is not contained within any other larger clique. The associated family of joint probability distributions are those satisfying the local Markov property (i.e., Eq. (2)). According to Hammersley-Clifford theorem [14,15], such a family of distributions are Gibbs distributions which can be factorized into the following form:(3)p(x)=1Z∏c∈Cψc(xc),where Z is the normalizing factor (also known as the partition function), ψc(xc) denotes the potential function of a clique c (or: clique potential) which is a positive real-valued function on the possible configuration xcof the clique c, andCdenotes a set of cliques2Note that any quantities defined on a non-maximal clique can always be redefined on the corresponding maximal clique, and thusCcan also consist of only the maximal cliques. However, using only maximal clique potentials may obscure the structure of original cliques by fusing together the potentials defined on a number of non-maximal cliques into a larger clique potential. Compared with such a maximal representation, a non-maximal representation clarifies specific features of the factorization and often can lead to computational efficiency in practice. Hence, without loss of generality, we do not assume thatCconsists of only maximal cliques in this survey.2contained in the graphG. We can verify that any distribution with the factorized form in Eq. (3) satisfies the local Markov property in Eq. (2).The global Markov property consists of all the conditional independences implied within the structure of MRFs, which are defined as:∀V1,V2,V3⊆V, if any path from a node inV1to a node inV2includes at least one node inV3, thenXV1⊥XV2|XV3. LetI(G)denote the set of such conditional independences. The identification of these independences boils down to a “reachability” problem in graph theory: considering a graphG′which is obtained by removing the nodes inV3as well as the edges connected to these nodes fromG,XV1⊥XV2|XV3is true if and only if there is no path inG′that connects any node inV1⧹V3and any node inV2⧹V3. This problem can be solved using standard search algorithms such as breadth-first search (BFS) [16]. Note that the local Markov property and the global Markov property are equivalent for any positive distribution. Hence, if a positive distribution can be factorized into the form in Eq. (3) according toG, then it satisfies all the conditional independences inI(G). Nevertheless, a distribution instance that can be factorized overG, may satisfy more independences than those inI(G)[13].MRFs provide a principled probabilistic framework to model vision problems, thanks to their ability to model soft contextual constraints between random variables [17,18]. The adoption of such constraints is important in vision problems, since the image and/or scene modeling usually involves interactions between a subset of pixels and/or scene components. Often, these constraints are referred to as “prior” of the whole system. Through MRFs, one can use nodes to model variables of interest and combine different available cues that can be encoded by clique potentials within a unified probabilistic formulation. Then the inference can be performed via Maximum a posteriori (MAP) estimation:(4)xopt=argmaxx∈Xp(x).Since the potential functions are positive, we can define clique energy θcas a real function on a cliquec(c∈C):(5)θc(xc)=-logψc(xc).Due to the one-to-one mapping between θcand ψc, we also refer to θcas potential function (or clique potential) on clique c in the remainder of this survey, leading to a more convenient representation of the joint distribution p(x):(6)p(x)=1Zexp{-E(x)},where E(x) denotes the energy of the MRF and is defined as a sum of clique potentials:(7)E(x)=∑c∈Cθc(xc).Since the “-log” transformation between the distribution p(x) and the energy E(x) is a monotonic function, the MAP inference in MRFs (Eq. (4)) is equivalent to the minimization of E(x) as follows:(8)xopt=argminx∈XE(x).In cases of discrete MRFs where the random variables are discrete3We should note that continuous MRFs have also been used in the literature (e.g., [19–21]). An important subset of continuous MRFs that has been well studied is Gaussian MRFs[22].3(i.e., ∀i∈V,Xiconsists of a discrete set), the above optimization becomes a discrete optimization problem. Numerous works have been done to develop efficient MRF inference algorithms using discrete optimization theories and techniques (e.g., [23–31]), which have been successfully employed to efficiently solve many vision problems using MRF-based methods (e.g., [32–36]). Due to the advantages regarding both the modeling and the inference, as discussed previously, discrete MRFs have been widely employed to solve vision problems. We will provide a detailed survey on an important number of representative MRF-based vision models in Section 3 and MAP inference methods in Section 4.Factor graph[37,38] is a unified representation for both BNs and MRFs, which uses additional nodes, named factor nodes,4We call the nodes in original graphs usual nodes when an explicit distinction between the two types of nodes is required to avoid ambiguities.4to explicitly describe the factorization of the joint distribution in the graph. More specifically, a setFof factor nodes are introduced into the graph, each corresponding to an objective function term defined on a subset of usual nodes. Each factor encodes a potential function defined on a clique in cases of MRFs5Each factor encodes a local conditional probability distribution defined on a usual node and its parents in cases of BNs.5(see Eq. (3) or Eq. (7)). The associated joint probability is a product of factors:(9)p(x)=1Z∏f∈Fϕf(xf).Similar to MRFs, we can define the energy of the factor graph as:(10)E(x)=∑f∈Fθf(xf),whereθf(xf)=-logϕf(xf). Note that there can be more than one factor graphs corresponding to a BN or MRF. Fig. 1(b) and (c) shows two examples of factor graphs which provide two different possible representations for the MRF in Fig. 1(a).Factor graphs are bipartite, since there are two types of nodes and no edge exists between two nodes of same types. Such a representation conceptualizes in a clear manner the underlying factorization of the distribution in the graphical model. In particular for MRFs, factor graphs provide a feasible representation to describe explicitly the cliques and the corresponding potential functions when non-maximal cliques are also considered (e.g., Fig. 1(c)). The same objective can be hardly met using the usual graphical representation of MRFs. Computational inference is another strength of factor graphs representations. The sum-product and min-sum (or: max-product6The max-product algorithm is to maximize the probability p(x) which is a product of local functions (Eq. (9)), while the min-sum algorithm is to minimize the corresponding energy which is a sum of local energy functions (Eq. (10)). They are essentially the same algorithm.6) algorithms in the factor graph [38,11] generalize the classic counterparts [39,40] in the sense that the order of factors can be greater than two. Furthermore, since an MRF with loops may have no loop in its corresponding factor graph (e.g., see the MRF in Fig. 1(a) and the factor graphs in Fig. 1(b) and (c)), in such cases the min-sum algorithm in the factor graph can perform the MAP inference exactly with polynomial complexity. Such factor graphs without loop (e.g., Fig. 1(b) and (c)) are referred to as factor trees.According to the order of interactions between variables, MRF models can be classified into pairwise models and higher-order models. Another important class is Conditional Random Fields (CRFs). Below, we present these three typical models that are commonly used in vision community.The most common type of MRFs that is widely used in computer vision is the pairwise MRF, in which the associated energy is factorized into a sum of potential functions defined on cliques of order strictly less than three. More specifically, a pairwise MRF consists of a graphGwith a set(θi(·))i∈Vof unary potentials (also called singleton potentials) defined on single variables and a set(θij(·,·)){i,j}∈Eof pairwise potentials defined on pairs of variables. The MRF energy has the following form:(11)E(x)=∑i∈Vθi(xi)+∑{i,j}∈Eθij(xij).Pairwise MRFs have attracted the attention of a lot of researchers and numerous works have been done in past few decades, mainly due to the facts that pairwise MRFs inherit simplicity and computational efficiency, and that the interaction between pairs of variables is the most common and fundamental type of interactions required to model many vision problems. In computer vision, such works include both the modeling of vision problems using pairwise MRFs (e.g., [41–43,36,44]) and the efficient inference in pairwise MRFs (e.g., [23,26,28,27,45]). Two most typical graph structures used in computer vision are grid-like structures (e.g., Fig. 2) and part-based structures (e.g., Fig. 3). Grid-like structures provide a natural and reasonable representation for images, while part-based structures are often associated with deformable and/or articulated objects.Pairwise MRFs of grid-like structures (Fig. 2) have been widely used in computer vision to deal with numerous important problems, such as image denoising/restoration (e.g., [41,46,47]), super-resolution (e.g., [48–50]), stereo vision/multi-view reconstruction (e.g., [51,32,52]), optical flow and motion analysis (e.g., [53–56]), image registration and matching (e.g., [33,57–59]), segmentation (e.g., [60,42,36,61]) and over-segmentation (e.g., [62–64]).In this context, the nodes of an MRF correspond to the lattice of pixels.7Other homogeneously distributed units such as 3D voxels and control points [33] can also be considered in such MRFs.7The edges corresponding to pairs of neighbor nodes are considered to encode contextual constraints between nodes. The random variable xiassociated with each node i represents a physical quantity specific to problems8An MRF is called binary MRF if each node has only two possible values, 0 or 1.8(e.g., an index denoting the segment to which the corresponding pixel belongs for image segmentation problem, an integral value between 0 and 255 denoting the intensity of the corresponding pixel for gray image denoising problem, etc.). The data likelihood is encoded by the sum of the unary potentials θi(·), whose definition is specific to the considered application (e.g., for image denoising, such unary terms are often defined as a penalty function based on the deviation of the observed value from the underlying value). The contextual constraints compose a prior model on the configuration of the MRF, which is often encoded by the sum of all the pairwise potentials θij(·, ·). The most typical and commonly used contextual constraint is the smoothness, which imposes that physical quantities corresponding to the states of nodes vary “smoothly” in the spatial domain as defined by the connectivity of the graph. To this end, the pairwise potential θij(·, ·) between a pair {i, j} of neighbor nodes is defined as a cost term that penalizes the variation of the states between the two nodes:(12)θij(xij)=ρ(xi-xj),where ρ(·) is usually an even and non-decreasing function. In computer vision, common choices for ρ(·) are (generalized) Potts model9Note that Ising model[65,41] is a particular case of Potts model where each node has two possible states.9[66,67], truncated absolute distance and truncated quadratic, which are typical discontinuity preserving penalties:(13)ρ(xi-xj)=wij·(1-δ(xi-xj))(Pottsmodels)min(Kij,|xi-xj|)(truncatedabsolutedistance)min(Kij,(xi-xj)2)(truncatedquadratic),where wij⩾0 is a weight coefficient10wijis a constant for all pairs {i, j} of nodes in the original Potts model in [66].10for the penalties, Kronecker delta δ(x) is equal to 1 when x=0, and 0 otherwise, and Kijis a coefficient representing the maximum penalty allowed in the truncated models. More discontinuity preserving regularization functions can be found in for example [68,69]. Last, it should be mentioned that pairwise potentials in such grid-like MRFs can also be used to encode other contextual constraints, such as star shape priors[70], compact shape priors[71], layer constraints[62], Hausdorff distance priors[72] and ordering constraints[73,74].The grid-like MRF presented above can be naturally extended from pixels to other units. For example, there exist works that use superpixel primitives instead of pixel primitives when dealing with images (e.g., [75,76]), mainly aiming to gain computational efficiency and/or use superpixels as regions of support to compute features for mid-level and high-level vision applications. Another important case is the segmentation, registration and tracking of 3D surface meshes (e.g., [77,78]), where we aim to infer the configuration of each vertex or facet on the surface. In these cases, the node of MRFs can be used to model the superpixel, vertex or facet, nevertheless, the topology could be a less regular grid.MRFs of pictorial structures (Fig. 3) provide a natural part-based modeling tool for representing deformable objects and in particular articulated objects. Their nodes correspond to components of such objects. The corresponding latent variables represent the spatial pose of the components. An edge between a pair of nodes encode various interactions such as kinematic constraints between the corresponding pair of components. In [43], Pictorial model[80] was employed to deal with pose recognition of human body and face efficiently with dynamic programming. In this work, a tree-like MRF (see Fig. 3) was employed to model spring-like priors between pairs of components through pairwise potentials, while the data likelihood is encoded in the unary potentials each of which is computed from the appearance model of the corresponding component. The pose parameters of all the components are estimated through the MAP inference, which can be done very efficiently in such a tree-structured MRF using dynamic programming [81,16] (i.e., min-sum belief propagation [39,40,11]).Later, part-based models have been adopted and/or extended to deal with the pose estimation, detection and tracking of deformable object such as human body [20,82–85], hand [86,87] and other objects [88,89]. In [88], the part-based model was extended, with respect to that of [43], regarding the topology of the MRF as well as the image likelihood in order to deal with the pose estimation of animals such as cows and horses. The topology of part-based models was also extend to other typical graphs such as k-fans graphs[90,91] and out-planer graphs[92]. Pictorial structures conditioned on poselets[93] were proposed in [85] to incorporate higher-order dependency between the parts of the model while keeping the inference efficient (since the model becomes tree-structured at the graph-inference stage). Continuous MRFs of pictorial structures were proposed in [20,86] to deal with body and/or hand tracking, where non-parametric belief propagation algorithms [19,21] were employed to perform inference. In the subsequent papers [82,87], occlusion reasoning was introduced into their graphical models in order to deal with occlusions between different components. Indeed, the wide existence of such occlusions in cases of articulated objects is an important limitation of the part-based modeling. Recently, a rigorous visibility modeling in graphical models was achieved in [94] via the proposed joint 2.5D layered model where top-down scene-level and bottom-up pixel-level representations are seamlessly combined through local constraints that involve only pairs of variables (as opposed to previous 2.5D layered models where the depth ordering was commonly modeled as a total and strict order between all the objects), based on which image segmentation (pixel-level task), multi-object tracking and depth ordering (scene-level tasks) are simultaneously performed via a single pairwise MRF model.The notion of “part” can also refer to a feature point or landmark distributed on the surface of an object. In such a case, MRFs provide a powerful tool for modeling prior knowledge (e.g., generality and intra-class variations) on a class of shapes, which is referred to as statistical shape modeling[95]. The characterization of shape priors using local interactions (e.g., statistics on the Euclidean distance) between points can lead to useful properties such as translation and rotation invariances with respect to the global pose of the object in the observed image. Together with efficient inference methods, such MRF-based prior models have been employed to efficiently solve problems related to the inference of the shape model such as knowledge-based object segmentation (e.g., [96,97]). However, the factorization of probability or energy terms into an MRF can be very challenging, where good approximate solutions may be resorted to (e.g., [97,98]). In this line of research, recently [99] proposed to employ divergence theorem to exactly factorize regional data likelihood in their pairwise MRF model for object segmentation.Remark.The computer vision community has primarily focused on pairwise MRF models where interactions between parameters were often at the level of pairs of variables. This was a convenient approach driven mostly from the optimization viewpoint since pairwise MRFs inherit the lowest rank of interactions between variables and numerous efficient algorithms exist for performing inference in such models. Such interactions to a certain extent can cope with numerous vision problems (segmentation, pose estimation, motion analysis and object tracking, disparity estimation from calibrated views, etc.). However, their limitations manifest when a better performance is desired for those problems or when graph-based solutions are resorted to for solving more complex vision problems, where higher-order interactions between variables are needed to be modeled. On the other hand, the rapid development of computer hardwares in terms of memory capacity and CPU speed provides the practical base and motivates the consideration of higher-order interactions in vision models. In such a context, higher-order MRF models have attracted more and more attentions, and many related vision models and inference methods have been proposed.Higher-order MRFs11They are also referred to as high-order MRFs in part of the literature.11involve potential functions that are defined on cliques containing more than two nodes and cannot be further decomposed. Such higher-order potentials, compared to pairwise ones, allow a better characterization of statistics between random variables and increase largely the ability of graph-based modeling. We summarize below three main explorations of such advantages in solving vision problems.First, for many vision problems that were already addressed by pairwise models, higher-order MRFs are often adopted to model more complex and/or natural statistics as well as richer interactions between random variables, in order to improve the performance of the method. One can cite for example the higher-order MRF model proposed in [100,101] to better characterize image priors, by using the Product-of-Experts framework to define the higher-order potentials. Such a higher-order model was successfully applied in image denoising and inpainting problems [100,101].PnPotts model was proposed in [102,103], which considers a similar interaction as the generalized Potts model [67] (see Eq. (13)), but between n nodes instead of between two nodes, and leads to better performance in image segmentation. This model is a strict generalization of the generalized Potts model and has been further enriched towards robustPnmodel in [104,105]. [106] used higher-order smoothness priors for addressing stereo reconstruction problems, leading better performance than pairwise smoothness priors. Other types of higher-order pattern potentials were also considered in [107] to deal with image/signal denoising and image segmentation problems. All these works demonstrated that the inclusion of higher-order interactions is able to significantly improve the performance compared to pairwise models in the considered vision problems.Higher-order models become even more important in cases where we need to model measures that intrinsically involve more than two variables. A simple example is the modeling of second derivative (or even higher-order derivatives), which is often used to measure bending force in shape prior modeling such as active contour models (i.e., “Snake”) [108]. In [109], dynamic programming was adopted to solve “Snake” model in a discrete setting, which is essentially a higher-order MRF model. A third-order spatial prior based on second derivatives was also introduced to deal with image registration in [110]. In the optical flow formulation proposed in [111], higher-order potentials were used to encode angle deviation prior, non-affine motion prior as well as the data likelihood. [112] proposed a compact higher-order model that encodes a curvature prior for pixel labeling problem and demonstrated its performance in image segmentation and shape inpainting problems. Box priors were introduced in [113] for performing image segmentation given a user-provided object bounding box, where topological constraints defined based on the bounding box are incorporated into the whole optimization formulation and have been demonstrated to be able to prevent the segmentation result from over-shrinking and ensure the tightness of the object boundary delimited by the user-provided box. [114] proposed a higher-order illumination model to couple the illumination, the scene and the image together so as to jointly recover the illumination environment, scene parameters, and an estimate of the cast shadows given a single image and coarse initial 3D geometry. Another important motivation for employing higher-order models is to characterize statistics that are invariant with respect to global transformation when dealing with deformable shape inference [115,116]. Such approaches avoid explicit estimation of the global transformation such as 3D pose (translation, rotation and scaling) and/or camera viewpoint, which is substantially beneficial to both the learning and the inference of the shape model.Meanwhile, global models, which include potentials involving all the nodes, have been developed, together with the inference algorithms for them. For example, global connectivity priors (e.g., the foreground segment must be connected) were used in [117,118] to enforce the connectedness of the resulting pixel labeling in binary image segmentation, which were shown to be able to achieve better performance compared to merely using Potts-model with smoothness terms (see Section 3.1.1). In order to deal with unsupervised image segmentation where the number of segments are unknown in advance, [119,120] introduced ‘label costs” [121] into graph-based segmentation formulation, which imposes a penalty to a label l (or a subsetLsof labels) from the predefined possible label setLif at least one node is labeled as l (or an element inLs) in the final labeling result. By doing so, the algorithm automatically determines a subset of labels fromLthat are finally used, which corresponds to a model selection process. Another work in a similar line of research is presented in [122,123], where “object co-occurrence statistics” – a measure of which labels are likely to appear together in the labeling result – are incorporated within traditional pairwise MRF/CRF models for addressing object class image segmentation and have been shown to improve significantly the segmentation performance.A Conditional Random Field (CRF) [124,125] encodes, with the same concept as the MRF earlier described, a conditional distribution p(X∣D) where X denotes a tuple of latent variables and D a tuple of observed variables (data). Accordingly, the Markov properties for the CRF are defined on the conditional distribution p(X∣D). The local Markov properties in such a context become:(14)∀i∈V,Xi⊥XV-{i}|{XNi,D},while the global Markov property can also be defined accordingly. The conditional distribution p(X∣D) over the latent variables X is also a Gibbs distribution and can be written as the following form:(15)p(x|D)=1Z(D)exp{-E(x;D)},where the energy E(x; D) of the CRF is defined as:(16)E(x;D)=∑c∈Cθc(xc;D).We can observe that there is no modeling on the probabilistic distribution over the variables in D, which relaxes the concern on the dependencies between these observed variables, whereas such dependencies can be rather complex. Hence, CRFs significantly reduce difficulty in modeling the joint distribution of the latent and observed variables, and consequently, observed variables can be incorporated into the CRF framework in a more flexible way. Such a flexibility is one of the most important advantages of CRFs compared with generative MRFs12Like [126], we use the term generative MRFs to distinguish the usual MRFs from CRFs.12when used to model a system. For example, the fact that clique potentials can be data dependent in CRFs could lead to more informative interactions than data independent clique potentials. Such an concept was adopted for example in binary image segmentation [127], where the intensity contrast and the spatial distance between neighbor pixels are employed to modulate the values of pairwise potentials of a grid-like CRF, as opposed to Potts models (see Section 3.1.1). Despite the difference in the probabilistic explanation, the MAP inferences in generative MRFs and CRFs boil down to the same problem.CRFs have been applied to various fields such as computer vision, bioinformatics and text processing among others. In computer vision, besides [127], grid-like CRFs were also employed in [128] to model spatial dependencies in the image, leading to a data-dependent smoothness terms between neighbor pixels. With the learned parameters from training data, a better performance has been achieved in the image restoration experiments compared to the classic Ising MRF model [41]. Hierarchical CRFs have also been developed to incorporate features from different levels so as to better perform object class image segmentation. One can cite for example the multi-scale CRF model introduced in [129] and “associative hierarchical CRFs” proposed in [130]. Moreover, CRFs have also been applied for object recognition/detection. For example, a discriminative part-based approach was proposed in [131] to recognize objects based on a tree-structured CRF. In [132], object detectors were combined within a CRF model, leading to an efficient algorithm to jointly estimate the class category, location, and segmentation of objects/regions from 2D images. Last, it is worth mentioning that recently, based on a mean field approximation to the CRF distribution, [133] proposed a very efficient approximate inference algorithm for fully connected grid-like CRFs where pairwise potentials correspond to a linear combination of Gaussian kernels, and demonstrated that such a dense connectivity at the pixel level significantly improves the accuracy in class segmentation compared to 4-neighborhood system (Fig. 2) [134] and robustPnmodel[105]. Their techniques were further adopted and extended to address optical flow computing [135,136], and to address cases where pairwise potentials are non-linear dissimilarity measures that do not required to be distance metrics [137].An essential problem regarding the application of MRF models is how to infer the optimal configuration for each of the nodes. Here, we focus on the MAP inference (i.e., Eq. (4)) in discrete MRFs, which boils down to an energy minimization problem as shown in Eq. (8). Such a combinatorial problem is known to be NP-hard in general [23,25], except for some particular cases such as MRFs of bounded tree-width [138,139,12] (e.g., tree-structured MRFs [39]) and pairwise MRFs with submodular energy [25,140].The most well-known early (before the 1990s) algorithms for optimizing the MRF energy are iterated conditional modes (ICM) [141], simulated annealing methods (e.g., [41,142,143]) and highest confidence first (HCF)[144,145]. While being computational efficient, ICM and HCF suffer from their limited ability to recover a good optimum. On the other hand, for simulated annealing methods, even if in theory they provide certain guarantees on the quality of the obtained solution, in practice from computational viewpoint such methods are impractical. In the 1990s, more advanced methods, such as loopy belief propagation (LBP) (e.g., [48,146,147]) and graph cuts techniques (e.g., [46,51,67,148,23]), provided powerful alternatives to the aforementioned methods from both computational and theoretical viewpoints and have been used to solve numerous visual perception problems (e.g., [48,58,46,148,32,60,42]). Since then, the MRF optimization has been experiencing a renaissance, and more and more researchers have been working on it. For recent MRF optimization techniques, one can cite for example QPBO techniques (e.g., [149–152]), LP primal–dual algorithms (e.g., [153,154,29]) as well as dual methods (e.g., [26,28,154,155]).There exist three main classes of MAP inference methods for pairwise MRFs and they also have been extended to deal with higher-order MRFs. In order to provide an overview of them, in this section we will first review graph cuts and their extensions for minimizing the energy of pairwise MRFs in Section 4.1. Then in Section 4.2 and Appendix B, we will describe the min-sum belief propagation algorithm in factor trees and also show its extensions to dealing with an arbitrary pairwise MRF. Following that, we review in Section 4.3 dual methods for pairwise MRFs, such as tree-reweighted message passing methods (e.g., [26,28]) and dual-decomposition approaches (e.g., [154,156]). Last but not least, a survey on inference methods for higher-order MRFs will be provided in Section 4.4.Graph cuts consist of a family of discrete algorithms that use min-cut/max-flow techniques to efficiently minimize the energy of discrete MRFs and have been used to solve many vision problems (e.g., [46,148,42,32,36,34]).The basic idea of graph cuts is to construct a directed graphGst=(Vst,Est)(called s-t graph13Note that generations such as multi-way cut problem [157] which involves more than two terminal nodes are NP-hard.13) with two special terminal nodes (i.e., the source s and the sink t) and non-negative capacity setting c(i, j) on each directed edge(i,j)∈Est, such that the cost C(S, T) (Eq. (17)) of the s-t cut that partitions the nodes into two disjoint sets (S and T such that s∈S and t∈T) is equal to the energy of the MRF with the corresponding configuration14The following rule can be used to associate an s-t cut to an MRF labeling: for a nodei∈Vst−{s,t},(i)ifi∈S, the label xiof the corresponding node in the MRF is equal to 0; (ii) if i∈T, the label xiof the corresponding node in the MRF is equal to 1.14x (up to a constant difference):(17)C(S,T)=∑i∈S,j∈T,(i,j)∈Estc(i,j).An MRF that has such an s-t graph is called graph-representable15Note that, in general, such an s-t graph is not unique for a graph-representable MRF.15and can be solved in polynomial time using graph cuts [25]. The minimization of the energy of such an MRF is equivalent to the minimization of the cost of the s-t-cut problem (i.e., min-cut problem). The Ford and Fulkerson theorem [158] states that the solution of the min-cut problem corresponds to the maximum flow from the source s to the sink t (i.e., max-flow problem). Such a problem can be efficiently solved in polynomial time using many existing algorithms such as Ford-Fulkerson style augmenting paths algorithms [158] and Goldberg-Tarjan style push-relabel algorithms [159]. Note that the min-cut problem and the max-flow problem are actually dual LP problems of each other [160].Unfortunately, not all the MRFs are graph-representable. Previous works have been done to explore the class of graph-representable MRFs (e.g., [161,24,25,140]). They demonstrated that a pairwise discrete MRF is graph-representable so that the global minimum of the energy can be achieved in polynomial time via graph cuts, if the energy function of the MRF is submodular (see Appendix A for the definition of submodularity). However, in numerous vision problems, more challenging energy functions that do not satisfy the submodular condition are often required. The minimization of such non-submodular energy functions is NP-hard in general [23,25] and an approximation algorithm would be required to approach the global optimum.More than two decades ago, [46] first proposed to use min-cut/max-flow techniques to exactly optimize the energy of a binary MRF (i.e., Ising model) for image restoration in polynomial time. However, the use of such min-cut/max-flow techniques did not draw much attention in computer vision community in the following decade since then, probably due to the fact that the work was published in a journal of statistics community and/or that the model considered in [46] is quite simple. Such a situation has changed in late 1990s when a number of techniques based on graph cuts were proposed to solve more complicated MRFs. One can cite for example the works described in [67,51,148], which proposed to use min-cut/max-flow techniques to minimize the energy of multi-label MRFs. In particular, the work introduced in [67] achieved, based on the proposed optimization algorithms, much more accurate results than the state-of-the-art in computing stereo depth, and thus motivated the use of their optimization algorithms for many other problems (e.g., [162–164]), also leading to excellent performance. This significantly popularized graph cuts techniques in computer vision community. Since then, numerous works have been done for exploring larger subsets of MRFs that can be exactly or approximately optimized by graph cuts and for developing more efficient graph-cuts-based algorithms.There are two main methodologies for solving multi-label MRFs based on graph cuts: label-reduction and move-making.The first methodology (i.e., label-reduction) is based on the observation that some solvable types of multi-label MRFs can be exactly solved in polynomial time using graph cuts by first introducing auxiliary binary variables each corresponding to a possible label of a node and then deriving a min-cut problem that is equivalent to the energy minimization of the original MRF. We can cite for example an efficient graph construction method proposed in [24] to deal with arbitrary convex pairwise MRFs, which was further extended to submodular pairwise MRFs in [140]. Such a methodology can perform MAP inference in some types of MRFs. However, the solvable types are quite limited, since it is required that the obtained binary MRF (via introducing auxiliary binary variables) should be graph-representable. Whereas, the other optimization methodology (i.e., move-making) provides a very important tool for addressing larger sub-classes of MRFs.The main idea of move-making is to optimize the MRF energy by defining a set of proposals (i.e., possible “moves”) based on the initial MRF configuration and choosing the best move as the initial configuration for the next iteration, which is done iteratively until the convergence when no move leads to a lower energy. The performance of an algorithm developed based on such a methodology mainly depends on the size of the set (denoted byM) of proposals at each iteration. For example, ICM [141] iteratively optimizes the MRF energy with respect to a node by fixing the configuration of all the other nodes. It can be regarded as the simplest move-making approach, where|M|is equal to the number of labels of the node that is considered to make move at an iteration. ICM has been shown to perform poorly when dealing with MRF models for visual perception, due to the small setMof proposals [35].Graph-cuts-based methods have been proposed to exponentially increase the size of the setMof proposals, for example, by considering the combination of two possible values for all the nodes(|M|=2|V|). In the representative works of [165,23], α-expansion and αβ-swap were introduced to generalize binary graph cuts to handle pairwise MRFs with metric and/or semi-metric energy. An α-expansion refers to a move from x to x′ such that: xi≠xi′⇒xi′=α. An αβ-swap means a move from x to x′ such that: xi≠xi′⇒xi, xi′∈{α, β}. [165,23] proposed efficient algorithms for determining the optimal expansion or swap moves by converting the problems into binary labeling problems which can be solved efficiently using graph cuts techniques. In such methods, a drastically largerMcompared to that of ICM makes the optimization less prone to be trapped at local minima and thus leads to much better performance [35]. Moreover, unlike ICM which has no optimum quality guarantee, the solution obtained by α-expansion has been proven to possess a bounded ratio between the obtained energy and the global optimal energy [165,23].In addition, range moves methods [166–168] have been developed based on min-cut/max-flow techniques to improve the optimum quality in addressing MRFs with truncated convex priors. Such methods explore a large search space by considering a range of labels (i.e., an interval of consecutive labels), instead of dealing with one/two labels at each iteration as what is done in α-expansion or αβ-swap. In particular, range expansion has been demonstrated in [167] to provide the same multiplicative bounds as the standard linear programming (LP) relaxation (see Section 4.3) in polynomial time, and to provide a faster algorithm for dealing with the class of MRFs with truncated convex priors compared to LP-relaxation-based algorithms such as tree-reweighted Message Passing (TRW) techniques (see Section 4.3). Very recently, [169] proposed a dynamic-programming-based algorithm for approximately performing α-expansion, which significantly speeds up the original α-expansion algorithm [165,23].Last, we should note that expansion is a very important concept in optimizing the energy of a multi-label MRF using graph cuts. Many other works in this direction are based on or partially related to it, which will be reflected in the following discussion.Graph cuts techniques have also been extended to deal with non-submodular binary energy functions. Roof duality was proposed in [170], which provides an LP relaxation approach to achieving a partial optimal labeling for quadratic pseudo-boolean functions (the solution will be a complete labeling that corresponds to global optimum if the energy is submodular). The persistency property of roof duality indicates that the configurations of all the labeled nodes are exactly those corresponding to the global optimum. Hence, QPBO at least provides us with a partial labeling of the MRF and the number of unlabeled nodes depends on the number of non-submodular terms included in the MRF. Such a method was efficiently implemented in [149], which is referred to as Quadratic Pseudo-Boolean Optimization (QPBO) algorithm and can be regarded as a graph-cuts-based algorithm with a special graph construction where two nodes in s-t graph are used to represent two complementary states of a node in the original MRF [150]. By solving min-cut/max-flow in such an s-t graph, QPBO outputs a solution assigning 0,1 or12to each node in the original MRF, where the label12means the corresponding node is unlabeled.Furthermore, two different techniques were introduced in order to extend QPBO towards achieving a complete solution. One is probing (called QPBO-P) [151,152], which aims to gradually reduce the number of unlabeled nodes (either by finding the optimal label for certain unlabeled nodes or by regrouping a set of unlabeled nodes) until convergence by iteratively fixing the label of a unlabeled node and performing QPBO. The other one is improving (called QPBO-I) [152], which starts from a complete labeling y and gradually improves such a labeling by iteratively fixing the labels of a subset of nodes as those specified y and using QPBO to get a partial labeling to update y.Besides, QPBO techniques have been further combined with the label-reduction and move-making techniques presented previously to deal with multi-label MRFs. For the former case, in [171], a multi-label MRF is converted into an equivalent binary MRF [24] and then QPBO techniques are employed to solve the linear relaxation of the obtained binary MRF. It provides a partial optimal labeling for multi-label MRFs. Nevertheless, a disadvantage of such an approach is the expensive computational complexity. For the latter case, an interesting combination of QPBO and move-making techniques was proposed in [172], which is referred to as fusion moves. Given two arbitrary proposals (x(1), x(2)) of the full labeling of the MRF, fusion moves combine the proposals together via a binary labeling problem, which is solved using QPBO so as to achieve a new labeling x′ such that:∀i,xi′∈{xi(1),xi(2)}. Using the proposed label selection rule, x′ is guaranteed to have an energy lower than or equal to the energies of both proposals (x(1), x(2)). Hence, fusion moves provides an effective tool for addressing the optimization of multi-label discrete/continuous MRFs. In addition, it turns out that fusion moves generalize some previous graph-cuts-based methods such as α-expansion and αβ-swap, in the sense that the latter methods can be formulated as fusion moves with particular choices of proposals. This suggests that fusion moves can serve as building block within various existing optimization schemes so as to develop new techniques, such as the approaches proposed in [172] for the parallelization of MRF optimization into several threads and the optimization of continuous-labeled MRFs with 2D labels.We should also note that different methods have been developed to increase the efficiency of graph-cuts-based algorithms, in particular in the context of dynamic MRFs (i.e., the potential functions vary over time, whereas the change between two successive instants is usually quite small). Below are several representative works in this line of research.A dynamic max-flow algorithm (referred to as dynamic graph cuts) was proposed in [173,27] to accelerate graph cuts when dealing with dynamics MRFs, where the key idea is to reuse the flow obtained by solving the previous MRF to initialize the min-cut/max-flow problems of the current MRF so as to significantly reduce the computational time of min-cut. Another dynamic algorithm was also proposed in [174] to improve the convergence of optimization for dynamic MRFs, by using the min-cut solution of the previous MRF to generate an initialization for solving the current MRF.In [154,29], a primal–dual scheme based on linear programming relaxation (referred to as FastPD) was proposed for optimizing the MRF energy, by recovering pair of solutions for the primal and the dual such that the gap between them is minimized.16FastPD can also be viewed as a generalization of α-expansion.16This method exploits information coming from both the original MRF optimization problem and its dual problem, and achieves a substantial speedup with respect to previous methods such as [23,153]. In addition, it can also speed up the optimization in the case of dynamic MRFs, where one can expect that the new pair of primal–dual solutions is closed to the previous one.Besides, [175,176] proposed two similar but simpler techniques with respect to that of [154,29] to achieve a similar computational efficiency. The main idea of the first one (referred to as dynamic α-expansion) is to “recycle” results from previous problem instances. Similar to [173,27,174], the flow from the corresponding move in the previous iteration is reused for solving an expansion move in a particular iteration. And when dealing with dynamic MRFs, the primal and dual solutions obtained from the previous MRF are used to initialize the min-cut/max-flow problems for the current MRF. The second method aims to simplify the energy function by solving partial optimal MRF labeling problems [171,177] and reducing the number of unlabeled variables, while the dual (flow) solutions of such problems are used to generate a “good” initialization for the dynamic α-expansion algorithm.Last but not least, based on the primal–dual interpretation of the expansion algorithm introduced by [154,29], an approach was proposed in [178] to optimize the choice of the move space for each iteration by exploiting the primal–dual gap. As opposed to traditional move-making methods that search for better solutions in some pre-defined move spaces around the current solution, such an approach aims to greedily determine the move space (e.g., the optimal value of α in the context of α-expansion) that will lead to largest decrease in the primal–dual gap at each iteration. It was demonstrated experimentally to increase significantly the optimization efficiency.Belief propagation algorithms use local message passing to perform inference on graphical models. They provide an exact inference algorithm for tree-structured discrete MRFs, while an approximate solution can be achieved for a loopy graph. In particular, for those loopy graphs with low tree-widths such as cycles, extended belief propagation methods such as junction tree algorithm[138,139,12] provide an efficient algorithm to perform exact inference. These belief propagation algorithms have been adopted to perform MAP inference in MRF models for a variety of vision problems (e.g., [43,48,58,179,92]).Belief propagation (BP)[39,40,11] was proposed originally for exactly solving MAP inference (min-sum algorithm) and/or maximum-marginal inference (sum-product algorithm) in a tree-structured graphical model in polynomial time. This type of methods can be viewed as a special case of dynamic programming in graphical models [81,16,180]. A representative vision model that can be efficiently solved by BP is the pictorial model [80,43] (see Section 3.1.2).In the min-sum algorithm17Note that all the BP-based algorithms presented in Section 4.2 include both min-sum and sum-product versions. We focus here on the min-sum version. Nevertheless, the sum-product version can be easily obtained by replacing the message computation with the sum of the product of function terms. We refer the reader to [38,11,12] for more details.17for a tree-structured MRF, a particular node is usually designated as the “root” of the tree. Then messages are propagated inwards from the leaves of the tree towards the root, where each node sends its message to its parent once it has received all incoming messages from its children. During the message passing, a local lookup table is generated for each node, recording the optimal labels of all children for each of its possible labels. Once all messages arrive at the root node, a minimization is performed over the sum of the messages and the unary potentials of the root node, giving the minimum value for the MRF energy as well as the optimal label for the root node. In order to determine the labels for the other nodes, the optimal label is then propagated outwards from the root to the leaves of the tree, simply via checking the lookup tables obtained previously, which is usually referred to as back-tracking. A detailed algorithm is provided in Algorithm 1 (Section B) based on the factor graph representation [38,11], since as we mentioned in Section 2.3, the factor graph makes the BP algorithm applicable to more cases compared to the classic min-sum algorithm applied on a usual pairwise MRF [48].Note that reparameterization (also known as equivalent transformation) of the MRF energy (e.g., [181,28]) is an important concept in MRF optimization. Two different settings of potentials (e.g., θi, θijin Eq. (11)) leading to the same MRF energy (up to a constant difference) for any MRF configuration differ by a reparameterization. Reparameterization provides an alternative interpretation of belief propagation, which for example leads to a memory-efficient implementation of belief propagation [28]. Meanwhile, max-flow based algorithms also have been shown to relate to the principle of reparameterization[27]. Such a relationship (via reparameterization) sheds light on some connection between max-flow and message passing based algorithms.The tree-structured constraint limits the use of the standard belief propagation algorithm presented above, whereas loopy MRFs are often required to model vision problems. Hence, researchers have investigated to extend the message passing concept for minimizing the energy of arbitrary graphs.Loopy belief propagation (LBP), a natural step towards this direction, performs message passing iteratively in the graph (e.g., [182,48,146,147]) despite of the existence of loops. We refer the reader to [48,146] for the details and discussion on the LBP algorithm. Regarding the message passing scheme in loopy graphs, there are two possible choices: parallel or sequential. In the parallel scheme, messages are computed for all the edges at the same time and then the messages are propagated for the next round of message passing. Whereas in the sequential scheme, a node propagates the message to one of its neighbor node at each round and such a message will be used to compute the messages sent by that neighbor node. [183] showed empirically that the sequential scheme was significantly faster than the parallel one, while the performance of both methods was almost the same.A number of works have been done to improve the efficiency of message passing by exploiting particular types of graphs and/or potential functions (e.g., [147,184,185]). For example, based on the distance transform algorithm [186], a strategy was introduced in [147] for speeding up belief propagation for a subclass of pairwise potentials that only depend on the difference of the variables such as those defined in Eq. (13), which reduces the complexity of a message passing operation between two nodes from quadratic to linear in the number of possible labels per node. Techniques have also been proposed for accelerating the message passing in bipartite graphs and/or grid-like MRFs [147,185], and in robust truncated models where a pairwise potential is equal to a constant for most of the possible state combinations of the two nodes [184]. Recently, [187] proposed a parallel message computation scheme, inspired from [147] but applicable to a wider subclass of MRFs than [147]. Together with a GPU implementation, such a scheme substantially reduces the running time in various MRF models for low-level vision problems.Despite the fact that LBP performs well for a number of vision applications such as [48,58], they cannot guarantee to converge to a fixed point, while their theoretical properties are not well understood. Last but not least, their solution is generally worse than more sophisticated generalizations of message passing algorithms (e.g., [26,28,45]) that will be presented in Section 4.3[35].Junction tree algorithm (JTA) is an exact inference method in arbitrary graphical models [138,139,12]. The key idea is to make systematic use of the Markov properties implied in graphical models to decompose a computation of the joint probability or energy into a set of local computations. Such an approach bears strong similarities with message passing in the standard belief propagation or dynamic programming. In this sense, we regard JTA as an extension of the standard belief propagation.An undirected graph has a junction tree if and only if it is triangulated (i.e., there is no chordless18A cycle is said to be chordless if there is no edge between any pair of nodes that are not successors in the cycle.18cycle in the graph). For any MRF, we can obtain a junction tree by first triangulating the original graph (i.e., making the graph triangulated by adding additional edges) and then finding a maximal spanning tree for the maximal cliques contained in the triangulated graph (e.g., Fig. 4). Based on the obtained junction tree, we can perform local message passing to do the exact inference, which is similar to standard belief propagation in factor trees. We refer the reader to [139,12] for details.The complexity of the inference in a junction tree for a discrete MRF is exponential with respect to its width W, which is defined as the maximum cardinal over all the maximal cliques minus 1. Hence, the complexity is dominated by the largest maximal cliques in the triangulated graph. However, the triangulation process may produce large maximal cliques, while finding of an optimal junction tree with the smallest width for an arbitrary undirected graph is an NP-hard problem. Furthermore, MRFs with dense initial connections could lead to maximal cliques of very high cardinal even if an optimal junction tree could be found [12]. Due to the computational complexity, the junction tree algorithm becomes impractical when the tree width is high, although it provides an exact inference approach. Thus it has been only used in some specific scenarios or some special kinds of graphs that have low tree widths (e.g., cycles and outer-planar graphs whose widths are equal to 2). For example, JTA was employed in [179] to deal with simultaneous localization and mapping (SLAM) problem, and was also adopted in [92] to perform exactly inference in outer-planar graphs within the whole dual-decomposition framework. In order to reduce the complexity, nested junction tree technique was proposed in [188] to further factorize large cliques. Nevertheless, the gain of such a process depends directly on the initial graph structure and is still insufficient to make JTA widely applicable in practice.The MAP inference in pairwise MRFs (Eqs. (8) and (11)), can be reformulated as an integer linear programming (ILP)[189] problem as follows:(18)minτE(θ,τ)=〈θ,τ〉=∑i∈V∑a∈Xiθi;aτi;a+∑{i,j}∈E∑(a,b)∈Xi×Xjθij;abτij;abs.t.τ∈τG=τ∑a∈Xiτi;a=1∀i∈V∑a∈Xiτij;ab=τj;b∀{i,j}∈E,b∈Xjτi;a∈{0,1}∀i∈V,a∈Xiτij;ab∈{0,1}∀{i,j}∈E,(a,b)∈Xi×Xj.where θi;a=θi(a), θij;ab=θij(a, b), binary variables19[·] is equal to one if the argument is true and zero otherwise.19τi;a=[xi=a] and τij;ab=[xi=a, xj=b],τdenotes the concatenation of all these binary variables which can be defined as((τi;a)i∈V,a∈Xi,(τij;ab){i,j}∈E,(a,b)∈Xi×Xj), andτGdenotes the domain ofτ. We will use MRF-MAP to refer to this original MAP inference problem. Unfortunately, the above ILP problem is NP-hard in general.20Note that, very recently, [190] experimentally demonstrated that for a subclass of small-size MRFs, advanced integer programming algorithms based on cutting-plane and branch-and-bound techniques can have global optimality property while being computational efficient.20Many approximation algorithms of MRF optimization have been developed based on solving some relaxation to such a problem.Linear Programming (LP) relaxation has been widely adopted to address the MRF-MAP problem in Eq. (18), aiming to minimize E(θ,τ) in a relaxed domainτˆG(called local marginal polytope) which is obtained by simply replacing the integer constraints in Eq. (18) by non-negative constraints (i.e., τi;a⩾0 and τij;ab⩾0). Such a relaxed problem will be referred to as MRF-LP. It is generally infeasible to directly apply generic LP algorithms such as interior point methods[191] to solve MRF-LP for MRF models in computer vision [192], due to the fact that the number of variables involved inτis usually huge. Instead, many methods have been designed based on solving some dual to MRF-LP, i.e., maximizing the lower bound of E(θ,τ) provided by the dual. An important class of such methods are referred to as tree-reweighted message passing (TRW) techniques (e.g., [26,28]), which approach the solution to MRF-LP via a dual problem defined by a convex combination of trees. The optimal value of such a dual problem and that of MRF-LP coincide [26]. In [26], TRW was introduced to solve MRF-MAP by using edge-based and tree-based message passing schemes (called TRW-E and TRW-T respectively), which can be viewed as combinations of reparameterization and averaging operations on the MRF energy. However, the two schemes do not guarantee the convergence of the algorithms and the value of the lower bound may fall into a loop. Later, a sequential message passing scheme (known as TRW-S) was proposed in [28]. It updates messages in a sequential order instead of a parallel order used in TRW-E and TRW-T, which makes the lower bound will not decrease in TRW-S. Regarding the convergence, TRW-S will attain a point that satisfies a condition referred to as weak tree agreement (WTA)[193] and the lower bound will not change any more since then.21[28] observed in the experiments that TRW-S would finally converge to a fixed point but such a convergence required a lot of time after attaining WTA. Nevertheless, such a convergence may not be necessary in practice, since the lower bound will not change any more after attaining WTA.21Regarding the optimality, TRW-S cannot guarantee the global maximum of the lower bound in general. Nevertheless, for the case of binary pairwise MRFs, a WTA fixed point corresponds to the global maximum of the lower bound, and thus the global minimum of MRF-LP [193]. Furthermore, if a binary pairwise MRF is submodular, a WTA fixed point always achieves the global optimum of the MRF-MAP problem. In [35], a set of experimental comparisons between ICM, LBP, α-expansion, αβ-swap and TRW-S were done based on MRFs with smoothness priors, showing that TRW-S and α-expansion perform much better than the others. For other representative methods solving a dual to MRF-LP, one can cite for example the message passing algorithm based on block coordinate descent proposed in [194], the min-sum diffusion algorithm [195] and the augmenting DAG algorithm22Both the min-sum diffusion algorithm and the augmenting DAG algorithm were reviewed in [155].22[196], etc. Note that, since the LP-relaxation can be too loose to approach the solution of the MRF-MAP problem, the tightening of the LP-relaxation has also been investigated for achieving a better optimum of the MRF-MAP problem (e.g., [197–199,30,200,201]).Another important relaxation (i.e., Lagrangian relaxation) to MRF-MAP is related to dual-decomposition[202], which is a very important optimization methodology. Dual-decomposition was employed in [45,156] for addressing the MRF-MAP problem (referred to as MRF-DD). The key idea is: instead of minimizing directly the energy of the original MRF-MAP problem which is too complex to solve directly, we decompose the original problem into a set of subproblems which are easy to solve. Based on a Lagrangian dual of the MRF-MAP problem, the sum of the minima of the subproblems provides a lower bound on the energy of the original MRF. This sum is maximized using projected subgradient method so that a solution to the original problem can be extracted from the Lagrangian solutions [156]. This leads to an MRF optimization framework with a high flexibility, generality and convergence property. First, the Lagrangian dual problem can be globally optimized due to the convexity of the dual function, which is a more desired property than WTA condition guaranteed by TRW-S. Second, different decompositions can be considered to deal with MRF-MAP, leading to different relaxations. In particular, when the master problem is decomposed into a set of trees, the obtained Lagrangian relaxation is equivalent to the LP relaxation of MRF-MAP. However, more sophisticated decompositions23A theoretical conclusion regarding the comparison of the tightness between two different decompositions has been drawn in [156].23can be considered to tighten the relaxation (e.g., decompositions based on outer-planar graphs [92] and k-fan graphs [91]). Third, there is no constraint on how the inference in slave problems is done and one can apply specific optimization algorithms to solve slave problems. A number of interesting applications have been proposed within such a framework, which include the graph matching method proposed in [203], the higher-order MRF inference method developed in [107], and the algorithm introduced in [204] for jointly inferring image segmentation and appearance histogram models. In addition, various techniques have been proposed to speed up the convergence of MRF-DD algorithms. For example, two approaches were introduced in [31]. One is to use a multi-resolution hierarchy of dual relaxations, and the other consists of a decimation strategy that gradually fixes the labels for a growing subset of nodes as well as their dual variables during the process. [205] proposed to construct a smooth approximation of the energy function of the master problem by smoothing the energies of the slave problems so as to achieve a significant acceleration of the MRF-DD algorithm. A distributed implementation of graph cuts was introduced in [206] to solve the slave problems in parallel.Last, it is worth mentioning that an advantage of all dual methods is that we can tell how far the solution of MRF-MAP is from the global optimum, simply by measuring the gap between the lower bound obtained from solving the dual problem and the energy of the obtained MRF-MAP solution.Recent development of higher-order MRF models for vision problems has been shown in Section 3.2. In such a context, numerous works have been devoted in the past decade to search for efficient inference algorithms in higher-order models, towards expanding their use in vision problems that usually involve a large number of variables. One can cite for example [100,101], where a simple inference scheme based on a conjugate gradient method was developed to solve their higher-order model for image restoration. Since then, besides a number of methods for solving specific types of higher-order models (e.g., [102,207,118,119,122]), various techniques have also been proposed to deal with more general MRF models (e.g., [208,209,107,210,211]). These inference methods are highly inspired from the ones for pairwise MRFs. Thus, similar to pairwise MRFs, there are also three main types of approaches for solving higher-order MRFs, i.e., algorithms based on order reduction and graph cuts, higher-order extensions of belief propagation, and dual methods.Most of existing methods tackle inference in higher-order MRFs using a two-stage approach: first to reduce a higher-order model to a pairwise one with the same minimum, and then to apply standard methods such as graph cuts to solve the obtained pairwise model.The idea of order reduction exists for long time. More than thirty years ago, a method (referred to as variable substitution) was proposed in [212] to perform order reduction for models of any order, by introducing auxiliary variables to substitute products of variables.24Here, we consider binary higher-order MRFs and their energy functions can be represented in form of pseudo-Boolean functions[161].24However, this approach leads to a large number of non-submodular components in the resulting pairwise model. This is due to the hard constraints involved in the substitution, which causes large difficulty in solving the obtained pairwise model. This may explain why its impact is rather limited in the literature [161,213], since our final interest is solving higher-order models. In [213], QPBO was employed to solve the resulting pairwise model, nevertheless, only third-order potentials were tested in the experiments.A better reduction method that generally produces fewer non-submodular components was proposed in [25], in order to construct s-t graph for a third-order binary MRF. This reduction method was studied from an algebraic viewpoint in [214] and led to some interesting conclusions towards extending this method to models of an arbitrary order. Based on these works, [210,215] proposed a generalized technique that can reduce any higher-order binary MRF into a pairwise one, which can then be solved by QBPO. Furthermore, [210,215] also extended such a technique to deal with multi-label MRFs by using fusion moves [172]. Very recently, aiming to obtain a pairwise model that is as easy as possible to solve (i.e., has as few as possible non-submodular terms), [216] proposed to approach order reduction as an optimization problem, where different factors are allowed to choose different reduction methods in order to optimize an objective function defined using a special graph (referred to as order reduction inference graph). In the same line of research, [211] proposed to perform order reduction on a group of higher-order terms at the same time instead of on each term independently [210,215], which has been demonstrated both theoretically and experimentally to lead to better performance compared to [210,215].Graph-cuts techniques have also been considered to cope either with specific vision problems or certain classes of higher-order models. For example, [102,103] characterized a class of higher-order potentials (i.e.,PnPotts model). It was also showed that the optimal expansion and swap moves for these higher-order potentials can be computed efficiently in polynomial time, which leads to an efficient graph-cuts-based algorithm for solving such models. Such a technique was further extended in [104,105] to a wider class of higher-order models (i.e., robustPnmodel). In addition, graph-cuts-based approaches were also proposed in [122,123,119,120,217] to perform inference in their higher-order MRFs with global potentials that encode co-occurrence statistics and/or label costs. Despite the fact that such methods were designed for a limited range of problems that often cannot be solved by a general inference method, they better capture the characteristics of the problems and are able to solve the problems relatively efficiently.As mentioned in Section 4.2, the factor graph representation of MRFs enables the extension of classic min-sum belief propagation algorithm to higher-order cases. Hence, loopy belief propagation in factor graphs provides a straightforward way to deal with inference in higher-order MRFs. Such an approach was adopted in [208] to solve their higher-order Fields-of-Experts model.A practical problem for propagating messages in higher-order MRFs is that the complexity increases exponentially with respect to the highest order among all cliques. Various techniques have been proposed to accelerate the belief propagation in special families of higher-order potentials. For example, [218,209,219] proposed efficient message passing algorithms for some families of potentials such as linear constraint potentials and cardinality-based potentials. Recently, the max-product message passing was accelerated in [220] by exploiting the fact that a clique potential often consists of a sum of potentials each involving only a sub-clique of variables, whose expected computational time was further reduced in [221].The LP relaxation of the MRF-MAP problem for pairwise MRFs (see Section 4.3) can be generalized to the cases of higher-order MRFs. Such a generalization was studied in [222,200], where min-sum diffusion[195] was adopted to achieve a method for optimizing the energy of higher-order MRFs, which is referred to as n-ary min-sum diffusion.25The method was originally called n-ary max-sum diffusion in [222,200] due to the fact that a maximization of objective function was considered.25Recently, such techniques were adopted in [223] to efficiently solve in a parallel/distributed fashion higher-order MRF models of triangulated planar structure.The dual-decomposition framework [202,154], which has been presented in Section 4.3, can also be adopted to deal with higher-order MRFs. This was first demonstrated in [107], where inference algorithms were introduced for solving a wide class of higher-order potential referred to as pattern-based potentials.26For example,PnPotts model [103] is a sub-class of pattern-based potentials.26Also based on the dual-decomposition framework, [115] proposed to solve their higher-order MRF model by decomposing the original problem into a series of subproblems each corresponding to a factor tree. In [224], such a framework was combined with order-reduction [210,215] and QPBO techniques [150] to solve higher-order graph-matching problems.Last, it is worth mentioning that the sparsity of potentials has been exploited, either explicitly or implicitly, in many of the above higher-order inference methods. For example, [225] proposed a compact representation for “sparse” higher-order potentials (except a very small subset, the labelings are almost impossible and have the same high energy), via which a higher-order model can be converted into a pairwise one by introducing only a small number of auxiliary variables and then pairwise MRF inference methods such as graph cuts can be employed to solve the problem. In the same line of research, [226] studied and characterized some classes of higher-order potentials (e.g.,PnPotts model [103]) that can be represented compactly as upper or lower envelope of linear functions. Furthermore, it was demonstrated in [226] that these higher-order models can be converted into pairwise models with the addition of a small number of auxiliary variables. [227] proposed to optimize the energy of “sparse” higher-order models by transforming the original problem into a relatively small instance of submodular vertex-cover, which can then be optimized by standard algorithms such as belief propagation and QPBO. This approach has been shown to achieve much better efficiency than applying those standard algorithms to address the original problem directly. Very recently, [228] took a further step along this line of research by exploring the intrinsic dimensions of higher-order cliques, and proposed a powerful MRF-based modeling/inference framework (called NC-MRF) which significantly broadens the applicability of higher-order MRFs in visual perception.On top of inference, another task of great importance is MRF learning/training, which aims to select the optimal model from its feasible set based on the training data. In this case, the input is a set of K training samples{dk,xk}k=1K, where dkand xkrepresent the observed data and the ground truth MRF configuration of the k-th sample, respectively. Moreover, it is assumed that the unary potentialsθikand the pairwise potentialsθijkof the k-th MRF training instance can be expressed linearly in terms of feature vectors extracted from the observed data dk, that is, it holdsθik(xi)=wTgi(xi,dk),θijk(xi,xj)=wTgij(xi,xj,dk), where giand gijrepresent some known vector-valued feature functions (which are chosen based on the computer vision application at hand) and w is an unknown vector of parameters. The goal of MRF learning boils down to estimating this vector w using as input the above training data.Both generative (e.g., maximum-likelihood) and discriminative (e.g., max-margin) MRF learning approaches have been applied for this purpose. In the former case, one seeks to maximize (possibly along with an L2-norm regularization term) the product of posterior probabilities of the ground truth MRF labelings∏kP(xk;w), where P(x; w)∝exp (E(x; w)) denotes the probability distribution induced by an MRF model with energy E(x; w). This leads to a convex differentiable objective function that can be optimized using gradient ascent. However, computing the gradient of this function involves taking expectations of the feature functions, giand gijwith respect to the MRF distribution P(x; w). One therefore needs to perform probabilistic MRF inference, which is nevertheless intractable in general. As a result, approximate inference techniques (e.g., loopy belief propagation) are often used for approximating the MRF marginals required for the estimation of the gradient. This is the case, for instance, in [5], where the authors demonstrate how to train a CRF model for stereo matching, as well as in [3], or in [2], where a comparison with other CRF training methods such as pseudo-likelihood and MCMC-based contrastive divergence is also included.In the case of max-margin learning [229,230], on the other hand, one seeks to adjust the vector w such that the energy E(xk; w) of the desired ground truth solution xkis smaller by Δ(x, xk) than the energy E(x; w) of any other solution x, that is,(19)E(xk;w)⩽E(x;w)-Δ(x,xk)+ξk.In the above set of linear inequality constraints with respect tow,Δ(x,x′)represents a user-specified distance function that measures the dissimilarity between any two solutionsxandx′(obviously it should holdΔ(x,x)=0), whileξkis a non-negative slack variable that has been introduced for ensuring that a feasible solutionwdoes exist. The distance functionΔ(x,x′)modulates the margin according to how “far” an MRF labeling differs from the ground truth labeling. In practice, its choice is largely constrained by the tractability of the whole learning algorithm. The Hamming distance is often used in the literature [231,232], due to the fact that it can be decomposed into a sum of unary terms and integrated easily in the MRF energy without increasing the order of the MRF model. However, visual perception often prefers more sophisticated task-specific distances that can better characterize the physical meaning of the labeling. For example, [233,234] have investigated the incorporation of various higher-order distance functions in MRF learning for the image segmentation task. Ideally,wshould be set such that eachξk≥0can take a value as small as possible (so that the amount of violation of the above constraints is minimal). As a result, during the MRF learning, the following constrained optimization problem is solved:(20)minw,{ξk}μ·R(w)+∑k=1Kξk,s.t.constraints(19).In the above problem, μ is a user-specified hyperparameter and R(w) represents a regularization term whose role is to prevent overfitting during the learning process (e.g., it can be set equal to∥w∥2or to a sparsity inducing norm such as∥w∥1). The slack variableξkcan also be expressed as the following hinge-loss term:(21)Loss(xk;w)=E(xk;w)-minxE(x;w)-Δ(x,xk).This leads to the following equivalent unconstrained formulation:(22)minwμ·R(w)+∑k=1KLoss(xk;w).One class of methods [235,236] aim to solve the constrained optimization problem (Eq. (20)) by the use of a cutting-plane approach when R(w)=∥w∥2. In this case, the above problem is equivalent to a convex quadratic program (QP) but with an exponential number of linear inequality constraints. Given that only a small fraction of them will be active at an optimal solution, cutting plane methods proceed by solving a small QP with a growing number of constraints at each iteration (where this number is polynomially upper-bounded). One drawback of such an approach relates to the fact that computing a violated constraint requires solving at each iteration a MAP inference problem that is NP-hard in general. For the special case of submodular MRFs, [237] shows how to express the above constraints (Eq. (20)) in a compact form, which allows for a more efficient MRF learning to take place in this case.Another class of methods tackle instead the unconstrained formulation (Eq. (22)). This is, e.g., the case for the recently proposed framework by [238], which addresses the above mentioned drawbacks of the cutting plane method by relying on the dual decomposition approach for MRF-MAP inference discussed previously in Section 4.3. By using such an approach, this framework reduces the task of training an arbitrarily complex MRF to that of training in parallel a series of simpler slave MRFs that are much easier to handle within a max-margin framework. The concurrent training of the slave MRFs takes place through a very efficient stochastic subgradient learning scheme. Moreover, such a framework can efficiently handle not only pairwise but also high-order MRFs, as well as any convex regularizer R(w).There have also been developed learning methods [239–241] that aim to deal with the training of MRFs that contain latent variables, i.e., variables that remain unknown during both training and testing. Such MRF models are often encountered in vision applications due to the fact that in many cases full annotation is difficult or at least very time consuming to be provided (especially for large scale datasets). As a result, one often has to deal with datasets that are only partially annotated (weakly supervised learning).Last but not least, there have also been proposed learning algorithms that are appropriate for handling the discriminative training of continuous MRF models [242].

@&#CONCLUSIONS@&#
