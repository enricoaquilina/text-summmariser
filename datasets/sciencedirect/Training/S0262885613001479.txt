@&#MAIN-TITLE@&#
Road traffic density estimation using microscopic and macroscopic parameters

@&#HIGHLIGHTS@&#
We propose an algorithm for road traffic congestion estimation from video scenes.We compare between macroscopic and microscopic parameters in terms of accuracy.The method proposed is accurate, and it is computationally inexpensive.It does not require segmentation or tracking of vehicles.It is robust towards illumination changes.

@&#KEYPHRASES@&#
Road traffic density estimation,Microscopic and macroscopic traffic parameters,Motion detection and tracking,KNN,LVQ,SVM,

@&#ABSTRACT@&#
In this paper we present a comparative study of two approaches for road traffic density estimation. The first approach uses the microscopic parameters which are extracted using both motion detection and tracking methods from a video sequence, and the second approach uses the macroscopic parameters which are directly estimated by analyzing the global motion in the video scene. The extracted parameters are applied to three classifiers, the K Nearest Neighbor (KNN) classifier, the LVQ classifier and the SVM classifier, in order to classify the road traffic in three categories: light, medium and heavy. The methods are compared based on their robustness to the classification of different road traffic states. The goal of this study is to propose an algorithm for road traffic density estimation with a high precision.

@&#INTRODUCTION@&#
The recent technological advances have made vehicles a lot safer, but in return the road environment has become more complex. This is mainly due to the rapid increase in the number of vehicles and the resulting consequences, such as traffic accidents, road congestions… etc.Road traffic congestion is a serious problem resulting in a large number of negative effects both economically and ecologically. Accidents, construction works, bad weather and poor traffic signal timing are likely to cause the traffic congestion.Intelligent transport systems (ITS) are the most recent solutions for this problem. More recently, the use of camera networks has shown promise in traffic monitoring. In contrast to loop detectors, video based systems are less disruptive, less costly to install and allow for a more detailed understanding of traffic flow patterns.Most existing approaches for classifying road traffic videos use a combination of segmentation and tracking [1–10]. The general procedure consists of the following three steps: (i) motion detection, (ii) tracking of the individual vehicles, and (iii) combining trajectories' information to derive an overall description of the traffic flow. The vehicle tracking framework has, as a disadvantage, an accuracy that is dependent on the quality of the segmentation. The segmentation task becomes more difficult with the presence of adverse environmental conditions, such as lighting (e.g. overcast, glare, shadows, occlusion, and blurring). Furthermore, segmentation cannot be performed reliably on low resolution images where the vehicles are only represented by a few pixels. Tracking algorithms also have problems when there are many objects in the scene, which is typically the case for highway scenes with congestion.L. Huang and M. Barth [1] proposed a multi-vehicle tracking approach, which combines both local feature tracking and a global color probability model. In cases with low occlusion, corner features detection and tracking algorithms can be used to estimate vehicle positions and trajectories [2]. When there is a high degree of occlusion, corner features can be tracked to provide position estimates of moving objects, then a color probability can be calculated in the occluded area to determine which object each pixel belongs to. This approach is adapted to both stationary and moving video cameras.Y. Kee and H.Yo-Sung [3] proposed a vehicle tracking algorithm that takes a new occlusion reasoning approach. They considered two different types of occlusions: explicit occlusion and implicit occlusion. Explicit occlusion represents the following situation: after two or more vehicles appear separately, they are merged due to some occlusion conditions. Implicit occlusion represents the initial occlusion where two or multiple vehicles appear as a single object. The authors also proposed a traffic flow extraction method with the velocity and trajectory of the moving vehicles. The proposed vehicle tracking system is composed of three parts: vehicle segmentation, vehicle tracking, and traffic parameter extraction. The vehicle segmentation part separates moving vehicles from their background. They employed the adaptive background approach, which does not update the background of moving objects. They also designed a 2D token-based algorithm for vehicle tracking, using Kalman filtering which has a low computational complexity. The traffic parameters extraction part estimates the traffic parameters, such as the vehicle count and the average speed. It also extracts the traffic flow. They then evaluated the proposed algorithm with some MPEG-7 test sequences.J. B. Kim, and C. W. Lee [4] proposed a wavelet-based vehicle tracking system for automatic traffic surveillance. In order to meet real-time requirements they used an adaptive thresholding and Wavelet-based Neural Network (NN). First, moving regions are extracted by performing a frame difference analysis on two consecutive frames using adaptive thresholding. Second, the Wavelet-based NN is used for recognizing the vehicles in the extracted moving regions. Wavelet Transform (WT) is adopted to decompose an image, and a particular frequency band is selected as an input for the NN, for vehicles recognition. Third, vehicles are tracked by using position coordinates and wavelet features difference values for correspondence in recognized vehicle regions.G. Mo and S. Zhang [5] adopted a multiple video object segmentation algorithm for vehicle detection. The algorithm consists of a training step and a segmentation step. At the training step, a codebook method is used for training: First, a scale-invariant feature transform method [6] is used to extract the features from vehicle image samples. Secondly, the images are segmented into small patches to cluster. Third, a new way is used to activate the codebook that can largely reduce the numbers of vehicles sample images: the key idea is to automatically learn a relatively large number of simple and compact appearance prototypes and represent the complex appearance distribution in relation to them. To obtain a set of informative locations for each image an interest point detector is applied. The amount of data to be processed can be reduced by extracting features in those locations, while the interest point detector preference for certain structures assures that “similar” regions are sampled on different objects. At the segmentation step, an implicit shape model is used to combine the recognition knowledge and the segmentation knowledge together. Finally, the authors used this algorithm to implement a prototype of a traffic flow analysis system, and achieved the expected results.In the work of F. Bardet and T. Chateau [7] multi-vehicles are tracked through a Markov Chain Monte-Carlo Particle Filter (MCMCPF) method. They have shown that integrating a simple vehicle kinematic model within this tracker allows for estimating the trajectories of a set of vehicles, with a moderate number of particles. Their paper also addresses vehicle tracking.Alternatively, several approaches have attempted to recover a holistic representation (macroscopic view) of traffic flow information directly, thereby avoiding the need for detecting and tracking individual moving objects. In Ref. [11] F. Porikli and X. Li proposed an unsupervised, low-latency traffic congestion estimation algorithm that operates on the MPEG video data. They extracted congestion features directly in the compressed domain, and employed Gaussian Mixture Hidden Markov Models (GM–HMM) to detect traffic condition. First, they constructed a multi-dimensional feature vector from the parsed DCT coefficients and motion vectors. Then, they trained a set of left-to-right HMM chains corresponding to traffic patterns and used a Maximum Likelihood (ML) criterion to determine the state from the outputs of the separate HMM chains. They then calculated a confidence score to assess the reliability of the detection results. The proposed method is computationally efficient and modular. Their tests prove that the feature vector is invariant to different illumination conditions, e.g. sunny, cloudy. Furthermore, they did not have to impose different models for different camera setups, thus they significantly reduced the system initialization workload and improved its adaptability. However the video should be recorded in MPEG format.A. B. Chan and N. Vasconcelos [12] proposed a method to model the traffic flow in a video using a holistic generative model that does not require segmentation or tracking. In particular, they adopted the dynamic texture model, an auto-regressive stochastic process, which encodes the appearance and the underlying motion separately into two probability distributions. With this representation, retrieval of similar video sequences and classification of traffic congestion can be performed using the Kullback–Leibler divergence and the Martin distance [13]. A drawback of this approach is the large computational load in fitting the model. As a result analysis is limited to relatively small image patches and might make this approach impractical for application to real-time traffic monitoring.K.G. Derpanis and R.P. Wildes in Ref. [14] described a system for classifying traffic congestion videos based on their observed visual dynamics. The proposed system treats traffic flow identification as an instance of dynamic texture, i.e. as spatiotemporal image patterns, best characterized in terms of the aggregate dynamics of a set of constituent elements, rather than in terms of the individuals (cf. spatial texture [15]). In particular, traffic patterns were classified directly in terms of measures of their dynamics aggregated over regions of image space–time, (x, y, t), rather than via the analysis of individual vehicles. Towards that end, an approach was developed that is based solely on observed dynamics (i.e., excluding purely spatial appearance cues). For such purposes, local spatiotemporal orientation is of fundamental descriptive power, as it captures the first-order correlation structure of the data, irrespective of its origin (i.e., irrespective of the underlying visual phenomena), allowing for the discrimination of pattern differences (e.g., levels of congestion). Correspondingly, each traffic scene is associated with a distribution (histogram) of measurements that indicates the relative presence of a particular set of 3D orientations in visual space–time, (x, y, t), as captured by a bank of spatiotemporal filters, and recognition was performed by matching such distributions. On the other hand, this approach has shown a limited ability to distinguish between completely stopped traffic and an empty roadway.To our knowledge, none of the above methods have focused on the comparison between the microscopic and macroscopic features based approaches to estimate road traffic density. This study provides quantitative comparisons between these two approaches with the focus emphasis on determining the best feature combination that provides the highest classification accuracy of traffic videos. Firstly we estimate the traffic parameters with both approaches. Then road traffic classification is achieved using a set of parameter combinations, extracted from the video stream, which feeds a K-Nearest Neighbor (KNN), a Learning Vector Quantization (LVQ), or a Support Vector Machine (SVM) classifier.Our goal is to compare the accuracy of the two methods, and also to propose an algorithm which provides the highest accuracy with a low number of traffic parameters. The proposed algorithm is compared against an existent method which used the same database in terms of classification accuracy and computational complexity.Microscopic parameters are obtained by averaging the parameters of all the individual vehicles on the road. Usually, these parameters are estimated by detecting and tracking each vehicle on the road. We consider three kinds of microscopic parameters: traffic velocity, road occupancy rate and traffic flow.The velocity is the most important parameter for describing the vehicle's behavior and the traffic conditions. The traffic velocity is obtained by averaging all the individual vehicles' speeds.To measure a vehicle's speed we need first to extract it by a motion detection method; for this we use the median approximated detection method [16], for its simplicity and efficiency in this application.A common approach to identifying the moving objects is background subtraction, where each video frame is compared against a reference or background model. Pixels in the current frame that deviate significantly from the background are considered to be moving objects. These “foreground” pixels are further processed for object localization and tracking.Due to the success of the non-recursive median filtering, N. Mcfarlane and C. Schoeld proposed a simple recursive filter to estimate the median, called the approximated median filter [17]. This technique has also been used in background modeling for urban traffic monitoring [18]. It finds the difference of the current pixel intensity value and the median of some recent pixel intensity values. It uses a buffer of size n, where n is the number of the last frames whose pixel values are considered for calculating the median value for the background model. A pixel is considered as a foreground pixel if it satisfies the following inequality:(1)Iij−Medij>Th.The median value is updated for the last n recent pixel values. ‘I’ represents the current frame and ‘Med’ is the median of the last n frames, Th is a threshold defined by the Otsu method. For each pixel (i,j), the difference of its value with the pixel value of the median of the last n frames is used to decide whether it is foreground or background.The vehicles are tracked from one frame to another by a kind of data association method that assigns the same label to the detections in consecutive frames, emanating from the same vehicle. In this paper we use a simple data association method. Considering that the trajectory of each vehicle is rectilinear, during some say n few consecutive frames, we use the correlation coefficient, defined in Eq. (2), to associate the detections of a given vehicle in these frames.(2)R=n∑i=1nxiyi−∑i=1nxi∑i=1nyin∑i=1nxi2−∑i=1nxi2n∑i=1nyi2−∑i=1nyi2where n represents the number of frames and (xi, yi) are the coordinates of the gravity center of the detected vehicle in frame i.R is maximum and equals 1 when the points (xi, yi) lie on a line. If R exceeds a threshold equal to 0.95, we consider that the n detections are likely to belong to the same vehicle [10]. Furthermore to enhance the data association we restrict the speed of a vehicle to be lower than a given maximum speed.The search for the best path, which corresponds to an object, is performed on three consecutive frames. This task must satisfy a constraint of the uniqueness association, which stems from the fact that a point of the image can represent only one physical point. This prohibits associating a detected object to several trajectories. Moreover, in some frames there may be no match between any detection and a confirmed trajectory, because some objects appear or disappear. So a point or a path may have either one association or none. Fig. 1shows an example of vehicle tracking.The distance traveled by a vehicle is measured in pixels and should be converted into meter. To avoid a prior calibration of the camera for this, we like in Ref. [10] evaluate a scale factor at each row of the image as follows: For each vehicle detected at a given row we calculate a scale factor as the ratio between the vehicle mean length and the length of this vehicle in pixels. An example of these calculated scale factors is shown in Fig. 2.Assuming that the scale factor is a linear function of the row number, we model it by the following equation:(3)qv=mv+b,where q represents the scale factor in meter/pixel, v the row number and m and b parameters that may be determined from the data set by a curve fitting, using the least squares criterion. This is illustrated in Fig. 2.The distance d traveled between two positions (v1, v2) is evaluated by:(4)d=∫v1v2qvdv.Finally, having an estimate of the distance traveled, we use the inter-frame sample time (∆t) to estimate the vehicle speed:(5)s^=dΔt.In this work we estimate the road occupancy rate as the ratio between the total surface of the road and the surface of all the vehicles present on the road. To calculate the road surface we need first to detect the road region. Many techniques for road detection and segmentation have been proposed with more or less success. They are usually based on the detection of markers, using the Hough transform for example. This also can be achieved by using a segmentation method based on extracting the regional maxima of the image, if there is no road marking as in our case.To extract the regional maxima of a numerical function f, which represents in our case the image, we apply a thresholding to the difference between f and its geodesic reconstruction, which is accomplished by a geodesic dilation of (f−1) under f[19], as illustrated in Fig. 3.(6)Maxf=T1f−δ∞f,f−1,where δ∞(f,f−1) presents the infinite size geodesic dilation operation and Th (f) is the thresholding function of f.(7)Thf=x\fx≥h.Fig. 4shows a road region segmented with this method.Regional maxima are flat zones that are connected to pixels of lower value while regional minima are flat zones that are connected to pixels of higher value. In the case of a dark road texture we have to extract the regional minima.The extraction of the regional minima of (f) uses the same process, but applied to (−f). We can also perform a geodesic reconstruction by erosion of (f+1) over f[19].(8)Minf=T1ϵ∞f,f+1−f.Fig. 5shows a dark road texture segmented with the minima regional.Since the vehicle occupies an area on the road during a period of time, we may consider that the area occupied by vehicles in the case of congested road traffic is larger than the area occupied in the case of light traffic. Therefore, the ratio between the total objects (vehicles) area and the road area defined in Eq. 9 may be used to infer the traffic density.(9)Occupancyrate=∑objectsarea/roadarea∗100.The road area is estimated by calculating the number of white pixels which represent the road region. To estimate the area occupied by the vehicles we use the motion detection method, described previously, followed by some morphological operations to enhance the moving objects extraction, mainly the holes filling operation.Road traffic flow is estimated by calculating the difference between the inflow and outflow, which are respectively, the average number of vehicles entering in the visual field of the camera, during a period of time, and the number of vehicles which leave it during the same period.We have estimated the number of vehicles passing by in a given region by using the same idea as the one proposed in Ref. [20]. Firstly a road region, delimited by two parallel lines located at the beginning of the road, is selected, secondly the approximate median filter motion detection method is applied and finally the following algorithm is used to detect any vehicle that enters the region of interest.To identify a vehicle in the selected region the following condition must be satisfied:(10)dj−1>S&&dj<=S,j=2,…,NC−10where S is a threshold that depends on the minimum size of the vehicles (in this paper we used S=60), j is the column index, d(j) is the area of a region from column j to j+10, and NC is the columns number.The area of a region is measured by the number of white pixels that it contains (Fig. 6).If a region satisfies the above condition, we can assume that it is a vehicle's template in the current frame, as shown in Fig. 7.The number of vehicles is counted as follows:1)If the vehicle template identified in the current image is matched four times or more continuously in the next few frames, this indicates that a car is passing by, and the total number of vehicles is incremented by one.If the vehicle template identified in the current image is not matched four times continuously in the next few frames, this indicates that a car has passed by, and the vehicle template is deleted.If the vehicle template identified in the current image is not matched in the previous frames, this indicates that a new car is appearing, so this new car is taken as the vehicle template.A road flow can be likened to a liquid flowing in a channel. Macroscopic representation of traffic borrows the notions of density and velocity specific to fluid mechanics. These two variables can be estimated by using a method based on estimating the optical flow field by a block matching algorithm.Block matching technique is commonly used for estimating the block motion vector [21], due to its simplicity. The matching error between the block at position (x, y) in the current image, It, and the candidate block at position (x+u, y+v) in the reference image, It−1, is often defined as the Sum of Absolute Difference (SAD):(11)SADxyuv=∑i=0B−1∑j=0B−1Itx+i,y+j−It−1x+u+i,y+v+j,where B is the block size.The best estimate of the block motion vectoru^v^is the one having the minimum matching error:(12)u^v^=argminuvSADxyuvWe used a search window of size (2R×2R) for blocks of size (R×R). The value R=16 pixels was found to achieve a good compromise between accuracy and complexity.The length of a motion vector represents the moving distance of the corresponding block. An example of motion vectors estimated by block matching is given in Fig. 8, for two different frames.Road traffic density and velocity are estimated by the density and the mean velocity of the non-zero motion vectors.There is no unified standard for the definition of traffic congestion, and each nation has a different definition. For example, the ministry of public security in China [22] defines congestion at crossings and sections as follows: for crossings, a distinction is made between a crossing with the control of a signaling beacon and a crossing without. The former is considered as congested if the vehicles cannot pass it during 3 times of green light show, while the latter is considered as congested if it is difficult for the vehicles to cross it and if the length of vehicles line is over 250m. A section is defined as congested when the vehicles in the lanes are blocked up and the vehicles line is longer than 1km.Using a combination of the traffic parameters extracted from a video clip the traffic congestion is classified as light, medium, or heavy, using one of the three classifiers: KNN, LVQ, SVM, which are well documented in the specialized literature [22–25].

@&#CONCLUSIONS@&#
