@&#MAIN-TITLE@&#
A multi-technique approach to bridge electronic case report form design and data standard adoption

@&#HIGHLIGHTS@&#
Electronic case report form design and data standard adoption have a major gap.A multi-technique approach achieves eCRF design with data standard compliance.The multi-technique approach includes information retrieval, natural language processing and ontology-based knowledgebase.The results of evaluation revealed a satisfactory performance. We also analyze the reasons of the missed and failed results.A usage intention of our approach is conducted in this study.

@&#KEYPHRASES@&#
Data standard,Common data elements,Case report form,Natural language processing,Ontology-based knowledgebase,

@&#ABSTRACT@&#
Background and objectiveThe importance of data standards when integrating clinical research data has been recognized. The common data element (CDE) is a consensus-based data element for data harmonization and sharing between clinical researchers, it can support data standards adoption and mapping. However, the lack of a suitable methodology has become a barrier to data standard adoption. Our aim was to demonstrate an approach that allowed clinical researchers to design electronic case report forms (eCRFs) that complied with the data standard.MethodsWe used a multi-technique approach, including information retrieval, natural language processing and an ontology-based knowledgebase to facilitate data standard adoption using the eCRF design. The approach took research questions as query texts with the aim of retrieving and associating relevant CDEs with the research questions.ResultsThe approach was implemented using a CDE-based eCRF builder, which was evaluated using CDE- related questions from CRFs used in the Parkinson Disease Biomarker Program, as well as CDE-unrelated questions from a technique support website. Our approach had a precision of 0.84, a recall of 0.80, a F-measure of 0.82 and an error of 0.31. Using the 303 testing CDE-related questions, our approach responded and provided suggested CDEs for 88.8% (269/303) of the study questions with a 90.3% accuracy (243/269). The reason for any missed and failed responses was also analyzed.ConclusionThis study demonstrates an approach that helps to cross the barrier that inhibits data standard adoption in eCRF building and our evaluation reveals the approach has satisfactory performance. Our CDE-based form builder provides an alternative perspective regarding data standard compliant eCRF design.

@&#INTRODUCTION@&#
The rapidly development of new research area and the wider adoption of informatics systems have resulted in the exponential growth of biological and clinical data. Although “big data” creates new research opportunities [1], researchers also face the difficulty of obtaining data as well as the high cost of data collection. Therefore, it has been inevitable that an urgent need for data harmonization, which would facilitate the subsequent data aggregation and sharing, has arisen.The use of a data standard is a critical requirement for such harmonization, and is also the first step towards data integration. A data standard is an agreed upon set of rules that allow information to be shared and processed [2]. It could be classified as semantic standard (i.e., terminology artifacts), syntax standard for data representation and format (i.e., markup language), and content standard, such as minimum information checklist or common data elements (CDEs) [3–5].As the National Institute of Health (NIH) encourages the use of CDEs [6], some researchers have designed their CRFs based on CDEs [7]. Several CDEs have been developed, for example, the Cancer Bioinformatics Grid (caBIG) [8], the National Institute of Neurological Disorders and Stroke (NINDS) common data element project [9], the Parkinson Disease Biomarker Program (PDBP) [7,10], as well as a number of other clinical CDE for a variety of different purposes [11–13]. The CDE is a logical unit of data that provides for the definition of data, including an identifier, an element type to indicate the value type, and detailed information, which is the meta-data that fully defines the semantics of the CDE [14]. To define the CDE in formal representation, the ISO/IEC 11179, which is a metadata repository standard, provides the standard syntax and grammar need to describe data element metadata. Many efforts have been made to adopt this standard, for example, the National Cancer Institute (NCI) Cancer Data Standards Repository (caDSR) implements the ISO/IEC 11179 standard for metadata registries when presenting CDEs in their repository [15]. The cancer Common Ontologic Representation Environment (caCORE) created by National Cancer Institute Center for Bioinformatics (NCICB) is an interoperability infrastructure that is based on model driven architecture and contains a metadata repository based on the ISO/IEC 11179 standard to allow semantic interoperability [16]. Another effort is the CDISC Shared Health and Research Electronic Library (CSHARE) and this utilizes the ISO/IEC 11179 standard as the semantic basis for its metadata and has adopt the ISO 21090 for the formal presentation of CDE data type [17].The CDE should be able to not only standardize data collection, but also should facilitate the follow-up comparison of results across multiple studies [18]. Nevertheless, CDEs are center-specific and are not a global standard; therefore such an approach, which is called computable semantic interoperability, may exhibits scalability problems when applied beyond a well-defined domain [19]. As a result, using CDEs is still a compromise solution in terms of current research domains. To address the issue of computable semantic interoperability, Payne et al. developed the Translational Research Informatics and Data Management Grid (TRIAD), which leverages the caGrid [20] middleware and extended this to support working interoperability. Such working interoperability means that stakeholders are able to negotiate and use context-relevant semantic models that enable better semantic exchange [19]. In the TRIAD, a CDE metadata registry repository called the MDR (metadata repository) Core is one of the system’s four major components.In clinical studies, the case report form (CRF) is an important tool for collecting data. The CRF is usually designed by researchers based on their study objective, for example, demographic information, medical history, and/or the results of clinical examination. Many clinical data capturing systems support electronic CFR (eCRF) design [21,22]. Through use of eCRFs, clinical research data is able to be captured and stored in clinical data repositories. For data integration and sharing purposes, Brandt et al indicated that there is a requirement for an information tool that will aid researchers in creating comprehensive and valid CRFs that can be mapped to a data standard [23]. Such an approach would enable the adoption of a data standard that can be used for clinical research applications, particularly if there is a tool supporting the retrieval and reuse of existing standard items [24].How to efficiently and precisely select data elements from a CDE repository in order to build an eCRF that is able to accurately reflect the study question is the challenge that needs to be met in this context, especially when some researchers might not be familiar with the application of CDEs. Most commercial available clinical data capturing systems do not allow users to associate their research questions with CDEs, but merely provide a list of hundreds of CDE for selection or allow simple searching of the CDEs. The lack of an informatics tool that is able to substantially increase efficiency has become a barrier that inhibits data standard adoption.To cross this barrier, we developed a multi-technique approach that included the creation of an ontology-based knowledgebase, the development of natural language processing and the creation of an information retrieval technique. In this study, we demonstrated this approach by implementing an eCRF builder that supports researchers and helps them design CDE compatible eCRFs.There were mainly three parts to the implementation of the multi-technique approach (shown in Fig. 1): (1) the creation of an ontology-based knowledgebase of the CDEs, (2) the development of an information retrieval strategy for suggesting the CDEs and (3) the linking of the CDEs to the clinical questions.This study took PDBP CDEs [25] as the example for demonstrating the process of creating an ontology-based knowledgebase. Originally, the PDBP CDEs were hosted in a straightforward relational database format. Our approach is compatible with the relational database format; however, such a format does not support formal semantic definitions. The ontology technique has been widely adopted in the clinical studies to allow semantic interoperability. Some studies have utilized ontology to harmonize their data standards [26] or to model the entities and relationships within study designs [27], while others have presented a clinical data element model using Web Ontology Language (OWL) [28,29]. In this research we would like to develop the CDE ontology to allow further semantic interoperability and to demonstrate the compatibility of our approach with semantic web technology.Even through PDBP CDEs are not ISO/IEC 11179 compliant; they still have a well defined structure. In this study, we developed a program using the Protégé API [30] that build this ontology using the PDBP CDE relational database. The CDE information contains general details, such as identifier, title, variable name and description, data definitions, which includes element type, the text of the suggested question, guidelines and pre-descriptions, categorization and classification. The categorization and classification predicate the restricted hierarchical structure of the CDEs. The hierarchy is composed of disease, domain and subdomain. Each disease contains specific domains and each domain contains specific subdomains; furthermore, each CDE element belongs to a specific subdomain. To represent these restrictions, we adopted the OWL sequence extension [31] to express the restricted hierarchical structure of the CDEs. The OWL sequence extension uses the hasNext property to point to the next member in the sequence and to identify that the content of the member is associated through the hasContents property. In this study, we created four OWL classes: Disease, Domain, Subdomain and CDE. Those classes are linked with each other in sequence using the OWL object properties (hasDomain, hasSubDoaim and hasCDE) and the owl:individual of the owl:class is associated through the hasIndividual property. By setting the rdfs:domain and rdfs:range of the object property, each owl:individual belonging to a specific owl:class will inherit the restriction. The general details and data definition information is then stored in each CDE entity via the OWL annotation property. There are 426 CDE entities under CDE OWL class. An example of CDE ontology structure is shown in Fig. 2.In order to allow researchers to adopt the data standard when carrying out eCRF design, this study developed an information retrieval strategy that provides question relevant CDEs to its researchers. The study question, which is input by the user, is treated as the query for CDE information retrieval. Since the question is able to be in a variety of formats, the use of a pattern matching search approach might not be appropriate. Our information retrieval strategy included three major steps (Fig. 1). Firstly, we need to index the CDEs from the knowledgebase to allow information retrieval. The second step was to generate the query from study question, which is in free text, and then to perform searching. Thirdly, we evaluate the quantity of searching results obtained and refined the query if necessary. An open source and full-featured text search engine, Apache Lucene, was adopted for implementing the information retrieval strategy [32].The description of each CDE entity from the knowledgebase was indexed by Lucene and the indexed result CDE indexed file was then used for searching. To improve the quality of searching, a description of each CDE entity was pre-processed using special characters to filter-out and term normalization based on part-of-speech tagging selection. This study indexed 426 descriptions from the PDBP CDE (see Fig. 3).In this step, we utilized natural language processing (NLP) to analyze the narrative study question using the Apache OpenNLP toolkit [33]. Initially, the text analysis is preprocessed to determine factors such as sentence boundary, token removal and symbol removal. In general, nouns and noun phrases are more representative of the research question concept, adjectives and adverbs then support the concept expression, we therefore used nouns, noun phrases, adjectives and adverbs when generating the query. To do this, the preprocessed text is part-of-speech tagged and then filtered through the part-of-speech tag filter so that only nouns, noun phrases, adjectives and adverb left. Finally, we used the SPECIALIST Lexical Tools, “norm” [34], to normalize each term in the text based on its lexical properties, such as inflection, alphabetic case, spelling variants and ligature. Eventually we generated a simple text as a query that was derived from the study question after carrying out the above processing.Query refinement is triggered if the number of searched results is less than what the researcher has set up. Lucene implements a variant of the TF–IDF scoring formula [35] that ranks the searching results. The TF–IDF weighting has been adopted not only for information retrieval but also for concept specificity [36] and terminology linking [37]. From the scoring formula [38], we should be noted that the Lucene searching is at the syntax level.However, the study question is usually in a widely varied narrative form, which will limit the syntax searching. To overcome this limitation and to achieve semantically searching, we refined the query text through semantical query expansion. We adopted a general dictionary, the WordNet [39] and the MIT Java Wordnet Interface (JWI) [40], to find semantically relevant words, such as synonyms, for each term in a query. For example, a question text “Is subject receiving drugs?” will have been turned into a query “subject receive drug” by the previous NLP steps. The semantical expansion step would expand this query to be “subject message content individual person receive drug medication” by adding synonyms.After semantically expansion, the number of terms in a query will increase and provide more opportunities to the retrieval of precise CDEs. However, there may also be a decline in the quality of the retrieved results since not all of semantically expanded queries are necessarily relevant to the study question. Therefore, we have set up a threshold-based method that assesses the quality of each expanded query. The definition of the threshold is shown as belowDefFor each expanded term in the query, let n be the number of pre-query result, s be the score of result and T be the inclusion threshold. If∑i=1nsin>Tthen include this expanded term in refined query.After the threshold-based refining, the final query would be “subject individual person receive drug medication” and this query has a higher chance of retrieving related CDEs.To keep a flexible linkage between the CDEs and the clinical questions, our approach is able to suggest up to five CDEs with an average score that is higher than the Lucene score threshold for each study question. The suggested CDEs are then sorted by the Lucene scoring function. The detailed information on each CDE is also provided for the researcher’s reference. A real-time interaction interface is implemented using the Ajax technique [41].Researchers can click on each CDE link to obtain the detailed information. We used SPARQL to obtain the detailed CDE information from the ontology-based knowledgebase. SPARQL is a query language for retrieving and manipulating data that is stored in the resource description framework (RDF) format [42]. An example described below is a SPARQL query for obtaining detailed information from knowledgebase for a CDE related to the date of an adverse event.PREFIX: <http://www.owl-ontologies.com/CDE.owl>SELECT?t ?d ?e ?vWHERE{:AdvEvntDateTime :title ?t .:AdvEvntDateTime :description ?d .:AdvEvntDateTime :elementType ?e .OPTIONAL{ :AdvEvntDateTime :preDefinedValues ?v .}};In this study, we utilized Apache Jena ARQ [43] which is a SPARQL processor to implement the knowledgebase retrieving task. If the researcher is not able to find a relevant CDE from the suggested CDE list, the interface will provide a CDE browser for searching the CDE.Once a researcher has selected a CDE, the CDE is then associated with the study question and from then onwards the subsequently collected clinical data is no longer linked with narrative question, but with data standard.To evaluate the performance of our multi-technique approach, we took the 23 PDBP CRFs [25] and the questions from the stackoverflow website [44] as the evaluation dataset. The PDBP CRF includes various collection topics like adverse events, behavioral history, and rapid eye movement behavior disorder. Each study question in the PDBP CRF has been linked with a CDE of the PDBP. Therefore, we treated the already established associations between the CDEs and CRFs as the golden standard when evaluating the performance of our builder. The 23 CRFs contained 433 non-duplicate and CDE effective study questions; those questions formed the positive dataset.Since the CDEs in our CDE ontology were related to general disease or general clinical research concepts, we need a set of questions that were completely unrelated to the CDEs as the negative dataset. The stackoverflow website is a question and answer site for programmers. All of questions are not related to clinical research but rather to computer science. We randomly select 283 questions from that site as questions that are not associated with any CDE and used these as the negative dataset. This dataset contained 716 questions, including both CDE-related and CDE-unrelated questions. To obtain an appropriate Lucene score threshold, we separate this dataset into a training dataset (215 questions) and a testing dataset (501 questions).For each question, our approach provided up to 5 CDEs, and four measures were used to describe the performance of multi-technique approach: precision (Eq. (1)), recall (Eq. (2)), F-measure (Eq. (3)) and error (Eq. (4)). To calculate these values, the system’s response was counted and categorized as true positive (TP: this question is CDE-related and its CDE exist in response CDEs), false positive (FP: this question is CDE-related and its CDE does not exist in response CDEs or the approach returned an empty query result), false negative (FN: this question is CDE-unrelated and the approach returned a non-empty query result) and true negative (TN: this question is CDE-unrelated and the approach returned an empty query result).(1)precision=TPTP+FP(2)recall=TPTP+FN(3)F-measure=2×precision×recallprecision+recall(4)error=FN+FPTP+FN+FPTo better understand the usage intention of our approach, we also did hands-on user testing. We recruited 16 clinical research staffs and system developers from the NIH and the Center for Systems and Synthetic Biology at the National Yang-Ming University. These individuals are independent from this study. We requested users to design an eCRF using our form builder; a 10min training session was carried out before the eCRF design started. Afterwards, the users filled out a post-test questionnaire and participated in an open-ended interview.

@&#CONCLUSIONS@&#
This study developed a multi-technique approach and demonstrated the approach by implementing a CDE-based eCRF builder that is able to help researchers constructing an eCRF, and use this to collect data that is not only compatible with the CDE content standard, but also allows for further computable semantic interoperability of the data. This approach to enforcing CDEs usage as part of eCRF design may also facilitate working interoperability. The utilization of the multi-technique during the development of the tool was also demonstrated and the results of an evaluation revealed a satisfactory performance. Nevertheless, two issues still need to be improved in the future. The first is the need to implement a better way of developing unambiguous CDEs, while the second is the development of a more precise and user-friendly tool for reusing CDEs.