@&#MAIN-TITLE@&#
A cycle-based evolutionary algorithm for the fixed-charge capacitated multi-commodity network design problem

@&#HIGHLIGHTS@&#
HightlightsNew and enhanced cycle-based neighborhood operators.An innovative perturbation strategy based on ejection chains, namely the Ejection Cycles.An efficient scatter search that considers the search history and “solvency-based” measures to produce offspring.Competitive results produced for well known benchmarks of literature.

@&#KEYPHRASES@&#
Multi-commodity network design,Scatter search,Evolutionary algorithms,Ejection chains,Iterated local search,

@&#ABSTRACT@&#
This paper presents an evolutionary algorithm for the fixed-charge multicommodity network design problem (MCNDP), which concerns routing multiple commodities from origins to destinations by designing a network through selecting arcs, with an objective of minimizing the fixed costs of the selected arcs plus the variable costs of the flows on each arc. The proposed algorithm evolves a pool of solutions using principles of scatter search, interlinked with an iterated local search as an improvement method. New cycle-based neighborhood operators are presented which enable complete or partial re-routing of multiple commodities. An efficient perturbation strategy, inspired by ejection chains, is introduced to perform local compound cycle-based moves to explore different parts of the solution space. The algorithm also allows infeasible solutions violating arc capacities while performing the “ejection cycles”, and subsequently restores feasibility by systematically applying correction moves. Computational experiments on benchmark MCNDP instances show that the proposed solution method consistently produces high-quality solutions in reasonable computational times.

@&#INTRODUCTION@&#
The fixed-charge capacitated multi-commodity network design problem (MCNDP) consists of designing a network on a given graph by selecting arcs to route a given set of commodities between origin-destination pairs. Each arc has a predefined capacity specifying the maximum flow that the arc can accommodate. Also, associated with each arc are fixed and variable costs, where the fixed cost is incurred only if the arc is selected, and the variable cost is a cost per unit of flow along the arc. Each commodity has an origin and a destination node and the amount to be transported. The objective is to minimize the total cost of establishing the arcs and routing the flows.The MCNDP has attracted much attention in the literature due to both its complexity (the problem is NP-hard in the strong sense), and a wide variety of applications in the areas of telecommunications, logistics, production and transportation systems (Balakrishnan, Magnanti, & Mirchandani, 1997; Magnanti & Wong, 1986; Minoux, 1986). Despite the significant efforts devoted to the development of exact methodologies for the MCNDP (Crainic, Frangioni, & Gendron, 2001; Hewitt, Nemhauser, & Savelsbergh, 2010), the literature still favors heuristic approaches when large-scale problem instances are involved. One of the most successful local search strategies for the MCNDP is proposed by Ghamlouche, Crainic, and Gendreau (2003), where new cycle-based neighborhood operators are incorporated in a tabu search framework. The cycle-based operators are subsequently used within a path-relinking algorithm (Ghamlouche, Crainic, & Gendreau, 2004), a multilevel cooperative framework (Crainic, Li, & Toulouse, 2006), and a scatter search (SS) (Crainic & Gendreau, 2007). In the latter paper, the authors conclude that the proposed SS failed to meet their expectations and further research is needed to realize the full potential of SS.Inspired and motivated by the advances in the heuristic approaches for the MCNDP, this paper contributes to the existing body of work by: (i) proposing an efficient iterated local search (ILS) that utilizes new and enhanced cycle-based neighborhood operators, long and short term memory structures, and an innovative perturbation strategy based on ejection chains (Glover, 1996) that aims at guiding the search towards unexplored regions of the solution space; (ii) introducing an efficient SS that considers the search history and “solvency-based” measures to produce offspring; and (iii) presenting results of computational experiments conducted on benchmark instances using an algorithm incorporating the various elements described above. The majority of the heuristics for the MCNDP utilize a trajectory-based or an evolutionary framework to select arcs for inclusion in the design, and subsequently call a commercial optimizer (e.g., CPLEX) to solve the corresponding flow subproblem. As the flow subproblems become larger, the solution time for repeatedly finding minimum cost flows might become significant, even though linear programming optimizers are relatively efficient. Towards this end, we call the linear programming (LP) solver as few times as possible in the proposed algorithm in order to reduce its computational requirements.The remainder of this paper is organized as follows. Section 2 provides a brief review of the recent literature on the MCNDP. Section 3 presents our evolutionary algorithm and all of its components, namely the Initialization phase, the SS, and the ILS. In Section 4 we describe details of our computational experiments and we also present results of applying the proposed algorithm to benchmark MCNDP instances from the literature. Conclusions are given in Section 5, where research prospects are also provided.A number of efficient algorithms have appeared in the literature to address the inherent complexity of solving the MCNDP. In this section, we provide a brief review of the available methods but focus on heuristic, as opposed to exact, solution algorithms for reasons stated earlier.Crainic, Gendreau, and Farvolden (2000) propose a simplex-based tabu search method for the MCNDP using a path-flow based formulation of the problem. Their method combines column generation with pivot-like moves of single commodity flows to define the path flow variables. In a similar fashion, Ghamlouche et al. (2003) describe cycle-based neighborhoods for use in metaheuristics aimed at solving MCNDPs. The main idea of the cycle-based local moves is to redirect commodity flows around cycles in order to remove existing arcs from the network and replace them with new arcs. The authors use the proposed neighborhood structures in a tabu search algorithm, where a commodity flow subproblem is solved to optimality at each iteration.Ghamlouche et al. (2004) propose an evolutionary algorithm for the MCNDP. Their solution framework is based on path relinking, in which cycle-based neighborhoods are used to generate an elite candidate set of solutions in a tabu search algorithm and for moving from the initial to the guiding solution. When updating the pool of solutions, the dissimilarity of solutions is considered as an additional component in calculating the solution value. Alvarez, González-Velarde, and De-Alba (2005) describe an SS algorithm for the MCNDP. The authors use GRASP, originally proposed by Feo and Resende (1995), to produce a diversified initial set of solutions. Each commodity path is subject to an improvement process. The solutions are combined by choosing the best path for each commodity among the solutions that are being combined. A feasibility restoration mechanism is also available for solutions that are infeasible. In contrast to the recombination process of Alvarez et al. (2005), our SS does not consider commodity paths to build a solution; instead, independent arcs are combined to create offspring. We believe that the latter enhances the SS algorithm’s capabilities, as more combinations can occur when arcs instead of paths are combined together, leading to a rich pool of offspring.A parallel cooperative strategy is described by Crainic and Gendreau (2002) using tabu search and various communication strategies. In a similar fashion, Crainic et al. (2006) propose a multilevel cooperative search on the basis of local interactions among cooperative searches and controlled information gathering and diffusion. The focus of their algorithm is on the specification of the problem instance solved at each level and the definition of the cooperation operators.Katayama, Chen, and Kubo (2009) propose a column and row generation heuristic for solving the MCNDP. The authors relax the arcs’ capacity constraints, while a column and row generation technique is developed to solve the relaxed problem. Using similar ideas, Yaghini, Rahbar, and Karimi (2013) present a hybrid simulated annealing (SA) and column generation (CG) algorithm for solving the MCNDP. The SA is used to define the open and closed arcs, wherein the flow subproblem is solved via CG.A local branching technique for the MCNDP is proposed by Rodríguez-Martín and Salazar-González (2010). Even though the method, originally proposed by Fischetti and Lodi (2003), is exact by nature, high quality heuristic solutions can be produced using an MIP solver as a “black box”. A solution framework that employs a combination of mathematical programming algorithms and heuristic search techniques is introduced by Hewitt et al. (2010). Their methodology uses very large neighborhood search in combination with an IP solver on an arc-based formulation of the MCNDP, and an LP relaxation of the path-based formulation using cuts discovered during the neighborhood search. A follow-up study by Hewitt, Nemhauser, and Savelsbergh (2012) introduces a generic branch-and-price guided algorithm for integer programs with an application to the MCNDP.In this section, we first present a formal definition of the problem including the notation that will be used in the rest of the paper and then describe in detail the components of the main algorithm.The MCNDP is defined on a graphG=(N,A),whereNis the set of nodes andAis the set of arcs. Each arc(i,j)∈Ahas an associated fixed cost fijthat is incurred if it is selected for inclusion in the network, has a cost per unit of flow cij, and has a capacity uij. A set of commodities denoted byPis given, where each commodity has an origin, a destination, and a quantity to be shipped from origin to destination. Problems with more than one origin or destination per commodity can be modeled by splitting commodities (see Holmberg & Yuan, 2000).The goal of the problem is to select a subset of arcs that are to be included in the final design of the network along with the commodity flows on these arcs, to minimize the total cost of the selected arcs and the flow distribution on the resulting network. For simplicity, we refer to the arcs that are included in the final design of the network as open arcs; otherwise, the arcs should be considered as closed. Binary variables yijare used, whereyij=1if the arc(i,j)∈Ais open, andyij=0otherwise. The flow on each arc(i,j)∈Athat is used for shipping each commodityp∈Pfrom its origin to its destination is denoted byxijp. Conservation of flow constraints must be satisfied at each node, and there are capacity constraints of the form∑p∈Pxijp≤uijfor each(i,j)∈A. The cost f(s) of a solution s that is defined by variablesxijpand yijfor(i,j)∈Aandp∈Pis computed using(1)f(s)=∑(i,j)∈A∑p∈Pcijxijp+∑(i,j)∈Afijyij.We adopt the convention thatf(s)=∞if solution s is infeasible.Two types of mathematical formulations for the problem appear in the literature; an arc-based and a path-based formulation. We refer to Gendron, Crainic, and Frangioni (1998), Frangioni and Gendron (2001) and Hewitt et al. (2010) for details of these mathematical formulations.Our proposed solution methodology is an evolutionary algorithm that evolves a population of solutions using the principles of SS and applies ILS (Lourenço, Martin, & Stützle, 2002) as an improvement method. Following the basic template of the SS framework, our solution approach is composed of three distinct phases: (i) an Initialization phase where a population of good and diverse solutions is produced and a Reference Set (set R) is initialized; (ii) a Scatter Search phase where a recombination process takes place to produce offspring; and (iii) an Education phase where these offspring (hosted in set C) are “educated” by attempting to improve their quality via the proposed ILS. Computational time is used as the termination criterion. The framework is given in Algorithm 1.Fig. 1 illustrates the proposed evolutionary algorithm in a flow chart, the different components making up different process steps, as they are described in the following. The scatter search is composed of the Solution Combination method and the pool of offspring, while the iterated local search (described in Section 3.6) consists of the local search and the ejection cycles. Evaluation of the solutions is performed by the Reference Set update rationale (described in Section 3.5.1).Details of the three phases of Algorithm 1 are explained in the subsections below. Prior to this, however, we describe a flow routing procedure that is used in each phase of our algorithm.In the Initialization phase of Algorithm 1, solutions are created by successively adding flow to an existing partial solution by selecting a commodity and routing its required flow from origin to destination. A similar solution creation method is used in some iterations of the scatter search phase where some open arcs are selected by the solution recombination method but more are needed to create a feasible flow. Finally, when applying iterated local search within the Education phase, re-routing of flow is applied both in the process for creating neighbors of the current solution and in the procedure for perturbing the current solution. In each of these phases, the routing or re-routing is determined from the solution of a shortest path problem that is obtained by applying Dijkstra’s algorithm. We now provide details of how these shortest path problems are defined.Consider a partial solution defined byyij=y¯ijandxijp=x¯ijpfor each arc(i,j)∈Aand each commodityp∈P. Thus,y¯ij=1for each arc (i, j) that is open in the partial solution,uij−∑p∈Px¯ijpis the remaining capacity in each arc (i, j). The aim is to route wpunits of flow of some commodity p from node ipto node jp, for appropriately defined wpand nodes ipand jp.There are two alternative shortest path problems that we define. For a single-path routing, a path from ipto jpis required such that each of arc of the path has a remaining capacity of at least wp, thereby allowing all of the desired wpunits of flow to be routed along this path. On the other hand, for multiple-path routing, it is sufficient to find a path where each of its arcs has a non-zero remaining capacity. For both types of routing, we define a shortest path problem on the original graphGwith a costc¯ijfor each arc(i,j)∈A,for suitably defined values ofc¯ij.For single-path routing, we define for each arc(i,j)∈Ac¯ij={wpcij+fij(1−y¯ij)ifwp≤uij−∑p∈Px¯ijp,∞otherwise.Thus, only arcs that can accommodate an additional wpunits of flow have a finite cost. For an arc (i, j) that can accommodate this additional flow,c¯ijis the cost of that flow in arc (i, j) plus any additional cost of opening arc (i, j) if it is not already open in the current partial solution.For multiple-path routing, we similarly define for each arc(i,j)∈Ac¯ij={min{uij−∑p∈Px¯ijp,wp}cij+fij(1−y¯ij)if∑p∈Px¯ijp<uij,∞otherwise.In this case, arc (i, j) can be used for an additionalmin{uij−∑p∈Px¯ijp,wp}units of flow. If this value is strictly positive, thenc¯ijis the is the cost of that flow in arc (i, j) plus any additional cost of opening arc (i, j); otherwise, arc (i, j) cannot accommodate any additional flow and therefore the value ofc¯ijis set to infinity.For both types of routing, Dijkstra’s algorithm is applied to find the shortest path from node ipto node jp. If the shortest path length is not finite, then no routing of flow is possible. Otherwise, in the case of single-path routing, a flow augmentation process adds wpunits of flow to all arcs of the shortest path from node ipto node jp. Analogously, in the case of multiple-path routing, if P is the shortest path from ipto jp, thenmin{min(i,j)∈P{uij−∑p∈Px¯ijp},wp}units of flow are added to all arcs of P.In the Initialization phase, each iteration of the construction heuristic selects an unrouted or partially routed commodity p at random. Then, a random choice is made as to whether a single-path or multiple-path routing is to be attempted with an equal probability for each choice.For a single-path routing, wpis the amount of flow for commodity p that is to be routed, and ipand jpare the origin and destinations nodes for this flow. If the shortest path computation, as described in Section 3.3, provides a solution with a finite shortest path length, then the flow is augmented. After removing commodity p from the set of unrouted or partially routed commodities and updating the variablesy¯ijandx¯ijp,the heuristic proceeds to the next iteration. Otherwise, no single-path routing of the chosen commodity exists, and this iteration is repeated using multiple-path routing.For a multiple-path routing, wp, ipand jpare defined as above. Following the shortest path computation described in Section 3.3, the flow is augmented and the values ofw¯p,y¯ijandx¯ijpare updated (with commodity p removed from the list of unrouted or partially routed commodities if wpis reduced to zero), and the heuristic proceeds to the next iteration.The construction heuristic is applied repeatedly until λ different solutions are created, among which μ are selected to build the Reference Set. Details about the creation of the initial Reference Set are given in Section 3.5.1.The SS phase evolves the Reference Set of solutions using an efficient recombination method as follows. A subset generation method selects κ solutions from the Reference Set, which form the Candidate Set (CS), and a solution combination method is then applied to produce one solution. This procedure is repeated until 2μ offspring are produced, which is double the number of parent solutions in the Reference Set. We choose μ best solutions, in terms of the solution cost, out of the 2μ offspring to proceed to the next phase. Other strategies were also tested, such as randomly choosing μ of 2μ solutions, but the algorithm performs better by choosing μ best solutions. The offspring are checked as to whether they meet the criteria to be inserted into the Reference Set or not, before proceeding to the Education phase. In the Education phase, ILS is used to improve the quality of each offspring, before these offspring are checked again for insertion into the Reference Set according to elitist criteria. These procedures are explained further in the following sections.The goal of using a Reference Set R is to maintain a balance between quality and diversity of solutions, and to avoid a premature convergence of the algorithm. An obvious measure of the quality of a solution s is its cost f(s). An alternative quality measure that becomes relevant after the evolutionary process has started is the Solvency Ratio, defined by(2)SR(s)=neo(s)/hits(s),where hits(s) denotes the number of times that solution s has participated in the recombination process to produce an offspring, and neo(s) denotes the number of educated offspring of s, which is the number of times that an offspring of s has been educated and included in R. The smaller the Solvency Ratio is, the lower the value of the particular solution s is to the evolution process. In this way, a higher cost solution with respect to the usual objective function f may be beneficial to the search if it produces well-educated offspring. The purpose of this ratio is to measure and consider the solvency performance of the solutions into the Reference Set, regardless of the solution cost. We wanted it to be independent of the solution cost, and represent clearly the solvency. It happens, lower solution costs parents to provide very high quality and well educated offspring. Nevertheless, we consider the solution cost when combining the solutions (see Section 3.5.2).Our diversity measure uses the Hamming distance between pairs of solutions,D(s,s′)=∑(i,j)∈A|yijs−yijs′|,for two solutions s and s′. The total dissimilarity for Reference Set R is then defined by(3)TD(R)=∑s,s′∈RD(s,s′),where the sum is over allμ(μ−1)/2pairs of solutions in set R.The creation of the initial Reference Set proceeds as follows. The first μ solutions among the λ generated within the Initialization phase are inserted into R. The remainingλ−μsolutions are then considered sequentially for replacing a solution in R. Specifically, if such a solution s satisfies the condition f(s) < f(sbest), where sbest is a least cost solution in R, or if there is a solution r ∈ R for which f(s) < f(r) and D(r, sbest) < D(s, sbest), then s is inserted into R. Otherwise, s is not included in R. When s is inserted, solution sworst ∈ R, where sworst has the largest cost among solutions in R, is removed from R.At each SS iteration of the evolutionary process, 2μ offspring are generated, and a sequence of decisions is made on whether to replace a solution in R with the offspring under consideration. In the later stages of the evolutionary process, this decision depends on the value of the SR obtained from (2), but a different process is used at the start of the evolutionary process when SR cannot be meaningfully computed. Specifically, let scand ∈ R be the candidate for removal from the Reference Set R, wherescand=sworstfor the first two iterations of the evolutionary process, and scand is the solution in R having the smallest Solvency Ratio from the third iteration onwards. An offspring s replaces scand in the Reference Set R if either f(s) < f(sbest), or if f(s) < f(scand) and TD(R) < TD(R∖{scand} ∪ {s}). This procedure differs from other studies where the usual practice is always to remove the worst-cost solution sworst from R without taking into account any effect it might have on the evolution.In this section, we discuss how our proposed solution combination method generates each offspring.Each offspring is generated from the candidate set (CS) comprising κ solutions from the Reference Set R. The solutions in CS are chosen probabilistically with a bias towards promising parents as determined by their Solvency Ratios (SR). Specifically, the probability of a solution s being included in the candidate set is proportional to SR(s). In this way, a solution s with a low SR(s) is gradually neglected, and the focus is on new solutions that produce well-educated offspring. Because the solvency ratio changes while SS iterations are being performed, the scatter search phase has a dynamic character, and premature convergence is typically averted. Furthermore, to enable diversification, a penalty (as expressed by the term αhits(s) in Eqs. (4) and (5) below) is used to weaken the impact of a frequently selected parent and thereby enable diversification.The arcs of the solutions in CS are combined to produce an offspring. For a given solution s, each arc (i, j) is either open ifyijs=1or closed ifyijs=0. We associate a valuef(s)+αhits(s)with solution s, where f(s) and hits(s) are previously defined, and α is a scaling parameter. We now introduce a scoring procedure to determine whether an arc (i, j) will be open or closed in the offspring, according to the following scores:(4)Opij=∑s∈CSyijsf(s)+αhits(s)∀(i,j)∈A(5)Clij=∑s∈CS1−yijsf(s)+αhits(s)∀(i,j)∈A.Opijand Clijare the scores for arc (i, j) being open and closed, respectively, and if Opij> Clij, the preferred arc status is open; otherwise its preferred status is closed.The preferred status of open or closed for each arc (i, j) is our starting point for creating a new solution from the solutions in the CS. We first assume that the open and closed arcs correspond to their preferred status, which implies that the values of the yijvariables are fixed. To determine values of thexijpvariables or conclude that there is no feasible solution with the fixed yijvariables, the associated capacitated multicommodity network flow problem is solved using an LP optimizer. If a feasible solution is obtained, then this is the offspring obtained from the candidate set (but with any open arc having a zero flow having its status changed to closed).If the multicommodity network flow problem is infeasible, then the offspring is created using similar methodology to that of the Initialization phase as described in Section 3.4. Specifically, for each arc (i, j) with a preferred status of open, we temporarily change the fixed cost to fij/M and the cost per unit of flow to cij/M, where M is a large constant. With these updated costs, the construction heuristic is applied, and the resulting solution is the offspring obtained from the CS. The low costs associated with the arcs having an “open” preferred status encourages Dijkstra’s algorithm to find shortest paths containing some of these arcs, which results in a large proportion of such arcs being open in the offspring solution.The μ elite offspring, chosen among the 2μ produced by the SS phase, are individually “educated” (i.e., improved) using ILS. The components of the ILS are shown in Algorithm 2.The proposed ILS has two main components, namely a local search and a perturbation strategy. The proposed local search uses new neighborhood operators and short term memory (represented by memory structureg→) to avoid cycling. The perturbation strategy, namely Ejection Cycles, partially modifies the current solution according to information gathered during the search (long-term memory depicted byh→) in the spirit of ejection chains (Glover, 1996).Our ILS neighborhood is based on the cycle-based operator, as originally proposed by Ghamlouche et al. (2003). Their approach is to select a pair of nodes containing a positive flow and then re-route the flows of the individual commodities between these nodes. In this paper, we design a more efficient and effective approach based on the notion of inefficient arcs and inefficient chains, as described below. Further, we allow a partial re-routing of flow that maintains flow feasibility. In contrast, Ghamlouche et al. (2003) remove all flow between the two selected nodes, and if the new flows do not result in a feasible solution, then a feasibility restoring routine is applied.Consider a solution defined by the variablesxijpand yijfor each arc(i,j)∈Aand each commodityp∈P. For each open arc (i, j), whereyij=1andxijp>0for at least one commodity p, we define the inefficiency ratio as(6)Iij=∑p∈Pcijxijp+fij∑p∈Pxijp,which is a measure of the average cost per unit of flow that is sent along arc (i, j). The lower the value of Iij, the more efficient we regard arc (i, j) for accommodating flows. The average inefficiency ratio is defined asI¯=∑(i,j)∈AIijyij/∑(i,j)∈Ayij,and we define a set of inefficient arcs asAI={(i,j)|yij=1,Iij>I¯},so that(i,j)∈AIif arc (i, j) has an inefficiency ratio that is higher than the average. Our aim is to create neighborhood moves that remove flows from some of the inefficient arcs in setAI.We now describe how our inefficient chains are constructed from a subset of the inefficient arcs. First, an arc is randomly chosen from the setAIof inefficient arcs to form a component of the first inefficient chain. If the current partial inefficient chain extends from node i to node j, then an arc(h,i)∈AIor(j,k)∈AIis added to the current chain (where nodes h and k are not included in the current chain). The arc added is chosen such that it has an inefficiency ratio that is as large as possible. Whenever an arc is included in a chain, it is deleted fromAI. The process of extending the current chain continues until no further extension is possible. UnlessAIis empty or contains a single arc, the process iterates with a random arc chosen to start a new chain. When the process ends, any chains containing a single arc are discarded. The latter are likely to be included in inefficient chains in a subsequent ILS iteration, since inefficient chains are reconstructed from scratch at each ILS iteration.Having constructed a set of inefficient chains, we now describe how our neighborhood is formed. Each neighbor is based on a sub-chain of an inefficient chain and is defined by the starting node i and the ending node j of the sub-chain. If a chain comprises nodesn1−n2−⋯−nm,then the (i, j) values are considered in the order(n1,n2),(n1,n3),…,(n1,nm),(n2,n3),(n2,n4),…,(n2,nm),…,(nm−1,nm).On the basis of our initial computational tests, we restrict our attention to sub-chains between i and j comprising at most ζ arcs, which helps to reduce computation times but, at the same time, does not significantly restrict the diversity of potential neighborhood moves.The key aspect of our neighborhood is the re-routing of flow from arcs of the sub-chain to other arcs of the network. An initial random decision is made as to whether a full re-routing or a partial re-routing is to be attempted for this sub-chain, with an equal probability for each choice. First, a listPIof commodities is formed that have a positive flow through at least one arc of the sub-chain. To obtain a neighbor solution, the list of commodities is scanned and a re-routing of flow is attempted for each commodity p ofPIin turn. Suppose that the flow enters the sub-chain at node ip, leaves the sub-chain at node jpand the amount of flow is wp.Dijkstra’s algorithm is applied to find a shortest path from node ipto node jpwith the goal of finding a suitable path for the re-routing of flow. The shortest path problem is created according to the description given in Section 3.3, but withc¯ij=∞for each arc between ipand jpin the selected chain. For the case of full re-routing, the method for single path routing of Section 3.3 is used, while for partial re-routing the multiple-path routing method is used. If the resulting shortest path length is not finite, then the flow remains unchanged in the trail solution being constructed. Otherwise the flow is augmented as described in Section 3.3, and a corresponding reduction is made to the flows in the sub-chain. When all of the commodities ofPIare considered, the trial solution is a potential candidate for being selected as the neighbor defining the next move. Additional trial solutions are created by removing the first element of listPIand repeating the process, again starting with a random decision as to whether a full or partial re-routing is to be attempted, untilPIis empty. The completed procedure is executed for every possible sub-chain.We illustrate the idea of re-routing flows by an example shown in Fig. 2. The example shows three commodities each with a different line pattern, and a graph where origin node 3 and destination node 8 define a part of the inefficient chain. The re-routing of the flows between nodes 3 and 8 causes individual commodity flow disconnections. The flow re-routings take place independently for each different commodity between its origin and destination nodes, i.e., the commodity shown with the solid black line must travel from node 4 to node 7, the dotted one must travel from node 6 to node 8, and the dashed one from node 3 to node 8. The gray lines depict possible alternative re-routing paths within the network. All three flow re-routings, for this particular example, result in one single neighbor.Another important component of our ILS is a frequency-based memory feature adopted by Paraskevopoulos, Tarantilis, and Ioannou (2012) that penalizes potential moves that alter flows that have been changed frequently in previous iterations of the search. A vectorg→of size|A|is used to store each value gij, which is the number of times that the value ofxijpis changed for somep∈P. After an improvement in the current solution is observed,g→is reinitialized to the zero vector.The following equation defines the local move cost from solution s to a trial solution s′ as(7)Δfmove(s,s′)=f(s′)−f(s)+β∑(i,j)∈Abijgij,where β is a scaling parameter, and bijhas a value equal to 1 if the arc (i, j) participates in the current local move from s to s′, and a value 0 otherwise. The component β∑(i, j) ∈ Abijgijis added to the cost of the local move to penalize moves that involve frequently selected arcs.Trial solutions with smaller values of Δfmove are generally preferred. However, it may be that this number is large enough to prevent the search from selecting a high-quality neighbor s′. To avert such cases, an aspiration criterion is used: if f(s′) < f(sILSbest), the penalty component is ignored so thatΔfmove=f(s′)−f(s). The neighborhood search procedure is shown in Algorithm 3.In Algorithm 3 the function IdentifyDifferentCommodities forms the listPIby identifying the different commodities that have positive flows between the nodes i and j of an inefficient chain k. CreateNeighbor creates a neighboring solution of s′, and RemoveFirstElement removes the first element of the list. Finally, the isFeasible is a boolean function that returns “true” if a particular combination (k, i, j) leads to some re-routing of flow.A major component of the ILS is its perturbation strategy (Lourenço et al., 2002). The goal is to partially rebuild the current local optimum, such that the new diversified solution preserves some information from the local optimum. The proposed perturbation strategy in this paper, namely Ejection Cycles (EC), applies multiple cycle-based moves in the spirit of ejection chains (Glover, 1996). The main idea of the ejection-chains strategy is to apply a compound move consisting of a series of consecutive local moves. Adopting this idea, our EC comprise a series of consecutive cycle moves of the type described in Section 3.6.1. The aim of EC is to perturb the structure of the current solution to achieve diversification, and also to remove some of the inefficient arcs from the solution.The are two phases to creating the sequence of local moves. The first phase creates inefficient chains to re-route flow using similar ideas to Section 3.6.1, but considers the previous usage of arcs in local moves instead of cost and also allows flows in arcs to violate capacity constraints. The second phase attempts to remove infeasibility by doing further flow re-routing, again using arc usage in determining the path.We now present more precise details of how our sequence of local moves is determined. In the first phase, we first find a set of inefficient chains and focus on sub-chains containing at most ζ arcs. For a given sub-chain, the listPIis formed, and ip, jpand wpare computed. The list of commodities is scanned and a re-routing of flow is performed for each commodity p ofPIin turn. However, in this re-routing, feasibility with respect to arc capacities is not enforced, as the second phase essentially operates a repair mechanism to restore feasibility. The first phase employs a full re-routing by applying Dijkstra’s algorithm to find a shortest path from ipto jp, where cost for each arc(i,j)∈Ais(8)c¯ij={cijhij+fij(1−y¯ij)if(i,j)∉F,∞otherwise,wherehij−1is the number of times that arc (i, j) has participated in a local move, and initialization setshij=1for all(i,j)∈A,andFis a set of forbidden arcs that initially comprises all arcs between ipand jpin the subchain. The hijvalues have a similar purpose to the gijvalues of Section 3.6.1 except that the method of initialization is different. Also, the re-initialization for gijis replaced by a scaling process for the hij. Specifically, to avoid hijbecome very large for some arcs (i, j), we periodically divide hijby hmin  for all(i,j)∈A,wherehmin=min(i,j)∈Ahij. If Dijkstra’s algorithm returns a shortest path length of infinity, then the current sub-chain is not considered further and another one is selected. Otherwise, a flow of value wpis added to each arc of the shortest path in the perturbed solution and removed from each arc of the sub-chain.When re-routing of flow between nodes ipand jpof the sub-chain is complete for eachp∈PI,we check if any arc has a flow that violates its capacity constraint. If there is no violation, then a new feasible solution is found and the EC terminates with a perturbed solution. When some flows violate arc capacities, we proceed as follows. LetAVdenote the set of arcs having a capacity violation. For all arcs(i,j)∈AV,a set of commoditiesPI′is selected whose removal from (i, j) restores feasibility but keeps the capacity utilization of the arc as high as possible. Specifically, the process of repeatedly selecting a commodity p with the largest flowxijpin (i, j) is inserted inPI′and the flow in (i, j) is reduced byxijpis applied until the flow in (i, j) is reduced to exactly uijor the next selection would cause the flow in (i, j) to become strictly less than uij. In the latter case, the final commodity p selected for insertion intoPI′is chosen to have minimal flow in (i, j) from among those commodities where the removal of their flow from (i, j) reduces the total flow in (i, j) to be less than or equal to uij. Having formedPI′,the respective ip, jpand wpare computed, and infeasibility chains that are formed in the same way as for inefficient chains, as described in Section 3.6.1.Having formed the infeasibility chains, the aim is to re-route the flow in the chain using the method described above. More precisely, Dijkstra’s algorithm is used to find a shortest path from the the starting node ipof the sub-chain to the ending node jp, where all arcs between ipand jpof the sub-chain are added to the setFand costs for the shortest path problem are defined by (8). If a suitable path for re-routing is found, then the trial solution s* is updated. The process of re-routing flow in other infeasibility chains continues until no capacity violations occur or no further re-routing is possible due to the constraints imposed by setF. If the former case, the EC terminates with a perturbed solution. In the latter case, the EC returns to the initial feasible solution s′, the first commodity of setPIis deleted and EC is applied on the remaining commodities in the set. As in Section 3.6.1, additional trial solutions are created by removing the first element of listPIand repeating the process untilPIis empty. The complete procedure is applied to all sub-chains, and terminates when the first feasible perturbed solution is found. The pseudo code of the EC is given in Algorithm 4.IdentifyViolatedArcs identifies the set of violated arcsAV. The function NeighborExists is a boolean function that returns “true” if there exist an alternative path that the flow can be re-routed, regardless the capacity constraints at arcs. If no alternative paths are found (in case all neighboring arcs have been assigned a cost of infinity), then NeighborExists returns “false”. The function UpdateViolatedArcs identifies which of the arcs of the re-routed paths are violated in terms of capacity constraints and updates the set of violated arcsAV. The function IdentifyExcessCommodities identifies the excess commodities that need to be removed from the violated arcs to restore capacity feasibility, while similarly UpdateExcessCommodities updates the excess commodities in the next iterations.This section presents the computational analyses conducted to evaluate the performance of the proposed algorithm and comparisons with the state-of-the-art. The section is structured as follows: In Section 4.1, we describe the data sets used in the experiments, followed by Section 4.2 which explains the way that the algorithmic parameters are calibrated. Sections 4.3 and 4.5 look at the effect of the network efficiency and the Solvency Ratio strategies used on the performance of the algorithm. The way in which the components of the proposed algorithm affect the solution quality is tested in Section 4.4. Finally, Section 4.6 presents extensive comparison results with state-of-the-art algorithms that have been proposed for the problem.To evaluate the performance of the proposed algorithm, computational experiments are conducted on the C and C+ benchmark instances described in Crainic et al. (2000) and are available online (http://pages.di.unipi.it/frangio/). These sets include instances with 20, 25, 30 and 100 nodes, 10–400 commodities and 100–700 arcs, and have been widely used in the literature. These instances differ from one another with respect to the nature of the arc capacities, which are either loose (L) or tight (T), and with respect to the relative importance of fixed costs (F) and the variable flow costs (V) per unit of flow. There also exist benchmark instances described by Alvarez et al. (2005) defined on an undirected graph using edges as opposed to a directed graph using arcs. These define a different problem than the one we address in this paper, as is discussed by Crainic et al. (2000), and is the reason why this set is not considered here.The proposed algorithm was implemented in a Visual Studio 2010 environment using the C++ programming language, and all runs were performed on a single core Xeon E5507 2.27 GHz using CPLEX 12.6 as the optimizer.The proposed Cycle-based Evolutionary Algorithm (CEA) uses five parameters; the number λ of initial solutions examined to produce the Reference Set R, the cardinality μ of R, the cardinality κ of CS, the maximum number δ of local search iterations without an improvement in the solution quality, and the maximum number ϑmax  of CPLEX calls for which an improvement in the current solution is not observed. The termination criterion is the computational time. Various time limits were used to test our algorithm according to different time limits used by the state-of-the-art algorithms of the literature.The scaling parameters α and β are self-calculated during the solution process, and are equal to the average cost of an arc in the current best solution found, i.e., α=β=f(sbest)/∑(i,j)∈Ayijsbest. The parameter λ does not appear to have a significant impact to the quality of the solutions; however, to have an adequate initial population size, we set it to 1500. Parameter ζ (see Section 3.6.1 for details) was set equal to 4, which means that local search attempts to re-route the flows of a maximum of four arcs of a sub-chain. Larger values led to infeasibilities in the neighboring solutions, either in the connectivity of the paths or the capacity of arcs.We setκ=3to preserve the SS character of the proposed algorithm. Parameter κ needs to be larger than 2 to enhance the recombination process, but should be relatively small to ensure that a large number of possible combinations among the solutions of the Reference Set is considered. We tried 4 and 5 which resulted in a poor variety of offspring, due to the limited number of combinations. The latter problem was more prominent in the later SS iterations, when convergence is close and the need for different offspring is more apparent.Parameters δ and ϑmax  are interrelated as they typically control the total number of local search iterations. In particular, ϑ tracks CPLEX iterations; it is initialized to 1 and is incremented by one unit until ϑmax  is reached. At each iteration, the number δ of local search iterations is set equal to 10ϑ. Our experiments indicate that values of ϑmax  equal to 6, 7, and 8 are appropriate, with values below 6 resulting in deterioration in the solution quality, and values greater than 8 slowing down the process without yielding any significant gain in the solution quality.Table 1 shows the computational experiments conducted to investigate on the algorithm’s behavior with respect to different sets of parameters. Different parameter sets were used for different groups of problems. For large-scale problems, the Reference Set was of relatively small sizes and δ was assigned high values, whereas opposite settings were used for small to medium scale problems, for reasons described above. Table 1 shows the C and C+ benchmark instances classified into 6 groups according to their size. The label for each group is a vector depicting the number of nodes, the number of arcs and the number of commodities. The problem instances within each group differ in the tightness of the arc capacity constraints and the relative importance of the fixed costs and the costs per unit of flow. The calibration was conducted by using one problem instance from each group, shown in the headings of the six main columns of Table 1. For each instance, ten runs, each with a run time of two hours, were conducted to retrieve the average solution values for each instance shown under the second column for each group. The parameter set that produces the best average (shown in bold font) for each group is fixed and used to solve the rest of the instances in that group to produce the results presented in the tables of this section.As Table 1 shows, the effect of the parameters of CEA varies according to the size of the problem solved. In small to medium scale instances, the evolutionary strategy had more impact than local search, since the cardinality of the neighborhood is relatively small and local search is unable to adequately explore the search space. In contrast, the solution neighborhood is enriched with more solutions and the impact of the local search is more prevalent in the solution process as the size of the problem instance increases. Driven by these observations, the size μ of the Reference Set takes larger values for small to medium scale problems, and relatively small values for the larger scale instances.We also conducted indicative t-tests for different parameter settings and we include the results in Table 2. Table 2 has six parts that refer to results regarding six different benchmark instances. Each part of the table has two rows; the first row indicates the pairs of parameters (θmax, μ) and the second row reports the p-value derived by comparing the two different sets of results derived by 10 runs of the algorithm. As Table 2 shows, for some of the pairs one can identify statistical significance (i.e., p-value ≤ 0.05), nevertheless for some other the difference is not statistically significant. We select the parameter setting with the best average value (see Table 1), and we keep it fixed for each group of instances to perform our experiments, regardless of the statistical significance status.To illustrate the impact of the network efficiency on the solution cost, we have conducted analyses to shed light into the behavior of the search on two problem instances, namely 20,230,200VT and 30,700,400VL. The network efficiency is defined with respect to either the maximum arc inefficiency or the average arc inefficiency, where the inefficiency measure is as defined in Section 3.6. The results are given in Fig. 3, which shows how the two efficiency measures and the total cost change as the search progresses over time, separately for instance 20,230,200VT on the left and for instance 30,700,400VL on the right.Fig. 2(a) and (b) respectively show the changes observed in the value of the best solutions found for the 20,230,200VT and 30,700,400VL problem instances over time. Similarly, Fig. 2(c) and (d) shows the maximum inefficiency of an open arc for different solutions found over time. We observe that as the algorithm iterates, the maximum arc inefficiency is dramatically reduced and follows a logarithmic trend. Conversely, Fig. 2(e) and (f) show an increase in the average efficiency of the arcs as the search progresses, which is indicative of an increase in the overall efficiency of the network as the solution quality is improved.Experimentation was conducted on different versions of the proposed CEA to investigate the effect of various components on the final solution quality. Three versions of CEA were thus considered: (i) Version “∖EC” is where a random perturbation strategy is used instead of EC. According to this random strategy, 25 percent of the commodities are selected at random, which are then removed and re-routed via the construction mechanism as discussed in Section 3.4. (ii) Version“∖SolvR” replaces the Solvency Ratio strategy with a random strategy for the parent selection and the Reference Set updating criteria. According to the random strategy, the parents that comprise the Candidate Set are selected at random and the elitist updating criteria described in Section 3.5.1 are used. (iii) Version “∖Ineff”, performs local moves on chains composed by all arcs of the network, disregarding any preference given to inefficient chains.The results of the experiments are reported in Table 3. In this table, column “CEA” shows the best results of these experiments, derived from 10 runs for each problem instance, where the computational time of each run is limited to two hours. The values under column “Percent deviations” in Table 3 show the percent deviations of the solution values obtained by the three versions of the CEA from those of the best solution value. In particular, the deviations are calculated as100(v(CEA)−v(Alg))/v(CEA),where v(Alg) is the solution value obtained by one of the three versions of CEA. We conducted statistical t-tests between the runs of CEA and the runs of different versions of CEA, and a footnote is used to indicate which of these differences were statistically significant.From Table 3, it can be easily observed that the impact of the EC in the quality of the final solution is significant, and can yield reductions of up to 4.24 percent in total cost. The maximum improvements afforded by the Solvency Ratio and the Inefficiency Measures are 4.03 percent and 8.93 percent, respectively. A negative deviation value in this table indicates that the solution found by the CEA is better. On average, the most significant impact comes from the Inefficiency Measures component with an average deviation of−1.56percent. The same statistics for the Solvency Ratio and the EC are−0.96percent and−0.80percent, respectively.To illustrate the effectiveness of the Solvency Ratio, tests were conducted on two instances, namely 100-400-30-FT and 20,300,200FT, for the reason that these two instances typically present the general behavior of the algorithm using solvency-based and random parent selection strategies.Fig. 4presents the comparisons between the two strategies. The first two SS iterations are used as a warm up for the solvency strategy, which is enabled from the third SS iteration onwards as is apparent from the figures. Fig. 3(a) and (b) show how the best solution values evolve over time. For 100,400,40FT, it is easily seen that solutions obtained by the random-based strategy are quickly trapped in a local optimum, whereas the solvency-based strategy is slower to improve the best solution initially, but displays a gradual yet continual reduction in the overall cost as the generations evolve, and terminates with a better overall solution. Instance 20,300,200FT exhibits a similar pattern, i.e., the solvency strategy provides a large improvement in the early SS iterations and then follows a less steep drop as the algorithm continues to improve the total cost. The random strategy is again trapped in a local optimum at iteration 12. Fig. 3(c) and (d) show the changes in the average solution cost in the Reference Set over the SS iterations. The main observations on the behavior of the solvency-bases strategy are similar to the first two figures.A “healthy” evolutionary process should typically produce a decent number of educated offspring at each SS iteration. Fig. 4(e) and (f) show that this is also the case in the proposed algorithm. In particular, the figures show that the random strategy has difficulties in producing educated offspring and therefore results in premature convergence. In contrast, the solvency strategy is able to update the Reference Set with educated offspring even near termination.In this section, we report comparative computational results of the proposed algorithm with the Cycle-based Tabu Search (CTS) of Ghamlouche et al. (2003), Path Relinking (PR) by Ghamlouche et al. (2004), Multilevel Cooperative Algorithm (MCA) by Crainic et al. (2006), Capacity Scaling Heuristic (CSH) by Katayama et al. (2009), IP Search (IPS) by Hewitt et al. (2010), the two algorithms based on Simulated Annealing and Column Generation (SACG1 and SACG2) described by Yaghini et al. (2013) the results for which are reported with time limits 600 and a 18,000 seconds, respectively, and Local Branching (LocalB) by Rodríguez-Martín and Salazar-González (2010). The algorithm described by Alvarez et al. (2005) could not be included in the comparisons as the authors do not report any results with the instances tested here; instead they use their own benchmark instances. The reason for not being able to test our algorithm on the Alvarez et al. (2005) benchmark set is that these instances are based on an undirected graph and work with edges, whereas the problem we solve is on a directed graph and our algorithm has been developed to operate on arcs.Table 4 shows the comparison results where the first column shows the name of the instance as characterized by the number of nodes, the number of arcs and the number of commodities. The solution values obtained by the proposed algorithm are reported under column “CEA”. The remaining five columns report the relative percentage deviations of the solution values found by the CEA from those reported by the papers quoted above, and is calculated as100(v(CEA)−v(Alg))/v(CEA),where v(Alg) indicates the solution value produced by the corresponding algorithm and v(CEA) the solution value produced by the CEA. A negative value indicates that the solution found by the CEA is better.The first seven rows describe, to the best that we were able to extract, the computational resources used to run the algorithms. The row titled “T. Lim. (seconds)” reports the time limit used by the authors of the corresponding algorithm, whereas the “Used Cores” row indicates how many cores from the original configuration of the CPU were used to run the algorithm. It is assumed that the computational power increases linearly with the number of cores used. Due to different computing facilities, we have normalized the computational times using the approach described in Dongarra (2014) and data from http://www.cpubenchmark.net/. All comparisons were made according to the Passmark CPU Score (PCPUS). As we were unable to find PCPUS for Sun systems on http://www.cpubenchmark.net/, we used the Dongarra (2014) list, and selected an Intel equivalent. The final scores are reported in the row titled “PCPU Score”. The running times were normalized by using CEA as the reference point, i.e., Norm.TL(Alg)= PCPUS(Alg)TL(Alg)/PCPUS(CEA).The table also reports some summary statistics in the last six rows, including the median and the average of the deviations. The “MaxImpr.” row shows the maximum improvement afforded by the CEA. The lower this value is, the better the performance of the algorithm. The LeastGap row shows the maximum deviation over instances for which CEA did not find a better solution. Finally, the row named “Impr./43” shows the number of instances out of the total 43 tested, where CEA yielded the same or better results over the algorithm it is compared with.As the results shown in Table 4 indicate, the CEA is competitive with the state-of-the-art. In particular, CEA is able to produce optimal solutions for the 25,100,10, 20,300,40 sets of instances as well as for the large scale problem instances 100,400,10FL and 30,700,100VL. Furthermore, the CEA finds optimal solutions for 25,100,30FT and 20,230,40VL which could not be found by any of the heuristics used for comparisons with the exception of the ones described by Yaghini et al. (2013). The maximum deviations of the CEA are−8.75percent compared with CTS,−8.46percent compared with PR,−5.49percent compared with MCA,−12.21percent compared with CSH,−11.28percent compared with IPS,−17.07percent and−1.06percent compared with SACG1 and SACG2, respectively, and−23.81percent compared with LocalB.Noteworthy is the fact that on large-scale problem instances 20,300,200FT, 100,400,30FT and 30,520,100FT, new best solutions were obtained with values 107546, 139535 and 97856, respectively. These instances have up to 100 nodes, 520 arcs and 200 commodities, and the new best solutions deviate by−0.29percent,−1.06percent and−0.70percent over the previous best known solutions, respectively.On average measures, CEA outperforms CTS, PR and MCA by achieving average improvements of−3.49 percent,−3.13 percent, and−2.46 percent, respectively. Compared with the rest, the CEA still remains competitive with average deviations sitting at−0.38 percent from CHS,−0.74 percent from IPS,−1.09percent from SACG1, 0.04 percent from SACG2 and−2.24percent from LocalB. Compared with CHS, the proposed algorithm produces better results by−0.37percent on average. Similarly, SACG2 produces results that are better by 0.04 percent. We also note that we are unable to consider the result of SACG2 for instance 30,520,400FT as this value is lower than the lower bound 150009 reported by Katayama et al. (2009), and any comparison for this instance would therefore be misleading.The above comparisons are based on the results derived by using the running time limits imposed by the original authors. Even though our time limit was 20,000 seconds, the CEA was able to discover the best solution in less than two hours for most problem instances. In fact 34 out of 43 solutions CEA produces are derived within 2 hours, out of which 9 refer to large scale instances (which are in total 16). For very large-scale instances, improvements were observed in later SS iterations which necessitated additional running time. The latter observation is as one would expect with evolutionary algorithms, i.e., a number of SS iterations are needed in order that the initial population of solutions can be evolved such that high quality solutions can be produced.To conduct more objective comparisons, we ran our algorithm under different time limits, the ones used by the authors of the state of the art algorithms in the literature. The results are shown in Table 5 where the time limits are normalized according to the approach described in Dongarra (2014) and data from http://www.cpubenchmark.net/. Table 5 provides the percentage deviations of the solution values produced by the CEA compared with other algorithms under different time limits, i.e., 1034, 1434, 1483, 2353, 3900, 5408, 44496, 45877 refer to the time limits (in seconds) applied by LocalB, PR, SACG1, CTS, IPS, CSH, SACG2 and MCA, respectively. Negative deviations show that our algorithm yields better quality solutions. In particular, CEA produces solutions that are, on average, lower in cost by 0.89 percent as compared to LocalB, 2.17 percent as compared to PR, 0.31 percent as compared to SACG1, 2.70 percent as compared to CTS, 0.38 percent as compared to IPS, 0.16 percent as compared to CSH and 2.44 percent as compared to MCA. It yields results that are higher in cost by only 0.04 percent on average as compared to SACG2. These results indicate that the performance of the CEA is competitive to the state-of-the-art based on comparisons under different time limits.The final set of comparisons relate to the computation times needed by the CEA and other state-of-the-art algorithms to obtain the best solutions, which are shown in Table 6. The times for the latter group have been adjusted using the PCPUSs introduced in Table 4, such that an objective comparison can be made. As CEA outperforms CTS, PR and MCA in terms of the solution quality, the main focus will be on comparisons with CSH, SACG2 and IP. As Table 6 shows, CEA needs 47 percent less running time than SACG2 on average, for producing solutions that deviate by 0.04 percent from the ones produced by SACG2.Lastly, the version of CPLEX we use (12.6) is estimated to be 10 percent faster than versions 9.1 and 11 (internal communication with IBM). Nevertheless, to be able to conduct further comparisons the number of times that the state-of-the-art algorithms call CPLEX should be known. At any case, 10 percent difference, regarding CPLEX speed, is typically considered to be small, so that it can be ignored.This paper presented an evolutionary algorithm for the fixed charge capacitated multi-commodity network design problem. The proposed methodology evolves a pool of solutions using scatter search principles, and includes an Iterated local search as an improvement method. The latter introduces new cycle-based neighborhood structures, short and long term memory structures for guiding the search, and an efficient perturbation strategy, inspired by ejection chains, to enable the search escape from local optima. An efficient recombination strategy is introduced which dynamically adjusts the preferences for inherited solutions based on the search history.Computational experiments on the benchmark instances of Crainic et al. (2000) show that the proposed CEA is highly competitive compared to state-of-the-art approaches. In particular, CEA is able to reproduce the 13 out of 17 optimum solutions for 17 problem instances previously solved by exact algorithms. CEA was also able to produce three new best solutions, in large-scale problem instances. In general terms, CEA’s performance is strong, thus placing it among the most efficient algorithms for the MCNDP.In terms of further research, a promising research direction is the use of a knowledge base where favorable paths for the commodities would be stored not only for speeding up the algorithm but also for guiding the algorithm towards producing unexplored solution structures. Another direction is to look at decomposition techniques to solve the flow subproblems with a view to reducing the computational times. Finally, it is worthwhile to explore the proposed evolutionary algorithm for solving other variants of the MCNDP or even to other problems that share common features with MCNDP.

@&#CONCLUSIONS@&#
