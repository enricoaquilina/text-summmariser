@&#MAIN-TITLE@&#
Feature selection for support vector machines using Generalized Benders Decomposition

@&#HIGHLIGHTS@&#
We formulated two versions of the feature selection problem.We developed an exact algorithm that solves the feature selection problem.We provided details about the algorithm and its convergence properties.The method is more accurate than benchmark methods.

@&#KEYPHRASES@&#
Data mining,Feature selection,Support vector machines (SVM),Generalized Benders Decomposition,

@&#ABSTRACT@&#
We propose an exact method, based on Generalized Benders Decomposition, to select the best M features during induction. We provide details of the method and highlight some interesting parallels between the technique proposed here and some of those published in the literature. We also propose a relaxation of the problem where selecting too many features is penalized. The original method performs well on a variety of data sets. The relaxation, though competitive, is sensitive to the penalty parameter.

@&#INTRODUCTION@&#
Classification is a common problem faced in many domains. For example, an insurance company might be interested in determining whether an applicant is a high insurance risk or not. A credit card company has to decide whether an applicant is good credit risk or not. Typically one uses a classifier (a rule or a function among many other options) that depends on the observations’ features to make a classification. A classifier might be obtained many ways; for example, a combination of rules of thumb used by experts or a combination of ad hoc rules that have worked in the past. It is also possible to construct one directly from historical data where the labels of observations are already known. Such an approach is called induction. In this work, we concern ourselves with the induction of a classification function from a set of observations while constraining the number of features used.Working with a smaller subset of features is useful for many reasons. For example, classification algorithms tend to overfit if the number of instances is low and the instances have some irrelevant features (i.e., noise). Similar issues can arise when features are correlated or if some have erroneous data. The induced classifier would have better predictive performance without these features. Learning theory suggests that the smallest set of features that can be used to approximate the target function is preferred over all others (see Vapnik, 1998) (a principle also known as Occam’s law of parsimony, Blumer, Ehrenfeucht, Haussler, and Warmuth, 1987). Most businesses would prefer a smaller feature set if collecting data for all possible features is costly. For a credit evaluation, having a simple classifier would mean collecting less data from the applicants which would save time and money for all involved parties.Informally, the feature selection problem (FSP) is defined as finding a subset of features that create classifiers with good predictive power. We use the definition given by Weston et al. (2000): given a problem with n features and a number M ≤ n, and a sample of size ℓ, construct a classifier that has the best predictive power (i.e., low generalization error) using M or fewer features. This is also consistent with the definition given by Kohavi and John (1997). One alternative formulation we will describe penalizes the number of features rather than constrain it to a specific number.We make two contributions to the feature selection problem. First, we offer two alternative formulations and describe an exact procedure to solve them optimally. Second, we show, empirically, that the classifiers obtained using the proposed method have good predictive accuracy.Section 2 discusses earlier research on the feature selection problem. A support vector machine specific formulation of the feature selection problem as well as the generalized Bender’s decomposition method (GBD) is given in Section 3. We report our computational experience in Section 4 and provide conclusions in Section 5.First, we introduce our notation and discuss our assumptions. A training sample S of size ℓ, created i.i.d., is composed of pairs (xi, yi) ∈ ℜn× { − 1, 1}, i = 1, …, ℓ. (xi, yi) represents the ith example in S. x represents an arbitrary vector in ℜnwhen the subscript is omitted. When both subscripts are specified, xijrepresents the jth feature of the ith example. When x is an arbitrary vector, xjrepresents the jth feature. Throughout, we will use i to index examples and j to index features. We will useAPTARABOLDu¯to represent a solution when u is a variable in a formulation. x′ represents the transpose of vector x. e is a vector of ones. We use boldface to represent vectors. All others are scalar quantities.Kohavi and John (1997) define two types of approaches to feature subset selection: filters and wrappers. A wrapper algorithm treats the learning algorithm as a black box and uses it as a function evaluator. A filter algorithm, on the other hand, runs independent of the learning algorithm, selecting a subset based on some predefined criteria. Olafsson and Yang (2005) provide a feature selection method based on partitioning that can be used either as a wrapper or a filter. Later, Saeys, Inza, and Larranaga (2007) proposed embedded feature selection algorithms as a third category. In this category, the feature selection algorithm is embedded in the construction of the classifier rather than treating the classifier as a black box. Several techniques outlined in Kohavi and John (1997) attempt to identify a subset that is deemed best for the learning problem without additional constraints on the number of selected features. For example, Almuallim and Dietterich (1994) use what they call “MIN-FEATURES” bias in selecting sufficient features for a binary formula. Given two formulas that are consistent with the training set, the algorithm selects the formula with less features. Kira and Rendell (1992) describe a procedure called RELIEF. The procedure tries to compute the probabilities for the following events. First pick a randomly selected training example (call it x) and its two nearest neighbors each belonging to different classes (call them xsand xd). Then compute the probability that x and xshave the same value, and x and xdhave different values on attribute A. The idea is, if attribute A is any good the probability for the first event should be much lower than the latter. Later Kononenko (1994) shows that if one picks k neighbors rather than two, the probability estimates can be shown to be highly correlated with the gini index.There are several other feature selection and dimensionality reduction techniques such as principal components, information gain, etc. (Guyon and Elisseeff, 2003). Several methods use distance measures (Piramuthu, 2004) and some are based on meta-heuristics (Pacheco, Casado, and Núñez, 2009; Unler and Murat, 2010). Because we are interested in SVM-based feature selection we will focus only on literature that deals with SVM explicitly. All SVM-based feature selection techniques we have encountered are wrapper, or embedded approaches. There are basically two formulations: one that explicitly constrains the number of features (FSP-M), and one that penalizes the number of features (FSP-λ). Penalization is done either by a penalty parameter on the number of features or by an appropriate penalty function that implicitly penalizes using features. We will refer to all methods that penalize features as FSP-λ regardless how the problem is formulated. We first provide formulations for our versions of the feature selection problem FSP-M and FSP-λ:(FSP-M)minw,b,ξ,σ12∑j=1nσjwj2+C∑i=1ℓξis.t.yi(∑j=1nσjwjxij+b)+ξi≥1∑j=1nσj≤Mξi≥0,σj∈{0,1}and(FSP-λ)minw,b,ξ,σ12∑j=1nσjwj2+C∑i=1ℓξi+λ∑j=1nσjs.t.yi(∑j=1nσjwjxij+b)+ξi≥1ξi≥0,σj∈{0,1}.FSP-λ is a relaxation of FSP-M and for appropriate choices of M and λ, they should produce similar results. When there is no feature selection (i.e.,σ= e and M = n), one recovers the following SVM formulation (Cristianini and Shawe-Taylor, 2000):(1)(SVM)minw,b,ξ12w′w+C∑i=1ℓξis.t.yi(w′xi+b)+ξi≥1,i=1,…,ℓξi≥0.Note here that w is an n-vector, b is a scalar andξis an ℓ-vector.Weston et al. (2000) provide a formulation for FSP-M and solve the following problem.(2)minσ{R2(σ)W2(σ,α*)+λ∑j(σj)p|APTARABOLDσ′e=M,σj≥0}where R(σ) is the radius of the ball containing the data in the feature space induced by a kernel and σ. It is found by solving another optimization problem for a given σ. The second component of (2) is(3)W2(σ,α*)=minα{∑i=1ℓαi−12∑i=1ℓ∑l=1ℓαiαlyiylKσ(xi,xl)|×∑i=1ℓαiyi=0,αi≥0}where Kσ(xi, xl) = K(σ*xi,σ*xl) =ϕ′(σ*xi)ϕ(σ*xl). Those familiar with SVM will recognize that (3) is the dual formulation for SVM when data is separable. Here,αis the dual variable corresponding to (1), and ϕ(.) is a mapping from ℜnto some [usually] higher dimensional space induced by kernel Kσ. Weston et al. (2000) solve (2) using a gradient descent algorithm. They also provide an approximation algorithm to obtain integer solutions. The authors report improved generalization performance both on synthetic and real-life data.Guyon, Weston, Barnhill, and Vapnik (2002) describe a procedure to solve FSP-M, called recursive feature elimination (RFE).The procedure seems to be effective for the gene selection problem they studied. After solving an SVM problem, RFE ranks the squared coefficients of the classifier (i.e.,wj2), and sets the smallest one to zero. Letm={k|wk2≤wj2,∀j≠k}(pick one index if there are multiple minima). A constrained version of SVM is solved again with the constraint wm= 0. This is repeated until the number of features with non-zero coefficients is M. If all attribute values are on a similar scale (e.g., they are standardized or normalized), the smallest coefficient has the least impact on the classification rule and can be eliminated. We note that one has to make sure that the feature values are of the same order of magnitude for this procedure to work. Clearly, gene sequencing data has that property.Existing work that uses the penalty idea does not use the objective function we have proposed in FSP-λ. Typically the margin term and the feature selection penalty is combined into a single penalty function. Bradley and Mangasarian (1998) and Fung and Mangasarian (2004) provide several formulations for FSP-λ using a one-norm SVM (ONSVM) formulation as a base. Bradley and Mangasarian (1998) penalize the use of features by introducing a concave exponential function of the form e − eαv, where v is used to constrain w (a common trick to convert absolute value problems to linear programming problems). In general, the concave minimization produces fewer features than ONSVM, but that comes at the expense of test accuracy. Fung and Mangasarian (2004), on the other hand, convert the dual of a one-norm SVM formulation to an equivalent, asymptotic exterior penalty problem. They show that this problem has feature selection properties and report significant reduction in dimension while keeping accuracy at acceptable levels.Rinaldi and Sciandrone (2010) offer an algorithm to combine the one-norm SVM with concave minimization for feature selection. They first solve a regular SVM, and discard the points that are not classified correctly. In the reduced set, they find a vertex solution of a linear program, where the objective function is of the form∇F(v¯)′v. Here v is the same variable discussed in the context of Bradley and Mangasarian (1998), F is a concave function, andAPTARABOLDv¯is the last solution in hand. If the new w has less components than the incumbent solution, they reduce the feature set accordingly, and move back to solving the SVM using the reduced feature set. They report improvement over RFE on data sets from the UCI machine learning repository. Dunbar, Murray, Cysique, Brew, and Jeyakumar (2010) show how to convert the elastic-net (see Hastie, Tibshirani, and Friedman, 2009, p. 662) into a convex minimization approach for good classification accuracy as well as feature selection. The feature selection capabilities vary depending on how the two objectives are penalized.One of the earliest feature selection formulations using SVM is the one-norm SVM formulation which yields a linear program:(ONSVM)minb,p,q,ξ(p+q)′e+C∑i=1ℓξis.t.yi((p−q)′xi+b)+ξi≥1,i=1,…,ℓp,q,ξ≥0.Its feature selection properties were noted in Bradley and Mangasarian (1998), Aytug, He, and Koehler (2008), and Hastie et al. (2009) among others. Its feature selection capability is simply an artifact of the properties of the optimal solution of a linear program.In general, it is hard to pass judgment whether FSP-M or FSP-λ is more practical. The former requires an explicit constraint on the number of features used whereas the feature selection capabilities of the latter are determined by the form of the penalty used.We will use the formulation by Bradley and Mangasarian (1998), called FSV, and the solution procedure as one of the benchmarks in this study. In addition, we will use RFE (Guyon et al., 2002), and ONSVM as second and third benchmarks.We will consider both formulations of FSP since both versions appear in literature. The solution procedure we will outline next can handle both formulations with minor modifications. In practice though, FSP-M is easier to use than our formulation of FSP-λ. Even though one may not quite know what M value is best, it is possible to experiment with different M values as M is an integer. For example, if one does not have an explicit criterion for specifying M, the smallest M value that does not significantly reduce predictive accuracy might be considered best. One can find one such value using a binary search. Unfortunately, unless there is a specific cost associated with data collection for each feature, it is hard to specify what λ should be in FSP-λ.It is possible to modify FSP-M by changing the constraint on the number of features to a budget constraint such as the one below:∑j=1nβjσj≤B,where βjrepresents the cost of an attribute. A similar modification is also possible in FSP-λ by replacing λ with λj. We will note how these can be handled when we discuss the solution procedure in the next section.Benders decomposition (Benders, 1962) is a well known decomposition technique for solving problems of typemaxx,v{c′x+f(v)|Ax+g(v)≥0,x≥0,v∈V}.Decomposition ideas, including Benders Decomposition, have been used in SVM research, particularly to improve run-time performance in large problems. For example, Trafalis and Ince (2002) use Benders Decomposition to solve the SVM formulation faster than its generic quadratic programming formulation. Our interest lies in the generalization of Benders’ approach known as Generalized Benders Decomposition. Geoffrion (1972) generalized Benders’ procedure to problems of type:maxx,v{f(x,v)|g(x,v)≥0,x∈X,v∈V}where g(x, v) is a vector of constraints. We will show that FSP yields such a problem, and can be solved by GBD. We will focus on the procedural aspects as it relates to our problem, and refer to the paper by Geoffrion when necessary for issues such as convergence, existence of certain structure, etc. The procedure starts with fixing a variablev=v¯and solving the problem (SUB)(SUB)LB=maxx∈X{f(x,APTARABOLDv¯)|g(x,v¯)≥0}using a method that provides the dual solutionAPTARABOLDu¯. Using the dual solution a relaxed master problem (RMP) is setup:(4)(RMP)UB=maxv,v0{v0|v0≤maxx∈X{f(x,v)+APTARABOLDu¯′g(x,v)}}.Clearly, the partial maximization solved while setting up RMP should be easy for this method to work. The partial optimization in the right hand side of (4) is easy to solve for example when f(x, v) and g(x, v) are separable (e.g., f(x, v) = g(x) + h(v)). Even when they are not separable one should be able to solve the partial maximization either analytically or by a fast algorithm. This is what Geoffrion calls Property P and is essential for the success of the algorithm (see Geoffrion, 1972, p. 251). The size of RMP grows as a new constraint (i.e., a cut) is added at each iteration. The procedure stops when UB = LB. We now describe the procedure to solve both versions of FSP.We will call the procedure outlined here GBD-M. For a given feasible partial solution,APTARABOLDσ¯,of FSP-M, we setup (SUB-M):(5)(SUB-M)LB=minw,b,ξ12∑j=1nwj2σ¯j+C∑i=1ℓξis.t.yi(∑j=1nσ¯jwjxij+b)+ξi≥1(6)ξi≥0Let αiand uibe the dual variables associated with constraints (5) and (6) respectively andα¯iandu¯itheir values at optimality. Serendipitously, many algorithms exist that solve SUB-M and also output dual solutions, for example SMO (Platt, 1998). The so-called relaxed master problem to find the next solution forσis:(7)(RMP-M)UB=maxσ0,σσ0s.t.∑j=1nσj≤Mσ0≤maxw,b,ξ≥0−12∑j=1nwj2σj−C∑i=1ℓξi+∑i=1ℓαi¯(yi(∑j=1nσjwjxij+b)+ξi−1)+∑i=1ℓui¯ξiσj∈{0,1}.Manipulating (7) further we get(8)σ0≤maxw,b,ξ≥0∑j=1nσj(−12wj2+wj∑i=1ℓα¯iyixij)+b∑i=1ℓα¯iyi+∑i=1ℓ(u¯i+α¯i−C)ξi−∑i=1ℓα¯i.The RHS of (8) can be simplified further by recognizing that∑i=1ℓα¯iyi=0andu¯i+α¯i−C=0using the KKT conditions of SUB-M, yielding (see Cristianini and Shawe-Taylor, 2000)(9)σ0≤∑j=1nσjmaxwj(−12wj2+wj∑i=1ℓα¯iyixij)−∑i=1ℓα¯i.The interchange of max with the summation from (8) to (9) is justified since the maximum of a finite sum of additively separable concave functions is the sum of the maximum of each. The maximum of−12wj2+wj∑i=1ℓα¯iyixijoccurs atw¯j=∑i=1ℓα¯iyixij. Substitutingw¯jinto (9) we get the following relaxed master problem (RMP-M)(10)UB=maxσ0,σσ0s.t.σ0≤12∑j=1nσj(∑i=1ℓα¯iyixij)2−∑i=1ℓα¯i∑j=1nσj≤Mσj∈{0,1}.Inequality (10) is called an “optimality cut”. At each iteration, we create and add a new version of (10) based on the last dual solution obtained in SUB-M. Setting up (10) required solving a partial maximization over w. Even though we did not have strict separability, the fact that we had σjinteract only with wjallowed us to solve the partial maximization analytically (i.e., property P). Without property P, constructing the cut would have been as hard as solving the original problem.In order to set up SUB-M for the first time we need a feasible solution. Any solution that satisfies(11)∑j=1nσj≤Mcan be used.In general, there is no guarantee that the solution of RMP-M will yield a feasible set in the corresponding SUB-M. However, it is easy to show that SUB-M always has a feasible solution in our formulation (for example, w = 0 is a feasible solution). This fact and moving (11) into RMP-M guarantee that we can find a feasible solution for SUB-M at each iteration. Consequently, we do not need to add feasibility cuts (see Geoffrion, 1972). We note that (11) can be replaced with a more general knapsack constraint such as∑j=1nβjσj≤βwhen, for example, there are real costs associated with collecting data. For clarity, an algorithmic description of GBD is given in Table 1.GBD-M is guaranteed to converge if SUB-M is a concave maximization problem over a convex set, and RMP-M has a finite solution space (Theorem 2.4, Geoffrion, 1972). In our case, SUB-M is a concave maximization problem over a convex set, and the solution space forσis finite sinceσ∈ {0, 1}n, guaranteeing convergence.It is easy to add other constraints into FSP-M. For example, by adding constraints to RMP-M, one can easily specify that if a particular feature is selected others should also be included (or excluded). In addition to being exact, such flexibility makes GBD-M useful for FSP-M.It is worth taking a closer look at RMP-M. We see that RMP-M is incentivized to find support vectors that generate the M largest wjin absolute value (see (10)). One way to do this is to find support vectors that have the highest norms in the subspace induced byσ. This is similar to the subproblem Weston et al. (2000) solve.As in the previous section, we call the procedure outlined in this section GBD-λ. Consider the following subproblem (SUB-λ) obtained from FSP-λ for a givenσ¯(12)(SUB-λ)minw,b,ξ12∑j=1nwj2σ¯j+C∑i=1ℓξi+λ∑j=1nσj¯s.t.yi(∑j=1nσ¯jwjxij+b)+ξi≥1(13)ξi≥0.Let αiand uibe the dual variables associated with constraint (12) and (13) respectively, andα¯iandu¯ithe associated optimal values. As in FSP-M consider the relaxed master problem (RMP-λ):(14)(RMP-λ)maxσ0,σσ0s.t.σ0≤maxw,b,ξ≥0−12∑j=1nwj2σj−C∑i=1ℓξi−λ∑j=1nσj+∑i=1ℓαi¯(yi(∑j=1nσjwjxij+b)+ξi−1)+∑i=1ℓui¯ξiσj∈{0,1}.Regrouping the terms in (14) we haveσ0≤maxw,b,ξ≥0∑j=1nσj(−12wj2+wj∑i=1ℓα¯iyixij−λ)+b∑i=1ℓα¯iui−∑i=1ℓ(u¯i+α¯i−C)ξi−∑i=1ℓα¯i.As we did in Section 3.2, using the KKT conditions of SUB-λ few terms drop and we havemaxσ0,σ∈{0,1}nσ0s.t.σ0≤maxw∑j=1nσj(−12wj2+wj∑i=1ℓα¯iyixij−λ)−∑i=1ℓα¯i.Using the reasoning in Section 3.2, and making the substitutionw¯j=∑i=0ℓα¯iyixij,we get (RMP-λ)(15)maxσ0,σ∈{0,1}nσ0s.t.σ0≤∑j=1nσj[12(∑i=1ℓα¯iyixij)2−λ]−∑i=1ℓα¯i.It is easy to see thatσj={0,ifλ≥12(∑i=1ℓα¯iyixij)21,o/wis an optimal solution of RMP-λ. Basically the net effect of penalizing features is to cut off those features with squared weights less than λ. This is very similar to what Guyon et al. (2002) do with one exception: at each iteration λ determines how many wjcan be eliminated (i.e., it can be zero or more) while Guyon et al. (2002) eliminate a fixed number of features, for example, one feature at a time.As we mentioned earlier the solution procedure stays the same even when we have a different penalty for each feature (i.e., we use λjinstead of λ). Substituting λjin (15) does not change the procedure except that the threshold each wjhas to cross varies from feature to feature.The algorithm to solve FSP-λ is essentially the same as GBD-M. The only difference is that in Step 2 of GBD-M we need to replaceargmaxσ0,σ∈{0,1}n{σ0|σ0≤12∑j=1nσj(∑i=1ℓα¯isyixij)2−∑i=1ℓα¯is,s=0,…,t,∑j=1nσj≤M}withargmaxσ0,σ∈{0,1}n{σ0|σ0≤∑j=1nσj[12(∑i=1ℓα¯isyixij)2−λ]−∑i=1ℓα¯is,s=0,…,t}.One downside of solving integer problems is that they are known to be NP-hard. As the number of constraints in RMP (of both formulations) increases it will take longer to find a solution. However, at each iteration we obtain a valid classifier one of which might have acceptable predictive accuracy. Consequently, one can terminate the procedure based on some secondary criteria long before GBD converges. For example, one could test the performance of each classifier on a validation set and stop when there is no improvement. Alternatively, one can run GBD for a fixed number of iterations and pick the classifier with the best validation error.We tested our approach on three sets of data to determine if the computational burden of implementing an iterative technique is justified by gains in predictive accuracy. The first and second sets, which we created, have 90 and 25 data set triplets (training, validation and testing) respectively. The third set has 25 data sets, several of which are downloaded from the Machine Learning Repository at the University of California, Irvine (Frank and Asuncion, 2010), and one from Guyon et al. (2002). Two of the data sets, GINA and SYLVA, are downloaded from http://www.agnostic.inf.ethz.ch(see 2007 International Joint Conference on Neural Networks competition).Data sets one and two were created such that it is possible to separate the two classes in a lower dimensional space (i.e., M < n). We first created ℓ random observations. Then each observation was normalized to fit in a ball of radius one. We then created a hyperplane with M nonzero random coefficients, and each observation was assigned a label using this hyperplane (true hyperplane). In data set one, we varied the number of attributes (n ∈ {20, 50, 100}) as well as the training set size (ℓ ∈ {500, 1000, 5000}). For each configuration we created 10 random instances, totaling 90 problems. In data set two, we kept ℓ and n constants at 1000 and 800 respectively but varied M, M ∈ {20, 100, 200, 400, 600}. For each M, we created five random instances totaling 25 problems. For each training data set, we created two additional data sets using the true hyperplane: one for validation and one for testing.As we highlighted before, finding an appropriate penalty for FSP-λ is difficult. So we use it as a benchmark only in the third set where we find a reasonable λ by trial and error. We benchmark our results for FSP-M and FSP-λ against RFE by Guyon et al. (2002), and FSV by Bradley and Mangasarian (1998) implemented in SPIDER (Weston, Elisseef, Baklr, and Sinz, 2006), as well as the one-norm SVM formulation (ONSVM) discussed in Section 2. We also included a filter method, based on information gain, implemented in WEKA (called InfoGainAttributeEval) (Hall et al., 2009). This filter ranks the attributes based on the change in entropy of y given the information provided by attribute j (i.e., H(y) − H(y|xj), where H is entropy.In methods that require tuning, we employed a search over their parameters. For example, for GBD, RFE and ONSVM, we employed a basic search suggested in Chang and Lin (2011) by varying C from 2−2 to 25 (pilot runs showed that smaller C values did not work well). For FSV, we varied its penalty parameter, λ, between zero and one. Since the results did not vary, we reported the results for λ = 1. Because we know the actual number of features in the first data set, this experiment provides us with an easy comparison among all methods. Theoretically, all methods should be able to find the true M features and produce a separating hyperplane with good predictive power. The main difference between the one-norm and other methods we tested is the fact that we have no control over how many features one-norm SVM uses to construct a classifier. All SVM formulations (GBD, ONSVM, RFE) were solved using CPLEX 10.2 (2007) on a 2.3 gigahertz six core machine with 32 gigabytes of memory.For all methods that required tuning we found the classifier that had the lowest validation error, and tested that classifier on the test sets. Only the test errors are reported.In Table 2, we report average absolute and percentage differences in test errors (TE) by GBD versus others. The column labeled GBD-M reports the actual average error rate achieved by GBD-M (averaged over ten instances). The rest of the columns report the average absolute difference TEGBD − TExunder Abs and average percentage difference100TEGBD−TExTEGBDunder Prcnt (averaged over ten instances). Bold numbers are category averages over 30 instances based on sample size ℓ. The last row displays overall averages based on 90 instances.In general, GBD-M has better overall test accuracy (see Total row in Table 2). However, for larger data sets GBD-M error rate is slightly worse than RFE and FSV, as well as ONSVM. The good news though is that, GBD-M is never worse, and actually more often better for problems with large number of attributes. This behavior is more important for FSP. Out of the 90 problems, GBD-M performed better than RFE on 32, and worse on 23. Similarly, GBD-M performed better than FSV 45 times, and worse 39 times. We note that the percent differences favor GBD-M (since the error rates are very small even small differences can appear as large percentage differences).We did not report feature usage explicitly but all methods use significantly less number of features relative to ONSVM. GBD-M used less features than RFE and FSV in a few instances, but the differences are negligible. In Table 3, we report the overlap between the induced classifiers and the true classifiers. We define overlap between two classifiers as the number of features at which both classifiers either have zero weights, or are both non-zero (i.e., n minus the Hamming distance). A value less than n shows that the classifier either used an irrelevant feature, and/or did not use a relevant one. The numbers in the table display average overlap (over 10 instances per cell), and the integers in parentheses show how many exact matches there are for each n, ℓ combination. Bold numbers are averages per n. RFE seems to do the best, closely followed by GBD-M. FSV is slightly worse in this measure. However, when we look at the inner product of the normalized classifiers and the true classifier (i.e.,APTARABOLDw^′w,whereAPTARABOLDw^is the induced classifier), we see that GBD-M has the highest value at 0.9966, followed by RFE at 0.9959, and then by ONSVM at 0.9939, and FSV at 0.9928. This shows that mismatched feature weights must be close to zero for most classifiers (i.e., zero in a location where it probably should be a small non-zero value or a nonzero value close to zero when it should have been zero).Fig. 1reports the differenceTEGBD-M−TEx(negative values favor GBD-M). We see that the performance of GBD-M improves as the percentage of features selected increases. Each bin in Fig. 1 represents a partition of the ratioMn. Bin one includesMnvalues between 0 and 0.2. Bin two includes values between 0.2 and 0.4, and so on. Because M was selected randomly given n, each bin includes roughly 20 percent of the data (18 observations). Knapsack type problems tend to be harder to solve for cases where the knapsack size is about half the size of the total weights of items. In this problem, that ratio corresponds to Bin 3. Fig. 1 suggests that GBD-M does well on those problems. It is, however, harder to explain why GBD-M has a harder time than other methods when it needs to select only a few features (Bin 1).Convergence of GBD-M (and GBD-λ) can take a long time for large n, even though GBD-M is guaranteed to converge in finite steps. To finish the experiments in a reasonable amount of time, we stopped both procedures after 250 iterations (i.e., 250 cuts) if they have not converged before then. Roughly half the problems with n = 20 converged well before 250 cuts. Very few problems with n > 20 converged. Interestingly, even though we stop the runs prematurely for larger problems, the test accuracy does not seem to suffer. Upon closer inspection, we also discovered that the best classifier in terms of test accuracy was found earlier, sometimes long before convergence, which suggests that even if GBD-M is terminated at an arbitrary number of iterations the performance may not suffer drastically. RFE has to iterate exactly n − M + 1 times. So its run time performance depends on how many features need to be eliminated. We observed that RFE typically ran in order of minutes with the exception of large data sets whereMnwas small. GBD-M, as expected, runs in the order of minutes for small problems and hours for large problems. FSV’s run time was in the order of minutes for small problems and hours for larger ones.Data set two was created to understand the behavior of GBD-M in terms of time and solution quality as we vary M. The results are consistent with data set one. GBD-M is competitive for all values of M but requires more time than benchmarks. Here we only used RFE and ONSVM as benchmarks since they were developed using the same codebase. Fig. 2shows that GBD-M outperforms RFE almost everywhere except M = 20. As expected, RFE spends significantly more time than GBD-M for small M as it needs to solve n − M + 1 SVM formulations (Fig. 3). However, consistent with knapsack problems, the computational burden on GBD-M increases drastically for problems whereMn=0.5,M = 400 in this data set. We excluded ONSVM from Fig. 3 since its computational burden is negligible in comparison to GBD-M and RFE.Some of the data sets in this experiment were edited so that there are no missing values and all categorical variables are converted to binary variables. To create a fair comparison, we first solved the one-norm SVM formulation and recorded the number of nonzero coefficients in the solution (i.e., M). We then solved FSP for each data set using its corresponding M. We partitioned the data sets into three: training (60 percent), validation (20 percent) and test (20 percent), with stratified sampling. For each method, we pick the classifier with the best validation accuracy after tuning and report that classifier’s test accuracy.In Table 4, we first report the error rate for GBD-M under column GBD-M in addition to M, n, ℓ. The remaining columns report the absolute difference TEGBD − TExfor each benchmark classifier. Cells marked with ‘–’ show that the method did not create a meaningful classifier (FSV failed several times and GBD-λ either used too many features or too few in several data sets). In this experiment the contrast is clearer in the sense that using roughly the same number of features GBD-M creates a better classifier on seven instances against RFE, 17 against FSV and 11 against ONSVM. It performed worse than FSV and RFE once, and twice against ONSVM. Because M was selected to be equal to the number of features used by ONSVM the differences in number of features used among methods are negligible. In a few instances GBD-M used one less attribute than ONSVM.As we discussed earlier, GBD-λ has very erratic behavior in terms of how many features it cuts out. In many problems it used significantly less features than others (even after trying several different λ values). In addition, in four problems, where others had no issues, GBD-λ had poor accuracy which reduced its overall accuracy significantly. In the remaining problems it is competitive. For example, it achieved the lowest error rate in GINA using only 538 features.We report the run-times of GBD-M, RFE and ONSVM in Table 5. Clearly, GBD-M is slower than RFE in most instances and it has a hard time when the number of features is roughly half the number of attributes. For example, we had to cap the run-time when solving GINA with GBD-M as it slowed down drastically after generating 40–50 cuts. As we discussed earlier, problems withMnratio around 0.5 tend to be harder problems for GBD-M in terms of its run-time performance. Sylva and Leukemia data sets posed no issues even though they are relatively large data sets.We presented an exact method for the feature selection problem for SVM based on the Generalized Benders Decomposition method. The solution procedure also provides insights into the procedures used by Guyon et al. (2002) and Weston et al. (2000). We demonstrated its utility on a large number of synthetic data sets as well as benchmark data sets. The method is quite useful for inducing classifiers. It can be generalized to cases where the feature selection constraints need not simply be the number of features used. Budget constraints as well as constraints representing interdependence among the features can easily be added to the procedure.Since RMP is an integer problem, scalability is an issue. We observed a drastic increase in run-time for problems where M is roughly half of n. However, we also observed that even in cases where the procedure was stopped before convergence we were able to obtain classifiers that performed better than the benchmarks.It is possible to extend this work in several dimensions. We employed a general purpose solver to solve RMP and SUB. It is possible to exploit the iterative nature of GBD and use the previous solution as a starting seed for the current iteration. Another idea is to solve a relaxation of RMP and find an effective rounding procedure. These two approaches may improve the computational performance of GBD.In this work, we did not explore how one can set λ values to obtain a good classifier. Perhaps the solution to the basic SVM problem can provide a starting point for choosing a value for λ. This needs to be explored as GBD-λ run-time requirements are significantly less than GBD-M.One way to think of GBD is as follows: at each iteration it finds a set of support vectors in the sub problem SUB and then RMP is used to select a subspace that best fits these support vectors. Some of the concave minimization ideas can be used here to select the subspace, given the support vectors.

@&#CONCLUSIONS@&#
