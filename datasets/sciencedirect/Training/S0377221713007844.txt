@&#MAIN-TITLE@&#
A modified variable neighborhood search for the discrete ordered median problem

@&#HIGHLIGHTS@&#
Location analysis.Discrete Ordered Median location problem.Variable neighbourhood metaheuristic.

@&#KEYPHRASES@&#
Discrete ordered median problem,Variable neighborhood search,Discrete facility location,

@&#ABSTRACT@&#
This paper presents a modified Variable Neighborhood Search (VNS) heuristic algorithm for solving the Discrete Ordered Median Problem (DOMP). This heuristic is based on new neighborhoods’ structures that allow an efficient encoding of the solutions of the DOMP avoiding sorting in the evaluation of the objective function at each considered solution. The algorithm is based on a data structure, computed in preprocessing, that organizes the minimal necessary information to update and evaluate solutions in linear time without sorting. In order to investigate the performance, the new algorithm is compared with other heuristic algorithms previously available in the literature for solving DOMP. We report on some computational experiments based on the well-known N-median instances of the ORLIB with up to 900 nodes. The obtained results are comparable or superior to existing algorithms in the literature, both in running times and number of best solutions found.

@&#INTRODUCTION@&#
Location analysis is a very active topic within the Operations Research community. It has given rise to a number of nowadays standard optimization problems some of which are in the core of modern mathematical programming. One of its more important branches is Discrete Location. Witnesses of its importance are a number of survey articles and textbooks that collect a large number of references on methodological results and applications, see e.g. Daskin (1995), Drezner and Hamacher (2002), Mirchandani and Francis (1990), Nickel and Puerto (2005) and references therein. Roughly speaking, Discrete Location problems typically involve a finite set of sites at which facilities can be located, and a finite set of clients, whose demands have to be satisfied from the facilities.An important aspect of a location model is the right choice of the objective function and in most classical location models the objective function is the main differentiator. Therefore, a great variety of objective functions has been considered.Discrete Ordered Median Problem was introduced to provide a unifying way to model many location models see e.g. Nickel (2001), Boland, Domínguez-Marín, Nickel, and Puerto (2006) and Nickel and Puerto (2005). It has been recognized as a powerful tool from a modeling point of view because it generalizes the most popular objective functions in the literature of location analysis and it also allows to distinguish the different roles played by the different parties in a supply chain network. The correct identification of the different roles played by the agents participating in logistics models has led to describe and analyze new types of distribution patterns, namely customer-oriented, supplier-oriented or third party logistics provider-oriented, see (Kalcsics, Nickel, Puerto, & Rodríguez-Chía, 2010a, 2010b; Puerto, Ramos, & Rodríguez-Chía, 2011). The objective function of DOMP applies a penalty to the cost of supplying a client which is dependent on the position of that cost relative to the costs of supplying the remaining clients. Therefore, it increases the flexibility in the modeling phase through rank dependent compensation factors which allow to model which party is the driving force in a supply chain.In the last years, a number of algorithms have been developed to attack the resolution of DOMP (see Boland et al., 2006; Kalcsics et al., 2010a, 2010b; Marín, Nickel, Puerto, & Velten, 2009; Nickel, 2001; Nickel & Puerto, 1999 & Rodríguez-Chía et al., Rodríguez-Chía, Nickel, Puerto, & Fernández, 2000). The first exact method was a branch and bound (B& B) algorithm presented in Boland et al. (2006). Later, a more specialized formulation was introduced in Marín et al. (2009) giving rise to a more efficient branch and cut (B&Cut) algorithm. Finally, in Marín, Nickel, and Velten (2010) the authors develop a new formulation that has allowed to solve larger size instances to optimality. The reader is referred to Marín et al. (2010) for a comprehensive literature review on exact methods for DOMP. More recently, the capacitated version of these problems has been also considered in Kalcsics et al., 2010a, 2010b where first attempts to solve capacitated versions of DOMP have been developed. However, none of these approaches leads to satisfactory results concerning the solution times of even medium size instances. In spite of that, the literature on heuristic algorithms for this family of problems is rather reduced. Domínguez-Marín, Nickel, Hansen, and Mladenović (2005) present two heuristic approaches, a Variable Neighborhood Search (VNS) and a genetic algorithm, for solving DOMP, whereas Stanimirovic, Kratica, and Dugosija (2007) proposes two Evolutionary Programs (with two different encodings: binary in HGA1 and integer in HGA2) based on new encodings of the solution for better evaluation of the objective function that improve the heuristic algorithms in Domínguez-Marín et al. (2005). In both papers, the authors use ORLIB N-median instances with up to 900 nodes for testing their results.The goal of this paper is to develop a modified VNS heuristic for DOMP which takes some advantage of the new available knowledge on the structure of this problem. Specifically, we use refined neighborhoods’ structures that favor faster improvement of the objective function in the local search phase. Moreover, we apply an efficient encoding of solutions avoiding sorting in each objective function evaluation. This specific encoding allows to obtain a faster implementation than the one in Domínguez-Marín et al. (2005) for the VNS paradigm to this family of problems. In addition, it provides results that compare with the best heuristic algorithms known so far for the DOMP (Stanimirovic et al., 2007). To this end, the paper is organized as follows. Section 2 is devoted to recall the DOMP. Section 3 describes our modified VNS algorithm for the DOMP. There, we describe our neighborhoods’ structures and the different elements that allow a better encoding of solutions avoiding sorting in each evaluation of the objective function. The presentation of the algorithm is modular. Thus, we present the different functions that are used in the algorithm, namely, Initial Solution, Variable Neighborhood Descent, Shaking and finally the actual algorithm Modified Variable Neighborhood Search. In Section 4, we report our computational results based on 8 problem types, previously considered in the literature, and on data taken from the benchmark instances of the ORLIB N-median library by Beasley (1990). The paper ends with some conclusions on the proposed algorithm and on the comparisons with previously available heuristics for the considered problem.In order to introduce the Discrete Ordered Median Problem (DOMP) formally, we define a set V of M discrete locations. These locations represent clients as well as potential plant locations.Moreover, let C=[cij] (i, j=1,…, M) be a non–negative M×M cost matrix, whereas cijdenotes the cost of satisfying the total demand of client i from a plant at location j. Thereby, we assume that cii=0 (∀ i=1,…, M). This property of C is called free self–service (FSS). For the sake of readability, we denote by G the number of different values assumed by the elements of matrix C. Moreover, we shall refer to the sorted values of C by c(j), j=1,…, G.Let N with 1⩽N⩽M−1 be the number of new plants which have to be located at the candidate sites. Then, the costs for satisfying the demand of the respective clients, given a feasible solution X⊂V with ∣X∣=N, can be represented by the following vectorc(X)≔(c1(X),…,cM(X))withci(X)=minj∈X{cij}∀i∈V.However, due to the desired flexibility, c(X) cannot directly be used to define the objective function of the DOMP. Instead, consider a permutation σXon {1,…,M} for which the inequalitiescσX(1)(X)⩽cσX(2)(X)⩽⋯⩽cσX(M)(X)hold. Using this permutation we define the sorted cost vector c⩽(X) corresponding to a feasible solution X as follows:c⩽(X)≔(cσX(1)(X),…,cσX(M)(X))or for shortc⩽(X)≔(c(1)(X),…,c(M)(X)).Furthermore, let λ=(λ1,…,λM) be an M-dimensional vector, with λi⩾0 (∀i=1,…,M) representing a weight on the ith lowest component of the cost vector c(X). Using the notation explained above the DOMP is defined as:(1)minX⊂V|X|=Nfλ(X)=∑i=1Mc(i)(X)·λi.The function fλ(X) is called ordered median function. An example illustrating the structure of the DOMP and the calculation of the ordered median function is given below.Example 2.1Let V={1,…,5} and assume that N=2 plants have to be located. Moreover, let the cost matrix C be as follows:C=0453310622730317350513230.Clearly G=8 and c(1)=0, c(2)=1, c(3)=2, c(4)=3, c(5)=4, c(6)=5, c(7)=6, c(8)=7.With λ=(0,0,1,1,0), an optimal solution of this problem instance is X={1,4}. Therefore, the demand of locations 1, 2 and 5 are satisfied by plant 1 whereas the demand of the remaining locations are satisfied by plant 4. Hence, c(X)=(0,1,3,0,1), c⩽(X)=(0,0,1,1,3) andfλ(X)=0·0+0·0+1·1+1·1+0·3=2.Note that by using appropriate values for λ, nearly all classical discrete facility location problems can be modeled by the above definition. In addition, a wide range of new and interesting problems can be derived. Some of these modeling possibilities are given in Table 1. For a more extensive list the interested reader is referred to Domínguez-Marín (2003) and Nickel and Puerto (2005).Since the DOMP contains, as a special instance, the discrete N-median problem which isNP-hard (see Kariv & Hakimi, 1979) the DOMP is alsoNP-hard. In spite of that, as mentioned in the Introduction, different integer linear programming formulations have also been proposed for DOMP which can solve to optimality medium size instances up to 100 nodes (Marín et al., 2009). Nevertheless, for larger sizes the exact approaches do not perform well. In the following section we propose a modified VNS heuristic for DOMP.The basic idea of VNS is to implement a systematic change of neighborhood within a local search algorithm (see Hansen & Mladenović (1997, 2001, 2001, 2003), Hansen, Mladenović, & Moreno-Pérez (2010) and Hansen, Mladenović, & Pérez-Brito (2001)). Exploration of these neighborhoods can be done in two ways. The first one consists of systematically exploring the small neighborhoods, i.e. those closest to the current solution, until a better solution is found. The second one consists of partially exploring the large neighborhoods, i.e., those far from the current solution, by drawing a solution at random from them and beginning a (variable neighborhood) local search from there. The algorithm remains in the same solution until a better solution is found and then jumps there. These algorithms rank the neighborhoods to be explored in such a way that they are increasingly far from the current solution. We may view VNS as a way of escaping local optima, i.e., a “shaking” process, where movement to a neighborhood further from the current solution corresponds to a harder shake. In contrast to random restart, VNS allows a controlled increase in the level of the shake.The standard application of VNS to the N-median problem proposed by Hansen and Mladenović (1997) encodes a solution by the indices of open facilities. Then, it needs to identify the allocation of demand points to open facilities to get the vector of costs. For the N-median problem updating the value of the objective function can be done step by step. As mentioned above N-median is a particular case of DOMP. For this reason Domínguez-Marín et al. (2005) applied the above structure to develop the first VNS algorithm for the DOMP. However, computation of the objective function value is much harder for DOMP than for N-median. Indeed, a major difficulty in the application of the above encoding to DOMP is to compute the variation between the objective function values when an interchange between two facilities is performed. This is so because one is forced to update and sort the whole cost vector after this interchange takes place. As a consequence, the complexity of this procedure applied to DOMP is higher due to this extra sorting which has to be done at each objective function calculation.Our approach is different. First of all, we refine an already used family of neighborhoods’ structures in a way that they favor faster local improvements in the objective function. This is improvement is attained because they bound, from above, the cost cijof allocations that are permitted in the considered neighborhood. This helps to speed up the local search phase. Moreover, the encoding is different.Let us denote by S={(X,a): X is a set of N potential locations of the new facilities and a is an allocation function a(i)∈X for all i∈{1,…,M}} a solution space of the problem. The solutions in S admit different neighborhoods’ structures depending on the number of new facilities k=1,…, kmax, (kmax⩽N) from the current solution that are replaced in the new solution; and on the properties of the allocation function a that is applied. We consider allocation functions ar, r∈{c(1),…,c(G)}, such that for any setXk′of new facilities to be included in the current solution X, satisfy ar(i)≠j for allj∈Xk′such that cij>r. (New allocations at a cost greater than r are forbidden.)We denote byNkr, k∈{1,…,kmax}, r=0,…, rmax(kmax⩽N, rmax⩽c(G)) the set of such neighborhoods’ structures and byNkr(X)the set of solutions defining the neighborhoodNkrof a current solution X. More formally(2)X1∈Nkr(X2)⇔|X1⧹X2|=kandar(i)≠j∀j∈X1⧹X2suchthatcij>r.Note that the cardinality ofNkr(X)is of the order of O(Nk(M−N)kMN), since k out of N facilities are dropped, k out of M−N added into the solution and different allocations can be done. Finally, we observe that the union of the setsNkr(X)together with X, is S.We represent solutions by open facilities but instead of using directly the vector of cost allocations, we maintain a list,L, with minimal required information to evaluate the objective function without sorting after each solution update. See (Mladenović, Labbé, & Hansen, 2003 and Stanimirovic et al., 2007) for related encodings valid for N-center and N-ordered median problems, respectively.Associated with each column j of the cost matrix, we define a listL(j). This list contains as many entries as the number of different values that appear in the column j of the cost matrix and it is sorted in increasing order of these values. Each record in the listL(j)keeps information on the facility, ‘F’, whose allocation costs are given in column j, a realizable allocation cost, ‘c(.)’, of serving from facility at F and the list of pointers ‘R’ to the rows where each cost in the column j appear in the cost matrix. This information describes any instance of a DOMP. In addition, each record has two more operational fields used for evaluation. The first one, called used, will keep track of the number of times that a demand point is served with the corresponding cost in the current solution whereas the second one will maintain the partial evaluation of the objective function (i.e., the cumulated cost) fλ. (See Table 2for an illustrative example of listsL(j)in Example 3.1.) Note that we have M lists that correspond to the M columns of the matrix C.The listLis computed once in the preprocessing phase and it is the result of a merge-sort, by the field c(.), of the sorted listsL(j), j=1,…, M. (Table 3shows the listLin Example 3.1.)From the listLwe generate TableT(L)which has M rows. (Table 4shows the TableT(L)of Example 3.1.) Row j has as many elements as|L(j)|. The i−th entry of this row refers toLi(j), the i−th element of the listL(j). It contains the pair (k,Ri) where k is the position ofLi(j)inLand Riis the field R inLi(j). (Recall that Riis the list of rows in the j−th column of C whose values, c·j, are equal to the field c(.) ofLi(j)).Now, we can easily evaluate any solution of DOMP. Indeed, consider the solution X={Fi1,…,FiN}. Let L(X) be the list that results from the merge-sort of the rows i1,…, iN of TableT(L)in increasing order of k and without repetitions of any element in the lists R of the previously chosen pairs (k,R). Thus, if the next element to be merged in a partially built list L(X) is(k¯,R‾)withR‾=R1‾⋃R2‾and the elements inR1‾were already covered by previous elements inserted in L(X), then we only add the pair(k‾,R2‾). (Expression (5) shows L({2,4}) in Example 3.1.)The next example illustrates the data structure used in our algorithm.Example 3.1Consider the cost matrix introduced in Example 2.1. First, we have computed the listsL(j), j=1,…, 5. (See Table 2.) For instance, the third element in the third row, namely [3,(1,4),5,•,•]t, comes from the third column of the cost matrix C and is built in the following way:•F=3. It indicates that this record (vector) refers to the third column of the cost matrix C.R=(1,4). The allocation costs c13=c43 assume the value c(.)=5. This field of R informs that the cost to which this element refers to, namely 5 (the sixth component of the vector of sorted costs), appears twice in column 3 of C, namely in rows 1 and 4.c(.)=5. The cost matrix to which we refer to is 5.From the listsL(j), it is easy to obtain the main listLby simply merging the listsL(j)in non-decreasing value of their fields c(.). Applying this merging to the elements in Table 2 results in the listLin Table 3.Once we have obtained the main listL, we generate the TableT(L)(see Table 4). Recall that each element is a pair (recordnumber,fieldR). For instance (15,(1,4)), in the third row, means that the cost 5 that appears in the record number 15 ofLcomes from rows 1 and 4.All the above is done only once in the preprocessing phase.In order to evaluate the objective function we use the following recursion. Assume that ∣L(X)∣=ℓ. Thus, we will consider, ET(L(X)), the evaluation table of L(X) that has ℓ columns. (See Table 6 in Example 3.1, ℓ=4.) We identify the columns in that table by their consecutive indices. Then, we refer to the elements of each column by their names and the superscript of the column, namely Fj,c(.)j, usedj,fλjpoint to the corresponding fields of the jth column in the evaluation table. For instance, in Example 3.1, j=2 in Table 6 refers to the second column which in fact comes from the fourth element inL. Then, F2=4 andc(.)2=0. Moreover, let us defineused‾j, j=0,…, ℓ, as the accumulated ‘used’ field up to the jth column of ET(L(X)). By convention we assume thatused‾0=0. For instance, in Table 6 of Example 3.1,used‾3=used1+used2+used3=1+1+2=4andused‾4=5.Now, we can properly state the formula offλjfor j=0,…, ℓ. First of all,(3)fλ0=0,(4)fλj=fλj-1+∑k=used‾j-1+1used‾jλkc(.)j.As mentioned above, from formula (4), we see that the evaluation of fλis linear, once the evaluation Table L(X) is obtained. (No sorting is needed.)Example 3.1ContinuationWith the information previously obtained, we can easily obtain the evaluation of any solution, for instance {2,4}, for λ=(0,0,1,1,0), namely f(0,0,1,1,0)({2,4}). First, we obtain L({2,4}) by merging rows 2 and 4 of TableT(L)up to an accumulated frequency of 5 different elements in the second field R. (The reader may observe that in each row of the above table, the cardinality of the union of the elements of the second field of the pairs in any row ofT(L)is always M.)(5)L({2,4})=[(2,(2)),(4,(4)),(11,(3,5)),(12,(1))].Table 6 shows ET(L({2,4})). Bold columns in Table 5induce the evaluation table in Table 6. The objective value of the solution {2,4} is given by the last field fλof the last record in the evaluation table ET(L({2,4})). In this case the last element of L({2,4}) is (12,(1)) which means that we have to consider the record number 12 in the listLbut only for client 1 (see Table 6).Finally, to compute the value of the objective function f(0,0,1,1,0)({2,4}) one simply needs to process, using the recursive formula (4), the four records shown in Table 6. The value of the objective function for the solution {2,4} is 6.Note that changing the solution from {2,4} to {1,4} can be done by merging N rows ofT(L). First, we compute the list L({1,4}) by removing from L({2,4}) the elements that correspond to the facility F2 (row number 2 ofT(L)) and then we insert the elements from row number 1 inT(L), up to a frequency of 5 (using that repetitions of elements in R are not allowed). The resulting list is:L({1,4})=[(1,(1)),(4,(4)),(6,(2,5)),(12,(3))].Again, the evaluation of the objective function f(0,0,1,1,0)({1,4}) simply needs to process the following four records shown in Table 7.With the recursive formula (4), the value of the objective function for the solution {1,4} is 2.In the following, we describe the modified version of the VNS algorithm for the DOMP. In the presentation we follow a modular description. We will describe, in subsections, the different subroutines that we use in the main algorithm. Then, we present its pseudocode. In the description of the heuristic, we use the following notation:•xcur: current solution (new facilities);fcur: current incumbent objective function value;goin: index set of the facilities to be inserted in the current solution;goout: index set of the facilities to be deleted from the current solution;g∗: current objective function value obtained in local procedures.In the following four subsections we describe the components of our modified VNS heuristic for DOMP.In this subsection we present the approach that we have followed to construct a solution to initialize our VNS heuristic. In order to choose the initial solution, we have compared several strategies on medium size instances of the problem (pmed16–pmed25) to conclude which one should be considered in the overall computational study. We have tested the following methods:1.Pure deterministic. Taking as initial solutions those facilities numbered {1,…,N} in the entire list of facilities.Pure randomization. Choosing N facilities at random.A Greedy algorithm. It works as follows: The first chosen facility is the one that minimizes the ordered median objective function assuming that we are interested in the 1-facility case. After that, in every step we choose the facility with minimal objective function value taking into account the facilities already selected. This procedure terminates as soon as N facilities are chosen.A Random-Greedy construction (RG-construction) (see e.g. Resende & González-Velarde, 2003). Our construction works by initially choosing ⌊ N/2⌋ facilities at random and then completing them until N, applying the greedy approach described above.We have tested the above methods in order to find the one with the best performance for our algorithm. Some of them result in very poor performance (as for instance the pure deterministic) and therefore were discarded. The remaining methods were combined in order to have the best possible initial solution in any of the ten executions that we run our heuristic for each instance. In order to do that, we tested several initialization strategies on medium size instances (pmed16–pmed25) for all problem types and a fixed upper limit of 300seconds of CPU time. We report in Table 8the performance of the three best strategies found: 10 executions with greedy initial solution (10 G), 1 execution greedy and 9 random (1G-9R) and 10 executions with initial solution from the RG-construction (10 RG).The best performance was obtained by the RG-construction for all instances and problem types and therefore, we decided to apply it as the initialization procedure within our heuristic (see Table 8).This procedure shows how to modify/update the objective function when a given solution is updated. In our descent phase we search deterministically within the neighborhoods’ structures given by the family Nkrdescribed above. We use only two different k-interchange neighborhoods’ structures Nkrcorresponding to r∈{⌈c(G)/2⌉,c(G)}. This means that when searching within the first neighborhood structure(Nk⌈c(G)/2⌉), we only allow allocation costs that are less than or equal to ⌈c(G)/2⌉, whereas in the second neighborhood structure(Nkc(G))we allow any allocation cost. The maximal cardinalitykmax′of the set of facilities to be added to a partial solution in our VND is a parameter that must be chosen in the implementation. Note that this parameterkmax′determines the neighborhood structure Nkrused in the search phase. First of all, we have used two approaches to choose the set, goin, of new facilities to be added to the current solution. The first one is a First improvement scheme that looks, in the neighborhood of the current solution, only for the first improvement found. The second one is a Best improvement that looks for new solutions amongst all the neighbors of the current solution in order to choose the best one. These two approaches to add facilities to the current solution have been combined with the different strategies, used in the routine goout, to select the facilities to be removed from the current solution. (The different combinations can be seen in Table 9.)Since, we have already described the two methods for choosing the set goin, we will assume, from now on, that the set of facilities that is added to the current solution is known.We have tested two methods for choosing the set goout, of facilities to be removed from a solution. The first one does a non-guided search on the current neighborhood within a pre-specified time limit and the second one selects these facilities according to a list of preferencesP, that provides a heuristic order to choose solutions within the neighborhoods’ structures. This list is computed only once at the preprocessing phase.The rationale of this heuristic order is based on the following construction. For each feasible facility j we compute the value fλ({j}), i.e., the solution of the 1-ordered median problem provided that the only open facility is j. The greater the value fλ, the lower the performance that one expects for that facility in any solution. Therefore, we set the list of preferences in decreasing order with respect to these values. Algorithms 3.1 and 3.2 describe the two methods to choose the set goout, namely by the Best update (non-guided) or by the guided list of preferences, Guided update, respectively.Algorithm 3.1Best update (c,λ,xcur,fcur,goin,M,N,k,P,tmax, var g∗, var goout∗)Finally, based on the different schemes to choose the sets goin and goout, the pseudocode of the local search VND routine that we have implemented within our VNS heuristic is described in the Algorithm 3.3.In the pseudocode of Algorithm 3.3, we show a new version of VND, where goin is actually a set, i.e., in an exhaustive search it would enumerate all possible subsets of facilities with cardinality k. Nevertheless, in our implementation we use the strategy of stopping the exploration based on the number of unsuccessful attempts at improvement. Specifically, we terminate the exploration of this neighborhood if this value is greater than N.Algorithm 3.2Guided update (c,λ,xcur,fcur,goin,M,N,k,P, tmax, var g∗, var goout∗)VND (c,c(G),λ,xcur,fcur,goin,M,N,kmax′,P, var f′, varxcur′)Table 9 reports the computational results of the different combinations of strategies for choosing the sets goin and goout in the VND routine. We have tested the approaches First/Best for goin and Guided/Best for goout, for the 40 benchmark instances considered (ORLIB N-median instances) and on problem types T1 and T4 (see Section 4 for their descriptions). The choices, First/Best, mean that selection for goin is based on first improvement/exhaustive search, respectively. On the other hand, the options Guided/Best, mean that selection for goout is based on the preference list described in Section 3.3 (see Algorithm 3.2) or on the best improvement (see Algorithm 3.1).Table 9 has two blocks. The first one reports on the results for the problem T1 and the second one for problem T4 (see Section 4). We have run the VND routine with a time limit of 30 seconds for the four possible combinations of IN-OUT. Each block reports the average objective value, Avrg. Obj.; the average gap, Avrg.Gap, and the number of optimal solutions found, Num.Opt. for the 40 considered instances of N-median problems in ORLIB. From these results, we conclude that the best combinations for the implementation of goin–goout are Best-Guided and Best-Best (see Table 9).In our implementation of the VNS, we have tested two shaking operators. The first one consists of choosing at random the set goin and selecting, according to a priority list, the set goout; whereas the second one chooses at random, both the sets goin and goout. Our computational experiments have shown that the second one performs better in terms of running times and quality of solutions. Therefore, our results are based on this second option.In the Shaking operator step, the incumbent solution xoptis perturbed in such a way that ∣xcur⧹xopt∣=k. Nevertheless, this step does not guarantee that xcurbelongs toNkr(xopt)due to randomization of the choice of goin and possible reinsertion of the same facility after it has been removed in the goout phase. Then, xcuris used as initial solution for VND routine in Local Search step. If a solution better than xoptis obtained, we move there and start again with small perturbations of this new best solution, i.e.,k← 1. Otherwise, we increase the distance between xoptand the new randomly generated point, i.e., we set k←k+1. If k reaches kmax(this parameter can be chosen equal to N), we return to Main step (within the modified VNS algorithm described in Section 3.5), i.e., the main step can be iterated until some other stopping condition is met (e.g. maximum number of iterations, maximum CPU time allowed, or maximum number of iterations between two improvements). Note that the element xcuris generated at random in Shaking operator step in order to avoid cycling which might occur if any deterministic rule were used.Table 10reports the results of the computational tests that we have compared in order to set the maximum size of the family of neighbors to be used in the algorithm. We have compared three different sizes kmax=3, 5, 10 on problem types T1 and T4 (see Section 4). In order to do that, we ran the algorithm with a time limit of 600 seconds and for the best two VND searches, namely “Best-Guided” and “Best–Best”. From the results we conclude that the best results are obtained using “Best-Guided” as VND routine and with a number of facilities replaced in each iteration of kmax=5 (see Table 10).Algorithm 3.4Modified VNS for the DOMPWe present in Algorithm 3.4, the Modified Variable Neighborhood Search (MOD-VNS) heuristic for DOMP. In its description we refer to the procedures described in Section 3.4. The algorithm has three main phases: (1) Preprocessing that sets the data structure used in the evaluation and update of new solutions and (2) Initialization that sets the initial solution, stopping criterion, etc. Finally, the third phase Main step that is the main loop including the Shaking operator and the Local Search (VND) which actually runs the VNS heuristic.In this section, we compare the results of MOD-VNS algorithm with existing algorithms from the literature. All test were implemented in C and compiled with Microsoft Visual C++ 6.0 and were run on a Pentium 4 at 1.8gigahertz. with 1GB of RAM. Our results are based on the benchmark instances of the ORLIB N-median data set publicly available electronically from http://people.brunel.ac.uk/∼mastjjb/jeb/info.html (see Beasley (1990)). As in previous papers on this topic, we considered N-median instances with 100⩽M⩽900 nodes and 5⩽N⩽200 potential new facilities and solved them for 8 different classes of λ-parameters already suggested in the literature. Each λ is associated with a different objective function and it models different types of DOMP, as already indicated in Section 2. The λ-parameters considered in our experiments are:•T1: λ=(1,…,1), vector corresponding to the N-median problem.T2: λ=(0,…,0,1), vector corresponding to the N-center problem.T3:λ=(0,…,0,1,…,1︸k), vector corresponding to the k-centrum problem, wherek=⌊M3⌋.T4:λ=(0,…,0︸k1,1,…,1,0,…,0︸k2), vector corresponding to the (k1,k2)-trimmed mean problem, wherek1=N+⌈M10⌉andk2=⌈M10⌉.T5:λ=(0,1,0,1,…,0,1,0,1)ifMiseven(0,1,0,1,…,0,1,0)otherwise, vector corresponding to an alternate series of zeroes and ones starting with zero.T6:λ=(1,0,1,0,…,1,0,1,0)ifMiseven(1,0,1,0,…,1,0,1)otherwise, vector corresponding to an alternate series of zeroes and ones starting with one.T7:λ=(0,1,1,0,1,1,…,0,1,1,0,1,1)ifM≡0(mod3)(0,1,1,0,1,1,…,0,1,1,0)ifM≡1(mod3)(0,1,1,0,1,1,…,0,1,1,0,1)ifM≡2(mod3), vector corresponding to an alternate series of zeroes and two consecutive ones starting with zero.T8:λ=(0,0,1,0,0,1,…,0,0,1,0,0,1)ifM≡0(mod3)(0,0,1,0,0,1,…,0,0,1,0)ifM≡1(mod3)(0,0,1,0,0,1,…,0,0,1,0,0)ifM≡2(mod3), vector corresponding to an alternate series of two consecutive zeroes and one starting with zero.Our stopping criterion is based on running time. We fixed a time limit of 200seconds for the small size instances (those up to 400 nodes) and we raised the time limit to 600seconds for the large size instances (those with more than 400 nodes). We run MOD-VNS algorithm 10 times for each instance. In all the executions MOD-VNS is initialized by the solution of the RG-construction as described in Section 3.2. We report the best solution found among these 10 executions. Since the CPU we used and those used by other authors are different (AMD Semprom 1.6 Stanimirovic et al., 2007 and pentium III 0.8 Domínguez-Marín et al., 2005), average running times were scaled down by the appropriate factors according to published standards. (See CPU speed benchmarks at http://www.spec.org/cpu2000/ in Table 11).In the following we describe, in an exhaustive way, our computational results. We report detailed results on the execution of MOD-VNS for the 8 types of λ-parameters on the 40 N-median instances of the ORLIB. We also compare our results with those available from the papers by Domínguez-Marín et al. (2005) and Stanimirovic et al. (2007). Problems T1 and T4 were reported on both papers, whereas results for problems T2, T3 and T5–T8 are only available from the second paper. Nevertheless, we are also interested in comparing our new implementation of VNS with the one in Domínguez-Marín et al. (2005). Therefore, we report the comparisons in different formats. Tables 12 and 14include the results of the VNS in Domínguez-Marín et al. (2005), the HGA1 given in Stanimirovic et al. (2007) and our results obtained by MOD-VNS. In addition, in Tables 13,15 and 16we report on the comparison of MOD-VNS and the genetic algorithm HGA1 in Stanimirovic et al. (2007). In order to compare the quality of our results, we use the best known solutions found that have been reported either by Domínguez-Marín et al. (2005) and/or Stanimirovic et al. (2007), with the exception of problems’ type T1 where we use those reported by Beasley in Beasley (1990).To assess the performance of our VNS, we show comparative results on these benchmark instances. The main criteria used for comparisons are the ones commonly used in the literature: gap and computing times. Consequently and like many previous studies, we make our comparisons based on information such as number of best solutions found, CPU time and gap obtained by each algorithm.Since our test problems for all problem types are based on the same 40 N-median instances of the ORLIB, we only include their names and sizes, i.e. number of nodes (M) and number of facilities to be located (N), in our first table, namely Table 12. We also remark that the last two rows in all tables report, respectively, the averages of the corresponding columns (one to the last row) and the number of times that the best result (either CPU time or objective function value) is obtained in that column (last row).Tables 12 and 14 present the detailed computational results of MOD-VNS as well as those obtained by VNS and HGA1, over the benchmark instances for the problem types T1 and T4. In these tables, we indicate the optimal or best known objective function values in the columns (OPT) in Table 12 and (BEST) in Table 14. The remaining columns show the objective values (OBJ), the running times in seconds (T) and the gap (GAP) obtained by each algorithm. (The reader should observe that 0.00 in columns T means that the CPU time was less that 0.01seconds.)Table 12 reports the results of MOD-VNS algorithm applied to problem type T1. We observe that the number of optimal solutions found are 31 out of 40 (31/40) for MOD-VNS, 23 out of 40 (23/40) for HGA1 and 17 out of 40 (17/40) for VNS. MOD-VNS takes clear advantage with respect to VNS in all rates, there is a tradeoff between quality of the solution and computing time required to obtain this solution: the average time is equal to 493.63 scaled seconds for VNS and 73.17 scaled seconds for MOD-VNS (less than 15% of the required running time). Notice that the maximal computing time required by MOD-VNS does not exceed 335seconds, whereas the maximum time required by VNS was almost 4774seconds.The reader may also observe that the quality of the solutions provided by MOD-VNS (gap of 0.03%, on average) is comparable, although slightly better than that of HGA1 (0.12%, on average). Nevertheless, running time required by MOD-VNS is larger on average than the running time for HGA1 although in 26 out of the 40 (26/40) instances MOD-VNS required less CPU time than HGA1 to find the best solution. This shows a similar behavior of both algorithms on problems of type T1.Table 14 reports the comparisons, in the same format as above, for problem type T4 ((k1,k2)-trimmed mean problem). Optimal solutions for these problems on N-median instances are not known so far, so we considered the best known solutions reported in the literature. MOD-VNS reached 35 out of 40 (35/40), HGA1 26 out of 40 (26/40) and VNS 17 out of 40 (17/40) of the best-known solutions. The behavior of MOD-VNS applied to T4 is quite similar to T1. We can observe that after a reasonable computing time MOD-VNS and HGA1 solve large (k1,k2)-trimmed mean problems. The computing time required by HGA1 (57.86seconds, on average) is shorter than the time needed by MOD-VNS (72.24seconds, on average) although as observed before in Table 12 for the problem type T1, MOD-VNS requires in 26 out of 40 instances less CPU time than HGA1 to find the best solution. Moreover, MOD-VNS provides higher number of best-known solutions than HGA1.By analyzing Table 13, we observe that the average gap over T2 (N-center) is about 28% for HGA1 and 22% for MOD-VNS. Note that this is the worst gap among all problem types (T1–T8). Nevertheless, in a number of instances, the best known objective function value was attained, 18 out of 40 (18/40) for MOD-VNS as compared to only 11 out of 40 (11/40) for HGA1. Concerning the CPU time, we point out that MOD-VNS is about 27% faster than HGA1 for this type of problem. Turning to problem type T3, we observe that the performance of both algorithms is similar, although MOD-VNS is slightly better both in CPU time (3% faster) and in quality of solutions (0.03% gap for MOD-VNS as compared to 0.11% gap for HGA1).From Tables 15 and 16, we observe that MOD-VNS and HGA1 also compare similarly on problem types T5, T6, T7 and T8. On the one hand, CPU time required by MOD-VNS is slightly smaller than the one needed by HGA1 on problems T6, T7 and T8 and slightly larger for T5; although in that case this excess is always less than 24%. On the other hand, the quality of solutions found by MOD-VNS is superior than the one obtained by HGA1, because the average gap provided by HGA1 over all instances is higher than that given by MOD-VNS. Moreover, as in all the previously commented problem types MOD-VNS reaches a higher number of best-known (optimal) objective function values than HGA1.It should be emphasized that the performance of both procedures is rather good on all problem types except for type T2 (i.e., N-center problems). For problem T2 the average gap is relatively large. In spite of that, the quality of the solutions obtained by MOD-VNS for problems of type T2 is better than that provided by HGA1.We show in Fig. 1a comparison of the performance of algorithm HGA1 versus MOD-VNS in terms of % gap (x-axis) and CPU time (y-axis) for problem types T1 and T3–T8. The reader should observe that N-center problems, namely problem type T2, are excluded from this comparative since its average gap for both algorithms is above 20%, some orders of magnitude greater than the rest. Thus, we have decided not to include it because the representation of those data on the same figure would have perturbed the visualization of the comparison of the remaining ones. From this table, we see that for problem types T3, T6, T7 and T8, MOD-VNS requires less computing time than HGA1 whereas the contrary occurs for problems T1, T4 and T5. However, in all problem types the quality of solutions obtained by MOD-VNS is higher than the quality of solutions obtained by HGA1 since MOD-VNS always gets gaps with respect to best known solutions smaller than the corresponding ones obtained by HGA1.From the above analysis, we observe that MOD-VNS performs better than HGA1 in terms of the quality of the solutions found (smaller gaps, higher number of best solutions found). Concerning average CPU times the performance of both algorithms is similar: in five out of eight problem types, namely T2, T3, T6, T7 and T8, MOD-VNS is slightly faster whereas the opposite occurs for T1, T4 and T5. Nevertheless, in all problem types the number of instances with less required CPU time is always higher with MOD-VNS than with HGA1.Finally, we would like to point out that the new encoding of solutions, specifically developed for applying VNS algorithms to the DOMP problem, yields results considerably better than previous encodings as observed comparing VNS and MOD-VNS when it is possible.

@&#CONCLUSIONS@&#
In this paper, we present a modified VNS heuristic, named MOD-VNS (there exists another VNS for the same problem in the literature) for the Discrete Ordered Median Problem (DOMP). This algorithm is based on refined neighborhoods’ structures that favor faster improvement of the objective function in the local search phase and allow an efficient encoding of the solutions of the DOMP avoiding sorting in the evaluation of the objective function at each considered solution. Comprehensive computational experiments, on ORLIB N-median instances, demonstrate the robustness of the proposed algorithm with respect to solution quality. For five problem types, namely T2, T3, T6, T7 and T8, running times for MOD-VNS are better on average than the corresponding running times obtained by HGA1; moreover in all problem types the number of instances for which MOD-VNS requires less computing time is greater than the corresponding one for HGA1. In spite of that, for 3 problem types, T1, T4 and T5, the average running times of our algorithm are higher than HGA1. Comparisons with results from the literature show the appropriateness of applying the proposed algorithm. Computational results show that in many cases MOD-VNS outperforms other existing algorithms for these problems in number of best solution found, average gap and CPU times.