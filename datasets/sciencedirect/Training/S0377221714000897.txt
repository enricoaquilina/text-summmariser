@&#MAIN-TITLE@&#
An adaptive multiphase approach for large unconditional and conditional p-median problems

@&#HIGHLIGHTS@&#
A novel multiphase for large p-median problems is proposed.An effective integration of aggregation, VNS and exact methods is introduced.New best solutions for some benchmark problems are found.A new large dataset for p-median problems with guaranteed optimality is constructed.An adaptation of our approach for the conditional p-median problem is presented.

@&#KEYPHRASES@&#
Variable neighbourhood search,Exact method,Aggregation,Large p-median problems,Adaptive learning,

@&#ABSTRACT@&#
A multiphase approach that incorporates demand points aggregation, Variable Neighbourhood Search (VNS) and an exact method is proposed for the solution of large-scale unconditional and conditional p-median problems. The method consists of four phases. In the first phase several aggregated problems are solved with a “Local Search with Shaking” procedure to generate promising facility sites which are then used to solve a reduced problem in Phase 2 using VNS or an exact method. The new solution is then fed into an iterative learning process which tackles the aggregated problem (Phase 3). Phase 4 is a post optimisation phase applied to the original (disaggregated) problem. For the p-median problem, the method is tested on three types of datasets which consist of up to 89,600 demand points. The first two datasets are the BIRCH and the TSP datasets whereas the third is our newly geometrically constructed dataset that has guaranteed optimal solutions. The computational experiments show that the proposed approach produces very competitive results. The proposed approach is also adapted to cater for the conditional p-median problem with interesting results.

@&#INTRODUCTION@&#
The p-median problem is a discrete location problem where the objective is to find the location of p facilities among n discrete potential sites in such a way to minimise the sum of the weighted distances between customers and their nearest facilities. The p-median problem becomes the conditional problem when some (say q) facilities already exist in the study area and the aim is to locate p new facilities given the existing q facilities. This problem is also known as the (p,q) median problem. A customer can be served by one of the existing or the new open facilities whichever is the closest to the customer. When q=0, the problem reduces to the unconditional problem (the p-median problem for short). A further but brief description related to the conditional p-median problem will be presented in Section 6 where some results are also given.The p-median problem is categorised as NP-hard (Kariv & Hakimi, 1979). For relatively large problems, optimal solutions may not be found and hence heuristic or metaheuristic methods are usually considered to be the best way forward for solving such problems. Mladenovic, Brimberg, Hansen, and Moreno-Perez (2007) provided an excellent review on the p-median problem focusing on metaheuristic methods. The p-median problem was originally formulated by ReVelle and Swain (1970). However, Rosing, ReVelle, and Rosing-Vogelaar (1979) enhanced the p-median problem formulation to reduce its solution time. In their model, the furthest p−1 assignments associated with each demand point are ignored. This reduction scheme is based on the observation that in the worst case, a demand point i is served by its (n−p+1)th closest site. The enhanced p-median formulation is formulated as follows:(1)Minimise∑i∈I∑j∈Fiwid(i,j)Yij(2)s.t.∑j∈FiYij=1∀i∈I(3)∑j∈JXj=p(4)Yij-Xj⩽0,∀i,j∈Fi(5)Xj∈{0,1}∀j,j∈J(6)Yij∈{0,1}∀i,j∈Fiwhere (I,J) is the set of customers (i∊I={1,…,n}) and set of potential sites (j∊J={1,…,M}) (i.e.:n=|I| and M=|J|) respectively; withe demand or weight of customer i; d(i,j) the distance between customer i and potential site j (Euclidian distance is used here); p the required number of facilities to locate; Yij=1, if customer i is fully served by a facility at site j and=0 otherwise; Xj=1, if a facility is opened at potential site j and=0 otherwise; Fiis the set of all sites except the p−1 furthest sites from demand point i.The objective function (1) minimises the total demand-weighted distance. Constraints (2) guarantee that each customer i is assigned to one facility only. Constraint (3) states that the number of facilities to be located is p. Constraints (4) ensure that customer i can only be allocated to facility j (i.e., Yij=1) if a facility is opened at site j (i.e., Xj=1). The use of the sets Fiin constraints (2), (4), and (6) yields a more compact formulation, requiring a fewer number of variables and constraints than the classical formulation.In some applications, p-median problems may involve a large number of demand points and potential facility sites. These problems arise, for example, in urban or regional areas where the demand points are individual private residences. Francis, Lowe, Rayco, and Tamir (2009) stated that it may be impossible and time consuming to solve location problems consisting of a large number of demand points. To simplify the problem, it is quite common to aggregate demand points (and/or potential facility sites) when solving large-scale location problems. In other words, the number of demand points (and/or potential facility sites) can be reduced from n to m points (m≪n) so that the approximated problem can be solved within a reasonable amount of computing time. However, aggregation introduces errors in the data as well as in the models output, thus resulting in less accurate results.The main contributions of this paper include: (i) a novel multiphase approach that incorporates aggregation, Variable Neighbourhood Search (VNS) and an exact method for solving large p-median problems, (ii) new best solutions for some benchmark problems, (iii) the construction of a new large dataset for p-median problems with guaranteed optimality, and (iv) an adaptation of the proposed approach for the conditional p-median problem.The paper is organised as follows. Section 2 presents a brief review of the past efforts at solving large p-median problems. Section 3 describes the ingredients that make up our method as well as the overall algorithm. Detailed explanations of the main steps and the “Local Search with Shaking” procedure are described in Section 4. Computational results are presented in Section 5 using large datasets including the one with guaranteed optimal solutions which we constructed. Section 6 presents a brief review on the conditional p-median problem followed by the adaptation and the implementation of our approach for this related problem. The last section provides a summary of our findings and highlights some avenues for future research.This section presents an overview of past efforts at solving large p-median problems (see Francis et al., 2009, for an excellent review). Hillsman and Rhoda (1978) introduced a classification of aggregation errors using three types, namely source A, B, and C errors. Source A error occurs when the distance between an Aggregate Spatial Unit (ASU) and a facility is utilised in the model, instead of the true distance between a Basic Spatial Unit (BSU) and a facility. Source B error exists in the special case when a facility is located at an ASU whereas source C error appears when a BSU is assigned to the wrong facility.Goodchild (1979) stated that aggregation tends to produce more dramatic effects on location than on the values of the objective function while also noting that there is no aggregation scheme without a possible resulting error. Bach (1981) mentioned that “the level of aggregation exerts a strong influence on the optimal locational patterns as well as on the values of the locational criteria”. Mirchandani and Reilly (1986) examined the effect of replacing distances to demand points (BSUs) in a region by the distance to a single point (ASU) representing that region.Current and Schilling (1987) proposed a method for eliminating source A and source B errors. They introduced a novel way of measuring aggregated weighted travel distances for p-median problems. Let d(i,j) denote the distance between the ith and the jth BSUs andd̃(k,j)the distance between the representative point of the kth ASU and the jth BSU. The distance between the kth ASU and the jth facility is traditionally defined as:(7)dˆ(k,j)=Wkd̃(k,j)whereWk=∑i∈Akwiwith Akbeing the set of aggregated BSUs at the kth ASU.To eliminate source A and B errors, the distance proposed in Current and Schilling (1987) is set as:(8)dˆ(k,j)=∑i∈Akwid(i,j)However, this method is not able to eliminate source C errors.Casillas (1987) introduced two measures to assess the accuracy of aggregated models. These include the cost error (ce=f(F′:C)−f(F′:C′)) and the optimality error (oe=f(F:C)−f(F′:C)) where F and F’ represent the optimal locations of the p facilities found with the original and the aggregated models respectively, while C and C’ denote the list of BSUs and ASUs. The objective functions f(F:C), f(F′:C) and f(F′:C′) represent the objective function evaluated using F and C, F’ and C, and F’ and C’ respectively.Oshawa, Koshizuka, and Kurita (1991) studied the location error and the cost error due to “rounding” in the unweighted 1-median and 1-centre problems in the one-dimensional continuous space. Aggregation error bounds for the median and the centre problems were developed by Francis and Lowe (1992). A Geographical Information System (GIS) method for eliminating source C error was proposed by Hodgson and Neuman (1993). Transport costing errors for the median problems were investigated by Ballou (1994) who demonstrated that cost errors increase with p but decrease with m. An investigation by Fotheringham, Densham, and Curtis (1995) suggested that the level of aggregation affects the location error more significantly than the objective function value. Francis, Lowe, and Rayco (1996) introduced a median row-column aggregation method to find an aggregation which gives a small error bound. In addition to the A, B, and C errors, Hodgson, Shmulevitz, and Körkel (1997) introduced source D error which arises when the BSU locations act as potential sites.Murray and Gottsegen (1997) investigated the influence of data aggregation on the stability of facility locations and the objective function for the planar p-median model. Demand point aggregation procedures for the p-median and the p-centre network location models were studied by Andersson, Francis, Normark, and Rayco (1998). Hodgson and Salhi (1998) proposed a quadtree-based technique to eliminate source A, B, and C errors in the allocation process. Bowerman, Calamai, and Brent Hall (1999) investigated the demand partitioning method for reducing source A, B, and C aggregation errors in p-median problems. Erkut and Bozkaya (1999) provided a review of aggregation errors for the p-median problem. Francis, Lowe, and Rayco (2000) computed error bounds for several location models. Plastria (2001) investigated how to minimise aggregation errors when selecting the ASUs location at which to aggregate given groups of BSUs. Hodgson (2002) introduced data surrogation error in the p-median problem which appears when an original population’s demand is substituted by inappropriate values.To solve large p-median problems without aggregation, Church (2003) put forward an enhanced Mixed Integer Linear Programming formulation called COBRA. He also proved that there are redundant assignment variables that can be consolidated if they satisfy some equivalent assignment conditions. These conditions are based on the order of closeness of facility sites with respect to pairs of demand points. This property leads to a reduction that can be up to 80% of the original number of variables. An enhanced model formulation referred to as Both Exact and Approximate Model Representation (BEAMR) was later proposed by Church (2008). Hansen, Brimberg, Urosevic, and Mladenovic (2009) introduced a primal–dual VNS metaheuristic for large p-median clustering problems where a Reduced VNS is used to get good initial solutions which are then fed into a VNS with decomposition. The worst-case analysis of demand point aggregation for the Euclidean p-median problem on the plane was investigated by Qi and Shen (2010). An alternative covering based formulation which has a small subset of constraints and variables is studied by Garcia, Labbe, and Marin (2010). This method is relatively more efficient when p is large.Avella, Boccia, Salerno, and Vasilyev (2012) designed an aggregation heuristic based on Lagrangean relaxation. They proposed three main procedures, namely a sub-gradient column generation, a core heuristic, and an aggregation heuristic. The first procedure solves the Lagrangean relaxation by combining sub-gradient optimisation with column generation. The core heuristic is defined by a subset of the most promising variables found according to the Lagrangean reduced costs associated with the open facilities as well as those associated with the allocation variables. An aggregation heuristic is then introduced to tackle the problem when the value of p is relatively small. Very recently, Irawan and Salhi (2013) introduced an approach using demand points aggregation and variable neighbourhood search for solving large-scale p-median problems. Their method used a multi-batch methodology where a learning process that feeds information from one batch to another is utilised. A batch consists of aggregated problems. In this paper, we propose a multiphase approach instead of a multi-batch approach. The first batch of the method by Irawan and Salhi (2013) is similar to our Phase 1 except that a more efficient implementation of the local search is adopted. Subsequent phases are also designed to guide the search in exploring new areas while retaining promising regions. Moreover, we also enhance the method used to solve the aggregated p-median problem; we present an efficient way in aggregating the demand points; and we put forward an effective implementation of the local search that is used to solve the disaggregated (original) problem.We propose an adaptive approach which consists of four phases. The main steps of these phases are depicted in Fig. 1but a brief overview is given below. Moreover, a visualisation of our methodology is presented in Appendix A whereas a detailed description of the main steps is given in the next section. In this study, for simplicity we consider potential facility sites as customer sites (i.e. M=n).In the first phase a learning process is conducted. Here, a clustering procedure is used to aggregate n BSUs into m ASUs, with m≪n. As each customer site acts as a potential facility site, the aggregated problem reduces to having m customers and m potential facility sites. This phase consists of solving a number of aggregated problems of m ASUs using a “Local Search with Shaking” procedure with the aim of choosing p facility locations. This will be described further in Section 4.3. Let L denote a list of distinct facilities obtained from the solutions of the aggregated problems.In Phase 2, |L| facilities are considered as the ‘promising’ facilities to set up an aggregated p-median problem which is then solved with a VNS or with CPLEX, depending on the size of the augmented problem. Namely, if |L| is relatively small then the problem is solved by CPLEX (|L|,|L|,p), otherwise a VNS (|L|,|L|,p) is adopted where Method (g,s,p) refers to the procedure ‘Method’ for locating ‘p’ facilities, using ‘s’ potential sites and serving ‘g’ customers. When the VNS is applied, the best solution found in Phase 1 is used as the initial solution. To get a feasible solution to the p-median problem with the original customers set, the “Local Search with Shaking” is then used with |L| potential facility sites starting from the solution returned by VNS or CPLEX in the previous step. We refer to this procedure as the “Local Search with Shaking (n,|L|,p)”. The best solution in this phase is then fed into the next phase (Phase 3).The third phase is an iterative process that incorporates potential facility sites aggregation and the use of the “Local Search with Shaking”. Unlike Phase 1, the aggregation here includes the promising sites found in the previous iteration. This set of promising sites is denoted by E. The resulting aggregated problem with n customers and m potential facility sites is then solved by “Local Search with Shaking (n,m,p)”. The obtained solution is then used as an initial solution for the next iteration and the process is repeated until a stopping criterion is met. In our study, the process stops when there is no improvement after a prescribed number of consecutive iterations which we denote by itermax.In the final phase (Phase 4), a post optimisation is carried out. Here, a local search is used to solve the original problem (without aggregation) starting from the best solution obtained in the previous phase. To speed up the search, a reduction scheme, which is described in Section 4.5, is also incorporated into the search.In this section we present the aggregation scheme and the distance calculation method. These are followed by the description of the “Local Search with Shaking”, the VNS, the exact method, and the local search which is used in the original problem.This subsection describes the procedure to aggregate n BSUs into m ASUs used in Phases 1(i) and 3(i). The set of the m ASUs includes the followings:•the promising facility locations obtained from previous iterations in Phase 3 (i.e. the set E). Note that in Phase 1, E=Ø.(mγ) pseudo randomly generated points, where γ is a parameter (γ>0).(m−|E|−mγ) randomly generated points.Firstly, the method includes the promising facility locations (E) as part of the aggregated points. We assume the use of these points may increase the probability of obtaining a good solution. Secondly the Basic Cell Approach (BCA), as shown in Fig. 2and briefly described below, is used to generate the subsequent mγ aggregated points. All demand points are covered by square cells and the cell information is used for determining m ASUs. This scheme overcomes the weaknesses of a simple random process when dealing with clustered demand points. In addition, it ensures that the generated ASUs are not too close to each other. This is achieved by imposing that the distance between any pair of ASU points is larger than a certain threshold which is based on the side of the cell. Finally, some randomly generated points are added to the set of ASUs to increase the diversity of the solutions.The BCA method is adapted from the approach given in Irawan and Salhi (2013) which is originally based on the one by Salhi and Gamal (2003) for the multisource Weber problem. An illustration of the BCA is shown in Fig. 2. The main steps of the method are formally given in Fig. 3.Initially, (cL⋅cW) square cells are constructed. We set the number of cells to be approximately m. Let δ denote the length of the side of the cell which is given by:δ=(xmin-xmin)mxmin-xminymin-yminwhere xmax and xmin refer to the maximum and the minimum x coordinate of the points respectively. Similarly, ymax and ymin refer to the maximum and the minimum y coordinate respectively. A cell is identified by its bottom-left corner. In other words, the coordinates of the bottom-left corner of the zth cell is denoted by (Xz,Yz), z=1,…,(cL⋅cW). The bottom-left corner of cell 1 is (X1,Y1)=(xmin,ymin) and successive cells are defined as follow:(Xz,Yz)=(xmin+δ(zmodcL),ymin+δ(zmodcW))The number of demand points in each cell, Gz, is then recorded and its corresponding probability distribution is calculated as Pz=Gz/n, z=1,…,(cL⋅cW).Next, a cell is chosen in a pseudo random manner based on the cumulative probability distribution. In other words, we generate randomly β∊(0,1) and choosez̃stz̃=F(z)-1(β)withF(z)=∑v=1zPv. For instance as an illustration,z̃=3in Fig. 4. A demand point (say point a) is then chosen randomly in the cellz̃as long as it satisfies the threshold distance separation criteriond→a=Minj∈C′d(a,j)⩾τ=ρδwith ρ being a parameter whose value is dynamically decreased if no aggregated point is found after a number of attempts being made (say ς, in our study ς=m). The selection of a cell and of a point within the cell is repeated until a prescribed number of ASUs is reached. The centroid of the points in a cell was also attempted but a preliminary study showed that the quality of the solution was found to be slightly inferior.When using the “Local Search with Shaking”, the VNS, or CPLEX to solve the aggregated p-median problem, the distance matrix between points in C’ (ASUs) has to be determined first. The way this is performed depends on the type of aggregated p-median problems used. The aggregated problems can be categorised as (m,m,p), (n,m,p), or (|L|,|L|,p) where (a,b,p) refers to solving the p-median with a customers and b potential sites.In Phase 1(iii), the (m,m,p) aggregated problem is solved. Here, the procedure to calculate the distance between points in C’ is performed first by constructing m clusters, and allocating all demand points to the nearest points in C’. Secondly, the total weight of each cluster Wk, k=1,…,m is computed. Finally, the approximate distance between each pair of points in C’, denoted bydˆ(k,j), is calculated using (7).In Phase 2(ii), the (|L|,|L|,p) aggregated problem is solved instead. In this case, we calculate the distance between each pair of points in L using (8) which is practical in this case as the (|L|,|L|,p) aggregated problem is solved only once. In Phases 2(iii) and 3(ii), where no clustering is needed, the true distance between the ith BSU and the jth facility, d(i,j), is used for both the (n,|L|,p) and the (n,m,p) aggregated problems.We use a “Local Search with Shaking” to speed up the search. This choice is due to the fact that our method is an iterative-based approach and therefore finding solutions to the aggregated problems with the VNS or CPLEX would be too time consuming. The method utilises one shaking and one call to the local search only. We explore the following two approaches:(a)Only the first neighbourhood (N1) is used. This shaking process can be considered as a perturbation. This is then enhanced by a local search.Similar to (a), but instead of using the first neighbourhood (N1), the kth neighbourhood (Nk) is randomly generated where k∊(1,kmax).We refer to (a) and (b) as Var1 and Var2 respectively. We carried out some preliminary experiments to test the performance of these two variants. The results, reported in the computational results section, show that Var2 is relatively superior.The shaking process adapted here applies the shaking algorithm used by Hansen and Mladenovic (1997). Let X denote the facility configuration of the current solution and H the set of potential facility sites. The kth neighbourhood structure Nkis defined as: Nk(X)=use of N1(X) k times with k=1,…,kmax andN1(X)=X-σ′∪σ″whereσ″is chosen randomly in H−X and σ′∊X is selected to yield the best improvement.The “Local Search with Shaking” is used to solve the (m,m,p), the (n,|L|,p), and the (n,m,p) p-median problems. In both Phases 2 and 3, this procedure takes the best solution from the previous steps as the initial solution when solving the (n,|L|,p), and the (n,m,p) p-median problems. For the (m,m,p) p-median problem solved in Phase 1, p randomly chosen points are considered instead.For the (m,m,p) problem, the local search process uses the procedure “FindBestCustomer” proposed by Irawan and Salhi (2013) combined with the use of an efficient data structure initially presented by Resende and Werneck (2007). The latter records intermediate calculations so to eliminate any unnecessary recomputations. A similar data structure was also successfully implemented by Osman and Salhi (1996) when solving the vehicle fleet mix problem. We refer to this local search as “IS-RW”. This procedure is based on the fast interchange heuristic introduced by Whitaker (1983) which is then adapted to increase the computational speed at the expense of a small loss in quality. The procedure identifies a point (say point i) among the potential facility sites to be inserted and a facility (say facility j, j∊X) that yields the highest saving to be removed. The procedure restricts the search as follow: it considers point i∊Sjwith Sjbeing the set of potential sites that are nearer to facility j than the other open facilities in the current solution (|Sj|<|H|−p). For the (n,|L|,p) and the (n,m,p) problems, we use the well-known fast swap-based local search procedure of Resende and Werneck (2007). We refer to this local search as “RW”. We can afford to use this local search here without the reduction scheme used in “IS-RW” as good solutions are usually obtained from Phase 1 which are then used as initial solutions for the (n,|L|,p) and the (n,m,p) problems.Fig. 5presents the “Local Search with Shaking” when solving the aggregated p-median problems with the use of Var2.We conducted preliminary experiments to test the performance of these two local searches namely IS-RW and RW. The results are reported in the computational results section.Variable Neighbourhood Search (VNS) is a metaheuristic first introduced by Brimberg and Mladenovic (1996) for solving continuous location-allocation problems. Hansen and Mladenovic (1997) formally formulated this heuristic and applied it to solve the p-median problem. VNS combines both local search and neighbourhood search. The first search looks for local optimality, while the latter aims to escape from these local optima by systematically using a larger neighbourhood if an improvement is not found and then reverts back to the smaller one otherwise. Initial VNS implementations are given in Hansen and Mladenovic (2001), but newer variants of VNS and successful applications can be found in Hansen, Mladenovic, and Perez (2010).The VNS or CPLEX is utilised to solve the augmented (|L|,|L|,p) p-median problems. The use of these relatively more intensive methods is acceptable as one run of VNS/CPLEX is needed only. Moreover, the size of the p-median problem (|L|,|L|,p) is still relatively small.When the VNS is used, we limit the computing time for solving the problem to Tvnsseconds. In this study, we set Tvns=10n0.25p0.5m0.5/1000 which is found based on a preliminary study. The algorithm of the VNS is based on the one by Hansen and Mladenovic (1997) incorporating the fast swap-based local search (RW). The enhanced formulation (1)–(6) of the p-median problem, as given by Rosing et al. (1979), is used in the CPLEX implementation.An additional post optimisation step to solve the disaggregated problem (original problem) starting from the best solution found in the previous phase is introduced. The main steps are similar to the ones proposed by Irawan and Salhi (2013) except here we adopt a more efficient implementation of the procedure “FindBestCustomer” where we restrict the search even further by imposing that the substituted location must lie within a certain covering radius (r). The value of r is based on the average of the longest distances from the facilities to their associated potential sites. In other words, we setr=Min(ro,Minj∈X(Rj))whereRj=mini∈Sjd(i,j)andro=(λ/p)∑j=1pRj. This setting is used to ensure that the search is more restrictive while remaining within each Rj, j=1,…,p. λ Is a correction parameter which we set, in our experiments, to 0.25. This was found empirically using a small sample problem. By using this restriction, the number of potential sites used in the procedure “FindBestCustomer” is drastically decreased which led to a massive reduction in the computing time of the local search without a significant loss in solution quality. Here, we do not apply the data structure of Resende and Werneck as the number of potential facility sites is relatively large (n) which creates an excessive memory problem due to the use of a two dimensional matrix as part of its data structure.To assess the performance of our solution method, we carried out an extensive computational study. The code was written in C++ .Net 2010 and used the IBM ILOG CPLEX version 12.5 Concert Library. The tests were run on a PC with an Intel Core i5 CPU 650@ 3.20gigahertz processor, 4.00gigabyte of RAM and under Windows 7(32bit).In our computational experiments, we used two existing datasets from the literature and a new dataset with guaranteed optimal solutions which we constructed. We first provide preliminary computational results for the two local searches and the two variants. Full computational experiments on the p-median problem are given next.These consist of the BIRCH and the TSP datasets. The BIRCH dataset is kindly provided by Avella et al. (2012) in http://iv.icc.ru/Papers.hatml whereas the TSP dataset can be downloaded from http://www.tsp.gatech.edu/world/countries.html.This is constructed using a well-defined though trivial geometric structure so to guarantee optimality when the value of p is equal to the number of groups/clusters in the dataset. We refer to the new dataset as the ‘Circle dataset’. Two examples of this dataset (n=500, p=4 and n=20,000, p=100) are shown in Fig. 6. The algorithm for generating the Circle dataset is given in Appendix B and its proof of optimality is provided in Appendix C. This is based on the following two items: (i) the p-median problem reduces to p 1-median problems; and (ii) the optimal centre of each cluster reduces to the same optimal centre of each ring which is the point as defined in our construction. Different instances of this dataset can be downloaded from the CLHO (2013) website (http://www.kent.ac.uk/kbs/research/research-centres/clho/datasets.html).The results of our experiments are presented in several tables. The notation in the tables is as follows:•n number of demand pointsp number of mediansZ: objective function valueTime: computational timeDeviation (%): this is the percent gap from the best known solution and is computed as:Deviation=100Zc-ZbZb, where Zcand Zbcorrespond to the Z value obtained with method ‘c’ and the best Z value respectively. This is equivalent to the optimality error as defined by Casillas (1987).‘Bold’ values in the table refer to the best solutions.In this subsection we conduct small experiments to assess the performance of the two local searches as well as the two variants which we discussed in the earlier section.Small experiments on 3 TSP datasets (mu1979, tz6117, and ym7663) were conducted to compare the performance of RW and IS-RW. Values of p varying from 10 to 100 with an increment of 10 are used. Each instance was executed 10 times for both local searches starting from the same initial solution. Table 1shows the performance of both local searches where Saving (%) refers to the percentage saving in CPU time of IS-RW over RW. From this table, it can be noted that IS-RW runs approximately 15–36% faster than RW. However, the quality of the solution obtained by IS-RW is, as expected, slightly affected with a deterioration between 2% and 11%. We use IS-RW when solving the aggregated problems (Phase 1) as the obtained facility locations of the aggregated problem do not necessary yield the corresponding best solution of the original problem. In other words, the main objective in Phase 1 is to identify the promising facility locations while consuming a smaller amount of computing time.We also tested Var1 and Var2 using IS-RW as a local search on the TSP dataset varying in size from n=734 to 9,976. We increase the value of p with n. Each instance was executed 10 times and every run in both approaches used the same initial solution. The summary results are presented in Table 2which shows the average of the objective function (Z), the deviation (%), the average total CPU and the average shaking time. As the “Local Search with Shaking” is used only once for each subproblem, referring to the average behaviour rather than the best is, in our view, more reliable.In general, Var2 generates better results than Var1 as it produces both a higher number of smaller average objective values and a smaller deviation (0.09841%). This means that conducting the shaking process k times does improve the quality of the solution. To our surprise, the table also shows that Var2 runs faster than Var1. This could be due to the fact that the shaking process does not only affect the performance of the local search but makes the task of the local search relatively easier. Moreover, the shaking time is found to be negligible when compared to the total CPU time (approx. 1.89% extra CPU time only for Var2 in the worst case, see for example instance ei8246). These results support our choice of using Var2 within the “Local Search with Shaking”.In our computational study, we set the parameters as follows: m=0.1n, T=10, Lmax=300, and itermax=5. Those parameters were chosen based on a small preliminary study. The number of aggregated points is only 10% of the number of demand points. The value of m affects the quality of the solution. The higher the value of m, the higher is the chance of getting a better solution. However, the computing time also increases with increasing values of m. The number of iterations (T) in Phase 1 also influences the quality of the obtained solution. The likelihood of getting a good solution increases when T is high, which also increases diversification, but at the expense of a longer computing time. We set Lmax=300 as CPLEX runs relatively long when the size of the problem exceeds this value. The method terminates when there is no improvement in five consecutive iterations (itermax=5). In addition, we fixed the seed for the random generator to a constant, say m, so the results can be reproducible if need be.Different settings were considered for the parameter ρ used to determine the threshold distance τ in the BCA method (τ=ρδ). Based on some preliminary tests, we set ρ=0 for the clustered datasets and ρ=0.25 for the non-clustered ones. The BIRCH and Circle datasets belong to the clustered dataset category whereas the TSP fits the non-clustered category. This choice of the parameter ρ implies that the threshold distance is relatively small for the clustered dataset as the demand points are spread in a certain area. For the non-clustered dataset, we set the number of aggregated points generated from the promising facility locations based on the BCA method to be 75% (γ=0.75), whereas the remaining 25% were generated randomly. For the clustered dataset, we set γ=0.90.The BIRCH dataset includes the largest instances tested in the literature (n ranges from 25,000 to 89,600). For the TSP dataset, we use the Italy, Sweden, Burma, and China instances (n ranges from 16,862 to 71,009).The results of our experiments on the BIRCH dataset are compared with the ones obtained by Irawan and Salhi (2013), Avella et al. (2012), and Hansen et al. (2009). We refer to these 3 methods as IS, AV, and VNSH respectively. The computational results of the AV and VNSH methods are taken from Avella et al. (2012). The value of p ranges between 25 and 64.The specification of the computer used to execute IS is the same as the one used here. Computational experiments for AV and VNSH were carried out by Avella et al. (2012) on an Intel Core 2Quad CPU 2.6gigahertz, 4.00gigabyte of RAM and under Windows XP64. Dongarra’s (1992) transformation is used to provide a fair comparison in terms of CPU time. The formulation of this transformation is as follow:T2=T1Nf1Nf2, where T1 denotes the reported time in Machine 1 and T2 the estimated time in Machine 2. Nf1 and Nf2 represent the number of Mflops in Machines 1 and 2 respectively. The software used to record the values of Nf1 and Nf2 can be downloaded from http://www.roylongbottom.org.uk. In that software, we record the value of 32 bit SSE MFLOPS. As we could not obtain precisely the number of Mflops of the computer used by Avella et al. (2012), we provide an approximation based on a slightly slower but similar computer available to us, namely a PC Intel Core 2Duo 2.6gigahertz, 4gigabyte of RAM.The computational results for our method (AA) on the BIRCH dataset are presented in Table 3where the summary results of the four methods (AA, IS, AV, and VNSH) are shown: the best known objective function (Z), the deviation (%), and the time (in seconds). In these experiments, we used two types of BIRCH instances, namely BIRCH instances of type 1 and BIRCH instances of type 3.On the BIRCH instances of type 1, the AA’s results are similar to the ones of IS but AA is approximately 15% faster than IS. AA provides better solutions compared to AV. Compared to VNSH, it produces similar objective function values, while yielding a slightly smaller deviation (0.0113%). On the BIRCH instances of type 3, the upper bound of VNSH was not provided by Avella et al. (2012). AA outperforms IS and AV where AA found 10 best solutions and yielded the smallest deviation (0.0014%). As stated in Hansen et al. (2009), our experiments also show that the BIRCH instances of type 3 are harder to solve compared to type 1 instances.Compared to AV, according to Dongarra’s (1992) transformation, the specification of the computer used to execute our method (AA) is approximately 25% faster than the one used by Avella et al. (2012). Table 3 also presents the transformed computing time (T2). Note that AA is faster than both IS and AV while generating relatively better results. Overall, AA found 5 new best solutions for this difficult type of instances.The computational results for the TSP dataset are given in Table 4. There are 4 instances (Italy, Sweden, Burma, and China), where each instance is solved with p varying from 25 to 100 with an increment of 25, totalling 16 instances. Our results are compared with the ones of IS. For p⩾100, we set γ=0.90 and ρ=0.10.In general, our method (AA) produces better results than the ones of IS. Here, AA yields a higher number of best solutions and a smaller deviation (0.04386%). The proposed approach also produces 13 new best solutions which can be used for further benchmarking. Based on the average computing time, AA runs approximately 12% faster than IS.By conducting experiments on the Circle dataset, the optimal errors can be obtained. The number of demand points (n) ranges from 20,000 to 60,000 with an increment of 10,000 whilst the number of opened facilities (p) is equal to 0.5%n and 1%n. There are 10 instances denoted by C1–C10. Table 5presents the results of these experiments. Z∗refers to the optimal objective function value.Table 5 shows that AA produces the optimal solutions for all instances. Overall, it seems that the Circle dataset can be solved quite easily by our method. With respect to the computing time, solving the problems with larger p requires more time than the one with smaller p. This could be partly due to the fact that in the “Local Search with Shaking”, kmax is set to p and as k∊(1,kmax), consequently a larger value of p will obviously increase the computational burden. It is worth noting that although these instances are visually trivial, they were constructed purposely this way to assess the ability of our approach to find optimal solutions. It is not that obvious for the algorithm to find the solutions as no extra information is fed into the search.In this section we review some papers focusing on the conditional p-median problem followed by the adaptation of our approach to this related problem and a summary of some computational results.The conditional location problem was first formally introduced by Minieka (1980) where conditional centres and medians on a graph were investigated. Chen (1990) developed a method for solving minisum and minimax conditional location-allocation problems with p⩾1. An algorithm that requires the one-time solution of an unconditional (p+1) centre or (p+1) median for solving the conditional (p+1) centre or (p+1) median on networks was suggested by Berman and Simchi-Levi (1990).Drezner (1995) proposed a general heuristic for the conditional p-median problem on both the network and the plane. In his paper, the term “(p,q) median problem” was introduced. Let Q denote the set of existing facilities where Q⊂J. The objective function for the p-median problem, Eq. (1), can be modified as follow (see Drezner, 1995):(9)Z=∑i∈IwiMinMinj∈Q{d(i,j)},Minj∈J,j∉Q{d(i,j)}AsDi=Minj∈Q{d(i,j)}can be computed for each i∊I beforehand, Eq. (9) can be rewritten as:(10)Z=∑i∈IwiMinDi,Minj∈J,j∉Q{d(i,j)}The use of Eq. (10) is computationally more efficient as it avoids unnecessary calculations.Berman and Drezner (2008) suggested a method for solving both the conditional p-median and p-centre problems. The method needs the one-time solution of an unconditional p-median and p-centre problem using the shortest distance matrix. A hybridization approach combining a harmony search and a greedy heuristic for solving p-median problems was recently proposed by Kaveh and Esfahani (2012).Our proposed method (AA) which is designed to solve large p-median problems can easily be adapted for tackling large (p,q) median problems. Our revised approach, which is referred to as AAq, contains the following minor modifications.(a)The aggregation methodThe q existing facility locations are always included in the promising facility locations (E) which are then used to aggregate the points. These locations are also used in Phase 1, meaning that E≠Ø in Phase 1 but E=Q.(b)The “Local Search with Shaking”In both the shaking and the local search, the existing facilities are always retained open in the solutions. In other words, the existing facilities are not even checked for possible removal.•The shakingWhen finding the best facility to be removed (say facility j) from the current solution, facility j is not one of the existing facilities (i.e. j∉Q).•The local searchThe implementation of the best improvement strategy does not include the existing facilities as these locations are always part of the solution.(a)The exact methodThe implementation of the exact method with CPLEX is still using Eqs. (1)–(6). However, constraints (11) are added to ensure that the existing facilities are always in the solution.(11)Xj=1∀j∈QThe introduction of such constraints (11) into the p-median formulation makes the problem relatively much easier to solve. In our study, we are now able to increase the value of Lmax from 300 to 1100 while using a similar amount of computational time.(d)The post-optimisation (the local search on the original problem)The modification in the post-optimisation is quite similar to the local search in the “Local Search with Shaking” described earlier in part (b).The configuration of the parameters used for solving the (p,q) median problem is similar to the one for the p-median problem except we set Lmax=1100. We test our modified method on the TSP dataset (Italy, Sweden, Burma, and China) that has already been tested on the unconditional p-median problem. We opted for this dataset so that we could use the solutions obtained by solving the p-median problem to set the existing q facilities in the (p,q) median problem. Namely, the q existing facility locations are obtained from the solution of the p-median problem solved in the previous section. For example for the (p=25,q=25) median problem, the existing 25 facility locations are from the solution of the (p=25) median problem. The objective function of the (p=25,q=25) median problem is then compared to the one of the unconditional (p=50) median problem which is taken as a lower bound. In this case, the objective function value of the (p=25,q=25) median problem should be worse than or equal to the one of the (p=50) median problem. Note that such a claim is only valid if an exact method is used instead of a heuristic.The computational results of AAq on the TSP dataset are given in Table 6. The deviation (%) is the gap between the objective function value found by the AAq and the one by AA. The results show that solving (p,q) using the AAq requires almost a third less amount of computing time than AA. This is quite expected as in the local search, the existing facilities are already fixed and consequently the number of combinations in the swapping procedure decreases drastically.On average, the deviation between the objective function value of the (p,q) median problem and the one of the p-median problem is 4.42%. Table 6 also shows interesting results where the objective function value of a more restricted problem happens to be, in some cases, smaller than the less restricted. For instance, the Z value for (p=25,q=75) median problem is smaller than the one of (p=50,q=50) problem. This could be due to two reasons: (i) the q facilities for the two cases are not necessarily the same and hence may have different effect and (ii) using a heuristic approach could also yield different solution for the two cases.We have also attempted another variant of the AAq where in the post-optimisation phase (Phase 4), a VNS is used instead of the local search. This was possible as the computing time of the AAq’s local search is usually quite fast. For the VNS, we set kmax=p. The use of the VNS in the post-optimisation yields slightly better deviation (4.41%) at the expense of a much longer computing time (almost 6 times). Out of 20 instances, the VNS improves slightly the quality of the solutions on 8 instances. The objective values are reported here for benchmarking purposes only. Italy: Z(p=50,q=25)=4272752.66; Z(50,50)=3697823.05; Sweden: Z(25,25)=10163883.03; Z(50,25)=8093829.14; Z(50,50)=7017515.03; China: Z(25,25)=82437310.68; Z(50,25)=66444176.54; Z(50,50)=57296280.91.An adaptive approach based on data aggregation, the use of a “Local Search with Shaking” and an efficient implementation of VNS/CPLEX is proposed to solve large unconditional and conditional p-median problems. The method consists of four phases. The first two phases are part of a learning process where demand point aggregation, a “Local Search with Shaking”, and a VNS/CPLEX are utilised to obtain a better initial solution. The third phase is an iterative-based phase which incorporates demand point aggregation and the “Local Search with Shaking” to improve the solution. The last phase is a post-optimisation process performed on the original problem.The computational results show that our approach performs well and runs relatively fast. For the unconditional problem, the proposed approach was tested on three types of datasets. The first one is the BIRCH dataset. On the BIRCH instances of type 1, our method produces better solution compared to the one by Avella et al. (2012) and similar solutions to the ones by Irawan and Salhi (2013) and by Hansen et al. (2009). On the BIRCH instances of type 3, our method is superior than the ones of Avella et al. (2012) and Irawan and Salhi (2013). On the TSP dataset, the results show that our method clearly outperforms the Irawan and Salhi (2013) method as it finds 13 new best known solutions for the 16 instances and reduces the average deviation. On the Circle dataset, which we constructed to guarantee geometrically optimal solutions, the proposed method is able to find all the optimal solutions. For the conditional problem, the adapted method was only assessed on the TSP dataset. The results also reveal that our method performs quite well when compared against the results of the unconditional p-median problem which are used as lower bounds.This study could be extended to investigate other related location problems such as large vertex p-centre problems and their counterparts on the plane. The proposed method could also be adapted for clustering of large datasets with higher dimension as part of data mining.

@&#CONCLUSIONS@&#
