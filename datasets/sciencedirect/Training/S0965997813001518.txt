@&#MAIN-TITLE@&#
Advanced parallel strategy for strongly coupled fast transient fluid-structure dynamics with dual management of kinematic constraints

@&#HIGHLIGHTS@&#
We provide a robust and accurate parallel solver for coupled fast transient dynamics.Kinematic constraints are handled through a dual method with no arbitrary parameters.This choice is original in the community of fast transient dynamics.The complex linear system at each step is solved using a new distributed solver.The performances of the strategy are demonstrated on complex industrial examples.

@&#KEYPHRASES@&#
Fast transient dynamics,Strongly coupled non-linear phenomena,Parallel computing,Fluid-structure interaction,Dual kinematic constraints management,Industrial simulations,

@&#ABSTRACT@&#
Simulating fast transient phenomena involving fluids and structures in interaction for safety purposes requires both accurate and robust algorithms, and parallel computing to reduce the calculation time for industrial models. Managing kinematic constraints linking fluid and structural entities is thus a key issue and this contribution promotes a dual approach over the classical penalty approach, introducing arbitrary coefficients in the solution. This choice however severely increases the complexity of the problem, mainly due to non-permanent kinematic constraints. An innovative parallel strategy is therefore described, whose performances are demonstrated on significant examples exhibiting the full complexity of the target industrial simulations.local density for structures and fluidstructural displacementstructural Cauchy stress tensorstructural Almansi–Euler strain tensorfluid velocityfluid pressurefluid total energystructural body forcesfluid body forces

@&#INTRODUCTION@&#
The present contribution deals with the simulation of complex strongly coupled fluid-structure systems submitted to fast transient loadings, such as high velocity impacts or explosions. Examples of such systems are:•nuclear vessels with internal immersed structures submitted to an explosion in the vicinity of the core of the reactor,rotor blade wheels impacting surrounding stators, which can occur for aeronautic engines or alternators of electric power plants (models with structures only, with couplings coming from numerous contacts, see Section 3.4 for instance),many kinds of infrastructures submitted to malevolent acts.The proposed work is not dedicated to producing quantitative results for the above situations, often strongly confidential, but focuses instead on providing robust, accurate and efficient parallel methods to perform such simulations. The case of distributed memory clusters is considered, since it presents the tightest algorithmic locks and is compulsory to address high numbers of cores (i.e. hundreds to thousands). The proposed strategy is then intended to be associated with a shared memory approach into a hybrid framework, shortly described in Section 5.A reference fluid-structure algorithm with kinematic constraints, both permanent and non-permanent, is first described in Section 2, based on a rigorous dual approach to compute the constraint forces, free from any non-physical parameter, prohibited for analyses performed in a safety environment, but exhibiting a complexity which causes the current parallel strategies for distributed memory clusters to fail as far as explicit time integration is concerned.An innovative parallel algorithm is thus mandatory to overcome the introduced algorithmic locks and one is provided in Section 3, mainly based on a specific distributed solver dedicated to the computation of constraint forces coupling several parallel processes. The large displacements of the structures submitted to high energy transient loadings are also taken into account by replacing the static initial distribution of the data onto the available processes by an updated domain decomposition.The performances of the proposed parallel strategy are demonstrated in Section 3 through two specifically chosen examples, the first with only permanent kinematic constraints and the second containing only structures but with very large relative displacements between the structural bodies, as well as failure, fragmentation and generalized contact. Section 4 is finally dedicated to the implementation of the complete methodology on a fully coupled fluid-structure system.The considered fluid-structure systems are governed by the following set of local equations [1] (see the nomenclature for the meaning of each variable):(1.a)Dynamicequilibriumforstructuresρq¨+∇·{σ[ε(q)]}=fvolstr(1.b)Momentumconservationforfluidsρu̇+∇P+ftrans(u)=fvolflu(1.c)Massconservationforfluidsρ̇+∇·(ρu)=0(1.d)TotalenergyconservationforfluidsĖ+∇·[u(E+P)]=0The description is Lagrangian for the structures and Eulerian for the fluid (or ALE, for Arbitrary Euler Lagrange, if the fluid grid has to be updated to follow the displacements of a surrounding structure).The equations for fluid dynamics are the compressible Euler equations, taking into account that only fast transient simulations are considered (i. e. a typical total physical time from 1 to 100ms), preventing any conductive or viscous phenomena to establish.The momentum conservation equation for fluids is written in its non-conservative form, since it provides an easy way to exhibit fluid-structure forces in the spatially discretized system (see Section 2.2). The transport force vector ftrans accounts for the non-linear convective terms, i. e.:(2)ftrans(u)=ρu·∇uThis set is completed by kinematic constraints, expressing the dirichlet boundary conditions and couplings between either structural entities (for example, interacting by means of unilateral contact) or fluid and structural entities. A general form of these constraints, variable with time, writes:(3)C(q,u)=SThe considered system is fully non-linear:1.the structural strains tensor is computed from displacement using the Almansi-Euler relation (i. e. the reference configuration is the actual configuration):(4)ε(q)=12(∇q+t∇q-t∇q∇q)the constitutive relations for structures, giving the stress tensor from the strain tensor, account for many kinds of irreversible phenomena such as plasticity, damage or failure,the kinematic constraints vary with both space and time.Space discretization is achieved by means of finite elements for the structure. For the fluid, a finite volume approach is always used for the mass and total energy conservations. As far as the momentum conservation is concerned, both finite volume and finite element approximations can be considered, with their own advantages and drawbacks, not discussed in this paper, and little impact of this choice on the parallel strategy described below. The latter finite element approach is preferred in the present case since it allows a more explicit expression of the fluid-structure interaction forces, the fluid velocities being located at the nodes of the fluid mesh. This would also be the case with node-centered polyhedral finite volumes however, coming again with its own specificities in terms of accuracy that shall not be discussed in the present paper, since this choice does not reduce the generality of the algorithmic developments proposed below.According to the fast transient dynamics context, time integration is carried through the explicit central differences scheme for structures and the explicit forward Euler scheme for fluids, yielding, from the step n to the step n+1 of the discrete time scale:(5)q̇n+1/2=q̇n-1/2+Δtq¨nqn+1=qn+Δtq̇n+1/2un+1=un+Δtu̇nusing the mid-steps to evaluate the structural velocities.The discrete system to consider at each time step then writes:(6)MSMFn+1Q¨n+1U̇n+1+Flinkn+1=FvolstrFvolflun+1-Fint(Qn+1)FP(Un+1)+Ftrans(Un+1)Cn+1Q̇n+3/2Un+2=Sn+1[ρ]n+1=[ρ]n+Fρ(U)[E]n+1=[E]n+FE(U)The mass matrices MSandMFn+1, for structures and fluids respectively, are made diagonal by classical mass lumping techniques (see for instance [2]). The matrixMFn+1is variable with time for fluids due to the eulerian/ALE representation. The internal forces Fint result from the integration of the elementary structural stresses, whereas FPresult from the integration of the elementary pressure forces and Ftrans are the nodal explicit transport forces for fluids.The discrete kinematic constraints are expressed by means of a coupling matrix Cn+1, acting on discrete velocities at next mid-step n+3/2 for structures and next step n+2 for fluids, and a right-hand side vector Sn+1, both variable with time.The system is completed by two finite volume equations, expressing the update of the density and the total energy within fluid cells using flux vectors Fρand FEcomputed on the faces of the cells. This finite volume choice for the conservations of mass and total energy has absolutely no consequence on the algorithmic considerations given in the next sections and a finite element approximation could also be used. Discussing the specificities of each approach in terms of accuracy of the physical solution is out of the scope of the current paper.Due to the explicit time integration, the system (6) is characterized by a stability condition limiting the size of the interval between two time steps. A sufficient condition is given by the Courant–Friedrich–Levy condition [3]:(7)Δt⩽minmesh cellslccwhere lcis the minimum dimension of one cell of the mesh and c the speed of sound in this cell.The classical range for the critical stability time step is from 0.1 to 1μm, which is compatible with the size imposed by the accurate representation of fast transient phenomena, such as impacts or explosions, considered in the present paper.Three main classes of kinematic constraints are considered.1. Permanent constraints with constant coefficients (see Fig. 1a)These are namely dirichlet boundary conditions, imposed motions or permanent relations between degrees of freedom (mechanisms for instance). The corresponding lines in the matrix Cn+1 and values in the right-hand side vector Sn+1 do not change with time.2. Permanent constraints with variable coefficients (see Fig. 1b)These occur mainly in the case of fluid-structure interaction with conforming meshes (i. e. the structural nodes and the fluid nodes are geometrically coincident). The constraints are written node by node along the normal direction to the structure, which follows its motion. The filling of the corresponding lines in matrix Cn+1 remains the same all along the simulation, but the values of the coefficients and of the right-hand side in Sn+1 change.3. Non-permanent constraintsThis is the general case, where the corresponding lines in the matrix Cn+1 and the right-hand side vector Sn+1, whose number is initially unknown, must be completely written at each time step. This situation occurs frequently in industrial fluid-structure simulations where the contact between structural elements is considered or where the structure undergoes large displacements, making it impossible to keep conforming meshes, using instead immersed boundaries-like fluid-structure interaction constraints (see [4,5,6] for a description of these constraints).Simple examples of such constraints among those classically encountered in industrial simulations are given by Fig. 2. The purpose of the current paper is not to list all the possible formulations for these constraints, focusing instead on their generic characteristics.The key issue in the system (6) is the evaluation of the force vectorFlinkn+1corresponding to the reactions of the system to the expressed kinematic constraints.First, the momentum conservation equation and the kinematic constraints equations in (6) can be rewritten to involve only accelerations, for both structure and fluid, using the time integration scheme to deduce velocities from accelerations:(8.a)Cn+1[Q̇n+3/2Un+2]=Sn+1⇔Cn+1[Q̇n+1/2+ΔtQ¨n+1Un+1+ΔtU̇n+1]=Sn+1⇔ΔtCn+1Q¨n+1U̇n+1=Sn+1-Cn+1Q̇n+1/2Un+1⇔C∼n+1Q¨n+1U̇n+1=S∼n+1(8.b)MSMFn+1Q¨n+1U̇n+1+Flinkn+1=FvolstrFvolflun+1-Fint(Qn+1)FP(Un+1)+Ftrans(Un+1)C∼n+1Q¨n+1U̇n+1=S∼n+1A convenient and classical way to discriminate the strategies for the computation of the reaction forces is to consider that the momentum conservation equation expresses a functional J to be stationary, J being of the form:(9)J=tQ¨n+1U̇n+1MSMFn+1Q¨n+1U̇n+1+RC∼n+1Q¨n+1U̇n+1-S∼n+1-tQ¨n+1U̇n+1FvolstrFvolflun+1-Fint(Qn+1)FP(Un+1)+Ftrans(Un+1)This yields the following expression for the vectorFlinkn+1:(10)Flinkn+1=∂R∂Q¨n+1U̇n+1Two main strategies are then classically considered for the evaluation ofFlinkn+1(see for instance [7–9] for chosen articles among the numerous references on this topic).1. Penalty approachThe forces are explicitly evaluated through a measure of the residual on each kinematic relation using arbitrary coefficients, one for each kinematic constraint:(11)R=tC∼n+1Q¨n+1U̇n+1-S∼n+1PC∼n+1Q¨n+1U̇n+1-S∼n+1⇔Flinkn+1=tC∼n+1PC̃n+1Q¨n+1U̇n+1-S∼n+1The larger the penalty coefficients in the diagonal matrix P are, the better the kinematic constraints are verified. However, this approach mainly corresponds to introducing additional springs in the system and therefore, increasing the value of the penalty coefficient reduces the size of the critical time step.Moreover, the expression (11) for the reaction forces slightly modifies the solution of the system (6), so that the considered residual is often approximated by an explicit expression built from known velocities given by the time integration scheme, which then appears at the right-hand side of the system:(12)Flink explicitn+1=tC∼n+1PCn+1Q̇n+1/2Un+1-Sn+12. Dual approachThe forces are expressed instead by means of additional dual unknowns, i. e. Lagrange Multipliers:(13)R=tC∼n+1Q¨n+1U̇n+1-S∼n+1Λ⇔Flinkn+1=tC∼n+1ΛInserting Eq. (13) into system (6) yields the linear system to be considered at each time step for the structural accelerations, the fluid acceleration and the Lagrange Multipliers:(14)MSMFn+1Q¨n+1U̇n+1+tC∼n+1Λ=FvolstrFvolflun+1-Fint(Qn+1)FP(Un+1)+Ftrans(Un+1)C∼n+1Q¨n+1U̇n+1=S∼n+1Kinematic constraints are exactly verified and this approach has no effect on the stability of the time integration. However, the system to solve at each step is no longer diagonal and is variable with time, which represents a severe drawback considering the large number of time cycles currently performed during fast transient dynamic simulations.Generic comparisons between the penalty and dual approaches can be found in [10] or [11]. Table 1summarizes the advantages and drawbacks of each approach in the framework of explicit dynamics. For the penalty approach, some recent works propose ways to avoid the effects on the size of the explicit critical time step, based on positive and negative inertia [12]. Anyway, if some valuable knowledge is available for the automatic evaluation of penalty coefficients for structural kinematic constraints, mainly unilateral contact (see for instance [13] for a reference paper in this field), such a procedure is very difficult to design for fluid-structure interaction, since poorly verified constraints will have immediate effects on the fluid flow For example, an automatic estimation method can be found in [14], but it is strongly embedded with the formulation of the considered fluid structure system and do not apply to the generic cases dealt with in the current paper. This classically comes with many preliminary simulations required to calibrate the penalty coefficients, followed by sensitivity analysis necessary to choose among the possible solutions obtained for different sets of parameters.Therefore, in the framework of safety analysis (cf. Section 1), where a minimum dependence to non-physical parameters is imposed, the dual approach appears to be the only acceptable, yielding the need for a solution strategy handling the significant increase of complexity arising in the system to solve, especially when parallel computing is involved. This makes the proposed computational method rather original in the community of fast transient dynamics, the most widely spread software programs in this field (for instance LS-DYNA [15] or RADIOSS [16]) avoiding this difficulty by relying only on penalty approaches.To conclude this section, it is noticeable that the choice of the dual approach is obviously more common in the framework of implicit mechanics, where the computational effort already comes from the solution of a global system, involving the stiffness matrix of the system [17]. However, the non-linear fast transient system considered above shows very different characteristics from those classically considered for implicit mechanics, especially due to non-permanent constraints, so that little numerical knowledge and technology can be transferred from the existing implicit methods to the problem described hereby.Combining full system (6) and system (14) exhibits the 3 main tasks to be performed at each time step of the explicit time integration.1. Main task 1: loop on cells and faces, to compute force vectors Fint, FPand Ftrans, as well as flux vectors Fρand FE.This loop is characterized by independent iterations, with very heterogeneous numerical costs from one iteration to another, since it depends on whether the cell is a structural cell or a fluid cell, on the formulation of the finite element approximations, on the nature of the constitutive law/equation of state attributed to each cell and on the current state of the mechanical system within each cell (for example, elastic or plastic for a structural cell).2. Main task 2: construction of the kinematic constraints, leading to the current Cn+1matrix and Sn+1right-hand side vector.The permanent constraints are easily handled at this step, which is mainly dedicated to the management of non-permanent constraints.This corresponds in most cases to contact detection between structural bodies, as well as writing fluid-structure relations (see Fig. 2). Practically, the subtasks to perform are spatial sorts for candidates and intersection/inclusion tests to write the actual constraints.3. Main task 3: computation of the link forcesThe system (14) is first condensed onto the Lagrange Multipliers, the computation of nodal accelerations being then straightforward once all the forces are known, thanks to the diagonal mass matrices for structures and fluids.The condensation writes:(15)C∼n+1MSMFn+1-1tC∼n+1Λ=C∼n+1MSMFn+1-1Fn+1-S∼n+1⇔Hn+1Λ=Bn+1whereFn+1=FvolstrFvolflun+1-Fint(Qn+1)FP(Un+1)+Ftrans(Un+1).The dimension of the condensed system is the number of kinematic constraints, and it is thus classically smaller that the global system by several orders of magnitude. If some constraints are redundant, the condensed system is singular but it is still positive semi-definite, stating that the kinematic solution is always unique, provided the problem is well-posed and all the constraints are compatible.To suit the needs for complex fast transient fluid-structure simulation integrated into design processes at the industrial level, parallel computing is mandatory to cope to engineering time constraints. Centralized supercomputers currently offer hundreds to thousands of cores that can be addressed simultaneously to reduce the time needed for simulations involving several millions of unknowns, provided the solution procedure is able to benefit efficiently from these cores, i.e. is scalable.The target hardware is clusters of multi-processor nodes, which provide the advantage of being quite common and may be assembled into massively parallel supercomputers such as TGCC/Curie in France (see Section 5). These computers are designed for a two level hybrid parallelism, with shared memory processing (SMP) inside the nodes and distributed memory processing between nodes. The current paper focuses on the latter, since it is mandatory to address large numbers of cores belonging to multiple nodes and exhibits tighter algorithmic locks compared to the former.A complementary shared memory solver is currently being developed for the dynamic system described in Section 2 and is quickly evoked in Section 5.As far as performances are concerned, the total number of cores currently targeted for the strongly coupled simulations considered in this paper goes from 512 to 2048. Taking into account that cluster nodes classically contain 8–32 cores available for SMP, this means that the number of needed nodes goes from 32 to 128, which sets the scalability expectations for the distributed memory solver described in the next sections. Table 2gives a short glossary of the classical terms used to analyze the performances of a parallel algorithm.The proposed solver classically relies on domain decomposition, so that the global system is split into smaller systems written on subdomains, each subdomain being attributed to one process running on one node of the used computational cluster. To obtain the solution for the global problem, the following tasks have to be fulfilled using additional communications between subdomains, performed using the classical Message Passing Interface (MPI) library [18]:1.transfer information between subdomains on their boundaries,control the load balancing for the loop on cells and faces,write kinematic constraints for degrees of freedom belonging to different subdomains,compute the link forces for such kinematic constraints.As far as kinematic constraints are concerned, no limitation in the domain decomposition process is allowed, such as forcing the constraints to involve only degrees of freedom belonging to one subdomain. The subdomain definition must be kept automatic and flexible to achieve robustness and simplicity in the industrial use of the proposed methods.Two types of boundaries are considered between subdomains, whether they involve structural elements or fluid elements (see Fig. 3). For fluid elements, ghost cells are updated through MPI communications before performing Main Task 1 (see Section 2.3), so that all local cells have correct neighbors on all their faces.As far as Main Task 1 is concerned, balancing the computational load among subdomains (i.e. MPI processes) is classically achieved by weighting the domain decomposition, taking into account the variable computational cost of the distributed cells (see Section 2.3). Estimating a priori the correct weight for each cell is difficult and therefore, a strategy a posteriori is preferred. It is based on the remark that the explicit time integration imposes a large number of time steps for current industrial simulations (for instance, 100,000 steps for a classical simulated time of 100ms, with a critical time step of 1μm).The first cycles of a simulation (classically from 100 to 1000) are thus performed with a domain decomposition with unitary weights. The load balancing is not optimal for these cycles, but it allows measuring the actual computational cost of each cell and building the correct weights for the domain decomposition. The subdomains are automatically updated at the end of this test sequence and the simulation is continued for its largest part with a well-balanced domain decomposition.For models where Main task 1 is dominant, this produces a mastered and scalable parallel algorithm, as it is demonstrated by the first example of Section 3.4. The focus is thus set in the next sections on models where Main tasks 2 & 3 concur with Main task 1, i.e. strongly coupled systems.Permanent constraints are written at the beginning of the simulation and are split onto subdomains during the domain decomposition process. They may involve degrees of freedom belonging to different subdomains, which must be dealt with for the computation of the link forces (see below).Non-permanent constraints are written at each time step and the involved degrees of freedom are unknown. Classically, it implies some interaction tests between master entities attached to mesh cells (i. e. master facets for contact or immersed structural elements for the constraints considered at Section 2.2) and slave entities attached to nodes (i. e. slave nodes for contact and fluid nodes interacting the immersed structure). Master entities are distributed onto subdomains like cells, whereas slave entities have to be duplicated to ensure that all the needed tests are performed and that no constraint is missed. Duplicated copies of one given slave node on subdomains other than the one it belongs to are called remote nodes (see Fig. 4, derived from Fig. 2 with subdomains).Remote slave entities are used to perform the interaction tests against each master entity on their own subdomain. Of course, since these tests are based on advanced spatial bucket sort operations, some duplications (and the associated MPI communications) are avoided thanks to simple geometric considerations (for instance, spatial filtering using the bounding box of each subdomain).Let us first exhibit the structure of the condensed operator Hn+1. Thanks to the diagonal mass matrices in the expression of the operator given by (11), extra-diagonal terms express the direct coupling of two different kinematic constraints (i. e. they share one or more degree(s) of freedom). This means that Hn+1 has a block-diagonal structure, each block corresponding to a group of coupled constraints (see Fig. 5-a for an example). Identifying a priori the coupled constraint groups provides an efficient way to solve the condensed system by treating each block independently.Considering subdomains introduces a differentiation between the constraint groups: some involve only degrees of freedom local to a subdomain (local groups), whereas some involve degrees of freedom belonging to several subdomains and are called global groups (see Fig. 5b). Local groups can be solved independently by the MPI processes that own the subdomains, bringing in some parallel acceleration, with no control on load balancing anyway. On the contrary, global groups are the key issue for the constraints management, since they require the condensed system to be considered at the global level, degrading the scalability of the proposed parallel approach with extra-communications and specific solving tasks.The basic approach to deal with global constraint groups consists in gathering on one MPI process the data necessary to build and solve the condensed system from all the considered subdomains, computing sequentially the corresponding Lagrange Multipliers and finally scattering the link forces onto the subdomains. Table 3gives the advantages and drawbacks of such a strategy.The drawbacks cause a dramatic loss of scalability each time the number and sizes of global groups increase, which frequently occurs when the considered mechanical system is strongly modified during the simulation (see the rotor/stator example in Section 3.4 for instance). This was anyway the first strategy implemented in the considered software (see again Section 3.4) and is referred to as the original approach for comparison purposes.To obtain the expected scalability, an innovative approach is mandatory, based on the following remarks:1.implementing a distributed memory parallel solver for the global condensed systems once built after the gathering step is useless, due to the relatively small size of these linear systems (see Section 2.3) and the cost of data redistribution before parallel solution, which easily compensates any upcoming speed-up,an increasing number of global constraints can be considered as an indicator that the domain decomposition has become inappropriate and should be rebuilt to relocate the kinematic constraints.To avoid centralizing the data to actually build the blocks of the condensed global operator, an iterative solution is designed to compute the global link forces. Considering the symmetric semi-definite positive nature of the operator, the chosen algorithm is the Preconditioned Conjugate Gradient solver [19].One iteration of the algorithm writes (from internal index k to internal index k+1):Estimated solutionΛk, residualRk, search directionPk and internal vectorZk known(16.a)Compute1stsearchparameterαk=tRkZktPkHn+1Pk(16.b)UpdateestimatedsolutionΛk+1=Λk+αkPk(16.c)ComputecurrentresidualRk+1=Bn+1-Hn+1Λk+1=Rk-αkHn+1PkIncase of no convergence (with a stop criterion based on the modulus of the residual):(16.d)SolvepreconditioningsystemQn+1Zk+1=Rk+1(16.e)Compute2ndsearchparameterβk+1=tRk+1Zk+1tRkZk(16.f)UpdatesearchdirectionPk+1=Zk+1+βk+1PkQn+1 is the preconditioner, deduced from Hn+1 (see below). The two main computational task s are the evaluation of vector Hn+1Pkin (16.a) and the solution of the preconditioning system (16.d).To describe the parallel execution of these tasks, the typical structure of a constraint matrixC∼n+1involving remote degrees of freedom is first given on Fig. 6, in the illustrative case of three constraints and three subdomains.The evaluation of vector Hn+1Pkis performed in two steps, to isolate computations that can be done locally on subdomains.(17.a)1.ComputeFk=tC∼n+1PkThe dot products of columns ofC∼n+1versus search direction Pkare computed on the subdomain that owns the corresponding constraints.The obtained contributions to vector Fkare then summed for the degrees of freedom which are both locally and remotely involved in coupled kinematic constraints, through relatively costless specific MPI communications.(17.b)2.ComputeHn+1Pk=C∼n+1[Mn+1]-1FkMn+1 is the diagonal matrix containing the masses associated with all constrained degrees of freedom, with duplication for remote degrees of freedom, so that the components of vectorHn+1Pkcan again be computed locally on the subdomain that owns each kinematic constraint.A correct preconditioner must both allow a distributed solution of system (16.d) and provide a good convergence rate to limit the number of iterations for convergence. The design strategy for preconditioner Qn+1 is illustrated on Fig. 7, based on the structure of operator Hn+1 corresponding to that of matrixC∼n+1presented on Fig. 6.An appropriate domain decomposition should limit the number of inter-domain couplings between kinematic constraints, so that the local blocks should be dominant in operator Hn+1. Preconditioner Qn+1 is thus obtained by ignoring the light grey blocks on Fig. 7-b, producing a series of local subsystems to compute internal vector Zn+1. This choice has a sense of optimality since the maximum amount of data is conserved from operator Hn+1 to ensure the best convergence rate, while keeping the solution of system (16.d) completely local. It also links the performance of the designed distributed solver to the quality of the domain decomposition (see below).It is remarkable that the choice of a Preconditioned Conjugate Gradient algorithm to compute constraint forces through Lagrange Multipliers within a multi-domain parallel framework brings the proposed method close to the well-known FETI-method [20,21], with strong differences and specificities however, linked to fast transient explicit dynamics and generic non-permanent constraints.Physical systems submitted to high energy loadings, such as explosions or impacts, or initially moving at high velocity, undergo large topological changes. As far as kinematic constraints and their interactions with domain decomposition are concerned, two main changes are considered:1.major changes in contact localization which are not taken into account by domain decomposition at the beginning of the simulation (see rotor/stator example in Section 3.4),structural fragments floating in the surrounding fluid, yielding fluid-structure interaction constraints involving different subdomains (see fluid-structure example in Section 4).The proposed automatic domain decomposition is based on Orthogonal Recursive Bisection[22], which means that the elements are distributed onto subdomains following their proximity in space, instead of using the mesh connectivity. The decomposition process is based on successive spatial splittings along the various spatial directions, using the centroid coordinates to dispatch the cells onto the subdomains. It is illustrated by Fig. 8.This elementary example, involving a fluid grid and an immersed structural mesh with no topological connection between them, demonstrates how close entities interacting together are as far as possible grouped into the same subdomain. Rebuilding domain decomposition using actual coordinates is thus a way to relocate most the non-permanent kinematic constraints, also based on proximity in space (see Fig. 2), inside subdomains.Finding robust criteria to automatically identify the times of the simulation when domain decomposition has to be updated is a current topic of research (see Section 5). To validate the strategic options described in the current paper, the update rate is imposed, possibly leading to useless updates, while still producing reliable results about the efficiency of the proposed method.The practical goals for the update process of the domain decomposition are the following:1.no centralization of the full model data on one MPI process to avoid the memory saturation of one computation node,implement a fully transparent process for the user (opposed to the classical manual save and restart procedures),keep a low updating numerical cost compared to the actual computation time spent between two successive updates.The chosen strategy is based on the transient cohabitation of two different domain decomposition, exploiting an object-oriented data structure bringing together all the data used on one subdomain into a single object. The actual update process is described on Fig. 9.The temporary memory overload is limited as expected during the updating process. It only comes from:1.the cohabitation on each MPI process of the old and new subdomain (with a limited amount of data corresponding to a fraction of the complete model),the storage of one single nodal or elementary global field on the process dedicated to the field rebuilding step (task 3.1 on Fig. 9).Transferring fields from the old domain decomposition to the new one is based on MPI collective communications (mostly MPI_ALLREDUCE function), to benefit from their optimization regarding the interconnection hardware, compared to node-to-node requests. It is verified in the following examples that the time needed to update the domain decomposition is indeed much lower than the actual computation time (see for instance the rotor/stator example of Section 3.4).All the above algorithms are implemented in EUROPLEXUS (abbreviated EPX) fast transient dynamics software, property of both Commissariat à l’Energie Atomique et aux Energies Alternatives (CEA, the French research center for nuclear energy) and Joint Research Center of the European Commission. EPX is developed within a consortium involving the co-owners and two major industrial partners, which are Office National d’Etudes et Recherches Aeronautiques (ONERA, the French aerospace labs) and Electricité de France (EDF, the French main electric company).For all the examples in the current paper (see also Section 4), the used hardware is a local 20 nodes cluster, each nodes being composed of two processors Intel Xeon E5630 (4 cores each, clock speed of 2.53GHz) sharing 48GB of Random Access Memory. Nodes are interconnected by a standard Gigabit network. This network architecture penalizes the performances, compared to high performance interconnections available on centralized supercomputers, but a local machine with exclusive access was mandatory to perform all the required detailed scalability analyses. It also pushes the implementation of the proposed parallel strategy to a high level of optimization.This first example intends to demonstrate the capabilities of the proposed parallel strategy with no non-permanent links, validating the weighted domain decomposition procedure and the management of permanent links.It consists in a 2D-plane strain tunnel with a step in its middle, filled with perfect gas and traveled by a shock wave (see Fig. 10for the complete setup). Two models are considered, the second inserting structural cylindrical obstacles in the tunnel, to test the use of heterogeneous elements formulations and materials, as well as permanent conforming fluid-structure constraints (see Fig. 1). The flow is blocked across the fluid domain boundary, except for the exit modeled using an absorbing boundary. For the case with obstacles, the ALE fluid grid is automatically rezoned to follow the structural motion. Obstacles are blocked on their axis, so that they are not blown away by the shock wave.The model with no obstacle contains 110,466 quadrangular fluid elements, whereas the model with obstacles contains 59,904 quadrilateral fluid elements and 44,836 triangular structural elements.The pressure fields for both cases, at respective times of 0.5ms, 1ms, 1.5ms and 2ms, are given on Fig. 11. For the case with obstacles, the structural displacements are amplified by a 5 factor, to emphasize the fluid-structure interaction.Fig. 12shows the obtained speed-ups for both simulations (for a final time of 5ms) from 1 to 32 MPI processes. In the case with obstacles, the results with and without weighting procedure for domain decomposition are given.Due to the small size of these tests, the parallel efficiency (i.e. the ratio between the actual speed-up and the theoretical speed-up) logically saturates for more than 16 processes. The weighted domain decomposition is about 13% more efficient in the case of fluid-structure interaction and heterogeneous element formulations and materials.The obtained results confirm the satisfactory behavior of the proposed parallel approach for such cases with no non-permanent kinematic constraint.The model is composed of a rotating wheel losing some of its blades, which impact the surrounding carter and as well as the following blades still attached to the wheel (see Fig. 13for the design and initial configuration, courtesy of ONERA – see acknowledgements). The release scenario of the blades is imposed, the main object of such simulations, often confidential and out of the scope of the current paper, being the analysis of the retention of the flying blades by the carter. Due to the confidentiality issues, the design of the carter is modified, as well as the material of the blades.All displacements are blocked along the carter’s boundary and the inner surface of the wheel is forced to rotate along its axis at the speed of 3.27.103 rad.s-1. It thus takes approximately 2ms for the rotor to complete a rotation. The release scenario for the blades mainly consists in the first release of a single blade after 0.1ms, followed by a grouped release of 6 blades at after 0.3ms. The elements of the blades reach failure when their equivalent plastic strain grows beyond 80%.The model is composed of 342,608 hexahedral finite elements for the wheel and the blades, as well as 16,000 quadrangular shell finite elements for the carter. A first simulation is performed to compute the stationary initial state of the rotor due to centrifugal forces. It is not considered in the performance analysis, since it does not present any difficulty for the parallel solver.The results of the transient simulation for 2ms (∼270,000 time cycles) are shown on Figs. 14 and 15. As expected, the released blades impact both the carter and the following blades and experience failure and fragmentation in the process. Contact is considered between all the blades (with potential self-contact for each blade) and between the blades and the carter. Close-up views on Fig. 15 illustrate the need for a high precision contact algorithm, due to complex geometrical situations and high velocities, which the proposed dual framework provides with absolutely no additional parameter, i.e. only the definition of the entities potentially in contact is needed. Contact occurs from the outset of the simulation between blade roots.This test is highly challenging for the proposed parallel strategy, due to the large relative displacements between the rotor and the stator on the one hand, and to the great number of contact constraints appearing during the simulation on the other hand.The parallel performances are given on Fig. 16a for simulations from 4 to 32 MPI processes. The order of magnitude for the time needed for the reference sequential simulation is 330h (i. e. approximately two weeks), whereas the time needed for the same calculation with 32 processes is about 20h.The speed-ups obtained with the original approach combining a static domain decomposition and a centralized solver for the global constraints are compared to those provided by the proposed alternative strategy, i.e. an updated domain decomposition and a distributed solver for the global constraints. For the special case of 16 MPI processes, an intermediate combination is considered, implementing only the updated decomposition while keeping the centralized solver for the global constraints. The physical time between two updates of the domain decomposition is 0.05ms.As expected, the scalability saturates very quickly with the original approach, the speed-up with 16 processes being lower than that for 8 processes, which already exhibits a poor efficiency. On the contrary, the new proposed strategy provides a satisfactory scalability up to 16 processes. For 32 processes, the speed-up is still growing, but the efficiency drops, mainly due to poor load-balancing in the contact detection. With the reduction of the time needed for the elementary work, this computational task becomes dominant and is currently not taken into account in the domain decomposition weighting procedure (see prospects on this subject in Section 5).The case with 16 processes shows that updating the domain decomposition provides the major acceleration in the solution, the use of the distributed solver for the global constraints yet ensuring an additional significant 35% increase in the efficiency. These results are illustrated by Fig. 16b, where the elapsed time for 100 cycles of the simulation is plotted against the physical simulated time. The dramatic increase in the cycle cost with the static domain decomposition is explained on Fig. 17, where the shape of one subdomain is shown at initial time and after 1ms, with the associated bounding box used to select the remote nodes which must be considered for contact inside this subdomain. Most of the contact constraints involving this subdomain after 1ms are global, yielding massive communications, for both data transfers for remote contact detection and centralized force computation. Logically, a slight improve in this situation is observed when the wheel has performed a complete rotation. The performances with static domain decomposition remain poor anyway due the advanced degradation of the rotor.On the contrary, the updated decomposition achieves the expected control over the cycle cost during the entire simulation. The steps in the curves on Fig. 16b corresponds to the instants when the domain decomposition is rebuilt. Each time, it reduces the amount of data to be transferred for contact detection between remote and local entities, explaining the small decrease of the cost per cycle. The choice of the update rate is arbitrary in the current case. It should be set automatically as an optimum between the actual cost of the update and the gain in performance it implies (see again Section 5). For instance, the average number of cycles performed here between two consecutive updates is 5000, corresponding to an approximate elapsed time of 1000 seconds. Compared to the 15s required for the update of the domain decomposition for this model, this means that a higher update rate could have been chosen, reducing the observed steps. This does not change the conclusions of the current demonstration.Finally, it is noticeable that the use of the distributed solver for the global contact constraints proves more and more useful as the simulation progresses, completely fulfilling the objective of controlling the cost per cycle, even with the advanced ruined state of the structure.Among the major industrial applications of the proposed computational framework evoked in the introduction, the current section focuses on the case of public infrastructures submitted to blast loading. Simulations implementing this strategy in the field of accidental transient situations for nuclear vessels can be found in [24,25].The model is composed of a single car from a subway train, surrounded by massive structures from a simplified station (the platform, the roof, one pillar and a partially open separation wall, see Fig. 18). The design of the station is arbitrary, whereas the model for the car comes from the Joint Research Center of the European Commission (see acknowledgements). Like in Section 3.4, due to confidentiality issues for this kind of simulations, the material properties used for the train car are slightly changed compared to those used in the original work. Detailed quantitative results are not given.The loading corresponds to a TNT charge, represented through an equivalent compressed gas bubble, located in the middle of the car. Again, due to confidential issues, the exact state of the bubble is not given. The metallic frame of the train car is composed of a three-layered elasto-plastic material. The windows are also modeled with a three-layered material, the two outer layers being composed of a specific brittle glass material for which only the common parameters are given, and the inner layer being composed of a soft elasto-plastic material bearing large strains to prevent the windows from fully fragmenting under the blast loading.The mesh contains 1,185,252 hexahedral elements for the fluid (i.e. the average cell size is 6.5cm), 119,160 triangular shell elements for the train (main frame and windows) and 19,730 hexahedral elements for the pillar and the separation wall. The roof and the platform of the station are represented with 18,648 quadrangular shell elements: these elements are ignored for elementary computations, since these structures are considered rigid, but they are taken into account in the contact between the train car and the station, as well as in the fluid–structure interaction.Fig. 19first shows the deformed shape of the structures during the simulation (for a final time of 20ms). The rigid station roof is hidden to improve the visibility of the train car. Due to the overpressure, the latter experiences failure at the junction between its own walls and roof. The windows are partially cut off the metallic frame in the vicinity of the initial explosive gas bubble. As time grows, the structural fragments of the car come in contact with the pillar on one side of the train and with the separation wall on the other side. In this latter situation, the fragments are folding up around the wall, which is made possible by the complete failure of the glass layers of the windows, leaving only the soft elasto-plastic interlayer to keep the pieces together, with no resistance to bending.Fig. 20shows the propagation of the overpressure in the fluid-structure system, using a volume rendering of the pressure field in the fluid domain. The pressure wave is first mostly guided by the frame of the train car before its failure, and then, it propagates in the entire volume, reflecting in particular on the station pillar after 5ms.Close-up views provided on Fig. 21illustrate both the interaction between the fluid and the failing car structure (using a slice-view of the fluid domain) and the contact occurring between the expelled doors of the car and the separation wall. Like for the rotor/stator state, high accuracy is achieved for the kinematic constraints (both fluid-structure interaction and contact) with no non-physical parameter and very simple inputs for the software.The parallel performances from 4 to 96 MPI processes are shown on Fig. 22. The reference sequential calculation requires about 100h (i. e. approximately 4days and a half). The same simulation with 96 processes and the proposed parallel framework requires about 4h.Compared to the rotor/stator, this test is slightly less challenging for the centralized approach for global kinematic constraints, since the structures moves slower and the structure of the condensed operator can be kept constant during several cycles. For the same reasons, the gain associated to the updated domain decomposition is smaller, and is thus not separated from the gain coming from the distributed solver for global constraints. In the proposed simulations implementing the new parallel strategy proposed above, the domain decomposition is updated every 2ms.The original approach (i.e. static domain decomposition and centralized solver) performs well from 4 to 16 MPI processes, since the gain in the speed-up is only 15% with the new approach for 16 processes, for respective efficiencies of 60% and 70%.For higher number of processes, two phenomena occur simultaneously. First, the centralized solving strategy reaches its limit and the gap between the original and the new approaches jumps to 45% for 32 processes and keeps increasing after. Second, the Gigabit network strongly saturates, penalizing the distributed solver, which trades centralized computations for updates of remote entities during the internal iterations of the Preconditioned Conjugate Gradient algorithm. A high-performance network is strongly needed to sustain the parallel efficiency in this situation. Anyway, the proposed approach manages to achieve a growing scalability up to 96 processes, with the hardware limits of the test cluster severely challenged.The present paper describes a parallel algorithm for fast transient fluid-structure dynamics, designed to handle the computation of forces associated to kinematic constraints (mainly fluid-structure interaction and unilateral contact) through Lagrange Multipliers, avoiding the use of any non-physical parameter, such as penalty coefficients, in the solution procedure.It demonstrates the significant increase of algorithmic complexity induced by the dual approach and introduces innovative strategies to still provide a both generic and efficient parallel framework. The performances of the proposed solution are validated on significant examples, with detailed scalability analyses and satisfactory results up to 96 MPI processes for a strongly coupled fluid-structure simulation.The current work is completed by a shared memory processing (SMP) strategy to be used inside subdomains, to address higher numbers of cores without increasing the number of subdomains beyond reasonable limits, given the complexity of the problem to solve when it involves large numbers of remote entities. This sticks to the nature of current supercomputers, which, except from particular components such Graphical Processing Units, can be seen as clusters of growingly large SMP nodes. As far as EPX software is concerned, SMP computation is achieved through of the KAAPI library (INRIA, http://kaapi.gforge.inria.fr). This library implements data flow graph programming, as well as flexible and versatile work stealing algorithms [26], mandatory to ensure load balancing between the SMP threads for the complex fluid-structure problems described in the above sections.Large scale simulations have been performed using the hybrid MPI/KAAPI strategy through a PRACE Preparatory Access on the TGCC/Curie supercomputer (ranked 11 in Top 500 in December 2012, http://www-hpc.cea.fr/fr/complexe/tgcc-curie.htm), resulting in scalable simulations from 32 to 1024 cores, yet with a parallel efficiency that does not match the expectations. The origins of this remaining lack of efficiency are quite known and illustrated by Fig. 23on the rotor/stator case introduced in Section 3.4. Different parallel configurations are considered, from 4 to 16 cores and the time needed to perform the main computational tasks for 5000 time cycles.1. KAAPI ensures load balancing as expected but some tasks scale better than others (for instance, Contact detection compared to Elementary operations).This is due to memory accesses inside the tasks: Elementary operations wait for data loaded from the central memory, whereas the Contact detection task is only bound by the geometrical operations to be performed. The solution is a better use of the cache memory in EPX, which is currently implemented.2. Classically with SMP parallel computations, the program goes through a succession of parallel and sequential sections. On Fig. 23, the residual sequential work is referred to as Other computations. The time needed for work is not reduced by adding threads, limiting the potential speed-up (known as Amdahl’s law [27]).Within the proposed hybrid approach, the improvements of the two complementary parallel approaches (i. e. MPI distributed memory and KAAPI shared memory) are totally independent, which greatly simplifies the development and optimization processes.Another main current research topic is the automatic adaptation of internal parameters of the parallel strategy. Some of these parameters have been introduced in Section 3.4, related to the weighting procedure and the update rate for the domain decomposition. KAAPI comes with its own parameters influenced by the hardware configuration, such as cache memory size or memory bandwidth. This short list is obviously not exhaustive. One key idea of this starting work is to take advantage of the large number of performed time cycles to adjust a posteriori the identified parameters using actual measures made during the first cycles of the simulation, yielding optimal performances for the rest of the calculation. The parameters may have to be adjusted again during the simulation, if the topology of the considered system is greatly modified (see the rotor/station and the subway explosion examples for instance).

@&#CONCLUSIONS@&#
