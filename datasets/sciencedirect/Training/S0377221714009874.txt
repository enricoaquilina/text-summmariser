@&#MAIN-TITLE@&#
Decomposition based hybrid metaheuristics

@&#HIGHLIGHTS@&#
Decomposition based hybrid metaheuristics are surveyed.Prominent design templates of hybrid metaheuristics are summarized.Focus on Lagrangian decomposition, column generation, and Benders’ decomposition.Promising future research directions are pointed out.

@&#KEYPHRASES@&#
Combinatorial optimization,Metaheuristics,Mixed integer programming,Hybrid optimization approaches,Decomposition techniques,

@&#ABSTRACT@&#
Difficult combinatorial optimization problems coming from practice are nowadays often approached by hybrid metaheuristics that combine principles of classical metaheuristic techniques with advanced methods from fields like mathematical programming, dynamic programming, and constraint programming. If designed appropriately, such hybrids frequently outperform simpler “pure” approaches as they are able to exploit the underlying methods’ individual advantages and benefit from synergy. This article starts with a general review of design patterns for hybrid approaches that have been successful on many occasions. More complex practical problems frequently have some special structure that might be exploited. In the field of mixed integer linear programming, three decomposition techniques are particularly well known for taking advantage of special structures: Lagrangian decomposition, Dantzig–Wolfe decomposition (column generation), and Benders’ decomposition. It has been recognized that these concepts may also provide a very fruitful basis for effective hybrid metaheuristics. We review the basic principles of these decomposition techniques and discuss for each promising possibilities for combinations with metaheuristics. The approaches are illustrated with successful examples from literature.

@&#INTRODUCTION@&#
Difficult combinatorial optimization problems are in practice frequently approached by means of metaheuristics. This term has originally been introduced by Glover (1977) and essentially refers to a broad class of problem-independent strategies for approximate optimization and problem solving. In fact, the boundaries of this class are today somewhat fuzzy, but typically cited representatives include neighborhood-search-based strategies like simulated annealing, tabu search, and variable neighborhood search, population-based methods like evolutionary/genetic algorithms and scatter search, and construction-oriented techniques like the greedy randomized adaptive search procedure and ant colony optimization.When one is confronted with a non-trivial combinatorial optimization problem from practice that should be solved “as well as possible”, a typical approach is to start with a relatively simple constructive heuristic and try to improve obtained solutions by means of local search. To overcome the trap of local optimality, which inherently appears in almost all non-trivial problems, metaheuristics are frequently the next step. Assuming one is still not satisfied with the obtained solutions, one will then typically try to refine the so far developed approach by tuning parameters and certain design decisions, but from a conceptual point-of-view even more important by identifying special characteristics of the problem at hand and finding effective possibilities to exploit them. In this way, one frequently ends up with a more complex solver that is not a straightforward, “pure” instantiation of a classical metaheuristic anymore, but rather a combination of cleverly chosen techniques that might even stem from diverse algorithmic research fields. This is what we refer to as hybrid metaheuristic (HM).The obvious motivation behind such hybridizations is to obtain better performing systems that exploit and unite advantages of the individual components, i.e., to benefit from synergy. The large number of publications on HMs and dedicated scientific events such as the ongoing series of Workshops on Hybrid Metaheuristics(Blum, Roli, and Sampels, 2004) and Workshops on Matheuristics(Maniezzo, Hansen, and Voss, 2006) document the popularity, success, and importance of this specific line of research. In fact, today it seems that choosing an adequate hybrid approach is determinant for achieving top performance in solving many real-world problems.This does, of course, not imply that more complex approaches are always the better choice. Increased complexity also comes with disadvantages: The software is more difficult to maintain and tune and adaption in problem specifications are frequently harder to adhere. Thus, a still very valid design goal also is to keep an optimization approach as simple as possible, and include extensions only if they indeed provide significant benefits. Such decisions are often difficult, and it shall be explicitly remarked that sometimes, especially when development time is limited or a large versatility is required due to expected changes in problem specifications, a simpler, pure approach might be the wiser choice.The idea of hybridizing different optimization approaches is not new but dates back to the origins of metaheuristics themselves. For a long time, however, such hybrids were not so popular since several separated and even competing communities of researchers existed who considered “their” favorite class of optimizers “generally best” and followed specific philosophies too dogmatically. It is mostly due to the no free lunch theorems (Wolpert and Macready, 1997) that this situation fortunately changed and people recognized that there cannot exist a general optimization strategy that is globally best. To solve a problem at hand most effectively, it almost always requires a specialized algorithm that needs to be compiled of adequate parts.Several publications exist that try to classify HMs or provide guidelines for their design. Talbi (2002) gives a general taxonomy, Cotta, Talbi, and Alba (2005) concentrate on parallel hybrids, and El-Abd and Kamel (2005) consider cooperative search strategies. Combinations of metaheuristics with exact optimization techniques are particularly addressed by Puchinger and Raidl (2005). Talbi (2013b) further describes a unified taxonomy of HMs with mathematical programming, constraint programming, and machine learning. More recent general surveys on HMs pointing out prominent design principles are provided by Raidl, Puchinger, and Blum (2010) and Blum, Puchinger, Raidl, and Roli (2011).Fig. 1illustrates the top levels of a basic taxonomy as suggested by Raidl (2006). First, we might distinguish what is hybridized: concepts of different metaheuristics; metaheuristics with problem-specific algorithms like simulations; metaheuristics with other techniques e.g. from operations research or artificial intelligence; or metaheuristics with human interaction. A second criterion for distinguishing HMs is the order of execution of the individual parts, which can be strictly sequential (i.e., a batch approach), interleaved, or parallel. A third criterion is what we call control strategy: In an integrative approach, we have some kind of master algorithm in which one or more subordinate components are embedded or called. In contrast, in collaborative hybrids individual components may run more independently and exchange information in some way. Last but not least a fourth criterion for characterizing HMs is the level of hybridization: In a high-level hybrid, individual components are coupled rather weakly and typically not so much communication takes place between them, while a low-level hybrid has strongly interwoven parts. For more details on such taxonomies we refer to the above publications; here we just introduced the nomenclature we will use later.When considering combinations of metaheuristics with other optimization techniques, hybrids with mathematical programming approaches became particularly popular over the last decade. The reason behind this is that especially classical mixed integer linear programming (MIP) techniques have pros and cons that are complementary to those of metaheuristics to a large degree. When a problem can be expressed by means of a linear objective function and linear constraints, chances are good that at least small instances can be solved to proven optimality by a state-of-the-art general purpose MIP solver like CPLEX,22http://www-01.ibm.com/software/integration/optimization/cplex-optimizer.GUROBI,33http://www.gurobi.com.or the freely available SCIP.44http://scip.zib.de.While these modern solvers are sometimes astonishingly effective in solving smaller problems or problems with certain structures, they are also known to typically scale rather poorly. When it comes to large instances of hard problems running times or memory requirements often become intractable. Hybrid metaheuristics might exploit the advantages of MIP solvers, e.g., by applying them to smaller subproblems while relying on the power of metaheuristic search as outer framework. Combinations of metaheuristics and mathematical programming techniques are frequently also termed matheuristics; see Raidl and Puchinger (2008) for a survey and Maniezzo, Stützle, and Voss (2009) for a comprehensive book on this topic. Further recommended books covering many more general aspects and applications of HMs are by Blum, Blesa Aguilera, Roli, and Sampels (2008) and Talbi (2013a).The following section reviews major classical design patterns of HMs that have proven to be successful already on many occasions. The main part of this article will concentrate on a class of hybrids which is not that commonly found but we nevertheless consider highly promising for many large real-world problems: Approaches that are based on decomposition techniques having the origins in solving large (mixed integer) linear programs: Lagrangian decomposition, Dantzig–Wolfe decomposition utilizing column generation, and Benders’ decomposition. Sections 3–6 overview the principles of these techniques and discuss possibilities of exploiting them in a hybrid metaheuristic context. Section 7 concludes this article and points out promising future research directions.The following design patterns describe typical strategies for obtaining hybrids by combining different simpler or more classical algorithms or concepts of such. This section summarizes the presentation of Raidl et al. (2010) and extends it with recent developments. Note that frequently, these design patterns also appear in combination, and sometimes there is also no clear borderline of what one calls a “real” hybrid and where a described pattern is actually part of some classical optimization approach.The probably most natural way of boosting many optimization algorithms is by supplying an already good starting solution. Such initial solutions might be obtained from problem-specific constructive heuristics or essentially any other optimization approach that is considered fast enough. Note that also classical exact optimization methods like branch-and-bound typically depend heavily on a good initialization in order to be able to prune the search space effectively from the very beginning. For population based metaheuristics like evolutionary algorithms it is crucial to start with a set of initial solutions that is diverse enough in order to avoid premature convergence. Multiple initialization procedures may be utilized, or a deterministic procedure might be randomized in order to obtain different promising starting solutions. Note that the latter approach is also systematically applied in the Greedy Randomized Adaptive Search Procedure (GRASP) (Feo and Resende, 1995).Another extremely commonly applied pattern is to embed some improvement method, e.g., a classical local search or more advanced metaheuristic like simulated annealing or tabu search, into a master algorithm. Such an approach thus follows an integrative control strategy. A typical representative that uses this pattern as main distinguishing feature is the memetic algorithm (Hart, Krasnogor, and Smith, 2005; Moscato, 1999), which essentially is an evolutionary algorithm including some local improvement technique that is applied to all or part of the newly created solution candidates of each iteration. In this way intensification of the heuristic search is frequently successfully enhanced. As another example, variable neighborhood search approaches (Hansen, Mladenovic, and Perez-Britos, 2001) can be seen to systematically extend this pattern by applying a series of neighborhood searches in a nested way. Last but not least, also classical branch-and-bound often relies on primal local improvement procedures for finding sooner better heuristic solutions and corresponding global bounds in order to prune larger parts of the search space.This kind of hybrids solves some larger problem by decomposing the whole optimization into multiple stages that are addressed by individual techniques in a typically sequential manner.Especially more complex real-world problems frequently involve hierarchical decisions and corresponding sets of variables. A first stage of optimization may then be used to fix higher-level decisions while the remaining lower-level variables are determined in one or more successive phases where the higher-level variables are considered fixed. As an example consider some vehicle routing problem (Fisher and Jaikumar, 1981): The principal decisions which customers are visited by which vehicle might be considered the upper level which is decided first, e.g., according to a clustering. Further details such as the specific vehicle routes and the exact timing of the visits may be optimized in a second stage, where we have the advantage that the problem decouples into independent subproblems for all the vehicles. Obviously the stages in such approaches are in general not independent and suboptimal solutions are therefore usually obtained. Nevertheless, for very large and difficult problems such approaches may be meaningful or even be the only practically reasonable possibility. Note, however, that in the context of vehicle routing problems also the alternative possibility of deciding an order for visiting all customers in a first stage, i.e., building one large route, and splitting it into several tours in the second stage can be effective, especially as the splitting step might be done efficiently via dynamic programming; Prins, Lacomme, and Prodhon (2014) presents a review on such approaches.As another example, consider the design of large telecommunication networks, where the decision process will again be hierarchical: Only after an optimization of the general layout and structure, technical details like link capacities and properties of routers will be derived (Martins and Ribeiro, 2006).Multi-stage approaches may be meaningful even for problems that do not have a natural hierarchy of decision variables. Multi-level refinement strategies (Walshaw, 2008) apply a recursive coarsening to create a series of approximations to the original problem. An initial solution is then identified for the coarsest level and iteratively refined at each level – coarsest to finest – until a solution for the original problem is obtained. Iterated multi-level algorithms extend this concept by iteratively re-coarsening the problem based on obtained solutions; in this way poor decisions in the coarsening may be redeemed. Multi-level refinement strategies have been successfully applied on problems such as multilevel graph partitioning, graph coloring, and very large vehicle routing problems. In general, they are a promising technique for improving the scalability of some optimization algorithm. As a recent example, Valejo, Valverde-Rebaza, Drury, and Lopes (2014) apply multi-level refinement to partition social network graphs and suggest a coarsening method exploiting the neighborhood similarity.Another kind of multi-stage hybrids are approaches involving (advanced) preprocessing techniques as well as kernelization methods. Especially in the latter, the original problem is reduced in polynomial time to a so-called problem kernel such that an optimal solution to it can, in polynomial time, be transformed back into an optimal solution to the original problem. Heuristic variants of such methods are variable fixing approaches, where in early stages variables are identified that are very likely to have certain values in good or optimal solutions and these variables are correspondingly fixed for the later stage(s). For examples of kernelization approaches to the vertex cover problem see Gilmour and Dras (2006) and to the multi-dimensional knapsack problem see Angelelli, Mansini, and Grazia Speranza (2010); Puchinger, Raidl, and Pferschy (2010).Especially in the field of evolutionary algorithms it is popular to represent candidate solutions in some indirect way, in this context often called genotype, and use some problem-dependent decoding algorithm to obtain the corresponding “real” solution (phenotype). The advantage of such an approach is mainly that the variation operators to construct new candidate solutions can be chosen more or less independently of the particular problem. Problem specificities like special constraints can often be nicely hidden from the optimization algorithm and possibly efficiently be handled by means of the decoder. Sometimes, genotypes represent only incomplete solutions, i.e., not all aspects are specified, and an “intelligent” decoding procedure augments the missing values by solving some remaining subproblem. Such approaches have been successfully used especially for (multi-dimensional) cutting and packing as well as scheduling and timetabling problems, where direct representations are often difficult to handle due to the many complex constraints. Indirect representations are in such cases frequently based on permutations, and decoders are derived from construction heuristics considering the objects in the orders specified by the permutations (Kellerer, Pferschy, and Pisinger, 2004).As another example consider solving a MIP involving integer variables as well as continuous variables. A metaheuristic might be used to tackle the integer variables only, while optimal values for the continuous variables are derived for each integer-candidate solution efficiently by means of a linear programming solver.So-called hyper-heuristics(Burke et al., 2013) also make heavy use of ideas similar to decoder-based approaches: They operate on a search space of heuristics or heuristic components rather than directly on the space of solutions in order to find/compile a heuristic that best solves the target problem.Sometimes local search is enhanced by using particularly large neighborhoods that are not investigated by naive enumeration but some more efficient algorithm. If the neighborhood structure and the corresponding algorithm are chosen appropriately, large portions of the search space might be efficiently covered. Such techniques are generally known as large neighborhood search(Shaw, 1998) or very large-scale neighborhood search (Ahuja, Ergun, Orlin, and Punnen, 2002).Many of today’s combinations of metaheuristics with MIP approaches follow this scheme as it is relatively straightforward to apply once a compact MIP model is available for the problem at hand: Part of the variables are fixed to the incumbent solution’s values and the others are kept open and optimized via a MIP solver. Of course the selection of the variables to be kept open and optimized might be crucial. Typically, they are selected either randomly or according to some greedy heuristic trying to identify weak parts of the solution. Often it is also meaningful to select variables that are strongly related together. For examples see Büdenbender, Grünert, and Sebastian (2000); Lopes, Morais, Noronha, and Souza (2014); Prandtstetter and Raidl (2008).Besides MIP solvers, also constraint programming is sometimes applied as, e.g., described by Shaw (1998) for vehicle routing problems and more recently by Di Gaspero and Urli (2014) for homecare scheduling problems. Furthermore, dynamic programming frequently is a promising candidate for identifying best solutions in large neighborhoods, providing the neighborhood structure is suitably chosen.In the context of problems where objects need to be partitioned into disjoint sets, such as vehicle routing problems, machine scheduling problems, and other assignment problems, cyclic and path exchange neighborhoods can be highly effective (Ahuja et al., 2002). In these neighborhoods a series of objects is exchanged among an arbitrary number of partitions in a cyclic or path-like fashion; the best move is determined by constructing an improvement graph where an arc corresponds to the movement of a single object and applying a shortest path-like algorithm.Obviously, it is not necessary to always find a best solution within a large neighborhood as it is the case in the above examples (except when prematurely terminating the MIP solver or dynamic programming). Sometimes also simpler but faster heuristics such as greedy constructive methods are used to assign promising values to the open variables. Such approaches are frequently called destroy-and-recreate or removal-and-insertion techniques, as a solution is partially dissolved and the missing parts are redetermined. A good example is the adaptive large neighborhood search heuristic for pickup and delivery problems with time windows (Ropke and Pisinger, 2006), which involves several competing types of large neighborhoods and corresponding sub-heuristics whose application is controlled by their historic performance. In fact, this principle of adaptive large neighborhood search turned out to work particularly well on a larger variety of vehicle routing problems and became quite popular over the recent years. For example, Hemmelmayr, Cordeau, and Crainic (2012) describe such an approach for two-echelon vehicle routing problems in city logistics and Azi, Gendreau, and Potvin (2014) for a vehicle routing problem with multiple routes per vehicle.Last but not least, note that decoder based approaches discussed in Section 2.4 might sometimes also be interpreted as large neighborhood search techniques, especially when incomplete solution representations in conjunction with more complex decoders are used.Here, the idea is to derive a new, possibly better solution from the properties (i.e., variable values) contained in two or more input solutions. The observation that high-quality solutions usually have many properties in common is exploited. In the simplest form this principle is applied in the classical recombination operator of evolutionary algorithms, where the attributes to be inherited are typically chosen in a computationally cheap, random fashion. A more advanced, but also computationally more expensive approach is path relinking(Glover, Laguna, and Martí, 2000), where a starting solution is more systematically transformed into a guiding solution by iteratively performing atomic changes. Thus, a path between the two solutions is traced in the search space, and a best solution on it is returned as result. For a recent example see Pessoa, Resende, and Ribeiro (2013), where path relinking is applied within a GRASP for set k covering.Even more systematic are optimal merging procedures, where the subspace of all solutions that can be constructed out of the properties appearing in a set of given input solutions is considered and a best solution returned. Depending on the underlying problem, identifying such an optimal offspring might be a hard optimization problem on its own, but due to the typically limited number of different properties appearing in the parents, it can often be solved in practical time. Applegate, Bixby, Chvátal, and Cook (1998) were one of the first describing such an optimal merging: For the traveling salesman problem, they derive a set of different high-quality tours by means of the chained Lin-Kernighan iterated local search algorithm. The sets of edges of all these solutions are merged and the problem is finally solved to optimality by means of a MIP approach on this strongly restricted graph. While merging appears in this example as second stage in a sequential two-stage approach and is only performed once, there are also cases of intertwined hybrids where optimal merging is used as a more systematic variation operator replacing classical recombination. Such operators are then also called optimal recombinations. For example, Blum and Blesa (2008) apply solution merging within a large neighborhood search for the k-cardinality tree problem, and Eremeev (2008) studies optimal recombination operators for the travelling salesman problem. Also the commercial MIP solver CPLEX contains optimal merging as heuristic procedure for obtaining improved incumbent solutions (Lodi, 2013).Many successful HMs exploit information on promising areas of the search space obtained by other techniques by intensifying or restricting the heuristic search to these regions.Problem relaxations are most frequently used for such purposes: Some of the problem’s constraints are dropped in order to obtain a relaxation that can be solved efficiently. The obtained solution may then be a good guiding point. If a (compact) MIP formulation exists for the problem at hand, its linear programming (LP) relaxation often is an obvious choice. An obtained fractional solution can frequently be rounded or in some other way repaired to obtain a feasible integral solution in its proximity, variables that have already integral values in the LP solution might be fixed, or LP values may be used to bias variation operators, e.g., by choosing close variable values with higher probabilities. Occasionally, dual variable information of LP solutions may provide even more helpful guidance. Chu and Beasley (1998), for example, make use of it in a genetic algorithm for the multi-dimensional knapsack problem by calculating so-called pseudo-utility ratios for the primal variables and using them in similar ways as described above for the primal LP solution values. Also Puchinger et al. (2010) point out that at least for this problem pseudo-utility ratios are significantly better indicators for the likeliness of the corresponding items to appear in an optimal integer solutions than primal LP variable values.Apart from the LP relaxation also other relaxations are sometimes exploited in conjunction with metaheuristics. Besides problem-specific approaches, Lagrangian relaxation has been particularly successful (Haouari and Siala, 2006; Jeet and Kutanoglu, 2007; Leitner and Raidl, 2008; Pessoa et al., 2013). In comparison to the LP relaxation, Lagrangian relaxation has the advantage of frequently yielding tighter optimality bounds. This, however, comes at the cost of a usually higher computational effort. We will consider Lagrangian relaxation in more detail in the context of decomposition approaches in Section 4.Also other information is sometimes exploited for guiding metaheuristics, e.g., when constructing candidate solutions, lower and/or upper bounds determined for partial solutions (Dowsland, Herbert, Kendall, and Burke, 2006) or reduced variable domains determined by constraint propagation techniques (Meyer and Ernst, 2004). More generally, in collaborative approaches solution migration can also be considered a technique where one optimization task provides guidance for another.Branch-and-bound is a fundamental tree-search approach for solving difficult optimization problems systematically to proven optimality. It relies on the calculation of lower and upper bounds for partial solutions in order to prune the search tree as far as possible. As already mentioned, good initial solutions and local improvement techniques frequently play an important role to obtain primal bounds. Besides these aspects, principles of local search based metaheuristics are sometimes mimicked by special control strategies for selecting the tree nodes (i.e., open subproblems) to be processed next or special branching strategies focusing the tree search to the neighborhoods of promising solutions.Danna, Rothberg, and Le Pape (2005) proposed guided dives, where the tree search temporally switches to a depth-first strategy and always considers next a subproblem where the branching variable has the value of the incumbent solution. Guided dives are repeatedly applied in regular intervals, results indicate a strongly improved heuristic performance.Fischetti and Lodi (2003), on the contrary, suggested local branching. Given an incumbent solution again, branching is performed by adding on the one hand a so-called local branching constraint that restricts the search space to the incumbent’s k-OPT neighborhood and on the other hand its inverse representing the remaining search space. The MIP solver is then forced to completely solve the k-OPT neighborhood before considering the remaining open nodes of the branch-and-bound tree. If an improved solution has been found, a new subproblem corresponding to the k-OPT neighborhood of the new incumbent is split off, otherwise a larger k may be tried. When no further improvements are achieved, the remaining problem is processed in a standard way. While local branching is often beneficial, it was also shown that the addition of the inverse local branching constraints frequently is counterproductive as many of these dense constraints degrade performance.Danna et al. (2005) further proposed relaxation induced neighborhood search (RINS), where occasionally a sub-MIP is spawned from a search-tree node corresponding to another special neighborhood of an incumbent solution: Variables having the same values in the incumbent and the current solution to the LP relaxation are fixed and an objective value cutoff corresponding to the current LP value is set. The subproblem on the remaining variables is then solved with limited time. In the authors’ experimental comparison of guided dives, local branching, and RINS on a collection of MIP models originating from diverse sources including job-show scheduling, network design, crew scheduling, lot-sizing, and railway line planning problems and MIPLIB-3.0,55http://www.or.deis.unibo.it/research_pages/ORinstances/MIPs.html.RINS performed best; it has been included in CPLEX as a standard strategy. More recently, Gomes, Santos, and Souza (2013) suggested an extension of RINS that explicitly explores pre-processing techniques. This method systematically searches for a suitable number of fixations to produce subproblems of controlled size, which are explored in a variable neighborhood descent fashion.Also somehow related are metaheuristics that utilize a complete solution archive to avoid reconsiderations of candidate solutions. Raidl and Hu (2010) proposed such an extension for a genetic algorithm based on a trie data structure, which actually closely resembles an explicitly stored branch-and-bound tree. When an already evaluated solution would be reconsidered, it is efficiently transformed into a usually similar but guaranteed new solution by appropriately traversing the trie. In this way, the metaheuristic is in principle turned into an exact tree-search based approach. Such a solution archive may be particularly effective in cases where the solution evaluation is expensive or decoder-based approaches with incomplete representations are used.Problem decomposition techniques are a class of approaches particularly aimed at solving very large or complex problems, frequently also involving different types of variables. The basic idea is to solve such a large problem by solving a series of smaller problems and appropriately combining the results.In fact, the previous sections already introduced several such techniques: Sequential multi-stage methods described in Section 2.3 follow this principle in a straightforward way. Large-neighborhood search methods, cf. Section 2.5, obviously also fall into this category and in general have the advantage to avoid the problem of fixing possibly suboptimal decisions too early. The same holds for advanced solution merging strategies, cf. Section 2.6, which in fact might also be considered special large neighborhood search approaches where the neighborhood is induced by more than one parent solutions.Certain metaheuristics have been suggested that explicitly make decomposition to their primary principle: Variable neighborhood decomposition search(Hansen et al., 2001) extends classical variable neighborhood search by fixing parts of incumbent solutions and applying some improvement method to the corresponding subproblems. Partial Optimization Metaheuristic Under Special Intensification Conditions (POPMUSIC) from Taillard and Voß (2001) is another general approach to solve very large problems by iteratively solving smaller parts with an effective problem-specific method. A more specific, recent example where such an approach has been applied to efficiently compute Steiner trees on large graphs is given by Leitner, Ljubić, Luipersbeck, and Resch (2014). Decomposition guided variable neighborhood search(Fontaine, Loudni, and Boizumault, 2011) utilizes the graph of clusters provided by a tree decomposition of the constraints graph to guide the exploration of large neighborhoods within a variable neighborhood search. More recently, Loudni, Fontaine, and Boizumault (2013) improved this method by new strategies for better controlling intensification and diversification, and Ouali, Loudni, Loukil, Boizumault, and Lebbah (2014) applied this method in a cooperative parallel way for solving weighted constraint satisfaction problems. Last but not least, the already mentioned multi-level refinement strategies(Walshaw, 2008), cf. Section 2.3, also fall into this category, although they follow a different bottom-up approach.All these approaches explicitly depend on the embedding of other, effective optimization procedures and can thus be said to be hybrids already by definition.In the following sections we will consider more specific decomposition approaches that build upon classical techniques coming from (mixed integer) linear programming. Lagrangian decomposition, Dantzig Wolfe decomposition with its related column generation, and Benders’ decomposition are prominent approaches for solving large LPs or MIPs having certain structures. We will review their basic principles and discuss successful and promising combinations with metaheuristic approaches. For an overview that also discusses these three MIP decomposition approaches and tries to reinterpret them as more general metaheuristic frameworks, see Boschetti, Maniezzo, and Roffilli (2009). For general in-depth introductions to MIP techniques and related topics the text books by Nemhauser and Wolsey (1988); Wolsey (1998) are recommended.Consider we are given a problem that can be modeled in the form(1)zMIP=min{cTx∣Ax≥b,Dx≥d,x∈Dn},where x is a vector of n non-negative decision variables of domainDn,cTxis the linear function to be minimized, and there are two sets of constraints Ax ≥ b and Dx ≥ d. Assume that the constraints Ax ≥ b are easy in the sense that we could solve the problem efficiently when dropping the “complicating” constraints Dx ≥ d. Such an approach would be a feasible relaxation but typically yield only a weak lower bound. Lagrangian relaxation (Fisher, 1981) replaces these constraints by corresponding penalty terms in the objective function:(2)zLR(λ)=min{cTx+λT(d−Dx)∣Ax≥b,x∈Dn}.Vector λ is the vector of real-valued Lagrangian multipliers, and for any λ ≥ 0, zLR(λ) ≤ zMIP holds; i.e., we have a valid relaxation of the original MIP (1). We are now interested in finding a specific instantiation of λ yielding the best—i.e., largest—possible lower bound, which leads to the Lagrangian dual problem(3)zLR*=maxλ≥0{zLR(λ)}.This Lagrangian dual is a piecewise linear, convex function which can usually be well solved by iterative procedures like subgradient methods; see, e.g., Barahona and Anbil (2000) for an advanced approach called volume algorithm.Given a solution λ to the Lagrangian dual problem (3) and a corresponding optimal solution x* to the Lagrangian relaxation (2) that is also feasible for the original problem (1), i.e., Dx* ≥ d, the following complementary slackness condition holds: x* is an optimal solution to the original problem (1) iff(4)λT(d−Dx*)=0.Provided the Lagrangian dual problem is solved to optimality, it can be shown that the Lagrangian relaxation always yields a bound that is at least as good as the one of the corresponding LP relaxation; in practice it often is far better.In case of Lagrangian decomposition (LD), the Lagrangian relaxation (2) decouples into a series of k subproblems that can be independently solved:(5)zLD(λ)=∑i=1,…,kmin{ciTxi+λT(di−Dixi)∣Aixi≥bi,xi∈Dni}.These subproblems can be of the same type or different.While Lagrangian relaxation in principle only yields a lower bound to the original minimization problem, it is usually not hard to extend the approach to a Lagrangian heuristic also yielding a feasible approximate solution and corresponding upper bound, e.g., by problem-specific repairing.We illustrate LD on the knapsack-constrained maximum spanning tree problem (KCMST) following Pirkwieser, Raidl, and Puchinger (2007). Consider a graph G = (V, E) with node set V and edge set E; each edge has associated a weight we≥ 0 and a price pe. The aim is to find a subgraph T = (V, E′), E′⊆E corresponding to a spanning tree whose total weight does not exceed a limit W ≥ 0 and whose total price is a maximum. Obviously, this problem is a combination of the well known minimum cost spanning tree problem (MST) when taking negative prices as costs and the 0–1 knapsack problem (KP).We express KCMST in the following way in order to apply LD:(6)max∑e∈Epexe(7)s.t.x=^aspanningtreeonG(8)∑e∈Eweye≤W(9)xe=ye,∀e∈E(10)xe,ye∈{0,1}∀e∈EBinary variables xe= yeindicate whether or not the corresponding edges e belong to the solution, i.e., e ∈ E′. In this model, duplicating x with y obviously is redundant, but it facilitates LD as the spanning tree condition (7) is expressed only on variables x and the knapsack constraint (8) only on variables y. Equalities (9) provide the linkage.By relaxing now these linking constraints (9) in a Lagrangian manner, we obtain the decomposition into the classical MST and KP:(11)zLD(λ)=max{(p−λ)Tx∣x=^aspanningtreeonG,x∈{0,1}E}+max{λTy∣wTy≤W,y∈{0,1}E}.The MST can be efficiently solved by well known algorithms like Prim’s MST algorithm, and the KP, which is only weakly NP-hard, by dynamic programming approaches like the COMBO algorithm (Martello, Pisinger, and Toth, 1999). These subproblems are iteratively solved within the subgradient procedure in order to find best possible Lagrangian multipliers λ and a corresponding upper bound for the optimal KCMST solution value.Feasible heuristic solutions are obtained as follows:•Occasionally, the intermediate spanning trees obtained by solving the MST subproblems may comply with the knapsack constraint (8), and thus we may directly obtain approximate KCMST solutions. This is not guaranteed, however.Infeasible intermediate spanning trees can be repaired, e.g. by a greedy approach that iteratively removes an edge with highest weight and reconnects the separated components by an edge with lower weight.As these possibly repaired intermediate solutions are typically only of moderate quality, it becomes natural to additionally apply some improvement heuristic. A straightforward extension is a local search utilizing an edge-exchange neighborhood, which considers all feasible single-edge replacements. Obviously, also more advanced metaheuristics might be applied here to obtain even better solutions.Besides this classical Lagrangian heuristic approach, metaheuristics may further benefit by more rigorously exploiting information obtained by the LD, in particular Lagrangian dual variable values. Haouari and Siala (2006) describe an effective strategy in the context of the prize collecting Steiner tree problem, which Pirkwieser et al. (2007) apply in a similar spirit to our example of the KCMST problem as follows.Consider a memetic algorithm for the KCMST, which represents candidate spanning trees directly by storing their edges. Initial solutions are random spanning trees, recombination is achieved by applying a random spanning tree algorithm on the merged edge sets of two parent trees, and mutation performs a random edge exchange. Any candidate tree not complying with the knapsack constraint is greedily repaired as described above. Edge-exchange local search is applied to each new offspring solution. This memetic algorithm is improved by making use of the above LD’s results as follows:•A few best solutions directly obtained by the LD are used to seed the initial population.The edge set E is reduced to only contain edges that appeared in at least one of the LD’s intermediate spanning trees (feasible or not). Thus, the memetic algorithm can also be said to act as solution merging strategy over all intermediate LD solutions, cf. Section 2.6.Most importantly, original edge profits peare replaced by reduced profitspe′=pe−λe,and recombination and mutation are biased in their random decisions to include edges with higher reduced profits more likely.Should a solution be found whose objective value corresponds to the LD upper bound, the search is terminated as the solution is proven optimal.Large-scale experiments on graphs with up to 8 000 nodes and over 23 000 edges have shown excellent results: More than 67 percent of all solutions could be solved to proven optimality, and average remaining relative gaps between the LD’s upper bounds and the found solutions’ objective values were less than 10−6. The pure memetic algorithm without the guidance by the LD could not compete at all, and the feasible solutions obtained from the pure LD were only moderately good. These surprising results of course also indicate that the KCMST problem, although NP-hard, can be solved relatively well in practice. It turned out that especially the LD’s reduced profits are an excellent indicator for how beneficial edges really are. In contrast, original edge prices pe, edge weights we, and even relative prices pe/wemay be frequently highly misleading when used for guiding heuristic search. Note that there are also some parallels to the works on the multi-constrained knapsack problem by Chu and Beasley (1998) and Puchinger et al. (2010) who used pseudo-utility ratios determined from dual LP values for guiding heuristic search.Leitner and Raidl (2008) describe another successful hybrid of LD and a variable neighborhood search in the context of a fiber optic network design problem, and Leitner and Raidl (2009) utilize large neighborhood search in combination with LD for solving the capacitated connected facility location problem. Boschetti and Maniezzo (2009) illustrate Lagrangian heuristics on the single source capacitated facility location problem and the membership overlay problem arising in the context of P2P networks. Thiruvady, Singh, and Ernst (2014) utilize LD in combination with ant colony optimization to solve a resource constrained job scheduling problem on multiple machines. By relaxing the linking constraints each machine’s scheduling problem can be solved independently, and the solution to the LD is used to effectively guide the ACO.Last but not least, we remark that (meta-)heuristics may also become attractive for solving harder subproblems in Lagrangian relaxation approaches, although one has to keep in mind that only optimal solutions to the subproblems will guarantee the validity of a bound obtained for an original problem.Dantzig–Wolfe decomposition(Dantzig and Wolfe, 1960) has originally been introduced for solving very large LPs having a special block-diagonal structure and relies on delayed column generation (CG). In mathematical programming, CG is a well-known technique to approach in particular MIP models involving an exponential number of variables. Such MIPs frequently arise in set covering or set partitioning formulations for, e.g., vehicle routing, network design, cutting and packing, and scheduling problems. Often such models can be shown to be substantially stronger than some compact formulation w.r.t. the LP relaxation. The challenge, however, is their very large number of variables which usually prohibits a direct application of a MIP solver. CG provides a possible practical solution approach at least for solving the LP relaxation of such MIP models. For in-depth information on CG we refer to Desaulniers, Desrosiers, and Solomon (2005) and Lübbecke and Desrosiers (2005).To illustrate the principles of CG, let us consider an abstract network design problem, in which we are given a graph G = (V, E). The node set V consists of a root node 0 (e.g., a central server), clients C⊂V, and possibly further nodes S = V∖{0}∖C. The edge set E represents potential links that may be installed for connecting the respective nodes at given costs ce> 0. The objective is to find a minimum cost subgraph G′ = (V′, E′), V′⊆V, E′⊆E corresponding to a network that provides for each customer c ∈ C a connection to the root 0. In the simplest form this problem corresponds to the classical rooted Steiner tree problem in which each client needs to be connected to the root by a simple path. More generally, various complicating conditions may be considered for the individual connections such as hop- or length-constraints or redundancy requirements like having some customers more reliably connected via two independent paths. Thus, our abstract network design problem covers many classes of more specific problems. We can approach it by the following connection formulation:(12)min∑e∈Ecexe(13)s.t.∑p∈Pkfpk≥1∀k∈C(14)xe−∑p∈Pk∣e∈pfpk≥0∀k∈C,e∈E(15)xe∈{0,1}∀e∈E(16)fpk∈{0,1}∀k∈C,p∈PkMost remarkably, this formulation considers for each customer k ∈ C explicitly the set of all possible feasible connections, referred to by Pk. Corresponding binary variablesfpkfor all p ∈ Pkand k ∈ C indicate which connections are realized. Binary variables xeindicate the edges that are part of the solution. The objective function (12) minimizes the costs of these selected edges. Inequalities (13) ensure that (at least) one connection from Pkis realized for each customer (cover constraints), and inequalities (14) provide the linkage between thefpkand xevariables, ensuring that all edges that are part of a chosen connection are indeed selected.Obviously, this MIP has far too manyfpkvariables – corresponding to columns in the matrix notation – to be directly solved even for small instances, as there are in general exponentially many possible connections. In short, CG solves the LP relaxation of above model, called master problem (MP), by starting with a restricted master problem (RMP) that is obtained from MP by considering just a small subset of all connections and corresponding variablesfpk,e.g., those contained in an initial heuristic solution. This RMP can be efficiently solved and is then iteratively extended by adding further variables for connections that likely lead to better solutions. Each time one or more variables have been added, the RMP is resolved until no further improvements are possible.The task of finding suitable variables to be added is the so-called pricing subproblem and directly derives from the mechanisms of the simplex method for solving LPs: When having a solution to the current RMP, we consider the corresponding dual variable valuesμk*andπk,e*associated with the cover inequalities (13) and linking constraints (14), respectively. They define reduced costs(17)c¯k,p=−μk*+∑e∈pπk,e*∀k∈C,p∈Pkfor all possible connections, and we have to identify a variable/connection with negative reduced costs, i.e.,c¯k,p<0,as only such variables may yield an improvement in the RMP. If we can prove that no such variable exists anymore, the current RMP solution is also optimal for the MP and CG terminates.In our abstract network design problem, the pricing problem is thus to find for some k ∈ C a feasible connection p⊆E having costs∑e∈pπk,e*<μk*or to prove that none exists. Depending on the problem’s specific requirements for connections, this task may be as simple as finding a shortest path or more complex when, e.g., hop- or length-constraints or redundancy requirements need to be satisfied. The conceptually nice aspect, however, is that these specificities only have to be dealt with in this pricing subproblem.As pointed out, CG only solves the LP relaxation of a MIP model. To obtain guaranteed optimal solutions for the MIP, the approach needs to be extended to a branch-and-price, which is an LP based branch-and-bound in which CG is performed at every tree-node. However, as such MIP models are typically relatively strong, good heuristic solutions can usually be directly obtained from the LP solution by simple rounding or repairing.With respect to metaheuristic hybrids several possibilities exist for boosting the performance.•If the pricing subproblem is hard, meta-heuristics may be well suited for identifying variables with negative reduced costs. It should just be kept in mind that in the end an exact approach is necessary to prove that no further variables with negative reduced cost exist in order to have the master problem solved exactly. Therefore, frequently a chain of algorithms is used for the pricing problem, starting with a fast greedy heuristic over some metaheuristic approaches up to a usually slowest exact approach; for example Puchinger and Raidl (2007) describe such a chained approach for a two-dimensional cutting problem.Metaheuristics may also become useful in conjunction with solving the integer master problem. On the one hand, the set of variables initially provided to the RMP obviously has a crucial impact on the performance, and thus deriving them from one or more good starting solutions originally determined by (meta-)heuristics often is beneficial. On the other hand, metaheuristics can also be used to derive a better final solution based on the results of CG than those obtained by simple rounding or repairing, cf. Section 2.7. In this context, several aspects may be exploited for guiding the heuristic search:•obviously, the LP solution;the overall set of variables finally contained in the RMP; when considering our network design problem, one may, e.g., restrict the heuristic search to only those edges that appear in some connection corresponding to an included variable; note here the strong parallels to the LD-based hybrids described in Section 4;reduced costs of variables may provide a more promising guidance than their LP values, again cf. Section 4;dual variable values may provide more fine-grained guidance, e.g., in the case of our network design problem they may indicate which edges are more likely to yield improvements or should better be spared.Instead of applying CG and a metaheuristic sequentially and providing guidance in only one way, a collaborative approach may be considered, with both algorithms running intertwined or in parallel and mutually exchanging information; i.e., while the metaheuristic continuously exploits current LP solutions, dual variable information etc., it also sends newly found good solutions to the CG and corresponding variables are inserted in the RMP.For successful examples of above mentioned concepts, see Filho and Lorena (2000), who describe a constructive genetic algorithm in conjunction with CG for graph coloring, Pirkwieser and Raidl (2010), who consider a variable neighborhood search and an evolutionary algorithm collaborating with CG to solve the periodic vehicle routing problem with time windows, and Massen, Deville, and Hentenryck (2012), who apply ant colony optimization for heuristic CG to solve a black-box vehicle routing problem. Massen, López-Ibáñez, Stützle, and Deville (2013) show how the latter pheromone-based heuristic CG can further be improved by automatic algorithm configuration methods.Alvelos, de Sousa, and Santos (2013) describe a general hybrid strategy called SearchCol, where CG and a metaheuristic are iteratively performed and information is exchanged between both. The metaheuristic works in a problem-independent way trying to find a best integral solution by searching over combinations of variables identified in CG, while the CG is perturbed in each iteration based on the metaheuristic’s result by fixing subproblem variables with special constraints.Benders’ decomposition (BD) has been originally suggested for solving large MIPs involving “complicating” variables (Benders, 1962). It can be regarded dual to CG, as instead of iteratively adding variables to a RMP, inequalities, i.e., rows, are added.We consider a MIP of the form(18)zMIP=min{cTx+c′Ty∣Ax+By≥b,Dx≥d,x∈Dn,y≥0}with two kinds of decision variable vectors x and y. The x variables are “complicating” in the sense that when they are temporarily fixed the remaining problem becomes considerable more tractable, e.g., because the remaining problem decouples into multiple independent problems with only continuous variables.Benders’ decomposition reformulates the MIP by considering only the x variables with their specific inequalities Dx ≥ d and accounting for the impact of the y variables by adding a function zSP(x) to the objective(19)zB=min{cTx+zSP(x)∣Dx≥d,x∈Dn},where zSP(x) is the result of the subproblem(20)zSP(x)=min{c′Ty∣By≥b−Ax,y≥0}.In this subproblem, x is thus assumed to be a given constant vector. As the remaining variables y are continuous, this subproblem is an LP, for which we can also consider its dual form(21)zDP(x)=max{wT(b−Ax)∣wTB≤c′,w≥0},where w is the vector of dual variables associated with the inequalities By ≥ b − Ax. Assuming the feasible region of this dual problem is bounded and not empty, let W be the set of corresponding extreme points. Then, we may also writezDP(x)=maxw∈WwT(b−Ax)and rewrite our master problem (18) as(22)zMIP=min{z∣z≥cTx+wT(b−Ax)∀w∈W,Dx≥d,x∈Dn}.Inequalitiesz≥cTx+wT(b−Ax)for all extreme points w ∈ W are called Benders’ cuts.Computationally, Benders’ decomposition starts by solving (22) with no or only a small set of initial Benders’ cuts. This reduced master problem (RMP) yields initial values for x for which the subproblem zSP(x) and its dual are solved. The dual solution corresponds to an extreme point w ∈ W for which a respective Benders’ cut can be derived and added to the RMP, usually cutting off its current solution x. The phases of (re-)solving the RMP and the subproblem are iterated until no new cut violated by x is found. The finally obtained solution x is then a minimum for the original MIP (18). More generally, the primal subproblem (20) may sometimes be infeasible, yielding an unbounded dual subproblem (21). In these cases, extreme rays are used to derive feasibility cuts (in contrast to optimality cuts), driving the process towards feasibility by forbidding the current RMP solution x.Note that in contrast to LD and CG, BD directly yields an optimal solution to an original MIP and not only to a relaxation of it; it is therefore not necessary to further embed it in a branch-and-bound to obtain an exact approach.As an example consider a special variant of the abstract network design problem (12)–(16) from Section 5, called the local access network design problem(Randazzo and Luna, 2001). Each customer needs to be connected to the dedicated root node by a simple path and in addition to the fixed costs cefor installing a link e also capacity-dependent costsce′arise. This problem can be modeled by the following multi-commodity flow formulation:(23)min∑e∈Ecexe+∑(i,j)∈EDc(i,j)′∑k∈Cfi,jk(24)s.t.∑(0,j)∈EDf0,jk=bk∀k∈C(25)∑(i,k)∈EDfi,kk=bk∀k∈C(26)∑(i,j)∈EDfi,jk−∑(j,i)∈EDfj,ik=0∀i∈V∖{0,k},k∈C(27)fi,jk≤bkx(i,j)∀k∈C,(i,j)∈ED(28)xe∈{0,1}∀e∈E(29)fi,jk≥0∀k∈C,(i,j)∈EDThe model sends for each customer k ∈ C an individual commodity of size bk> 0 (corresponding to the required capacity) from the root 0 to node k. For this purpose, arc set ED is defined to contain two reversely directed arcs for each edge in E, and for each such arc and each commodity k ∈ C a flow variablefi,jkis used. Equations (24)–(26) are the classical flow conservation constraints. Inequalities (27) link the design variables xewith the flow variables so that a flow may only occur over edges that are part of the solution.We perform BD by considering variables x as the “complicating” ones and obtain as reformulated master problem(30)zMP=min{z∣z≥cTx+wT(b−Ax)∀w∈W,x∈{0,1}|E|},where Ax + Bf ≥ b corresponds to the flow conservation and linking constraints (24)–(27). Thus, we have to select a minimum cost subset of edges only constrained by the Benders’ cutsz≥cTx+wT(b−Ax). The respective Benders’ subproblem becomes(31)zSP(x)=min{c′Tf∣Bf≥b−Ax,f≥0}.Due to the fixed x, the flow variables for different commodities k ∈ C are here not linked anymore, and this subproblem decouples into |C| independent problems of finding a minimum cost path from the root to each customer k ∈ C. Some of these problems may be infeasible as the root and respective customers might not be connected in the given solution x. Feasibility cuts based on extreme rays of the dual are then identified for moving towards feasibility. For example, such a Benders’ cut might enforce to have at least a certain number α of edges from a specific subset E′⊆E selected, i.e.,∑e∈E′xe≥α.Randazzo and Luna (2001) present a comparison of a Lagrangean relaxation-based branch-and-bound, a branch-and-cut, and the above sketched BD extended with certain strengthening constraints. In their experiments, BD was the only algorithm able to solve to optimality all instances within a day, leading to the conclusion that it is the most robust method. More generally, different classes of Benders’ cuts can be derived for this and related problems, and they have specific, partly complementary impacts on the overall performance (Costa, 2005; Magnanti, Mireault, and Wong, 1986). It is thus important to carefully choose a suitable set of Benders’ cuts that are actually added in each iteration.In classical BD as introduced above, the subproblem must be an LP involving only continuous variables. However, BD has also been generalized to certain kinds of non-linear problems (Geoffrion, 1972). Hooker and Ottosson (2003) proposed logic-based BD, an approach that is applicable to an even wider class of subproblems including in particular also discrete ones. This is achieved by generalizing the LP dual to an inference dual. Constraint programming techniques turn out to be especially useful for handling discrete subproblems.Similarly as with LD and CG, there are several possibilities for metaheuristics to come into play with BD:•Although smaller than the original problem, the RMP is frequently still difficult to solve, especially when already many Benders’ cuts have been added. Metaheuristics may provide good approximate RMP solutions in much shorter time, and these solutions may be sufficient in order to achieve substantial speedups of the overall approach. If finally, when the metaheuristic cannot identify an improved master solution, the RMP is solved to optimality and no further Benders’ cuts can be identified, the whole approach is still complete. Poojari and Beasley (2009) describe such an approach for solving general MIPs in which a genetic algorithm together with a feasibility pump heuristic are applied to the RMP. The authors argue that a population based metaheuristic like a genetic algorithm is particularly useful as it provides multiple solutions in each iteration giving rise to more Benders’ cuts. Similarly in spirit, Lai and Sohn (2012); Lai, Sohn, Tseng, and Chiang (2010) propose a genetic algorithm/BD hybrid for solving the capacitated plant location problem; results indicate a tremendous saving of computation time in comparison to classical BD. Lai, Sohn, Tseng, and Bricker (2012) further discuss such an approach for a vehicle routing problem. Rei, Cordeau, Gendreau, and Soriano (2008) suggest to use local branching, cf. Section 2.8, for solving a MIP master problem in order to sooner find improved upper as well as lower bounds.Initial solutions obtained by some (meta-)heuristic may be used to derive an initial set of Benders’ cuts in order to start with a more meaningful first RMP. For example, Easwaran and Üster (2009) apply a tabu search to warm-start a BD approach for a supply chain network design problem.Subproblems need not necessarily always to be solved to optimality in order to obtain useful Benders’ cuts, even when completeness of the whole approach shall be retained (Zakeri, Philpott, and Ryan, 1999). Especially when considering difficult subproblems in logic-based BD, constraint programming and metaheuristics have a great potential for speeding up the overall approach by providing helpful cuts much faster. For example Hooker (2007) describes a constraint programming based BD that substantially outperforms pure MIP as well as constraint programming approaches on a large class of planning and scheduling problems. Cordeau, Stojković, Soumis, and Desrosiers (2001) solve an aircraft routing and crew scheduling problem by applying BD and use CG-based heuristics for the RMP as well as the integer subproblem.Raidl, Baumhauer, and Hu (2014) proposed an exact logic-based BD approach for a bi-level capacitated vehicle routing problem and sped it up considerably by first solving all instances of the master problem as well as all subproblems by means of a fast variable neighborhood search heuristic. Invalid Benders’ cuts possibly cutting off feasible solutions might be created. In a second phase, all these heuristically generated Benders’ cuts are verified by re-solving the corresponding subproblems exactly by means of MIP, yielding possibly corrected cuts that replace the invalid ones. When finally also the master problem is solved exactly and no further Benders’ cuts can be derived, a proven optimal solution is obtained.

@&#CONCLUSIONS@&#
Hybrid metaheuristics have shown to be successful advanced approaches for solving a wide range of practically relevant problems. On many occasions they are leading methods when dealing with large, complex combinatorial problems that cannot be solved to proven optimality in reasonable time. When designed appropriately HMs can significantly benefit from the advantages of the underlying basic strategies and exploit synergy.The design of successful hybrids, however, usually is not trivial, and one should be aware that a more complex system is not necessarily always better. In fact, the principle of “keeping things as simple as possible but not simpler” still holds also in this context. To develop an “as good as possible” heuristic optimization software for a new complex problem typically requires a considerable amount of experience, research of existing work on related problems, testing, comparing, and fine-tuning. This review intended to give an overview on the most successful design patterns of HMs and particularly focused on decomposition-based hybrids with origins in mathematical programming.Decomposition approaches, in general, are particularly useful to address large problems possibly consisting of several dependent, difficult subproblems. Lagrangian decomposition, Dantzig–Wolfe decomposition with its column generation, and Benders’ decomposition are well-known and frequently applied methods. The great potential they have when extended and combined with metaheuristics, however, has only been recognized more widely over the last decade. Besides, e.g., just providing initial approximate solutions for primal bounds, we have seen that far more possibilities exist for fruitful combinations. Metaheuristics are able to substantially enhance the applicability of these classical decomposition techniques – and vice versa.More work is necessary to get an even better understanding of the conditions under which the individual design patterns are best suited. From the author’s point of view, exploiting dual information in metaheuristics provided by Lagrangian relaxations and column generation approaches as well as metaheuristic approaches for solving separation problems in column generation and Benders’ subproblems in logic-based Benders’ decompositions may be particularly fruitful for future research.