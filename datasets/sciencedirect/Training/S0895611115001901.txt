@&#MAIN-TITLE@&#
System models for PET statistical iterative reconstruction: A review

@&#HIGHLIGHTS@&#
We present a review of works that estimate the system matrix for PET reconstruction.The works are classified according to the method used to compute the system matrix.Advantages and disadvantages of each approach are stated.The state of the field of system models for iterative reconstruction is analyzed.

@&#KEYPHRASES@&#
Nuclear imaging,PET,Statistical reconstruction,System matrix,System model,System response model,

@&#ABSTRACT@&#
Positron emission tomography (PET) is a nuclear imaging modality that provides in vivo quantitative measurements of the spatial and temporal distribution of compounds labeled with a positron emitting radionuclide. In the last decades, a tremendous effort has been put into the field of mathematical tomographic image reconstruction algorithms that transform the data registered by a PET camera into an image that represents slices through the scanned object. Iterative image reconstruction methods often provide higher quality images than conventional direct analytical methods. Aside from taking into account the statistical nature of the data, the key advantage of iterative reconstruction techniques is their ability to incorporate detailed models of the data acquisition process. This is mainly realized through the use of the so-called system matrix, that defines the mapping from the object space to the measurement space. The quality of the reconstructed images relies to a great extent on the accuracy with which the system matrix is estimated. Unfortunately, an accurate system matrix is often associated with high reconstruction times and huge storage requirements. Many attempts have been made to achieve realistic models without incurring excessive computational costs. As a result, a wide range of alternatives to the calculation of the system matrix exists. In this article we present a review of the different approaches used to address the problem of how to model, calculate and store the system matrix.Nuclear medicine imaging techniques, whose two major subbranches are single-photon emission computed tomography (SPECT), and positron emission tomography (PET), provide images reflecting physiologic functions unlike other traditional techniques, such as X-ray computed tomography or ultrasound, which yield anatomical structures. In order to image properties of the body's physiology, both the PET and SPECT techniques begin with the injection of a radiopharmaceutical into the subject under study. A radiopharmaceutical consists of a tracer compound that interacts with the body and a radioactive label that, by way of the emission of gamma rays, is used to track the flow and distribution of the tracer compound. A wide range of radioactive-labeled compounds have been synthesized as tracers that target biologically interesting markers or pathways thus permitting the measurement of quantities that go from glucose metabolism [1] to gene expression [2].SPECT uses radionuclides that emit single gamma rays. Such radionuclides require the use of a collimator between the patient and the detectors in order to define the direction from which the gamma rays are coming, thereby providing spatial information. SPECT collimation does an inefficient use of the emitted radiation, as the collimation strongly limits the number of detected gamma rays. On the other hand, PET studies do not require a physical collimation, but use a scheme of detection called “electronic collimation” (shown in Fig. 1), allowed by detection of collinear photon pairs generated by positron emitting radioisotopes.The first PET scanners were introduced in the mid 70s. Since then, they have been used for a broad range of applications, which are covered in Section 2.1. The requirements that these applications have been imposing have translated into several challenges, as described in Section 2.2. In order to overcome these challenges, the PET technology has evolved and improved in a variety of ways. These include the mathematical procedures for image reconstruction, and the statistical models on which these are often based, which constitute the topic addressed by this article.Nowadays, the majority of applications of PET in clinical (i.e., human patient) medicine are found in oncology imaging, either for cancer diagnosis or for cancer treatment planning and guidance [3–6]. However, there are other uses of clinical PET that do not involve oncology [7,8]. Some of them have been around for a long time such as cardiac imaging with PET [9], others are relatively new and provide new insights into how to make diagnosis of diseases where other techniques fail to provide sufficient information.Although initially conceived for whole-body clinical purposes, the earlier clinical systems were also used for brain and animal (predominantly in the larger laboratory animals such as nonhuman primates) imaging [10]. These two areas have steadily gained importance in the clinical and research environments, the former (brain imaging) due to the explosive interest in the early diagnosis of neurodegenerative disorders and in the study of cognitive and affective processes, the latter (animal imaging) due to the increased use of small-animal models for studying disease and for designing and evaluating new drugs in the pharmaceutical research and development pipeline. As technological advances have allowed whole-body PET scanners to settle in the clinical practice, there has been significant motivation to design dedicated units that maximize the PET performance in brain and small animal studies. Given the characteristics of the objects being imaged in these fields, the resolution and sensitivity of whole-body human PET are often insufficient for a broad range of applications, and specific devices for brain and small-animal PET have been developed. The improved ability of brain-dedicated PET scanners to provide reliable quantitative assessment of the activity in small cerebral regions involved in specific functional pathways has been already demonstrated [11–13]. Similarly, the development of small animal PET scanners has allowed the clear visualization and precise quantification of the biodistribution of PET tracers in small structures within rodents (see [14–16] and the references therein).Despite the major advances mentioned above, the use of PET in a wider range of applications will be dictated, along with other limitations (cost-effectiveness, availability of PET tracers, user friendliness, etc.) by the requirements these applications impose on the intrinsic performance of the PET tomographs. The challenges that should be confronted depend on the final target.Improving the typical spatial resolution of around 5mm of the human images is not a main goal in the current routine clinical PET applications. However, there is a need to improve the signal-to-noise ratio of whole-body studies. The mean photon path distance from emission to detection is greater than in other modalities, which increases the probability of attenuation and/or Compton scattering inside the patient's body, especially in larger persons. This increases the amount of noise, produces quantization error and reduces contrast in the reconstructed images.On the other hand, animal PET scanners are designed to perform studies on small subjects (typically mice and rats), which often demand to assess concentrations of radioactive nuclide in structures with dimensions of a millimeter or less. The parameter getting the most attention is thus spatial resolution, with the objective of achieving sub-millimeter isotropic volume resolution. Whether this range is able to be reached is not clear since there are several complex factors interacting to limit spatial resolution during data formation and collection (positron range, pair non-collinearity, crystal penetration, inter-crystal scatter, detector decoding errors, etc.). [17] performs a quantitative analysis of these effects and estimates that the ultimately achievable spatial resolution for pre-clinical PET cameras with appropriate detector designs is 0.83mm.Furthermore, since the amount of injected dose is rather limited for small animals (in order to not perturb their biological systems), a high level of sensitivity is required to achieve an acceptable number of coincidences.For neuro-imaging, the desired spatial resolution is slightly higher compared to the whole-body imaging (around 3mm3 is normally sufficient). However, in applications in which the structure of distribution of tracer within the organ is important for diagnosis, higher resolution might be needed. Furthermore, brain applications constitute an area of wide use of kinetic modeling methods in PET [18]. The kinetic studies are based on dynamic PET data where short frames, with low signal-to-noise ratio, are acquired in order to capture the temporal distribution of the activity concentration after the radiotracer injection. This makes high sensitivity a particularly important requirement in most brain applications.In all the modalities, the requirements are first addressed at the stage of design and development of instrumentation. For example, in body systems, the introduction of the time-of-flight (TOF) systems, that are able to approximately locate the annihilation event along the path of the two photons, has led to remarkable improvements in the final SNR in the images. In small animal devices, the measurement of the depth of interaction (DOI) of photons within detectors reduces the magnitude of the parallax effect, which is known to significantly degrade the resolution of the images [19]. These advanced technologies greatly alleviate some of the PET issues, but there is a point at which the instrumentation encounters some limitations. For example, the requirements of sensitivity of the small animal applications can be satisfied using detector modules with long crystals, placed in a gantry with the smallest possible diameter [20]. However, both long pixelated crystals and small gantries lead to a significant resolution loss due to the parallax effect. Moreover, the increase of the gantry diameter to reduce the penetration effect (at the cost of reducing the sensitivity) would lead to another form of spatial resolution degradation due to the non-collinearity of the pairs [21].Under these paradoxical circumstances, other means to meet the demanding requirements the PET studies impose have been explored. A particularly important area is the reconstruction step, in which the events collected are converted to three-dimensional images. Certain algorithms are capable of preserving and even enhancing (as shown in [22] for the parallax error correction) the hardware advantages.As pointed out in the previous section, the achievement of the ultimate PET performance will require the help of advanced reconstruction techniques [23]. In this section, we introduce the different approaches to estimate the three-dimensional radiotracer distribution from the recorded data (Section 3.1). Then, we focus on the successful type of reconstruction methods known as iterative algorithms, specifically on the statistical model these methods make use of (Section 3.2) and on the effects of the PET acquisition process they should take into account and compensate (Section 3.3).The early techniques of PET image reconstruction were analytical approaches based on the method of Filtered Back Projection (FBP) [24]. FBP is a mathematical technique based on an idealized model of PET, which ignores many significant physical factors. Specifically, FBP assumes that the number of events measured in each LOR (line of response, see Fig. 1) is equal to the line-integral of the radio-isotope distribution along the line connecting the centers of the two corresponding crystals, from which the image can be reconstructed using analytical inversion formulas. In spite of its approximate essence, FBP has enjoyed widespread use due to its computational simplicity. Regrettably, FBP amplifies the noise when it is applied to the low-count data of PET imaging (it is still a good method for applications where the number of measured counts is high, as computed tomography, and/or where linearity of the reconstruction is needed). Consequently, there has been a great interest in the development of reconstruction alternatives that provide better resolution and noise characteristics [25–28].The introduction of the iterative reconstruction methods allowed for the explicit inclusion of the physical factors ignored by analytical schemes. These algorithms achieve improved spatial resolution and signal-to-noise ratio, while maintaining the quantitative nature of the data [29,30]. The price of this added refinement is that the resulting set of equations describing the problem becomes very large, and solving for the tracer distribution by direct inversion of the problem becomes unworkable. In this case, the equations must be solved using iterative methods. All iterative reconstruction methods share a number of common traits. The general model, shown in Fig. 2, involves repeating the process of projecting an image estimate (the so-called forward projection operation), comparing the estimated projections to the measured data to compute some form of error, backprojecting the error (backward projection operation) and using the error to update the image estimate.Two wide classes of iterative algorithm exist, namely algebraic and statistical. Classical algebraic iterative algorithms, such as ART and MART [31] are based on the Kaczmarz method of solving systems of linear equations [32]. Although not widely used in nuclear medicine, they form an important basis for understanding the statistical algorithms that were developed later and for which iterative methods are mainly used. Statistical algorithms have two basic parts: a statistical criterion (the basis for determining which image among the many possible is to be the solution) and a numerical algorithm (the method for finding the solution prescribed by the criterion). The most successful early statistical algorithm, MLEM [33] uses a Poisson model of the produced data, related to the maximum likelihood (ML) statistical criterion, along with the expectation maximization (EM) algorithm as numerical method. A serious disadvantage of MLEM is its slow convergence, which can lead to prohibitive reconstruction times on standard computer platforms [27]. Several acceleration techniques have been proposed for the MLEM algorithm. Lewitt and Muehllehner [34] improve convergence speed by incorporating an over-relaxation parameter. Tanaka [35] uses a frequency amplification method to accelerate the estimation of the higher-frequency components in the image. The ordered subsets expectation maximization (OSEM) method [36] divides the data into subsets, giving a speed-increase factor proportional to the number of subsets chosen. Browne and DePierro [37] proposed the row-action maximum-likelihood algorithm (RAMLA), which uses as many subsets as there are projections. However, these block methods do not necessarily converge, as reported empirically by Byrne [38]. The space-alternating generalized EM (SAGE) [39] improves the convergence rate. Rebinning methods such as single slice rebinning (SSRB) [40] or Fourier rebinning (FORE) [41] can reduce the dimensionality of 3D acquisitions by performing a set of 2D reconstructions to obtain volumetric data, although there is a loss of image quality with respect to the more time consuming fully 3D implementation.In addition to the slow convergence issue, there is another important drawback related to the MLEM derived methods: at high iteration numbers, images exhibit high-variance behavior [42]. This is usually attributed to either the fact that there is no explicit stopping rule in this kind of iterative reconstruction or to the statistical (noisy) nature of the detection process and reconstruction method [43]. A usual approach to overcome this drawback is either design some kind of practical stopping rule [44–46] or to smooth the images with kernels [47,48] filters [49] or wavelet based methods [50]. As for solutions integrated in the reconstruction algorithm, the maximum a posteriori (MAP) methods [51–54] offer a very convenient way of reinforcing desirable properties in the reconstructed image by incorporating a priori information (priors) that models the distribution of activity and noise in the data. The regularizing influence of the prior controls the variance of the reconstruction, and hence the MAP methods do not exhibit the instabilities at higher iterations encountered using MLEM [55]. On another note, dynamic RAMLA (DRAMA) is an iterative algorithm similar to RAMLA, which optimizes noise propagation from each subset to the reconstructed image by introducing and optimizing subset dependent relaxation parameters [56,57].There exist also statistical algorithms based on a Gaussian model of the produced data, related to the weighted least squares (WLS) criteria, which result in quadratic objective functions. These can efficiently use many established numerical algorithms, such as coordinate gradient [58,59] or coordinate descent [60], and result in faster reconstruction algorithms.Conventional PET reconstruction algorithms involve a preprocessing step where the data is binned as a function of the LOR spatial orientations, through structures called sinograms. The sinograms are a representation of the detections measured at a given plane through two-dimensional matrices, with the vertical columns representing evenly spaced projection angles and the horizontal rows representing spatial positions within the projections. Alternatively, LOR-based algorithms enable reconstruction of raw histograms (also known as LOR histograms), which use the native projection space as discretized by crystal pair indices, eliminating the interpolation step required to form the uniform sinograms [61,62]. Another option is the storage mode known as list-mode. List-mode acquisition is achieved by storing information regarding the acquired events as they are detected one-by-one in the form of a list. The list-mode data is not amenable to sinogram-based reconstruction methods without first histogramming the data. However, there exist several versions of the MLEM and OSEM algorithms [63–65] for this type of data.In order to perform the forward and backward projection operations, the iterative reconstruction algorithms require the use of the so-called system matrix (also known as system response matrix or system model). The system matrix describes the relationship between sources and data, taking into account the physical properties of the data acquisition process. Specifically, in a reconstruction in which the continuous radiotracer distribution is approximated as a sum of discrete basis functions bj(j=1, 2…N) (normally cubic voxels, although blob basis functions have shown better noise properties [66]) and in which the scanner contains di(i=1, 2…M) possible LORs each p(bj, di) term of the N×M system matrix stands for the probability that an event generated in the region defined by the basis function bjis detected in a detector pair di(see Fig. 3).Besides the system matrix, iterative algorithms usually make use of a second matrix, whose N terms p(bj) stand for the probability of detecting a pair of photons arbitrarily emitted from within function bj(considering all possible detectors). Since this matrix accounts for the discrepancies in the sensitivities of the different basis functions, it is usually referred to as sensitivity matrix. Although alternative direct approaches exist [63,67–69], the terms of the sensitivity matrix are typically computed as the summation over the probabilities of detection of bjin each LOR digiven by the system matrix terms (i.e.p(bj)=∑i=1Mp(bj,di)).The quality of the images obtained by the iterative algorithms depends strongly on how faithfully the theoretical model incorporated through the system and sensitivity matrices depicts the real PET process. On the other hand, the price of the enhancements achieved by an accurate modeling is that the reconstructions require greater execution times and/or storage resources. This is because iterative algorithms perform repeatedly the forward and backward projection operations that require the knowledge of the whole system matrix, whose size is large and its computation is costly in high resolution 3D PET systems. Reaching a good compromise between image quality and efficiency is thus a key challenge in iterative reconstruction. This is the reason why much effort has been devoted to find alternative implementations of the system matrix that provide high quality images along with feasible reconstruction time and storage conditions. A review and classification of the numerous works on this topic is provided in this article.There are a variety of factors, inherent to the PET data acquisition process, that can contribute to the degradation of the reconstructed images:•The geometry of the specific scanner.The positron physics (see Fig. 4), including the distance the positron travels before annihilating, which is termed positron range and the angular deviation from the ideal antiparallel directions of the emitted gamma rays, which is termed non-collinearity.The unwanted coincidences (see Fig. 5), including the coincidences in which the photons detected come from different annihilation pairs, termed random coincidences, and those in which one or both of the photons of the pair scatter in the object being scanned, termed scattered coincidences.The attenuation of photons prior to detection due to the absorption within the object being imaged (see Fig. 5).The intrinsic detector resolution. This is mainly conditioned by the geometric resolution of the detectors, which is roughly half the detector pixel width, and by two important physical effects: the crystal penetration and the inter-crystal scatter (depicted in Fig. 6). The crystal penetration occurs when a photon enters the crystal at an oblique angle and travels with no interaction through this crystal, before being absorbed in a different crystal. In this case, the oblique event is assigned a wrong LOR, and this source of error is known as parallax error (similar to the well-known parallax error when making a distance measurement using a thick ruler and viewing from an oblique angle). The inter-crystal scatter occurs when a photon undergoes several interactions in the scintillator and the event is assigned a wrong LOR. Inter-crystal scatter can arise for any angle of photon entry (oblique or non-oblique) into the crystal.The variations in the detection efficiency between detector pairs, due to physical constraints, like crystal imperfections, or to effects that take time during the data acquisition, such as misalignments in the timing windows, or the dead time the counting system is occupied processing previously detected events.In order to compensate these effects during the image generation process it is necessary to develop models that describe them properly. The integration of all the models in the system matrix can be a challenging task if the matrix is too large and/or its calculation is too complex. Moreover, the amount of randoms increases at the square of the activity in the subject. In consequence, random coincidences do not fit in the linear formulation that relates the source and data vectors through the system matrix.An approximate alternative procedure consists of considering certain effects (typically randoms and scatter coincidences on one side and attenuation and differences in detector efficiencies on the other) as errors in the data and treat them in the measurement space. Several methods exist to estimate or measure these nuisance components, with a high level of accuracy. The most commonly implemented methods for estimating the randoms are the delayed coincidence method, that measures the random coincidences directly and the singles-based method, that estimates them from independent (singles) measurements [70]. Photon scatter correction is a slightly more complicated problem, that has generated a number of advanced approaches and still remains an area of active research. A discussion of existing ideas is beyond the scope of this paper, but several works provide an extensive account of methods to estimate the scatter [71,19,72,73].Concerning the attenuation correction, one approach involves using a radioactive source of single photons (or photon pairs) that moves in a circular path just inside the detector ring. The probability of survival for each LOR of the scanner can be computed as the ratio of the number of photons (or photon pairs) detected with the patient present (the transmission scan) to the number detected in the absence of the patient (blank scan) [19]. Alternatively, the transmission scan can be used to reconstruct an image of the attenuation values via statistical methods [74–76]. With the advent of multimodality PET/Computed Tomography scanners, attenuation correction using separately acquired anatomical images has become a popular alternative [77,78].Normalization weights to compensate the varying sensitivities among the different LORs include direct [79] and component-based methods [80–83]. In direct methods a source of activity that illuminates all LORs is scanned. Then, the normalization factors are estimated as the ratio between the ideal number of coincidences and those actually measured. The problem of these direct methods is that they require long scans for sufficient statistical accuracy. The component-based methods represent the normalization weights as a product of factors representing the different effects affecting detection efficiency. These approaches reduce the number of counts required by reducing the degrees of freedom in the normalization model so that the correction factors are computed by averaging over multiple detector pairs.Once the values for correcting the different degradation factors have been estimated using one of the above mentioned methods, the corrections can be applied in two different ways. A first option is to apply the corrections before the reconstruction step, subtracting or multiplying the estimated values to the raw data. However, precorrected scans are no longer Poisson, which could bias the reconstruction in iterative algorithms. Alternatively, the corrections can be incorporated to the iterative reconstruction process itself, minimizing their impact on signal-to-noise ratio (SNR). In order to account for scatter and randoms in the reconstruction, their respective corrections are commonly added to the forward projection [84]. Rehfeld and Alber [85] prove the robustness of this approach for the scatter correction by comparison to a reconstruction that incorporates the model to the system matrix. An alternative scatter and randoms multiplicative scheme is proposed in [86]. The attenuation and normalization corrections are always multiplicative and can be easily incorporated to the algorithm. Several iterative reconstruction schemes incorporating one or both of these corrections in the forward, backward projection and/or the sensitivity term have been proposed [87–89]. In the following, the modeling of these effects will just be considered in the cases in which it is explicitly part of the system matrix, which, as will be shown later, includes the option of multiplying the elements of the system matrix by the normalization and attenuation correction factors.The three following subsections deal with the three basic methodologies that, in descending order of accuracy, can be used to accomplish the computation of the system matrix. The fourth subsection describes how these methodologies can be combined to form practical hybrid approaches.The most accurate way of determining the system matrix would be, under ideal experimental conditions, its direct measurement from the scanner for which data are going to be reconstructed. The elements of the system matrix could be obtained by positioning and scanning a source at every basis function location, and recording the response (see Fig. 7a). Unfortunately, this is also the most challenging approach. On the one hand, for a full 3D matrix, the huge number of scans required makes this approach burdensome. For example, [90] make a temporal estimation of 2.6 years to complete the system matrix acquisition of a clinical scanner, even after assuming scanner symmetries. On the other hand, the experimental setup can be tedious due to the need to precisely position the source, a task that requires complex and expensive equipment (usually sophisticated robots that can only place one source at a time). These obstacles still make a pure empirical methodology presently impractical. However, as will be discussed in detail below, several practical approaches have been proposed that estimate the whole system matrix from a representative set of point or line source measurements.A variety of software packages offer the possibility of simulating the physics of a PET scanner very precisely. These tools rely on Monte Carlo algorithms based on the repeated random sampling of probability models of the transport of radiation through matter. A number of different PET simulators have been developed (PETSIM [91], SIMSET: Simulation System for Emission Tomography [92], Eidolon [93], GATE, the GEANT4 Application for Tomographic Emission [94], that encapsulates the accurate GEANT libraries for the simulation of the passage of particles through matter [95], GRAY [96], PENELOPET [97], SORTEO, Simulation Of Realistic Tridimensional Emitting Objects, [98]), each of them with different advantages, disadvantages and levels of accuracy. Overviews of these packages can be found in [99,100], and an experimental comparison between two popular PET MC simulators, SIMSET and GEANT, is performed in [101].An alternative to the direct measurement of the system matrix is the MC simulation of the above mentioned source scanning process (see Fig. 7b). The feasibility and benefits of this approach have been shown by a number of authors [102–107,101,108,109,21,110,22]. Nevertheless, the simulated acquisition of the system matrix has some issues, for which a variety of different solutions have been proposed:Although current Monte Carlo simulators provide very accurate modeling of the photon transport in the object being imaged and in the detector crystals, there are still effects in the data acquisition process (light propagation effects, electronic noise, etc.) and features of the devices (light guides, magnetic shields, etc.) that are difficult or impossible to reproduce by the simulators and hence a MC approach will be always less precise than an empirical methodology.MC simulators are fairly slow and a substantial number of simulations (i.e. decays per simulated source) are required to reduce the statistical noise of the estimated system matrix. If the number of elements of the matrix is too high or not enough computational power is available, the MC based computation of the whole system matrix can be unworkable. Several investigators have dealt with the compromise between time and statistical accuracy in the MC calculation of the system matrix. On the one hand, several authors [111,101,110]) have studied the effect of the number of simulated decays on the noise propagation from the matrix to the images. Other works try to accelerate the MC simulation without jeopardizing the accuracy of the system model by considering symmetries [111] or alternative radioactivity distribution models [112]. In [113], different denoising filters are applied to the elements of a short-time simulated system matrix.The computational time demand that the statistical accuracy imposes on the simulation of the system matrix often forces MC approaches to precompute the system matrix and store it to be used during reconstruction. The size of the system matrix increases proportionally with the number of basis elements and the number of LORs. In high resolution 3D PET scanners, these numbers can lead to matrix sizes of the order of 1012 elements. Under these circumstances, the computation of the system matrix poses a challenge from the storage viewpoint. Several strategies have been proposed in order to save space, which are not strictly restricted to the MC methodologies but are inherent to any approach in which the system matrix values are precalculated offline and stored. These works share two common tactics to reduce the storage requirements, mainly exploiting the sparsity and the symmetries of the system matrix. Additional compression can be obtained resorting to quasi-symmetries [43] and shift invariances [114].On the other hand, alternatives to the explicit computation of the matrix that keep the accuracy provided by the simulation tools exist. These works use fast voxel-driven [115] or LOR-driven [116] stochastic sampling protocols to implement the projection operations on-the-fly. But in addition to the pure storage problem, the precomputation once and for all of the system matrix entails other complications:•It is a rather rigid approach. On the one hand, the configuration of the scanner and the grid of basis functions cannot be modified without running a new set of simulations. From a development perspective (e.g. when various crystal dimensions are to be investigated or when a multiresolution scheme needs to be evaluated) a more flexible approach is desirable. Furthermore, a precalculated system matrix cannot account properly for the medium-related effects (basically positron range and scatter and attenuation in the source), which may vary significantly from one study to another. The estimation of these effects has to be based on general assumptions of the imaged object or alternatively be considered apart, and applied in the form of corrections. In order to overcome this drawback, [110] proposed MC simulations that make use of realistic anatomical maps obtained from computed tomography (CT) images, which allow to include the scatter and attenuation effects in the simulated matrix in a very accurate way.Even if the size of the system matrix can be reduced enough to store it as a file, the access to disk for every forward and backward projection operation can considerably slow down the reconstruction. In [21] the elements of the system matrix were stored in an efficient way that allows fast sequential access during the reconstruction.The sorting of the simulated data into the matrix is a computation-expensive procedure hindered by the huge size of the matrix. Rafecas et al. [105] employed a database management system to overcome this problem.In the case of continuous detectors devices [117–120], the positions d assigned to the detected photons do not correspond to the centers of discrete crystals, but to continuous locations within scintillator blocks. The resulting p(b, d) terms are, therefore, continuous functions of d which should be discretized in order to be stored. The data under reconstruction would have to be rebinned to fit the discretized system matrix terms, losing part of its original precision, which is precisely the key benefit of the continuous scintillators technology.Finally, there exist analytical implementations of the system matrix. Unlike the MC approaches, the analytical approaches are noise-free and usually very fast. This normally allows the computation of the elements of the system matrix on-the-fly at reconstruction time, thus avoiding the storage problems mentioned above. However, in its quest to speed up the reconstruction process, these methods overlook important physical effects of the PET acquisition process and use simple geometric system models based on the intersection of the lines [121], or the tubes of response (volumes centered around the LOR, referred to as TORs) [122–125] with voxels or blob basis functions [62,126] (see Fig. 8, top). Certain improved projectors allow to include more basis functions than those strictly involved in the intersections, making use of bilinear and trilinear interpolations [63,127] or the Wu-antialiased line-tracing algorithm [128,129]. An example of trilinear interpolation is shown in Fig. 8 (middle) to illustrate this kind of procedures. [130,131] propose more sophisticated ray-tracers incorporating a linear attenuation model to a basic LOR-basis intersection scheme. The approaches based on the calculation of the solid angles subtended at the detectors by each basis function (Fig. 8, middle) [132,133,29,55,134,135] can take into account the depth dependence (i.e. the distance of the positron annihilation point from the detectors). However, the complexity of the on-the-fly solid angle calculations can compromise the reconstruction time and some authors choose the precomputation of the matrix [135].A variety of articles [136,137,87,138–140,114] show how the enhancement of simple analytical models with detector response weights (accounting for the detection-associated effects, mainly intercrystal penetration and scatter) may improve the quality of the results without significantly compromising the efficiency of the calculations. Fig. 8 (bottom) illustrates the principles in which these procedures are based on. All these approaches start with a simple geometrical calculation (line-voxel intersection or distance) to which a spread function approximating the additional effects is imposed. Although all these works propose analytical detector responses, [137] includes as well a MC estimation and [87] states that their kernels should ultimately be based on measured line spread functions. These two authors point to an interesting direction: the benefits that can be obtained exploiting synergies between the basic methodologies for the system matrix computation. A large number of hybrid approaches have been proposed following this philosophy, always seeking for higher precision models at affordable storage and/or time requirements.For example, a number of works exist in which the system matrix is calculated through a combination of parametrized functions representing the scanner response and the interpolation or extrapolation of a limited number of source acquisitions. Fig. 9illustrates the main procedures to which these works restort. In some cases, the acquisitions are identified directly with the model [141–143], and the parametrization is performed only in the projection or image space variables, allowing for an efficient size reduction, for example, by grouping LORs by their incident angles. In other cases, the models are parametrized as well, and the acquisitions serve to extract the parameters that define preestablished analytical responses. A great variety of such models has been used, including circular [144] and elliptical [145,146] Gaussians, combination of half Gaussians [147,144,90,148], superposition of symmetrical Gaussian functions [147,90] or convolution of Gaussians with rectangular pulses [149], which sometimes are different for the transaxial and axial components [147,90,144] or for opposite or adjacent detector blocks [149].The source acquisitions can be simulated [145,149,146,142] or measured [141,147,90,144,148,143]. MC simulations provide the origin of positron emission exactly, whereas in experimental measurements, this information is only available when using point sources. This has the advantage of allowing to perform the whole data collection through a single uniform source simulation study. However, only [142] takes advantage of this issue, and only simulates one cylindrical source. In the rest of the cases, either simulated or measured point sources (or lines in the case of [141]), with representative locations strategically chosen to minimize the number of acquisitions, are used. Furthermore, due to the lack of certain components of the detection process in the simulators, several discrepancies of the simulated responses have been observed with respect to the, always more accurate, measured approaches [150].Alternatively, the use of parameterized responses can be used to reduce the storage requirements rather than the number of measurements (which in MC estimations are not such a critical factor). In this spirit, the works of [43,151] perform a preliminary MC calculation once, and then approximate the results are by spline profiles whose coefficients fit in RAM memory. Fig. 10shows the operating principles of these methods. The coefficients can be retrieved during reconstruction for a fast calculation of the system matrix terms using interpolation techniques.Finally, the combination of empirical measurements and MC simulations is exploited in [152], where a MC obtained system matrix is updated toward an ideal system matrix by training the resulting forward projector with an artificial neural network. The inputs are digital images of a phantom and the desired outputs are the corresponding projections obtained in a PET scanner, from which the neural network can learn physical effects that might not be described correctly using MC simulations.The most general of the hybrid models introduced in the previous section are those models whose calculation involves each of the three basic methodologies (empirical, Monte Carlo and analytical). This is commonly achieved by modeling each effect that occurs in the data stream of a PET scanner separately. In the following, we introduce the common principles of the great variety of approaches that rely on the separation of effects to compute the system matrix (Section 5.1). Then, as shown in Fig. 11, we classify these approaches in two groups, according to the way they group the separated models for the different effects. On the one hand, there are schemes based on the factorization of matrices (Section 5.2). These well-established schemes can in turn be classified in three subgroups (see Sections 5.2.1, 5.2.2 and 5.2.3) according to the emphasis they put in the matrices that account for the resolution “blurring” factors. In Section 5.2.4, we point out how such matrices can be used in reconstruction frameworks different from the factorization of effects. On the other hand, there are approaches that combine the effects through multi integral schemes. As pointed out in Section 5.3, the development of these methods is hindered by the high computational times such integrals impose.The benefits of the effects separation rely on explicitly considering the diversity of the phenomena occurring during the PET acquisition process, for which an homogeneous treatment may result in non-optimal calculations. By modeling each effect using the methodology that best fits to its requirements, these approaches achieve the best synergy between methodologies. For example, it is not worth to perform extensive experimental measurements or simulations to model the response of effects for which accepted analytical fast noise-free models are available. This is clearly the case of the geometrical factors, but also of other physical effects, such as the photon non-collinearity deviation angle, which is known to have a Gaussian distribution [153,154]. However, for those effects for which a model can hardly be expressed using finite analytical calculations an experimental estimation can be used instead. This can be the case for phenomena such as the photon interaction physics within the scintillator. While in this case an empirical model should provide more accurate results, the complexity of the measurement process can lead to the preference for a MC estimation.For certain phenomena, the choice of methodology to estimate a model still remains an open problem. This is the case for the positron range, for which, as stated by [155] a considerable variation has been found among the analytical [156,157], simulated [158,159,155] and measured [160,161] models accounting for the complicated path that positrons follow in tissues.The degree of separation between the different effects present during the acquisition process may vary as well among the different proposals. For example, certain works merge the effects of crystal penetration and inter-crystal scatter in a single detector model, while others account for them through two different models obtained through two different methodologies. In any case, since the model used to describe each effect, or collection of effects, may be changed from one reconstruction to another without altering the rest of the models, the separated-effects approaches enjoy a great flexibility in terms of scanner configurations (geometry, material, etc.), experimental conditions (isotope, subject under study) or reconstruction features (type and size of the basis function).Furthermore, the individual estimation of the different effects allows putting the emphasis of the model on the more degrading factors, according to the PET modality or scanner for which the reconstruction is going to be performed. For example, in a small animal scanner, a greater effort is typically done in the detector response model responsible for the intercrystal penetration and scatter, whereas the human studies are usually more concerned with the attenuation and the scatter in the object. Many authors focus their work on calculating just one or several parts of a well-established scheme of separated effects, while using standard models for the others.Independently of the effects considered and the methodology used to model each part of the process, a different problem arises when the individually modeled effects have to be combined to form the whole system matrix. Two important different approaches exist to address this issue, namely the factorization and the integral formulation, which are described in the following.The most popular way of combining models is to express the system matrix as a product of independent matrices, each one standing for one or a collection of effects. This scheme achieves substantial savings in storage, since each matrix component is highly structured and sparse. It is advantageous as well in terms of reconstruction time, since the forward and backward projection can be performed efficiently with a sparse-matrix multiplication operation, although some authors precompute certain matrix products to reduce the computational load, when memory is not a constraint [162].Although earlier versions of the factorization of effects had been proposed [163,164], and implicit factorizations in the form of the previously mentioned attenuation and normalization multiplicative corrections had been previously applied, nowadays, all the factorized schemes are based on the scheme proposed in the seminal works of [165,55], for 2D and 3D acquisitions respectively:(1)P=pdet.senspblur.projpattpgeopblur.imIn a reconstruction in which the continuous radiotracer distribution is approximated as a sum of bj(j=1, 2…N) discrete basis functions and in which the scanner contains di(i=1, 2…M) possible detector pairs, pdet.sensand pattare the M×M diagonal Attenuation and Detector Sensitivity matrices. The diagonal elements (i, i) of these matrices contain, respectively, the factors that account for the attenuation and detector efficiency in each LOR di. These factors match the attenuation and normalization coefficients used for data correction that have been introduced in Section 3.3 and can be thus calculated using one of the cited methodologies.pgeois the N×M geometric projection matrix with each element (i, j) equal to the probability that a photon pair emitted in a basis function bjreaches the front faces of the detector pair diin the absence of attenuation and assuming perfect photon-pair collinearity. In principle, the non-collinearity should be a geometrical effect as it concerns the direction of the gamma rays. However, all the approaches assume this factor is depth-independent and lump it with the blurring matrices that will be described next, based on the fact that it is a phenomenon affecting the resolution of the images. The computation of the geometric projection matrix is normally based on basic intersection or solid angle approaches, with some authors averaging several points within the voxels [55,166] or multiple rays within the LORs [167–169] to improve the accuracy of the model. Other works focus as well on compacting the storage of the geometric projection matrix [55,166,170] or accelerate its calculation [171], since it can still be large in high-resolution applications.Although different implementations of the three above described matrices exist, their physical meaning is always the same in all factored schemes. However, for the two remaining matrices, namely the Projection Blurring matrix pblur.projand the image blurring matrix pblur.im, the different implementations may imply remarkable conceptual differences. These matrices are intended to account for the factors that limit the spatial resolution, typically the positron range, non-collinearity, crystal size and mispositioning of photons caused mainly by crystal penetration and multiple photon interactions, etc. Each element (i, i′) in pblur.projrepresents the probability that an event which should have been detected along LOR diis actually detected in LOR di′ (as shown in Fig. 12, left) whereas each element (j, j′) in pblur.imstands for the probability that a decay originated within a basis function bjis placed in another basis function bj′ (as shown in Fig. 12, right). According to their definitions, the pblur.projand pblur.immatrices account, respectively, for the overall detection (i.e data-space) and emission (i.e image-space) blurring. Three categories of approaches (differentiated by the use of these matrices) exist, as described in the following subsections.Surprisingly, just a small number of recent works apply strictly the definition of the matrices putting the effects in the pblur.projor pblur.immatrices, according to the level (data or image) to which they belong. For example, the positron range is best described acting on the image since the detector actually sees the true image as blurred by its effect. In contrast, the non-collinearity, crystal size, penetration and intercrystal scatter are best described as acting on the localization of the LORs. Following these guidelines, the approach of [172] consists of estimating separated analytical models for the non-collinearity, penetration and positron range and a measured model for the inter-crystal scatter, and then placing each of these models in its corresponding Projection or image blurring matrix. The method developed by Zhou and Qi [170] is more general in terms of the effects modeled, as it estimates the two blurring matrices using a limited number of point-source scans. This allows to account not just for certain predefined mentioned effects, but for any other resolution degradation factor that takes place during the source acquisitions.A number of authors ignore the image blurring matrix and assume that the resolution spread happens only in the space of the acquired data. The main argument for this approach is related to physics. In applications with 18F the positron range is sub-millimeter and the majority of blurring effects (mainly parallax and intercrystal scatter) are related to the detection events. Therefore, it seems a priori more logical to spend more effort in modeling the distortion of the data. Many successful system matrix implementations are based on this claim, differing just in the way the projection blurring is estimated. In some cases this matrix is estimated from source measurements obtained from MC simulations [165,55,173–176,169] whereas in other cases it is estimated empirically [173,177], which allows to account for all the classical detection blurring effects, and even the emission associated effect of the positron range [162].The projection-based factorization is a well-established approach that has led to high quality efficient reconstructions, but it has two weak points. First, the correct measurement of the projection blurring would require a collimated source, but this is impractical and uncollimated sources are used instead [177]. Alessio et al. [162] demonstrate with MC simulations that a non-collimated source is acceptable for modeling the radial blurring (i.e. blurring among LORs in different radial positions), but that it ignores the azimuthal blurring (i.e. blurring among LORs with different angular orientations), since all angles are simultaneously collected during acquisition. Second, and more inconvenient, even if the size saving from factoring the effects is remarkable, the Projection Blurring matrix is still a non-stationary 4D matrix, whose large size makes its storage challenging unless simplifications are introduced. For example, the sparsity of the matrix is increased by ignoring the axial component of the blurring [165,55,173,177], or truncating the kernels (i.e. considering that a LOR diblurs just into a few neighboring elements di′) [165,169].The dimensionality of the sinogram blurring matrix is reduced as well by assuming certain spatial invariances. Specifically, some approaches assume that the blurring remains the same for all azimuthal angles and only obtain the matrix terms for a single projection angle of di[165,55,173,174,174]. Other approaches assume as well that the blurring terms are identical for all the ring differences (the axial difference between the two crystals that define the LOR di) and ring planes (i.e. the relative position of the transaxial plane containing the detectors in the scanner). Such approaches apply the projection blurring obtained for the LORs diat the central transaxial plane (i.e. at 0mm from the axial center of the scanner) to all other rings [165,55,174].A third alternative arises in response to the inconveniences of the projection-based approaches. The so-called image-space approaches attempt to incorporate all the blurring effects in the image blurring matrix, based on the idea that the reconstructed image may be seen as the blurring of the true image by a point spread function (PSF) [178]. As stated by [168], resolution modeling in image space is more than only incorporating the effect of the positron range, because the procedures used to estimate the PSFs usually account automatically for the detector resolution and other blurring factors. The image-based approaches have not been extensively explored prior to the recent years, but have considerably evolved in their relatively short life. The simplest approaches represent the PSF as a set of shift-invariant Gaussian kernels, with deviations heuristically estimated [179,180,178,181–184]. Added refinements have been subsequently introduced to these basic kernels, like shift variances [185,186,168,187,188], anisotropy [185,186,168,187,188], the use of more complex functions, like mixtures of Gaussians and exponential [186,189] or Pearson distributions [168], and sophisticated procedures for the estimation of the corresponding spread parameters based on measurements of points [189] or arrays of point sources [188]. In [171] the image blurring matrix is obtained empirically, it is compatible for Time of Flight PET devices and it can correct the sampling artifacts of the geometrical projector. Kotasidis et al. [190] adds the dependency on progressively increasing count rate statistics to the PSF model introduced in [188].The main benefit of the image approaches is that they are computationally more efficient. Even if the image blurring matrices have been proven to be also non-stationary and asymmetrical, the image space is three dimensional, in contrast to the four dimensional projection domain. Furthermore, the image-space approach is attractive because it can easily be applied to list-mode MLEM reconstruction.Since the major factors affecting the spatial resolution occur in the projection space, and with the image-based approaches only approximating these effects, the price for the gained efficiency and versatility could be expected to be the outperformance of projection-based approaches over image-based approaches. Surprisingly, the preliminary comparisons [189,188] between both approaches demonstrate very similar performance. As pointed out by Kotasidis et al. [188] the outcome of such comparisons might be strongly dependent on the specific implementations characteristics of the reconstruction. Therefore, it remains to be demonstrated if the image-based approach is a valid alternative to the projection-based approach until a reliable method to compare these two approaches is well established.At this point, and although the topic is out of the scope of this paper, it is interesting to note that the projection and image-based matrices that characterize the blurring processes in the factored approaches can be used in a variety of ways different from their incorporation to the system matrix. Several works illustrate their use to rebin the sinograms [104], deblur the registered data [191,80,192–194], or to deconvolve the images reconstructed without resolution modeling [186,195–197,168]. In other cases, the kernels are applied between iterations, blurring the forward projection with the projection kernel [198,199] or deconvolving the updated images with the image kernel [200,184]. These approaches are not restricted to the iterative algorithms but can be applied as well to FBP [192] or ART [198] to compensate for the blurring processes. In other cases, novel iterative schemes are developed, in which the image updates are alternated with deblurring update steps [201,202].Although the results in terms of resolution have been shown to be close, in shorter times, to those obtained when the models are included in the system matrix, several authors have observed amplified levels of the statistical noise in the emission data [192] and the background noise in the images [168]. Among the possible reasons for this issue, Rahmim et al. [127] prove the fact that while system matrix modeling is able to improve both resolution and noise characteristics of the reconstructed images, the deconvolution schemes have their tradeoffs. Spinelli et al. [203] propose the use of an anatomical prior in order to reduce the amount of noise introduced when the forward projection is blurred with a projection space kernel.As an alternative to the matrix factorization, several works exist in which the independent models are combined in a scheme of multivariate integrals [204–208,88,69,209]. In these types of schemes, the models for the different effects are approximated by probability distributions. A uniform distribution is normally used for the angular orientation of the emitted photon pairs, which determines the geometrical factor. The crystal penetration is approximated by an exponential distribution representing the probability of interaction of 511keV photons with the crystals, although [208] demonstrates that an uniform distribution works with certain scintillator materials. For the inter-crystal scatter, some authors use Gaussian distributions [208,69] whereas others perform MC simulations to estimate ad hoc distributions [88,172]. A good agreement with fully MC based calculations using less computation has been shown [206–208,88,69] by these approaches.However, in the integral schemes the separation of effects loses the sparsity of their factored counterparts, which makes the storage of the system matrix an impractical task. Furthermore, the resulting integrals are too complex to give place to closed expressions and have to be solved numerically, which makes the on-the-fly calculation of the matrix a computational challenge. In order to speed up the computation, these schemes typically ignore important effects. In several cases [205–207,209] the effect of inter-crystal scatter is ignored, and none of the integral proposals explicitly accounts for the positron range, non-collinearity, detector sensitivity and object attenuation, although [88,207] include the last two in the form of multiplicative corrections. Furthermore, even after these simplifications, only [207,88,209] get to incorporate their models in a reconstruction algorithm, and their implementations are based on several assumptions and approximations of the formulas, that go beyond the unavoidable numerical integration approximations.As stated for the above-mentioned integral approaches, a point has been reached where there exist such complex schemes for the system matrix computation, that they are not yet ready to be implemented for reconstruction (although can be excellent development tools for new system designs). In these circumstances, the focus of the PET system matrix literature is broadening its scope from the development of new sophisticated calculation methods to other important related issues. In this regard, two recurrent themes can be found nowadays in the field of system models for iterative reconstruction, which are addressed in the following subsections.The first current research topic deals with the efficiency of the iterative reconstructions with accurate system models. Concerning the reduction of the long reconstruction times, these works rely mainly on the technological advances that provide increasingly greater computational power. The hardware acceleration of the iterative reconstruction is not new, and dates back to 1987, when [210] implemented the MLEM algorithm in a Cray-I supercomputer. Afterward, implementations of the reconstructions that can run on several CPUs with lower cost using protocols like the message passing interface (MPI) or OpenMP were preferred [132,211,130,212–215], given that the iterative schemes are naturally suited for parallelization and can benefit from the advantages of distributed computing. But nowadays the most appealing solution to speed up the iterative reconstructions is the graphics processing unit (GPU)-based implementation of the algorithms. Although GPUs were initially developed to provide high-definition graphics for video games in real time (at a very low cost due to their sheer commercial volume), recent improvements in the computing power and programability have favored their use as cost effective high performance coprocessors in time-consuming scientific applications. This includes the acceleration of iterative reconstruction with accurate system matrix for PET.In the earlier GPU-implementations the representations of the system responses were not the most sophisticated. In ascending order of complexity, these representations include intersections enhanced with linear interpolation [216], voxel-line distances weighted with Gaussian detector responses [217] and shift invariant [218] or variant [219] Gaussian kernels. However, it has been sufficient to demonstrate the GPU possibilities and has encouraged several groups to perform GPU-reconstructions with system models previously exploited in conventional CPUs, showing remarkable speed-ups. For example, [220–222] propose GPU-accelerated implementations of the MOLAR, FIRST and DIRECT reconstruction packages (and the system models therein) proposed respectively in [87,43,223]. [170,224] implement MLEM and MAP with factorized system matrices and [225] does likewise with the integral model of [204] and the OSEM algorithm.The success in the acceleration of well-established schemes has led to the development of new forms of iterative reconstruction specially suited to GPU implementations, in order to achieve further speed-up. In [226], the MLEM algorithm is reformulated so that it can scale up to many GPU nodes with less frequent inter-node communication, which demonstrates a greater effectiveness when executed on a multi-GPU cluster. In [227], a new system model specially suited for GPU implementation is developed. In this work, detector response functions for each LOR are precalculated in CPUs and modeled by polynomials whose coefficients are stored in the GPU and used to calculate the elements of the matrix on-the-fly. This approximation reduces the number of conditional statements and the required memory size, which matches GPU implementation better. Although the results do not show remarkable speedup with respect with a non-GPU-matched (Gaussian) model, the extent to which the efficiency of acceleration depends on the manner of implementation remains an open question, which will likely give scope for future research.In addition to being an excellent tool for the reduction of the reconstruction times, the hardware acceleration has been exploited to improve the performance of the MC packages [228,229]. This allows not only to improve the tradeoff between time and statistical noise of the simulated models but has even enabled the development of new MC-based methodologies for the scatter correction [230,231].The storage reduction is the second battlefront of the system modeling for iterative reconstruction in the context of the efficiency. The high resolution applications impose an increasingly fine discretization of the image and projection spaces, which translates into an enormous number of basis functions and LORs. These numbers lead to prohibitive dimensions of the system matrix, incompatible with its RAM allocation. As has been mentioned, the use of symmetries is a valuable technique to reduce the memory for storing the system matrix, and has been well studied for cubic voxels in a Cartesian grid, for which the maximum obtainable symmetry is 4-fold in the transaxial plane and 2-fold in the axial dimension. The new strategies to reduce storage requirements point to the exploitation of further symmetries, based on the use of alternatives either to the conventional voxels or to the rectangular assemblies. Concerning the former strategy, the use of polar basis functions is not a new approach [210], but has been recently taken up again by several authors [232–235] in order to explore their potential advantages for fully exploiting the rotational symmetry of cylindrical scanners. As for the latter, in [236], the object grids are rotated by a Gaussian rotator in order to have the same cylindrical symmetry as the scanner, which reduces the system matrix storage by a factor proportional to the number of scanner blocks. Alternatively, [124] propose a new organization of projection data combined with highly rotation-symmetric voxel assemblies, reporting factors of reduction even higher than the transaxial block symmetry of the system.The system matrix dimensions are even larger for scanners with depth of interaction (DOI) measurement, in which the number of crystal pairs increases in proportion to the square of the number of DOI layers. For this kind of scanners, [237] present a methodology for achieving very high rates of system matrix compression, using the concept of virtual detector ring (which permits to represent multiple layers of detector elements in the radial dimension as a single detector volume).The second current research topic is concerned with assessment of the advantages and limitations of the system modeling, and with how they depend on the different metrics and image contexts. In principle, a proper system modeling is assumed to visually improve the images, providing higher resolution for small or narrow structures, as corroborated by the traditional quality metrics. It is questionable, however, if this will make a real difference in the daily performance of PET-based studies, which will be worth the cost of replacing a operating model by a likely more complex alternative. Indeed, some authors argue that these enhancements are deceptive and that they lead to degradation elsewhere. Excellent discussions on the importance of analyzing the statistical properties of the various quantitative metrics, and of properly assessing in which conditions the use of system modeling is beneficial can be found in [238,239]. There are three recurrent topics that these and other studies within this framework address.First, some papers address the features that are affected by the resolution modeling in a less straightforward way than the improved image resolution. For example, [240] evaluates the impact of resolution modeling on the contrast recovery, an issue of the reconstructions which has been less studied than resolution. And above all, the effect on the noise structure, which has been observed to diminish voxel variance while increasing intervoxel covariance [189,127]. [241] provides formulations for how changes in variance and covariance are expected to impact various measures, and evaluates the noise properties in different combinations of reconstruction methods and parameters. [238,242] propose more thorough noise propagation models in order to accurately characterize the noise texture of reconstructed images in the presence of resolution modeling.Second, it is also very important to notice that there are unresolved aspects of the iterative reconstruction with system modeling which remain to be fully understood. A collection of authors have recently investigated the causes and possible corrections of an old problem [47] that becomes much more severe with system modeling, that is, the edge artifacts, reminiscent of the Gibbs ringing overshoot at the edges [243–245,238,242]. Along with the edge-overshoot, [246] considers other artifacts, like hyper-resolution and reduced resolving power.Finally, there is a collection of works that focus on evaluating the impact of the model on the measurement of specific features involved in real-application reconstructions. These include, for example the quantification of several biological parameters in a clinical study of the dopamine transporter [247], the standardized uptake value of lump node metastasis in FDG-PET/CT [248] or the glucose metabolic rate, perfusion and cardiac output in dynamic mouse studies [249]. More generally, [238,242] analyze the limitations that system modeling encounters in the context of quantitative images, where increased intervoxel correlations can lead to significant loss of precision in small regions of interest.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
