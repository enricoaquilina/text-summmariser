@&#MAIN-TITLE@&#
Random Balance: Ensembles of variable priors classifiers for imbalanced data

@&#HIGHLIGHTS@&#
Proportions of the classes for each ensemble member are chosen randomly.Member training data: sub-sample and over-sample through SMOTE.RB-Boost combines Random Balance with AdaBoost.M2.Experiments with 86 data sets demonstrate the advantage of Random Balance.

@&#KEYPHRASES@&#
Classifier ensembles,Imbalanced data sets,Bagging,AdaBoost,SMOTE,Undersampling,

@&#ABSTRACT@&#
In Machine Learning, a data set is imbalanced when the class proportions are highly skewed. Imbalanced data sets arise routinely in many application domains and pose a challenge to traditional classifiers. We propose a new approach to building ensembles of classifiers for two-class imbalanced data sets, called Random Balance. Each member of the Random Balance ensemble is trained with data sampled from the training set and augmented by artificial instances obtained using SMOTE. The novelty in the approach is that the proportions of the classes for each ensemble member are chosen randomly. The intuition behind the method is that the proposed diversity heuristic will ensure that the ensemble contains classifiers that are specialized for different operating points on the ROC space, thereby leading to larger AUC compared to other ensembles of classifiers. Experiments have been carried out to test the Random Balance approach by itself, and also in combination with standard ensemble methods. As a result, we propose a new ensemble creation method called RB-Boost which combines Random Balance with AdaBoost.M2. This combination involves enforcing random class proportions in addition to instance re-weighting. Experiments with 86 imbalanced data sets from two well known repositories demonstrate the advantage of the Random Balance approach.

@&#INTRODUCTION@&#
The class-imbalance problem occurs when there are many more instances of some classes than others [1]. Imbalanced data sets are common in fields such as bioinformatics (translation initiation site (TIS) recognition in DNA sequences [2], gene recognition [3]), engineering (non-destructive testing in weld flaws detection through visual inspection [4]), finance (predicting credit card customer churn [5]), fraud detection [6] and many more.Bespoke methods are needed for imbalanced classes for at least three reasons [7]. Firstly, standard classifiers are driven by accuracy so the minority class may be ignored. Secondly, standard classification methods operate under the assumption that the data sample is a faithful representation of the population of interest, which is not always the case with imbalanced problems. Finally, the classification methods for imbalanced problems should allow for errors coming from different classes to have different costs.Galar et al. [8] systemize the wealth of recent techniques and approaches into four categories:(a)Algorithm level approaches. This category contains variants of existing classifier learning algorithms biased towards learning more accurately the minority class. Examples include decision tree algorithms insensitive to the class sizes, like Hellinger Distance Decision Tree (HDDT) [9], Class Confidence Proportion Decision Tree (CCPDT) [10] and a SVM classifier with different penalty constants for different classes [11].Data level approaches. The main idea in this category is to pre-process the data so as to transform the imbalanced problem into a balanced one by manipulating the distribution of the classes. These algorithms are often used in combination with ensembles of classifiers. This category can be further subdivided into methods that increase the number of minority class examples: Oversampling [12], SMOTE [13], Borderline-SMOTE [14] and Safelevel-SMOTE [15] among others; and methods that reduce the size of the majority class, such as random undersampling, this approach has been used both with and without replacement [16]. These techniques can be jointly applied to increase the size of the minority class while simultaneously decreasing the majority class.Cost-sensitive learning. While traditional algorithms aim at increasing the accuracy by giving equal weights to the examples of any class, cost-sensitive methods, such as cost-sensitive decision trees [17] or cost-sensitive neural networks [18], assign a different cost to each class. The best known methods in this category are the cost-sensitive versions of AdaBoost: AdaCost [19,20], AdaC1, AdaC2 and AdaC3 [21].Ensemble learning. Classifier ensembles have often offered solutions to challenging problems where standard classification methods have been insufficient. One approach for constructing ensembles for imbalanced data is based on using data level approaches: each base classifier is trained with a pre-processed data set. As data level approaches usually use random values, the pre-processed data sets and the corresponding classifiers will be different. Another strategy is based on combining conventional ensemble methods (i.e., not specific for imbalance) with data level approaches. Examples of this strategy are SMOTEBagging [22], SMOTEBoost [23] and RUSBoost [24]. It is also possible to have ensembles that combine classifiers obtained with different methods [25].In general, according to [8], algorithm level and cost-sensitive approaches are more data-dependent, whereas data level and ensemble learning methods are more versatile.Here we propose a new preprocessing technique that can be used to build ensembles, for two-class imbalanced learning tasks, based on a simple randomisation heuristic. The data for training an ensemble member is sampled from the training data using random class proportions. The classes are either undersampled or augmented with artificial examples to make up such a sample.The rest of the paper is structured as follows. Section 2 presents the performance measures used in the experimental evaluation. Section 3 briefly overviews some of the most relevant methods in imbalanced learning, those used in the experimental study. Section 4 explains the proposed method. In Section 5 we provide a simulation example that tries to give some insight in why the method works. An experimental study is reported in Section 6, and finally, Section 7 contains our conclusions and several future research lines.When working with binary classification problems instances can be labelled as positive (p) or negative (n). In binary imbalanced data sets usually the minority class is considered positive while the majority class is considered negative. For a prediction there are 4 possible outcomes: (a) True Positive: prediction is p and the real label is p. (b) True Negative: prediction is n and the real label is n. (c) False Positive: prediction is p and the real label is n. (d) False Negative: prediction is n and the real label is p. Given a test dataset, containing P examples of the positive class and N examples of the negative class, TP is the number of True Positives, FP is the number of False Positives, TN is the number of True Negatives and FN the number of False Negatives.The True Positive Rate (TPR), also called Sensitivity or Recall, is defined asTP/Pand False Positive Rate (FPR) is defined asFP/N. The precision is defined asTP/(TP+FP).Commonly used measures of performance for imbalanced data are the Area Under the ROC (Receiver Operation Characteristic) curve [26], the F-Measure [27] and the Geometric Mean [28]. The F-Measure is defined as2×precision×recallprecision+recall. The Geometric Mean is defined asTP/P×TN/N. The ROC Curve is a two-dimensional representation of classifier performance, it is created by plotting the TPR against the FPR for different decision thresholds. The Area Under the ROC curve (AUC) is a way to represent the performance of a binary classifier using a scalar.In recent years, numerous techniques have been developed to deal with the problem of class-imbalance datasets. This section is a sort summary of the subset of methods tested in this article. The methods are organized using the same classification presented in the introduction:•Data level approaches.–Random Undersampling. This technique will randomly drop some of the examples of the majority class. When it comes to sampling without replacement, an example of the minority class can appear only once in the sub-sampling; with replacement, the same example can appear multiple times.Random oversampling [12] consists of adding exact copies of some minority class examples. With this technique overfitting is more common than in the prior technique.SMOTE (Synthetic Minority Over-sampling Technique [13]) although this technique has “oversampling” in the name, it does not add copies of existing instances, but creates new artificial examples using the following procedure: a member of the minority class is selected and its k nearest neighbours (from the minority class) are identified. One of them is randomly selected. Then, the new example added to the set is a random point in the line segment defined by the member and its neighbour. A value ofk=5has been recommended and is the one used in this study. This method tries to avoid overfitting using a random procedure to create the new samples, but this can introduce noise or nonsensical samples.Ensemble learning. One of the keys for good performance of ensembles is the diversity, there are several ways to inject diversity into an ensemble, the most common is the use of sampling. In Bagging [29], each base classifier is obtained from a random sample of the training data. In AdaBoost [30] the resampling is based on a weighted distribution, the weights are modified depending on the correctness of the prediction for the example given by the previous classifier. Bagging and AdaBoost have been modified to deal with imbalanced datasets:–SMOTEBagging [22] combines Bagging with different amounts of SMOTE and Oversampling in each iteration, so that the data set is completely balanced and consists of three parts: (i) a sample with replacement of the majority class, keeping the original size; (ii) oversampling of the minority class; and (iii) SMOTE of the minority class. The Oversampling percentage varies in each iteration (ranging from 10% in the first iteration to 100% in the last.) The rest of the positive instances are generated by the SMOTE algorithm.SMOTEBoost [23] and RUSBoost [24] are both modifications of AdaBoost.M2 [30], in each iteration, besides the instance reweighting done according to the algorithm Adaboost.M2, SMOTE or Random undersampling is applied to the training set of the base classifier. Boosting based ensembles tend to perform better than bagging based ensembles, however, in Boosting based ensembles, the base classifiers are trained in sequence which slows down the training, and they are more sensitive to noise. SMOTEBoost and RUSBoost are more robust to noise because they introduce a high degree of randomness by creating or deleting instances.Although the most popular methods are modifications or variations of bagging or boosting, there are methods that do not perform resampling, oversampling or undersampling and, instead of that, they make partitions. One method described in [31], which will be called “Partitioning” in this paper, is similar to undersampling based ensembles, it breaks the majority class into several disjoint partitions and constructs several models which use one partition from the majority class and the entire minority class.Most of the above methods, at the same time they increase accuracy in minority class, they decrease overall accuracy compared to traditional learning algorithms. Some approaches combine both types of classifiers, one trained with the original skewed data and other trained according one of the previous approaches in an attempt to cope with the imbalance. Reliability Based Classifier [32] trains two classifiers and then chooses between the output of the classifier trained on the original skewed distribution and the output of the classifier trained according to a learning method addressing the curse of imbalanced data. This decision is guided by a parameter whose value maximizes, on a validation set, the accuracy and a measure designed to evaluate the performance of a classifier in imbalanced classifiers, such as the geometric mean.This section presents the main contribution of the paper. In this section we present a new preprocessing technique called Random Balance, this technique can be used within an ensemble to increase the diversity and deal with imbalance. We also describe a new ensemble method for imbalanced learning called RB-Boost (Random Balance Boost) which is a Random Balance modification of AdaBoost.M2. We also explain the intuition behind the method in an aside subsection.When dealing with imbalanced dataset the three common data-level approaches to balancing the classes are listed below1Note that although random undersampling and SMOTE are mentioned because they are the most used techniques, more sophisticated techniques could be used resulting in variants of the proposed method.1:•The new data set is formed by taking the entire minority class and a random subsample from the majority class. The method has a parameter N that is the desired percentage of instances that belongs to the majority class in the processed dataset. For example, consider a data set with 20 instances in the minority class and 480 instances in the majority class. ForN=40, the desired number of instances from the majority class is 30 so that the 20 instances of the minority class make up 40% of the data.The new data set is formed by adding to it(M/100)×sizeMinoritysynthetic instances of the minority class using the SMOTE method. The amount of artificial instances is expressed as a percentage M of the size of the minority class, and is again a parameter of the algorithm. In the example above, if we chooseM=200, 40 examples from the minority class will be generated through SMOTE.Use both undersampling and oversampling through SMOTE to reach a desired new size of the data and proportions of the classes within.The problem with these data-level approaches is that the optimal proportions depend on the data set and are hard to find, it is known that this proportions have a substantial influence on the performance of the classifier. The proposed method relies completely on randomness and repetition to try to overcome this problem.While preprocessing techniques are commonly used to restore the balance of the class proportions to a given extent, Random Balance relies on a completely random ratio. This includes the case where the minority class is over-represented and the imbalance ratio is inverted.An example of the sampling procedure can be seen in Fig. 1. Given a data set, a different data set of the same size is obtained for each member of ensemble where the imbalance ratio is chosen randomly. In this example, the initial proportions of both classes appears on the top. Classifiers1,2…,Tare trained with variants of this data set where the ratio between classes varies randomly. In iteration 1, the imbalance ratio has been slightly reduced. In iteration 2, the ratio is reversed, the size of the previous minority class exceeds the size of the previous majority class. And in iteration 3, the minority class has become even smaller. All these cases are possible since the procedure is random.The procedure is described in the pseudocode in Algorithm 1. The fundamental step is to randomly set the new size of the majority and minority classes (lines 6–7). Then SMOTE and Random Undersampling (resampling without replacement) are used to respectively increase or reduce the size of the classes to match the desired size (lines 8–11 or lines 12–15 as required.).We call this generic ensemble method Ensemble-RB. Additionally, it can be combined with Bagging, resulting in what we call Bagging-RB.Pre-processing strategies can have important drawbacks. Undersampling can throw out potentially useful data, while SMOTE increases the size of the dataset and hence the training time. Random-Balance maintains the size of the training set and because it is a process which is repeated several times, the problem of removing important examples is reduced.Algorithm 1Pseudocode for the Random Balance ensemble method.Random BalanceXRequire: Set S of examples (x1,y1), …, (xm,ym) wherexi∈X⊆Rnandyi∈Y={-1,+1}(+1: positive or minority class,-1: negative or majority class), neighbours used in SMOTE, kEnsure: New setS′of examples with Random Balance1:totalSize←S2:SN←{(xi,yi)∈S|yi=-1}3:SP←{(xi,yi)∈S|yi=+1}4: majoritySize←SN5: minoritySize←SP6: newMajoritySize←Random integer between 2 and totalSize-2//Resulting classes will have at least 2 instances7: newMinoritySize←totalSize – newMajoritySize8: ifnewMajoritySize<majoritySizethen9:S′←SP10: Take a random sample of size newMajoritySize fromSN, add the sample toS′.11: CreatenewMinoritySize-minoritySizeartificial examples fromSPusingSMOTE, add these examples toS′.12: else13:S′←SN14: Take a random sample of size newMinoritySize fromSP, add the sample toS′.15: CreatenewMajoritySize-majoritySizeartificial examples fromSNusingSMOTE, add these examples toS′.16: end if17: returnS′The data sets generated in Random Balance have instances from the original training data and artificial instances. For Random Balance, the probability of including an instance is different for minority and majority instances. Given p positive instances and n negative instances withm=n+pand assuming thatp⩾2the probability of including an instance of the minority class is:Pmino=1m-3∑i=2p-1ip+∑i=pm-21=1m-3m-p+32-1pIn the generated data set, each class has at least two instances. Then, there aren-3possible sizes of the minority class in the generated data sets (from 2 tom-2). The summation∑i=2p-1ipis for the cases when the number of instances in the minority class is reduced (from p instances we randomly take i so the selection probability isi/p), while∑i=pm-21is for the cases when the minority size is increased (the selection probability is 1).Analogously, the probability of selecting an instance of the majority class will be:Pmajo=1m-3m-n+32-1nFig. 2shows the probabilities of selecting an instance in the generated data set as a function of the percentage of instances from the minority class, for a data set with 1000 instances. The probability of selecting an instance of the minority class decreases when the data set is more balanced.It can be seen that ifp⩽nthenPmajo⩽Pmino,Pmino⩾0.75andPmajo⩾0.5. For a perfectly balanced data set, the probability of selecting an instance is a bit greater than 0.75 because there will be at least two instances of each class. The problem of discarding important instances of the majority class is ameliorated because the expected number of base classifiers that are trained with a given instance of the majority class is greater than 50%. Moreover, some of the instances included in the data set will also used to generate artificial instances.The ROC space is defined by FPR and TPR as x and y axes respectively because there is a trade-off between this two values. A classifier can be represented as a point in this space and all base classifiers in an ensemble can be represented as a cloud of points. Fig. 3a shows the cloud of points for a Bagging ensemble trained with the credit-g dataset, the color of each point represents the percentage of the instances than belong to the positive class in the dataset used for training that base classifier. It is easy to appreciate that all of the members of the ensemble are trained with samples which vary very slightly the proportion between classes. In contrast, Fig. 3b shows the cloud for an ensemble of Random Balance classifiers, the large variability in the ratio between classes in the datasets used to train each of the base classifiers, including cases in which the positive class becomes larger than the negative, makes the base classifiers of the ensemble spread out over the ROC space.In the proposed method, the base classifiers are forced to learn different points on the ROC space and thereby expected to be more diverse and to improve the ensemble performance (see Fig. 3). Diversity is generally considered beneficial for ensemble methods, including the imbalanced case [33].There are several modification of AdaBoost.M2 for imbalanced problems. The best known of these methods is SMOTEBoost [23].As in AdaBoost.M2, the examples of the training data have weights that are updated according to a pseudo-loss function. For each base classifier the weighted training data is augmented with artificial examples generated by SMOTE.RUSBoost [24], as SMOTEBoost, is also an AdaBoost.M2 modification, but in this case instances of the majority class are removed using random undersampling in each iteration. No new weights are assigned; the weights of the remaining instances are normalized according to the new sum of weights of the data set. The rest of the procedure is as in AdaBoost.M2 and SMOTEBoost.Both methods apply a preprocessing technique to the data and simultaneously alter the weights. Following this philosophy we propose RB-Boost, whose pseudocode is described in Algorithm 2. It is also a modification of AdaBoost.M2, in which line 3 is changed to generate a data set according to the procedure shown in Fig. 1. The number of instances removed by undersampling is equal to the number of instances introduced by SMOTE. The algorithm works as follows: for each of the T rounds (lines 2–11) a data setSt′is generated according to the Random Balance procedure (line 3). DistributionDt′is updated, maintaining for each instance of the original data set its associated weight and assigning a uniform weight to the artificial examples (line 4). Then a weak learning algorithm is trained usingSt′andDt′(line 5), this classifier will give a probability between 0 and 1 to each class.2In experiments, J48 classification tree with Laplace smoothing has been used as a weak classifier. The prediction returned by the classifier is the probability calculated taking into the instances that end in the leaf. With Laplace smoothing this is(ai+1)/(A+c), whereaiis the number of instances of class i in the leaf, A is the total number of instances in the leaf, and c the number of classes.2The pseudo-lossεtof the weak classifierhtis computed according to the formula presented in line 6. The distributionDt′is updated to make the weights associated with wrong classifications higher than the weights given to correct classifications (line 7–9). Finally, the different classifiers outputs are combined (line 11) taking into account their respectiveβt(obtained in line 7).Algorithm 2Pseudocode for the RB-Boost ensemble method.RB-BoostRequire: Set S of examples(x1,y1), …,(xm,ym)wherexi∈X⊆Rnandyi∈Y={-1,+1}(+1: positive or minority class,-1: negative or majority class),Weak learner, weakLearnNumber of iterations, TNumber of neighbours used in SMOTE, kEnsure: RB-Boost is built//Initialize distributionD11:D1(i)←1mfori=1,…,m2: fort=1,2, …, Tdo3:St′←RandomBalance(S,k)4:Dt′(i)←Dt(j)ifSt′(i)=St(j)else1m, fori=1,…,m//If the example is from the sample it maintains its weight, if the example is artificial it has the initial weight.5: UsingSt′and weightsDt′, train weakLearnht:X×Y→[0,1],6: Compute the pseudo-loss of hypothesisht:εt=∑(i,y):yi≠yDt(i)(1-ht(xi,yi)+ht(xi,y))7:βt←εt/(1-εt)8: UpdateDt:Dt+1(i)←Dt(i)·βt12(1+ht(xi,yi)-ht(xi,y))9: NormalizeDt+1: LetZt←∑iDt+1(i)Dt+1(i)←Dt+1(i)Zt10: end for11: returnhf(x)=argmaxy∈Y∑t=1Tlog1βtht(x,y)To test-run the idea we carried out experiments with generated data. By contrasting the Random Balance with Bagging, we intend to gain more insight and support for our hypothesis that the Random Balance heuristic improves diversity in a way which leads to larger AUC.3The varying parameter for the ROC curve is the threshold on the class membership probability estimated by the whole ensemble, not a particular base classifier.3We generated two 2-dimensional Gaussian classes centred at (0, 0) and (3, 3), both with identity covariance matrices. To simulate unbalanced classes, 450 points were sampled from the first class, and 50 points from the second class (10%). Each ensemble was composed of 50 decision tree classifiers.4MATLAB’s Statistic Toolbox was used for training the decision trees and estimating AUC.4The ensemble output was calculated as the average of the individual outputs. An example of the classification boundaries for the Random Balance ensemble and the Bagging ensemble is shown in Fig. 4. To evaluate the individual and ensemble accuracies as well as the AUC, we sampled a new data set from the same distribution and of the same size. The numerical results for this illustrative example are given in Table 1.It can be observed that the boundary lines for the Random Balance ensemble are more widely scattered compared to these for the Bagging ensemble, stepping well into the region of the majority class. Table 1 shows also the average results from 200 iterations, each iteration with freshly sampled training and testing data. The results indicate that: (i) individual errors of the decision trees for the ensemble-RB are larger than these for the Bagging ensemble, (ii) RB has a higher classification error than Bagging, and (iii) RB has a better AUC than Bagging. All differences were found to be statistically significant (two-tailed paired t-test,p<0.005). This suggests that the better AUC produced by the ensemble-RB may come at the expense of slightly reduced classification accuracy. Since AUC is often viewed as the primary criterion for problems with unbalanced classes, the results of this simulation favor the ensemble-RB.Kappa-error diagrams are often used for comparing classifier ensembles [34,35]. Consider a testing set with N examples and the contingency table of two classifiers,C1andC2.C2correctC2wrongC1correctabC1wrongcdwhere the table entries are the number of examples jointly classified as indicated, anda+b+c+d=N. Diversity between the two classifiers is measured byκ[36] as(1)κ=2(ad-bc)(a+b)(b+d)+(a+c)(c+d)Kappa is plotted on the x-axis on the diagram. Smaller kappa indicates more diverse classifiers. The averaged individual error for the pair of classifiers is(2)e=12c+dN+b+dN=b+c+2d2NThe error is plotted on the y-axis of the diagram. An ensemble with L classifier generates a “cloud” ofL(L-1)/2points on the kappa-error diagram, one point for a pair of classifiers.We calculated the centroid points of 200 RB and 200 Bagging ensemble clouds following the simulation protocol described above. Fig. 5shows the kappa-error diagrams with the centroids, 200 in each subplot. The black points correspond to ensembles whose AUC is larger than the respective AUC of the rival ensemble. Out of the 200 ensembles, RB had larger AUC in 127 cases, which is seen as the larger proportion of black triangles in the left subplot compared to the proportion of black dots in the right subplot.As expected, the ensemble-RB generates substantial diversity compared to Bagging, which is indicated by the stretch to the left of the set of points in the left subplot. The cloud of points is tilted, showing that the larger diversity is paid for by larger individual error. An interesting observation from the figure is that the black markers (triangles and dots) are spread uniformly along the point clouds, suggesting that there is no specific diversity–accuracy pattern which is symptomatic of better AUC.Two collections of data sets were used. The HDDT collection5Available at http://www.nd.edu/dial/hddt/.5contains the binary imbalanced data sets used in [37]. Table 2shows the characteristics of the 20 data sets in this collection.The KEEL collection6Available at http://sci2s.ugr.es/keel/imbalanced.php.6contains the binary imbalanced data sets from the repository of KEEL [38]. Table 3shows the characteristics of the 66 data sets in this collection7Notice that several of the data sets come from data sets that were originally multiclass, the 66 datasets have been derived from 16 original sets.7.In both tables, the first column is the name of the data set, the second the number of examples, the third the number of attributes and the last is the imbalance ratio (the number of instances of the majority class for each instance of the minority class).Many data sets in these two collections are available or are modifications of data sets in the UCI Repository [39].Weka [40] was used for the experiments. The ensemble size was set to 100, for some methods this is not the exact size, but it is the maximum, since some method have a stopping criteria. J48 was chosen as the base classifier in all ensembles.8J48 is the Weka’s re-implementation of C4.5 [41].8As recommended for imbalanced data [37], it was used without pruning and collapsing but with Laplace smoothing at the leaves. C4.5 with this options is called C4.4 [42].The results were obtained with a5×2-fold cross validation [43]. The data set is halved in two folds. One fold is used for training and the other for testing, and then the roles of the folds are reversed. This process is repeated five times. The results are the averages of these ten experiments. Cross validation was stratified: the class proportions was approximately preserved for each fold.Given the large number of methods and variants tested, the comparisons are divided into families. Each family includes different types of classifier ensembles depending on the main diversity-generating strategy. We distinguished three such families: Data-preprocessing-only, Bagging and Boosting. The names, abbreviations and descriptions of the methods can be found in Tables 4–6.The scores obtained by the proposed methods: E-RB, BAG-RB and RB-B are shown in Table 7, the reader is encouraged to consult the full table of results in Supplementary material. Some of the methods obtain low result in certain datasets. The reason is that some of the performance measures are a geometric mean (the G-mean) and a harmonic mean (the F-measure) so the results are biased towards the lower of the two values that are combined in the measure. With a classifier that always predict the majority class the accuracy will be very high (depending on the imbalance ratio), the AUC will be 0.5 if all the instances are given the same confidence; but for these two means the value will be 0.We used the most common configurations of SMOTE where the number of synthetic instances was set to 100%, 200% and 500% of the minority class. In the variants called ESM and BAGSM, the minority class was oversampled to match the size of the majority class. For the undersampling ensembles, the size of the majority class was reduced to match the size of the minority class.In addition, optimized versions of some the ensemble methods were tried. In the Data-preprocessing-only and the Bagging families we included three versions: optimizing the amount of SMOTE oversampling, optimizing the amount of Undersampling and optimizing both simultaneously. In all these variants we used a 5-fold internal cross-validation9That means that the training set is repeatedly divided into train and validation sets to find the optimal parameter value, and then the classifier is finally built using the complete training set.9and tested 10 different amounts of SMOTE and Undersampling, which means that the version that optimizes both parameters simultaneously has evaluated 100 possible combinations. These amounts are expressed in terms of the difference between the majority and minority class sizes, as shown in Fig. 6, for SMOTE, in this case, a value of 0% means not to add any instance, a value of 100% means to create as many as necessary to match the size of the majority. For Undersampling a value of 0% means not to delete any instance, a value of 100% means remove instances to match the original size of the minority class. Once found, the parameters that maximize the AUC for a single decision tree are used for constructing the ensemble.The Data-preprocessing-only also includes the Partitioning (or Random Splitting) method, described in [31]. In that work, the ensemble size was the Imbalanced Ratio, while in this work it is 100,10To achieve this size, as many partitions as necessary are created. e.g. if the imbalance ratio is 5, the 100 classifiers are created using 20 times the partitioning technique.10as for the other methods in this section, in order to make a fair comparison.The Bagging family includes the Reliability-based Balancing (RbB) method [32]. The classifiers obtained with this method can be seen as a mini-ensemble of two classifiers, the first one using the original imbalanced class distribution (IC), the second one using a classifier with balanced data (BC). In order to have ensembles of 100 classifiers, two ensembles of 50 classifiers are combined. The first classifier is obtained with Bagging. For the second classifier two configurations are considered: Bagging with SMOTE and Bagging with Undersampling. RbB uses a threshold to determine which label return, when the reliability provided by IC is larger than the threshold, the final label corresponds to the label returned by IC, in the opposite case the label corresponds to the label returned by BC. This threshold is selected for each dataset, considering the values from 0.0 to 1.0 in steps of size 0.05, the threshold chosen is the one for which the sum of accuracy and geometric mean is maximized over a validation dataset.The Data-preprocessing-only family includes the Random Balance ensemble (E-RB), while Bagging family includes the combination of Bagging and Random Balance (BAG-RB).In the Boosting family, we have compared the most popular algorithms. For completeness, we included the standard boosting variants AdaBoost.M1 and MultiBoost. Both were tested with reweighting as well as with weighted resampling [44].The main contenders in this family were the boosting variants especially designed for unbalanced data sets: SMOTEBoost, with three different rates of SMOTE, and RUSBoost.The proposed method: RB-Boost was also added to the Boosting family.For comparison between multiple algorithms for each family and multiple data sets we used average ranks [45]. For a given data set, the methods are sorted from best to worst. The best method receives rank 1, the second best receives rank 2, and so on. In case of a tie, average ranks are assigned. For instance, if two methods tie for the top rank, they both receive rank 1.5. Average ranks across all data sets are then obtained.The first question is whether there are any significant differences between the ranks of the compared methods. The Friedman test and the subsequent version of Iman and Davenport [46] test were applied.To detect pairwise differences between a designated method and the remaining methods, we used the Hochberg test [47], which was found to be more powerful than the Bonferroni–Dunn test [48,49].Table 8a shows the results of the comparison of the algorithms of the Data-preprocessing-only family in the form of average ranking calculated from the area under the curve. The second column shows the average rank of each method. The Iman and Davenport test gives a p-value of 6.1904e−86, which means that it rejects the hypothesis that the compared algorithms are equivalent. The last column shows the adjusted Hochberg p-value between E-RB and the respective method of that row. An adjusted p-value less than 0.05 means that the two methods are significantly different with a significance ofα=0.05. The table shows that the Random Balance ensemble (E-RB) has a demonstrably better AUC than all the other ensembles in this family.Table 8b shows the average ranks for the Bagging family calculated using the same measure. With p-value of 8.1240e−56, the Iman and Davenport test discards the hypothesis of equivalence between the algorithms. The combination of Bagging with the proposed method obtains the best ranking and also presents significant differences with the other methods.Table 8c shows the average ranks for the Boosting family. With p-value of 1.0638e−37 the Iman and Davenport test discards the hypothesis of equivalence. The proposed algorithm RB-Boost takes the top spot for the AUC criterion, and there are significant differences with all other algorithms, except RUSBoost, which occupies the second position (adjusted Hochberg’s p-value of 0.10634).Table 9a shows the average ranks for the data-processing according to the F-Measure. In this case the Iman and Davenport test gives a p-value of 2.3794e−44, so the compared algorithms are not equivalent. The Random Balance ensemble gets the best ranking, but this time there are no statistically significant differences with the next three algorithms.Table 9b shows the average ranks for the Bagging family according to the F-Measure. The Iman and Davenport test discards the hypothesis of equivalence between the algorithms with p-value of 1.2896e−23. The proposed method obtains the second highest ranking, but there are no significant differences from the first method.Finally, Table 9c shows the average ranks for the Boosting family. With p-value of 6.912e−11 the Iman and Davenport test discards the hypothesis of equivalence between the algorithms. The proposed algorithm has the best place in the ranking with significant differences with all remaining algorithms in this family.Fig. 7shows scatter plots with the average ranks for the three families of methods. The best methods according to the AUC appear at the left, and the best methods according to the F-Measure appear at the bottom.Similar patterns appear in the left and center plots:•In the case of data processing family, ensembles which only use Random Undersampling (ERUS, ERUSR and EPart) obtain the three worst results for the F-Measure but according to the AUC criterion they are much better, only surpassed by E-RB.The ensembles that apply only SMOTE (ESM/100/200/500, EopS) are grouped into a cluster and methods that combine bagging and SMOTE (BAGSM/100/200/500, BAGopS) are grouped into another cluster.The proposed method appears far ahead of the other methods on the AUC criterion and is the best or the second best on the F-Measure criterion.The right plot, showing the Boosting family, reveals that the methods are much closer to the diagonal line where the ranks for the AUC and the F-Measure are identical. The proposed method RB-Boost is located at a considerable distance from the other methods on both axes, which indicates its advantage.Table 10shows the rankings of the three families according to the geometric mean. The proposed methods get the best positions in the data processing and bagging families, in both cases significantly according to Hochberg’s Test. But it gets the third position in the Boosting family ranking.Although accuracy is not usually considered an adequate performance measure for imbalanced data, for the sake of completeness, Table 11shows the average ranks for the considered ensemble methods according to this measure. As it could be expected, the methods that do not consider imbalance (i.e., Bagging, AdaBoost and MultiBoost) have the top ranks for their respective families. .In this paper we have used several different measures to evaluate the performance of various methods. Some measures such as AUC, F-Measure and Geometric Mean are specific to unbalanced datasets, while accuracy is not specific to unbalanced. A combined average rank has been calculated to show the overall performance of the four measures. This time the average rank for each method is the average of their average ranks for each measure. Table 12shows the average ranks for the considered ensemble methods according to the combination of measures. In all families, the proposed method obtains the best position. In this case it is not appropriate to apply any test to detect equivalence between methods or pairwise differences because the values are not independent, for each dataset-algorithm pair there are several values (one per measure).After comparing the methods within their own families, we performed a comparison between the methods that have achieved first place in their respective rankings.Table 13shows the average ranks for the best methods in each family, calculated for each different measure. With p-values of 8.113e−6 and 0.04933 the Iman and Davenport test discards the hypothesis of equivalence between the algorithms in AUC and F-Measure. By contrast a p-value of 0.91474, in the case of ranking calculated with the best methods according to their geometric means, indicates that there is not significant differences between methods, RUSBoost obtains the top position but it is equivalent to the next two methods.In the ranking calculated from the AUC, the best position is for Bagging-RB, which shows significant differences with Ensemble-RB. In the ranking calculated with the F-Measure, the best position is for RB-Boost. In this case, despite the p-value given by the Iman and Davenport test, the post hoc Hochberg test found no significant differences between the methods atα=0.05; the p-value of Hochberg between the first ranking method and the last one is 0.05954. The method which obtains the best rank according to accuracy is Multiboost with resampling, but we emphasize that accuracy is not the best measure to evaluate classification methods in imbalanced dataset. And finally, the method that obtains the best average ranking considering all measures is one of the proposed methods: Random Balance Boost (RB-B).The outputs of the classifiers in an ensemble can be combined in several ways [50]. For Ensemble-RB and Bagging-RB, the outputs are combined using the simple average of probabilities. For RB-Boost, the outputs are combined using a weighted average (line 11 in Fig. 2), because it is the method used in AdaBoost.M2 and its variants for imbalance (RUSBoost, SMOTEBoost).This section considers other combination methods for Ensemble-RB and Bagging-RB: majority voting and product of probabilities. Tables 14 and 15show the average ranks for the considered fusion rules. Iman and Davenport Test discards the hypothesis of equivalence between the algorithms in all cases. Ensemble-RB and Bagging-RB show the same behavior: for AUC the order of fusion rules is average, product and majority voting, while for F-Measure the order is majority voting, average and product. When comparing the best method with the remaining methods, the adjusted p-values for Hochberg’s procedure are small (<0.015). Hence, which fusion rule is used gives significant differences.Decision trees are usually used as base classifiers, since they are simple and fast to compute, and they are unstable (small variations in the training set can result in different trees and different predictions), which contributes to the diversity of the ensemble. This section considers the performance of the proposed ensemble methods with other two base classifiers: nearest neighbour (1-NN) and SVM with Gaussian kernel. Due to the high computational cost of SVM classifiers, the size of all ensembles used in the comparison of this section was set to 50.Tables 16–18show the average ranks for, respectively, Ensemble-RB, Bagging-RB and RB-Boost. These tables show, for AUC and F-Measure, the average ranks of the three considered base classifiers with the corresponding ensemble methods. Decision trees work better in these ensembles than the other two considered alternatives: in all the ranks, decision trees have the top position. The differences are larger for AUC than for F-Measure.In all the previous experiments the ensemble size was 100. This section considers the effect of the ensemble size in the performance. Fig. 8shows the performance as a function of the ensemble size. The ensemble sizes vary from 5 to 100 in steps of size 5. The top two plots show the average value of the performance measure (AUC or F-Measure) across all the data sets, for Ensemble-RB, Bagging-RB and RB-Boost. As usual with ensembles, the performance improves with size, but the improvements are smaller as the size grows. The bottom two plots shows the performance but using average ranks instead of the mean across all the data sets. For each method, 20 sizes are considered(5,10,15,…,100), and the average ranks are computed for them. The values are in the interval [1,20], and smaller values represent better performance. The average ranks are better as the ensemble size increases.Fig. 9shows six plots, each one compares a pair of methods across the considered ensemble sizes. Each plot shows, as a function of the ensemble size, the percentage of data sets with the best performance in terms of AUC (left plots) or F-Measure (right plots). The selected pairs are the two methods with best average ranks in each family (Tables 8 and 9).According to the AUC, the methods based on RB have a percentage of victories around 70%. In the left center plot, when comparing Bagging-RB with BAGopB (Bagging with the amount of SMOTE and Undersampling selected by cross validation), the initial percentage is smaller but it increases to greater values with the ensemble size.For the F-Measure (right plots), Ensemble-RB and Bagging-RB are not better than the corresponding pairs (ESM200 and BAG500), this was expected as in Tables 8 and 9 the differences for the considered pairs were not significant. When comparing RB-Boost with RUSBoost the difference is greater, although it decreases with the ensemble size.

@&#CONCLUSIONS@&#
