@&#MAIN-TITLE@&#
FReET: Software for the statistical and reliability analysis of engineering problems and FReET-D: Degradation module

@&#HIGHLIGHTS@&#
FReET software for statistical and reliability analyses has been developed.Small-sample simulation techniques have been implemented.Degradation modeling enables degradation module FReET-D.Applications of software for reliability analysis of concrete structures are listed.

@&#KEYPHRASES@&#
Statistical analysis,Sensitivity,Reliability,Monte Carlo simulation,Latin Hypercube Sampling,Simulated annealing,Random fields,Material degradation,

@&#ABSTRACT@&#
The objective of the paper is to present methods and software for the efficient statistical, sensitivity and reliability assessment of engineering problems. Attention is given to small-sample techniques which have been developed for the analysis of computationally intensive problems. The paper shows the possibility of “randomizing” computationally intensive problems in the manner of the Monte Carlo type of simulation. In order to keep the number of required simulations at an acceptable level, Latin Hypercube Sampling is utilized. The technique is used for both random variables and random fields. Sensitivity analysis is based on non-parametric rank-order correlation coefficients. Statistical correlation is imposed by the stochastic optimization technique – simulated annealing. A hierarchical sampling approach has been developed for the extension of the sample size in Latin Hypercube Sampling, enabling the addition of simulations to a current sample set while maintaining the desired correlation structure. The paper continues with a brief description of the user-friendly implementation of the theory within FReET commercial multipurpose reliability software. FReET-D software is capable of performing degradation modeling, in which a large number of reinforced concrete degradation models can be utilized under the main FReET software engine. Some of the interesting applications of the software are referenced in the paper.

@&#INTRODUCTION@&#
The presence of uncertainty in the analysis and design of engineering systems has always been recognized. Uncertainties are involved in every part of the system: structure – load – environment. Traditional approaches simplified the problem by considering the uncertain parameters to be deterministic, and accounted for the uncertainties through the use of partial safety factors in the context of limit states. Such approaches do not absolutely guarantee the required reliability and they do not provide information on the reliability achieved and/or on the influence of individual parameters on reliability. Therefore, attention is being given today to fully probabilistic approaches and software tools which can be used for such purposes. Important topics can thus be treated in an advanced manner, e.g. the probabilistic vulnerability assessment of civil infrastructure systems followed by efficient decision-making processes.The standard definition of an engineering problem featuring uncertainty or randomness which is to be analyzed using computers is as follows. A random response of the studied engineering system (e.g. a structure) is represented by random variable Z. In statistical analyses, Z may represent a random response of a system (e.g. deflection, stress, ultimate capacity, etc.) or, during reliability determination, Z is called a safety margin. Random variable Z is a function of basic random variablesX=X1,X2,…,XNvar(or random fields):(1)Z=g(X),where the function g(X), a computational model, is a function of a random vector X (and also of other, deterministic quantities). Random vector X follows a joint probability distribution function (PDF) fX(X) and, in general, its marginal variables can be statistically correlated. This paper deals with situations when the information about fX(X) is limited to the knowledge of univariate marginal distributionsf1(x),…,fNvar(x)and a correlation matrix, T (a symmetric square matrix of order Nvar). The output variable (or generally a vector) Z represents a transformed variable and the task is to perform statistical, sensitivity and possibly reliability analyses upon it. It is assumed that the analytical analysis of the transformation of input variables to Z is not possible.Approaches focused on the estimate of statistical moments of response quantities, such as means or variances, are commonly termed statistical analyses. In sensitivity analysis, approaches aiming at the quantification of the sensitivity of output (response, failure probability) to variations in input variables are applied. The main result of reliability analysis is an estimate of the theoretical failure probability.Statistical analyses can be viewed as estimates of probabilistic integrals [1]. For example, the estimate of the mean value of Z is, in fact, an approximation to the following integral:(2)μg=∫g(X)fX(X)dX.Higher statistical moments of the response can be obtained by integrating polynomials of g(X).If g(X) represents a failure condition, then it is called the limit state function and Z becomes the safety margin. Usually, the convention is that it takes a negative value if a failure event occurs; Z⩽0, and a survival event is defined asZ=g(X)>0. The limit state function can be an explicit or implicit function of basic random variables and it can take either a simple or a rather complicated form (e.g. a computer program). The performance of the system and its components may be described considering a number of limit states (multiple limit state functions). The aim of reliability analysis is the estimate of unreliability using a probability measure called the theoretical failure probability, defined as(3)pf=P(Z⩽0).This failure probability is again calculated as a probabilistic integral:(4)pf=∫I[g(X)]fX(X)dX=∫DffX(X)dX.The functionI[g(X)]is an indicator function that equals one for failure event (g⩽0) and zero otherwise. In this way, the domain of integration of the joint PDF above is limited to the failure domain Dfwhere g(X)⩽0.The explicit calculation of the integral in Eq. (2) or the failure probability integral in Eq. (4) is generally impossible. A large number of efficient stochastic analysis methods have therefore been developed during the last seven decades.A straightforward solution for these tasks is numerical simulation. The interest in simulation methods started in the early 1940s with the purpose of developing inexpensive techniques for testing engineering systems by imitating their real behavior. These methods are commonly called Monte Carlo simulation techniques. The principle behind the method is to develop an analytical model – a computer based response or limit state function (Eq. 1) that predicts the behavior of the studied system and repeats it many times under many possible conditions. This simulation principle has remained formally the same up until the present day.For example, the mean value of Z can be estimated using the best linear unbiased estimator (the arithmetical mean) as:(5)μg‾≈1Nsim∑i=1Nsimg(Xi).The Nsim samples Xi(realizations, integration points) of the basic random vector X are selected to have an identical probability 1/Nsim (see the weighting function in Eq. (2)). Similarly, the failure probability can be estimated as the ratio of the number of samples that yield failure to the total number of samples Nsim.The common feature of the many different techniques covering all the above-mentioned categories is the fact that they require repetitive evaluation (simulation) of the response or limit state function g(X). The development of methods is from a historical perspective a struggle to decrease the amount of simulations, or avoid an excessive number of them. Crude Monte Carlo simulation cannot be applied to time-consuming problems, as it requires a large number of simulations (the repeated calculation of structural response) to deliver statistically significant estimates of the outputs.In the context of reliability analyses, this obstacle was historically successfully solved for by the approximation techniques FORM and SORM, e.g. [2,3]. In spite of some problems concerning accuracy, these techniques are widely accepted today and have become in some cases standard tools in code calibration. Once this was achieved, research then focused on the development of advanced simulation techniques which concentrate simulations in the failure region [4]. Among the many efficient methods developed during the last decades, Latin Hypercube Sampling and response surface methodologies are often used e.g. for computationally demanding continuum mechanics problems.The development of many reliability methods, varying in efficiency, accuracy and suitability for a particular class of problems, can be tracked from the proceedings of major reliability conferences e.g. ICOSSAR [5] and ICASP [6]. Two main groups can be distinguished: simulation and approximation methods. These methods are implemented in many different modified versions in statistical and reliability software. Such software usually offers the possibility of working with a user-defined limit state function (a multipurpose use) or is integrated fully with a specific FEM solver.The objective of the paper is to present methods for efficient statistical, sensitivity and reliability assessment implemented in FReET software [7]. Attention is given to those techniques that have been developed for the analysis of computationally intensive problems; nonlinear FEM analysis being a typical example. The paper shows the possibility of “randomizing” computational tasks in the sense of the Monte Carlo type of simulation. The stratified simulation technique Latin Hypercube Sampling is used in order to achieve variance reduction of the estimated outputs at a given number of simulations. The technique is used both at the level of random variables and that of random fields.The paper contains basic information on FReET software, its degradation module and the implemented methods with relevant references. The most interesting applications of FReET software are referenced in the paper.For time-intensive calculations, small-sample simulation techniques based on stratified sampling of the Monte Carlo type represent a rational compromise between feasibility and accuracy. Therefore, Latin Hypercube Sampling (LHS) [8–10], which is well known today, has been selected as a key fundamental technique. LHS belongs to the category of advanced stratified sampling techniques which result in the very good estimate of statistical moments of response using small-sample simulation. More accurately, LHS is considered to be a variance reduction technique, as it yields lower variance in statistical moment estimates compared to crude Monte Carlo sampling at the same sample size; see e.g. [11]. This is the reason the technique became very attractive for dealing with computationally intensive problems like e.g. complex finite element simulations. The software therefore uses the main icon depicted in Fig. 1.The basic feature of LHS is that the range of univariate random variables is divided into Nsim intervals (Nsim is a number of simulations); the values from the intervals are then used in the simulation process (random selection, median or the mean value). The selection of the intervals is performed in such a way that the range of the probability distribution function of each random variable is divided into intervals of equal probability,1/Nsim. The samples are chosen directly from the distribution function based on an inverse transformation of the univariate distribution function. The representative parameters of variables are selected randomly, being based on random permutations of integers k=1, 2,…,Nsim.Every interval of each variable must be used only once during the simulation. Being based on this precondition, a table of random permutations can be used conveniently, each column of such a table belonging to a specific simulation and each row corresponding to one of the input random variables; see the bottom of Fig. 2.It has been shown that a preferable LHS strategy is the approach suggested in [12,13], where the representative value of each interval is the mean value (Fig. 3):(6)xi,k=∫yi,k-1yi,kxfi(x)dx∫yi,k-1yi,kfi(x)dx=Nsim∫yi,k-1yi,kxfi(x)dxHere fiis the probability density function of variable Xi, and the integration limits are:(7)yi,k=Fi-1kNsim,k=1,…,NsimThe sample averages equal exactly the mean values of variables and the variances of the sample sets are much closer to the target values compared to other selection schemes, see [14] for details. For some probability density functions (including e.g. Gaussian, Exponential, Laplace, Rayleigh, Logistic, Pareto, etc.) the integral (6) can be solved analytically. For others, the extra effort of computing the numerical integration is definitely worthwhile.Once Nsim samples of each marginal variable are generated, separately, the correlation structure prescribed by the target correlation matrix must be taken into account. There are generally two problems related to the statistical correlation: First, during sampling an undesired correlation can occur between the random variables [15]. For example, instead of a correlation coefficient of zero for the uncorrelated random variables an undesired correlation of e.g. 0.4 can be generated. This can happen especially in the case that only a very small number of simulations (in the order of tens) are carried out, where the number of interval combinations is rather limited. The second task is to introduce the prescribed statistical correlation between the random variables defined by the correlation matrix.This can be achieved by rearranging the order of samples of each variable in the LHS simulation plan in such a way that either they diminish the undesired random correlation when unit matrix T is required or they introduce a target correlation structure. Such a rearrangement of the sample ordering can be achieved via several different techniques published in the literature on LHS (e.g. [16,17]); however, some serious limitations have been found by the authors while using them.A robust technique to impose statistical correlation based on the stochastic method of optimization called simulated annealing has been proposed by Vořechovský and Novák [14]. The imposition of the prescribed correlation matrix on the sampling scheme can be understood as a combinatorial optimization problem: The difference between the prescribed (target) T and the generated (actual) A correlation matrices should be as small as possible. Let us denote the difference matrix (error-matrix) E:(8)E=T-ATo obtain a scalar measure of the error a suitable norm of the matrix E is introduced. Two different norms have been defined in [14], denoted as ρmax and ρrms. These norms have to be minimized from the point of view of the definition of the optimization problem; the objective function is the error norm and the design variables are related to the ordering in the sampling scheme (Fig. 2). Clearly, in real applications the number of all the possible actual correlation matrices A is extremely large: consider all(Nsim!)Nvar-1different mutual orderings of the sampling table. Clearly, we want to find an efficient near-optimal solution. This is achieved by the application of the algorithm briefly described below.In each step of the combinatorial optimization algorithm, mutation is performed by a transition called a swap from the parent configuration to the offspring configuration. A swap (or a trial) is a small change to the arrangement of the sampling table in Fig. 2. It is done by randomly interchanging a pair of values, xijand xik. In other words, one needs to randomly generate i (select the variable), and a pair, j, k, (select the pair of realizations to interchange); see Fig. 2. One swap may or may not lead to a decrease (improvement) in the error norm. Immediately, one configuration between the parent and offspring is selected to survive. The Simulated Annealing algorithm [18] is employed for the selection step. The advantage of this compared to some simple evolution strategies is that there is a nonzero probability of accepting an offspring configuration with a higher error than its parent (hill climbing). The acceptance rule with decaying probability of hill climbing provides a mechanism for accepting increases in a controlled fashion (cooling schedule). It is possible that accepting an increase in the error norm will reveal a new configuration that will avoid a local minimum or at least a bad local minimum in future. Details on the algorithm and also on the implementation can be found in [14].Extensive studies on the performance of the algorithm [19–21] show that it performs considerably better than other widely used algorithms for correlation control, namely both Iman and Conover’s (1982) Cholesky decomposition [16] and Owen’s (1994) Gram-Schmidt orthogonalization [17].When using Monte Carlo-type simulation, the adequacy of a given sample for the purpose of giving acceptable estimates of desired statistical quantities cannot be determined a priori, and thus the ability to extend or refine an experimental design may be important. This can be done very easily in crude Monte Carlo sampling. Very often, though, running each realization (as either a physical or virtual experiment) is very expensive. In conventional Latin Hypercube Sampling, however, it is necessary to specify the number of simulations in advance. If too small a sample set is used (i.e. a set that does not give acceptable statistical results), the analyst normally has to abandon the results and run new analyses with a larger sample set. It is thus desirable to start with a small sample and then extend (or refine) the design if deemed necessary. The extension would permit the use of a larger sample set without the loss of any of the already performed, and possibly quite expensive, calculations (experiments).This problem has been overcome by the method called Hierarchical Latin Hypercube Sampling, which was proposed recently in [22,23]. Note that a similar solution has been published in [24]. The method combines the addition of simulations to the current sample set (hierarchical refinement of sampling probabilities) while maintaining the desired correlation structure by employing an advanced correlation control algorithm [14] for the extended part of the sample. The initial LH-sample can have an arbitrary number of simulations and the added sample must have an even integer times more sampling points than the current sample size (e.g. twice more, see Fig. 4).The subsets sampled by the proposed method can be merged together, exploiting the property of variance reduction, yet retaining the sampling flexibility. The whole procedure of a cascade of sampling runs can be fully automated and the stopping criterion might be e.g. the significance of output statistics, or the desired computational time. The simulation can simply be stopped during run-time depending on the current accuracy of results and the analyst’s budget. In this way, e.g. some crude pilot studies can later be efficiently reused and refined. Refining an existing experiment’s design instead of re-creating a new one offers the advantage that reusing sample points helps to decrease the overall computational cost.Numerical studies have shown that the extended sample has all the properties that the same LH sample would have when simulated in a single LHS run. The advantage in sample size flexibility is obvious.At a higher level of uncertainty modeling, the spatial variability of the mechanical and geometrical properties of a system and intensity of load should be represented by means of random fields. Because of the discrete nature of the finite element formulation, the random field must also be discretized into random variables. This process is commonly known as random field discretization. The computational effort in reliability problems generally increases with the number of random variables. It is therefore desirable to use a small number of random variables to represent a random field. To achieve this goal, the transformation of the original random variables into a set of uncorrelated random variables can be performed through a well-known eigenvalue orthogonalization procedure. This procedure is a discrete version of the well-known Karhunen–Loève expansion of random fields using a set of uncorrelated Gaussian variables and orthogonal functions. It has been demonstrated that a few of these uncorrelated variables with the largest eigenvalues are sufficient for the accurate representation of a random field.Let us consider the fluctuating components of a homogenous random field, which is assumed to model the material property variation around its expected value. Correlation characteristics can be specified in terms of the autocorrelation matrixCxxconstructed by discretization using an autocorrelation function and FEM mesh geometry. An eigenvalue orthogonalization procedure will transform variables into uncorrelated space:(9)CXX=ΦΛΦTThe covariance matrix in the uncorrelated space Y is the diagonal matrixΛ=Cyy. The vector of uncorrelated Gaussian random variables Y can then be simulated in the traditional way (Monte Carlo simulation). The transformation back into correlated space yields the vector X using eigenvectorsΦ:(10)X=ΦYAs shown in [25], the LHS method can be advantageously used for the simulation of uncorrelated Gaussian variables Y. The superiority of this stratified technique also remains here for the accurate representation of the random field, thus leading to a decrease in the number of simulations needed [26].The concept of the Karhunen–Loève expansion of random fields has been extended for both the continuous and discrete representation of cross correlated random fields; see [27]. Translation from Gaussian random fields into other distribution functions can be conveniently achieved by employing Nataf transformation [28].An important task in structural reliability analysis is to determine the significance of random variables. With respect to the small-sample simulation techniques described above the most straightforward and simplest approach uses the non-parametric rank-order statistical correlation between the basic random variables and the structural response variable [29,30]. The sensitivity analysis is obtained as an additional result of LHS, and no additional computational effort is necessary.The relative effect of each basic variable on the structural response can be measured using the partial correlation coefficient between each basic input variable and the response variable. The method is based on the assumption that the random variable which influences the response variable most considerably (either in a positive or negative sense) will have a higher correlation coefficient than the other variables. Because the model for the structural response is generally nonlinear, a non-parametric rank-order correlation is used by means of the Spearman correlation coefficient or Kendall tau. Sensitivity analysis can be depicted using parallel coordinates [31]; a strong positive influence (high correlation coefficient) results in parallel lines between the input variable and the response variable, while a strong negative influence results in a bundle of intersecting lines.In cases when we are constrained by the use of only a small number of simulations (tens, hundreds) it can be difficult to estimate the failure probability. The following approaches are therefore utilized here; they are approximately ordered from elementary (extremely small number of simulations, inaccurate) to more advanced techniques:•Cornell reliability index - calculation of the reliability index from an estimate of the statistical characteristics of the safety margin.The curve fitting approach - based on the selection of the most suitable probability distribution of the safety margin.FORM approximation (Hasofer-Lind index).Importance sampling techniques.Response surface methods.These approaches are not described here as they are well-known in the reliability literature, and also the provision of all details is beyond the aim of this paper. In some cases, these techniques do not always belong to the category of very accurate reliability techniques (especially the first three in the list). However, they represent a feasible alternative in many practical cases.FReET multipurpose probabilistic software for the statistical, sensitivity and reliability analysis of engineering problems (Novák, Vořechovský and Rusina [7,32,33]) is based on the efficient reliability techniques described above. There are three basic parts.The “Random Variables” window (Fig. 5) allows the user-friendly input of basic random variables of the analyzed problem. Uncertainties are modeled as random variables described by their probability density functions (PDF). The user can choose from a set of selected theoretical models such as normal, lognormal, Weibull, rectangular, etc. Random variables can be described in three ways. The first option is to describe them by their statistical characteristics (statistical moments): the mean value, standard deviation (or coefficient of variation), coefficient of skewness and kurtosis excess. Alternatively, they can be set based on their parameters or on a combination of parameters and moments. The number of free parameters is identical in all three modes (moments, parameters or a mixture of both) and it represents the degrees of freedom of the distribution. A special feature is enabled: the user can work with a variable that represents the i-th greatest or smallest variable of n independent and identically distributed (iid) variables selected from the basic (elemental) distribution (order statistics). In this way, e.g. the smallest of the n iid random variables can be selected and the software works with this transformed distribution as if this was on the list of available elemental distributions. This feature is accessible from the “Distribution details” window, Fig. 6, and this window also provides the option of performing basic computations with a single random variable.Another option allowing definition of the distribution of a single random variable is to use raw data. Upon loading an arbitrary list of values, the program either enables the use of a histogram or proposes the best matching available parametric distribution according to the Kolmogorov–Smirnov test.The “Statistical Correlation” window serves for the input of target correlation matrix T, Fig. 7. The user can work at the level of a subset of correlation matrices (each related to a group of random variables) or at the global level (all random variables resulting in a large correlation matrix). The level of correlation during interactive input is highlighted, and the positive definiteness is checked. Note that Simulated Annealing applied for correlation control does not require the positive definiteness as it automatically delivers a sample having the nearest positive semidefinite correlation matrix to the target matrix T.Random input parameters are generated according to their PDF using LHS sampling. Samples are reordered by the Simulated Annealing approach in order to match the required correlation matrix as closely as possible, Fig. 8. Generated realizations of random parameters are used as inputs for the analyzed function (computational model). The solution is performed Nsim times and the results (structural response) are saved. At the end of the whole simulation process the resulting set of structural responses is statistically evaluated. The results are: estimates of the mean value, variance, coefficient of skewness and kurtosis, and the empirical cumulative probability density function estimated by an empirical histogram of structural response. This basic statistical assessment is visualized through the “Histograms” window. It is followed by reliability analysis based on several approximation techniques: (i) the basic estimate of reliability by the Cornell safety index, (ii) the curve fitting approach applied to the computed empirical histogram of response variables and (iii) the simple estimate of probability of failure based on the ratio of failed trials to the total number of simulations; see Fig. 9.Additional information regarding the problem solved is obtained via the sensitivity analysis of each response function based on its rank-order correlation coefficient. Even though this is actually a byproduct of the simulation which does not require any special additional effort, it provides very useful information in many cases. If the correlation coefficient between a certain input variable and output variables is close to zero, we can conclude that the input variable has (in its simulated range) a small or even negligible effect on the output. This can sometimes help to decrease the probabilistic dimension of the problem because such an input can be considered deterministic; see Fig. 10.State-of-the-art probabilistic algorithms are implemented in FReET to compute the probabilistic response and reliability. FReET is a modular computer system for performing probabilistic analysis developed mainly for computationally intensive deterministic modeling and the running of user-defined subroutines. The main features of the software are (version 1.6):The fundamental part of the software is the user-friendly handling of inputs - basic random variables and theirs statistical correlation. The main features are:•A friendly Graphical User Environment (GUE).30 probability distribution functions (PDF), mostly 2-parametric, some 3-parametric, two 4-parametric (Beta PDF and normal PDF with a Weibullian left tail).Unified description of random variables with the optional use of statistical moments or parameters or a combination of moments and parameters.PDF calculator, Fig. 6.Extreme value distributions and order statistics for any available parametric distribution.Statistical correlation (there is also a weighting option).Categories and comparative values for PDFs.Visualization of basic random variables, including statistical correlation in both Cartesian and parallel coordinates, Fig. 10.The user has several options to define the analyzed function. The complexity of the task is decisive for the selection of an appropriate interface. Several efficient and user-friendly options are implemented:•Closed form (direct), using the implemented Equation Editor (simple problems), Fig. 11.Numerical (indirect), using a user-defined DLL function that can be prepared in practically any programming language (C++, Fortran, Delphi, etc.).General interface to third-party software using user-defined *.BAT or *.EXE programs based on input and output text communication files.Multiple response functions assessed in the same simulation run.The assessment of outputs (the results of Monte Carlo-type simulation) consists of:•Histograms of output variables.Sensitivity analyses.Reliability estimates by various simulation and approximation methods.Limit state functions.Parametric studies.Cost/Risk assessment.Both standard and advanced statistical, simulation and reliability techniques are implemented:•Crude Monte Carlo simulation.Latin Hypercube Sampling (3 alternatives).Hierarchical Latin Hypercube Sampling (extension of sample size).First Order Reliability Method (FORM).Curve fitting.Simulated Annealing employed for correlation control over inputs.Bayesian updating.Response surface.Importance sampling around mean values.When assessing the durability of concrete structures it is common to distinguish the initiation period during which the passive, thin layer on the reinforcement’s surface is destroyed (e.g. depassivation due to carbonation or chloride ingress) and the propagation period of the reinforcement corrosion process [34].The description of relevant limit states and associated reliability assessment methodology are dealt with briefly in e.g. [35] for both the initiation as well as the propagation period. Probabilistic modeling of the carbonation process is described in more detail elsewhere [36,37], modeling of chloride ion ingress e.g. in [38,39].Verification of limit states associated with durability may be performed according to one of the safety formats given in fib Model Code 2010 [40]: (i) full probabilistic format; (ii) partial safety factor format; (iii) deemed-to-satisfy approach; (iv) avoidance-of-deterioration approach. Note that approach (i) is the only option which provides quantitative information about the safety level and that there are also other reasons why the dominance of the full probabilistic safety format is evident.It appears that predictive models are needed to estimate how resistance, loads and safety level will change over time. The utilization of such models is highly advantageous when checking a probability limit condition: considering the Ultimate Limit State (ULS) and the Serviceability Limit State (SLS), the general condition for the probability of failure Pfmay be formulated (reformulating Eq. (3)) as:(11)Pf=P(A⩾B)<Pd,where A is the action effect, B is the barrier and Pdis the target probability value. Reliability index β is alternatively utilized instead of the probability of failure in practice (this is well-known and is today prescribed in design codes). Generally, both A and B (and hence Pfor β) are time dependent. The broad utilization of such an approach is still prevented by the insufficient dissemination of information on stochastic analysis, by the lack of experimental evidence and suitable models, and due to the lack of efficient software. Durability and reliability issues should be addressed during the design process and discussed with stakeholders, whose needs create the basis for the application of a performance-based approach [41].In the case of durability, the limit state can also be expressed by means of the service life format as:(12)Pf=P(tS⩽tD)⩽Pd,where tDis the design life; the service life tScan be determined as the sum of two service-life predictors (periods):(13)tS=ti+tp.In (13), tiis the time at which the initiation of reinforcement corrosion takes place and tpis the part of the service life after corrosion initiation – the propagation period. Frequently, the initiation period only serves as the decisive limit state – considered to be alimit for service life, see e.g. the JSCE Guidelines [42]:(14)tS=ti.The direct consequence of passing this limit state is that possible future measures needed to repair the structure become more expensive. The principal factors causing depassivation of reinforcement in concrete are carbonation and chloride ingress. The variables in Eq. (11) for the case of concrete carbonation are denoted as follows: B is concrete cover thickness and A is the depth of carbonation at time tD. In the case of chloride ingress: B is the critical concentration of Cl− which leads to steel depassivation and A is the concentration of Cl− in contact with the reinforcement at time tD.Carbonation affects the pore structure of concrete, thus also changing the chloride profile. Nevertheless, due to the slow progress of carbonation this effect is usually exhibited in the surface layer of concrete only, and in practical life time assessment it may be neglected.Assessing the progress of reinforcement corrosion, i.e. the propagation period tp, the relevant limit states according to Eq. (11) are constructed with consideration given to the critical tensile stress of concrete, the critical crack width, or the limiting reinforcement cross-sectional area (with regard to either the SLS or the ULS) [35].A comment regarding the level of representation in space: several variables applied in the assessment of deteriorating concrete structures show random spatial variability. In contrast, the majority of published analyses deal with 1D representation, which enables the investigation of “points in space” or “hotspots”, during which only the temporal variability is taken into account. In order to rectify this deficiency, numerous proposals for approaches which also facilitate the analysis of the spatial characteristics of deterioration processes have recently appeared, e.g. [43–46]. Frequently, random fields in 2D space are used, often simulated by means of random variables, generated for a chosen mesh in stochastic finite element analysis. The requirement for data concerning the correlation structure in space creates a challenge in practical cases; therefore, monitoring/testing can be employed. A more appropriate and economical definition of tican then be based on the reinforcement depassivation reached in a certain proportion of the structure.Many predictive computational models for the degradation of reinforced concrete structures with different levels of sophistication may be found in the literature. A common feature of all these models is that input data are very uncertain. The authors and their co-workers have developed a software implementation where relatively well-known and simple models are chosen and employed within the framework of a unified software environment – currently encompassing 39 models or their modified versions. This program, FReET-D, features a combination of analytical models and simulation techniques which have been amalgamated to form specialized software for assessing the potential degradation of newly designed as well as existing concrete structures [47,48].The software package FReET-D, version 1.2 (2012), is efficient and user friendly; models for carbonation, chloride ingress, reinforcement corrosion, sulphide, acid and frost attack are provided. FReET-D actually represents a specialized module of FReET software [7], described above. All features implemented in FReET can therefore also be utilized for durability limit states defined by FReET-D. The full probabilistic safety formats are thus employed, serving also for the provision of quantitative safety level information. The uncertainties associated with parameters involved in deterioration processes are modeled by random variables and several simulation techniques may be optionally used. Statistical, sensitivity as well as reliability analysis is provided. The implemented models may serve directly in the durability assessment of concrete structures in forming durability limit states, i.e. the assessment of service life and the level of the relevant reliability measure. Several features are offered, including parametric studies, Bayesian updating and statistical correlation of input variables. Models are implemented as pre-defined dynamic-link library functions selected from the literature. Some of them were originally developed as deterministic models and have been converted into probabilistic form for the purposes of the presented software. Note that the availability of more models seems to be beneficial in some situations, bearing in mind that a more sophisticated computational model requires a larger amount of input data, which are not always available.Some applications are described e.g. in [39,49–51].FReET software is used worldwide for both research and teaching purposes. However, the applications of FReET software as a complex system for the reliability analysis of concrete structures are some of the most successful and interesting. Efficient nonlinear numerical analysis techniques implemented in ATENA software [52,53] and stochastic methods have been combined to offer an advanced tool for the reliability assessment of concrete structures. The combination of all parts (structural analysis, reliability assessment, inverse analysis and degradation modeling) is presented together as SARA software [54–63]. There are, of course, applications in other areas of engineering in which the basic concepts of stochastic simulation as implemented in FReET software can be used, e.g. in geomechanics [64]. But we will provide the list of applications bellow related to FReET and SARA only.The presented approach has been used for the statistical, sensitivity and nonlinear reliability analysis of concrete structures. This was the primary impulse for software development. The finite element model of concrete structure developed in ATENA software is randomized using FReET software, which generally results in a bundle of load–deflection curves. Based on statistical processing of the results, ultimate capacity, sensitivity and reliability can be assessed. The main interest is focused on probabilistic bridge assessments, including degradation and retrofitting modeling. The majority of bridges analyzed by the complex system SARA are located in the Czech Republic, Austria, Italy and Germany. Refs.: [65–73].It is well known that concrete exhibits size effect. Deterministic energetic size effect can be captured by nonlinear fracture mechanics, but there is an additional source of size effect: uncertainties related to the strength of the material used. Such a size effect is a statistical size effect and cannot be captured without the application of statistics and probability. The SARA software system can do that, as has been verified with several size effect experiments. The probabilistic treatment of nonlinear fracture mechanics in the sense of extreme value statistics has been recently applied mainly for crack initiation problems which exhibit Weibull-type statistical size effect. Refs.: [74–81].The recently proposed inverse analysis is based on a coupling of stochastic nonlinear fracture mechanics analysis and an artificial neural network. This inverse analysis utilizes the SARA software package. Small-sample simulation is used efficiently for the virtual simulation of a training set for an artificial neural network. The approach has been used for material parameter identification, damage identification, inverse analysis and model updating. The original procedure has also been applied for inverse reliability analysis. Refs.: [82–93].Complex simulation software can be efficiently used for code verification and calibration purposes. Questions always arise at the academic level concerning the correctness and reliability level of design code formulas vs. newly proposed formulas. No real guarantees and information on safety can be obtained using the partial safety concept as accepted in the present design codes.In the context of transferring to new design codes for the assessment of civil engineering structures, the current topic is designing according to the Eurocodes (EC). The engineering community is currently discussing and organizing lectures focused on this issue. These lectures very often just deal with issues arising due to the differences between current design methodology and designing according to the EC. Refinement of calculations in terms of more accurate reliability estimates (quantified using failure probability) necessitates a truly probabilistic framework – fully probabilistic computation. Refs.: [94–98].A unique application of the first version of FreET software should be mentioned [99]: An extension of the subway system in Prague, Czech Republic, crosses the Vltava River. Acouple of large concrete tunnels, which are curved in plan as well as in elevation, were cast in a dry dock excavated in the bank of the river, Fig. 12[100]. After the first tube was cast, it was launched in a trench excavated in the river bed. Each tunnel was cast in 12m long segments. The total number of 14 segments formed a large tube, 168m long. The outside width of the cross-section was 6.48m, and the height was 6.48m, too. The thickness of the walls and the top and bottom slabs was about 0.7m. During launching, the tube was suspended at one third of its length from its front on a pontoon, and at the back end the tube was supported by hydraulic telescopic sliding shoes. During launching the tube was subjected to two forces -actions acting in opposite directions; (i) that of its own weight and (ii) that of the buoyancy of the water. Both these actions exhibit statistical scatter due to the differences in dimensions and in the density of the concrete. The actual load is the difference between the two actions, and therefore it is extremely sensitive to the scatter of geometrical imperfections. Due to the expected variability, balancing forces (tanks of water) were considered in some range and then applied in reality. There was a danger that such imperfections might result in an out-of-control situation in which the necessary balancing forces would be outside of the feasible range. A statistical feasibility study was therefore needed.The imperfections of the individual segments were simulated as random variables – a total of 211 random variables were used in the study! A simulation of the random process of concrete tube casting (geometry) with the statistical assessment of forces was performed. The updating of weights according to the results of the casting of previous segments was suggested and implemented into the virtual numerical procedure. The segments were continuously updated – the 2nd segment was based on the 1st segment, the 3rd segment was based on the 1st and 2nd segments, etc. An improvement in the forces in each random realization can be expected from this technique – the resultant uplift force will be close to the target one.The good convergence of such a procedure is shown in Fig. 13, using several Monte Carlo-type simulations only. The basic nominal starting thickness of the first segment was 0.7m, the target uplift force being approx. 9kN/m (per meter of tunnel tube). The good convergence of both means and percentiles was observed, which obviously contributes to reliability.The virtual simulation detailed above was purely theoretical, and so the question naturally arises: what will the result be during the real casting of tubes? The suggested method of segment updating was followed in reality as much as possible. Real values for uplift forces based on geometry and density measurements are shown in Fig. 13. The behavior confirms the efficiency of the updating procedure tested virtually before casting.The statistical simulation of this technical process was aimed at the decision-making process before launching and thus at assessing the reliability of the entire project. The successful launching of the subway tubes with segment geometry updating demonstrated very good agreement with the preliminary statistical simulation.

@&#CONCLUSIONS@&#
The paper describes the main software features and stochastic methods implemented in FReET. Efficient techniques of employing stochastic simulation methods were combined in order to offer an advanced tool for the probabilistic assessment of user-defined problems at ultimate capacity and serviceability limit states.The presented software tools may be applied in the advanced design of structures, when making decisions about alternatives, when searching for optimum life-cycle cost solutions, and in cost-effective decision-making processes concerning maintenance inspection and planning. With regard to this, the time aspect emphasizes the urgent need for durability limit state consideration. The software implementation in FReET-D can help users to choose appropriate models and assess the service life issue as applied to concrete structures.Real world engineering structural design, development and assessment is very challenging as it is subjected to a whole host of sources of variation. Probabilistic techniques are therefore used in various engineering fields, offering advantages over the alternative, but more traditional, deterministic methods that might otherwise be employed. Small-sample probabilistic simulation of the Monte Carlo type can address a lot of the shortcomings of classical deterministic approaches and a ready-to-use software program has been developed for the analysis of any user-defined problem. Its wide range of applicability, both practical and theoretical, provides the opportunity for further intensive development of the software tools it offers.