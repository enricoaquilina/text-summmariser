@&#MAIN-TITLE@&#
Improving aspect extraction by augmenting a frequency-based method with web-based similarity measures

@&#HIGHLIGHTS@&#
Augment frequency-based aspect extraction with web-based similarity measure.Improve the efficiency of aspect extraction methods that use PMI-IR measures.Demonstrate the generality of the proposed method across various products.

@&#KEYPHRASES@&#
Aspect extraction,Online reviews,PMI-IR,Frequent aspects,

@&#ABSTRACT@&#
Online review mining has been used to help manufacturers and service providers improve their products and services, and to provide valuable support for consumer decision making. Product aspect extraction is fundamental to online review mining. This research is aimed to improve the performance of aspect extraction from online consumer reviews. To this end, we augment a frequency-based extraction method with PMI-IR, which utilizes web search in measuring the semantic similarity between aspect candidates and target entities. In addition, we extend RCut, an algorithm originally developed for text classification, to learn the threshold for selecting candidate aspects. Experiment results with Chinese online reviews show that our proposed method not only outperforms the state of the art frequency-based method for aspect extraction but also generalizes across different product domains and various data sizes.

@&#INTRODUCTION@&#
Online reviews play an important role in online advertising and marketing (Barton, 2006). Online reviews have been recognized as a type of word-of-mouth that helps managers and manufacturers in brand building, product development, and quality assurance. According to recent social commerce statistics from Kelton Research/Bazaarvoice, 83% of consumers believed that it is important to read user-generated content before making a decision about banking or other financial services. As the volume and velocity of online reviews rapidly increases, it becomes ever more challenging for customers to read through entire reviews. Aggregated ratings alone do not address the above challenge for two main reasons: (1) customers really care about aspects (i.e., parts and attributes) of a product, and (2) customers have their own preferences for aspects of a product (Liu & Seneff, 2009; Snyder & Barzilay, 2007). Therefore, product aspect extraction from the text of online reviews is instrumental for leveraging the online word-of-mouth for individual and business decision making.Extracting product aspects from online consumer reviews remains a difficult task due to some unique characteristics of online reviews such as unstructured text, and the colloquial and casual style of Internet language (Pollach, 2005). Product reviews are commonly written by consumers, not paid professionals. The extant methods for product aspect extraction from online reviews can be classified into four main categories (Liu, 2012, chap. 5): (1) extraction based on frequent nouns and noun phrases (Hu & Liu, 2004a), (2) extraction by exploiting opinion and aspects relations (Qiu, Liu, Bu, & Chen, 2009), (3) extraction by supervised learning (Jin, Ho, & Srihari, 2009), and (4) extraction using topic modeling (Liu, Cao, Lin, Huang, & Zhou, 2007). The first type of method (e.g., Apriori) is focused on those aspects that occur frequently, which depends on pruning methods to improve the relevance of the extraction results. The second type (e.g., double propagation) relies on the dependency relationship to propagate information between aspects and opinions. Although such a method is capable of dealing with infrequent aspects, noise as well as information could be propagated. The third type is most commonly used, which generally outperforms its unsupervised counterpart. Nevertheless, it requires training data, which is an inherent limitation of supervised methods. Annotating online reviews with product aspects is both labor-intensive and time-consuming, particularly in light of the high variety and volume of products and high dimensions of product aspects. The last type of method is aimed at discovering the main themes that pervade a large and otherwise unstructured collection of documents by building topic models (Blei, 2012). The topics discovered by such a method generally contain both aspects and opinions about product entities, so additional work is required to separate the two types of information.In this research, we propose a method for product aspect extraction by augmenting frequency-based extraction with PMI-IR. PMI-IR (Turney, 2001) is used to measure the semantic similarity between aspect candidates and product entities. Compared with previous frequency-based methods, our proposed method has several advantages. First, it leverages a universal search engine rather than a static collection of online reviews to estimate the similarities between candidate aspects and an entity. A universal search engine routinely maintains the coverage and freshness of its content, which enables more updated and complete estimates of similarities with no extra effort. Second, it prunes frequent aspect candidates based on the PMI-IR score between a candidate aspect and the target entity under review instead of between the candidate and multiple discriminator phrases, which improves the efficiency of aspect extraction that uses PMI. Third, it only requires small datasets for threshold learning, which can easily scale up. Our experiment results show that the proposed methods outperform the state-of-the-art frequency-based method.The rest of this paper is organized as follows. In Section 2, we provide background and review related work on aspect extraction. In Section 3, we introduce the experiment design in detail, followed by results and discussion in Section 4. We conclude the paper with Section 5.The process of product review mining mainly consists of the following steps (Popescu et al., 2005): identify product aspects, identify opinions regarding product aspects, determine the polarity of opinions, and rank opinions based on their strength. Thus, product aspect extraction is fundamental to review mining. Before providing a systematic review of aspect extraction methods, we define some key concepts based on the previous literature (Hu & Liu, 2004a, 2004b; Liu, 2012, chap. 5; Liu, Wu, & Yao, 2006).Definition (entity): Entity e is described as a target product, service, topic, issue, person, organization, or event, discussed in an online review. Entity e can be represented as a two-element vector: e=(P, AT), where P denotes a hierarchy of parts which is organized based on semantic relations, and AT denotes a set of attributes of e.Definition (aspect): Aspect a∈ (P ∪ AT) is described as a part or an attribute of entity e.For example, cell phone is the entity that a review targets, and both screen and weight are discussed in the review. Since screen is a part of and weight is an attribute of a cell phone, so both are considered as aspects of cell phone. Aspect has also been referred to as product feature (Hu & Liu, 2004a, 2004b) or opinion target (Jakob & Gurevych, 2010). We chose to use the term aspect to avoid confusion with the term feature used in machine learning literature (Liu, 2012, chap. 5), and to include those aspects that not have associated opinions. Aspect is commonly represented as a noun or noun phrase. There are primarily four types of methods for extracting product aspects from online reviews (Liu, 2012, chap. 5).Hu and Liu (2004a, 2004b) laid the groundwork for applying Apriori algorithm of association rule mining to aspect extraction by treating frequent nouns and noun phrases as aspect candidates. The algorithm achieved a precision of 80% and a recall of 72%. Scaffidi et al. (2007) improved the above method by introducing pruning methods to filter those English words that are unlikely to be aspects, such as those words that are more likely to be a verb than a noun. In addition, they pruned some compound words based on the difference in frequency distribution between online reviews and generic text. These pruning strategies improved the precision of aspect extraction to the range of 85–90%. Nevertheless, they show two major limitations: (1) the pruning strategies were developed based on the morphological forms of English words, which are difficult to be extended to other languages like Chinese; and (2) they chose corpora of spoken and written conversations as generic text, and difference in conversational structure would have impact on the performance of text classification (Tavafi, Mehdad, Joty, Carenini, & Ng, 2013). Li, Ye, Li, and Law (2009) extended the method of Hu and Liu (2004a) to Chinese by introducing additional pruning steps aspect extraction from Chinese reviews. Xu, Huang, and Wang (2013) treated frequent sets of skip-bigrams, which are word pairs that allow skips between words, as candidate aspects. They used skip-bigrams to mitigate the negative impact of errors in Chinese part-of-speech tagging, which led to increased recall but reduced precision (below 0.5). Ferreira et al. (2008) found that Hu and Liu’s approach is effective for text that mainly consists of on-topic content. Following Hu and Liu (2004a), Blair-Goldensohn et al. (2008) extracted aspects based on syntactic patterns, relative word frequency, and a sentiment lexicon.Popescu, Nguyen, and Etzioni (2005) employed PMI-IR (pointwise mutual information–information retrieval) (Turney, 2001) to estimate the semantic associations between aspect candidates and discriminator phrases (i.e., part-of and is-a relations). Their method is based on the KnowItAll, a web-based, domain-independent information extraction system (Etzioni, Cafarella, Downey, et al., 2005). The PMI scores of each noun or noun phrase were then converted into a binary value using a Naive Bayes classifier. Compared with Hu and Liu’s method (2004a), the PMI-IR method was empirically found to improve the precision of aspect extraction by an average of 22% at a small cost of recall for about 3%. Despite the promising results, this approach exposes several limitations. First, the identification of part-of and is-a relations relies on English lexical resources such as WordNet, which may not be readily available in other languages. The performance for automatic extraction of the above two types of patterns remains low for languages such as Chinese (Cao, Cao, & Wu, 2013). Second, the coverage of part-of and is-a relations in established lexical resources may not keep up with the high variety and dynamics of content in online reviews. Third, the Naïve Bayes classifier is a supervised method, which by nature requires a set of positive and negative reviews for each type of products as seeds in training models separately. Moreover, their selection of seed reviews, particularly negative seeds, was not random, in order to prevent the PMI scores to become zero. Further, the performance of the method largely depends on the aspects that have been selected into seeds. Additionally, choosing aspects that are semantically similar brings up the question of how to calibrate the semantic similarity. Fourth, for each candidate aspect, the approach queries a search engine in relation to each discriminator phrase separately, which is inefficient. Our proposed method has close resemblance to that of Popescu et al. (2005), but addresses the above limitations.This category of methods leverages the relationship between aspects and opinions, because opinions are generally expressed on some aspects in online reviews (Liu, 2012, chap. 5). Such dependency relationship was first used to find the nearest nouns and noun phrases of a set of known opinion words. As a result, this method is capable to identify infrequent aspects missed by the Apriori algorithm. Shi and Chang (2006) developed an “opinion first and aspect second” approach for Chinese reviews, which extracted implicit aspects using an opinion lexicon pertaining to a product genre that was compiled manually.Qiu et al. (2009), Qiu, Liu, Bu, and Chen (2011) proposed double propagation that simultaneously extracts aspects and expands opinion lexicons by moving forward and backward between opinion mining and aspect extraction. The method also incorporates pruning to improve its precision. The experiment results (Qiu, Liu, Bu, & Chen, 2011) showed that double propagation improved the F-score of aspect extraction by 10% over that of Hu and Liu (2004a). Zhang, Liu, Lim, and O’Brien-Strain (2010) employed the part-whole pattern and no pattern to increase the recall and ranked candidate aspects by importance to increase the precision of double propagation. However, the approach was not found to scale well (Zhang, Liu, Lim, & O’Brien-Strain, 2010) as the precision dropped as the size of the corpora was increased.This is by far the most dominant method for aspect extraction. Most of the supervised methods for aspect extraction utilized machine learning techniques such as Hidden Markov Model, Support Vector Machines, and Conditional Random Fields (Jakob & Gurevych, 2010; Jin et al., 2009; Yu, 2009; Yu, Zha, Wang, Wang, & Chua, 2011). These techniques build models of aspects from labeled training data. Some of them also require lexicons, domain and/or linguistic knowledge base (Kovelamudi, Ramalingam, Sood, & Varma, 2011; Liu et al., 2006), or ontology support (Hobbs & Riloff, 2010). Supervised methods have distinct advantages and disadvantages. On one hand, these methods generally perform well. On the other hand, they require review data with proper annotation to build classification models.Topic modeling (Blei, 2012) has been extended to aspect extraction under the assumption that: (1) reviews have latent structure of topics; (2) topics can be inferred from word-review co-occurrences; and (3) words are related to topics and so are topics to reviews. Based on the mathematical framework, there are two main categories of statistical topic models: pLSA (probabilistic Latent Semantic Analysis) (Hofmann, 1999) and LDA (Latent Dirichlet Allocation) (Blei, Ng, & Jordan, 2003). Compared with pLSA, LDA does not require training data to estimate model parameters and thus is more scalable. Titov and McDonald (2008a, 2008b) proposed two static methods for labeling sentences with topics or aspects, one is based on LDA and the other on multi-grain topic models (MaxEnt-LDA Hybrid model), which could achieve accuracy comparable to that of a standard supervised model. Jo and Oh (2011) proposed Sentence-LDA (SLDA) and extended SLDA to build aspect and sentiment unification model. Like other probabilistic generative models such as pLSA and LDA, SLDA mines both aspects and sentiments simultaneously. Unlike other models, however, SLDA assumes that all words in a single sentence are generated from the same aspect, and the extended SLDA has relaxed this assumption. Si et al. (2013) applied a continuous Dirichlet Process Mixture model to learn a set of daily topics from Twitter for stock market prediction, where the topic extraction has resemblance to aspect extraction. Although these topic modeling methods can take advantage of the big data of online reviews for parameter estimation, they are unable to distinguish between aspects and opinions but treat both of them as topics (Liu, 2012, chap. 5).In this research, we propose a method that extracts aspects based on frequent nouns and noun phrases. It adapts PMI-IR to the problem of aspects extraction and addresses the limitations of previous methods to improve the performance and generality of aspect extraction.In this section, we propose a method for product aspect extraction from online reviews. The method consists of three components: frequency based mining and pruning, order-based filtering, similarity-based filtering, as shown in Fig. 1. The third component proceeds in two sub-steps. In the first step, we extended PMI-IR to measure the semantic association between frequent aspects and the corresponding product entity. In the second step, we developed a threshold learning method for selecting the final set of aspects.Following Hu and Liu (2004a), we performed frequent aspect mining using the Apriori algorithm to generate candidates of product aspects. One major step in Apriori algorithm (Agrawal & Srikant, 1994) is to identify frequent items from a collection of data. Another critical step is to prune candidate aspects. To this end, we applied compactness rules and redundancy rules (Hu & Liu, 2004a):•Compactness rules: we designed these rules to filter some higher-order candidate aspects that do not make sense. For example, both game and CPU are candidate aspects of mobile phone, and they often co-occur in a sentence. Thus, game CPU is a higher-order candidate aspect, but it is not an actual aspect of the mobile phone. This problem is attributed to the Apriori algorithm’s ignorance of the relative positions of its component nouns in generating higher-order frequent aspects. By applying the compactness rules, those candidate aspects in which lower-order nouns or noun phrases are located distant from each other (e.g., game CPU) would be eliminated from the consideration of actual aspects.Redundancy rules: we used these rules to prune those single-word candidate aspects that seldom appear alone, but often along with other candidate aspects. For example, time and standby time are both candidate aspects of a mobile phone. Based on the occurrence frequency of time (i.e., the single-word candidate aspect alone) and its co-occurrence frequency with standby (e.g., another candidate aspect), time was considered as redundant and pruneda accordingly.In addition, we extended general concepts and domain-specific common concepts strategies (Li et al., 2009) to filter the following types of candidate aspects:•General concepts: The intuition behind this strategy was that an aspect should be expressed in specific rather than general terms. There are some frequent nouns or noun phrases that are generic enough to be used to reference any type of entities such as people, place, thing, and event.Domain-specific common concepts: Some nouns or noun phrases are unlikely to express aspects such as my opinion, expectation, uses, and aspects despite of their high occurrence frequencies in online reviews. This type of nouns or noun phrases was treated as domain-specific common words or stop words in this research.An aspect, particularly those related to entity parts and part attributes, are generally expressed words in certain order. However, the basic Apriori algorithm also ignores the word order in combining more than one candidate aspect. For instance, battery life and screen size describe aspects of camera, but their reverse orders do not (without changing the syntactic structure). Thus, we incorporated word order as an additional clue to prune aspect candidates by following (Li & Luo S., 2010). The method for word order acquisition is briefly described as the following.Assume that f={t1, t2,…,tn} is a frequent aspect consisting of n words; sentence s contains f, where ti[i=1,…,n] appears at position Psiin s. The position of tiin f, Loc(ti), is derived as the sum of Psiover all the frequent aspects that contain f (see Eq. (1)):(1)Loc(ti)=∑s⊇f=(t1…tn)PsiThe word order in f will be determined based on their corresponding Loc(ti) scores sorted in a descending order. For example, f={speed, options}, where t1 denotes speed and t2 option. The aspect candidate f appears in sentence s “I played with the high speed filming a bit more, and there are four options, ranging from 0.5x–8x.” In this case, ps1=6, ps2=15. Assuming that f also appears in three other sentences, among which two have t1 (speed) appearing before t2 (options), and one in the opposite order, then Loc(t1)<Loc(t2), and accordingly the aspect would be extracted as speed options.We extended PMI-IR to estimate the semantic similarity between a frequent aspect and a product entity, using Eq. (2):(2)PMI-IR(entity,aspect)=log2hits(entity∧aspect)hits(entity)hits(aspect)Here, hits (·) denotes the number of pages returned from a search engine for a query consisting of the phrase (·). The higher the PMI-IR score, the stronger the semantic association between the frequent aspect and the entity, and accordingly the higher probability of the aspect candidate being an actual aspect. After sorting all the candidate aspects in the descending order of their PMI-IR scores, we selected those aspects whose scores are above certain threshold.To determine the threshold, we adopted RCut (Yang, 2001). The RCut strategy was originally applied in text classification to sort texts and classes based on their similarity by assigning a text to the top t categories. When t is set to 1, RCut assigns the text to a single category (Joachims, 1998). Since there are only two target categories in aspect extraction (i.e., aspect or not), we set t to 1 in the current study.Among the various criteria for selecting a thresholding method, we focused on its generality. More strictly, generality implies that certain threshold values can be extended to different types of products. This is important for aspect extraction, particularly in view of the potentially large number of product types. To address this issue, we aimed to determine a single threshold for a collection of different types of products. Moreover, we validated the generality of RCut based on whether its effectiveness is indifferent to the types of products under review. Furthermore, a classification method belongs to the paradigm of supervised machine learning, which is subject to the availability of training data. Methods that require smaller training data sets are considered more robust. To this end, we tested the sensitivity of threshold values to the size of training data by experiment.Taking a set of frequent aspects extracted by the previous components as the input, the threshold learning and testing goes through the following steps:Step 1: Randomly split reviews into training data and testing data.Step 2: Compute the PMI-IR value for each frequent aspect extracted from the training data.Step 3: Select the PMI-IR value that optimizes the performance of aspect extraction (i.e., F-score).Step 4: Repeat the above steps for each of the products in the training data.Step 5: Determine the threshold α as the average PMI-IR value of all of the selected products.Step 6: Evaluate the performance of α on the test data by choosing frequent aspects with PMI-IR scores greater than α.We evaluated the proposed method for aspect extraction by conducting experiments.Previous research on aspect extraction has predominantly studied English reviews, partly because there is a convenient sample of annotated dataset of online reviews in English (e.g., Hu & Liu, 2004a). We chose to conduct the experiment with Chinese reviews.Given the lack of any publicized dataset in Chinese, we built a dataset from scratch. Following the composition of the English dataset (Hu & Liu, 2004a), the review data was collected from the domain of electronic products. The dataset covers four types of electronic products, including mobile phone, digital camera, MP3 player, and LED monitor, and eight specific products. These reviews were downloaded from IT168 (http://www.it168.com), one of the most reputable online communities of digital products in China. To test the generality of the threshold determination method, a fifth distinct product type, namely book, was also included. Specifically, the reviews of one of the most popular titles of the year were downloaded from Amazon.cn (http://www.amanzon.cn), one of the largest online bookstores in China. The PMI-IR was computed using Baidu (http://www.baidu.com), which had the largest share in the Chinese search engine market.We randomly selected 120 reviews for each of the above nine products. These reviews went through a cleansing process, which involves removing reviews that contain less than three words or more punctuation marks than words, and removing punctuation marks and hyperlinks. We then proceeded with manual annotation of the remaining reviews, accounting for anywhere between 60 and 100 for the different products (see Table 1).Four human coders were recruited to extract product aspects from the cleansed online reviews independently. The coders were first given detailed instructions and opportunities to clarify any questions they may have about the task; and then they were asked to identify all aspects of the product mentioned in each review. A comparison between four coding results showed that the average percentage of pairwise agreement across all the products was 92.5%. The average pairwise agreements range between 88.9% and 93.6% across product types. Only those features that were unanimously identified by all the coders were selected as the ground truth for testing aspect extraction methods. In other words, any inconsistent aspects between any pairs of the coding results were dropped.The descriptive statistics of the finalized dataset is reported in Table 1, which describes the number of reviews, the number of aspects per review, the length of reviews, and the number of reviews per aspect.Precision and Recall were selected as the evaluation metrics, as defined in Eqs. (3) and (4). In addition, F-score was also included to address a possible tradeoff between the first two metrics, as defined in Eq. (5).(3)precision=|extracted_aspects∩actual_aspects||extracted_aspects|(4)recall=|extracted_aspects∩actual_aspects||actual_aspects|(5)F-score=2*precision*recallprecision+recallLi et al. (2009) was selected as the baseline method for comparison for two reasons: (1) the method is based on frequent nouns and noun phrases, which falls into the same category as our proposed method; and (2) it represents the state-of-the-art research in Chinese review mining, which has demonstrated superior performance to that of Hu and Liu (2004a).To determine the threshold for PMI-IR scores and to test the performance of the proposed aspect extraction method, a subset of four products, including one from each type of the electronic products) was randomly selected as the training, and the remaining was held out as the test data. To test the generality of the method, we also conducted additional experiments that included book reviews into the training and the test data separately.

@&#CONCLUSIONS@&#
