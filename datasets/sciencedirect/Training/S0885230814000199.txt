@&#MAIN-TITLE@&#
Interpretable parametric voice conversion functions based on Gaussian mixture models and constrained transformations

@&#HIGHLIGHTS@&#
New voice conversion functions based on bilinear frequency warping and constrained amplitude scaling.Good overall conversion performance using more intuitive and informative parameters.Useful as an analysis tool to visualize spectral differences between different voices or styles.

@&#KEYPHRASES@&#
Voice conversion,Gaussian mixture models,Frequency warping,Amplitude scaling,Spectral tilt,

@&#ABSTRACT@&#
Voice conversion functions based on Gaussian mixture models and parametric speech signal representations are opaque in the sense that it is not straightforward to interpret the physical meaning of the conversion parameters. Following the line of recent works based on the frequency warping plus amplitude scaling paradigm, in this article we show that voice conversion functions can be designed according to physically meaningful constraints in such manner that they become highly informative. The resulting voice conversion method can be used to visualize the differences between source and target voices or styles in terms of formant location in frequency, spectral tilt and amplitude in a number of spectral bands.

@&#INTRODUCTION@&#
Voice conversion (VC) (Moulines and Sagisaka, 1995) is the technology that allows transforming the voice characteristics of a speaker (the source speaker) into those of another speaker (the target speaker) without altering the linguistic message. The applications of VC include the personalization of artificial speaking devices, the transformation of voices in the movie, music and computer game industries, and the real-time repair of pathological voices.Among all possible voice characteristics, the timbre, which is closely related to the short-time spectral envelope, has attracted most of the attention of researchers. During the training phase, given a number of speech recordings from the two involved speakers, VC systems extract their corresponding acoustic information and then learn a mapping function to transform the source speaker's acoustic space into that of the target speaker. During the conversion phase, this function is applied to transform new input utterances from the source speaker. Various types of VC techniques have been studied in the literature: vector quantization and mapping codebooks (Abe et al., 1988), more sophisticated solutions based on fuzzy vector quantization (Arslan, 1999), frequency warping transformations (Rentzos et al., 2004; Shuang et al., 2006; Suendermann and Ney, 2003; Valbret et al., 1992), artificial neural networks (Desai et al., 2010; Narendranath et al., 1995), hidden Markov models (Duxans et al., 2004; Lee et al., 2010; Zen et al., 2011), and Gaussian mixture model (GMM) based VC (Benisty and Malah, 2011; Helander et al., 2010; Kain, 2001; Stylianou et al., 1998; Toda et al., 2007), which currently is the dominant technique.Recently, the set of linear transforms characterizing the traditional GMM-based VC systems were replaced by a set of frequency warping (FW) plus amplitude scaling (AS) transforms (Erro et al., 2010; Godoy et al., 2012; Tamura et al., 2011; Toda et al., 2001) to improve the quality and naturalness of the converted speech. Unlike the former, FW+AS transformations have a clear physical interpretation (see Fig. 1). FW is a nonlinear operation that maps the frequency axis of the source speaker's spectrum into that of the target speaker. Since it does not remove any detail of the source spectrum but just moves it to a different location in frequency, FW preserves the quality of the converted speech well. However, the conversion accuracy achieved via FW is moderate because it does not modify the relative amplitude of meaningful parts of the spectrum. For this reason, FW is complemented with AS to compensate for the differences in the amplitude axis, typically by means of smooth corrective filters.In the works referenced above, particular signal representations were required for the specific FW+AS methods to be applicable, whereas current trends in speech synthesis technologies are pushing research toward methods that can be applied to well known parametric representations of speech. That is why it was shown in Zorila et al. (2012) that GMM-based FW+AS methods can be applied to a simple cepstral representation of speech, overcoming the need of specifically designed vocoders. In Erro et al. (2012), the FW functions were constrained to be bilinear (BLFW), which led to a more elegant formulation of FW+AS in the cepstral domain with very few conversion parameters. The performance of BLFW+AS was found to be as good as that of the best existing GMM-based parametric VC methods (Erro et al., 2013b).Following the line of BLFW+AS and in continuation of our preliminary work (Erro et al., 2013a), this paper goes one step beyond in making VC functions more understandable and controllable by users while reducing even more the number of involved conversion parameters. We suggest imposing constraints to the AS part of the VC function as it was done previously with the FW part. More specifically, we propose a new way of expressing the AS function as a combination of a spectral tilt related term and a set of smooth bandpass filters. We will show that the resulting VC functions are very informative in the sense that all their parameters can be interpreted from a physical point of view. Therefore, the method can be applied not only to synthesize high-quality converted voices but also to analyze the differences between the involved voices.The remainder of the paper is structured as follows. Section 2 contains a brief description of the BLFW+AS VC method. In Section 3 we present the modified method and describe the corresponding automatic training procedures. The performance of this method is experimentally evaluated and discussed in Section 4. Section 5 shows how the proposed method can be used as an analysis and visualization tool. Finally, the conclusions of this work are summarized in Section 6.In the cepstral domain, FW transformations are equivalent to multiplicative matrices (Pitz and Ney, 2005) and AS can be implemented by means of additive cepstral terms (Godoy et al., 2012). Given an input p-dimensional cepstral vector x and a GMM θ, the BLFW+AS operation proposed by Erro et al. (2013b) can be formulated mathematically as follows:(1)F(x)=Wα(x,θ)x+s(x,θ)where Wαis the matrix that implements the BLFW transform, which was proposed by McDonough and Byrne (1999). Wαcan be expressed in terms of a single parameter α which will be referred to as the warping factor:(2)Wα=1−α22α−2α3⋯−α+α31−4α2+3α4⋯⋮⋮⋱This parameter controls the shape of the desired FW transformation according to the following curve (see Fig. 2):(3)ωα=tan−1(1−α2)sinω(1+α2)cosω−2αIn the original BLFW+AS implementation, both the current warping factor α(x, θ) and the AS vector s(x, θ) are obtained by means of a weighted combination of the individual contributions of each Gaussian acoustic class:(4)α(x,θ)=∑k=1mpk(θ)(x)⋅αk(5)s(x,θ)=∑k=1mpk(θ)(x)⋅skwhere pk(θ)(x) denotes the probability that x belongs to the kth Gaussian mixture of θ and m is the total number of mixtures. The elementary factors and vectors of the transformation, {αk} and {sk}, result from a data-driven training procedure based on error minimization. Given a training set of N source-target vector pairs, {xn} and {yn}, the training process is carried out sequentially in two steps. In the first step, the warping factors {αk} are calculated by minimizing the error between the warped source vectors and the target vectors. An iterative algorithm was proposed by Erro et al. (2013b, 2012) to train all the warping factors {αk} simultaneously while dealing with the strongly nonlinear relationship between α and Wα. Omitting the theoretical fundamentals – interested readers should refer to Erro et al. (2013b) –, the algorithm can be summarized as follows:Step 1Null initialization: αk=0 for k=1, 2, …, mCreate partially warped vectors {zn} using the warping matrix that results from expressions (2) and (4) for the current warping factors:(6)zn=Wα(xn,θ)xnCalculate corrective terms {Δαk} for the warping factors {αk} by solving a least squares problem:(7)αm×1=[Δα1…Δαm]T=(DTD)−1DTewhere(8)DNp×m=p1(θ)(x1)d(z1)⋯pm(θ)(x1)d(z1)⋮⋱⋮p1(θ)(xN)d(zN)⋯pm(θ)(xN)d(zN),eNp×1=y1−z1⋮yN−zNand d(·) is an operator that returns a vector whose ith element is(9)d(z)[i]=(i+1)⋅z[i+1]−(i−1)⋅z[i−1],i=1...pUpdate the warping factors {αk} by means of the corrective terms {Δαk}:(10)αk(updated)=αk+Δαk1+αk⋅ΔαkIf |Δαk|<0.001 for all k, exit. Otherwise, go back to step 2.Once the warping factors have been determined, residual vectors containing the differences between warped and target vectors are calculated for the N training pairs:(11)rn=yn−Wα(xn,θ)xn,n=1…NThen, class-specific scaling vectors {sk} are trained in such manner that these differences are maximally compensated. The least squares training algorithm proposed by Erro et al. (2013b, 2012) is the following:(12)Sm×p=[s1s2…sm]T=argminSˆ||R−PSˆ||2=(PTP)−1PTRwhere p is the vector dimension (equal to the cepstral order) and P and R are given by(13)PN×m=p1(θ)(x1)p2(θ)(x1)⋯pm(θ)(x1)p1(θ)(x2)p2(θ)(x2)⋯pm(θ)(x2)⋮⋮⋱⋮p1(θ)(xN)p2(θ)(xN)⋯pm(θ)(xN),RN×p=r1Tr2T⋮rNTAlthough the resulting vectors {sk} are optimal from a mathematical point of view, they are not informative in the sense that it is not straightforward to understand the information they convey. On the contrary, the meaning of the warping factors {αk} is clear: positive values of α mean higher formant frequencies (α≈0.1 for male to female conversion) and negative values mean lower formant frequencies. This informative transparency is partially due to the BL constraints imposed to the FW operation, which simplifies the warping curves and makes them dependent on a single meaningful parameter. At the same time, this helps increasing the robustness of the system. Inspired by this idea, in this paper we propose to constrain the AS vectors as well in order to get a new type of transformation which can be understood, intuitively manipulated and even used to analyze the differences between the source voice and the target voice.Assuming that BLFW is sufficiently accurate, the aim of AS is not making new formants appear in the converted spectrum, but just correcting the relative intensity of the already existing ones after FW. Consequently, the spectral response of the AS filter represented by the elementary vectors {sk} should be smooth by definition. In the original BLFW+AS system (Erro et al., 2013b), this aspect was not taken into account explicitly. Smoothness was guaranteed simply by using a GMM with few acoustic classes, because intra-class averaging prevented sharp peaks in {rn} from being transferred to {sk}. Additionally, BLFW+AS did not take into account that a significant portion of the additive terms {sk} might be directly related to the spectral tilt differences between the two involved speakers. In our new proposal we force the AS frequency response to be formed by a weighted combination of a spectral tilt related term with 1dB/decade slope and B smooth Hanning-like bands equally spaced in the Mel frequency scale. The AS elementary vectors {sk} are now forced to be the result of the following combination:(14)sk=τkt+∑j=1Bβk,jbj,k=1…mwhere t and {bj} are p-dimensional column vectors containing the cepstral representations11The design of the method is strongly linked to the properties of cepstral parameterization. Other speech representations such as line spectral frequencies are not suitable because they cannot be combined as in expression (14) and they require a more careful definition of the error to be minimized during training.of the tilt-related term and the B bands, respectively. These representations are constant and can be calculated numerically. This means that the shape of skdepends exclusively on τkand {βk,j}. Fig. 3shows the spectral shapes that correspond to t and {bj}, all reconstructed from their 24th-order Mel-cepstral parameterization (p=24) for fs=16kHz and B=9.Thus, the value of τkcoincides with the slope (dB/decade) by which the two involved voices differ at the kth acoustic class of model θ, whereas {βk,j} represent the exact amplitude (dB) of the complementary smooth amplitude correction envelope at equally spaced points in the Mel-frequency scale. Remarkably, the dimensionality of the resulting voice conversion function is reduced significantly because each amplitude scaling vector skis now given by 1+B weights. To prevent the tilt-related term from being diluted within the corrective bands, we propose the following two-step weight optimization during training. First we simultaneously optimize the weights of all the tilt-related terms by solving the corresponding least squares system:(15)τm×1=[τ1τ2…τm]T=argminτˆ||Γ−Qτˆ||2=(QTQ)−1QTΓwhere(16)QNp×m=P⊗t,ΓNp×1=[r1Tr2T…rNT]TP and {rn} are given by expressions (13) and (11) respectively, and ⊗ denotes the Kronecker product. Next, we update the residuals by subtracting the tilt-related terms and we determine the weights of the corrective bands for all the classes by solving another least squares system. This can be expressed mathematically as follows:(17)βmB×1=[β1,1…β1,Bβ2,1…β2,B…βm,1…βm,B]T=argminβˆ||Γ˜−Q˜βˆ||2=(Q˜TQ˜)−1Q˜TΓ˜where(18)Q˜Np×mB=P⊗[b1b2…bB],Γ˜Np×1=Γ−QτΓ, Q and τ are given by expressions (15)–(16). Although these operations involve the inversion of large matrices, the efficiency of the computation can be increased by exploiting the properties of the Kronecker product.The speech data used in the evaluation experiments were taken from the CMU ARCTIC database (Kominek and Black, 2003). Four US English speakers with the same dialectal characteristics were selected from this database: two female speakers, slt and clb, and two male speakers, bdl and rms. 50 parallel training sentences per speaker were randomly selected for training and a different set of 50 sentences was separated for testing purposes. The remaining sentences of the database were simply discarded. The sampling frequency was 16kHz. We used the vocoder described in Erro et al. (2011) to parameterize the speech signals into Mel-cepstral coefficients and to reconstruct the waveforms from the converted vectors. The order of the Mel-cepstral analysis was 24 (plus the 0th coefficient containing the energy, which does not take part in the conversion). The frame shift was set to 8ms. During conversion, the mean and variance of the source speaker's logf0 distribution were replaced by those of the target speaker by means of a linear transformation. In order to find the correspondence between the source and target cepstral vectors extracted from the parallel training utterances, we calculated a piecewise linear time warping function from the phoneme boundaries given by the available segmentation. Similarly as in Erro et al. (2013b, 2012), the GMMs used in our experiments had 32 mixtures with full-covariance matrices.The first aspect to be evaluated is how much the conversion performance of the VC system is degraded with respect to the baseline case – BLFW+AS – when the aforementioned simplifications are made. Although objective measures such as Mel-cepstral distortion (MCD) between converted and target vectors are not always correlated with subjective measures – for instance, it is known that alleviating the oversmoothing effect of standard GMM-based VC results into worse MCD scores and better subjective scores at the same time (Benisty and Malah, 2011; Godoy et al., 2012) – they can still help to determine the best configuration or dimensions of a given method. In our case, given the similar nature of BLFW+AS and its AS-constrained version (from now on, BLFW+CAS), MCD is a reliable measure of how accurate the conversion will be for different number of AS bands, B. In this experiment, VC functions were trained for all voice pairs using the training dataset; then, MCD scores were calculated using the test material. For a test dataset containing N paired vectors {xn} and {yn}, the MCD score achieved by a given VC function F(·) is computed as22An additional multiplicative factor 2 may be needed in expression (19) depending on how cepstral coefficients are defined.:(19)MCD{F(·)}=10log10⋅1N∑n=1N||yn−F(xn)||Fig. 4shows the average MCD scores achieved by BLFW+CAS for B={9, 14, 19}. Two more methods are included for comparison: BLFW+AS and the standard GMM-based method based on joint statistical modeling of source-target acoustic pairs (Kain, 2001) (from now on, JGMM). Given the inherent particularities of BLFW-based methods, we compute average scores not only for all possible combinations of voices but also separately for intra-gender and cross-gender combinations. JGMM performs better than the other methods in terms of MCD, which is consistent with the findings of previous works (Erro et al., 2013b). Regarding BLFW+CAS, we can observe that imposing constraints to the AS terms means reducing the accuracy of the method. Specifically, the use of 9, 14 and 19 AS bands produces an average MCD increment of 4.9%, 2.2% and 0.6% respectively. The MCD scores achieved by BLFW+CAS get closer to those of BLFW+AS as the number of unknowns of their respective VC functions becomes similar. Cross-gender VC is more sensitive than intra-gender VC to the reduction of B. The main reason for this is the fact that CAS is less capable than AS of absorbing the inaccuracies of BLFW, which are logically larger when the involved voices exhibit very different vocal tract lengths.For a more precise objective characterization of the methods, we also computed the quotient between the variance of the converted parameters and that of the natural target parameters. Defining vari{·} as the variance of the ith element of its input vector set, the variance quotient (VarQ) measure we have used can be expressed as:(20)VarQ{F(·)}=1p∑i=1pvari{F(xn)}n=1…Nvari{yn}n=1…NVariance measures have been shown to be good indicators of the degree of oversmoothing (Benisty and Malah, 2011; Godoy et al., 2012; Toda et al., 2007) and BLFW+AS method was already shown to produce good variance-related scores even without an explicit modeling of the variance of the converted parameters (Erro et al., 2013b). In this experiment, VarQ scores were calculated for all voice pairs and a single average score was calculated for each VC method and configuration. As shown in Table 1, the variance of the converted parameters is closer to natural for low values of B. Indeed, low B means smoother AS terms which preserve better the variance of the natural source parameters. As reported in previous works (Erro et al., 2013b; Godoy et al., 2012; Toda et al., 2007), JGMM achieves quite poor scores for this specific type of measure due to the oversmoothing effect.One of the main advantages of reducing the dimension of the VC function is that it can be estimated more reliably from a given amount of training data. BLFW+AS was already shown to be more robust than other GMM-based methods (Erro et al., 2013b). The next experiment aims at confirming that BLFW+CAS (B=9) is even more robust than BLFW+AS when the amount of training data is progressively reduced. Assuming a fixed GMM learned from the whole training dataset of the source speaker (50 utterances, as specified in Section 4.1) – this is a reasonable assumption as discussed in Erro et al. (2013b) – we trained VC functions after reducing the available amount of parallel training data (initially 50 parallel utterances) by factors M={2, 4, 8, 16}. For each value of M, the parallel training dataset was split into M subsets, each one yielding a different VC function; then, their M corresponding MCD scores were averaged to get a single score per each data reduction factor. Finally, the factor-dependent scores were also averaged over all possible voice pairs. Remarkably, for M=8 and M=16 some of these partial MCD scores had to be discarded because the VC had been badly estimated due to insufficient data. Both BLFW+AS and BLFW+CAS were equally prone to this phenomenon.The results shown in Fig. 5reveal that the MCD scores produced by BLFW+CAS grow less rapidly than those produced by BLFW+AS when the amount of parallel training data is reduced. This means that in principle BLFW+CAS is more robust against parallel training data scarcity. Unfortunately, for the amounts of data that allow a correct (numerical error free) training of the VC function, the MCD scores achieved by BLFW+CAS remain always higher (worse) than the baseline scores. Taking into account that numerical errors start to appear at the same training conditions for both methods, we can conclude that robustness is not a practical advantage of the constrained one unless the observed MCD gap is meaningless from a perceptual point of view.The next logical step is to quantify the loss of conversion accuracy of BLFW+CAS by means of a subjective test. To do this we conducted a perceptual test in which 20 volunteer listeners with good English speaking skills (including 6 speech processing experts) were asked to listen to several converted-target pairs and rate two aspects: (i) the similarity between voices and (ii) the quality of the converted ones, both in a 1-to-5 scale (1=“very bad”, …, 5=“excellent”). The test was carried out via a web interface and listeners were asked to use headphones. The natural signals used as reference were analyzed and reconstructed by means of the same vocoder we used for VC in order to focus the attention of the evaluators on the methods themselves. Given the subtle differences between BLFW+CAS configurations and the annoyance of listeners when asked to rate very similar things, we evaluated only the most basic configuration of BLFW+CAS, namely that with B=9 (although fewer bands can be used, this is a representative operation point with a good trade-off between dimension, objective accuracy and informative transparency). Again, we compared it with BLFW+AS and JGMM. According to Fig. 4, it is reasonable to assume that BLFW+CAS is equivalent to BLFW+AS when B is high enough.The resulting mean opinion scores (MOSs) are shown in Fig. 6. Once again, since the relative scores of JGMM and BLFW+AS are consistent with those reported previously (Erro et al., 2013b, 2012), we focus our attention on BLFW+CAS. For this configuration, subjective scores confirm that the converted voices yielded by BLFW+CAS are not as similar to the target as those converted by BLFW+AS or JGMM. This means that the effect of reducing B is not negligible from a perceptual point of view. The quality scores, however, are similar for both BLFW-based methods, far beyond the performance of JGMM. In short, we can affirm that the AS constraints have no negative impact on the subjective quality measures, and the similarity between converted and target voices can be effectively controlled through the number of AS bands, B, which determines also the number of parameters of the VC function.Given all the experiments conducted in this section, we can conclude that in terms of performance BLFW+CAS has no particularly relevant advantage with respect to its predecessor, BLFW+AS. Thus, we can consider that the main advantage of BLFW+CAS is the fact that it can be used as an analysis tool which helps rapidly visualizing the differences between source and target voices. Next section will illustrate this interesting property of the method.This section studies the information provided by BLFW+CAS-based VC functions for three different types of data: (i) several voices chosen from the CMU ARCTIC database; (ii) neutral and emotional speech from a single female speaker; (iii) normal and Lombard speech from a single male speaker. Given the performance limitations of the method when B=9, in this section we set B=14, which gives a good balance between conversion accuracy and suitability for graphical display of information.Fig. 7shows the evolution of the parameters of a BLFW+CAS based VC function trained for two different voice pairs, slt-bdl (female-male) and slt-clb (female-female), using the same experimental setup as in Section 4.1. Instantaneous values of the BLFW factor α are obtained through expression (4). For the remaining parameters, we get instantaneous values by combining expressions (5) and (14):(21)τ(x,θ)=∑k=1mpk(θ)(x)⋅τk,βj(x,θ)=∑k=1mpk(θ)(x)⋅βk,jRegarding the evolution of α, the differences between the two VC pairs are obvious: while conversion toward bdl requires a strongly negative factor, which means that the formants are located at lower frequencies, the factor required by clb is much closer to zero (slightly negative), which reveals that clb is quite similar to slt in terms of formant location. Unvoiced frames are not of interest here because they have no formants, strictly speaking. Hence, abrupt α jumps are observed at voiced-unvoiced transitions for bdl. The spectral tilt related parameter of BLFW+CAS, referred to as τ in Section 3, is equally informative. While bdl requires the addition of a positive spectral slope, which denotes a more pressed phonation, clb requires a negative one, thus exhibiting a smoother phonation than slt. These observations correlate well with our initial expectations according to previous informal analyses by listening. Finally, the fine details of the conversion are captured by the relative amplitudes of the 14 Hanning-like spectral bands. In this case, the variability of the values required by bdl reveals that the differences between slt and bdl are hardly captured by a simple warping+tilt model; specific corrections are needed at specific frequencies. By contrast, in the clb case the amplitudes of the corrective bands are more uniform both in time and in frequency, which means that a warping+tilt model can capture most of the differences between slt and clb. This is probably the reason why slt-clb conversion was usually more accurate than others in previously published works (Erro et al., 2013b; Godoy et al., 2012).For this experiment we selected 30 parallel sentences from one female emotional voice in the database reported by (Sainz et al., 2012). Using dynamic time warping for source-target alignment, we trained BLFW+CAS functions with 32 Gaussian mixtures to convert neutral speech into all the available emotions: anger, happiness and sadness. However, we found the sad style to be spectrally similar to neutral style. Therefore, only the two remaining cases are plotted in Fig. 8. For an easier visualization, we plot the transformation of a single word taken from a longer utterance. Once again, the irregularities observed at unvoiced segments can be excluded from the discussion since they are not significant from a physical point of view but just the result of a numerical optimization procedure. Given that the speaker is the same for all emotions, the BLFW factor α is not far from zero (null warping), the happy style exhibiting slightly positive values and the angry style negative ones at some segments. Thus, the BLFW seems to be somewhat correlated with the valence of the emotion, whereas the spectral tilt increment needed to transform neutral speech into both angry and happy speech is linked to arousal. Of course, prosody is out of the scope of this tool even though it plays a crucial role in rendering emotions. Finally, according to the amplitudes of the AS bands, there seem to be some differences between anger and happiness at high Mel-frequencies (mid-high linear frequencies). Both emotions show a similar contrastive pattern there, its magnitude being larger for anger. Admittedly, this can be a consequence of having used a too simple model for spectral tilt. A more sophisticated model inspired by speech production theories and spectral models of the glottal source may result in a better visual analysis.In this last example, BLFW+CAS VC is used to illustrate the spectral changes related to the Lombard effect. Lombard speech is the type of speech humans produce in noisy environments in order to preserve the intelligibility. We took 50 normal and Lombard utterances spoken by the same UK English speaker from the database described by Cooke et al. (2013). The parameters of the resulting VC function are shown in Fig. 9. We can observe that there is no particular shift of the formant structure, while the spectral tilt is substantially increased. Apart from this, the AS bands reveal that there is also a visible amplification of the 12th band, as happened in neutral-angry and neutral-happy conversion too. This coincidence found for different voices seems to confirm that a more sophisticated model of the spectral tilt should be considered in future versions of the method in order to mimic the enhancement of mid-high frequencies more properly.

@&#CONCLUSIONS@&#
We have shown that GMM-based voice conversion functions operating in the cepstral domain can be designed in such manner that relevant information about the physical differences between voices or styles – relative location of the formants in frequency, relative spectral tilt and relative amplitude in specific frequency bands – becomes accessible. Therefore, the resulting system can be used as an automatic analysis and visualization tool. The new conversion method achieves good quality scores in comparison with the baseline systems, and the trade-off between conversion accuracy and dimensionality of the conversion function can be controlled by means of one of its parameters, namely the number of amplitude correction bands. In spite of the good overall performance of the method, our experiments suggest that it can still be improved by means of a more sophisticated modeling of the spectral tilt. In any case, even without any further improvement, we have shown that the proposed method is useful to analyze the differences between voices or styles whenever some parallel utterances are available for training. As a last remark, the tool has been tested using speech synthesis databases only. The use of noisy or reverberant material could produce unexpected results, though this needs to be confirmed by future works.