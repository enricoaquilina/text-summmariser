@&#MAIN-TITLE@&#
An efficient frequency-domain adaptive forward BSS algorithm for acoustic noise reduction and speech quality enhancement

@&#HIGHLIGHTS@&#
We propose a new Frequency-domain adaptive decorrelating algorithm.The proposed algorithm improves convergence speed even with long adaptive filters.The new algorithm is efficient in Speech quality Enhancement and Acoustic Noise Reduction applications.The proposed algorithm distorts less the speech signal at the output.

@&#KEYPHRASES@&#
Adaptive filtering,Speech enhancement,Frequency-domain,Time-domain,Decorrelation,

@&#ABSTRACT@&#
Graphical abstractImage, graphical abstractBlind Source SeparationForward Blind Source SeparationShort Fourier TransformImpulse responseSymmetric Adaptive DecorrelationForward Symmetric Adaptive DecorrelationAdaptive Noise CancellationLeast Mean SquareNormalized LMSFrequency Least Mean SquareTransform Domain LMSTwo-channel variable step-size forwardTwo-sensor Gauss-seidel fast affine projectionTime Domain Symmetric Adaptive DecorrelationMean Square ErrorSignal to Noise RatioSegmental signal to noise ratioMean square errorSegmental mean square errorCepstral DistanceMean Opinion ScoreDecibelVoice activity detectorExpectation operatorDiscrete Time indexMean averaging value of the SegSNRMean averaging value of the SegMSEMean averaging value of the CDReal and adaptive impulse responses lengthMean averaging value of the CDSampling frequency.source number 1source number 2output number 1output number 2post-filter number 1post-filter number 2Direct IR path for speechDirect IR path for noiseSpeech signalNoisereal filter vectors (cross-IR paths)Adaptive filter vectorslength of the real impulse responses.channel observation number. d ∈ {1, 2}.number of IR of each paths. i, d ∈ {1, 2}coefficient index,m=0,....,L−1.cross-IR between each channel with i, d ∈ {1, 2}.noisy observations with d ∈ {1, 2}.diffuse noise components present with d ∈ {1, 2}.Fixed step-sizesSmall positive constantVariancesSTFT of s1STFT of s2STFT of sSTFT of bSTFT of m1STFT of m2STFT of w12STFT of w21STFT of f12STFT of f21STFT of u1STFT of u2STFT of p1STFT of p2DSP of m1DSP of m2Speech detectorSilence detector

@&#INTRODUCTION@&#
The problem of extracting speech signals from noisy observations has been firstly studied in [1] and [2] for stationary and non stationary signals, respectively. These last studies have given solutions that lead to the resolution of many practical problems in many applications, such as automatic speech recognition, speech enhancement, hand-free telephony, and teleconferencing [3,4]. The problem of separating non-stationary signals cannot be resolved for a single microphone without further information of the signal and noise properties [5]. Furthermore, if the spectrum of speech and noise are overlapping, then cancelling the noise will introduce important distortions in the enhanced signals [6,7]. It was shown that when two or more microphones are used, further information on signal and noise properties can improve their separation [8]. For example, when two microphones configuration is used, a classical adaptive noise canceller (ANC) scheme with noise reference signal is often employed [9]. This is not the real situation, because in practise, the noise microphone also gets the signal as well as the noise and the automatic noise canceller ends up removing the desired signal with the noise [10,11]. The loosely spaced microphones configuration does not progress any more the situation, because very long filter lengths are needed and this leads to excess mean square error (MSE) [12,13].There are a number of applications (i.e., where there is a natural acoustic barrier for sound signals) between microphones that the basic adaptive filtering method will work, but these situations are not so commonly met in daily office or outdoor situations [14,15].Over the last few years, several speech signal enhancement and acoustic noise reduction algorithms have been proposed [16]. A particular attention has been reserved to the situation when significant leakage of the primary signal occurred onto the noise reference, this is mainly due to the primary and reference microphone spacing [17]. This problem decreases the efficiency of the noise cancellation and also generates alteration of the signal components in the output speech signals that are processed by these algorithms. It is proven in [18], that the highest signal to noise ratio (SNR) obtained at the output of such a noise reduction algorithm is equal to the noise to signal ratio present on the reference input. However, some enhancement is achievable if the primary signal is alternating and the filter is adapted only during periods when the primary signal is not present, but this relies on an efficient detection of the primary signal [19]. Newly, the blind source separation structure have been also used to optimally separate speech signal and noise from a noisy observation than can be in most general case convolutedly mixed [19]. These techniques of BSS have been used in fullband and subband configurations and in time-domain and frequency-domain also. Furthermore, much adaptive and non adaptive techniques and algorithms have been proposed with the BSS structure to improve convergence speed of removing the acoustic noise components, the excess steady state values, and the quality of the enhanced speech signal, and often used to improve the intelligibility of the corrupted speech signal [20]. We recall here that two suitable types of blind separation (BSS) structures named forward and backward, are frequently used in the last decade to overcome the problem of signal leakages [5,21].In this work, we focus our interest on the forward BSS (FBSS) and we propose a new frequency domain symmetric adaptive decorrelation (FD-SAD) algorithm to estimate and suppress coherent noise components from very noisy observations. The paper is organized as follows: Section 2 presents the used simplified mixing model for generating the test signals. In Section 3, we describe the FBSS structure and its optimal resolution. In Section 4, we present the formulation of our proposed algorithm. The simulation results of the proposed algorithm and its time domain version (TD-SAD) are presented in Section 5, and, finally, we conclude this work in Section 6.In this section, we present the full acoustics echo path that generate the convolutive mixing signals, then the simplified model of this convolutive mixing signal will be presented.In Fig. 1, we present a full two-channel convolutive mixing model with two punctual signals, g1(n) and g2(n), and two microphones that generates two noisy signals m1(n) and m2(n). The functions f11, f12, f22 and f21 present the impulse responses (IR) of each channel. In this model, the functions f11 and f22 are called the direct IR paths, and f12 and f21 are the cross-coupling effects between the two channels. In addition, η1(n)  and η2(n)  are a diffuse noise components at the noisy observation m1(n) and m2(n), respectively [5].According to Fig. 1, the noisy observation m1(n) and m2(n) are measured as follows:(1)md(n)=∑i=12∑m=0L−1fid(m)si(n−m)+ηd(n)d=1,2.where the following parameters are used:L:length of the real impulse responses.represent the channel observation number. d ∈ {1, 2}.number of IR of each paths. i, d ∈ {1, 2}coefficient index,m=0,....,L−1.represent the cross-IR between each channel with i, d ∈ {1, 2}.represent the noisy observations with d ∈ {1, 2}.represent diffuse noise components present at each sensor with d ∈ {1, 2}.In this paper, we use a simplified version of this model that is consistent with the physic and represent adequately the microphones spacing as in actual situation. This model is presented in the next sub-section.In the full mixing model of two-channel that is presented above, we suppose that the source signal number are smaller or equal to the microphones number, and the original signals are statistically independent [5]. In this paper, we consider the particular situation of two sources and two microphones. The simplified mixing model that we adopt in this paper is shown in Fig. 2and generates the mixing signals m1(n) and m2(n). This simplified convolutive mixing model is validated in theory and practice [5]. In this paper, we suppose that the sources are a speech signalg1(n)=s(n), and a punctual noiseg2(n)=b(n). In this model, we make the following assumptions:1The source signals (speech and punctual noise) are statistically independents.The direct acoustic IR paths equal to the unit impulse response, i.e.f11=f22=δ(n)[19].We consider that no diffuse noise components are present in the microphones vicinity (η1(n)=η2(n)=0) [5].The noisy observation that are generated by this simplified convolutive mixing model are given by(2)m1(n)=s(n)+b(n)*f21,(3)m2(n)=b(n)+s(n)*f12The star in (2) and (3) represent the convolution operator. In the frequency domain, the observation relations of (2) and (3) are transformed and given by the following frequency relations:(4)(M1(ω)M2(ω))=(1F21(ω)F12(ω)1)(S(ω)B(ω))where M1(ω) and M2(ω) are the Short-Time Fourier Transform (STFT) of m1(n) and m2(n). One of the two sources S(ω) is speech, and the second one B(ω) can represent either the car noise or far-end speech that we want to remove. F12(ω) and F21(ω) represent the cross-coupling effects between the channels. The simplified model is given by Fig. 2.In this section, a full presentation of the Forward blind source separation structure and its optimal solution are presented in the next sub-sections.The FBSS structure that we consider in this paper is given by Fig. 3. The theoretical solution of the problem is given by settingw21(n)=f21andw12(n)=f12[5]. The least squares (LS) solution to this problem is obtained by minimizing the mean square error MSE of u1(n) and u2(n), or equivalently in the Fourier domain:(5)(U1(ω)U2(ω))=(1−W21(ω)−W12(ω)1)(M1(ω)M2(ω))where W21(ω) and W12(ω) represent the frequency responses of the separating filters w12(n) and w21(n) respectively. U1(ω) and U2(ω) are the STFT of the partial outputs u1(n) and u2(n), respectively. Inserting Eq (4) in Eq (5), we get the input-output relationship:(6)(U1(ω)U2(ω))=(P1(ω)F21(ω)−W21(n)F12(ω)−W12(n)P2(ω))(S(ω)S(ω))where(7)P1(ω)=1−F12(ω)W21(ω)(8)P2(ω)=1−F21(ω)W12(ω)The optimal solution for retrieving the original signals from u1(n) and u2 (n) (in minimum distortion solution sense) we should have:(9)(S1(ω)S2(ω))=(U1(ω)1(1−F12(ω)W21(ω))U2(ω)1(1−F21(ω)W12(ω)))Where S1(ω) and S2(ω) are the STFT of the final outputs s1(n) and s2(n), We can easily see that the good output signals of the FBSS structure are obtained by using post-filters p1(n) and p2(n) at the output of the FBSS structure, as shown in Fig. 3. From (7) and (8), the frequency two post-filters P1(ω) and P2(ω) of their time domain version p1(n) and p2(n), receptively, are ideally given by:(10)(P1(ω)P2(ω))=(1(1−F12(ω)W21(ω))1(1−F21(ω)W12(ω)))In practice, the cross-filters w12(n) and w21(n) are updated by using adaptive algorithms. In the steady state when the two adaptive filters tend asymptotically to the theoretical solutions, the two post-filters P1(ω) and P2(ω) lead to the same ideal solution:(11)P1(ω)=P2(ω)=11−F12(ω)F21(ω)For the computation of these post-filters, it is proposed in [19] techniques that estimate theses post-filters directly and adaptively. In this paper, the post-filters are discard from the study, and since we are interested in the reduction of the speech distortion, we focus only on the output u1(n) that corresponds to the enhanced and denoised speech signal.In Fig. 4, we show a block diagram of the proposed frequency domain symmetric adaptive decorrelation (FD-SAD) algorithm. In this Figure, we show that the observation m1(n) and m2(n) are firstly transformed in the frequency domain by a short-time Fourier transform (STFT) at each frame k and then we obtain the frequency observation versions M1(ω, k) and M2(ω, k). These two observations are used to compute the STFT of the filtering error U1(ω, k) and U2(ω, k) that are given as follows:(12)U1(ω,k)=M1(ω,k)−W21(ω,k)M2(ω,k)(13)U2(ω,k)=M2(ω,k)−W12(ω,k)M1(ω,k)As it is shown in Fig. 4, this technique is implemented with the forward BSS structure. The new proposed algorithm is based on a frequency domain implementation of the adaptive filters W12(ω, k) and W21(ω, k), which is updated by an adaptive algorithm on a frame-by-frame basis. To update the coefficients of the cross-adaptive filters W12(ω, k) and W21(ω, k), we have used the frequency domain LMS algorithm (FLMS) [21]. For each frame k, we propagate the following equation:(14)W12(ω,k)=W12(ω,k−1)+μ12(ω,k)U2(ω,k)M1H(ω,k)DSM1(ω,k)(15)W21(ω,k)=W21(ω,k−1)+μ21(ω,k)U1(ω,k)M2H(ω,k)DSM2(ω,k)where μ12(ω, k) and μ21(ω, k) are two step-sizes control of the convergence behaviour of the two cross-adaptive filters W12(ω, k) and W21(ω, k), respectively. D is the length of the STFT (FFT). The ‘H’ symbol represents the Hermitian operator. The two parametersSM1(ω,k)andSM2(ω,k)represent the power spectral densities (PSDs) of the two observations M1(ω, k) and M2(ω, k) respectively. In this work, we propose to use a smoothing and recursive estimation of these two PSDs as follows:(16)SM1(ω,k)=α1SM1(ω,k)+(1−α1)|M1(ω,k)|2(17)SM2(ω,k)=α2SM2(ω,k)+(1−α2)|M2(ω,k)|2From Fig. 4, we can well see that the signals at the outputs of the proposed algorithm are obtained within a permutation. However, one can get the useful signal at the appropriate output by taking advantage of the non-stationary of speech, which is basically an intermittent signal [5,20]. In this work, in order to get the enhanced speech signal at the output U1(ω, k), and the noise components at the output U2(ω, k), we have used a voice activity detector (VAD) that controls, in the frequency domain, the convergence of the two cross-adaptive filters. The control is done by the use of a voice activity detector (VAD) to control the adaptation of the filters: i.e., the filter W21(ω, k) is adapted during noise-only periods, whereas the filter W12(ω, k) is adapted only during voice activity periods. This VAD-controlled adaptive scheme yields de-noised speech at the output u1(n), and achieves good convergence of the adaptive algorithms. During noise-only periods, the structure controlled by the VAD behaves as an ANC, as described in [20].(18)W12(ω,k)=W12(ω,k−1)+μ12(ω,k)Ψ(ω,k)U2(ω,k)M1H(ω,k)DSM1(ω,k)(19)W21(ω,k)=W21(ω,k−1)+μ21(ω,k)Θ(ω,k)U1(ω,k)M2H(ω,k)DSM2(ω,k)where Ψ(ω, k) is “one” in speech signal periods and “zeros” in the speech signal absence. The parameter Θ(ω, k) is selected by the following relation:(20)Θ(ω,k)=1−Ψ(ω,k)Relations (18), (19), and (20) show clearly that the adaptive filters W12(ω, k) and W21(ω, k) are updated alternatively. This alternation mechanism of adaptation allows extracting speech signal from the output u1(n) and the noise at u2(n).As we are interesting only on the speech enhancement signal, we will focus only on the output u1(n). The last step of the proposed algorithm is the restoration of the time domain version of the speech signal, we have used an overlap-add technique to reconvert the frequency domain version of the enhanced speech signal U1(ω, k) to the time domain [21–23], i.e. u1(n). The proposed algorithm scheme is given in Fig. 5.In order to evaluate the performances properties of the proposed algorithm which is presented in the previous Section, we have done intensive experiments in terms of the following objective criteria:(i)The segmental signal to noise ratio (SegSNR),The cepstral distance (CD),The segmental mean square error (SegMSE),The system mismatches (SM).Furthermore, we compare our proposed algorithm with the following algorithms of the state of the art:(i)The time domain symmetric adaptive decorrelation (TD-SAD) algorithm, which is the time-domain version of our proposed algorithm [9].The two-channel variable-step-size forward adaptive (2C-VSSF) algorithm [10].The two-sensor Gauss–Seidel fast affine projection (TS-GSFAP) algorithm [11].The comparison is done in the configuration of very noisy observations and two loosely spaced microphones. To represent appropriately the effect of the distance between the two microphones on the characteristics of the signals, we have used the specific model proposed in [5,19] which yields simulated impulse responses f12 and f21 [The sampling frequency is fs=8kHz; the corresponding reverberation time is 30.8ms; the length of the impulse responses isL=256]. The punctual speech signal is a sentence of about 40 seconds; the punctual noise source is a USASI (United State standard institute now ANSI), this noise type is stationary with speech like spectrum that we usually used to test the convergence speed of algorithms.These two punctual sources are taken form AURORA data-base. For the input signal-to-noise-ratio (SNR), it is selected to be equal to 0dB at the two observations m1(n) and m2(n). We recall here that this proposed specific model in [19] highlights the physical phenomenon of microphones spacing and allow representing the behaviour of the BSS algorithms in real situations. In all simulations, we have considered the case of loosely spaced microphone where the post-filters effect is neglected [19].This simulation is done to show the time evolution of the output speech signal s1(n) obtained by the four algorithms, i.e. (i) TD-SAD, (ii) TC-VSSF, (iii) TS-GSFAP, and (iv) the proposed frequency-domain symmetric adaptive decorrelation (FD-SAD) algorithm. The simulation parameters of each algorithm are summarized in Table 1. In this experiment, the real impulse responses f12(n) and f21(n) have been constructed with a random noise variance of 0.5 according to the proposed model in [19]. The time evolution of the source signals, the mixtures, and the obtained output speech signals with the TD-SAD, 2C-VSSF, TS-GSFAP, and the proposed FD-SAD algorithm are given by Fig. 6.From this result, we can only say that the four adaptive algorithms behave very well and allow to remove fully the noise at the output u1(n). Also, we conclude that our proposed algorithm have successfully applied in the field of acoustic noise reduction and speech enhancement application.In the following Sections, we will quantify the performances of each algorithm in terms of: (i) the System Mismatch (SM), (ii) Segmental Signal-to-Noise-Ratio (SegSNR), the Segmental Mean Square Error (SegMSE), and the Cepstral Distance (CD) criteria. Note that this result was obtained thanks to the use of a manual voice activity detector (VAD) system as explained before.In order to quantify the adaptive behaviour of the proposed FD-SAD algorithm in comparison with the TD-SAD, 2C-VSSF, and the TS-FSFAP algorithms, we have used the system mismatch (SM) criterion which is evaluated for each algorithm according to the following expression:(21)SMdB=20log(∥h21−w21∥∥h21∥)where w21(n) and h21 are the adaptive and real filters of the Forward BSS structure, ‖.‖ symbolizes the Euclidian norm, and log is the logarithm of basis 10. This experiment is carried out in terms of SM criterion. This simulation uses the same parameters of each algorithm as given in Table 1, except for the adaptive and real filters that is taken equal to L=256 coefficients. The obtained results of the SM criterion for each algorithm are reported on Fig. 7. From this figure, we note a faster convergence speed of the proposed algorithm in comparison with the other ones in the transient regime especially when the adaptive filters length is selected high, i.e. L ≥ 256 coefficients. However, we have also noted a similar weak steady state of the TS-GSFAP and the proposed FD-SAD algorithms in comparison with the TD-SAD and the 2C-VSSF ones. The good behaviour of the 2C-VSSF algorithm in the steady state regime allows to conclude that the variable stepsize techniques can also be used to improve the behaviour of the TS-GSFAP and the proposed FD-SAD algorithm. In conclusion, the proposed FD-SAD algorithm have shown the best convergence speed performance thanks to the frequency conversion of the algorithm and has also shown a poor steady states values that can be improved by the use of a frequency variable step-sizes, this point could be developed in further work on this same proposed FD-SAD algorithm.To quantify the noise reduction performance of the proposed algorithm in comparison with its temporal version, i.e. TD-SAD, and 2C-VSSF, TS-GSFAP algorithms, we have evaluated the segmental SNR (SegSNR) given by (22), where s(n)and u1(n) represent respectively the original speech signal and the enhanced one. The parameter Q is the mean averaging value of the SegSNR, i.e.Q=1024.(22)SegSNRdB=10log(∑λ=0Q−1|s(λ)|2∑λ=0Q−1|s(λ)−u1(λ)|2Ψ(n))The simulations parameters of each algorithms are given in Table 1, with L=256 coefficients. We have used five types of reel noises, i.e. USASI (United state of America standard institute, now ANSI), white noise, street noise, babble noise, and F16 aircraft noise. All these noises are taken from AURORA database, sampled at 8kHz and coded on 16bits. The input SNRs at each algorithm are selected to be equal to −6dB, 0dB, and 6dB.Table 2and Fig. 8show the obtained results of the SegSNR criterion values with the four algorithms, i.e. TD-SAD, 2C-VSSF, TS-GSFAP, and the proposed algorithm. From this result, a good behaviour of the FD-SAD algorithm, in comparison with the other algorithms is noted. However, this good performance of the proposed algorithm is motivated by the fact that this algorithm has the faster convergence speed performance, which allows reaching high SegSNR values.However, in the steady state regime, we have noted a slight degradation of the proposed algorithm in comparison with the other algorithms. This problem can be resolved by using a frequency variable step-size that depends especially on the SNR values.We propose to use a segmental mean square error (SegMSE) criterion to show the behaviour of the proposed frequency domain FD-SAD algorithm in comparison with its time domain version, i.e. TD-SAD algorithm, 2C-VSSF, and the TS-GSFAP algorithm in acoustic noise cancellation aptitude. As the output speech signal u1(n) is controlled by the adaptive filters w21(n), we concentrate on the evaluation of the SegMSE criterion calculated on the output of adaptive this filter, i.e. w21(n) for each algorithm. This SegMSE criterion is given by the following relation:(23)SegMSEdB=20log(∑k=0D−1u1(k)Θ(k))We note that D represent the time averaging frame length of the output s1(n). The presence of the voice activity detector (VAD of relation (20)) in the update relation (19) of the adaptive filter w21(n) for each algorithm means that the SegMSE criterion is estimated only in the absence speech periods, or alternatively, it is estimated only when the acoustic noise components are presents alone thanks to the intermittence property that of the speech signal.The simulation parameters of each algorithm are summarized in Table 1, except for the adaptive filter length that is taken equal to L=256 coefficients. We have carried intensive experiment with the four algorithms by using the same simulations parameters that are given in the previous Section 5.4, i.e. four noise types, taken from AURORA data-base, three input SNRs, i.e. - 6dB, 0dB, and 6dB. The obtained results of these intensive experiments are summarized in Table 3, and shown in Fig. 9. According to the obtained results of Table 3 and Fig. 9, we can easily see that the proposed FD-SAD algorithm behaves more efficiently the other algorithms. This is true for all of the other noise types that we have used in simulations. However, we have noted that the 2C-VSSF algorithm have shown very competitive performances that are very close to those obtained with the proposed algorithm. In the other hand, the time domain TD-SAD algorithm has shown good the smallest performance in comparison with the other ones. For the TS-GSFAP algorithm, its performances are best than that of the TD-SAD, but very close to that of the 2C-VSSF algorithm. All these remarks lead us to thank to can improve the proposed algorithm by using frequency variable step-sizes to enhance the mean square error (MSE) behaviour.We have evaluated the distortion of the output speech signal of the proposed algorithm by the use of the Cepstral Distance (CD) performance criterion. This CD is estimated by the log-spectrum distance between the original speech signal s(n) and the output of the proposed algorithm u1(n). It is given by the following relation:(24)CDdB=∑ξ=0T−1IFFT[log(|S(ω,ξ)|)−log(|U1(ω,ξ)|)]2Θ(k)where S(ω) and U1(ω) represent respectively the short-time Fourier transform (STFT) of the original speech s(n) signal and the enhanced one u1(n) ate each frame ξ, and T is the mean averaging value of the CD. In this Section, we have evaluated the CD criterion for the four algorithms, i.e. TD-SAD, 2C-VSSF, TS-GSFAP, and the proposed one under the same simulation conditions than those used in the two above sections i.e. Sections 5.3, and 5.4. The obtained results of this simulation for four noise types and four algorithms, and three input SNRs, are summarized in Table 4, and shown by Fig. 10. We can see from the results analysis of Table 4 and Fig. 10 that the proposed frequency domain algorithm have shown the best performance in term of CD values, this means that the intelligibility of processed the output speech signal is more improved (more intelligible) than the output speech signals obtained with the others algorithms. Also, this results confirm that the frequency implementation of the proposed algorithm doesn't improve only the output segmental SNR and the MSE but also improve the intelligibility of the signal in term of the cepstral distance. This good behaviour also show that the frequency domain algorithms distorted less the speech signal when the aliasing between the frames is small. This result shows well that the proposed algorithm outperforms the other algorithms, and the enhanced output speech signals, obtained with the proposed algorithm, sound much clearer with much less speech distortion than the other ones. This good property makes of the proposed algorithm a good adaptive algorithm alternative for this type of application, i.e. acoustic noise reduction and speech quality and intelligibility enhancement.

@&#CONCLUSIONS@&#
