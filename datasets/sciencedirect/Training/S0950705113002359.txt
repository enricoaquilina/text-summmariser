@&#MAIN-TITLE@&#
Evidential classifier for imprecise data based on belief functions

@&#HIGHLIGHTS@&#
An evidential classifier (EC) working with credal classification is proposed.EC can reduce the error rate thanks to the introduction of meta-class.Credal classification is fit for dealing with the imprecise (overlapped) data set.A particular two-step combination procedure is introduced in EC.The discounted technique is involved in the fusion process to get reasonable result.

@&#KEYPHRASES@&#
Belief functions,Evidence theory,K-nearest neighbors,Credal classification,Outlier detection,

@&#ABSTRACT@&#
A new evidential classifier (EC) based on belief functions is developed in this paper for the classification of imprecise data using K-nearest neighbors. EC works with credal classification which allows to classify the objects either in the specific classes, in the meta-classes defined by the union of several specific classes, or in the ignorant class for the outlier detection. The main idea of EC is to not classify an object in a particular class whenever the object is simultaneously close to several classes that turn to be indistinguishable for it. In such case, EC will associate the object with a proper meta-class in order to reduce the misclassification errors. The full ignorant class is interpreted as the class of outliers representing all the objects that are too far from the other data. The K basic belief assignments (bba’s) associated with the object are determined by the distances of the object to its K-nearest neighbors and some chosen imprecision thresholds. The classification of the object depends on the global combination results of these K bba’s. The interest and potential of this new evidential classifier with respect to other classical methods are illustrated through several examples based on artificial and real data sets.

@&#INTRODUCTION@&#
There exist two main families [1] of classification methods: (1) the model-based methods, and (2) the cased-based methods. The former use classical density estimation techniques, and the posterior probabilities of objects (data) are computed from conditional densities of the classes and the prior probabilities using Bayesian inference. However, the exact conditional density functions of each class are rarely available in many cases, which limits the application of the Bayesian classification procedure. The latter are widely used in practice and they directly estimate the class probabilities using the training data set. The K-nearest neighbor (K-NN) approach [2,3], decision trees [5,4], support vector machine (SVM) [6] are all the well-known case-based methods. In particular, the K-NN approach remains the simplest effective method, and it has been widely applied in many practical applications so far. In the original voting K-NN method, the object is assigned to the majority class according to its K-nearest neighbors (KNNs)1In this paper, KNNs denotes the K nearest neighbors of a given object, whereas K-NN denotes the classical K-Nearest Neighbor classification method.1in the training data space. A weighted version of K-NN, known as WK-NN, taking into account the distance between the object and its KNNs has been proposed by Dudani in [7] to outperform the voting K-NN method. K-NN has been extended to work with the fuzzy classification [8,9] to allow the objects to belong to different specific classes with different membership functions. A K-NN method based on imprecise probabilities has also been developed in [10], and it can handle uncertain data in a very generic way.In the classification of uncertain and imprecise data, some data points (objects) that are very close can sometimes truly originate from different classes. Such objects are very difficult to classify correctly into a particular class using the given information. In this case, the classes of these objects will be very imprecise.2The uncertainty refers to the randomness. The data is said uncertain when it can belong to different specific classes with probability mass assignments. The imprecision refers to the indistinguishability among a group of classes. It occurs when some specific classes are sufficiently close with respect to the distance of the object to these classes. In such case, the data is said to belong to an imprecise class, or a meta-class.2Moreover, the data set to classify may also contain noises and outliers in some applications, which makes the classification problem very hard to solve. In the unsupervised data clustering techniques, Dave et al. [11–13] have proposed the concept of noise clustering based on fuzzy c-means (FCM) with robustness improvements in [14]. An evidential version of such method [15] inspired by FCM and Noise-Clustering algorithm [11] has been developed in the belief functions framework to deal with imprecise data and noises. The uncertain and imprecise information can be well modeled by belief functions introduced by Shafer in Dempster–Shafer theory (DST) of evidences [16–19]. The belief functions have been already used in the data classification [20,21,1], data clustering [15,22–25], and imprecise decision-making support [26]. Several K-NN methods based on DST have been introduced to deal with uncertain and imprecise data. Indeed, an evidential version of K-NN, denoted EK-NN [20,21], has already been proposed to model the uncertainty using the ignorant class. A fuzzy version of EK-NN, denoted FEK-NN, has been also proposed in [9]. Based on EK-NN and FEK-NN approaches, a fuzzy evidential reasoning associated nearest neighbor (FE-NN) algorithm was introduced in [27] with an adaptive neighbor selection, which makes it insensitive to the number K of nearest neighbors. A quick K-NN evidence classification algorithm, called super-ball search evidence classification algorithm (S-BSEC) was proposed in [28] to reduce the computation time. In [29], an ensembling technique was applied for the combination of evidential K-NN classifier based on DST to improve the accuracy. The common characteristics of all the aforementioned evidential methods is that they consider only the specific classes and the full ignorant class as the possible classification results. Because of this characteristics, all the particular meta-classes defined by the disjunction of several specific classes are not taken into account at all in these methods. Stated otherwise, only the singleton classes and the full ignorant class defined as the disjunction of all singleton classes are the possible solutions for the classification of an object, which appears to be a theoretical limitation of all these methods. In this work, we propose to use all the possible meta-classes (i.e. all the partial ignorance) to better represent the imprecision of class of the data (objects) that are difficult to be correctly committed to a specific class. By doing this, one can reduce the misclassification errors and, as we will show in the sequel, one can make the data classification more robust to noises and outliers.In this paper, we develop a new evidential classifier (EC) method based on K-NN approach that takes into account rigorously all the possible meta-classes in order to characterize the imprecision of class of data. We consider a frame of discernment Ω={w0,w1,⋯,wh} representing all possible single classes of an object to classify. The particular closure element3The element w0 is introduced explicitly to distinguish the full ignorant (outlier) class represented by Ω={w0,w1,⋯,wh} from the meta-classes that are subsets of Ω⧹{w0}. The meta-classes including w0 (e.g. w0∪wi) are not taken into account in this work because w0 is not precisely defined but corresponds only to the default closure element of the frame of discernment.3w0 is included in the frame Ω to explicitly represent the potential unknown class. In our EC approach, the focal elements (representing the classes) associated with the object are extended to the power-set of Ω denoted by 2Ωwhich consists of all the subsets of Ω. In EC, one object can belong with different masses of belief not only to any singleton (specific) class (e.g. wi), but also to all the subsets of Ω (e.g. wi∪wjor wi∪wj∪wk, etc.) corresponding to the possible meta-classes. This type of classification done by EC is called a credal classification because the classification can be done on specific classes, meta-classes and ignorant class as well thanks to the belief functions.In EC, the object too far from the other data will be committed to the full ignorant class w0∪w1∪⋯∪whrepresented by the whole frame of discernment Ω which will characterize the outlier and noise class. The objects that turn to be very difficult to classify correctly from the given attributes information will not be committed to a specific class (contrariwise to what the classical K-NN and EK-NN methods do), but will be more prudently committed to a proper meta-class with a high mass of belief. Of course the introduction of meta-classes in the EC method reduces the rate of misclassification error by increasing the partial imprecision of the classification result. In practice, there is always an acceptable compromise to find between the imprecision and the error rate depending on the application under concern. In many applications, specially those related to defense and security (like in target classification and tracking), it is generally preferable to get a more robust (and partially imprecise) result that could be precisiated later with additional techniques, than to obtain directly (with a high risk) a wrong precise classification from which an erroneous fatal decision would be drawn. The output of EC can also be used as an alerting mean for requesting extra sophisticated (and generally more costly) techniques, like those used in the military applications. The use of such additional sophisticated techniques highly depends on the importance of the consequences of the decision to take. If such expensive techniques are directly applied for the precise classification of the whole data set, the price may be too high to accept. In fact, these additional techniques are not necessary for these objects that have been precisely classified by EC. The objects in meta-classes are usually a small subset of the total data set. So the price for the specific classification of the few objects in meta-classes using costly sophisticated techniques can be acceptable, but not for classification of the whole data set at the very beginning of the classification process. The use of EC method allows us to select automatically the objects that need a deeper attention and analysis.The illustration of typical credal classification is shown in Fig. 1. In this very simple example, we consider the data points belonging to the three specific classes (i.e. w1, w2, w3) as training data. One sees that the object to classify labeled by a black star symbol in the middle of the classes w1 and w2 is very difficult to correctly assign to the particular class w1, or to w2 because these two classes play a symmetrical role (seem indistinguishable) with respect to this point. In such case, it is more intuitive and natural to prudently commit this object to the proper meta-class4It is worth noting that meta-class introduced in EC is different from the ∊-tube introduced in SVM, which is to improve the generalization performance of the classifier. The meta-classes are used in EC to characterize the imprecision of the class of objects (data) that are hard to correctly classify.4w1∪w2 rather than to the specific class w1, or to w2, because all the information we can reasonably infer is that this object belongs to one of the specific classes in w1∪w2 without knowing which one precisely. By doing this, we reduce the risk of misclassification error. The object in the middle of w1 and w3 labeled by a red star symbol will be similarly classified into w1∪w3, rather than committed to w1 only, or to w3 only. The object labeled by the magenta pentagon symbol is too far from the others, and it will be committed to the ignorant (outlier) class by EC.It is important to underline that the EC method proposes an imprecise and prudent classification solution whenever the available information used for the classification appears insufficient to make a precise and correct classification. EC is used to reduce the risk of classification errors that can yield dramatic consequences with important collateral damages (like in target classification and tracking applications, in battlefield surveillance, or in any crucial defense and security applications). In the classification of the uncertain data, the output of EC with imprecise classification serves as a new classification-making support mean for alerting the classification system that additional data acquisition resources (if available) are necessary to alleviate uncertainty for making a precise classification.In the EC approach, K basic belief assignments (bba’s) associated with one object to classify are constructed with respect to its KNNs. An imprecision threshold t is required in the determination of the bba’s. At first, the bba’s related to the same class label are combined using the simple averaging method. The number of the K nearest neighbors in each class will be taken into account to discount the combination results. The bba’s of the modified combination results of the different classes are then globally fused with a new rule proposed in this work, and the conflicting beliefs produced by the conjunctive combination of the belief of different classes are automatically committed to the associated meta-class. The classification of the object is based on the global fusion result. The credal classifications of the EC method can be reduced to hard classifications thanks to different methods5Typically, the pignistic probability transformation introduced by Smets in [17,18].5of approximation of a general bba into a probability measure.This paper is organized as follows. After a brief recall of the basics of belief functions theory in Section 2, we present the principle of the EK-NN method in Section 3. The new EC approach is then detailed in Section 4. Several examples are given in the Section 5 to show the performances of EC with respect to the main classical methods. Concluding remarks and perspectives are given in the last section of this paper.The original Belief Functions (BF) theory has been developed by Shafer [16,18,17,19] and is also referred in the literature as Dempster–Shafer theory (DST), or the mathematical theory of evidence. In this theory, one starts with a frame of discernment Ω={w1,…,wi,…,wh} consisting of a finite discrete set of mutually exclusive and exhaustive hypotheses where the true value of an unknown parameter can belong (i.e. typically the true class of an object for the application involved in this work). The power-set of Ω, denoted 2Ω, is the set of all the subsets of Ω. For example, if Ω={w1,w2,w3}, then 2Ω={∅,w1,w2,w3,w1∪w2,w1∪w3,w2∪w3,Ω}. The singleton class (e.g. wi) is called specific class. The disjunctions of single classes that represent the partial ignorance in 2Ω(e.g. wi∪wj, or wi∪wj∪wk, etc.) are called meta-classes.A basic belief assignment (bba) is a function m(.) from 2Ωto [0,1] satisfying(1)∑A∈2Ωm(A)=1m(∅)=0The quantity m(A) can be interpreted as a measure of the belief that is committed exactly to A. The subsets A of Ω such that m(A)>0 are called the focal elements of m(.).In the classification of a data set X={x1,⋯,xn} over a frame Ω={w1,⋯,wh}, the credal classification is to classify each object using the belief functions framework, and it can be defined as n-tuple M=(m1,⋯,mn), where miis the basic belief assignment of the object xi∈X,i=1,…,n associated with the different elements of the power-set 2Ω. We recall that if the frame Ω has ∣Ω∣=h elements (h>1), the objects can be committed with different probabilities only to h distinct singleton classes if we use a probabilistic approach. Whereas, in a credal classification method, the objects can belong to 2∣Ω∣>∣Ω∣elements (classes and meta-classes) with different masses of belief.In DST framework, several distinct sources of evidence characterized by different bba’s are combined by Dempster–Shafer (DS) rule [16]. Mathematically, the DS rule of combination of two bba’s m1(.) and m2(.) defined on 2Ωis defined6This rule is mathematically defined only if the denominator is strictly positive, i.e. the sources are not totally conflicting.6by mDS(∅)=0 and for A≠∅, A, B, C∈2Ωby(2)mDS(A)=∑B∩C=Am1(B)m2(C)1-∑B∩C=∅m1(B)m2(C)=∑B∩C=Am1(B)m2(C)∑B∩C≠∅m1(B)m2(C)DS rule is commutative and associative, and thus the result of combination is the same when the sources are combined altogether, or when the sources are combined sequentially. In DS rule, the total conflicting belief mass∑B∩C=∅m1(B)m2(C)is redistributed back to all the focal elements through the normalization procedure.In the decision-making support, bba can be transferred into probability measure using the pignistic probability transformation BetP(.) introduced by Smets in [17,18] and defined by(3)BetP(A)=∑A∈B1|B|m(B),|A|=1,A∈Ω.The lower and upper bounds of imprecise probability associated with bba’s correspond to the belief function Bel(.) and the plausibility function Pl(.) [16]. [Bel(.),Pl(.)] is interpreted as the imprecise interval of the unknown probability P(.), and they can also be used for decision-making support when adopting pessimistic or optimistic attitudes if necessary.Let us consider a group of objects X={x1,…,xn} to classify over the frame of the classes Ω={w1,⋯,wh}. In EK-NN [20], the imperfect information is modeled by the ignorant focal element w1∪…∪wi∪…∪whdenoted also by Ω, and the bba of the object xiassociated to its close neighbor xjlabeled by ws∈Ω is defined as:(4)mjxi(ws)=αe-γsdijβ(5)mjxi(Ω)=1-αe-γsdijβAs recommended in [20], the default value of the discounting factor α can be taken to 0.95, and the parameter γsmust be chosen inversely proportional to the mean distance between two training data belonging to class ws. The parameter β usually takes a small value and because it has in fact a very little influence on the performance of EK-NN, one takes β=1 as its default value. Generally, dijcorresponds to the Euclidean distance between the object xiand the training data xj. K bba’s corresponding to the K nearest neighbors of the object are constructed according to Eqs. (4) and (5). From Eq. (4), one can see that the bigger distance dijwill yield smaller masses of belief on the corresponding class ws. It means that if the object xiis far from the neighbor labeled by class ws, this neighbor can provide little support to xibelonging to the corresponding class ws.From Eqs. (4) and (5), one sees that only the two focal elements wsand Ω are involved in a given bbamjxi(.),i=1,…,n. The classification result of each object xiis obtained by the fusion of the K bba’s using DS rule given in Eq. (2). Because of the very particular structure of the bba’s, DS rule produces as focal elements of the fusion only a specific class wsand the total ignorance class Ω. Therefore, no partial ignorance (meta-classes), say wi∪⋯∪wj≡{wi,⋯,wj}, can become a focal element in EK-NN method.It is worth to note that DS rule for the fusion of the K bba’s with such particular structure is not very effective for the outlier detection. Indeed, for any outlier very far from its KNNs having same class label, the most mass of belief will be committed to the specific class after the fusion process using DS rule (when K is big enough). This behavior is abnormal since the ignorance class Ω should normally take a larger mass of belief when the object corresponds to a true outlier. Because of this DS behavior, this object will be incorrectly considered belonging to a specific class rather than the outlier class. This behavior is not satisfactory in some real applications, like in target tracking in cluttered environments. This behavior is clearly illustrated in the following simple example.Example 1Let us assume that an object x is located very far from all the training data (so that x must reasonably be considered as a true outlier), and assume that all its KNNs belong to the class w. The biggest distance between x and the K neighbors is d, and the corresponding bba’s are mk(w)=δ and mk(Ω)=1−δ with δ∈(0,1) for k=1,2,…,K. In such case, the combination of these K bba’s with DS rule gives m(w)⩾1−(1−δ)Kand m(Ω)⩽(1−δ)K. This result indicates clearly that the belief on the specific class m(w) increases when the number K of nearest neighbors increases. So even if the value δ>0 is very small, the belief committed to w can become very large when K is big enough. It can be easily verified that whenK>log0.5log(1-δ), one has m(w)>0.5>m(Ω). For example, if one takes δ=0.1, and K>6, then m(w)>0.5>m(Ω). This inappropriate result is due to the DS combination of such particular structure of bba’s. To overcome this serious problem, we propose to replace DS rule by the averaging rule in our EC method.Moreover, the EK-NN method appears not very effective to reveal the imprecision degree of the objects that belong to different classes, especially for the objects lying in the overlapped zone of different classes, as shown in the following example.Let us consider that an object x lies in the partially overlapped zone of two classes w1 and w2. We select K=2p nearest neighbors for the classification, and we assume that p neighbors labeled by w1 are at the same distance d1 of x, and the other p neighbors labeled by w2 are also at the same distance d2 of x. If d1 is quite close to d2, the object very likely belongs to w1 and w2 with similar degrees of belief, and the corresponding bba’s are given by: mi(w1)=α, mi(Ω)=1−α, i=1,…,pand mj(w2)=β, mj(Ω)=1−β, j=p+1,…,2p with α=β+∊ (∊ being a very small positive value). The fusion results of the 2p bba’s using DS rule are obtained by:In the classification of uncertain and imprecise data, EC works with the credal classification, and it can produce specific classes (e.g. wi), meta-classes (e.g. wi∪wj) and the full ignorant class (e.g. Ω) as well. The degrees of belief to associate an object to different classes are mainly determined by the KNNs of the objects taking into account the distances between the object and its KNNs. If the object is very close to a particular class, then it is naturally committed to this specific class. But if the object is simultaneously close to several different classes and it is difficult to commit it to a particular specific class, then the object is automatically and prudently committed to a proper meta-class (i.e. the disjunction of these singleton classes) in order to avoid classification error. The zone covered by the meta-class is called the imprecise zone or the overlapped zone of the different classes included in the meta-class. Any object far from its KNNs according to a given imprecision threshold is automatically committed to the full ignorant class (outlier). This threshold should be determined according to the outlier rate one expects. Its exact value can be optimized using the training data as it will be explained in the sequel.In EC, the K bba’s of one object corresponding to its KNNs are constructed using both the distance between the object and its neighbors and some given imprecision thresholds. As in the EK-NN approach, there are only two focal elements (i.e. a specific class and the ignorant class) involved in the structure of original bba’s. If the distance is smaller than the imprecision threshold, most of the mass of belief goes to the corresponding specific class. Otherwise, it will go to the whole set of classes Ω. The bba’s associated with the same class will be combined using the simple averaging of bba’s because of the inappropriate behavior of DS rule (as shown in Examples 1). The number of KNNs in each class may be different. So the combination results associated with different classes are discounted with different weighting factors determined by the number of KNNs in each class. The discounted results are then globally fused with a new combination rule inspired from Dubois–Prade (DP) [30] combination rule. The conflicting beliefs revealing the degree of object belonging to the meta-class are committed to the associated meta-class. The global fusion results are then used for the classification of the object.The principle of EC method for data classification can be summarized by the flowchart in Fig. 2.The EC method consists of two steps: (1) the construction of bba’s, and (2) the combination of the bba’s. These steps are presented in details in the next subsections.In EC, the KNNs of the data point xiare found at first, similarly to the K-NN method. Let us consider the frame of the classes Ω={w0,w1,⋯,wh}. The extra element w0 is explicitly included in the frame to represent the unknown class for the exhaustivity of the frame Ω. w0 is also used to distinguish the ignorant (outlier) class Ω={w0,w1,⋯,wh} from the meta-class {w1,⋯,wh}. We assume that one of the KNNs, say xj, of the object xito classify is labeled by class ws. In EC method, the determination of bba’s associated with xiand xjis based on the imprecision threshold ts. If the distance7The Euclidean distancedij=d(xi,xj)=‖xi-xj‖=(xi-xj)T(xi-xj)is used here.7dijbetween xiand xjis smaller than ts, it indicates that xilies in the acceptance zone of wswith respect to xj, and xiwill be considered most likely to belong to class ws. If dijis larger than ts, it means that xiis beyond the acceptance zone, and we can get little information regarding the class of xiwith respect to its neighbors xj. Then, most of the mass of belief will be committed to the whole set of the classes Ω. So, for any object xi, its bba’s associated with one of its KNNs xjlabeled the class wscan be defined according to this basic principle.There exist several methods for the construction of bba’s [20,31], and the choice of the suitable method depends on the actual application and the available computational resources. The sigmoid function can be used to model fuzzy membership function and human epistemic uncertainty, and it is often used to solve classification problem when a detailed description is lacking [32]. The sigmoid function model provides a very flexible and convenient way well adapted to define the bba using the distance measure and the threshold tuning parameter ts. So it is adopted here for the determination of bba’s. More precisely, the bba is defined as follows: s=1,…,h(6)mws(ws|dij)=11+eλs(dij-ts)(7)mws(Ω|dij)=1-11+eλs(dij-ts)where λsis the slope of the tangent at the inflection point. tsis the abscissa of the inflection point of the sigmoid, and we call it the imprecision threshold. In the determination of these tuning parameters, the average distance between each training data in wsand its KNNs is calculated at first, and the mean valued¯wsof all the average distances for the training data in wsis used to determine λsand ts. The mean valued¯wsis calculated from the training data by(8)d¯ws=1Ns∑i=1Ns1K∑j=1K‖xis-xi,j‖=1KNs∑i=1Ns∑j=1K‖xis-xi,j‖where Nsis the number of training samples in classws,xis,i=1,…,Nsare the training samples in ws, and xi,j, j=1,…,K are the K nearest neighbors ofxisin training data space.The slope λsis defined by the inverse ofd¯ws, and the biggerd¯wsproduces the smaller λs, that is(9)λs=1d¯wsThe threshold tsis defined by(10)ts=ρ×d¯wswhere the coefficient ρ is a positive number. The bigger imprecision threshold tsgenerally leads to the smaller number of outliers. If the imprecision threshold tsis too small then most of the mass of belief will be committed to the ignorant class, and this will cause too many objects in ignorant (outlier) class. If tsis too big then most of the mass of belief will be committed to a specific class. It is not effective in the outlier detection, and too big tsmay lead to a lot of objects in the meta-class which is not efficient in the classification. In the real applications, the value of ρ can be optimized using the training data, and this will be introduced in sequel.Based on Eqs. 6 and 7, K bba’s corresponding to the KNNs of xican be constructed and combined as explained in the next subsection.Two sub-steps are mainly included in the combination of bba’s: (1) the sub-combination of the bba’s associated with the same class label; (2) the global fusion of these sub-combination results about different classes.Because DS rule is not very effective for the outlier detection due to the particular structure of bba’s (as already pointed out in Section 3), we just use the very simple averaging combination method in the sub-combination of the bba’s associated with the same class. The averaging rule works in fact very well and has a low computation burden and that is the reason why we recommend it here. Mathematically, the averaging rule is defined by(11)mws(.)=1ns∑i=1nsmiws(.)where nsis the number of KNNs labeled by class ws.mws(.)is nothing but the simple arithmetic (componentwise) mean of all the bba’s associated with class wsin assuming the same weight for each bbamiws(.).Example 1 (continued): If we use the averaging rule of combination given by Eq. (11) in the Example 1, one gets the following combination resultm(w)=1K∑k=1Kmk(w)=δm(Ω)=1K∑k=1Kmk(Ω)=1-δwhere δ is a very small positive number. One clearly sees that the number of K nearest neighbors has no effect on the final combination result. The ignorant class Ω has a bigger mass of belief than the specific class w, which is perfectly coherent with our analysis since this object is very likely an outlier.In the KNNs of an object under consideration, the number of the neighbors in each class may be not the same. Therefore, the sub-combination results about the different classes obtained with the averaging fusion rule must not be considered with same quality (reliability) at the global fusion level. It is necessary to consider different weighting (discounting) factors depending on the number of KNNs in each class. The sub-combination results are then modified using the classical Shafer’s discounting method [16] as follows:(12)mws′(ws)=αs·mws(ws),forws≠Ωmws′(Ω)=1-∑ws⊂Ωmws′(ws)The bigger number of KNNs in one class leads to the higher reliability of the sub-combination results about this class. The sub-combination results about the majority class will be considered as fully reliable, and its weighting factor is one. The relative weighting factors αsof the other sub-combination results are defined by(13)αs=nsmax(N)where nsis the number of K nearest neighbors in the class ws, and N={n1,⋯,ng} with∑i=1gni=K·gis the number of different classes among the K nearest neighbors of the object.The discounted bba’s about different classes are then combined altogether by the following global fusion rule of combination. In the global fusion, the partial conflicting belief (e.g. m1(A)m2(B)) reveals the degree of commitment of the object in the meta-class (e.g. A∪B). These conflicting beliefs are preserved and committed to the associated meta-class and a new rule of combination is proposed for EC, which is inspired from Dubois–Prade (DP) rule [30], for the fusion of the bba’s with such particular structure. Mathematically, the new EC global fusion rule is defined by m(∅)=0, and for all A≠∅ by(14)m(A)=∏∩i=1gBi=Amwi′(Bi),if|A|=1orA=Ω,∏∪i=1|A|wi=A∩i=1|A|wi=∅mwi′(wi)∏j=|A|+1gmwj′(Ω),if|A|>1.where ∣A∣is the cardinality of the element A, that is the number of the singleton elements included in A. If ∣A∣=1, then it means that A is a specific class, and if ∣A∣>1 then it indicates that A is a meta-class. The combined bba obtained by the fusion Eq. (14) is a proper bba satisfying m(∅)=0 (by definition), and∑A⊆Ωm(A)=1because all the conflicting masses are committed to the corresponding meta-classes according to the second formula of Eq. (14).The conjunctive combination is used in the first formula of Eq. (14) to calculate the belief committed to the specific classes and to the ignorant class, since the degree of belief to commit the object in a specific class or ignorant class depends on the consensus of all the sources of evidences. The mass of belief of each meta-class is computed by the disjunctive combination according to the second formula of Eq. (14). It reflects the degree of belief that the object simultaneously belongs to the specific classes included in this meta-class. Since all the partial conflicting beliefs are preserved and redistributed to the associated meta-classes as with DP rule, the combined bba is a proper normalized bba. This EC approach using meta-classes is a kind of un-supervised learning method, but the determination of one object belonging to a meta-class (e.g. A∪B) depends on the labels of the object’s close neighbors (some of them belong to the class A, and others belong to the class B). For convenience, the pseudo-code of the EC algorithm is given in Table 1.Example 2 (continued): If EC is applied in the Example 2, the fusion results are given by m(w1)=α(1−β), m(w2)=β(1−α),m(w1∪w2)=αβ and m(Ω)=m(w0∪w1∪w2)=(1−α)(1−β). If α and β take values greater than 0.5, then the most mass of belief will be committed to the meta-class w1∪w2, which means that w1 and w2 are almost indistinguishable for this object. These fusion results are coherent with the common sense and characterizes well the imprecision of class of the object belonging to w1 and w2.Guideline of tuning the parameter: The parameter ρ involved in Eq. (10) is associated with the imprecision threshold. Its value can be optimized by the cross-validation (e.g. leave-one-out) in the training data space. In the optimization procedure, the specific classification of each sample is determined according to the classical pignistic probability BetP(.) that allows to transform any bba into a probability measure. Then, one can make sure that the specific classification of the samples is the best possible by tuning the parameter ρ for the given value of K nearest neighbors. The optimized value of the parameter ρ corresponding to the lowest error rate can be found over a grid search. In the real data applications (as in the Simulation No. 3 described in Section 5.3), the parameter ρ has been optimized by the leave-one-out cross-validation method using the training data set.Expressive power of EC: The expressive power of a method concerns its ability to identify and manipulate complex propositions. To examine the expressive power of EC, let us consider n-classes data set to classify. EC deals with 2nclassifications by credal classification (including specific classes, meta-classes and ignorant class) for the n-classes data set, whereas K-NN produces only n specific classifications, and EK-NN provides n+1 classifications including one extra ignorant class. So EC produces more enlarged classifications and has a better expressive power in general than K-NN and EK-NN.Complexity of EC: EC has a high computational complexity, and this is the price to pay to get a method with a better expressive power. In EC, the KNNs of each object have to be firstly sought by calculating the distances between the object and all the training data as well as in the classical K-NN methods (e.g. K-NN, EK-NN), and this step usually requires a high computation burden. The computation procedures of EC and EK-NN are more complex than the K-NN and WK-NN methods due to construction and combination of bba’s. In EK-NN, the K bba’s of one object are directly combined by DS rule to obtain the classification results. In EC, the complexity of computation mainly depends on the actual application under concern. If the KNNs of one object have the same class label, the arithmetic mean of the K bba’s can be directly considered as the classification result by EC, which is more simple than EK-NN. Nevertheless, if the KNNs of one object belong to several different classes, the discounting procedure and the global fusion must apply. Then, the computational complexity of EC becomes higher than EK-NN. However, the computation burden of EC should be smaller than EK-NN for the classification of the separate data classes where the objects can be well committed to the specific classes. EC requires higher computational complexity than EK-NN when dealing with the partly overlapped data classes, where the objects difficult to classify into the specific class are preferentially associated with meta-classes. In real applications, the majority of objects do belong to the specific classes, and there are usually only few objects that have to be assigned to the meta-classes. Therefore, the computation burden of EC for the total test data set is generally not so much higher than with the classical EK-NN method.Example 3 (EC implementation): Let us consider the frame of classes Ω={w0,w1,w2,w3}, and an object xiwith 5-nearest neighbors including three neighbors labeled by classes w1 and two neighbors by w2 with the corresponding bba’s satisfying EC bba modeling:m1(w1)=0.7,m1(Ω)=0.3m2(w1)=0.6,m2(Ω)=0.4m3(w1)=0.8,m3(Ω)=0.2m4(w2)=0.9,m4(Ω)=0.1m5(w2)=0.7,m5(Ω)=0.3m1(.), m2(.), m3(.) assign more belief on w1, and m4(.) and m5(.) assign more belief on w2. After combining m1(.), m2(.), and m3(.), and combining m4(.) and m5(.) using averaging combination rule given by Eq. (11), we getmw1(w1)=0.7,mw1(Ω)=0.3mw2(w2)=0.8,mw2(Ω)=0.2Since most of the KNNs (three out of five) support with high belief the class w1, the discounting factor determined by Eq. (13) is taken to α1=1. This indicates in fact thatmw1(.)does not need to be discounted. The two neighbors among the five nearest neighbors supporting w2 are combined by a simple averaging to obtainmw2(.). This bbamw2(.)is discounted by applying Eq. (12) with the factor α2=2/3≈0.667, and we obtainmw2′(w2)=0.5333,mw2′(Ω)=0.4677The global fusion result ofmw1(.)combined withmw2′(.)using Eq. (14) ism(w1)=0.3274,m(w2)=0.1600,m(w1∪w2)=0.3733,m(Ω)=0.1403This result indicates that xihas got its biggest mass of belief committed to the meta-class w1∪w2. Among the 5-nearest neighbors of this object, three of them belong to w1, whereas the other two belong to w2. It implies the object is likely simultaneously close to w1 and w2, and it is difficult to commit it correctly to the specific class w1, or to w2. So the meta-class w1∪w2 appears to be a reasonable compromise for the classification of this object, which can reduce the misclassification error.This work mainly focuses on the extension of K-NN method under belief functions framework. The additional meta-class in belief functions framework can reflect the imprecision of the classification for some hard-to-classify data. This additional flexibility allows us to get a deeper insight in the data. In the multi-source information fusion systems (like in multi-sensor target identification systems), the proposed EC method can be applied for the target classification using the information from a single sensor. The different sensors can provide complementary information, and the output of EC from different sensors will be fused to reduce the imprecision of classification from each single sensor in order to get a more precise and correct classification. This potentiality and flexibility of the EC approach will be investigated in our future research works.Three simulations have been carried out to test and evaluate the performance of EC with respect to K-NN, Fuzzy K-NN (FK-NN)8In FK-NN, the labeled samples can be assigned class memberships in several ways, and we consider that the labeled samples are given complete membership in their known class and nonmembership in all other classes for simplicity here.8[8], EK-NN, FE-NN, Support Vector Machine (SVM) [6], Classification And Regression Tree (CART) [4] and Artificial Neural Networks (ANN) methods [33]. The parameters of EK-NN were automatically optimized using the method introduced in [21]. The code of EK-NN can be downloaded from [34]. The parameters used in FE-NN are:α0=0.95,β=1,γj=1δj, where δjis the standard deviation of distances between all possible pairs of training data belonging to wj. In SVM, we selected a Gaussian Radial Basis Function kernel with σ=0.125. In ANN, we used the feed-forward back propagation network with epochs=1000 and goal=0.001. In all of our tests, we have observed that EC provides good results when tsis between 2 and 5 times of the average distanced¯ws, which means that the parameter ρ can be taken in [2,5] as a practical recommendation. Its default value can be taken at ρ=3 for convenience. In order to show the effect of the threshold tson the results, we have tested different values in our experiments. In the application of EC based on real data set, ρ was automatically optimized using training data. All the codes were developed and run in MATLABTM. To show the ability of EC to deal with the meta-classes, the classification of each object is based on the maximal mass of belief criterion.The K-NN-based methods do not allow to commit objects in a meta-class because they work under the probabilistic framework only, and therefore the meta-class is not a possible result. The EC method works with the belief functions framework, and because of this it can produce more classes (i.e. the meta-classes) which offer the flexibility to not assign an object to a specific class whenever the available information is not good enough. Our next examples and simulation results clearly show that EC can reduce the misclassification errors thanks to the introduction of the meta-class. This flexibility and specificity of the EC method is potentially very interesting in some applications. This is the main advantage of such EC approach. In the following simulations, we evaluate the performances and we precisely quantify the reduction of the misclassification errors rate obtained by EC with respect to other classical methods.This simulation consists of two particular tests to show the difference in credal classification obtained by EC with respect to K-NN and EK-NN. The default value ρ=3 has been used in the determination of imprecision threshold tsgiven by Eq. (10).In this first test, two classes of artificial data sets Ω={w0,w1,w2} are composed by two rings of points with one extra outlier as shown by Fig. 3(a). Each class has 505 training data points and 506 test points, including the outlier. K=11 neighbors have been used since this value produces good results for both K-NN, EK-NN and EC. The classification results of the test points by different methods are given by Fig. 3(b)–(d). In the titles of the subfigures, we also provide the computational time t (in seconds) required for each simulation. For notation conciseness, we have denoted wte≜wtest,wtr≜wtrainingand wi∪⋯∪k≜wi∪⋯∪wk.As we can see on Fig. 3(a), the data points originating from w1 do cross the data points originating from w2. K-NN and EK-NN just commit the most objects in the overlap zone to w1 as shown on Fig. 3(b) and (c). Obviously, such hard classification will cause many false classifications. EC provides one more meta-class w1∪w2 than EK-NN and K-NN as shown on Fig. 3(d). The classes w1 and w2 are really indistinguishable for these objects in the overlap zone, and these objects are almost impossible to classify correctly into a particular class. So they are prudently committed to the meta-class w1∪w2 by EC. By doing this, one reduces the number of misclassification errors. The object too far from the others is reasonably classified as an outlier by EC as shown on Fig. 3(d), which means that we cannot get useful information about the class of this object. However, the outlier point is considered in class w2 by EK-NN due to the behavior of DS rule with such particular structure of bba’s as shown in Section III-B. This object is also classified into w2 by K-NN because of the limitation of the probability framework. This example shows the interest of the credal classification provided by the EC approach.In this second test, the data set shown by Fig. 4(a) was generated from three 2D Gaussian distributions characterizing the classes w1,w2 and w3 with the following means vectors and covariance matrices (I being the 2×2 identity matrix):μ1=[00]T,Σ1=2·Iμ2=[60]TΣ2=2.5·Iμ3=[34]T,Σ3=1.5·IWe have generated 100 random samples from each of the three classes w1, w2 and w3, and half of the samples from each class was used as training data, and the other half for the test data. We selected K=9 nearest neighbors in all of the three methods. The classification results of test data by K-NN, EK-NN and EC are respectively shown in Fig. 4(b)–(d).We can see in Fig. 4(b) that K-NN produces classification results only into three specific classes w1, w2 and w3 for the test data, even if some data points originated from the different classes overlap on their borders. The EK-NN method behaves similarly to the K-NN for the classification of the specific classes, but three objects far from the training data are committed to the outlier class (labeled by the magenta star symbol) by EK-NN, as shown in Fig. 4(c). With EC, some objects located in the middle of different classes that are difficult to assign to a specific class, are automatically committed to the corresponding meta-classes to avoid the misclassification. For example, five objects labeled by black star symbol in middle of w1 and w2 are considered belonging to w1∪w2, since their KNNs are involved with both w1 and w2. It is similar for the objects belonging to w1∪w2 and w2∪w3. On Fig. 4(d), one object labeled by black hexagram symbol in the center of the three classes belongs to the imprecise class w1∪w2∪w3, because its KNNs are associated with all the three specific classes w1, w2 and w3. Also on Fig. 4(d), two objects (represented by magenta star symbols) very far from the others are automatically committed to the outlier class. This example shows the effectiveness of EC for the classification of close data set.In these two tests, we can observe that EC requires more execution time t than EK-NN and K-NN. This indicates that the computational complexity of EC is larger than EK-NN and K-NN. This is normal because additional meta-classes are involved in the EC method, and this is the price to pay for the credal classification.In this simulation, the performances of EC are compared with K-NN, FK-NN, EK-NN, FE-NN, SVM, CART and ANN on a 4-classes problem. The data set is generated from three 2D Gaussian distributions characterizing the classes w1, w2, w3 and w4 with the following means vectors and covariance matrices:μ1=-50,Σ1=1006,μ2=50,Σ2=1006μ3=05,Σ3=6001,μ4=0-5,Σ4=6001There are 4×100 test samples, and the training sets contain 4×N samples (for N=100, 150, 200). Different values of K ranging from 5 to 25 neighbors have been tested. For each pair (N,K), we have estimated the error rate Reand the imprecision rateRIjby the averages computed from a Monte-Carlo simulation based on ten independent random generations of the data sets. These rates with different numbers of training samples (for N=100, 150, 200) are shown in Table 2. The lowest error rates of K-NN, WK-NN, EK-NN and FE-NN with the corresponding value of K ranging from 5 to 25 are also given in Table 2. In EC, two values of ρ=3 and ρ=5 have been tested to show their influences on the results. Moreover, three popular classification algorithms: SVM, CART, ANN are also included for the comparison with EC.When an object is committed to a meta-class by EC, it means that this object belongs to one of the specific classes included in the meta-class, and it also means that the available information is not sufficient to do a precise classification without a high risk of error. In fact, the meta-class clearly reveals the imprecision (ambiguity) of the classification of this hard-to-classify object. One should be very cautious with such objects, and better adapted and sophisticated techniques (if available) should be used to obtain the precise classification of these objects. The imprecise objects do not count really as misclassifications, but as imprecise classifications in fact. In our simulations, a misclassification is declared (counted) as soon as an object truly originated from wiis classified into A with wi∩A=∅. If wi∩A≠∅ and A≠withen it will be considered as an imprecise classification. The error rate Reis calculated by Re=Ne/T, where Neis number of misclassification errors, and T is the number of objects under test. The imprecision rateRIjis calculated byRIj=NIj/T, whereNIjis number of objects committed to the meta-classes with the cardinality value j. The outlier rate Rois calculated byRIj=No/T, where Nois the number of the objects in the ignorant (outlier) class. NA appearing in Table 2 means not applicable in the corresponding method.When the K number of nearest neighbors is ranking from 5 to 25, the best classification results of these different methods with corresponding K value are shown on Table 2. We can see that the classical neighbor-based methods including K-NN, FK-NN, EK-NN and FE-NN obtain the similar results, whereas the proposed EC produces smaller error rate than with the other methods, since the meta-classes are additionally taken into account in EC. In fact, the class w1 intersects with w3 and w4, and w2 intersects with w3 and w4. The objects in the overlapped areas are very difficult to classify, and most of them are wrongly classified by the different methods, except by EC. The imprecise objects are mostly committed to the associated meta-classes by EC. That is the reason why EC causes the fewest errors but of course brings more imprecision in classification result. If the value of ρ involved in EC method increases from 3 to 5, the imprecision rate will increase a bit, but the error rate and the outlier rate decrease. So ρ can be selected according to the compromise one wants between the error rate and the imprecision rate. Generally speaking, the tuning of ρ does not effect too much the results. Our analysis indicates that the results are not very sensitive to the tuning of ρ.In this third simulation, the EC approach and other methods have been applied to four real data sets provided in the open UCI Machine Learning Repository [35]. More precisely, one has tested the Iris, Ecoli, Breast cancer and Yeast real data sets. Three classes named as cp, im and imU are selected in Ecoli data set to the evaluate our method, since these three classes are close and hard to classify. For similar reasons, we also choose the three classes CYT, NUC and ME3 in the Yeast data set. The main characteristics of the four data sets are summarized in Table 3, and all the detailed information can be found in [35].The k-fold cross validation is applied to measure the performance of the proposed method. k generally remains an unfixed parameter [36], and we use the simplest 2-fold cross validation here, since it has the advantage that the training and test sets are both large. Each sample has been used for both training and testing on each fold. The samples in each classes are randomly assigned to two sets S1 and S2, and the two sets have equal size. We then train on S1 and test on S2, and reciprocally. In the training data sets, we have used the leave-one-out cross validation method to optimize the tuning parameter ρ of EC. The best value of the parameter (the one corresponding to the lowest error rate) has been found by a grid search with a sampling size of 0.1 spanning the range [2,5] for ρ. The performances are measured by the number of ReandRIj. In our analysis, we took j=2 because none object is committed to the meta-classes with cardinality greater than two. In these data sets, there is no outlier detected, and the belief on the ignorant class is proportionally distributed to the other focal elements. The classification results obtained by the different methods for different values of K are shown in Tables 4–7.One can see from Tables 4–7, the classical neighbor-based methods as K-NN, FK-NN, EK-NN and FE-NN generally produce the similar error rates. EC provides the smallest error rate in the four real data sets under test, since the samples difficult to classify are reasonably and automatically committed to the associated meta-classes. This performance evaluation of the methods based on very different real data sets illustrates that EC reduces the rate of misclassification error. With EC, one sees that there exist some samples committed to the meta-classes, and it clearly indicates that the attributes used for the classification are not sufficient for doing a correct specific classification of the objects assigned to the meta-classes. For these objects, other complementary sources of information will be necessary to get more precise (refined) classification results.A new evidential classifier (EC) based on K-nearest neighbor method has been proposed for classifying uncertain and imprecise data set. In EC, the K bba’s associated with each object are obtained using the distances between the object and its K-nearest neighbors, and a chosen imprecision threshold. The bba’s associated with the same class are combined by the averaging rule of combination, and the number of the K nearest neighbors in each class is also taken into account to discount the combination results. These discounted bba’s are then combined altogether with a new rule in which the conflicting beliefs are used to reflect the degree of commitment of the object with its associated meta-class. Because the EC method generates decisions on specific classes, on meta-classes and on the ignorant class as well, it produces a wider choice in classification results (called a credal classification) than the K-NN, FK-NN and EK-NN methods. The credal classification can be easily adapted to work under the probability framework for making a hard classification if necessary thanks to the pignistic transformation. However, it is not always recommended to make a hard classification from an approximation of a bba into a probability measure because of loss of information which increases the risk of final misclassification. It is generally better and more reasonable to ask for additional sources of information (if possible) before taking the final decision on a specific class. The interest and effectiveness of the EC approach have been shown through three simulations based on both artificial data sets and real data sets. As research perspectives, we plan to apply this new EC method on realistic scenario related with target classification and tracking applications. We will also examine the possibility to adapt or extend EC approach for working with interval-valued data that have recently been proposed in [37] for constructing regression and classification models, and for improving the SVM method.

@&#CONCLUSIONS@&#
