@&#MAIN-TITLE@&#
Parallel SRP-PHAT for GPUs

@&#HIGHLIGHTS@&#
Parallelization of SRP-PHAT algorithms.Optimal implementation of the proposed parallel SRP-PHAT for GPGPUs.Achievement of 1.5–6 times speedup in sound source localization.

@&#KEYPHRASES@&#
Sound source localization,SRP-PHAT,GPUs,

@&#ABSTRACT@&#
The steered response power phase transform (SRP-PHAT) is one of the widely used algorithms for sound source localization. Since it must examine a large number of candidate sound source locations, conventional SRP-PHAT approaches may not be used in real time. To overcome this problem, an effort was made previously to parallelize the SRP-PHAT on graphics processing units (GPUs). However, the full capacities of the GPU were not exploited since on-chip memory usage was not addressed. In this paper, we propose GPU-based parallel algorithms of the SRP-PHAT both in the frequency domain and time domain. The proposed methods optimize the memory access patterns of the SRP-PHAT and efficiently use the on-chip memory. As a result, the proposed methods demonstrate a speedup of 1276 times in the frequency domain and 80 times in the time domain compared to CPU-based algorithms, and 1.5 times in the frequency domain and 6 times in the time domain compared to conventional GPU-based methods.

@&#INTRODUCTION@&#
The steered response power with a phase transform filter (SRP-PHAT) is a robust sound source localization (SSL) algorithm that can handle noisy sound signals in reverberant environments (DiBiase et al., 2001). However, since it performs a grid search examining a large number of candidate sound source locations, the SRP-PHAT is computationally expensive and may not be used in real time. Many approaches have been proposed for a fast SRP-PHAT (Zotkin and Duraiswami, 2004; Peterson and Kyriakakis, 2005; Do and Silverman, 2007; Dmochowski et al., 2007; Cho et al., 2009; Yook et al., 2015). For instance, a hierarchical search method (Zotkin and Duraiswami, 2004; Do and Silverman, 2007) gradually prunes the candidate sound source locations in a coarse-to-fine search. A drawback of this method is that it may prematurely prune the sound source with the highest power before it reaches the final decision. Alternatively, a hybrid method (Peterson and Kyriakakis, 2005) first generates a small set of candidate sound source locations using a time difference of arrival (TDOA) based search. It then performs an SRP-PHAT based grid search on this small set of candidate locations. If TDOA estimation is unsuccessful in the first step, then the SRP-PHAT based SSL will fail in the final decision. In Dmochowski et al. (2007), an inverse mapping function relates a relative TDOA to a set of candidate locations. Only the output powers of the locations that are inversely mapped by the TDOA are considered to find the maximum power location. This method may also fail to find the maximum power location, because it searches only those few locations that are inversely mapped by the relative TDOA. A real-time SRP-PHAT method was proposed for humanoid robots (Cho et al., 2009). It uses a pre-computed look-up table that contains a complete set of regions having unique TDOA values. Although it is guaranteed to find a global maximum power location, this method is suitable for small-scale microphone array systems because there may be too many candidate coordinates in the look-up table for a large-scale microphone array system (Cho et al., 2009). Recently, improvement of this method was proposed by adopting the idea of the hierarchical search method (Yook et al., 2015).All of the above methods focus on removing repetitive computation from the algorithm or reducing the number of candidate sound source locations that must be searched. However, with the support of single-instruction multiple-data (SIMD) on modern central processing units (CPUs), a parallelized design of the SRP-PHAT on a CPU has also been suggested (Lee and Kalker, 2010). In addition, as general purpose computing on graphics processing units (GPUs) becomes common, parallelized designs of the SRP-PHAT on GPUs have been proposed (Silveira et al., 2010; Lee and Yook, 2011; Minotto et al., 2012). It is known that the bottleneck for most GPU-based parallel processing tasks is the amount of memory access, rather than the computation itself (Williams et al., 2009; Jang et al., 2011). To maximize the efficiency of the SRP-PHAT, therefore, it is imperative to process memory access efficiently so that it can take the best advantage of GPUs. The previous GPU-based parallel SRP-PHAT (Minotto et al., 2012) demonstrated much improvement in their performance compared to the CPU-based implementations. However, it did not exploit the maximum performance from GPUs since on-chip memory usage was weakly considered.In this paper, we propose a GPU-based parallel algorithm of the SRP-PHAT optimized for the efficient use of memory on GPU devices. The proposed method reduces space complexity by maximizing the use of on-chip memory on the GPU. In addition, it reduces the bank conflicts and utilizes the cache line in its maximum capacity by vectorizing memory access. As a result, the proposed method greatly reduces the overall execution time of the SRP-PHAT-based sound source localization.The remainder of this paper is organized as follows. In Section 2, we review the sequential SRP-PHAT in the frequency domain (FD) and time domain (TD). In Section 3, we analyze the parallelism of the SRP-PHAT. In Section 4, we present the details of GPU-based parallel algorithms of the SRP-PHAT. Section 5 discusses the experimental results of the proposed methods. Finally, we draw conclusions in Section 6. Some notations used in this paper are summarized in Table 1.SRP-PHAT estimates the location of a sound source by searching for the location with the maximum steered response power (SRP). The location of the maximum SRP,qˆ, can be obtained as follows:(1)qˆ=argmaxq∈{q1,…,qQ}pqwhere Q is the number of candidate sound source locations and pqis the SRP of a given location q. Since pqcan be computed either in the FD or TD, there are two types of SRP-PHATs.In the FD, pqfor a location q is calculated as follows (Minotto et al., 2012):(2)pq=∫02π∑m=1MXm(ω)|Xm(ω)|ejωτ[q,m]2dωwhere M is the number of microphones,Xm(ω)is the Fourier transform of the mth microphone signal for frequencyω, andτ[q,m]is the time of arrival (TOA) of a sound signal from the location q to the mth microphone. After calculatingpqusing (2) for each candidate location, the FD SRP-PHAT finds the sound source location using (1). Table 2shows an implementation of the SRP-PHAT in the FD. In the table, xm[t] is the sound signal witht=1:T.In the TD,pqis calculated as follows (Minotto et al., 2012):(3)pq=∑m′=1M−1∑m″=m′+1MRm′,m″[τ[q,m′]−τ[q,m″]]where the cross correlation,Rm′,m″[τ[q,m′]−τ[q,m″]], is computed in advance. The cross correlation can be obtained using the Fourier transform as follows.(4)Rm′,m″[τ[q,m′]−τ[q,m″]]=12π∫02πXm′(ω)Xm″*(ω)|Xm′(ω)Xm″*(ω)|ejω(τ[q,m′]−τ[q,m″])dωwhereXm*(ω)is the conjugate ofXm(ω)andτ[q,m′]−τ[q,m″]is the TDOA between microphonesm′andm″. After calculatingpqfor each candidate location using (3), the TD SRP-PHAT finds the sound source location according to (1). Although Fourier transform and inverse Fourier transform may be used for fast computation of cross correlation, the overall algorithm is still called a time domain SRP-PHAT because the accumulation of the cross correlation occurs in the time domain once the cross correlation has been obtained. Therefore, it has a finite localization resolution unlike the frequency domain SRP-PHAT. Table 3shows an implementation of the TD SRP-PHAT. In the table, n denotes a pair of microphonesm′andm″. The cross correlation between each pair of microphones is computed in advance and stored inRn.Parallelism is a characteristic of computation in which two independent operations are carried out simultaneously without affecting each other. For example, thread-level parallelism (TLP) allows the simultaneous execution of independent tasks in different threads (Hennessy and Patterson, 2011). Dependency here means that the execution order of operations must be preserved. For instance, there are dependencies within a thread such as read-after-write (RAW), write-after-write (WAW), and write-after-read (WAR), among which only RAW is considered as a true dependency. However, RAW, WAW, and WAR are all considered as true dependencies in multiple threads, whereas the read-after-read (RAR) dependency can be resolved by broadcasting. The GPUs organize a number of threads (up to SIMD width) into groups called warps and schedules them in a round-robin fashion (Jog et al., 2013). Therefore, intra-warp as well as inter-warp dependencies are also considered as true dependencies.The SRP-PHAT in the FD designed as in Table 2 contains three RAW dependencies:Xmon lines 2, 4, and 13, s on lines 13 and 15, and p on lines 16 and 18. Thus, the SRP-PHAT in the FD can be divided into three blocks: fast Fourier transform (FFT), SRP calculation, and maximum SRP search blocks. The FFT block on lines 1–6 has TLP among up toM×logTthreads. The innermost loop on lines 12–14, however, cannot be parallelized since the result must be stored in the same variable s, causing a RAW dependency. Similarly, the SRP calculation block on lines 10–17 cannot be parallelized due to a RAW dependency for storing the result in the same variable p. The outer loop on lines 8–22 has TLP among Q threads, except lines 18–21. Note that the RAW dependency on lines 16 and 18 can be resolved separately by parallel reduction.The SRP-PHAT in the TD designed as in Table 3 contains four RAW dependencies:Xmon lines 2, 4, and 9,Znon lines 9 and 13,Rnon lines 13 and 19, and p on lines 19 and 21. Thus, the SRP-PHAT in the TD can be divided into five blocks: FFT, cross spectrum (CS) calculation, inverse FFT (IFFT), SRP calculation, and maximum SRP search blocks. The FFT (lines 1–6) and IFFT (lines 12–14) blocks have TLP among up toM×logTthreads andN×logTthreads, respectively. The inner loop on lines 8–10 has TLP since the computation of each element is independent of the results of previous iterations. The CS calculation block on lines 7–11 has TLP since the RAR dependency among the threads sharingXmmay be removed by memory broadcasting. Lines 16–25 have TLP except lines 21–24, while the loop in the SRP calculation (lines 18–20) cannot be parallelized since the results must be stored in the same variable p causing a RAW dependency. Finally, the RAW dependency on lines 21–24 can be resolved by parallel reduction as in the FD SRP-PHAT.A GPU chip along with the global memory (the DRAM on the graphics card) is called a device, while a CPU chip along with its external DRAM is called a host. A typical GPU chip consists of a set of streaming multiprocessors (SMs) and an L2 cache. An SM, in turn, has a set of cores, a set of registers, and an L1 cache. Fig. 1shows a typical configuration of a modern computer system with a GPU. The memory bandwidth of the buses between the host and device, global memory and L2 cache, L2 cache and L1 caches, and cores and registers are approximately 32 GB/s, 336 GB/s, 1.6 TB/s, and 8 TB/s, respectively (Farber, 2011). The memory bandwidth values given here represent optimistic maximum possible values assuming no memory access conflicts. On the other hand, the theoretical computing speed of a single core of a GPU is quite fast compared to the memory bandwidth. For example, while a single precision (4-byte) 3 TFLOPS may require up to 12 TB/s memory bandwidth, global memory can provide only 336 GB/s. Hence, global memory access is a serious bottleneck.In CUDA applications, a single core executes one thread at a time. A group of 32 threads in an SM constitutes a warp that executes SIMD operations. A set of warps in an SM is called a thread block. A set of thread blocks is called a grid (or kernel). An application is composed of a sequence of grids (Hennessy and Patterson, 2011).As discussed in the previous section, the constraining factor for GPU-based calculation is mainly memory bandwidth. In this section, we propose GPU-based parallel algorithms for SRP-PHAT, in both the FD and TD, which are optimized for efficient memory access. Note thatτ[q,m],τ[q,n]≡τ[q,m′]−τ[q,m″], and2πk/Tcan be computed in advance and stored in tables.Table 4shows the proposed parallel SRP-PHAT algorithm in the FD. It consists of four kernels: FFT executed byM×Tthreads in parallel (lines 1–2), SRP computation (lines 3–17), and maximum power search (MPS) by parallel reduction (line 18).Xmon lines 1 and 2 in Table 4 are computed by a thread whose index is determined by m and k. The batch mode CUFFT (NVIDIA, 2014) is used for all microphones at once. In theory, T-point parallel FFT takesOlogTassuming that the number of available cores, C, is larger than T. When the number of available cores is less than the number of FFT points, however, only C points out of T points can be computed in parallel and this has to be repeatedT/Ctimes. Therefore, it takesO((T/C)logC)in practice. The total number of samples for the FFT isM×T, and the time complexity becomesO(M(T/C)logC). The amplitude ofXm[k]is discarded and only the phase is kept.Each thread for the SRP computation executes lines 3–16 (except lines 3–4) in Table 4. SinceXm[k],τ[q,m], and2πk/Tare used repeatedly when calculating SRPs, it is better to precompute and store them in the shared memory rather than global memory to minimize memory access time. Also, X is transposed when it is loaded into the shared memory to improve memory access pattern.τ[q,m]does not need to be shared by the threads. Therefore, if there is enough local memory (registers),τ[q,m]may be stored in the local memory for even faster memory access. If the shared memory is not large enough to accommodateXm[k]for all microphones and frequency bins, only a small sub-band,κ, of Xmis loaded into the shared memory at a time. The loaded sub-band stays in the shared memory until all the threads using it have finished. In this way, unnecessary global memory accesses forXm[k]are avoided by loading it only once.To reduce unnecessary thread context switching, a thread (lines 3–16) computes the SRP for a group of locations,ρ. Thus, a thread is identified by thread index(ρ,κ). A thread computes the partial SRP using only a sub-bandκfor all locations inρ. So, the time complexity of each thread isOρκM. Since there are Q candidate locations to search, the number of total threads running in parallel for the SRP calculation isQ/|ρ|×T/|κ|. The time complexity, then, becomesO((Q/C)TM)in practice reflecting the number of available cores.Finally, once the SRPs for all candidate locations are obtained, the maximum SRP location is searched using parallel reduction. The reduction can be implemented in two levels. That is, a local maximum SRP is found in each SM. Then, the global maximum is found among the local maxima. The time complexity of the reduction isO(logQ)in theory. In practice, however, it becomesO((Q/C)logC)due to the number of available cores.Table 5shows the proposed parallel SRP-PHAT algorithm in the TD. It consists of five kernels: FFT executed byM×Tthreads in parallel (lines 1–2), cross spectrum computation byT/|κ|threads in parallel (lines 3–8), IFFT byN×Tthreads in parallel (line 9) whereNis the number of microphone pairs, SRP computation by Q threads in parallel (lines 10–15), and maximum power search by parallel reduction (line 16).The FFT is executed as in the FD SRP-PHAT. Then, a cross spectrum of each microphone pair is computed.Xmfor all microphones is usually too big to be loaded into the local memory. Thus, only a small sub-band of frequency bins,κ, is loaded into the shared memory of an SM (see Fig. 2). SinceXm[k]is accessed repeatedly by a thread, keepingXm[k]in shared memory greatly improves performance. The cross spectrum computation kernel (lines 3–8) is responsible for computing only one sub-band of frequency bins,κ, for all microphone pairs. Thus, the total number of threads isT/|κ|, which is typically larger than the number of available cores,C. The time complexity for parallel cross spectrum computation isO((T/C)N).Rn[k]is computed using a parallel version of the IFFT for all microphone pairs in parallel (line 9). The total number of points for the IFFT isN×T, and the time complexity isON(T/C)logC. Not all values of the generalized cross correlation,Rn, may be used for the SRP-PHAT due to the limitation on possible TDOA values, which are affected by the physical configuration of the microphone array used. In order to reduce memory traffic, only those values needed for the SRP-PHAT are kept inRnbefore it is loaded into shared memory (line 10). This needs to be done only once per SM.τ[q,n]is encoded in advance using the fewest number of bits possible to minimize memory transfer traffic. In addition,τ[q,n]may be transposed and reorganized in advance such that memory coalescing and cache performance is maximized. Furthermore, it allowsR˙n[τ[q,n]]to be broadcasted because adjacent locations tend to use the same TDOA values. One thread is responsible for the SRP computation of only one candidate location (lines 10–15). A thread identified by the thread index q computes the SRP,pq, for a candidate location q. The number of total threads required is Q, which is typically much larger than C. The time complexity becomesO((Q/C)N).We generated experimental data through simulation and real recording both in the environment described in Table 6. The real recording was performed using a 16-sensor cylindrical microphone array with a radius of 12.2cm and a height of 24cm located at the center of the room. The sound sources were located at 90°, 120°, 150°, and 180° at a distance of 200cm from the center of the room and at a height of 44cm. The azimuth, elevation, and radius of the search range were 0–360°, 0–89°, and 100–300cm in a hemispherical coordinate system, respectively. The number of candidate locations to search varied: 3888, 97,200, and 388,800.τ[q,m]andτ[q,n]were generated in advance accordingly. The number of microphones also varied: 8, 16, and 32. The 32-sensor data were generated using a similar method to that described by (Allen and Berkley, 1979). For some applications such as a spoken language based humanoid robot interface (Cho et al., 2009; Yook et al., 2015), estimated angles within ±10° from the true sound source direction are generally acceptable. The sound source localization accuracy was 99.99% when the estimated angles within ±10° of the sound source were considered correct. When estimated DOAs that were within ±5° of the true azimuth and ±3° of the true elevation were considered correct, the sound source localization accuracy was 76% (Yook et al., 2015). Note that the heights of the sound sources were 170cm in (Yook et al. 2015). The FFT and IFFT were implemented using the CUFFT library (NVIDIA, 2014).In order to make a comparison with conventional methods, Minotto's method (Minotto et al., 2012) was implemented. In addition, a CPU version of the SRP-PHAT was implemented. The CPU version was also run in parallel with 8 threads. The automatic vectorization capability was also utilized. To make a fair comparison between the CPU and GPU implementations, the data transfer time between the host and device was included in the execution time of the GPU experiments. We measured the average execution times per frame for both the FD SRP-PHAT and TD SRP-PHAT with different values of M, Q, and using different devices as described in Tables 7–11. In Tables 7, 9, and 11, the bold faces mean the fastest results. In Tables 8 and 10, the bold faces mean speedups compared to the conventional methods. The average execution time per frame was measured using the NVIDIA Profiler of the CUDA Toolkit.Table 7 shows the average execution time per frame in milliseconds with a varying number of microphones (8, 16, and 32). The number of candidate locations to search was set to 97,200. For the FD SRP-PHAT, the proposed method showed an average speed increase of 1.5 and 1080 times compared to the conventional GPU and CPU implementations, respectively. The 1.5 times speed improvement was due to the efficient memory access pattern of X and the aggressive use of shared and local memory. In the TD SRP-PHAT, the proposed method showed an even better speed improvement: 7.5 times faster than the conventional GPU-based method. This improvement was due to the efficient memory access schemes that maximize memory coalescing and cache performance. In particular, the cross spectrum computation block reduced the number of global memory accesses and memory conflicts considerably compared to the conventional method. As a result, the speedup factor increased as the number of microphones increased.Table 8 shows the average per frame execution time of each kernel in milliseconds with a varying number of microphones: FFT, CS, IFFT, SRP, MPS, and data transfer (DT). Data transfer refers to the waveform data and final search result transfer between the main memory of the host and the global memory of the device. The conventional and proposed methods showed similar execution times for the FFT, MPS, and DT kernels in the FD SRP-PHAT, and FFT, IFFT, MPS, and DT kernels in the TD SRP-PHAT. However, the SRP computation in the proposed FD SRP-PHAT showed a speed improvement of 1.5 times. It can be seen from the table that most of computation time was spent in SRP computation for the FD SRP-PHAT. This is because SRP computation of the FD SRP-PHAT involves a large number of complex exponential multiplication operations. For the TD SRP-PHAT, however, the SRP computation block was not as dominantly time-consuming as for the FD SRP-PHAT. In the TD SRP-PHAT, the CS and SRP kernels showed as much as 16.5 and 9.4 times speed improvement, respectively. This is because the threads were organized in such a way that memory conflict was avoided andXm[k]was loaded into shared memory. Most of the gains in speed were due to the improved efficiency of the cross spectrum and SRP computation kernel.Table 9 shows the average per frame execution time in milliseconds with a varying number of candidate locations to search (3888, 97,200, and 388,800). The number of microphones used was set to 16. In the FD SRP-PHAT, the proposed method showed an average speed improvement of 1.5 and 1472 times compared to the conventional GPU and CPU implementations, respectively. In the TD SRP-PHAT, the proposed method also showed better performance improvement: 4.5 times faster than the conventional GPU-based method. Again, this improvement was due to efficient memory usage. When the number of search locations was 388,800, the proposed method ran 4.4 times faster than the conventional method. When the number of computations was small, however, memory optimization did not contribute a great deal. For example, when there were only 3888 search locations, the speed improvement factor was 3.4 times for the TD SRP-PHAT. This was anticipated because GPUs work better for computationally intensive tasks. The proposed method allocated one thread for each candidate location for the SRP computation. Therefore, when there are a small number of candidate locations, many GPU cores may be idle, causing low utilization of the multi-core capability. Conversely, when the number of candidate locations is much larger than the number of cores available, fine grain parallelism (i.e., a larger number of threads) is not advantageous because of the unnecessary thread scheduling overhead.Table 10 shows the average per frame execution time of each kernel in milliseconds with a varied number of candidate locations searched. As in Table 8, the SRP kernel in the FD SRP-PHAT and the CS and SRP kernels in the TD SRP-PHAT showed significant speed increase.Table 11 shows the average execution time per frame for various devices. The number of microphones was 16 and the number of search locations was 388,800. On all three devices, the proposed methods outperformed the conventional methods. The proposed FD SRP-PHAT ran 1.7 times faster on the average than the conventional method. The proposed TD SRP-PHAT showed an even more speed increase: 5.6 times faster on the average. Although the GTX 680 had 3 times more cores than the GTX 580, the latter showed the maximum speed increase for the TD SRP-PHAT. This was because the number of SMs in the GTX 580 is twice as many as in the GTX 680, which means more shared memory in the GTX 580 and the proposed methods made full use of shared memory.

@&#CONCLUSIONS@&#
