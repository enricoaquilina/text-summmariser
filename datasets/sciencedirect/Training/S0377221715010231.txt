@&#MAIN-TITLE@&#
Supply chain forecasting: Theory, practice, their gap and the future

@&#HIGHLIGHTS@&#
The literature on supply chain forecasting is critically reviewed.The process of involving the forecasting community towards that task is described.Gaps between theory and practice are identified.Data and software related issues are explicitly considered.Challenges are summarized followed by suggestions for further research.

@&#KEYPHRASES@&#
Supply chain forecasting,Forecasting software,Forecasting empirical research,Literature review,

@&#ABSTRACT@&#
Supply Chain Forecasting (SCF) goes beyond the operational task of extrapolating demand requirements at one echelon. It involves complex issues such as supply chain coordination and sharing of information between multiple stakeholders. Academic research in SCF has tended to neglect some issues that are important in practice. In areas of practical relevance, sound theoretical developments have rarely been translated into operational solutions or integrated in state-of-the-art decision support systems. Furthermore, many experience-driven heuristics are increasingly used in everyday business practices. These heuristics are not supported by substantive scientific evidence; however, they are sometimes very hard to outperform. This can be attributed to the robustness of these simple and practical solutions such as aggregation approaches for example (across time, customers and products).This paper provides a comprehensive review of the literature and aims at bridging the gap between theory and practice in the existing knowledge base in SCF. We highlight the most promising approaches and suggest their integration in forecasting support systems. We discuss the current challenges both from a research and practitioner perspective and provide a research and application agenda for further work in this area. Finally, we make a contribution in the methodology underlying the preparation of review articles by means of involving the forecasting community in the process of deciding both the content and structure of this paper.A supply chain consists of all the parties involved, directly or indirectly, in fulfilling a customer demand (Chopra & Meindl, 2012). A ‘party’ is any decision making unit within the supply chain. It could be an organization or a business unit within an organization. The supply chain extends from the final customer through a variety of retailers, wholesalers and distributors, and goes back to the manufacturers and their component and raw material suppliers. Within the chain, there are flows of materials and products, information and money. Whilst financial flows are undoubtedly important, the focus in this paper is on the flows of materials, products and information. The integration of financial forecasts into an organization's planning system is beyond the scope of this review.The final customer's demand sets the entire supply chain in motion. It generates a course of actions at retailing organizations to respond to such demand, by having the necessary products and services in place to satisfy the customers. These ultimately involve the generation of demand at the next level upstream11The characterization of the information and materials flow upstream and downstream the supply chain respectively serves only discussion purposes and indeed many researchers opt for the presentation of these flows in reverse (vertical) order, or in a horizontal fashion.in the supply chain, at wholesalers or distributors, who subsequently respond by placing requests on manufacturers, and so on. This upstream flow of requests constitutes the transmission of information from one supply chain member to another. This information flow is complemented by a flow of materials / products downstream the supply chain to satisfy these requests. Although the length of supply chains may vary considerably, satisfaction of the final consumers’ requests is the raison d’être of all supply chains.In addition to the length of supply chains discussed above (and the flow of materials and information across them) there is another key operational dimension involved in their structure: their depth. Supply chains are often geographically dispersed and they are sometimes referred to as supply nets rather than chains although we will retain the latter term for the purposes of our discussion. At any supply chain level, they involve many customers and suppliers (being placed in various locations) and of course very many products, all of which form, either separately or in combination with each other, natural cross-sectional hierarchies. Demand will then need to be aggregated at these various hierarchical echelons to inform decision making at a wide range of organizational and functional levels.If the final consumers’ demand were constant, or known with certainty well in advance, then the operation of a supply chain would be a straightforward (backwards) scheduling exercise. However, demand is not known and thus it needs to be forecasted. It is the uncertainty associated with this demand that makes supply chain management very difficult. In addition, the frequency with which forecasts are produced varies considerably not only between the various supply chain organizations but also within each of those organizations depending on the decision making processes they serve. Retail inventory replenishments, for example, rely upon frequent short term forecasts, whereas aggregate sales planning may take place quarterly. This leads to another hierarchical feature of supply chains which is temporal, rather than cross-sectional, in nature.The objective of every supply chain should be to maximize the overall value generated (Chopra & Meindl, 2012). The value (also known as supply chain surplus) a supply chain generates is the difference between what the final product is worth to the customer and the costs the supply chain incurs in filling the customers’ requests. (These costs may be purely organizational or may include environmental costs as well). Such costs are an increasing function of the uncertainty associated with the demand and thus supply chain forecasting plays a major role in increasing the overall value.Given the length of a supply chain which may be considerable, spanning various supplying levels, as well as the nature of supply chain decisions, that typically are hierarchical in nature, we argue that supply chain forecasting does not merely relate to specific techniques but rather to approaches and strategies that may capture these very supply chain characteristics. The purpose of this paper is not to provide the state of the art of forecasting methods22For a comprehensive review of forecasting developments in the field of Operational Research, interested readers are referred to the work of Fildes et al. (2008). Syntetos, Boylan and Disney (2009a) provide a review of inventory forecasting methods in the preceding 50 years, and Boylan and Syntetos (2010) review forecasting methods in an intermittent demand (spare parts) context., unless the development of such methods originates in a supply chain context. Rather we emphasize forecasting strategies and approaches that stem directly from the structure and nature of supply chains. Is there a supply chain feature that gave birth to a particular forecasting method or makes a forecasting approach different in that context of application? If the answer is yes, then the relevant methods and approaches are discussed. If the answer is no, then references are given to other recent review and state of the art papers for interested readers to follow. The same principle applies also to reviewing the measurement of forecast performance. Forecast accuracy metrics and related advances in research are not covered in this paper as such metrics have not been developed from a supply chain perspective. However, we do refer to accuracy implication metrics (i.e. assessment of the utility rather than accuracy of the forecasts) as such metrics are of direct relevance to a supply chain setting.At this point we should also mention that although the term ‘demand’ is often used in this paper when referring to forecasting, typically demand will not be known and actual sales are being used as an approximation. The terms ‘demand’ and ‘sales’ are used interchangeably in this paper, although strictly speaking the latter is often used as an approximation for the former.In the next sub-section we discuss the main features of supply chains relevant to forecasting and we construct a framework to facilitate the conceptual positioning of various studies in the area of supply chain forecasting. We then discuss the methodological approach taken in this paper towards the organization of the material and the structure and content of this work and we close with an indication of the organization of the remainder of the paper.The longer supply chains are, and the more organizations they involve, the more difficult it becomes to coordinate them. The collaborative practices within supply chains vary considerably. There are three key features that have implications for supply chain forecasting:•Under certain conditions, the variance of demand is amplified as it progresses upstream, making it more difficult to forecast accurately;There are potential gains in forecast accuracy which may be achieved by different forms of collaboration, including sharing of demand information between different levels of the supply chain;The practice of collaboration has resulted in some major initiatives like Collaborative Planning, Forecasting and Replenishment (CPFR) and Vendor Managed Inventory (VMI) systems that have had important implications for the practice of supply chain forecasting.In Fig. 1we present graphically the first two issues discussed above. Please note that within each echelon (i.e. Manufacturer, Wholesaler and Retailer) there may be many such organisations. For example, a manufacturer's supply chain may involve many suppliers and retailers, and a retailer's supply chain may involve many manufacturers and suppliers.Supply chain forecasting is a hierarchical process informing various levels of decision making. It spans from inventory control at the individual stock keeping unit (SKU) level to strategic planning at a highly aggregate level. A common implicit assumption in most of the operational research and operations management literature is that information is available at the required level of decision making; for example, lead time demand across all customers for a particular SKU in inventory control. However, such information is typically accumulated from order line transaction data or, indeed, it may be the outcome of distilling relevant information from higher order data. In general, the compatibility between the forecast output that is required at a certain decision making level and the input data that form the time series being extrapolated has received limited attention in the forecasting literature. Such hierarchical connections dominate real world practices and have significant implications for many aspects of supply chain decision making, from operational to strategic. Reconciliation of forecasts is an area of great importance and an integral part of Sales and Operations Planning (S&OP) processes.There are some key features of supply chains that naturally lead to hierarchical structures:•Products: Supply chains contain thousands of SKUs and decisions may be required, for example, at the individual SKU level in inventory control, at a product family level in Master Production Scheduling, or across all SKUs in aggregate capacity planning.Suppliers: Supply chains involve the supply of goods from a range of suppliers that may be as near as next door to their clients or as far away as the other side of the world. Similar geographical dispersion applies to distribution centres. Emphasis on specific suppliers may relate to the development of collaboration strategies and demand information sharing; or companies may be interested in a group of suppliers, that are geographically located close to each other, as part of a lead time reduction strategy from a specific part of the world; or they may be interested in all suppliers when it comes to a major decision on a new IT platform.Customers: Supply chains involve servicing a large number of industrial customers or final consumers that, further, will also be typically geographically dispersed. Companies will often be interested in the needs of a specific (big) customer in terms of prioritisation of activities, or in a group of customers that are geographically clustered together for transportation purposes, or in all customers for aggregate sales planning. Customers are frequently grouped into markets, which are groups of customers that share specific features. For instance, one may group a few key accounts into single-customer groups of their own, other large customers into one group, and medium to small customers in yet another group. Such a grouping into “markets” may reflect different go-to-market strategies or different marketing mixes, and forecasts may be desired at the aggregate market level.Locations: Suppliers and customers have in common that they may be geographically dispersed. In addition, a company that is forecasting for its supply chain will itself often have multiple locations to plan for, e.g., different factories and/or distribution centres.In addition to the hierarchical effects discussed above, time may also lead to hierarchical structures in supply chains for the following reasons:•Time buckets: The time buckets in which demand data are collected (that may also determine the forecast frequency) are rarely consistent across the companies forming a supply chain. This is not only a forecasting matter but also a reflection of the operational differences between the various stages of the supply chain. One stage might schedule activities by the day, another week by week and a plant might plan production monthly. The time buckets simply reflect the impact of these fundamental operational differences that make it very difficult to introduce the same bucket length for all stages;Forecast horizons: The forecast horizons involved in supply chain problems are very different. Inventory control, for example, necessitates forecasts over a lead time (or a lead time plus the review period, for periodic inventory control applications). In the presence of logistical constraints or complex ordering cost structures, which may offer opportunities for cost reductions by batching orders, forecasts may be needed for horizons longer than the lead time. On the other hand, annual sales planning necessitates forecasts over a financial year;History of the data: Despite the recent advancements in IT, companies do not necessarily store long demand histories (and, depending on the circumstances, long histories may become obsolete quickly). Aggregation of demand data across (similar) SKUs may help to identify seasonal patterns, or other components that cannot be ‘seen’ at the individual SKU level due to the shortness of data. In addition, and as will be discussed later in the paper, the history of data available may also determine whether certain forms of temporal aggregation (non-overlapping) are feasible or not;Frequency of demand: Intermittence is a fundamental concept in supply chain forecasting. Sporadic demand characterises many supply chains and many forecasting methods have been proposed to handle such demands. These methods have not necessarily originated in the supply chain literature, but their properties do depend on supply chain features, as discussed later in this paper. As previously discussed, in this work we do not focus on methods but rather on approaches and strategies, and such approaches will also be discussed for intermittent demand items;Points in time: For intermittent demand forecasting, not necessarily all points in time (all time buckets) are equally important for extrapolation purposes. Since replenishments are most often (but not always) triggered by a demand occurrence, the forecasts produced at the end of the demand occurring periods (i.e. when an issue occurs – issue points) are those that determine the inventory implications of using a particular estimator. The difference between issue points only and all points in time is an issue that is rooted in the supply chain context.In the upper part of Fig. 2we present the hierarchical elements discussed in Sections 1.2 and 1.3 along three main dimensions: ‘location’, ‘products’ and ‘time’.Thus far it has been argued that the hierarchical elements and the length of supply chains (the stages that they encompass) constitute their main features and determine the needs of supply chain forecasting.The framework presented in Fig. 2 is informed by the requirements of supply chain forecasting and reflects its main dimensions (length – and the information and materials flow, and depth – and the pertinent hierarchical elements, including time). It will facilitate the organisation of this paper and the conceptual positioning of the studies we review.The framework offers a four-dimensional structure within which supply chain forecasting hierarchies may be positioned. We claim that it is the simplest structure which can attain such positioning. The echelon dimension is necessary for any consideration of forecasting that relates to inventory management. The location dimension is also relevant to inventory management and is essential for any consideration of forecasts that inform transport planning. It is also crucial for inventory management/warehouse location decisions, as well as the decision to allocate given areas to different warehouses. The product dimension relates to inventory management, transport planning and also warehouse planning (e.g. where to locate products within a warehouse). Finally, the time dimension is essential for all forecasting problems, not just those that relate to supply chain forecasting.The structure of this paper and its content were determined based on a unique (to the best of our knowledge) approach that involved the contribution of the forecasting research community towards deciding on the thematic coverage of this work. We have tried to consult as many colleagues as possible and organize the paper around the areas that the forecasting community feels collectively are important and worth discussing in such a review paper rather than reflect our own background and particular perspectives.To that end, a questionnaire, the aim of which was to identify the most important areas that should be addressed in this paper, was prepared and distributed through two main routes:1.First a hard copy of the questionnaire was distributed to the participants of the 26th European Conference on Operational Research (EURO), 1-4 July, 2013. The questionnaire may be found inAppendix Aat the end of the paper.Second, an announcement was made through the ORACLE (the news-magazine of the International Institute of Forecasters, IIF) inviting colleagues to electronically fill in the same questionnaire through SurveyMonkey.All responses (in both cases) were anonymous, unless the participants wished to indicate their name, in which case they participated in follow up discussions during the preparation of the manuscript. There were 43 completed questionnaires that were collected through the process discussed above, all of which were considered when structuring and writing the paper. The detailed analysis of the questionnaires can be found inAppendix B. (Any limitations in the paper remain of course attributable only to the authors of this work). The questionnaire results showed the following research areas as being the most important over the past 20 years: (i) collaboration, (ii) methods, and (iii) planning. Those respondents who mentioned topics linked to ‘planning’ used terms such as ‘MRP’, ‘Lean’ and ‘Lifecycle Management’. Whilst these subjects are undoubtedly important, we considered them more relevant to ‘supply chain planning’ than ‘supply chain forecasting’ and so did not pursue them further. The theme of ‘collaboration’ was strongly linked to the term ‘information sharing’ and articles relating to collaboration and information sharing in forecasting are reviewed in Section 2 of this paper. The theme of ‘methods’ is very broad and includes such terms as ‘ARIMA’, ‘causal models’, ‘exponential smoothing’, and ‘intermittent demand’. We have examined each of these topics in this review but found very little on causal methods in the context of supply chain forecasting and so this is picked up in the section on ‘gaps in research’.The remainder of our paper is organised as follows: in the next three sections the main theoretical developments in the area of supply chain forecasting are reviewed. The organization of the material follows the theoretical framework developed above to reflect the main dimensions of supply chain forecasting: length (discussed in Section 2), depth (discussed in Section 3) and time (discussed in Section 4). Each section concludes with a synthesis of the main gaps between the theory and practice of supply chain forecasting for the relevant dimension. Important analytical results in these areas are summarized and presented in three self-contained appendices at the end of the paper (Appendix C, DandEfor the issues of length, depth and time respectively). The summarization of those results and the provision of a single point of reference to facilitate their collective retrieval is viewed as one of the contributions of this work. Section 5 addresses the role of judgment in supply chain forecasting. In Section 6 we review data and software related issues in the area of supply chain forecasting. Finally, in Section 7, we present a number of challenges for theorists, practitioners and software developers, and offer the conclusions of this work along with a number of suggestions for further research.As a supply chain lengthens, there are more stages at which orders are placed. The original consumer demand is translated into an order from the retailer to replenish its stock; this depletes the stock at the next stage (e.g. wholesaler), which necessitates an order on the next stage (e.g. manufacturer), and so on until the end of the chain is reached.The term “Bullwhip Effect” was coined by Lee, Padmanabhan, and Whang (1997) who observed amplification of demand variance through the stages of Procter and Gamble's supply chain for their product “Pampers” (babies’ diapers). Lee et al. (1997) identified four causes of the Bullwhip Effect: demand signal processing, rationing and shortage gaming, batch ordering and price fluctuations. The effects of various forecasting methods on the magnitude of the Bullwhip effect were recently considered by Najafi and Zanjirani Farahani (2014).In this section, we review research on the propagation of demand through the supply chain, and analytical results on the amplification of demand variance. We then proceeed to the evaluation of Information Sharing as an approach to mitigating the Bullwhip Effect. Information sharing of the original consumer demand has been made possible by advances in Information Technnology and by the adoption of such approaches as Collaborative Planning Forecasting and Replenishment (CPFR). There has been an ongoing debate in the academic literature on whether Information Sharing is necessary to counter the Bullwhip Effect. Some of the authors of this review paper have contributed towards this debate. Inevitably, this has coloured the line taken in this review, but we have endeavoured to give a fair summary of the views of those who disagree with us.Demand patterns may be described as stochastic processes, which represent the evolution of demand over time. For example, demand may be modelled as an Auto-Regressive Integrated Moving Average (ARIMA) process. One of the simplest ARIMA processes is the AR(1) model, which captures first-order auto-regression of a demand series. This is an idealized demand process, but there is some empirical evidence that such processes are adequate models for many demand series that are observed in practice. For example, Ali, Boylan, and Syntetos (2012) found that 30% of the SKUs they investigated had demand that could be represented by AR(1) processes.Lee, So, and Tang (2000) analysed the propagation of AR(1) demand for a two-stage supply chain, with one member at each stage of the chain. The downstream member places orders on the upstream member according to an Order-Up-To (OUT) inventory rule. The order-up-to-level is reviewed at the end of each period and amended in accordance with the revised forecasts. Lee et al. (2000) showed that an AR(1) demand process at the downstream member translates, using an OUT system, to an ARMA(1,1) order process on the upstream member. Their analysis of variances over lead-time identified three possibilities: (i) if there is positive auto-correlation in the demand, then the variance of the orders is greater than the variance of the demand itself, and a Bullwhip Effect is observed; (ii) if there is no auto-correlation, then there is no Bullwhip Effect; (iii) if there is negative auto-correlation then there is an Anti-Bullwhip Effect, whereby the orders, over lead-time, are less variable than the demands over lead-time.Lee et al.’s work on the propagation of demand through the supply chain, for an OUT inventory system, was later extended by Gilbert (2005) to a general ARIMA(p, d, q) process. Gilbert (2005) showed that if downstream demand is of ARIMA form then, for an OUT system and an optimal (Minimum Mean Square Error) forecasting method, upstream demand will also be ARIMA. The upstream ARIMA process has the same orders of auto-regression and differencing as the downstream demand but has moving average terms of ordermax(p+d,q−L)for lead-time L. This establishes the dependence of the MA component of the ARIMA order process on the lead-time, as well as the demand process at the downstream member.In practice, optimal forecasting methods are not always employed in supply chains. Often, simpler methods such as Simple Moving Averages (SMA) or Single Exponential Smoothing (SES) are used instead. Alwan, Liu, and Yao (2003) assumed that demand followed an AR(1) process, but that forecasting is performed using SMA or SES. They proved that, for Simple Moving Averages of length n, an AR(1) demand process with an OUT inventory system translates to an ARMA(1, n) process. For Single Exponential Smoothing, with smoothing parameter α, Alwan et al. (2003) proved that an AR(1) demand process with an OUT inventory system translates to an ARMA(1, ∞) process.Ali and Boylan (2012) generalized the results of Alwan et al. (2003), for AR(1), to an ARIMA(p, d, q) demand process. They showed that, for an OUT system using Simple Moving Averages of length n, such a demand process translates to anARIMA(p,d,q+n)order process. The auto-regressive parameters remain unchanged; expressions for the upstream moving average parameters were also given by Ali and Boylan (2012).Ali and Boylan (2012) generalized Alwan et al.’s results for Single Exponential Smoothing (SES). Instead of using an ‘infinite representation’ of SES (where the summation of weighted terms extends back to an infinite history), the authors adopted a more realistic finite representation, where the summation stops at t (the length of the data history). In this case, an ARIMA(p, d, q) demand process translates, approximately, to anARIMA(p,d,t−1)process.In summary, considerable progress has been made over the last fifteen years in analysing the translation of demand processes through the supply chain, for an OUT inventory system. Complete representations have been found for ARIMA(p, d, q) demand processes, both for optimal and non-optimal (SMA or SES) forecasting methods. These results are summarised inAppendix C. Ali (2008) showed how these results may be readily extended to multi-stage supply chains.There are two restrictive assumptions that have been made in the research reviewed above: (i) there is only one member at each stage of the chain; and (ii) an OUT inventory system is employed. To make progress with the first issue, greater understanding is needed of cross-sectional aggregation of demand processes. This will be discussed in the next section of this review paper. To advance our understanding of the second issue, some simulation studies have been conducted on other inventory systems such as “order point, order-up-to-level”; see, for example, Syntetos, Georgantzas, Boylan, and Dangerfield (2011). However, analytical results have not yet been obtained for such systems. A further limitation of the literature on demand propagation is the lack of empirical studies. There have been no studies, of which we are aware, comparing the upstream demand processes identified though standard criteria (e.g., AIC) with those expected from analytical results.Lee et al. (2000) went further than simply analysing the propagation of demand processes through a supply chain. They also advocated “information sharing” of demand in supply chains, whereby the upstream member is informed not only of the order quantity at time t, Yt, but also of the demand on the downstream member, Dtat time t. This enables the upstream member to forecast the downstream member's demand directly, and eliminates reliance on the orders themselves. This would be beneficial, in terms of forecast accuracy, if the variance of demand is less than the variance of orders, as it is (for AR(1) demand) when there is positive auto-correlation in the demand series. This benefit in forecast accuracy flows through to inventory benefits at the upstream member of the supply chain.There is some empirical research which supports Lee et al.’s conclusions, which were based on a theoretical analysis. Hosoda, Naim, Disney, and Potter (2008) analyzed a real-world two-echelon (retailer-supplier) retail supply chain (focusing on a number of products that operate under an everyday low price strategy) and explored the benefit of the retailer sharing real Electronic Point Of Sales (EPOS) data with its supplier. The benefit for the supplier was assessed through the standard deviation of its forecast errors, which are linearly related to the inventory costs when safety stocks are determined based on the newsboy formulation. Standard deviation forecast error reductions between 8% and 19% were reported when exploiting the shared EPOS data. In addition, inventory benefits of demand information sharing have also been found for a multi-retailer, single manufacturer supply chain by Raghunathan (2003) (correlated demand between retailers) and by Cheng and Wu (2005) (uncorrelated demand).Although these research results are encouraging, it must be acknowledged that empirical case-study evidence is scarce. This paucity of evidence is partly due to the slow take-up of Demand Information Sharing in practice and partly due to the lack of published evidence on it (although there is some case-study material, e.g., the Barilla case, Hammond, 1994). Holweg, Disney, Holmstrom, and Smaros (2005) looked at several implementations of Collaborative Planning Forecasting & Replenishment (CPFR) and Vendor Managed Inventory (VMI) schemes across industries and countries, and concluded that the slow progress in adopting these strategies may be due to a lack of common understanding of such collaborative concepts, in addition to the difficulty of integrating external collaboration with internal production and inventory control. Some of the characteristics required for successful initiatives were highlighted by Disney, Holmstrom, Kaipia, and Towill (2001).Demand Information Sharing is valuable if it is possible to replace the task of forecasting a more variable process (the orders) with the task of forecasting a less variable process (the demand). This is the essence of the argument advanced by Lee et al. (2000) to support an Information Sharing strategy.Lee et al. (2000) noted the following relationship, for AR(1) demand process of the formDt=τ+ρDt−1and an OUT system, between the order process Ytand the demand process Dt:(1)Yt=Dt+ρ(1−ρL+1)1−ρ(Dt−Dt−1)where ρ is the autoregressive parameter for the the AR(1) demand process and L is the lead-time.They comment that, when the upstream member knows the parameters associated with the demand process Dt, the upstream member can utilize this equation to estimate the actual value of Dt. However, the authors proceed by assuming that the upstream member would not utilize this equation to infer the actual value of Dt.Ragunathan (2001) develops this argument further. He re-writes the above equation by expressing Dtin terms of YtandDt−1and uses this recurive relationship to obtain:(2)Dt=1−ρ1−ρL+2∑i=0t−1(ρ−ρL+21−ρL+2)iYt−i+(ρ−ρL+21−ρL+2)tD0By substitution of Eq. (2) into Eq. (1), he obtains a recursive relationship for Yt:(3)Yt=1−ρL+21−ρτ+ρL+2(1−ρ)1−ρL+2∑i=0t−1(ρ−ρL+21−ρL+2)iYt−i+ρL+2(ρ−ρL+21−ρL+2)tD0+1−ρL+21−ρɛt+1Therefore, Ytcan be estimated by the upstream member, based on the first two terms on the right hand side of (3), as the term including D0 converges to zero, andɛt+1, unknown to all supply chain members, is replaced by its expected value of zero. Of course, this depends on the upstream member having knowledge of the downstream member's demand stream parameters τ and ρ.The variance of the expression (3) converges to the variance from Demand Information Sharing as t approaches infinity. Raghunathan (2001) presents simulation results to show that the value of information sharing converges to zero rapidly. Hence, he concludes that there is no need to share demand information.The above discussion is limited to AR(1) demand processes. However, analyses of ARMA(p,q) and ARIMA(p,d,q) processes by Zhang (2004) and Gilbert (2005), respectively, also concluded that inference of demand renders Demand Information Sharing redundant.All of the papers reviewed in this section are based on the assumption of demand processes with unchanging parameters. There have been no analytical studies, to the best of our knowledge, which address models with time-varying parameters.Ali and Boylan (2012) revisited the issue of Downstream Demand Inference by questioning the assumption that the downstream member would share, for each individual Stock Keeping Unit, the specific ARIMA process (e.g., ARIMA(0,1,1)) and the specific ARIMA parameters but not share the demand value itself. This had been the common assumption made by Lee et al. (2000), Raghunathan (2001), Zhang (2004) and Gilbert (2005). Ali and Boylan argued that it is unlikely that supply chain partners would invest in a formal information sharing mechanism just to share the information on demand process and parameters and not the actual value of demand itself.Ali and Boylan (2011) proved two ‘Feasibility Principles’, based on the assumption that an upstream member is not aware of the demand process or parameters at the downstream member. These principles apply for an ARIMA demand process with optimal (Minimum Mean Squared Error) forecasts.Principle I: If the upstream member can infer the demand process at the downstream member, the demand values at the downstream member cannot be exactly calculated.Principle II: If the upstream member cannot infer the demand process at the downstream member, then it may be possible for demand values at the downstream member to be calculated if a certain demand process is assumed from a restricted subset of the possible processes.Taken together, the two principles imply that it is not possible for the upstream member to infer the demand process and the demand values at the downstream member.Ali and Boylan (2012) also analyzed the feasibility of inferring demand processes and demand values for ARIMA demand and non-optimal forecasting methods. For Single Exponential Smoothing, such inference is not possible. However, Ali and Boylan (2012) showed that this inference is possible for Simple Moving Averages, if the length of the Simple Moving Average is known. In this case, there is a one-one correspondence between demand and order processes (no change to p and d, and the upstream value of q is found by adding n – the length of the moving average – to the downstream value of q).Research into the upstream propagation of demand processes, downstream demand inference and the benefits of demand (forecast) information sharing for supply chain forecasting currently flourishes and, as discussed in this section, many important developments have been made in this area. However, research has been predominantly theoretical in nature and subject to very specific assumptions and demand process formulations. Given the importance of this area for real world practices and the (slowly) increasing implementation of collaborative approaches to supply chain forecasting and decision making, further research into the following areas would appear to be merited.So far, academic investigations have looked only at OUT systems (see, e.g., Ali & Boylan, 2010). An exception is the study conducted by Syntetos et al. (2011), although that work relied upon simulation (system dynamics) rather than mathematical analysis. Only OUT systems have been used so far in terms of theory development and despite the fact that such systems are often employed for stock control purposes, there are certainly many other alternatives, the analysis of which should offer valuable insights for practitioners. A further restriction is that demand processes are generally assumed to be known and stationary.There has been no analysis in this area for slow or intermittently moving items. Such an investigation would necessitate an Integer ARMA (INARMA) rather than ARIMA underlying framework. The relationship between these two frameworks is considered in some detail in Section 4 of the paper.There is a need for some rigorous quantification of Information Sharing benefits in terms of forecast accuracy (and inventory reduction) benefits. Some real world case evidence has been presented but there is a need for more rigorous analysis which would isolate the effect of Information Sharing from other improvements in inventory management. Those who have advocated Downstream Demand Inference (DDI) as an alternative to Information Sharing have yet to offer any case studies of successful implementation of DDI.Demand for products (or service parts) is realized at an individual order line level, meaning that a particular business customer or end consumer requests a certain number of items for a particular SKU at a particular point in time. Demand is then naturally aggregated by the supplier along three key dimensions: locations, products and time, to inform decision making at various organizational and functional levels. This forms the basis of forecasting and planning, i.e. the extrapolation of some specific level of requirements into various time horizons.In practice, different decisions require forecasts at different levels of aggregation. For example: budgeting requires annual demand forecasts across all SKUs and customers; inventory control requires lead time demand forecasts per SKU and location; contractual arrangements and price negotiation rely upon annual forecasts across all relevant SKUs per customer; distribution management and scheduling require weekly or monthly demand forecasts across SKUs per customer/location – or aggregation across customers in a geographical area. A common implicit assumption in most of the forecasting and operational research and (operations management) literature is that we utilize data that are as aggregate or disaggregate as our required forecasting output. However, the degree of aggregation of the forecasting output does not necessarily need to match with the existing data structure (which may be more aggregate or more disaggregate than the forecasts driving decision making). The degree of aggregation of the forecasting output (i.e. the forecast we use to make decisions) is actually a function of the decision making problem forecasting it tries to support. This should be taken as a given for the forecasting process and algorithms. On the contrary, inputs to the forecasting process are very often driven by existing data structures defined by the software solutions being employed and relevant organizational practices. Although the two may indeed match sometimes, this is not necessarily the case. A further challenge arises when we wish to use the same forecast for multiple purposes. In this situation, there is a need to design a forecasting process that addresses the multiple needs of the organization, and which produces forecasts that are consistent across different levels of a hierarchy. These themes are discussed further in Sections 3.3 and 3.4.Apart from the cases when demand inputs are at the required output aggregation level, the following three scenarios will typically occur:•Forecasts are required at some higher level than the demand input data, for one of the three key dimensions: locations33As discussed in the main body of the paper, we consider locations along with customers. A retailer may be one single customer for a CPG manufacturer, but each one of the retailer's distribution centers may need a separate forecast. DCs may differ because of regional particularities or because of operational conditions (e.g., only a few DCs could have the equipment to deal with frozen goods, or a few DCs may be dedicated to deal with promotional quantities)., products and time. In this case, either forecasts are produced at the input level and then are aggregated to the required output level or demand is aggregated at the required output level and then extrapolation takes place at the level under concern.Forecasts are required at some lower level than the demand input data, for one of the three key dimensions: locations, products and time. In this case, either forecasts are produced at the input level and then disaggregated to the required output level or demand is disaggregated at the required output level and then extrapolation takes place at the level under concern.Forecasts are required at some lower (higher) level than the demand input data for some dimension(s) but at a higher (lower) level for some other dimension(s).Although these hierarchical connections dominate real world practices and have significant implications for all levels of decision making, from tactical to operational to strategic, very little guidance may be found in the academic literature in this area. In addition, software providers are also currently in great need to further support the design of their demand management solutions with conclusive empirical findings and insights (Singh, 2013). Studying the effects of different forms of aggregation (location, product and time), and their combination, in relation to relevant decision making levels, in a systematic format that allows linkages to be identified and generic insights to emerge, is yet to be conducted in the academic literature.For the remainder of this section we emphasize single forms of aggregation across products and locations. The issue of temporal aggregation is discussed separately in Section 4. Although originating in fields other than supply-chain forecasting (mainly in economics and time-series studies), hierarchical forecasting very naturally reflects important supply-chain characteristics and offers ample scope for the introduction of innovative forecasting methodologies. First, we consider the problem of forecasting seasonal demand and present some important developments in this area that rely upon product cross-sectional aggregation practices. Then we review generic developments in product and customer (location) cross-sectional aggregation theory, followed by a discussion on forecast reconciliation, which is an important requirement of S&OP. We close with remaining gaps in the area of forecasting by cross-sectional aggregation.Demand uncertainty is among the most important challenges facing modern supply chains. In response, companies often invest in new forecasting software or consider implementation of new forecasting methods. However, there is another option: utilizing the hierarchical characteristics of data. Doing so does not necessarily require changing the suite of methods available for the forecasting task. Hierarchies offer an important forecasting opportunity, enabling logical techniques of aggregation and disaggregation. Cross-sectional aggregation in particular sums demands across items or customers (locations) and all items are reported for the same time periods. Useful information may then be extracted from the aggregate series that would otherwise be potentially lost at the individual SKU level due to the shortness of data.Consider the problem of forecasting seasonal (trended or untrended) demand for example from short demand histories. Accurately estimating seasonality often requires lengthy time series. (Hyndman & Kostenko, 2007, discuss minimum sample size requirements for common seasonal forecasting models). However, if we cross-sectionally aggregate demand data to a group (say, product family) level, we gain the longer data histories typically associated with that family, making it easier to determine the seasonal pattern of demand. For instance, different brands of ice cream will have a similar seasonality with a summer peak, which may not be easily detected for low-volume flavors but can be estimated at a group level and applied on the product level. Similarly, one can consider grouping locations instead of (or in addition to) products: retail stores in a given region or even across multiple regions may have similar patterns, e.g., all along the seashore. However, care must be taken, as appropriate groupings may differ among locations and groups: seashore and ski resort stores will exhibit very dissimilar seasonal patterns of sunscreen lotion sales.Traditionally, the seasonal component is estimated from an item's own data history, resulting in the determination of individual seasonal indices (ISI). However, when the demand history is short (and the data are very noisy), ISI will not provide accurate forecasts. In some cases, such as when there are only two (or less) years of data available, ISI may be worse than non-seasonal methods even when the data are seasonal (Chen & Boylan, 2008). A possible answer is to use the knowledge of demand at some group level to improve forecasts at the individual level. This approach is usually referred to as group seasonal indices (GSI). With GSI, instead of finding seasonal indices at the item level, demand is aggregated across a group of items that are thought to share a common seasonal pattern and seasonal indices are found at the group level. These indices are then applied to the individual product level in order to extrapolate future individual product demands. There are two approaches for obtaining group seasonal indices:1.Sum demand across the product group and then estimate the seasonal indices from this aggregate series. Such an approach was proposed by Withycombe (1989) (WGSI).Calculate individual seasonal indices (ISI) for each item in the group but then average these ISIs across the whole group and use this average for every single item in the group (Dalhart, 1974) (DGSI). A weighted average also could be used, with faster moving products having a greater weight.From a practical perspective, it is then important to decide whether to:1.Use the ISI approach for all series (traditionally what companies do);Use the GSI approach for all series and, if so, whether to use DGSI or WGSI;Choose, for each series, whether to use ISI or GSI, again deciding between DGSI and WGSI.The third strategy was investigated by Chen and Boylan (2007) for untrended data, who found that the choice between ISI and WGSI depends on the Coefficient of Variation (CV) of the deseasonalized series. Specifically, WGSI is more accurate than ISI if CVINDIVIDUAL> CVAGGREGATE. So the condition for WGSI being more accurate is that the coefficient of variation of the deseasonalised individual series (CVINDIVIDUAL) must be greater than the coefficient of variation of the deseasonalised aggregate series (CVAGGREGATE). [A complete set of rules indicates when it is best to apply the ISI, WGSI and DGSI methods (Chen & Boylan, 2007).] Chen and Boylan (2008) showed that the use of the rule can result in accuracy benefits on real data.Dekker, van Donselaar, and Ouwehand (2004) and Ouwehand, van Donselaar, and de Kok (2005) proposed an aggregation approach for forecasting seasonal trended data. Level and trend are updated at individual product level. Seasonal indices are estimated from the aggregated demand data of all products within a product family. These seasonal indices are then used, in conjunction with the item level and trend estimates, to forecast at the individual product level using the Holt-Winters method.A key assumption in the application of all grouping methods is that a grouping mechanism is available. How the group is formed though can have a large influence on the forecast accuracy results (Ouwehand et al., 2005). Some practitioners would prefer to use their organization's standard product groups, which of course lend themselves to easy aggregation from a database and ERP standpoint. However, a group of items that may be homogenous in product characteristics, brand or any other feature chosen to define a product hierarchy may not necessarily have the same seasonal patterns (Zotteri, Kalchschmidt, & Caniato, 2005). Therefore, it may be advantageous to use different groupings for seasonal forecasting. Boylan, Chen, Mohammadipour, and Syntetos (2014) examined theoretically, and by means of experimentation with empirical data, how groups should be formed in order to use the GSI methods discussed above. They proposed a K-means clustering method which was found to constitute a viable alternative to a company's own grouping method or that may be used with confidence if a company lacks a grouping method. For a further discussion on recent developments on the application of product-group seasonal indices to seasonal products please refer to Mohammadipour, Boylan, and Syntetos (2012). The issue of seasonal grouping is one which would appear to merit further theoretical and empirical investigations. Finally, we note that the issues identified here also apply to grouping approaches to estimating other quantities of interest where a single product's time series may not yield enough information such as promotion effects, price elasticity, effects of calendar events, lifecycle patterns, and cross-elasticities with other products.In this section we consider some important developments in hierarchical product aggregation (Section 3.3.1) and hierarchical customer/location based decisions (Section 3.3.2). Both Sections 3.3.1 and 3.3.2 only consider aggregation within a specific hierarchy. Although the different hierarchies do not differ intrinsically as to how they should be treated in aggregation, we do distinguish between product and customer/location related studies. From an inventory perspective, the aggregation of requirements across a group of products ordered from the same supplier is a very natural thing to do and such a process is supported by inventory software packages. Similarly, the aggregation of forthcoming requirements from a specific customer, or the aggregation of demand across customers in a specific region is important for marketing and sales initiatives and is supported by customer relationship management (CRM) packages. This is a very significant qualification to be made since, and as will be discussed in the next section of the paper, temporal aggregation approaches are not typically supported by software solutions.Two approaches have dominated the literature on hierarchical aggregation forecasting, namely bottom-up and top-down.•The bottom-up approach involves forecasting at the individual SKU level (lowest level of the hierarchy) and then, if needed, aggregating such forecasts to obtain an estimate at some higher level of the hierarchy (see Hyndman, Ahmed, Athanasopoulos, & Shang, 2011). If a forecast is required at the individual SKU level only, then aggregation becomes redundant.The top-down approach involves forecasting at an aggregate level and then, if needed, disaggregating the forecasts to lower hierarchical levels (see, e.g., Boylan, 2010). For example, one could multiply the aggregate demand forecast by the (forecast of the) ratio of the respective individual item demand to aggregate demand (called proration in the literature, see, e.g., Fliedner, 1999; Strijbosch, Heuts, & Moors, 2008) in order to estimate individual SKU requirements. Gross and Sohl (1990) demonstrated, in a large empirical study, that simple sample averages for the proration factor are very effective44Gross and Sohl (1990) found that two simple disaggregations may be particularly effective: (i) For each period, calculate the proportion that an individual series contributes towards the aggregate series. Then average the proportions over all periods; (ii) Calculate the total, over all periods, for the indivudual series, and then do the same for the aggregate series. Then calculate the ratio of the former to the latter. Athanasopoulos et al. (2009) refer to the first method as the average historical proportions and the second as the proportions of the historical averages. For the first approach, they point out that we need not restrict our methods historical proportions. Forecasted proporions can be used instead. Boylan (2010) noted that although this has not been tested on supply chain data, the use of forecasted rather than historical proportions appears to be promising and is a feature typically offered in forecasting software.. Obviously, if a forecast is required at the aggregate level only, then disaggregation becomes redundant.A third variant, middle-out, essentially combines these two approaches by forecasting on a middle level in the hierarchy and then aggregating bottom-up, as well as disaggregating top-down.Discussions on the comparative performance of top-down and bottom-up approaches have been present in the literature for a long time (not necessarily in a supply chain context only) and, to a certain extent, they are still inconclusive. An important determinant of such comparative performance is the hierarchical level (aggregate or subaggregate) for which a forecast is needed. The middle-out approach is usually subsumed under the two other approaches – considered from “below”, middle-out is top-down, while considered from “above”, middle-out is bottom-up.Some scholars advocate the top down approach (Barnea & Lakonishok, 1980; Gross & Sohl, 1990; Fliedner, 1999), whereas others disagree (Dangerfield & Morris, 1992; Gordon, Dangerfield, & Morris, 1997; Weatherford, Kimes, & Scott, 2001) and, among other reasons, argue that the outperformance of bottom-up is due to the fact that information loss is substantial when aggregating time series; that is, the data is too aggregated to represent diverse demands55Zotteri et al. (2014) argue that the best level of aggregation for forecasting should be chosen by the forecasters in consideration of the trade-off between sampling error (data inadequate to generate reliable forecasts) and specification error (data too aggregated to represent diverse demand).. Some additional arguments that are often put forward by practitioners against top-down approaches have been summarised by Dawson (2013):•Individual products may be in different classes;The parameters used for disaggregation (usually historical percentages of items’ contribution to the group) are subject to sampling variability;It is not possible to estimate the standard deviation at the individual level from the standard deviation at the aggregate level.Nevertheless, there are instances in supply chain forecasting when the top-down approach is clearly more appropriate than bottom-up methods. Boylan (2010) argues that if there is a change in policy, for example, with regards to pack sizes, then aggregate data (in suitable units of measurement) is a better guide to the future than previous sales of a particular pack size. In this case, the aggregate forecast may require disaggregation using judgmental estimates (rather than sample derived proportions as discussed above) on the effect of introducing new pack sizes. In addition, and as discussed in the previous sub-section, longer data histories make model identification and component estimation more reliable at the aggregate level. However, it is true that if forecasts are required at the individual SKU level and thus we need to disaggregate, strong assumptions are being made that the lower level series are following the same pattern(s) as the aggregate one, and those assumptions may be incorrect.An alternative line of attack is to take a contingent approach and analyze the conditions under which one method produces more accurate forecasts than another (e.g., Shlifer & Wolff, 1979; Lutkepohl, 1984; Viswanathan, Widiarta, & Pilpani, 2008; Rostami-Tabar, Babai, Ducq, & Syntetos, 2016). Widiarta, Viswanathan, and Piplani (2008) evaluated top-down versus bottom-up forecasting in a production planning context for the purpose of estimating SKU level requirements. In their work, aggregate demand series were assumed to be composed of several correlated sub-aggregate components. Each such component was assumed to follow a stationary (first order univariate moving average) time series process, which is correlated over time. Finally, as is common in a production-planning environment, it was assumed that simple exponential smoothing is used to extrapolate requirements under both methods. The researchers showed that the performance of the two methods is nearly identical, regardless of the correlation coefficient between the item demands, the items’ proportion in the family demand and the coefficient of the serial correlation term of the demand process.In a later work, and under the same assumptions, Widiarta, Viswanathan, and Piplani (2009) examined analytically the relative effectiveness of top-down and bottom-up approaches for forecasting aggregate rather than SKU level demand. The authors concluded that: “there is no difference in the relative performance when the time series for all of the sub-aggregate components follow a first-order univariate moving average process with identical coefficients of the serial correlation term (p. 87)”. They also performed a simulation study to examine the case of differing coefficients of the serial correlation term among the sub-aggregate components. It was found that performance differences “…are relatively insignificant when the correlation between the sub-aggregate components is small or moderate (p. 87).”Sbrana and Silvestrini (2013) extended the work of Widiarta et al. (2009) by employing an unrestricted multivariate framework allowing for interdependency between its variables. They showed theoretically (and by means of simulation) that the previously identified similar performance of bottom-up and top-down holds even when the moving average parameters of the individual components are not identical. In addition, they showed that the relative forecasting accuracy of the two methods depends on the parametric structure of the underlying framework.More recently, Rostami-Tabar et al. (2016) analyzed theoretically and by means of simulation on theoretically generated data the relative performance of top-down and bottom-up for forecasting both aggregate and SKU level demand. The latter was assumed to follow a non-stationary ARIMA (0,1,1) demand process and exponential smoothing (which is optimal for this demand process) was assumed to be employed for forecasting purposes. An important finding was that the forecast accuracy improvements achieved by bottom-up and top-down for non-stationary demands are higher than those associated with stationary cases. The theoretical findings were validated through empirical analysis on data from a European superstore; this is an important aspect of the study under concern since no other empirical evidence has been put forward in the literature in this area.As previously noted in this sub-section, discussions remain largely inconclusive as to which method (top-down or bottom-up) performs better under which situation (forecasting at the aggregate or disaggregate level) and expanding the empirical knowledge base in this area would be of a great benefit for real world practices. Similarly, and since hierarchical forecasting is a complicated procedure, analytical results are hard to obtain and currently there is scope for more theoretical contributions in this area (Strijbosch et al., 2008; Strijbosch & Moors, 2010).Hierarchical customer or location-based forecasting does not differ from product hierarchical forecasting in terms of the main issues determining the comparative performance of the bottom-up and top-down approach. Performance appears to be conditioned on the forecasting methods being used, the modelling assumptions being made (for analytical type work), the desired level of aggregation of the output of the forecasting process, and the criteria employed for constructing a ‘family’ of components (however these components are defined: customers, locations).Fliedner and Lawrence (1995) suggested that the method to identify group candidates is not very important. They observed that sophisticated clustering techniques used to identify family members did not lead to better forecast performance than when families were randomly determined. However, Zotteri et al. (2005) showed that what is driving forecast accuracy is the choice of the appropriate level of aggregation (e.g. store level, distribution center level or supply chain level). Forecasters do not actually choose the level of aggregation of the output of the forecasting process. But this does not mean that they have no discretion as to the choice of the proper level of aggregation for producing the forecasts. Forecasting with more aggregated data than required lowers sampling error but raises specification error (Zotteri, Kalchschmidt, & Saccani, 2014). The former stems from using data that is inadequate for estimating the demand pattern, either in terms of quantity (limited size of the actual sample) and / or quality (one-off events that may distort the picture of what is happening in terms of demand). The latter refers to pooling data from different sources, leading essentially to combining apples and oranges. For example, different supermarkets have different intra-day seasonality of customer traffic, so aggregating data across stores may lead to an overestimation of demand in one store at a particular time while underestimating demand at another time.On the contrary, using more granular data than the process requires lowers specification error, but increases sampling error. The best forecasting strategy then is the one that considers the trade-offs between sampling and specification error and minimizes the sum of these errors. Zotteri and Kalchschmidt (2007) studied the problem of aggregation across locations by evaluating these components of the forecasting error under the assumption of stationary non-correlated demand (over time and across products and markets). They concluded that the appropriate level of aggregation depends on the size and homogeneity of the supply chain and suggested metrics that one may adopt to support the choice of the appropriate forecasting process, thus providing help to managers in defining the proper level of aggregation for a specific situation. Further, they suggested that it would be interesting to model clustering of stores as an intermediate alternative between a very detailed and a very aggregate model like the one considered in their research (Caniato, Kalchschmidt, Ronchi, Verganti, & Zotteri, 2005). This would help companies to identify the level of aggregation that better solves the trade-off between specification and sampling error. This would probably entail the modeling of both intra-cluster and inter-cluster heterogeneity.There have been only a few studies considering the effects of hierarchical location-based decision making despite the importance of such decisions for supply chain management. The trade-off between sampling and specification error should govern the choice of the level of aggregation to produce forecasts, and more empirical insights into how this trade-off may be best determined would be very valuable both from an academic and practitioner perspective.The previous subsections addressed three “standard” ways of forecasting hierarchical time series: bottom-up, top-down and middle-out. One completely different and promising approach to hierarchical forecasting has recently been proposed (Athanasopoulos, Ahmed, & Hyndman, 2009; Hyndman et al., 2011; Hyndman & Athanasopoulos, 2014). In this optimal combination approach, one first forecasts all time series in the hierarchy separately and independently, which yields base forecasts that will usually not satisfy the summation constraints given by the hierarchy. These separate base forecasts are then combined using a linear transformation, which can encode information about covariances between the time series, if available. The final forecasts are then, by definition, hierarchically consistent.This approach has a number of conceptual and practical advantages over bottom-up, top-down and middle-out approaches:•The entire base forecast vector is included in the reconciliation, not only one particular level in the hierarchy. Therefore, one can naturally use different forecasting techniques and model different dynamics for different hierarchical nodes.This property of the optimal combination approach may explain why it has been found to improve forecasts across the entire hierarchy, in contrast to bottom-up aggregation, which often is found to be better at lower levels but worse at higher ones, and to top-down disaggregation, which often yields good forecasts at higher and worse ones at lower levels.This approach naturally allows combining multiple hierarchies, such as product, geographic or customer hierarchies. In principle, bottom-up can do so, too, by forecasting the most fine-grained time series, e.g., sales of a single SKU to a single customer in a single location – but such time series will frequently be intermittent and hard to forecast. Conversely, it is unclear how top-down and middle-out approaches could deal with multiple hierarchies without some sort of reconciliation at all, since a priori, the top level forecasts in the different hierarchies are likely to be different.The optimal combination forecast also easily addresses weighted summation. For instance, one could include sales prices in the summation matrix to forecast revenues, or purchasing prices, to forecast cash requirements.However, some open issues remain;•The modified bottom level forecasts can in principle end up negative, which may be the optimal mathematical solution to the generalized least squares (GLS) problem, but is likely to be nonsensical in a supply chain setting without returns. To avoid this, one can use non-negativity constraints on the problem.Realistic hierarchies can involve thousands of products and hundreds of locations. The corresponding GLS may be too large to solve without special numerical methods. Hyndman et al. (2011), Section 5, discuss some potential remedies, and Hyndman, Lee, and Wang (2014a) have recently proposed a fast recursive approach for specific hierarchy topologies.Further, and in relation to the point raised above, forecasts from all levels need to be produced (as opposed to a particular level which is the case for either bottom-up or top-down) and this introduces an extra level of complexity when applied in practice.Finally, the fact that forecasts from all levels need to be taken into account when producing the final (reconciled) forecasts may also prove to be problematic when judgmental adjustments are performed at only one level, say the most aggregated or the most disaggregated one. Linking judgmental interventions to hierarchical forecasting is an issue that requires further research and this is discussed in more detail later on in the paper.The optimal combination approach has been implemented in the hts package (Hyndman, Lee, & Wang, 2014b) for the statistical computing environment R. However, is not currently supported by standard inventory (forecasting) software packages whereas both top-down and bottom-up approaches are. It is important to note that this interesting approach is operationally very complex but consistent with current practices of many managers and thus might fit very well with the mind-set of many potential users.Except for Fliedner and Lawrence (1995) and the literature referred to in Section 3.2 (in the context of determining groups for optimal determination of seasonal and other group-based indices), the formation of groups within a hierarchy in order to maximize the accuracy gains from hierarchical forecasting has received very little attention in either the academic or the practitioner literature. Indeed, practitioners usually rely on predefined hierarchies based on non-forecasting considerations, which may or may not make sense for hierarchical forecasting. Given the intuitive appeal of grouping methods for hierarchical forecasting, and the promising empirical results that have been presented in this area, more research into the formation of hierarchical groups would appear to be merited.The previous subsections addressed hierarchically consistent point forecasts, which will of course be the point of departure for any decisions made in the supply chain. However, no supply chain can function without buffer and safety stocks, which are set based on future expected volatilities of demands, i.e., prediction intervals or density forecasts, which should of course incorporate the covariance structure of the reconciled hierarchical forecasts. Unfortunately, there appears to be very little work investigating density forecasts in an aggregation or supply chain context, except for the very specific cases discussed in Section 2.1 andAppendix C.This entire section has for its overarching theme the reconciliation of cross-sectional hierarchical forecasts. As discussed, there has been much research on possible reconciliation techniques from a theoretical and mathematical point of view. However, in practice, often divergent forecasts originate in different business units in a firm or from different nodes in a supply chain, which then need to be reconciled in a Sales & Operations Planning (S&OP) process (Grimson & Pyke, 2007). Rarely will all participants agree on a single algorithm to reconcile forecasts. More likely, group-based reconciliation will take place, at least for more important sub-hierarchies. In this context, the group dynamics of real-world decision makers reconciling forecasts, e.g., in a Delphi environment, become relevant – and there does not seem to be much research on this topic (de Almeida, Marins, Salgado, Santos, & da Silva, 2015), in contrast to a very active research community working on single-person cognitive biases in forecasting. Interpersonal, rather than statistical, reconciliation processes were only very recently suggested in the literature (Spithourakis, Petropoulos, Nikolopoulos, & Assimakopoulos, 2015) and more research into their potential effectiveness should be of great value to real world practices. [At this point it is important to note that a recent special issue of the International Journal of Forecasting (2011, volume 27) on group judgmental forecasting does not focus on supply chains.] In particular, the effects of automatic reconciliation (optimal, top-down, bottom-up, middle-out) are yet to be understood when incorporating judgment at various levels of the hierarchy.The Auto-Regressive Integrated Moving Average (ARIMA) framework of analysis has been most useful for research in supply chain forecasting; all the developments described in Section 2 (Length) and many of the developments discussed in Section 3 (Depth) (and later on in this current Section, and the relevant Appendices), assume that the underlying demand structure may be represented by an ARIMA form. However, when the demand is intermittent (count data) then Integer ARMA (INARMA) models need to be considered. Such models have had applications is different areas such as medical science and economics but unfortunately have not been extensively considered in a supply chain context.Both ARIMA and INARMA frameworks insightfully distinguish between stochastic underlying patterns. However, they do not readily lend themselves to practical forecasting applications and it is true to say that neither framework is being widely used in practice for forecasting purposes. Consequently, although these frameworks may facilitate modelling of demand series, they would appear to be less helpful in terms of actual extrapolation needs.Alternatively, exponential smoothing methods have been given a statistical foundation by the use of state space models with a single source of error (SSOE) (for the linear form of which equivalent ARIMA models exist). State space models distinguish between an observation (actual demand) and system / state (underlying structure) equation, each being associated with an error (Gaussian white noise process) term. Whereas the multiple source of error (MSOE) formulation of these models assumes distinct error terms (for the observation and system equation) with no cross-correlation, in the SSOE models the same error term appears in both equations. This has been claimed to be preferable because it allows the state space formulation of non-linear as well as linear cases, and, in more practical terms, it also allows the state equations to be expressed in a form which coincides with the error-correction form of the usual smoothing equations. That is, the SSOE approach enables one to see directly the relevance of exponential smoothing methods. Nevertheless, it has been criticized as not being a model in an Operational Research sense (Johnston, 2000), since it works backwards from the method to the model, rather than the other way around which is what we strive to achieve. (A model should offer the opportunity to gain insights on real data to derive properties of various estimators; see also Johnston, Harrison, Marshall, & France, 1986.)Although a classification of stochastic processes is most useful for theoretical analysis, a classification of forecasting methods is more relevant to practical forecasting needs. Pegels (1969) classified exponential smoothing methods according to factors such as the inclusion of trend and seasonality. However, the classification did not include methods for intermittent demand (such as Croston's method, for example (Croston, 1972) or the Syntetos–Boylan Approximation (SBA) (Syntetos & Boylan, 2005)) even after extension by Gardner (2006). If we move sufficiently low with regards to the level of granularity of demand data we, unavoidably, end up with intermittence. In addition, in a service parts context demand series are almost invariably intermittent in nature, calling for relevant estimators and classification schemes that distinguish between them. In a supply chain context, we need to take into account the fact that demand series are often associated with occasional demand occurrences interspersed with many zero demand observations. Thus, a solution that recognizes this very supply chain characteristic need to be adopted.Johnston and Boylan (1996) and Syntetos, Boylan, and Croston (2005) argued that previous attempts which appeared in the literature, as well as standard classification schemes employed by software packages, to distinguish between ‘fast’ and intermittent demand items are ad-hoc both in terms of the criteria employed for classification purposes but even more importantly in terms of the cut-off values assigned to such criteria.Johnston and Boylan (1996) considered an important question: what is the degree of intermittence that renders Croston's method more appropriate than SES. The researchers worked on the premise that it is preferable to compare directly estimation procedures for the purpose of establishing regions of superior performance and then categorize demand based on the results66SKU classification for (inventory) forecasting purposes typically works in the opposite way: ad hoc classification rules are used to separate SKUs into categories, followed by the specification of a forecasting method for each of the categories. But if the purpose of classification is the selection of forecasting methods, then it makes more sense to compare directly possible estimators and then categorize demand based on regions of superior forecast performance.. They compared SES and Croston based on their simulated Mean Squared Error (MSE) and under a wide range of realistic assumptions as far as the demand generation process was concerned. They came to the conclusion that Croston should be preferred to SES for inter-demand intervals greater than 1.25 forecast revision periods. The contribution of their work lies more in the identification of the inter-demand interval as a classification parameter rather than the exact value (cut-off point) assigned to it.Syntetos et al. (2005) extended the work discussed above to consider theoretically derived MSE expressions. They compared the theoretical performance of SES, Croston and SBA through their theoretical MSE over a fixed lead time of duration L. Such comparisons resulted in regions where one method performs better than others and enabled the selection of specific methods under specific demand characteristics. All three methods were assumed to be employed with a common smoothing constant α and the analysis was subject to a stationary underlying demand framework and the assumption of a Bernoulli demand arrival process.The MSE over a fixed lead time of duration L, assuming error auto-correlation, is calculated as follows (Strijbosch, Heuts, & van der Schoot, 2000, Syntetos et al., 2005):(4)MSEL.T.=L{LVar(Estimate)+LBias2+Var(ActualDemand)}It follows from Eq. (4) that the MSE over lead time L of Method A is greater than the MSE over lead time L of Method B if and only if(5)Var(Estimate)A+BiasA2>Var(Estimate)B+BiasB2where the subscripts refer to the forecasting methods employed.Considering inequality (5) it is obvious that the comparison between any two estimation procedures is only in terms of the bias and the variance of their one step-ahead estimates. The comparisons resulted in operationalized classification schemes along the lines presented in Fig. 3. In addition to the average inter-demand interval (p), the squared coefficient of variation of the demand sizes (CV2) was also identified as an important parameter to distinguish between alternative methods’ performance. The empirical utility of the schemes discussed above has been confirmed in many studies (e.g., Ghobbar & Friend, 2002, Regattieri, Gamberi, Gamberini, & Manzini, 2005, Boylan, Syntetos, & Karakostas, 2008). Further refinements of these schemes (and empirical evidence on their performance) are offered by Kostenko and Hyndman (2006) and Heinecke, Syntetos, and Wang (2013).So far, we have juxtaposed intermittent and non-intermittent demand time series. In practice, a given time series may switch between intermittency and non-intermittency. For instance, sales may be intermittent during most of the time, but turn non-intermittent during promotions, during high season (e.g., ice cream or air conditioning spare parts in summer, or heating, headlight or bumper automotive spare parts in winter), or before certain calendar events (e.g., “typical” presents like jewellery, perfume or chocolates before Valentine's day or Christmas). In such cases, detecting the regimes and appropriate change points between regimes is an important pre-processing step to apply to a time series before selecting the forecasting model. To date, there has been little research on this matter.An important question that emerges at this point is whether there may be alternative ways to deal with intermittence - other than introducing methods specifically designed for such demand patterns, like Croston or SBA, and attempting to distinguish between them. Such an approach aggregates demand in lower frequency time units (say from daily to weekly) thereby reducing (or even eliminating) the presence of zero observations. Temporal aggregation is known to generally reduce demand volatility and has been identified as one of the most important areas in a service parts forecasting context urgently requiring further research (Gardner, 2011). This is discussed in more detail in the following sub-section.Forecasting by temporal aggregation is the process of aggregating demands from higher frequency to lower-frequency time buckets - for example aggregating daily data to weekly - and using the aggregate time series to generate forecasts (Syntetos, 2014). This intuitively appealing approach will very often (but not always) reduce demand uncertainty for both fast and slow moving items. However, the benefits of temporal aggregation may not be well-understood by managers and applications are not typically supported by forecasting software – in contrast with cross-sectional aggregation that is supported by inventory and CRM packages, amongst others (please refer also to the previous section).There are two forms of temporal aggregation: non-overlapping and overlapping. The first divides the historical information into consecutive non-overlapping blocks of equal length. In overlapping aggregation, the blocks are also of equal lengths but, at each period, the oldest observation is dropped and the newest is included. In this case, the data are actually moving sub-totals of demand history. (Calendar adjustments may sometimes be required when aggregating daily or weekly data into monthly, in which case the blocks will not have the exact same length, e.g. monthly blocks will consist of 28, 29, 30 or 31 days. This may be more of a problem in overlapping rather than non-overlapping aggregation.)Non-overlapping aggregation results in a considerable reduction of the number of periods; say from 21 daily demands to 3 weekly demands. Practitioners have expressed concerns over the natural loss of information associated with temporal aggregation. However, this concern is most relevant for short demand histories (and non-overlapping aggregation). Should long demand series be available the loss of information resulting from aggregation needs to be contrasted to the potentially improved forecast accuracy associated with the uncertainty reduction.Temporal aggregation has been used extensively in engineering and physics for signal processing - mainly smoothing for noise reduction and identifying 'true' signals (see, for example, Carstensen, Telford, & Birks, 2013); it has also found popularity among scholars in financial time series analysis in the quest for more effective ways to aggregate volatility estimators (Meddahi & Renault, 2004) or time series models in general (Silvestrini & Veredas, 2008). Despite this wide attention from other academic disciplines, aggregation has never attracted considerable attention in an Operational Research / Management Science context. Although Willemain, Smart, Shocker, and DeSautels (1994) pointed towards that direction, the effects of aggregation were not considered until 2011 when Nikolopoulos et al. empirically assessed the relevant benefits in an intermittent demand forecasting context.As discussed in the previous sub-section, demand data is frequently intermittent. In this case, aggregation of demand into lower-frequency time buckets reduces the presence of zero observations – there are fewer months than weeks with zero sales for example – which can improve forecast performance. Intermittent demand items (such as spare parts) pose considerable forecasting challenges, both in terms of capturing underlying time series characteristics and fitting standard forecasting models. Temporal aggregation is known to be applied widely in practical military settings (very sparse data), the after-sales industry (service parts) and elsewhere. Nikolopoulos, Syntetos, Boylan, Petropoulos, and Assimakopoulos (2011) demonstrated the value of (non-overlapping) temporal aggregation in such a context. Although this paper was exploratory in nature, it provided some much-needed empirical evidence of the potential benefits of using temporal aggregation in a supply chain context by means of experimenting with the demand histories of 5000 stock keeping units from the Royal Air Force (UK). In addition to resulting in improved forecast accuracy, the proposed temporal aggregation forecasting framework (referred to as ADIDA: Aggregate Disaggregate Intermittent Demand Approach to forecasting) was also found to lead to stock control performance improvements (Babai, Ali, & Nikolopoulos, 2012)77The fact that a particular forecasting method or approach may perform better than one other in terms of forecast accuracy does not necessarily imply that such benefits carry also over to the implications (utility) of those methods for the decision making process they serve. Put simply, in an inventory control context for example, the fact that estimator X performs better than estimator Y in terms of forecast accuracy (however this is measured) does not mean that estimator X leads to a better trade-off between customer service levels and inventory investment, which is was matters from a practitioner perspective. As such it has become apparent that we need to distinguish between accuracy and accuracy-implication (utility) metrics (Boylan and Syntetos, 2006; Syntetos et al., 2010).and to lend itself to application in non-intermittent demand contexts as well.Spithourakis, Petropoulos, Babai, Nikolopoulos, and Assimakopoulos (2011) provided the first results for fast-moving data by demonstrating that in the M3 competition almost all competing methods could have benefited in terms of forecasting accuracy if applied via the ADIDA temporal aggregation framework. ADIDA has been found effectively to operate as a forecasting method ‘‘self-improving’’ mechanism; changing the data series features through frequency transformation may help extrapolation methods achieve better accuracy performance. Building on those empirical results, some theoretical analysis was presented for data following an AR(1) and MA(1) process by Rostami-Tabar, Babai, Syntetos, and Ducq (2013) and a more general ARMA(1,1) process by Rostami-Tabar, Babai, Syntetos, and Ducq (2014). This analysis led to theoretically determined optimum aggregation levels for the ADIDA framework (as opposed to the empirically determined optimum levels of aggregation suggested by Nikolopoulos et al., 2011) under specific demand generation processes. Further theoretical developments for both fast and slow moving data were presented by Spithourakis, Petropoulos, Nikolopoulos, and Assimakopoulos (2014) providing theoretical bounds for the temporal aggregation process to lead to improvements in forecasting accuracy.Kourentzes, Petropoulos, and Trapero (2014) extended the fundamental idea of ADIDA by means of estimating time series structural components across multiple frequencies. Multiple time series are constructed from the original time series, using temporal aggregation, and automatically selected exponential smoothing methods are used to produce forecasts. Subsequently, the time series forecasts from each aggregation level are combined so as to construct the final forecast. Empirical evaluation of the proposed framework demonstrated significant improvements in forecasting accuracy for longer-term forecast horizons.Petropoulos and Kourentzes (2015) further extended the research discussed above by means of considering extrapolation method combinations. They empirically evaluated forecast combinations for intermittent demand data using both method and temporal combinations of forecasts. The first are based on combinations of different methods on the same time series, while the second use combinations of forecasts produced on different aggregated frequencies of the time series, based on temporal aggregation. Results indicated that appropriate combinations lead to improved forecasting performance over single methods, as well as simplifying the forecasting process by limiting the need for manual selection of methods.The work described thus far in this sub-section refers to the process of estimating mean demand. However, forecasting of the demand variance is equally important in a supply chain (inventory control) context. The effect of temporal aggregation for variance estimation remains to date a largely unexplored area with only a minimal number of contributions offered in the literature. In particular, Syntetos and Boylan (2006) suggested a cumulative (smoothed) Mean Squared Error (MSE) procedure for forecasting the variance of the lead time demand. The traditional approach to such an estimation procedure is to consider the one-step-ahead (smoothed) MSE and multiply it by the (constant) lead time, assuming independence of the forecast errors over time. However, even in the presence of stationary independent demand, the forecast errors will be auto-correlated (even though demand is not – please refer also to Eq. (4)) (Syntetos et al., 2005) and thus the traditional approach will under-estimate variability (and, further, lead to safety stocks lower than those required to meet a prescribed service target) (Syntetos, Teunter, & Prak, 2015b). Direct updating of the cumulative lead time forecast error avoids this problem, in addition to having an intuitive appeal and being easy to implement in practice. Despite a number of counter arguments to this approach [e.g., (i) different forecasting periods may be needed for different SKUs; (ii) situations with stochastic lead times cannot be accommodated] this remains the sole proposition in the literature for exploiting the concept of temporal aggregation for variance estimation. At this point it should be mentioned that matching the aggregation level to the (constant) lead time has also been found to perform well for mean demand estimation both in terms of forecast accuracy (Nikolopoulos et al., 2011) and stock control performance (Babai et al., 2012).Temporal aggregation appears to be a very promising approach, both for fast and intermittent demand items. However, there is considerable scope for further work in this area both for mean demand and demand variance estimation. The latter is a relatively unexplored area but a most important one for inventory management as safety stocks (inventory investments) link explicitly to the estimated demand variance. Further, the effects of temporal aggregation on the volatility of the series are not clearly understood. Temporal aggregation very often, but not always, reduces the coefficient of variation (CV) of demand. Theoretically identifying the cases where temporal aggregation increases volatility should be of great value for practical forecasting applications.Another interesting line of research would be identifying the extent to which temporal aggregation could help us decide what data collection frequencies and respective forecasting horizons we should be using in practice, as for example switching from observing demand weekly to bimonthly may lead to significant improvements in our forecasting capability. This is very important if we take into account that in many application areas we are interested in a cumulative forecast. This issue has been considered by Nikolopoulos et al. (2011) providing evidence that tying the forecasting horizon to the lead time could lead to considerable forecast accuracy improvement, when measuring performance at that level of aggregation (i.e. equal to the lead time) (see also Nikolopoulos & Petropoulos, 2015, for a discussion of this issue in the context of high uncertainty).A final important question, in terms of data characterization, is which demand framework we should be using in practice to distinguish between alternative demand patterns. Both ARIMA and INARMA frameworks insightfully distinguish between stochastic underlying patterns. However, they are associated with estimation procedures that are not well-used in practice. Pegels’ classification does cover methods that are being used in practice, but fails to reflect the fact that demand is very often intermittent, and thus estimators specifically designed for such demand patterns need to be considered. In that respect the classification scheme proposed by Syntetos et al. (2005) is relevant, although its construction is limited to MSE considerations and specific forecasting methods, in addition to not taking into account inventory implications. Distinguishing between alternative demand patterns is a fundamental issue in supply chain forecasting and further work in this area would be of great theoretical and practical value.Some aspects of the time dimension considered in this paper link to the potential usefulness of incorporating judgment into the forecasting process. First, it is clear that the application of statistical forecasting methods requires long demand histories. Should only short demand histories be available statistical methods cannot be implemented (or any implementation would be sub-optimal since limited data precludes the identification of the underlying structure and components of the series). Note that such long demand histories need not necessarily come from the time series to be forecasted itself. Indeed, forecasting for new products is often performed by selecting appropriate “predecessor” or “similar” products to the one to be forecasted88Related to this is the application of the hierarchical Bayes approach for short time series demonstrated by Yelland (2010) in the context of forecasting the demand for parts at Sun Microsystems. This works by pooling information across products where there is little or no time series history.. By fitting appropriate models to these longer time series of sales of “similar” products, one can forecast a new fashion product's entire lifecycle, or a new durable good's adoption curve (Goodwin, Meeran, & Dyussekeneva, 2014). A related approach is not to use similar products, but similar locations, by offering a new product initially at a small sample of test stores and extrapolating to other stores (e.g., Fisher & Rajaram, 2000), which is, however, only possible if the goods already physically exist, so requires separate production runs and can only be used for short- or medium-term forecasts.Either one of these approaches, of course, leads to the question of how to select “similar” products (locations) that are likely to have sales patterns that match the new product (location) to be forecasted. Approaches include simply using the product category, assessing product/location attributes for explanatory power using machine learning techniques, clustering locations on sales patterns, and various judgmental approaches. In any case, forecasts based solely on similar products will be highly uncertain and should be compared to and updated based on initial sales of the new product as soon as these become available.For short time series that at least allow some statistical model estimation, complementing statistical forecasts with managerial judgment may lead to improved forecast performance. (This is also very important when we have special events, like a product promotion, on the horizon and have limited data to estimate their effects on sales, or when there are changes in prices or assortments.) Such judgmental inputs may take various forms, from a simple override of the forecast parameter values, to overriding model and method selection to directly judgmentally adjusting the statistical forecasts. In this paper only the last case is considered. Although the word ‘short’ needs to be qualified (and the cut-off points between the scenarios discussed above are clearly context-based) the message we wish to convey is that the relevance of judgmental inputs in the forecasting process is very much related to the length of the historical series.Further, the comparative performance of judgmental and statistical forecasting also relates to the forecast horizon. Statistical forecasting should usually be preferred for very short time horizons. Such a choice is also necessitated if there is a high volume of short-term decisions that preclude the use of judgmental forecasts. For example, a supermarket that requires forecasts every half day in order to replenish stock for say 70,000 SKUs could not rely upon judgmental approaches (unless by exception). At the other extreme, it is clear that the longer the forecast horizon the greater the probability that the series will undergo some structural changes and (explanatory) factors that have not been considered in the past will come into play. In such cases pure judgmental forecasts should perform better. In contrast to short term forecasting, it is the nature of the decisions involved in long term forecasting as well as the likely change in demand dynamics over the forecasting horizon that calls for human input rather than reliance on a statistical model. It makes sense then that an integrated approach would be more useful for medium forecast horizons where useful information external to the statistical model may be brought by managers into the forecasting process by means of adjusting the statistical forecasts.This above discussion is graphically depicted in Fig. 4where we show the type of forecasting in relation to the demand history and forecast horizon.In this review, given the SCF context, we will discuss the seminal contributions for the middle case described in Fig. 4, focussing particularly on judgmental interventions to statistical forecasts (often referred to in the literature as judgmental adjustments or judgmental revisions). This is because the case of pure judgmental forecasting relates to single echelon tasks and it involves a plethora of studies from the field of psychology dealing with cognitive functions and biases – an area that is clearly outside the scope of this review paper. On the other hand judgmental interventions involve the use of relevant and, hopefully reliable, information coming from the whole supply chain, from shortages in supply to increases in end-product demand and consumer behavior. Such information is difficult to integrate in theoretical models and empirical evidence has been the predominant source of useful insights that have helped to develop our understanding in this area.Although judgmentally adjusting statistically derived forecasts is a very common practice in industry (Fildes & Goodwin, 2007), this is an area that has attracted relatively little attention in the academic literature. There are a few empirical studies available analyzing the effectiveness of judgmentally adjusted forecasts and their performance in comparison with the base-case (the unadjusted statistical forecasts).The first large-scale empirical evidence was provided by Mathews and Diamantopoulos with a series of contributions (1986, 1989, 1990, 1992) showing that judgmental revisions improve accuracy. Their results were verified over a sample of more than 900 products from a major UK pharmaceutical manufacturer and retailer. The first study (1986) examined the improvement of judgmental interventions over one quarter and the main result was that the revised forecasts had less variance. The longitudinal extension of this study came three years later (1989) when data and forecasts over six consecutive quarters were examined. Stronger evidence was found of improvements in the forecasting process as a result of the judgmental interventions. The third study (1990) revealed that both the magnitude and direction of the adjustments are affected by both product-specific and reviewer-specific factors, but any potential accuracy improvements can be attributed only to the latter. The final study (1992) demonstrated only marginal differences in the forecasting accuracy related to judgmentally adjusted versus non-adjusted forecasts.This stream of research was not taken up until about 15 years later by Fildes, Goodwin, Lawrence, and Nikolopoulos (2009); data were collected on more than 60,000 forecasts and judgmental adjustments from four supply-chain companies. In three of the companies using monthly data (home cleaning, pharmaceutical and food & beverages) judgmental adjustments were found, on average, to increase forecast accuracy. However, a detailed analysis revealed that, while the relatively larger adjustments tended to lead to greater average improvements in accuracy, smaller adjustments often damaged accuracy. Furthermore, positive adjustments were much less likely to improve accuracy than negative adjustments. They were also made in the wrong direction more frequently, suggesting a bias towards optimism. To take this into account some models were subsequently developed to eradicate relevant biases. In the fourth company (major UK retailer) that was adjusting forecasts at a smaller percentage but observing demand more frequently (weekly data), judgmental adjustments were actually found to decrease forecast accuracy.The richness of the data gathered through this project led for the first time to the analysis of the effects of integrating management judgment into intermittent demand forecasts (Syntetos, Nikolopoulos, Boylan, Fildes, & Goodwin, 2009b) as well as the evaluation of the utility of judgmental adjustments through inventory control (rather than) forecast accuracy metrics (Syntetos, Nikolopoulos, & Boylan, 2010). The latter study found an impressive amplification effect: a small improvement in forecasting accuracy via judgmental adjustments may result in far more significant inventory investment savings or increased customer service levels.At more or less the same time, an equally important database was constructed by Franses and Legerstee containing SKU level statistical and judgmentally adjusted forecasts from the pharmaceutical industry, across thirty-five (35) countries and seven (7) distinct product categories. This led to a series of publications (Franses & Legerstee, 2009, 2010, 2011a, 2011b, 2013) all dealing with different aspects of that database and testing hypotheses similar to those assessed by Fildes et al. (2009). An important conclusion (contradictory to the finding by Fildes et al., 2009) was reached by Franses and Legerstee (2010) that judgmentally adjusted forecasts are, at best, equally good to pure statistical forecasts, but most often are worse, with the profound deficiency being attributed to experts' tendency to put too much weight on their own contribution. More recently Petropoulos, Fildes, and Goodwin (2015) looked at the same database examining some behavioral aspects and provided evidence that experts change their behavior when performing judgmental adjustments to statistical forecasts after ‘big losses’ (judgmental adjustments that significantly decrease forecast accuracy).Acknowledging the small number of empirical studies in this field, it is surprising to note the almost complete absence of formal OR modelling in order to offer insights into the impact of such judgmental adjustments on supply chain performance and the potential mitigation mechanisms necessary. We could identify only one study by Syntetos et al. (2011) that employed a System Dynamics modelling approach to theoretically evaluate the effects of forecasting and order replenishment adjustments for a wide range of scenarios, involving three different inventory policies, seven different (combinations of) supply chain points of intervention and four different (combinations of) types of judgmental intervention (optimistic and pessimistic).It is evident, given the small number of empirical and theoretical studies as well as the inconclusive empirical results, that there is still considerable space for empirical investigations and more importantly theoretical and methodological contributions towards identifying the specific conditions under which judgmental adjustments do lead to improvements in forecast accuracy and forecast utility. Another promising direction for further research is the assessment of the implications of superimposing judgmental adjustments on other layers of the supply chain and/or the decision making spectrum and the resulting interactions between such judgmental interventions. (An example is the very recent investigation of the effects of integrating management judgment into stock replenishment decisions by Syntetos, Kholidasari, & Naim, 2015a.)There is also little evidence on the effectiveness of judgmental interventions in relation to the rationale behind such interventions. There is not even a single study to date that links the adjustments to some justification behind these adjustments99There are also very few studies on behavioural models that may adjust forecasts for weather conditions (short term weather estimates) (e.g. Nikolopoulos and Fildes, 2013) which is a very important issue for many manufacturers.. Further, our understanding on the effects of successive judgmental interventions is very limited. How do people adjust forecasts when they know that these forecasts have already been subject to modification by someone else in the supply chain? How do judgmentally adjusted forecasts interact with judgmental adjustments further down the decision making process? For example, practitioners confirm that replenishment orders, transportation routes, production schedules and so on, all of which rely upon a (possibly judgmentally adjusted) forecast, are frequently subject to additional judgmental modifications.As described in the previous sections, there have been major advances on the algorithmic and statistical sides of the problem of forecasting in a supply chain context. Nevertheless, major problems remain from a software and especially from a data perspective.Academic research has in recent years devoted more and more attention to investigating desirable features of Forecasting Support Systems (FSS; Fildes & Goodwin, 2013). Apart from general User Experience (UX)/User Interface (UI) requirements, specific FSS features need to take known cognitive and behavioral biases in (frequently non-expert) forecasters into account (Fildes et al., 2009) and should include, displaying statistical and business information about a time series to be forecast, allowing for judgmental adjustments, providing for or even requiring the user to give reasons for adjustments, tracking, monitoring and evaluating adjustments, and enabling users to share information within the system (Spithourakis et al., 2015). Unfortunately, we know of no commercially available FSS in this sense. In our experience, most forecasters employ a mixture of tools, using Microsoft Excel or a dedicated forecasting tool for forecasting, capturing forecasts again in Excel or in the company ERP and overwriting forecasts based on judgment again in Excel, with no provision for tracking reasons for adjustment or evaluating whether adjustments in fact improved the forecast.There is a gap to be bridged between statistical (and behavioral) theory and operational and economic practice. Software developers play a key role in addressing this gap. However, it must be kept in mind that developers usually receive just as little training in statistics or forecasting as statisticians and forecasters receive in software development (cf. the well-known statistical/numerical weaknesses in Microsoft Excel, McCullough & Heiser, 2008). Modern software development is a vast, complex and rapidly changing field – just as statistics. It makes little sense to train software developers in rudimentary statistics, nor to train statisticians in rudimentary software development practices, to have them write forecasting software that will, necessarily, perform rudimentarily. Instead, what is needed is a close cross-functional collaboration between software developers and statisticians/forecasters, all of whom speak each others’ languages to an extent that allows them to create working forecasting software or full FSS. Such a close collaboration will be easier in developing standard software, where economies of scale allow retaining statistical specialists, than in custom development, where statistical knowledge can, at best, be included only on an ad hoc basis for cost reasons.The recent trend towards “Big Data”, which is facilitated by new developments in database technology (Januschowski, Kolassa, Lorenz, & Schwarz, 2013), has major implications for forecasting in supply chains. For instance, data are kept at finer and finer granularities – whereas in the past, retailers would store aggregated sales on an SKU × store × week basis, today they commonly keep far more disaggregated data, at least on a daily level. Increasingly, data are also stored disaggregated per offer (“on day t, we sold 8 units at regular price, 6 units on a buy-one-get-one offer available with a newspaper coupon, and 14 units at 30% off available to shopper card holders”), or even on transaction log (TLOG) or basket level data. Highly granular data raise the question of what aggregation level to forecast on, as detailed in previous sections, and frequently lead to the problem of forecasting intermittent time series.A related dynamic is the increased availability of information linking specific purchases to specific customers. This has of course always been possible for mail order and online retailers (where online retailers also know the customer's browsing history on their site, or even on other sites via cookies, while mail order retailers are of course limited to knowing what customers actually ordered in the end), but the increasing ubiquity of shopper cards makes this information available to an extent to traditional brick-and-mortar retailers as well. Retailers have mainly used this information in data mining and association analyses to produce personalized recommendations and discount coupons (Humby, Hunt, & Phillips, 2008), but it may be possible to also leverage it in forecasting.At the same time, established retailers face increased competition from new entrants into the retail market, be it pure online players or new entrants into a geographical market, like German discounters taking market share from the leading UK grocer Tesco. In addition, consumer products manufacturers, especially in the fashion sector, increasingly integrate vertically in the supply chain by opening their own web or brick-and-mortar stores, competing directly with established retailers. Incumbents frequently react by increasing the scope and complexity of promotions they offer, which in turn increases the complexity of the data they have and of the forecasting task they face (see above).In addition, participants in the supply chain increasingly aim at leveraging social media information in forecasts. Positive or negative publicity can have a major impact on customers’ demand, but including this information in forecasts is so far an unsolved problem at the intersection of forecasting algorithmics, textual analysis and data management.The common theme of these developments is that more and more data of higher and higher complexity is in principle available to be leveraged in supply chain forecasting. However, established Enterprise Resource Planning (ERP) systems typically spread these data over many disparate tables and/or separate systems. One apparently banal but nevertheless important challenge for software providers thus is to access and combine all relevant data – including the use of APIs and feeds for external data, e.g., from social media sites – and to ensure data consistency. This is not a forecasting issue properly speaking, but a data management problem which needs to be solved before causal forecasting methods can even be brought to bear.A related problem lies in the quality of the data available. For instance, DeHoratius and Raman (2008) found that 65% out of nearly 370,000 inventory records examined at a retailer's 37 stores were inaccurate. Possible reasons for this level of data inaccuracy include inexact deliveries, undocumented goods movements between the backroom and the shop floor, theft, untracked spoilage and customers or store associates shifting product between locations in the store. This particular data quality problem has two consequences in the present context of forecasting in supply chains: first, forecasts will be biased low by undetected out-of-stocks. And second, even good forecasts will lead to incorrect reordering decisions and suboptimal inventories if paired with incorrect system inventories. Other forecast-relevant data issues include miscoded promotions, incorrect prices, or untracked special influences like unscheduled store closings or employee strikes. Thus, data quality problems limit the impact of improved forecasting algorithms or processes such as information sharing on improving inventory positions, and data quality is one key issue that needs to be addressed by retailers.In this section, we have focused on the retailer as the supply chain node which is closest to the end customer. Of course, these dynamics propagate to higher levels in supply chain via information and materials flows (Fig. 1), and also by consumer product manufacturers expanding into the retail sector. Thus, the issues identified above also apply to higher levels in the supply chain.Overall, the move towards more and more data counteracts the trend towards faster and faster implementation of analytics and forecasting software (Moore's law): retailers’ expectations outstrip software vendors’ capabilities (Parkinson's law). Consequently, the quest for improved performance of databases and algorithm implementations is not likely to end any time soon.The paper has discussed thus far a number of gaps between forecasting theory and practice. Academics are often accused of not researching issues of practical relevance, resulting in forecasting theory lagging behind real world practices. Although this is often true, the converse is also frequently the case. Practitioners and software packages do not often employ methods or approaches that have been shown (in academic studies) to be both effective and easy to implement, thus leading to real world practices lagging behind theoretical developments.Failed academic attempts to serve real world applications may stem from the fact that the issues being tackled are simply of no practical importance at all. In addition, though, the extremely fast developments and changes in the area of supply chain forecasting render relevant theoretical developments a very difficult exercise as the ‘problem setting’ keeps on evolving at a very high speed. For example, the introduction of multichannel operations discussed in the previous section introduces a very challenging interface between the dimensions of length and depth. Indeed, our own theoretical framework may be subject to modifications as shown below (please see Fig. 5).Traditional upstream propagation of demand does not reflect the fact that consumers may in fact order directly at levels further up than the retailer. Consider for example the case that the consumers may order directly at the wholesaling or manufacturing level that also faces demand as propagated through the supply chain. This would naturally generate the need to look at a different form of aggregation; one that combines (sums) the propagated demand up to a particular supply chain level and the direct orders received from consumers at that level. Exactly the same concept applies (perhaps even more often) to orders placed by the retailers directly on the manufacturers.However, and as discussed above, practitioners (and software manufacturers) also contribute to the gap between theory and practice by not employing methods and approaches that would make a considerable difference in their operations. Some indicative examples are the following:•The classification scheme(s) developed by Syntetos et al. (2005) have been shown in a number of studies to offer considerable benefits in terms of forecasting and stock control and they have indeed been successfully implemented in practice, unfortunately in very few cases (e.g. by JDA software). Similarly, and although not explicitly considered in this paper, the SBA estimator has repeatedly been shown to offer considerable forecast accuracy improvements and to be a very robust estimator, and yet it is being used in only a few real world applications we are aware of (e.g. Syncron).The literature on judgmental adjustments of statistical forecasts has shown that (large) negative adjustments tend to improve forecast accuracy, whereas (large) positive adjustments result in an opposite effect. Judgmental adjustments of statistical forecasts take place routinely in industry, and yet companies are not introducing authorization mechanisms according to which negative adjustments can be freely conducted but positive adjustments require explicit justification or authorization.Although further research is needed in order to fully appreciate the benefits (and drawbacks) of temporal aggregation it is clear that at the very minimum it is worthwhile to experiment with its implications for decision making. Yet, software packages do not support temporal aggregation.Cross sectional aggregation for seasonal (trended or untrended) forecasting is both intuitively appealing and has been shown to offer considerable benefits when demand histories are short. However, not many companies utilize such an approach to estimating seasonality.Before we close this section we should mention that the emphasis of our paper has been on classical time series approaches; causal methods have not been covered. With the recent ‘hype’ around Big Data, many practitioners have been trying to leverage causal effects such as weather and social media activity. Forecasting solutions are increasingly expected to use Twitter, Facebook and other external leading indicators leading to what may be called ‘predictive analytics’. Although there does not seem to be a clear-cut separation between ‘forecasting’ and ‘predictive analytics’ we take the view that the latter does not usually utilize time series by themselves. Further, identifying the extent to which explanatory variables may indeed help the forecasting task has not received considerable attention in the supply chain context and arguably there has not been much written on the state-of-the-art of using such (relevant) explanatory variables. There is a developing literature in this area in the field of Marketing Analytics but such developments have not been matched in the supply chain discipline. Integration of Marketing and SCM related work to comprehensively establish ‘what can be actually done’ in the field of analytics, coupled with the industry progressing beyond this current ‘hype’ phase to determine ‘what needs to be done’ (other than unrealistically expecting everything to be linked to everything) should be of great value towards further bridging the gap between forecasting theory and practice.In this sub-section we suggest a number of important issues where there is a need for further experimentation and theoretical research.How to make effective use of information contained in social media? Getting extremely noisy external causal factors right (particularly social media) currently presents practitioners and software developers with a number of challenges. In addition, unifying different data sources, and then making sense of them seems also to be an area requiring further attention.How to forecast discrete predictive densities? Most theoretical results, as well as most forecasting software packages, make heavy use of the normal distribution. This is defensible in high count time series, in which actual count data can reasonably be approximated by normal distributions. However, exploding data storage and analysis capacities (“Big Data”) are driving a trend towards storing and forecasting data in finer and finer levels of granularity, so that the case for the normal approximation becomes weaker and weaker. To appropriately set safety stocks for highly granular count data, we need to understand discrete predictive distributions. There has been remarkably little work in this regard, and research in this direction is urgently needed.How to structure hierarchies? In any forecasting process, the first step is the identification of the outputs to be forecast. When we require a demand forecast, we must specify the aggregation level needed. In turn, this requires identifying the:•Product: SKU, item, family, etc.Market / Location: customer, POS, region, country, business area, etc.Time: hour, day, week, month, year, etc.The forecasts needed to support the decision-making process may be aggregate in one or some of those dimensions and granular in others. For example, we may require product forecasts by SKU but market forecasts only by region. How to structure the hierarchy to best forecast this type of hybrid decision-making problem is a topic about which much remains to be learned.What are the current limitations of forecasting software? A recent collection of articles in a Forecasting Guide published by Foresight: The International Journal of Applied Forecasting comes to the clear conclusion that software limitations are impeding progress in hierarchical forecasting. From an inventory perspective, aggregating requirements across a group of products ordered from the same supplier is a very natural thing to do, and such a process is supported by inventory software packages. Similarly, the aggregation of forthcoming requirements from a specific customer or the aggregation of demand in a specific region is highly relevant to marketing and sales initiatives and is supported by customer relationship management (CRM) packages. However, and as discussed in this paper, this is not the case for temporal aggregation. Nor is there support for non-traditional reconciliation approaches, those that rely on combining forecasts. Expanded functionality here should not require considerable interventions. Indeed, Section 4.2 discusses a multiple aggregation prediction algorithm (MAPA, Kourentzes et al., 2014; Petropoulos and Kourentzes, 2014) that has been made available in the open-source statistical software package R and provides a template for commercial-software implementation.How to forecast lead-time demand variance? There is an important issue in inventory forecasting that has not yet been sufficiently addressed. Here, we need forecasts not only of mean demand, but variances as well (to set safety stocks). For instance, even if we were to restrict ourselves to a temporal hierarchy, which of the following is the better route to obtain an estimate of the variance of lead-time demand: (i) to forecast the demand variances for each of the next months and add these over the lead time, or (ii) to extrapolate directly the cumulative lead-time demand variance (assuming constant lead times)? No clear answer is available here because the issue has not been adequately studied. We need to extend the experiments with temporal aggregation to cover demand-variance estimation.A related area of neglect is the potential for aggregation techniques to provide superior estimates of quantile or density forecasts and other parameters of forecast distributions in the context of hierarchies. Quantile forecasts, prediction intervals and density forecasts have proven very difficult to obtain for single time series. Aggregation may help or hinder us here, but there has been remarkably little work to show how.How to combine temporal and cross-sectional hierarchies? Our review has addressed both cross-sectional and temporal hierarchies, but as separable structures. In reality, companies must make decisions in both dimensions. A unified approach – one that simultaneously considers choices of aggregation levels and frequency along multiple dimensions – would be a valuable step in the right direction. The problem with the separation of the cross-sectional and temporal dimensions is that the right level of cross-sectional aggregation may vary across time frequencies and vice versa. Procedures that combine forecasts for a cross-sectional hierarchy, such as the optimal reconciliation technique discussed in Section 3.4, and procedures that combine forecasts over time frequencies, such as the multiple temporal-aggregation technique discussed in Section 4.2, may conceivably be pooled to form a holistic – or, idiomatically speaking, whole-istic – strategy for forecasting hierarchies.How to reconcile to achieve consensus? Instead of using statistical procedures to reconcile forecasts across a hierarchy, users can choose to create an interpersonal reconciliation process. As proposed in a recent study by Spithourakis et al. (2015), the process may nurture consensus among forecasters and make managers at different levels of a hierarchy interdependent. This too is an area where research is merited. (Some interesting issues in this area are discussed by Fisher, Hammond, Obermeyer, & Raman, 1994).How to forecast supply? This paper has focused on forecasting demand, but there are also conditions under which we need to forecast supply (as well as possibly other variables such as stock actually available in the store or in the warehouse, that is affected by errors as shown by DeHoratius & Raman, 2008). For instance, suppliers may not be perfectly dependable, delivering either too late or less than was ordered. In such a case, we need safety stocks not only to buffer against demand volatility, but also against supply volatility. Similarly, many companies operate so-called reverse logistics; that is, they acquire used materials from later stages in the supply chain and remanufacture them. Examples encompass beverage companies receiving empty bottles in deposit schemes, or recyclable materials being collected. In either case, supply is not deterministic and needs to be forecasted. There is a lack of research on specific challenges in this area, in particular on optimal inventory policies in the presence of uncertainty in both supply and demand.Forecasting Support Systems (FSS). As noted above, there appear to be no commercial FSSs worthy of the name available for productive forecasting. On the one hand, this seems to be an opportunity for software vendors. On the other hand, this very lack calls for an explanation, and market research as to why none of the major forecasting software providers has stepped up to provide a fully-fledged FSS would be an interesting topic for academic researchers.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
