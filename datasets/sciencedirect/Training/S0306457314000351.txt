@&#MAIN-TITLE@&#
Open domain question answering using Wikipedia-based knowledge model

@&#HIGHLIGHTS@&#
The use of Wikipedia as a knowledge source for question answering system.Wikipedia article content & structure, infobox, category, and definition are used.Each knowledge source has its unique strength for certain question types.Answer merging strategy for multiple answer matching modules.

@&#KEYPHRASES@&#
Question-answering,Wikipedia,Semi-structured knowledge,

@&#ABSTRACT@&#
This paper describes the use of Wikipedia as a rich knowledge source for a question answering (QA) system. We suggest multiple answer matching modules based on different types of semi-structured knowledge sources of Wikipedia, including article content, infoboxes, article structure, category structure, and definitions. These semi-structured knowledge sources each have their unique strengths in finding answers for specific question types, such as infoboxes for factoid questions, category structure for list questions, and definitions for descriptive questions. The answers extracted from multiple modules are merged using an answer merging strategy that reflects the specialized nature of the answer matching modules. Through an experiment, our system showed promising results, with a precision of 87.1%, a recall of 52.7%, and an F-measure of 65.6%, all of which are much higher than the results of a simple text analysis based system.

@&#INTRODUCTION@&#
The goal of a question answering (QA) system is to directly return answers, rather than documents containing answers, in response to a natural language question. The answers can be fact-based short answers, lists of instances, or descriptions about a particular topic. Many of the initial efforts in QA research, ignited by the QA track in TREC (Dang, Kelly & Lin, 2007; Voorhees, 2004), have focused on mining unstructured texts such as news sites and blogs. However, these systems show a relatively low performance, with at most 71% accuracy for factoid questions, 48% F-score for list questions, and 33% F-score for descriptive questions. On the other hand, specialized QA systems have relied on well-structured knowledge bases in specific domains (Demner-Fushman & Lin, 2007; Frank et al., 2007). Although these works achieved high accuracy, building large-scale, well-structured knowledge bases for a general domain QA is a very expensive task.Wikipedia is a semi-structured and wide covering, rapidly growing knowledge source that has been built through a collaborative effort of volunteers. Wikipedia has become a stable and sufficiently large knowledge source for many knowledge-based engineering works (Bizer et al., 2009; Hoffart et al., 2013; Nastase & Strube, 2008; Suchanek et al., 2007). Furthermore, Wikipedia was applied to QA systems as a knowledge base (Ahn et al., 2004; Buscaldi & Rosso, 2006; Simmons, 2012). However, these systems utilized only parts of Wikipedia information. Thus, we developed an open-domain QA system that fully utilizes semi-structured Wikipedia knowledge model. The knowledge model can serve as certified information sources and enable a QA system to generate correct answers in a general domain. We exploit the category structure, article structure, infoboxes, definitions, redirection, and article contents of Wikipedia as knowledge sources for a QA system. We assume each knowledge source has its own strengths for answering different types of answer formats such as factoid, list, and description. For example, an infobox is effective in answering factoid questions, and the category structure is effective in answering lists of questions.Well-organized knowledge bases do not guarantee high performance if the questions are in natural language instead of formal query. Mapping linguistic expression in questions to knowledge representation in knowledge-base is another hard task if we build full-featured knowledge from Wikipedia like YAGO (Hoffart et al., 2013). F-scores of QA systems in QALD task are about 50%, which are much lower than expected result (http://greententacle.techfak.uni-bielefeld.de/~cunger/qald/). The task covers extracting answers from well-organized knowledge-bases for given natural language questions. So, our system is based on a conventional QA system, which consists of a question analysis module, document retrieval module, and answer matching module. To this end, the different types of knowledge sources are converted into text documents for the document retrieval module. Instead, specialized answer matching modules are developed for the knowledge types.Section 2 describes the question analysis, while Section 3 describes our Wikipedia QA system. Section 4 describes the experiment used, and concluding remarks are given in Section 5.To make full use of knowledge sources of Wikipedia for many types of questions, it is critical to analyze user questions in terms of the nature of the answers being sought. The availability of a question categorization scheme will help not only in analyzing an incoming user question but also in identifying QA capabilities and techniques to be developed in the future (Oh et al., 2011). To this end, we collected 600 questions from a commercial Korean website, Naver™ Manual QA Service (http://kin.naver.com). The questions were analyzed to characterize the types of questions and answers. The results of our analysis are shown in Table 1for surface-level question type classes determined based on interrogative pronouns. These results are further divided into answer formats corresponding to the classes used in TREC (Voorhees, 2004). While TREC used a “definitional” type, we generalized it as a “descriptive” type, which includes “definitional,” “reasons,” and “methods” types (Oh et al., 2009).A user question in natural language form is analyzed using multiple linguistic analysis techniques including POS tagging, chunking, and named entity tagging (Lee et al., 2006; Lee & Jang, 2011). The analyzed result of a question has three components including answer format (AF), answer theme (AT) and question target (QT). The AF has three possible values: factoid, list, and descriptive. They can be distinguished based on the surface-level description of questions. For example, “Where is the Nile River located” looks for a single factoid answer, whereas “Who are American politicians who have emigrated from Austria” requires a list of answers. A descriptive question needs an answer that contains definitional, causal, or method information about a key term, as in “What is X,” “What is the cause of X,” or “What is the method for X?”. An AT is the class of the object or description sought by the question, such as PERSON, LOCATION, and DATE for a factoid; list answer format; and DEFINITION, REASON, and METHOD for a descriptive answer format. We used a total of 147 answer themes, which are organized into a hierarchical structure (Lee et al., 2006). A QT consists of two parts: object and property. The former is the main object or event that the question is about, whereas the latter is the property of interest that a question attempts to get at regarding the object. In “Where is the Nile River located,” for example, the object is “Nile River” and the property is “be located.” The key elements in detecting the question target are the predicate-argument structure or noun phrase structure in the dependency structure of the given question. When the property is not clear, it can remain empty.Given a question q, we want to find a question analysis result r=(af, at, qt) which most likely explains what the question means as follows;rˆ←argmaxrSQ(r|q)SQ(r|q)=SAF(af|q)·SAT(at|q)·SQT(qt|q)where SQ() is a score for question analysis, and SAF(), SAT(), SQT() are scores for analyzing AF, AT and QT, the scores are normalized between 0 and 1.Ideally, questions should be answered through a direct comparison to a well-structured knowledge base. However, the cost of building and searching a well-structured large scale knowledge base is too expensive, so we rely on a conventional QA system architecture consisting of a question analyzing module, document retrieval module, and answer matching module. The system select the best answer a for given question q that maximizes the multiplication of question analysis score SQ(r|q), document retrieval score SD(d|r) and answer matching score SA(M)(a|q, r, d) by module M as follows:aˆ←argmaxaS(a|q)S(a|q)=SQ(r|q)·SD(d|r)·SA(M)(a|q,r,d)where r, a, d are question analysis result, answer candidate and retrieved document, respectively. The scores are normalized between 0 and 1. An additional answer merging module combines and ranks the answers generated from the answer matching modules (Fig. 1). To this end, article section titles, infoboxes, and article categories are converted into text documents. Wikipedia definition and redirection databases are managed for accurate and wide coverage answers.We extract answers from article content using a traditional answering method. Most state-of-the-art QA systems implement a technique for extracting paragraph-sized passages of text from a large corpus (Khalid & Verberne, 2008). Therefore, we split articles into small passages based on their article structure. Entities that match to the answer theme are selected as answer candidates from retrieved documents for factoid and list questions, and all sentences in the retrieved documents are answer candidates for descriptive questions. Answer matching score is measured based on the distance between question words and answer candidate a in document d for factoid, list questions. For descriptive questions, the score is measured based on the content similarity and the pattern similarity measures. The content similarity is a similarity between words in the question and words in the answer candidate. The pattern similarity is the similarity between sentence patterns for descriptive answers and answer candidates. We defined lexico-syntactic patterns in regular expression format that embed Korean sentence styles for definition, reason, and method descriptions (Table 2).SA(AC)(a|q,r,d)=Sdist(q,a,d)forfactoid,listquestionsScsim(q,a)·Spsim(q,a)fordescriptivequestionsBecause the sections are divided based on important properties or issues that many users are interested in, the article’s section structure is a valuable knowledge source in a QA system. Sometimes, it is very hard to determine proper answer themes for certain questions. For example, the answer theme for the question, “What about damage in Sumner area by 2011 Christchurch earthquake,” is not clear because our answer theme structure does not have “damage” type. The answer theme for this question is a description rather than a pre-defined type. We can find answers for such questions using article title, section titles and QT of questions. To this end, an article is divided into multiple sections where a document pair <dt, dc> is assigned to each section, where dt=<tA, tU1,…,tUN, tS> is a title document that includes article title tA, section title tS, and its upper level section titles tU1,…,tUN, and dc is a section content that can be an answer candidate of a matched question. Thus, the object and property names of the QT are used as a query for the document retrieval module. The answer matching module finds the object name at the beginning of the retrieved document and matches the property in the remaining parts of the document as follows;SA(AS)(a|q,r,d)=1ifquestiontargetmatchesarticletitleandpropertymatchessectiontitle0otherwiseFig. 2shows an article structure for “2011 Christchurch earthquake” and its generated documents. The top rectangle is the article title, and the ellipses are subsection titles. The white and gray rectangular papers are section title documents and section content documents, respectively. For above question, a section title document with an id of 12354 is retrieved and its corresponding section content document selected as an answer candidate.Some Wikipedia knowledge is encoded in the network structure of articles and categories. In particular, the categories are organized in a taxonomy-like structure (Ponzetto & Strube, 2007). Categories for an article are generalized descriptions of the article. Thus, we can find article names via the descriptions encoded in their categories. For example, the article “Arnold Schwarzenegger” is under categories such as “Austrian immigrants to the United States” and “American actor-politicians,” as shown in Fig. 3. In this case, we can answer the question “Who are American politicians emigrated from Austria?” The category structure can find answers for list questions. Unfortunately, Wikipedia categories do not form taxonomy with a fully-edged subsumption hierarchy, but only a thematically organized thesaurus. Paths through a non-isa link may connect to noisy category names. We applied the method of Ponzetto and Strube (2007) to filter non-isa relations from the upper categories of articles in Korean Wikipedia. Because the category naming conventions in English and Korean Wikipedia differ, we devised a name analysis rules for Korean. Plural nouns in English category names are strong evidences to determine types of articles. We can infer “Arnold Schwarzenegger” is a kind of “Person” from the word “immigrants” in category names. But it is not obligatory to use plural nouns in Korean category names. So we predefined clue words for each entity type and filtered the categories of which entity types of head words are differ from other majority category names (Table 3). For example, “민속(folklore)” is not a clue word for “Person” unlike other category names, so “American folklore” has non-isa relation to “Arnold Schwarzenegger” in Fig. 3.For each article, after isa upper categories are determined, a category structure document is generated. The document contains a one-line description where article name and its upper category names are listed sequentially: <article name, (upper category names,)+>. Because more generic category names are less informative, we set the maximum depth of the upper category path as three. Fig. 4shows an example document for the category structure in Fig. 3. After a category structure document is retrieved through the document retrieval module, the article name, located at the beginning of the document, is selected as an answer candidate. Answer matching score for category structure module is as follows;SA(CS)(a|q,r,d)=1ifallwordsinqarefoundintheretrieveddocument0otherwiseFor the above question, the category document of Fig. 4 is retrieved as a relevant document, and “Arnold Schwarzenegger” is selected as an answer.An Infobox displays the most relevant facts of an article as a table of property-value pairs. The basic form of the infobox document is a list of triples: <article title, property name, property value>. All possible alternations of triples are expressed in the document using an article redirection and property name alternatives. Article redirection has different notations for an article assigned by authors. For example, the article “Obama” is redirected to “Barack Obama.” A property name can be expressed in different lexicals based on the author’s selection. For example, “nationality” for the “president” infobox template is semantically equivalent to “citizenship” for the “writer” infobox template. We manually built a property name alternative database for persons, organizations, locations, entertainment, and product templates. Fig. 5shows a part of an infobox document for “Barack Obama.”For a given question, the document retrieval system searches relevant infobox documents, and the answer matcher compares questions and each line in the retrieved documents. Property values are extracted as answer candidates if an article title and property name are matched to the question. For the question, “Who is Obama’s predecessor?” after the document in Fig. 5 is selected, the answer matcher scans each line in the description and finds “George W. Bush” as an answer candidate. The answer matching score of answer candidate a by infobox module is as follows:SA(IB)(a|q,r,d)=1ifarticletitleandpropertynameofagivenlineindmatchestotheobjectandpropertyofQTinr0otherwiseWe manage Wikipedia article titles and their first paragraph as a definition knowledge base. If the answer theme of a question is DEFINITION, our QA module looks up the definition knowledge base using the object of the QT. For questions such as “What is a tsunami” or “Who is Obama,” the system searches Wikipedia article titles that match “tsunami” or “Obama.” If one or more articles are found, the system suggests contents of the articles as answer candidates.Our QA system makes use of multiple QA modules employing different answer finding methods. A strategy that determines the sequence of module invocations to be invoked when finding an answer is selected based on several factors such as the expected AF, AT and QT of the question as in Table 4. Given two modules, QA1→QA2 shows that QA1 and QA2 are invoked in sequence, whereas QA1+QA2 indicates that they are to be processed in parallel. The algorithm for the sequence and parallel invocations are described in Tables 5and 6respectively. The composition of module invocations are determined based on the module tests for each question type (Table 9). For factoid question, precisions of IB and AS are higher than those of other modules. So, the strategy invokes IB and AS modules first, and the scores of answer candidates generated from the two modules are merged based on the parallel invocation algorithm (Table 6). If the score of top-ranked answer is higher than predetermined threshold, the answer is final by the sequential invocation algorithm (Table 5), otherwise, CS and AC modules are invoked in parallel. We set the thresholds of QA modules based on repeated experimental results.We downloaded Korean Wikipedia from a Wikipedia dump site.1http://dumps.wikimedia.org, 2010/11/04 ver. for Korean Wikipedia.1The articles were preprocessed into predefined document types, as shown in Table 7. All the documents were analyzed and indexed using multi-level linguistic analysis techniques such as POS tagging, and named entity tagging. Evaluations were made by three human judges who understand the functionality of QA modules. When the decisions conflicted we followed the majority. For factoid questions, we used the TREC “exact answer” criterion. For descriptive questions, our evaluation was based on whether candidate answer sentences contain “key phrases,” similar to the TREC “nugget” criterion (Dang & Lin, 2007). For an effective comparison, we employed the mean reciprocal rank (MRR), precision, recall, and F-score. We grouped our QA modules as follows:•Group 1: a traditional QA module for article contents (AC, Baseline).Group 2: single modules including the infobox module (IB), category structure module (CS), article structure module (AS), and definition module (DEF).Group 3: dual combined modules including a merged module of AC and IB (+IB), AC and CS (+CS), AC and AS (+AS), AC and DEF (+DEF).Group 4: a merged module of all modules (+ALL).The overall evaluation results are shown in Table 8. The +ALL module shows the highest performance as expected. When a baseline module was chosen for the given query set, a total of 162 answers were correct. On the other hand, 316 correct answers were returned when all the modules were invoked and the answers merged. This indicates that the additional correct answers were extracted from semi-structured knowledge sources. All modules in group 2 show high precision but low recall. This means that we can easily extract correct answers from the semi-structured information, but coverage of the information is still low. All modules in group 3 show higher precision and recall than the baseline module. This means that each semi structured information source covers its own unique types of questions. For example, infobox has the advantage of finding an entity’s properties such as “Who designed the Eiffel Tower?” In particular, the +DEF module showed the highest F-measure over other modules in group 3. This indicates that our question set includes many definitional questions, and Wikipedia covers many of the concepts or entities referred to in these questions. The +AS module can find answers for complex questions for which it is difficult to identify an answer theme. The +CS module can find a list of entities as answers, as most Wikipedia categories represent abstract concepts and have member entities.All modules show high MRR scores of between 0.840 and 0.910 in groups 1, 3, and 4. Single modules in group 2 show 1.0 MRR scores except the AS module. Because we focused on precision rather than recall, we set a high threshold in the answer matching modules. Although our system extracts a maximum of five answers for a question, the high threshold values cut off answer candidates even if they are in the fifth rank. Therefore, the improvement in MRR of the +ALL module is only 8.3%, which is relatively lower than the improvement of the F-measure.We analyzed the effects of Wikipedia’s semi-structure information on different types of questions: factoid, list, and descriptive questions. Table 9shows a comparison among the six methods for the three question types. The AC (baseline) module shows a lower performance than the +ALL module in all question types. The +IB module improved the performance of the factoid questions, and generated 18 additional answers, 16 of which are correct. The +IB module does not generate additional answers for list or descriptive questions. The +CS module improved the performance of list questions, responding to 24 more questions. The +AS module improved the performance of factoid and descriptive questions. The module responded with correct reason- and method-type answers to 10 more questions. The +DEF module responded to 92 more descriptive questions, showing the highest impact in our experiment.Since multiple components involved in answering process, an incorrect answer should be traced back to identify the first place where the error occurred (Moldovan et al., 2003). We analyzed errors based on the factors described in Oh et al. (2009). Table 10shows the analysis results of 47 errors. The top-5 cut-off strategy charges the largest proportion (27.7%), as many correct answers were cut-off, especially for factoid questions in the AC module. The answer theme analysis and document indexing/retrieval module generated a considerable number of incorrect answers (38.3%) mostly in the AC module where possible answers in a document should be preliminarily indexed with their entity types. This observation indicates that a traditional QA module based on simple text documents shows lower performance than rich knowledge based modules. Document retrieval module can be improved by utilizing Wikipedia title matching strategy rather than simple content matching strategy. Errors in the question format analysis also induced incorrect answers (12.8%). When the incorrect question format is identified, the system relies on an unreliable answer finding strategy. Answer merging strategy induced three wrong answers (6.4%), when prior module generated incorrect answers with high confidence in the module invocation sequences. Because IB module is more reliable than others for factoid questions, errors in IB module likely propagate to final decision. However the percentage of the strategy error is relatively lower than other error types. This means that the modules have their own strength to specific question types, and the strict answer merging strategy is effective.

@&#CONCLUSIONS@&#
