@&#MAIN-TITLE@&#
Accurate algorithms for identifying the median ranking when dealing with weak and partial rankings under the Kemeny axiomatic approach

@&#HIGHLIGHTS@&#
An accurate heuristic solution to the median ranking problem is proposed.The reference paradigm is the Kemeny–Snell axiomatic framework.Simulation studies and real data applications are performed to evaluate accuracy efficiency of our proposal.

@&#KEYPHRASES@&#
Preference rankings,Median ranking,Kemeny distance,Social choice problem,Branch-and-bound algorithm,

@&#ABSTRACT@&#
Preference rankings virtually appear in all fields of science (political sciences, behavioral sciences, machine learning, decision making and so on). The well-known social choice problem consists in trying to find a reasonable procedure to use the aggregate preferences or rankings expressed by subjects to reach a collective decision. This turns out to be equivalent to estimate the consensus (central) ranking from data and it is known to be a NP-hard problem. A useful solution has been proposed by Emond and Mason in 2002 through the Branch-and-Bound algorithm (BB) within the Kemeny and Snell axiomatic framework. As a matter of fact, BB is a time demanding procedure when the complexity of the problem becomes untractable, i.e. a large number of objects, with weak and partial rankings, in presence of a low degree of consensus. As an alternative, we propose an accurate heuristic algorithm called FAST that finds at least one of the consensus ranking solutions found by BB saving a lot of computational time. In addition, we show that the building block of FAST is an algorithm called QUICK that finds already one of the BB solutions so that it can be fruitfully considered to speed up even more the overall searching procedure if the number of objects is low. Simulation studies and applications on real data allows to show the accuracy and the computational efficiency of our proposal.

@&#INTRODUCTION@&#
The consensus ranking problem, also known as social choice problem, arises any time n subjects (or judges) are asked to express their preferences on a set of m objects. These objects are placed in order by each subject (where 1 represents the best and m the worst) without any attempt to describe how much one differs from the others or whether any of the alternatives is good or acceptable. Every independent observation is a permutation of m distinct positive integer numbers. To be more specific, when the subject assigns the integer values from 1 to m to all the m items we have a complete (or full) ranking. Whenever instead the judge fails to distinguish between two or more items and assigns to them the same integer number (expressing indifference to the relative order of this set of items), we deal with tied (or weak) rankings. Moreover we have a partial ranking when judges are asked to rank a subset of the entire set of objects (e.g. pick the three most favorite items out of a set of five) (D’Ambrosio & Heiser, 2014; Marden, 1996). Rankings are by nature peculiar data in the sense that the sample space of m objects can be only visualized in a(m−1)-dimensional hyperplane by a discrete structure that is called the permutation polytope, Sm. A polytope is a convex hull of a finite set of points inRm(Heiser, 2004; Thompson, 1993). For example the space considering 4 objects with all possible ties is a truncated octahedron that can be visualized in Fig. 1(Heiser & D’Ambrosio, 2013). As we already pointed out, the permutation polytope is inscribed in a(m−1)-dimensional subspace, hence, for m > 4, such structures are impossible to visualize. The permutation polytope is the natural space for ranking data. To define it no data are required, it is completely determined by the number of items involved in the preference choice; data add only information on which rankings occur and with what frequency they occur. This space is discrete and finite. It is characterized by symmetries and it is endowed with a graphical metric.The problem of combining rankings to obtain a ranking representative of the group has been studied by numerous researchers in several areas, e.g. voting systems, economics, machine learning, psychology, political sciences, for more than two centuries. In the framework of distance-based models for rankings, searching for consensus ranking is a very important step in modeling the ranking process (Marden, 1996). These models are usually exponential family models (Diaconis, 1988) and they are completely specified by two parameters, a dispersion parameter and a consensus (central) ranking. Maximum likelihood estimates of the dispersion parameter assume the knowledge of the central ranking. When the consensus ranking is not known it should be estimated. Unfortunately, even if there are close formulas for this estimation they are not feasible because of the complexity of the problem (Critchlow, Fligner, & Verducci, 1991; Critchlow, 1985; Diaconis, 1988; Fligner & Verducci, 1986, 1988). Several methods to aggregate individual preference rankings have been proposed since the works of Arrow (1951)Barthelemy and Monjardet (1981)Black (1958)Bogart (1973)Cook and Seiford (1978)Coombs (1964)Davis, DeGroot, and Hinich (1972)de Borda (1781)de Condorcet (1785)Emond and Mason (2002)Goodman and Markowitz (1952) and Meila, Phadnis, Patterson, and Bilmes (2012).In this paper, we propose two heuristic algorithms called QUICK and FAST to derive the consensus ranking from the aggregation of individual preferences within the Kemeny and Snell axiomatic framework. Both algorithms can be viewed as alternatives to the branch-and-bound algorithm by Emond and Mason. The BB algorithm turns out to be a time consuming procedure when the number of objects is high and especially when the internal degree of consensus present in the data is weak. Both QUICK and FAST algorithms can deal with complete and tied rankings as well as with incomplete (or partial) rankings. As a matter of fact, the QUICK algorithm is the building block of the FAST algorithm. Both provide savings in computational time, but the FAST algorithm is more accurate because it finds more than one of the solutions found by the BB algorithm and it can also easily deal with problems characterized by a large number of objects to be ranked and weak and partial rankings and/or a low degree of internal consensus. On the other hand, the QUICK algorithm turns out to be really useful when the number of objects is limited because it returns one of the solutions found by the BB, or a really close solution, in a considerably small amount of time.The rest of the paper is organized as follow. In Section 2 we briefly present some of the proposed approaches to aggregate preference rankings and derive a consensus. In Section 3 we describe the branch-and-bound algorithm by Emond and Mason. Section 4 is devoted to describe the proposed algorithms, then in Sections 5 and 6 we present a simulation study and applications on real data to evaluate both the accuracy and the efficiency of our proposal. Concluding remarks are then found in Ssection 7.The term consensus ranking is a generic name for any ranking that summarizes a set of individual rankings. There exist two broad classes of approaches to aggregate preference rankings in order to find a consensus (Cook, 2006):•ad hoc methods, which can be divided into elimination (for example the American system method, the pairwise majority rule, etc.) and non-elimination (for example Borda’s methods of marks (1781), Condorcet’s method (1785), etc.);distance-based models, according to which it is necessary to define a distance of the desired consensus from the individual rankings.A more detailed description of both these approaches can be found in Cook (2006).How to aggregate subjects preferences to create a consensus is a problem that goes back to 1781 when Borda formulated the method of marks (also known as Borda’s count) for determining the winner in elections with more than 2 candidates. This method is quite simple and it is based on calculating the total rank for each alternative. For example, if we consider the rankings in Table 1the total rank for each alternative is given by:•A=12×2+5×1+7×3=50,B=12×1+5×2+7×2=36,C=12×3+5×3+7×1=58,In the last century the rank aggregation problem has been approached from a statistical perspective. Kendall (1938) was the first to propose a method to aggregate input rankings to find a consensus. He studied the consensus problem as a problem of estimation and he proposed to rank items according to the mean of the ranks assigned, thus proposing a method equivalent to Borda’s one. Moreover he suggested to consider the Spearman rank correlation coefficient ρ, that, given two preference rankings R and R*, is defined as:(1)ρ=1−6∑i=1ndi2n3−n,wheredi2(R,R*)=∑j=1m(Rj−Rj*)2is the squared difference between rankings R and R* (Kendall, 1948, page 8). The Spearman’s ρ is equivalent to the product moment correlation coefficient and it treats rankings as they are scores summing the square of ranked differences.Kendall (1938) proposed his own correlation coefficient, named after him as Kendall τ, by introducing the concept of ranking matrices. The ranking matrix associated with the ranking Riof m objects, is a m × m matrix {aij} whose elements are defined as:(2)aij={1ifobjectiisrankedaheadofobjectj−1ifobjectiisrankedbehindobjectj0iftheobjectsaretied,orifi=j.The Kendall correlation coefficient τ between two rankings, R, with score matrix {aij}, and R*, with score matrix {bij}, can be then defined as the generalized correlation coefficient:(3)τ(R,R*)=∑i=1m∑j=1maijbij∑i=1m∑j=1maij2∑i=1m∑j=1mbij2.In the same period Kemeny (1959) and Kemeny and Snell (1962) proposed and proved an axiomatic approach to find a unique distance measure for rankings and define a consensus ranking. They introduced four axioms, reported in Table 3, that should apply to any distance measure between two rankings. They also proved the existence of a distance metric that satisfies all these axioms, known as Kemeny distance, and its uniqueness. By using the score matrices as defined by Kendall, Kemeny’s distance between two rankings R (with score matrix {aij}) and R* (with score matrix {bij}) is defined as:(4)dKem(R,R*)=12∑i=1m∑j=1m|aij−bij|.Kemeny and Snell then suggested the idea to use this distance function to define the median ranking as a specific definition of consensus ranking. According to their definition, the median ranking is the point in the ranking space that shows the best agreement with the set of input rankings. More formally, given a set of n independent input rankings{Ri}i=1n,the median rankingS^is the point (or the points) for which∑i=1nd(Ri,S)is a minimum. Following the Kemeny and Snell approach, the research of the median ranking requires searching the space of all possible rankings of m object. Given a set of n independent input rankings the problem consists in finding the rankingS^that best represents the combined preferences of the judges. This is a NP-hard problem. When we have m objects, there are m! possible complete rankings. In case we deal with tied rankings, the analysis is more complex as, by including ties, the number of possible rankings approximates12(1ln(2))m+1m!(Gross, 1962). In other words, the complexity of the search of the median ranking is entirely determined by the number of objects to be ranked.Bogart (1973, 1975) generalized the Kemeny and Snell approach by considering both transitive and intransitive preferences. Cook and Saipe (1976) proposed a branch-and-bound algorithm to determine the median ranking out of a set of n independent preference rankings deriving a solution by adjacent pairwise optimal rankings. Emond and Mason (2002) pointed out that Cook and Saipe’s method does not guarantee that all solutions are found and in some examples local optima were encountered. Cook, Kress, and Seiford (1997) proposed a general representation of distance-based consensus with the aim of associating a value to rank positions and developed models for deriving a consensus. Cook, Golany, Penn, and Raviv (2007) presented a branch-and-bound algorithm for finding the consensus ranking in presence of partial rankings, but not allowing for ties.Emond and Mason (2002) proposed a new rank correlation coefficient called τxthat is equivalent to the Kemeny and Snell distance metric. They defined the score matrices in a slightly different way respect to the Kendall’s representation:aij=1if object i is either ranked ahead or tied with object j, andaij=0only ifi=j. Using these score matrices, they defined their rank correlation coefficient as:(5)τx=∑i=1m∑j=1maijbijm(m−1).Note that τxis equivalent to Kendall’s τ when ties are not allowed. By using this correlation coefficient they proposed a branch-and-bound algorithm to deal with the median ranking problem when the number of object m is at most equal to 20 in a reasonable computing time. Given n weak orderings of m objects,R1,…,Rn,where each ordering carries a positive weight, wk, median rankingS^is the one (or the ones) that maximizes the weighted average correlation with the n input rankings or, equivalently, is the one (or the ones) that minimizes the weighted average Kemeny distance to the n input rankings,(6)max∑k=1nwkτX(S,R(k))∑k=1nwk.Indicating as {sij} and {rij}(k) the scoring matrices for S and the kth ordering R,k=1…,n,the problem is:(7)max∑k=1nwk{∑i=1m∑j=1msijrij(k)}=max∑i=1m∑j=1msijcij,wherecij=∑k=1nwkrij(k). The score matrix {cij} was called by Emond and Mason Combined Input Matrix (CI) because it is the result of a summation of each input ranking. Defined in this way, it summarizes the rankings information in a single matrix.Emond and Mason conceived a branch-and-bound algorithm to maximize Eq. (7) by defining an upper limit on the value of that dot product. This limit, considering that the score matrix {sij} consists only of the values 1, 0 and−1,is given by the sum of the absolute values of the elements of CI:V=∑i=1m∑j=1m|cij|.If a weak ordering of m objects is given as initial solution, it is possible to compute the associated score matrix {sij} and evaluate the value of expression (7). Then it is possible to define an initial penalty P by subtracting this value from V. The problem is to search the set of all weak orderings of m objects to find those with the minimum penalty. This set can be divided into three mutually exclusive branches based on the relative position of the first two objects in the ordering represented in the initial solution, labeled as i and j. An incremental penalty for each of the branches can be calculated, by considering the corresponding elements cijand cjiof the CI matrix, as specified in Table 4. If the incremental penalty for a branch is greater than the initial penalty, then we do not consider it any longer because all orderings in that branch will have a penalty larger than the initial one.If the incremental penalty of a branch is smaller (or equal) than the initial penalty, we then consider the next object in the initial solution and create new branches by placing this object in all possible positions relative to the objects already considered.The algorithm continues in an iterative way by including all other objects until all branches to be considered are checked. The BB algorithm works with complete, incomplete and partial rankings. It deals with incomplete rankings thanks to the convention that unranked objects do not add anything in forming the combined input matrix. Emond and Mason stated that the computation time needed to reach a solution(s) depends both on the inherent degree of consensus in the sample of judges and on the quality of the initial solution used to initialize the algorithm. For an extensive discussion on the branch-and-bound algorithm we refer to Emond and Mason (2000, 2002).The first element to be evaluated in developing our algorithm is the combined input matrix. This matrix contains all the information about the rankings expressed by all the subjects and, if it is a valid score matrix, then the median ranking can be found immediately. Unfortunately such a situation rarely happens. But by evaluating the CI with more attention it is possible to identify a good candidate to be the median ranking that can be used as an input in the algorithm. LetQ=1be a vector of ones of size m. Let {cij} be the m × m combined input matrix. By taking into account all the combinations of m objects, each pair of items is evaluated once by considering the two associated cells in CI. A moderately accurate first candidate to be the median ranking can be computed as follow:If signcij=1&signcji=−1,thenQi=Qi+1;If signcij=−1&signcji=1,thenQj=Qj+1;If signcij=1&signcji=1,thenQi=Qi+1,Qj=Qj+1.In this way, we obtain the updated rank vector Q containing the number of times each object is preferred to the others in the pairwise comparisons. This vector is the starting point of our algorithm. The first step is to compute the score matrix, {qij}, associated with Q. Then we compute the associated penalty as:(8)P=V−∑ijcijqij.After this step we take into account the object in Q ranked at the second position, and we evaluate Eq. (8) by placing that object in all possible positions relative to the object ranked ahead, including ties. In other words, in the first step the second ranked object is placed ahead, in a tie and behind the first object, keeping all other objects fixed in the initial position. The penalty (Eq. (8)) is then evaluated for these three rankings and we continue by evaluating only the ranking with the lowest penalty in the successive step, updating the candidate median ranking. Once the penalties are computed, we update the candidate by selecting the ranking that is associated with the minimum penalty. Subsequently we add the object ranked in the third position in the initial Q vector, and again we compute the values of Eq. (8) by placing that object in all possible positions relative to the objects already ranked ahead, including all possible ties. As before, we update the candidate median ranking by selecting the one that minimizes the penalty. We continue in this way until all the objects are processed and we reach a possible solution.We use, then, the obtained solution as starting point for a new complete loop. The overall procedure is repeated again by considering also the reverse ranking of the initial Q vector as candidate median ranking. The complete algorithm is summarized in Box 1. Note that when we evaluate the penalty, we consider all the objects in the ranking that is considered as candidate solution. This is a fundamental difference with the original algorithm, because Emond and Mason calculate the penalty values only by considering the elements of the combined input matrix associated with the processed objects, and updating the penalty by adding up these partial values. Indeed, we never use this penalty update.We call this algorithm “QUICK” because it is able to reach at least one solution, or a solution really close to the true one, in few seconds even when working with a huge number of objects. In our experience, by using our definition of starting point Q, at least one solution is found. But, sometimes, solutions were also reached with random starting points. For this reason, we decided to use the QUICK algorithm as building block of our accurate FAST algorithm for the median ranking problem, whose pseudo-code is shown in Box 2. Of course, our FAST algorithm is useful when the complexity of the problem is really intractable, e.g. when the number of objects to be ranked is high and the internal degree of consensus is low. Among the solutions returned by the QUICK algorithm, the median rankings are those showing the highest value of the average τxrank correlation coefficient.We implemented the BB algorithm by Emond and Mason, as well as both the QUICK and FAST algorithms in MatLab and in R environments. The reported results are based on codes written in MatLab language. A beta version of the R ConsRank package is available upon request to the authors, as well as the MatLab codes. Analysis were made by using a Computer Intel Core i5-3317U 1.70 GHz and 4GB of RAM.To evaluate the performance of our algorithms in terms of accuracy and efficiency, we performed a simulation study. Ranking data were simulated according to a distance-based model by selecting three different levels of the dispersion parameter θ, which governs the degree of consensus in the sample of rankings. In the distance-based models framework, for a given consensus, S, a distance function, d, and some real parameter θ, the density with respect to the Uniform distribution is equal tofθ(a;S)=C(θ)exp(−θd(S,a)),where a is a ranking and C(θ) is a normalizing constant. For more details on distance-based models we refer to Feigin and Cohen (1978)Marden (1996) and Critchlow et al. (1991).The three levels chosen for θ were 0.7, 0.4 and 0.1, the distance used was the Kemeny distance. We decided to consider 4 different levels for m: 4, 9, 15 and 20. In the case of 4 and 9 objects, we repeated the experiment both considering only complete rankings and the full space of complete and tied rankings, while in the case of 15 and 20 objects we decided to limit the experiment only to complete rankings sampled from a limited sub-population of size 10 millions. These sub-populations were generated from the full rankings space of 10 objects by adding the remaining objects in such a way that they were at first ranked below, later ranked ahead, and then randomly ranked in a middle position. Sample size was always equal to 200. Another experiment involved incomplete rankings. We chose a scheme of the type “pick k out of m”, and precisely: pick 2 out of 4, pick 5 out of 9 and pick 10 out of 15. Rankings were sampled in this way: first we extracted a random number of rankings (from a minimum of 15 to a maximum of 30) according to the uniform distribution by settingθ=0from the corresponding spaces, then we generated the weights from a normal distribution with means randomly generated between 10 and 30 and standard deviations randomly generated between 2.5 and 9. After we normalized the weights and multiplied them by the total sample size to have data sets approximatively of size 200. Each experiment was repeated ten times, for globally 240 data sets. Table 5summarizes the experimental design. For each data set we ran the BB, the QUICK and the FAST algorithms. We checked the median rankings found by the three algorithms as well as the elapsed time in seconds to reach the solutions. We used the BB algorithm as benchmark to check the accuracy of our algorithms in terms of solutions. The initial solution for all the algorithms was the updated rank vector Q as defined in Section 4.Table 6shows in the first column a summary of the solutions reached by the BB algorithm. In the second and in the third columns respectively summary measures of the coincident solutions returned by the QUICK and FAST algorithms with respect to the ones handed back by the Emond and Mason’s one are shown. Note that always both QUICK and FAST algorithms found at least one solution, and the proportion of solutions found by the FAST algorithm was always higher (or equal) to the one returned by the QUICK. There were no relevant differences among the factors of the experimental design except, as expected, that the lower θ, the higher the number of solutions identified. This is due to the fact that in this particular experiment there was a moderate internal degree of consensus present in the data, even when θ was set equal to 0.1. Table 7reports the solutions returned by the BB algorithm and the number of coincident solutions recovered by the QUICK and FAST algorithms in the experiment with incomplete rankings. In this case, due to the sampling procedure, the internal degree of consensus in the data sets was quite poor. The experiments with 9 and 15 objects respectively count a maximum number of solutions equal to 31 and 7761. In one case the QUICK algorithm did not find one of the BB solutions, but it did not happen with the FAST algorithm. This particular case is helpful to understand why we called this algorithm “FAST”. The BB algorithm found 25 solutions in 24240.054 seconds ( ∼ 6.733 hours), each one with an average τxequal to 0.106. The FAST algorithm could find 6 of the 25 solutions in 64.932 seconds. The two solutions found by the QUICK algorithm were found in 0.693 seconds and were really close to be real solutions because they were characterized by an average τxequal to 0.104. This was the unique case in which the QUICK algorithm did not find one of the BB solutions.Figs. 2–4 show the distribution of working time of both BB and QUICK algorithms. We do not show the box-plots relative to the FAST algorithm because its computing time was approximately equal to the number of iterations multiplied by the computing time of the QUICK algorithm. As it can be noted, the QUICK algorithm is on average faster than the BB algorithm, and the variability of the computing time increases as the value of θ decreases.Table 8summarizes the computing time for the experiment involving incomplete rankings. The computation time for the QUICK algorithm has not a considerable variability while, especially in the case of 15 objects, BB computational time shows a higher variability.The first real data application is about the data reported by Emond and Mason (2000, page 28) which are shown in Table 9. The first 15 columns represent the objects to be ranked with labels in the first row, while the last column reports the weight associated with every ranking. By using the BB algorithm we obtained exactly the following solutions (as also reported by Emond & Mason, 2000, page 29), with an average τxequal to 0.166:1.< D L (E-M) (A-B) I P (C-N) H F G (O-Q) >< D L (E-M) (A-B-P) (C-N) I H F G (O-Q) >< D L (E-M) (B-P) A (C-N) I H F G (O-Q) >Computing time was equal to 5113.608 seconds. We ran the QUICK algorithm on these data obtaining solution number 3 in a computing time of 0.155 seconds. Then we ran our FAST algorithm with 100 starting points, obtaining exactly all solutions with a computing time of 12.627 seconds.The second data set used to compare the computing time of the algorithms is the famous data set about voters for the 1980 election of American Psychological Association president (Diaconis, 1988; Murphy & Martin, 2003). This data set contains the rankings expressed by 15,449 psychologists on five candidates:A=Bevan,B=Iscoe,C=Kiesler,D=Siegle andE=Wriths. Of these rankings only 5738 are complete, while the remaining are partial rankings. As shown in Table 10all the algorithms reached the same unique solution characterized by an average τxequal to 0.023.The third data set used is known as the Sports data set and it comes from Louis Roussos (Marden, 1996). In this data 130 students of the University of Illinois were asked to rank seven sports according to their preference of participating in. The sports considered were: A=baseball, B=football, C=basketball, D=tennis, E=cycling, F=swimming and G=jogging. Also in this case there is a unique solution, and the results are reported in Table 11. Also in this case all the algorithms reach the same unique solution characterized by an average τxof 0.428, as reported in Table 11.To test the ability of our algorithms to deal with rankings with a large number of objects the forth data set is a random subset of the rankings collected by O’Leary Morgan and Morgon (2010) on the 50 American States. The number of items (the number of American States) is equal to 50, and the number of rankings is equal to 104. These data concern rankings of the 50 American States on three particular aspects: socio-demographic characteristics (as population in 2008, GPD per capita, median household income, total expenditures, etc.), health care expenditures (as per capita hospital expenditures, percent of people covered by health insurance, percent of people covered by employment base insurance, etc.) and crime statistics (as crime rate, number of arrests, murder rate, etc.). It was unfeasible to run Emond and Mason’s algorithm on this data. The orderings corresponding to the three solutions found by the FAST algorithm, characterized by an average τxequal to 0.298, are reported in Table 12. These solutions were obtained in 1177.274 seconds (∼19 minutes) with 1000 iterations. The QUICK algorithm found 1 solution (solution 2 in Table 12) in 16.384 seconds.In this paper we proposed two accurate algorithms (the QUICK and the FAST) to solve the problem of identifying the median ranking in situations involving full, weak and partial ranking. Our approach lies into the Kemeny and Snell theoretical framework. Our algorithms can be considered as an alternative to branch-and-bound algorithm proposed by Emond and Mason (2002). The BB algorithm results to be a time demanding procedure when the number of objects is high especially when the degree of internal consensus in the data is weak. Our approach is heuristic and, thus, it does not return all the possible solutions that can be found by an exhaustive search (as in the BB algorithm). For this reason it may happen that the QUICK does not reach a solution being stuck in a local optimum, however even if this happen the FAST, by repeatedly running the QUICK algorithm with random permutations of the m items, in our experiments, always identifies a global optimum. Nevertheless, finding all the solutions in presence of multiple median rankings could not always be the final goal of the analysis, especially considering that the returned solutions are mutually coherent since they present the same value of the average τx. We illustrated the performance of both these algorithms in terms of accuracy and computational efficiency via simulated and real data sets. As shown by the results of the simulation studies, when the number of objects is smaller than 15, the FAST algorithm on average recovers all the solutions handed back by the BB algorithm. On the other hand, when the number of objects is equal or higher than 15 the FAST recovers on average the 70 percent of the BB solutions. The QUICK algorithm always finds at least one of the solutions in a sensibly lower amount of time with respect to the BB algorithm. When dealing with partial rankings and a weak internal degree of consensus, the FAST algorithm again shows a good performance. Indeed, even if it does not return all the BB solutions, it always returns more than one solution in a limited amount of time. In this case, the QUICK also finds at least one BB solution in a considerably shorter time. Moreover, as can be noted from the real data analysis, when the number of objects is smaller than 20, the QUICK again always finds one of the BB solutions in a shorter period of time respect to the BB algorithm. If the number of objects is greater than 20, as in the case of the 50 American States data set, Emond and Masons algorithm is unfeasible, while the FAST finds three solution in less than 20 minutes. To some extent, the impact of the result of our proposal can be compared to that one obtained by Mola and Siciliano (1997) in the field of classification and regression trees (Breiman, Friedman, Olshen, & Stone, 1984). As an example, D’Ambrosio, Aria, and Siciliano (2007)Siciliano and Mola (2000) and D’Ambrosio, Aria, and Siciliano (2012) considered the FAST algorithm to speed up the splitting procedure in tree growing that proved to be effective respectively to deal with huge and complex data sets as well as to improve the computational cost of using ensemble methods and finally to accelerate the missing data imputation within the statistical learning paradigm.

@&#CONCLUSIONS@&#
