@&#MAIN-TITLE@&#
Multiple-source learning precedence graph concept for the automotive industry

@&#HIGHLIGHTS@&#
Lack of data prevents applying assembly line balancing algorithms in practice.A learning precedence graph approach offers a low cost solution for this problem.However, it needs long warm-up period and is highly sensitive to the input data.We extend the existing approach essentially to overcome those drawbacks.Computational analyses show a high acceleration in learning precedence information.

@&#KEYPHRASES@&#
Combinatorial optimization,Assembly line balancing,Incomplete precedence graph,Learning approach,Decision support,

@&#ABSTRACT@&#
In modern production systems, customized mass production of complex products, such as automotive or white goods, is often realized at assembly lines with a high degree of manual labor. For firms that apply assembly systems, the assembly line balancing problem (ALBP) arises, which is to assign optimally tasks to stations or workers with respect to some constraints and objectives. Although the literature provides a number of relevant models and efficient solution methods for ALBP, firms, in most cases, do not use this knowledge to balance their lines. Instead, the planning is mostly performed manually by numerous planners responsible for small sub-problems. This is because of the lack of data, like the precedence relations between the tasks to be performed. Such data is hard to collect and to maintain updated.Klindworth, Otto, and Scholl (2012) proposed an approach to collect and to maintain the data on precedence relations between tasks at a low cost, as well as to produce new high-quality feasible assembly balances based on this data. They utilize the knowledge on former production plans available in the firm. However, due to reliance on the single source of information, their concept needs long warming-up periods. Therefore, we enhance the concept by incorporating multiple sources of information available at firms, as the modular structure, and present guidelines on how to conduct valuable interviews. The proposed interview enhancements improve the achieved results significantly. As a result, our approach generates more efficient new feasible assembly line balances without requiring such long warming-up periods.

@&#INTRODUCTION@&#
The assembly line balancing problem (ALBP) was formulated by Salveson (1955) over 60years ago and was proven to be NP-hard (Wee & Magazine, 1982). Since that time a huge amount of real-world problems and restrictions found their way into mathematical models and the scientific literature (for an overview see, e.g., Boysen, Fliedner, & Scholl, 2008). Because of customization of the products and, consequently, a high variety of product variants with volatile demands that have to be produced on the same assembly line, assembly lines have to be re-balanced intermittently (Boysen, Fliedner, & Scholl, 2009b).Balancing assembly lines manually is a time expensive and challenging job, especially for assembly lines with a large number of work operations (tasks) to be performed. Thus, balancing an average assembly line in the automotive industry requires assignment of about 1000–3000 tasks to workstations. Overall, by automation of assembly line balancing about one third of the planner’s time can be saved (Hirschbach, 1978, chap. 2). Furthermore, the solution quality of the manual assembly line balancing is, at best, only suboptimal, because, in order to be handled manually, this very large optimization problem is divided into manageable subproblems assigned to different planners in the firm.Although there exist fast exact solution methods as well as highly effective heuristic approaches (see Becker & Scholl, 2006), the implementation of such methods can be found in the relevant industries only seldom. This is because of several reasons. First of all, in the past computers were less powerful and the implementation and computation would have caused a lot of additional costs without direct noticeable improvements. This argument does not count so much nowadays, because the computation capacity grew rapidly in the last 20years and the development of very effective heuristics allows us to get optimal or nearly optimal solutions with low computational times.Secondly, and what is in our perception the fundamental reason, firms do not collect and store all the necessary data for solving the ALBP automatically, because such data collection is too expensive. In the first line, information about precedence relations between the tasks has to be specified. Precedence relations show that certain tasks have to be completed before other tasks can be started. E.g., every tire has to be mounted before it can be fixed with the screws. All the precedence relations must be met to guarantee feasible line balances. At firms, the knowledge about the precedence relations exists implicitly in the heads of the planners, who generate feasible plans for their segments of the assembly line. Hence, methods for effective and cost-efficient collecting of this implicit knowledge are of the highest importance, in order to be able to balance assembly lines (semi-)automatically.Klindworth, Otto, and Scholl (2012) analyzed the literature for the existing manual and (semi-)automated methods for gathering information to identify ALBPs. All the methods are time- and cost-intensive, fault prone or they miss important information and, thus, do not guarantee feasible solutions of the ALBP. Generalizing the ideas of Minzu, Bratcu, and Henrioud (1999) and Altemeier (2009, chap. 5.2), Klindworth et al. (2012) developed a new approach, which we call basic learning graph concept (BLGC), that leads to applicable for practice solutions. This concept uses former (feasible) production plans that are available and documented in firms. From those available plans, the concept allows generating new feasible production plans even if precedence relations or independencies between pairs of tasks are evaluated only partially, i.e., no high initial investment is required to implement the approach. Moreover, the concept is expandable and new information can be easily integrated to produce feasible production plans of even better quality.However, as we will show in this paper, the performance of BLGC worsens significantly at certain conditions which are widespread on real-world assembly lines. To overcome this disadvantage, we propose the new extended learning graph concept (ELGC). ELGC integrates further valuable information available in practice and the results of an effective interview technique. We will show that easy but structured, focused questions suffice to efficiently collect enough information for formulating a meaningful ALBP very close to the underlying but unknown problem. Elaborated computational experiments (Section 6) show that ELGC outperforms BLGC dramatically, especially in the initial time, where only few production plans are available. Although the difference between two approaches gets smaller with each additional production sequence, after 20 input production sequences (or about 20months) BLGC still cannot catch up with ELGC approach. Even then, ELGC brings by 2.5percentage point more improvement than BLGC compared to the best historical production plan.Our findings further contribute to close the gap between theory and practice. Since assembly line planners will be able to implement the balancing methods proposed in the literature in real-world planning situations, a feedback to the research community about the requirements on models and methods will lead to adjustments and refinements in this field. The concepts and findings in this paper can be applied to all the generalizations of assembly line balancing problems as well as to other problems for which precedence relations between operations must be considered, such as project scheduling problems. Thus, we contribute to a wide range of well investigated problems in the literature that currently cannot be solved automatically in the real world because of the lacking input data.We will proceed as follows. In Section 2, we shortly present BLGC. Section 3 describes two important constituent parts of the new approach ELGC. These are the incorporation of the known hierarchical and modular structure of products and assembly tasks as well as the implementation of interviews. In Section 4, we introduce interview enhancements that improve the effectiveness of interviews by several times. The overall logics of ELGC and implementation advice is presented in Section 5. Our conclusions are confirmed by extensive tests provided in Section 6. We give a conclusion and an outlook in Section 7.In this section, we give a brief introduction into assembly line balancing (Section 2.1) and the learning precedence graph approach BLGC (Section 2.2).The basic, or simple, version of the assembly line balancing problem (SALBP) is to assign a set of n non-dividable tasks to stations, which are ordered sequentially along a paced line, according to some objective. Each workpiece is available at each station for a given time, called the cycle time c. In this time span, a worker or a machine at each station fulfills the tasks at the workpiece. For each task j∊V,|V|=n, a deterministic time tjis determined, e.g., by MTM-technique (see Hu et al., 2011). The sum of the tasks’ times, which are assigned to one station, must not exceed the cycle time c. Further, the tasks underlie precedence relations because of technical or organizational restrictions. All information about tasks, inclusive their times and precedence relations, is stored in the directed and acyclic precedence graph G=(V,E). In this graph each node j∊V stands for a task with the weight tj, and the set of arcs E marks the precedence relations, i.e., (i,j)∊E if and only if task i is a direct predecessor of j. From E we can easily derive the transitive closure EI, the set of direct and indirect (transitive) precedence relations. The order strength of graph G,OS(G)=2·|EI||V|·(|V|-1)refers to the degrees of freedom for planning. If OS=1, a graph is highly restricted and only one task sequence can be generated, while it is possible to derive|V|!different task sequences from graphs with OS=0 (see, e.g., Scholl, 1999, chap. 2).Fig. 1shows a precedence graph where solid lines mark the direct precedence relations and the dotted lines the indirect ones. The order strength of the presented graph G isOS(G)=2·86·5=0.5. If we set cycle time c at 5, then an optimal solution will require three stations: {1,3},{4,5},{2,6}. The idle time of a station is the time during the cycle, where no tasks are performed. For example, the idle time of the first station with tasks 1 and 3 in the station load is c−t1−t3=5−4−1=0.Different objective functions in the formulation of SALBP exist. Since it is prevalent in most real-world problems, we refer to the objective “minimizing the number of stations m for a given cycle time c” throughout the article. Nevertheless, our approaches and findings can be transferred to models with other objectives as well as to most other versions of generalized ALBP.Although task times are assumed to be deterministic for SALBP and most versions of ALBP, they may vary over time in practice. Due to this, for the current task assignment, the cycle time may be violated at some stations. Therefore re-balancing is regularly performed at firms. On the one hand, process planners provide updates in their MTM-analyses, e.g., whenever new handling tools or new ideas of process development are implemented. On the other hand, task times vary because of changes in the model mix assembled at the assembly line due to fluctuations in demand. For example, the option “air conditioner” may be chosen for half of all the ordered cars in the first period and for a fifth of the orders in the second period. A common practice in planning and modeling of assembly line balancing problem is to use the average task time and treat it as deterministic in each period. Thus, the task times differ from period to period. In our example, if the task “mount air conditioner” takes ten time units, we set the task time to five time units in the first and to two time units in the second period (see Boysen, Fliedner, and Scholl (2009a), for more information).As mentioned above, the precedence graph is not explicitly documented at firms, but assembly line planners respect implicitly the precedence relations of the target graph in every task sequence they build. Since a given precedence graph is the basic assumption and an ultimate piece of information in assembly line balancing models, the existing solution methods cannot be applied to automate and improve the assembly line balancing in manufacturing. We examine the best known data collecting method BLGC in the next section.BLGC is a two-sided approach to deduce the real precedence graph, which we will call target graph from now on. As discussed above, the target graph is usually not known and not documented at firms. Klindworth et al. (2012) propose to utilize the stored at firms information on the former production plans to construct the so-called maximum graphG‾=(V,E‾), which contains all tasks with their task times and at least all the precedence relations of the target graph, i.e.,EI⊆E‾I. Production plans document sequences in which tasks have to be performed at the assembly line. As a rule, planners generate such plans every month for their part of the assembly line. Additionally to the available production plans, feasible task sequences can stem from planning workshops where mainly changes or enhancements in the production process are discussed. The idea of BLGC is that if task i precedes task j in one feasible task sequence and follows task j in another one then tasks i and j are independent of each other, i.e.,(i,j)∉E‾Iand therefore (i,j)∉EI.For example, let the first input task sequence be π1=1→2→3→4→5→6. It initializes the maximum graph and every direct or indirect precedence relation (i,j) of this sequence becomes a precedence relation in the maximum graph. Therefore, the maximum graph contains the highest number of direct or indirect precedence relations that is possible for a non-cyclic directed graph, orOS(G‾)=1in this step. In the following, we call the maximum graph initialized by the first input sequence as first maximum graph. With each new input task sequence, it might be possible to find pairs of tasks i,j∊V, where j precedes i, but the actual maximum graph contains the opposite precedence relation(i,j)∈E‾I. In this case, we delete (i,j) fromE‾Iand the maximum graph gets more degrees of freedom.The received maximum graph is, as a rule, much more restrictive than the target graph. However, it guarantees solutions that are also feasible for the target graph and, therefore, the found number of stations can be used as an upper bound on the number of stations. Of course, it may happen, but is very unlikely in practice, that one of the input sequences represents an optimal solution for the (unknown) target graph. In this case, it is possible to find at least this optimal solution by solving every further maximum graph. But generally, the quality of received assembly line balances based on BLGC highly depends on the proximity of the maximum graph to the target graph. In the Appendix I, we provide several statements on the necessary number of input sequences to receive the maximum graph, which is equivalent to the target graph.Another part of BLGC is the usage of information from interviews or other adequate sources, e.g., CAD-data or some expert interviews, which provide a subsetEI⊆EIof confirmed precedence relations of the target graph. From this, the minimum graph G=(V,E) is constructed. Because the minimum graph is less restrictive than the target graph, the optimal solution of the minimum graph does not have to be feasible for the target graph, but it can provide (sharp) lower bounds for the target graph. If no information is available, the minimum graph contains no arcs and powerful methods for the well-known bin-packing problem (e.g., Scholl, Klein, & Jürgens, 1997) can be used to solve the ALBP based on the minimum graph and thus get lower bounds for the target graph.If the lower bound (provided by the minimum graph) is equal to the number of stations in a feasible solution (deduced from the maximum graph), an optimal solution of the target graph has been found and proven. Note, that this does not require the target graph to be known. The results of the computational tests of Klindworth et al. (2012) confirm that in many cases the bounds are equal or very close to each other.BLGC has several disadvantages. On the one hand, the maximum graph in BLGC is constructed mainly from the information on the former production plans. Thus, it achieves its effectiveness only several months after the start of production. On the other hand, the benefit of BLGC highly depends on the characteristics of the input information. More precisely, the higher is the difference between the input sequences the more effective BLGC is, because degrees of freedom can only be deduced from pairs of tasks that have been planned in both possible orders. As we have seen at our cooperation partners, the plans differ very much in sections of the assembly lines where re-balancing is performed often. A frequent re-balancing may be necessary, because fluctuations in demands of some individual options lead to highly varying average task times (see Section 2.1). However, it is common in industry that certain independent tasks i and j never change their relative position to each other in production plans. First of all, this is because the planners are responsible only for their own segment of the assembly line, and therefore only for a sub-set of tasks. Secondly, some sections of the assembly line are seldom re-balanced. Such sections contain “standard” tasks which have to be performed for all the product variants and, hence, their average times do not change over time. Such limitations on the variability of production plans, which are widespread in industry, significantly restrict the effectiveness of BLGC.We can (at least partially) compensate the mentioned drawbacks of BLGC by conducting interviews and integrating the information on the modular structure into the graph.Taking into account the modular structure is an effective and a low-cost way to improve the performance of BLGC dramatically. Modular structure is usually known before the start of production and thus is especially vital for BLGC before a critical number of production plans gets known. We provide a definition of modules, comment on the information availability in the industry and analyze their impact on the minimum graph and the maximum graph in Section 3.1.Interviews on the precedence relations are known to be rather costly (see below). However, they are indispensable because they can be applied to any pair of tasks at any time, including the sections with “standard” tasks and during the first months after the start of production. Section 3.2 discusses the impact of interviews. We examine two straightforward techniques: naive approach (Section 3.2.1) and interviews with continuous updates (Section 3.2.2).To evaluate proximity of the resulting minimum and maximum graphs to the target graph, we look at their difference in the order strength OS.Modularity became an essential approach of product and production design in the last decades in many industries. At design and production planning stages, the components (e.g., parts or sub-assemblies) are grouped based on certain characteristics to sets called modules, each with defined functionality and interfaces to other modules (Baldwin & Clark, 1997; Wilhelm, 1997). The main reasons for this prevalence of modularity are low costs and flexibility in major managerial decisions, such as outsourcing of processes, customization of the product and introduction of further innovations in product and process design. For an overview on possibilities, advantages and disadvantages of modularity, we refer the reader to, e.g., Ammer (1985), Muffatto and Roveda (2000), Pfaffmann and Stephan (2001), Jose and Tollenaere (2005), Jiao, Simpson, and Siddique (2007), Baldwin and Woodard (2009) or Pandremenos, Paralikas, Salonitis, Chryssolouris (2009).Information on modules can be incorporated at low cost. A precedence graph for the modules is documented by the planners and is already contained in the information system of the firm, as we observed at and discussed with our partners in the automotive industry. Even provided the (very uncommon) situation that this is not the case at some firms, it is manageable and relatively cheap to collect information on the precedence relations between modules. Finally, the precedence relations between modules are stable. Since products are designed along a modular or platform strategy today, the module graph is developed mostly in the product and production process design phase. Subsequent changes and refinements usually affect the relationships of tasks within the modules and only seldom between them.Modularity is often implemented top-down, i.e., the workpiece is divided into several main modules, which consist of sub-modules in their turn. For the sake of simplicity, we assume a two-level hierarchy with modules on the first level and all the tasks on the second level. Nevertheless, our approach can be applied to a general case with more aggregation levels.In line with the insights provided by our practice partners, we assume that all the tasks from any two different modules are connected with the same kind of precedence relations. In other words, either all the tasks from module A precede each of the tasks from module B, or vice versa, or all the tasks from module A are independent from each task in module B. Thus, modules contain valuable information on the precedence relations between tasks, which has to be incorporated into a learning precedence graph concept. Fig. 2shows a module graph that contains the four modules A, B, C and D, each module consists of four to six tasks. We name the precedence relations between the modules as intermodular precedence relations and the ones between tasks within each module as intramodular precedence relations.From a given module graph, we can derive dependencies for the minimum graph and independencies for the maximum graph.Minimum graph. Since A proceeds modules B and C (and indirectly Module D), all the tasks of A must proceed all the other tasks in the graph. We can directly use the intermodular precedence relations from A to the other modules (documented at the firm) as well as from B to D and from C to D to build the minimum graphG. The resulting graphGcontains 6⋅(6+4+5)+6⋅5+4⋅5=140 direct and indirect task to task precedence relations and, hence, has got an order strength of OS(G)=0.667.Maximum graph. In our example, all the tasks of Module B are independent to all the tasks of Module C. If we assume that the currently available maximum graph is based on a single sequence, i.e., its order strength equals to one, we can delete 6⋅4=24 direct or indirect precedence relations from it. Thus, its order strength decreases toOS(G‾)=0.886. Solely by considering the modular structure and one feasible sequence, we narrowed down the gap between the maximum and the minimum graph from 210 to 46arcs or to a difference in order strength of 0.219.Impact of modular structure. The modular structure is a valuable source of information. Let us look at the size of the gap between the available maximum and minimum graph, which is an upper bound of how different the maximum graph may still be from the (unknown) target graph. Let all the modules be of equal size and τ be the number of tasks per module. Then, for a typical assembly line in automobile industry of 1000–3000 tasks, the consideration of just 20 modules leads to a gap between the maximum and the target graph of not more thanτ-1|V|-1=50-11000-1≈0.05(!) (see Theorem 3 in Appendix II). Note that the size of the gap for graphs with equal-sized modules does not depend on the order strength of the (unknown) target graph or the order strength of the module graph. This is due to the fact that the numbers of deduced dependencies and independencies compensate each other to the same sum.Different types of interview methods have been proposed in the literature on the collection of information on precedence graphs. These are mainly approaches developed for robotic assemblies and, therefore, focus on parts or subassemblies of the product and the connections between them. For example, approaches based on CAD-data (e.g., Bourjault, 1987; De Fazio & Whitney, 1987; Homem de Mello & Sanderson, 1991; Lambert, 2006).This thread of the literature points out the important trade-off between the number of questions and their complexity. For example, we need only a few questions of the kind “Name all the parts that have to be mounted before part X”, but a lot of simple yes/no questions of the type “Is it necessary that part X is mounted before part Y?”. This trade-off is imminent to all the interview approaches. Thereby complex questions are prone to frequent faults and incompleteness of the answers. Moreover, it may be impossible to find a knowledgeable planner, who is able to provide the complete answer on a complex question.In general, the approaches with focus on parts do not immediately suite for final assemblies with high share of manual labor. We cannot equate parts and tasks, because a task may affect no part, one connection of parts or several parts simultaneously. Examples of those tasks are maintenance operations of a handling tool (no part), insertion of a clip (one connection) or transportation of a set of parts in a box (several parts). Further, such approaches were not developed for mixed assembly lines, i.e., with high product variety produced on the same line.Therefore in the following, we ask simple “yes/no” questions on the precedence relations between the tasks. I.e., the interviewer asks an omniscient planner for a pair (i,j) of tasks i,j∊V and gets answers “no”, “yes, i before j” or “yes, j before i”. Therefore, for a graph G=(V,E) the number of non-repeated questions cannot exceed|V|·(|V|-1)2.Interviews are known to be slow in approaching the target graph. Thus, Halpern, Sarisamlis, and Wand (1982) carried out interviews with “yes/no” questions and it took up to 4h to collect the complete information for the precedence graph consisting of less than 30 tasks. Since, firstly, the number of possible precedence relations and, thus, the number of questions increase quadratically to the number of tasks, and, secondly, there are several thousands of tasks to be performed at modern assembly lines, interviews are not justifiable as a stand-alone data collection method. But interviews are very useful as a part of a learning precedence graph concept, as we show below. In the following, we examine efficiency of interviews in detail.A straightforward, or naive, approach to interviews is to ask not repeating questions.Minimum graph. Assuming, no precedence relations are known in the beginning, the lower bound on the number of questions to get all the precedence relations of the target graph G=(V,E) equals to the number of direct precedence relations |E|. On the one hand, all the indirect precedence relations can be deduced from them, and on the other hand, fewer questions will not lead to the target graph since, by definition of direct precedence relations, no arc (i,j)∊E can be deduced from two other direct precedence relations.Now we are interested in the effects a limited number of questions q can have. At the start of the interview, the probability to find a pair of tasks i,j∊V,i≠j such that (i,j)∊EIor (j,i)∊EIequals the order strength. Since we avoid repetitions, the expected number of precedence relations X that can be found with q questions follows the hypergeometric distribution. The probability to get r precedence relations with q questions isP(X=r)=|EI|r|V|·(|V|-1)2-|EI|q-r|V|·(|V|-1)2q. Additionally, further indirect precedence relations can be potentially inferred from these answers.Maximum graph. Provided we possess the first maximum graph, then the lower bound on the number of questions to achieve the target graph is|V|·(|V|-1)2-|EI|.Note that it is not possible to deduce new independencies from already investigated independencies. In other words, the number of independencies to be asked for depends linearly on the order strength of the target graph (OStarget) and quadratically on the number of tasks. Therefore, the number of independencies after q questions follows a hypergeometric distribution and equals in expectation to 2q⋅(1−OStarget).However, the information on independency between tasks does not mandatory lead to a decrease in the order strength of the maximum graph. Fig. 3shows a part of a maximum graph with three tasks i, j and k. If the planer answers that there is no precedence relation between task i and task k, we can only conclude that (i,j)∉EIor (j,k)∉EI. It is not clear which precedence relation can be deleted. Thus, the indirect precedence relation from i to j will still exist implicitly as long as no information on the independency between i and j or between j and k is available (cf. also with Section 4.1).To avoid unnecessary questions, the matrix of known indirect precedence relations has to be updated after each new discovered dependency. Two effects on the interview and its results can be observed from this. First, the number of identified (direct and indirect) precedence relations increases, which is visible in the construction of the minimum graph. Second, the number of possible questions in the next step of the interview decreases, because we forbid asking for already discovered relations. The second effect helps to avoid redundant questions, which is relevant both in the construction of the minimum graph and maximum graph.The influence of the discussed effects depends not only on the number of tasks and the order strength, but also on the further parameters of the graph. Let us illustrate their impact on the minimum graph. Fig. 4shows two (target) graphs each with twelve tasks, |EI|=36 and an order strength of ∼0.55. In the right-hand graph no indirect precedence relations exist and, therefore, the expected number of precedence relations found after two questions is, according to the hypergeometric distribution and the formula introduced in Section 3.2.1,361·301662·1+362·300662·2≈1.09. In the left graph, it may happen that the answers on the two questions indicate precedence relations between (i,j) and (j,k). In such a case we can also infer on an (indirect) precedence relation between (i,k). Therefore the number of estimated precedence relations for the left graph is about 1.13, or about 3.59% higher than in the first graph (see Appendix III). This size of the effect becomes even larger with further questions.To estimate the effectiveness of the interviews, we conduct a Monte Carlo simulation. We generate nine groups of 25 instances (225 in total, 300 tasks per instance) of order strengths 0.1,0.2,…,0.9 with dataset generator SALBPGen of Otto, Otto, and Scholl (2013). For each instance, ten interviews each with 1000 randomly generated (but unique) questions asking an omniscient planner are simulated according to the continuous updates approach (Section 3.2.2).Fig. 5shows the results of the simulation for the minimum graph. The dotted line shows the number of dependencies that were named by the planner in the answers. As we see, this number is proportional to the order strength of the target graph. The solid line describes the number of all (including deduced) discovered precedence relations. As we observe, the number of deduced indirect precedence relations increases much faster with higher order strength. On average in our experiment, we got immediately 893 relations from the answers on interview questions for each target graph with order strength of about 0.9 (i.e., |EI|≈40365 direct and indirect precedence relations). From this, we can deduce further 4940 additional relations. In contrast, at low order strength, e.g., about 0.1, we found 95 relations directly and got only 10 more arcs by deducing on average. Still even for instances with order strength of 0.9, the achieved order strength of the minimum graph is as low as ∼0.14.The effect of interviews on the order strength of the maximum graph is negligent (see Fig. 6). The average number of the independencies, integrated into the maximum graph, is just 7. Almost all the discovered independencies does not impact on the order strength of the maximum graph because of the effects described in Section 3.2.1. Such independencies are marked as “latent” in Fig. 6.Recall that a useful measure for the quality of approximation of the (unknown) target graph is the gap in the achieved order strength between the maximum graph and the minimum graph. This gap was about 0.821–0.998 on average after the conducted interviews. In other words, the value-add of the interviews is much lower than incorporation of the modular structure. For a medium assembly line with 300 tasks and typical size of modules of 15–100 tasks per module, the gap in OS between the minimum graph and the maximum graph equals to about 0.06–0.33 (cf. Section 3.1 and Appendix III). This result is even more discouraging, given the high cost and time requirements of interviews. Therefore in the next section, we look at approaches to enhance effectiveness of the interviews.In this section, we introduce approaches to increase the effectiveness of the interviews. The first one, Focusedmin, specializes on the construction of the minimum graph (Section 4.1). The second one, Focusedmax, puts focus on the construction of the maximum graph (Section 4.2). In Section 4.3, we look at further important ideas to improve the assembly balances that can be reached by the minimum graph and the maximum graph.To improve the construction of the minimum graph, it is important to create beneficial conditions for deducing as many dependencies as possible. Two observations are useful here, which we united into the Focusedmin approach. First of all, it is worth asking on precedence relations of tasks, for which some precedence relations are already known. Therefore we set such tasks to priority group of tasks. We choose with a predefined probability p if the next pair should necessarily include at least one task from the priority group. However, for high values of OStarget, rather soon almost all the tasks in the minimum graph have precedence relations and the proposed enhancement loses its effectiveness. Therefore, secondly, we bound the number of tasks in the priority group. We add a suitable task into the priority group, only if such bound will not be exceeded. If the relationships of some task from the priority group to all the other tasks are already known (asked), then the task can be removed from the priority group and the next suitable task is added to it. In our pre-tests, the best results were achieved with a low number of tasks in the priority group.Minimum graph. From Fig. 7, we see that focused interviews with higher p outperform the interviews with lower p. For target graphs with an order strength of about 0.3 and at p=90% Focusedmin discovered seven times more dependencies as the continuous updates approach. Even for cases with a highly restrictive target graph (OS=0.9), Focusedmin still reveals about three times as many direct or indirect precedence relations.Maximum graph. Focusedmin has a positive effect on the maximum graph by decreasing the number (and share) of discovered latent independencies and thus increasing the share (and absolute number) of effective independencies (see Table 1). Indeed, especially at higher levels of p, Focusedmin results in a higher probability to ask on precedence relations between the two tasks in a “chain of tasks” with known precedence relations between them. For example, for instances with OStarget=0.1, Focusedmin with p=90% finds 63 effective independencies on average, which is nine times higher than in case of the continuous updates approach.Main problem of the interviews at constructing the maximum graph is that a lot of independencies cannot be incorporated into the maximum graph immediately and have to be stored as latent independencies unless the precedence relations of the tasks “in-between” are clarified. A way to overcome it, is to ask only about the direct precedence relations in the current maximum graph, keeping the maximum graph updated after each question. Thus, in the example in Fig. 3, we are allowed to ask about tasks (i and j) or (j and k), but not about tasks (i and k).Maximum graph. The resulting Focusedmax approach is very powerful (see Table 1). It can not only incorporate all the found independencies into the maximum graph, but it identifies much more independencies in total than the continuous updates approach. For example, in case OStarget=0.9, Focusedmax identified almost seven times more independencies (737 vs. 108 correspondently). The main driver behind of this effect is very simple to explain. Following Focusedmax approach, it is impossible to put questions to pairs of tasks that are indirectly related to each other in the target graph. Indeed, such tasks can never be related by a direct precedence relation in the maximum graph. For example, if the underlying target graph is the one in Fig. 1, we will never put questions to the following pairs of tasks: (1, 5) and (1, 6). Thus, on each step of Focusedmax, the probability to discover an independency equals to the share of pairs with independent tasks in the target graph among the still unknown precedence relations. It is#independentpairs#allpairs-#indirectlydependentpairs. This is much higher compared to the continuous updates approach, especially for high OStarget.Minimum graph. The side effect of Focusedmax is that the share of (direct and indirect) dependencies among pairs of tasks with still unknown precedence relation is much lower than in the continuous updates approach, especially for instances with high OStarget(Fig. 7).Below the line, the optimal solutions (or upper and lower bounds) based on the maximum and minimum graphs matter most. However, precedence relations can have very different consequences on the optimal balance. For example, let us take the target graph from Fig. 1, first maximum graph as 1→2→3→4→5→6 and set cycle time at c=4. If the only independency, which we discovered for the maximum graph is between tasks 2 and 3, then the best balance that we can achieve based on this graph is with five stations (e.g., {1},{3},{2},{4},{5,6}). However, if this only discovered independency is between tasks 3 and 4, we can achieve a balance with four stations (e.g., {1},{2},{4},{3,5,6}).In the following, we sketch some ideas, how to put questions selectively in the framework of Focusedmin and Focusedmax approaches. Thereby, the effectiveness of these approaches in terms of the number of discovered dependencies and independencies is preserved. However, the solutions (or bounds on the optimal solution) for the resulting maximum and minimum graphs tend to improve, as illustrated in the computational experiments in Section 6.Enhancing Focusedmax in construction of maximum graph. Often the reason of the large idle time at stations is that assignment of a suitable task to such stations is prohibited by precedence relations. Therefore it may be beneficial to ask on precedence relations of tasks belonging to the stations with the highest idle time in the first line.Let an optimal solution of the maximum graph at hand be available. In our example in Fig. 1 at cycle time of c=4, the first maximum graph (before we start interviews) provides the only possible solution {1},{2},{3},{4},{5,6}, which has five stations. The idle time on these stations is 0, 0, 3, 0 and 2, correspondently. The largest idle time is at the third station, to which task 3 belongs. Therefore, it seems worthwhile to ask on the precedence relations of task 3. According to the logics of Focusedmax, we select direct neighbors of task 3 and put questions to (2, 3) and (3, 4). This leads to a new maximum graph with a solution with just four stations.Note, that often assembly line instance cannot be solved to optimality within the reasonable time limit. In such a case, we rely on solutions that determine the best known upper bound.Enhancing Focusedmin in construction of minimum graph. The same logic can be applied to the minimum graph. We may accept into the priority group only tasks stemming from the stations with the lowest idle time. However, often, we cannot solve the instance to optimality. In this case, the available feasible assembly line balance, as a rule, does not contain important information to improve the lower bound of the minimum graph.Therefore, we propose different logic. From the bin packing problem, which we receive from SALBP after relaxing the precedence relations, we know that flexibility in assignment of the largest tasks is very important. Therefore, we take tasks into the priority group in the decreasing order of their task times.The new approach ELGC extends BLGC by considering the modular structure of the precedence graphs (Section 3.1), which is already available at firms in most cases, and simple, but effective interview techniques described in Sections 3.2 and 4. These additional pieces of information are utilized to improve both the maximum graph and the minimum graph. For the maximum graph, we look for the information on independencies of pairs of tasks, whereas for the minimum graph, we look for the precedence relations between the tasks. Overall, both the minimum graph and the maximum graph constructed according to ELGC will be much closer to the target graph than those in BLGC.Construction of the maximum graph. Similar to BLGC, we initialize the maximum graph with the first feasible input sequence, taken, for example, from a production plan. Afterwards, we integrate the information on the identified independencies from the modular structure (Section 3.1) and the conducted interviews (following one of the methods described in Sections 3.2 and 4). Note, that unless Focusedmax is used, not all independencies found in the interviews lead to deletion of direct arcs in the actual maximum graph (see Section 3.2). Therefore, we store information on such independencies until further information allows us to identify the correspondent direct arc to be removed from the graph. We continuously refine the maximum graph with each new available production plan or additional interviews.Construction of the minimum graph. We initialize the minimum graph as a set of tasks V with task times tj,j∊V but without any precedence relation. Then we incorporate the available information on dependencies between tasks from the modular structure and interviews. Further interviews help to approximate the minimum graph closer to the target graph.Fig. 8provides an overview of ELGC, its input data and the effect on the number of arcs in the maximum and the minimum graph.ELGC preserves the advantages of BLGC. ELGC is expandable, because additional information can be incorporated at any time to improve both the maximum and the minimum graph. E.g., if tasks have to be performed for alternative options they cannot underlie precedence relations, as Altemeier (2009, chap. 5) stated. As BLGC, our new approach can be implemented at low initial investment and is cheap in maintenance.ELGC overcomes the major disadvantages of BLGC. Firstly, the additional information can be incorporated into the first maximum graph and thus increases the available degrees of freedom in planning after the first input sequences significantly. Therefore, ELGC is already effective at the start of the production and the (semi-)automation of assembly line balancing can be applied much earlier than BLGC. Secondly, in ELGC in contrast to BLGC, independencies between tasks can be discovered with help of interviews even if their ordering remained the same in the available production plans.We present computational experiments and their results in this section. We will analyze the necessity of considering the modular structure and interviews in a learning graph concept and, specifically, the performance of the proposed extended learning graph concept.In our experiments, we compare different ELGC approaches, which take into account modular structure and interviews, and contrast their performance to the basic approach BLGC. We refer to ELGCplain as the method utilizing interviews with continuous updates. ELGCmaxO and ELGCminO, where O stands for “orientation at”, rely on Focusedmax and Focusedmin approach for interviews, correspondently. Both ELGCmaxO and ELGCminO incorporate techniques of solution-oriented interviews. To clarify the relative value-add of modules and interviews, we also evaluate ELGC without interviews and mark it ELGC-I.In Section 6.1, we describe the settings for our tests. We look at the performance of solution-oriented enhancements of interviews and describe parameters of ELGCmaxO and ELGCminO in Section 6.2. Afterwards, we compare the performance of BLGC and ELGC approaches before start of production, when only numbering of tasks is available (Section 6.3). The relative contribution of modules, interviews and interview enhancements is contrasted in Section 6.4. Section 6.5 looks at the performance dynamics of the methods, when new production sequences get available. Section 6.6 concludes with comparison of the recommended approach to the effectiveness of the manual planning.For our computational experiments, we generated 225 instances with the benchmark data generator SALBPGen by Otto et al. (2013). Every instance contains 300 tasks with task times from 1 to 1000 time units. We assume the cycle time of c=1000. Within each module, the task times follow one of the following three kinds of distribution, truncated normal distributions with mean value at 0.5⋅c, at 0.1⋅c or a bimodal truncated normal distribution with modes at 0.1⋅c and at 0.5⋅c. We build three groups of instances with 75 instances each: those with small (μ=15,τ=20), medium (μ=6,τ=50) and large (μ=3,τ=100) modules. We subdivide each group further into clusters of 25 instances, where the module graph has low (0.35), medium (0.60) or high (0.90) target order strength. We generated the instances in the following way. Firstly, we took the instances from the small (with 20 tasks per instance), medium (with 50 tasks per instance) and large (with 100 tasks per instance) benchmark data set of Otto et al. (2013), correspondently, and used them as modules. Afterwards, we generated the module graph with SALBPGen with the predefined target order strength and connected the modules with the modules connector (see Otto et al., 2013).The simulated interviews include 1000 questions per instance to pairs of tasks (i,j) with i,j∊V,i≠j and are conducted according to the correspondent learning graph approach. Overall, for each learning graph approach it is assumed that the initial numbering of tasks is available (referred to as sequence 1, or the first maximum graph). The further input sequences, utilized by the learning graph concept (see Section 3.2), are generated by successively assigning tasks at random to one of the remaining positions in the sequence with respect to the restrictions of the target graph. As mentioned in Section 3.2, certain parts of the assembly lines are re-balanced less often in practice. Therefore, we clustered the assembly line into three equally sized sections. In the middle section, input sequences are generated as described above, whereas we set the first and the last sections to be “standard” sections, where re-balancing is performed seldom. We model “standard sections” in the data generation process by assigning a high priority to each task of the section to stay at its actual position in the sequence, i.e., we incorporate additional priority values to the task if it is near its position in the last sequence. For details of the task assignment procedure we refer the reader to Klindworth et al. (2012) and Otto, Otto, and Scholl (2012). For each instance, we generated 20 input sequences. Of course, the more sequences are used, the larger effect on a learning precedence graph concept can be obtained. However, Klindworth et al. (2012) showed that the influence of additional sequences declines rapidly.To solve the SALB problems constructed on the received minimum, maximum and target graphs as well as to find bounds, we use the exact solution method SALOME by Scholl and Klein (1997). For each target and maximum graph, we set the time limit of the solution method to 100seconds. Overall, after taking the best relevant bounds from all the conducted experiments, the optimum of 198 out of the 225 instances is not known. Thus, we utilize the best known upper (UB*) and lower (LB*) bounds of the target graph for our analysis. For the minimum graph, we allow for more computation time (600seconds), since only the lower bounds and the optimal solutions of this graph are meaningful for a learning concept.The computational experiments were conducted on a personal computer with Intel®Core i7 with logical eight cores and 4.0GB RAM. Each test was conducted on one core of the processor only.ELGCmaxOAs discussed in Section 4.3, it is worth to solve periodically the currently available maximum graph. The tasks at the stations with the largest idle time within the optimal (or best known) solution are set to priority. We ask (randomly selected) questions to priority tasks, unless no priority tasks with unknown precedence relations are available.In our experiments, we update priorities of tasks after each q=40,50,100 or 200 questions by solving the current maximum graph with SALOME at 10seconds time limit. After 1000 questions, the resulting maximum graph is solved with 100seconds time limit, in line with our experiment design (Section 6.1). We contrast the results to the case, where no priorities are set, which we mark as q=1000.From Table 2, we see that application of the solution-oriented interviews with q=50 reduces average relative deviation from the best known solution by 1percentage point (ppt) from 3.7% to 2.6%. We can further improve the results, by taking minimum from these solutions and the intermediate solutions of the maximum graph. For example, in case of q=50, we have100050-1=19intermediate solutions. Here as well, the best result is achieved at moderate levels of q (q=50), where the average relative deviation was lowered by 1.6percentage point compared to the case of q=1000. With q=50, ELGCmaxO shows a better result in the majority of instances (128) and a worse result only in 30 cases out of 225.In the following, if we speak about ELGCmaxO, we imply that it was run at q=50.ELGCminOFollowing the gained insights from Section 4.1, we set p high and ask questions in the lexicographical order (p=100%). In the solution-oriented version of ELGCminO (referred further to as ELGCminO), the tasks are included into the priority group in the decreasing order to the task times (cf. Section 4.1). In the plain version of ELGCminO (referred further to as Focusedmin), no such rules are set.We solved the minimum graph at 600seconds time limit with SALOME, constructed after 1000 interview questions and incorporating modules. ELGCminO improved lower bound for 10 instances and by up to 3percentage point compared to Focusedmin. In just 3 cases, ELGCminO revealed a worse performance. So we conclude on the importance of the solution-oriented interview approach.An important quality measure for the learning precedence graph approaches, is the deviation of the upper bound for the maximum graph and the lower bound for the minimum graph from the optimal solution of the target graph (m*).UBmaximumgraphm∗-1shows, how far is the found assembly line balance from the one achieved if the full information on the precedence graph is available.1-LBminimumgraphm∗indicates, how close we approach the full-information solution from below. For the instances, where the optimal solutions of the target graph are unknown, we use UB* and LB* instead of m*.Basic BLGC performs poorly in the first months of the production, until the critical mass of historical production plans is available. Therefore, in this experiment we looked at graph construction approaches at their first application, where only the numbering of tasks is known and no further production plans. The maximum graph of BLGC in this case is a fully ordered sequence of tasks (OS=1), whereas the minimum graph contains no arcs (OS=0).From Table 3, we see that approaches with focused interviews perform best. The maximum graph, constructed with ELGCmaxO, reaches 2.2% average deviation from UB*, whereas the minimum graph, constructed with ELGCminO, is only by 0.09% away from LB*. However, it seems much harder to improve the lower bound for the minimum graph, at least when solving graphs with the SALOME algorithm. Thus, the solution of BLGC reaches already 0.5% deviation from the best known lower bound of the target graph. An unexpected, but a positive result, is that ELGCmaxO, which focuses on the maximum graph construction, performs slightly better than ELGCplain even in its non-core area – the construction of the minimum graph. The same is true for ELGCminO.Since the target graph is mostly unknown in the real-world, we can estimate the solution quality for learning graph approaches as the distance between the upper bound based on the maximum graph, and the lower bound based on the minimum graph (see Section 2.2). The relative gap between these bounds,UBmaximumgraphLBminimumgraph-1, expresses the worst case deviation of the UBmaximum graphand the (unknown) optimal solution of the target graph.We see that this indicator correctly predicts the relative performance of the maximum graph construction methods. The estimated gap to the target graph for the best maximum graph approach ELGCmaxO is 6.2% which roughly corresponds the actual gap to the best known lower bound of the target graph, which is 6.0%.In the following, we compare, how large is the relative contribution of modules, interviews and interview enhancements in the solutions of the best performing approaches ELGCmaxO and ELGCminO. We estimate it by taking difference between ELGC-I and BLGC (effect of modules), ELGCplain and ELGC-I (effect of interviews) and ELGCplain and ELGCmaxO/ELGCminO (effect of interview enhancements). In Fig. 9, we set the total improvement from BLGC to ELGCmaxO/ELGCminO to 100%. We can observe that the effect of interviews (with continuous updates) is rather modest. They bring 10% improvement for the minimum graph and just 4% improvement for the maximum graph. The largest effect results from the information on modules: 64% in case of the maximum graph and 71% in case of the minimum graph. If we look at the total effect of interviews (36% in case of the maximum and 29% in case of the minimum graph), the enhancements to the interviews have increased their performance by32%+4%4%=9and three times, correspondently.Overall, the interviews in total (with enhancements) perform extremely well. They even significantly outperform modules, if we look at the effectiveness of the revealed information, i.e., how much performance improvement is brought per task pair. Indeed, the overall effect of interviews comprises about the half of that of the modules (36% vs. 64% for the maximum and 29% vs. 71% for the minimum graph). However, the modules contain information on much more pairs of tasks:τ2·|VM|·(|VM|-1)2, where |VM| is the number of modules. This amounts to1002·3!1!·2!=30Kpairs in case of τ=100, 37.5K for τ=50 and 42K for τ=20 in our generated data. In other words, modules provide information on 30–42 times more task pairs than the conducted interviews with 1000 questions. Thus, the impact per task pair is about 15 times larger in case of interviews than that in case of modules.Let us look at the impact of modules and interviews in more detail. Tables 4 and 5provide differences in average relative deviations from UB* (or LB*, in absolute value): |BLGC–ELGC-I| provides the impact of modules and |ELGC-I−ELGCmaxO/minO| stays for the impact of interviews.For the maximum graph (Table 4), interviews bring solutions up to 13.8percentage point closer to that of the (unknown) target graph. Further, we can observe a “substitution effect” in our data. The information from the modular structure is incorporated first, so that modules have a “first-move” advantage in our experiment. Indeed, the optimal solution of a highly restrictive graph is likely to have a lot of idle time in its stations; so that the potential decrease in the required number of stations after some number of independencies is introduced, is high as well. To the contrary, a graph with a low order strength is, ceteris paribus, likely to have small idle times in its optimal solution, so that the potential to increase the number of stations after introducing additional independencies is, on average, lower.Therefore the contribution of interviews grows whenever modules provide less information on independencies. It is at a larger number of tasks per module and at higher OS of the module graph. Note, that the total improvement, from modules and interviews together, decreases in these cases, because the share of independencies in the target graph gets lower.For the minimum graph (Table 5), we observe an opposite situation with a “complementary effect”. It is more beneficially to introduce additional arcs into a more restrictive graph, because much more (indirect) dependencies can be additionally deduced. Indeed, the contribution of interviews is higher for higher order strength of the module graph. However, the results in Table 5 have to be interpreted with caution. Note, that the BLGC lower bound was improved only for a small number of instances (16% of instances).Since BLGC needs several production plans (or input sequences) to warming-up, it would be interesting to compare, how many production plans are needed in order for BLGC to catch up with ELGC approaches. In this section, we analyze the maximum graph only, since additional sequences do not lead to improvement in the minimum graph. We contrast the performance of BLGC to ELGCplain and ELGCmaxO.In line with experiments of Klindworth et al. (2012), additional input sequences allow to reach much better assembly balances based on the constructed maximum graph. Thus after 20 input sequences, BLGC reaches impressive 2.20% and ELGCmaxO even 0.91% in terms of average relative deviation from UB* (Table 6).With each additional sequence and if no further interviews are conducted, the difference between graph construction approaches flattens out. However, even after 20 input sequences, the result of ELGCmaxO (0.91%) cannot be completely caught up (with 1.08% of ELGCplain). On the other hand, the advantage from using the modular structure (ELGCplain compared to BLGC) remains at up to impressive 1percentage point.Overall, ELGCplain needs about 4 input sequences (i.e., about 4months) to outperform the initial performance of ELGCmaxO, whereas BLGC needs more that 20 input sequences (about 20months) to do it.Note that for a number of instances, the solution of the maximum graph was better than that of the target graph. It is a “time limit effect”. A more restrictive graph, which the maximum graph is in comparison to the target graph, is sometimes easier to solve. This effect is even more visible, if we take into account intermediate solutions of the maximum graph, for example, after 1, 2, 3, 4, 5, 10 and 15 sequences (see Table 7). In this case, even on average, ELGC approaches have negative average deviation from the target graph solution, i.e., they improve it by 0.23–0.26%. It is astonishing that this effect is observed for about one third of all instances.Since ELGCplain is much more restrictive than ELGCmaxO, incorporation of the intermediate solutions of the maximum graphs has a much larger positive impact on it. So that it almost reaches ELGCmaxO in its average deviation (−0.23 vs. −0.26%). Therefore, we recommend solving the maximum graph several times during its construction process.To check, whether the observed effect is originated solely due to the difference in the computational time, we repeated the test with the same time limit (300·8=2400seconds) for the target graphs. The solution for the target graph was improved for 19 instances only and therefore the observed trends remain the same. Still, in nearly 30% of all instances, ELGCmaxO and ELGCplain lead to better solutions than optimizing the target graph within the enlarged time limit. We conclude that the maximum graph contains enough degrees of freedom and often appears to be “easier to solve” than the target graph (see more on the difficulty, or “trickiness” of the SALBP instances in Otto et al., 2013).Perhaps, the most important measure for practitioners is the improvement on the solution quality of the maximum graph approach compared to the best manually planned task sequence. This measure is important, because it illustrates the benefits of learning graph approaches without the reference to the unknown target graph and, therefore, its optimal solution. The ratio is calculated as the relative benefit of the number of stations mbestin the best input task sequence and the number of stationsm¯∗found for the corresponding maximum graph within the time limit, thusImpr=mbestm¯∗-1.Table 8presents results for BLGC and ELGCmaxO, whereby we take into account solution of all the eight maximum graphs (after 1, 2, 3, 4, 5, 10, 15 and 20 sequences). The overall average improvement on the manual planning of BLGC and ELGCmaxO varies from 13.66% to 22.96%, correspondently. In other words, with help of BLGC without any additional effort or investments, it is possible to reduce the number of stations and, therefore, the operating costs enormously. Considering the modular structure and a 1000-question focused interview leads to additional improvements up to about 2.5percentage point compared to BLGC. The lower performance of both approaches at instances with higher order strength and larger modules results from better input task sequences, i.e., the best input sequence for those instances is, on average, about 6percentage point closer to UB*.

@&#CONCLUSIONS@&#
